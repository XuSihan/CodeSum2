[{"methodBody": ["METHOD_START", "{", "NodesInfoResponse   nodeInfos    =    client (  )  . admin (  )  . cluster (  )  . prepareNodesInfo (  )  . clear (  )  . execute (  )  . actionGet (  )  ;", "assertNotNull ( nodeInfos )  ;", "assertNotNull ( nodeInfos . getNodes (  )  )  ;", "assertEquals ( expected ,    nodeInfos . getNodes (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertNumberOfNodes"], "fileName": "org.elasticsearch.cloud.azure.classic.AbstractAzureComputeServiceTestCase"}, {"methodBody": ["METHOD_START", "{", "AbstractAzureComputeServiceTestCase . nodes . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clearAzureNodes"], "fileName": "org.elasticsearch.cloud.azure.classic.AbstractAzureComputeServiceTestCase"}, {"methodBody": ["METHOD_START", "{", "TransportService   transportService    =    internalCluster (  )  . getInstance ( TransportService . class ,    nodeName )  ;", "assertNotNull ( transportService )  ;", "DiscoveryNode   discoveryNode    =    transportService . getLocalNode (  )  ;", "assertNotNull ( discoveryNode )  ;", "if    (  (  . nodes . put ( discoveryNode . getName (  )  ,    discoveryNode )  )     !  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" Node    [  \"     +     ( discoveryNode . getName (  )  )  )     +     \"  ]    cannot   be   registered   twice   in   Azure \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["registerAzureNode"], "fileName": "org.elasticsearch.cloud.azure.classic.AbstractAzureComputeServiceTestCase"}, {"methodBody": ["METHOD_START", "{", "String   value    =    setting . get ( settings )  ;", "if    (  ( value    =  =    null )     |  |     (  ( Strings . hasLength ( value )  )     =  =    false )  )     {", "throw   new   IllegalArgumentException (  (  (  \" Missing   required   setting    \"     +     ( setting . getKey (  )  )  )     +     \"    for    \"  )  )  ;", "}", "return   value ;", "}", "METHOD_END"], "methodName": ["getRequiredSetting"], "fileName": "org.elasticsearch.cloud.azure.classic.management.AzureComputeServiceImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( refreshInterval )     !  =    null )     &  &     (  ( refreshInterval . millis (  )  )     !  =     0  )  )     {", "if    (  (  ( client )     !  =    null )     &  &     (  (  ( refreshInterval . millis (  )  )     <     0  )     |  |     (  (  ( System . currentTimeMillis (  )  )     -     ( lastRefresh )  )     <     ( refreshInterval . millis (  )  )  )  )  )     {", "if    ( logger . isTraceEnabled (  )  )", "logger . trace (  \" using   cache   to   retrieve   client \"  )  ;", "return   client ;", "}", "lastRefresh    =    System . currentTimeMillis (  )  ;", "}", "try    {", "gceJsonFactory    =    new   JacksonFactory (  )  ;", "logger . info (  \" starting   GCE   discovery   service \"  )  ;", "String   tokenServerEncodedUrl    =     ( GceMetadataService . GCE _ HOST . get ( settings )  )     +     \"  / computeMetadata / v 1  / instance / service - accounts / default / token \"  ;", "ComputeCredential   credential    =    new   ComputeCredential . Builder ( getGceHttpTransport (  )  ,    gceJsonFactory )  . setTokenServerEncodedUrl ( tokenServerEncodedUrl )  . build (  )  ;", "Access . doPrivilegedIOException ( credential :  : refreshToken )  ;", "logger . debug (  \" token    [  {  }  ]    will   expire   in    [  {  }  ]    s \"  ,    credential . getAccessToken (  )  ,    credential . getExpiresInSeconds (  )  )  ;", "if    (  ( credential . getExpiresInSeconds (  )  )     !  =    null )     {", "refreshInterval    =    TimeValue . timeValueSeconds (  (  ( credential . getExpiresInSeconds (  )  )     -     1  )  )  ;", "}", "Compute . Builder   builder    =    new   Compute . Builder ( getGceHttpTransport (  )  ,    gceJsonFactory ,    null )  . setApplicationName ( GceInstancesService . VERSION )  . setRootUrl (  . GCE _ ROOT _ URL . get ( settings )  )  ;", "if    ( GceInstancesService . RETRY _ SETTING . exists ( settings )  )     {", "TimeValue   maxWait    =    GceInstancesService . MAX _ WAIT _ SETTING . get ( settings )  ;", "RetryHttpInitializerWrapper   retryHttpInitializerWrapper ;", "if    (  ( maxWait . getMillis (  )  )     >     0  )     {", "retryHttpInitializerWrapper    =    new   RetryHttpInitializerWrapper ( credential ,    maxWait )  ;", "} else    {", "retryHttpInitializerWrapper    =    new   RetryHttpInitializerWrapper ( credential )  ;", "}", "builder . setHttpRequestInitializer ( retryHttpInitializerWrapper )  ;", "} else    {", "builder . setHttpRequestInitializer ( credential )  ;", "}", "this . client    =    builder . build (  )  ;", "}    catch    ( Exception   e )     {", "logger . warn (  \" unable   to   start   GCE   discovery   service \"  ,    e )  ;", "throw   new   IllegalArgumentException (  \" unable   to   start   GCE   discovery   service \"  ,    e )  ;", "}", "return   this . client ;", "}", "METHOD_END"], "methodName": ["client"], "fileName": "org.elasticsearch.cloud.gce.GceInstancesServiceImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( gceHttpTransport )     =  =    null )     {", "if    ( validateCerts )     {", "gceHttpTransport    =    GoogleNetHttpTransport . newTrustedTransport (  )  ;", "} else    {", "gceHttpTransport    =    new   NetHttpTransport . Builder (  )  . doNotValidateCertificate (  )  . build (  )  ;", "}", "}", "return   gceHttpTransport ;", "}", "METHOD_END"], "methodName": ["getGceHttpTransport"], "fileName": "org.elasticsearch.cloud.gce.GceInstancesServiceImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( gceHttpTransport )     =  =    null )     {", "gceHttpTransport    =    GoogleNetHttpTransport . newTrustedTransport (  )  ;", "}", "return   gceHttpTransport ;", "}", "METHOD_END"], "methodName": ["getGceHttpTransport"], "fileName": "org.elasticsearch.cloud.gce.GceMetadataService"}, {"methodBody": ["METHOD_START", "{", "final   URI   urlMetadataNetwork    =    new   URI ( GceMetadataService . GCE _ HOST . get ( settings )  )  . resolve (  \"  / computeMetadata / v 1  / instance /  \"  )  . resolve ( metadataPath )  ;", "logger . debug (  \" get   metadata   from    [  {  }  ]  \"  ,    urlMetadataNetwork )  ;", "HttpHeaders   headers ;", "try    {", "headers    =    Access . doPrivileged ( HttpHeaders :  : new )  ;", "GenericUrl   genericUrl    =    Access . doPrivileged (  (  )     -  >    new   GenericUrl ( urlMetadataNetwork )  )  ;", "headers . put (  \" Metadata - Flavor \"  ,     \" Google \"  )  ;", "HttpResponse   response    =    Access . doPrivilegedIOException (  (  )     -  >    getGceHttpTransport (  )  . createRequestFactory (  )  . buildGetRequest ( genericUrl )  . setHeaders ( headers )  . execute (  )  )  ;", "String   metadata    =    response . parseAsString (  )  ;", "logger . debug (  \" metadata   found    [  {  }  ]  \"  ,    metadata )  ;", "return   metadata ;", "}    catch    ( Exception   e )     {", "throw   new   IOException (  (  (  \" failed   to   fetch   metadata   from    [  \"     +    urlMetadataNetwork )     +     \"  ]  \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["metadata"], "fileName": "org.elasticsearch.cloud.gce.GceMetadataService"}, {"methodBody": ["METHOD_START", "{", "return   GceModule . computeServiceImpl ;", "}", "METHOD_END"], "methodName": ["getComputeServiceImpl"], "fileName": "org.elasticsearch.cloud.gce.GceModule"}, {"methodBody": ["METHOD_START", "{", "String   gceMetadataPath ;", "if    ( value . equals (  . GceAddressResolverType . GCE . configName )  )     {", "gceMetadataPath    =    Strings . replace (  . GceAddressResolverType . GCE . gceName ,     \"  {  { network }  }  \"  ,     \"  0  \"  )  ;", "} else", "if    ( value . equals (  . GceAddressResolverType . PRIVATE _ DNS . configName )  )     {", "gceMetadataPath    =     . GceAddressResolverType . PRIVATE _ DNS . gceName ;", "} else", "if    ( value . startsWith (  . GceAddressResolverType . PRIVATE _ IP . configName )  )     {", "String   network    =     \"  0  \"  ;", "String [  ]    privateIpConfig    =    value . split (  \"  :  \"  )  ;", "if    (  ( privateIpConfig . length )     =  =     3  )     {", "network    =    privateIpConfig [  2  ]  ;", "}", "gceMetadataPath    =    Strings . replace (  . GceAddressResolverType . PRIVATE _ IP . gceName ,     \"  {  { network }  }  \"  ,    network )  ;", "} else    {", "throw   new   IllegalArgumentException (  (  (  (  \"  [  \"     +    value )     +     \"  ]    is   not   one   of   the   supported   GCE   network . host   setting .     \"  )     +     \" Expecting    _ gce _  ,     _ gce : privateIp : X _  ,     _ gce : hostname _  \"  )  )  ;", "}", "try    {", "String   metadataResult    =    Access . doPrivilegedIOException (  (  )     -  >    gceMetadataService . metadata ( gceMetadataPath )  )  ;", "if    (  ( metadataResult    =  =    null )     |  |     (  ( metadataResult . length (  )  )     =  =     0  )  )     {", "throw   new   IOException (  (  (  (  (  \" no   gce   metadata   returned   from    [  \"     +    gceMetadataPath )     +     \"  ]    for    [  \"  )     +    value )     +     \"  ]  \"  )  )  ;", "}", "return   new   InetAddress [  ]  {    InetAddress . getByName ( metadataResult )     }  ;", "}    catch    ( IOException   e )     {", "throw   new   IOException (  (  (  \" IOException   caught   when   fetching   InetAddress   from    [  \"     +    gceMetadataPath )     +     \"  ]  \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.elasticsearch.cloud.gce.network.GceNameResolver"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "return   Controller . doPrivileged ( operation )  ;", "}", "METHOD_END"], "methodName": ["doPrivileged"], "fileName": "org.elasticsearch.cloud.gce.util.Access"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   Controller . doPrivileged ( operation )  ;", "}    catch    ( final   PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedIOException"], "fileName": "org.elasticsearch.cloud.gce.util.Access"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "Controller . doPrivileged (  (  ( PrivilegedAction < Void >  )     (  (  )     -  >     {", "action . run (  )  ;", "return   null ;", "}  )  )  )  ;", "}", "METHOD_END"], "methodName": ["doPrivilegedVoid"], "fileName": "org.elasticsearch.cloud.gce.util.Access"}, {"methodBody": ["METHOD_START", "{", "char [  ]    passphrase    =     \" keypass \"  . toCharArray (  )  ;", "KeyStore   ks    =    KeyStore . getInstance (  \" JKS \"  )  ;", "try    ( InputStream   stream    =     . class . getResourceAsStream (  \"  / test - node . jks \"  )  )     {", "assertNotNull (  \" can ' t   find   keystore   file \"  ,    stream )  ;", "ks . load ( stream ,    passphrase )  ;", "}", "KeyManagerFactory   kmf    =    KeyManagerFactory . getInstance (  \" SunX 5  0  9  \"  )  ;", "kmf . init ( ks ,    passphrase )  ;", "TrustManagerFactory   tmf    =    TrustManagerFactory . getInstance (  \" SunX 5  0  9  \"  )  ;", "tmf . init ( ks )  ;", "SSLContext   ssl    =    SSLContext . getInstance (  \" TLS \"  )  ;", "ssl . init ( kmf . getKeyManagers (  )  ,    tmf . getTrustManagers (  )  ,    null )  ;", "return   ssl ;", "}", "METHOD_END"], "methodName": ["getSSLContext"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureDiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "Path   tempDir    =    createTempDir (  )  ;", ". keyStoreFile    =    tempDir . resolve (  \" test - node . jks \"  )  ;", "try    ( InputStream   stream    =     . class . getResourceAsStream (  \"  / test - node . jks \"  )  )     {", "assertNotNull (  \" can ' t   find   keystore   file \"  ,    stream )  ;", "Files . copy ( stream ,     . keyStoreFile )  ;", "}", "}", "METHOD_END"], "methodName": ["setupKeyStore"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureDiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "AzureDiscoveryClusterFormationTests . logDir    =    createTempDir (  )  ;", "SSLContext   sslContext    =    AzureDiscoveryClusterFormationTests . getSSLContext (  )  ;", "AzureDiscoveryClusterFormationTests . httpsServer    =    MockHttpServer . createHttps ( new   InetSocketAddress ( InetAddress . getLoopbackAddress (  )  . getHostAddress (  )  ,     0  )  ,     0  )  ;", "AzureDiscoveryClusterFormationTests . httpsServer . setHttpsConfigurator ( new   HttpsConfigurator ( sslContext )  )  ;", "AzureDiscoveryClusterFormationTests . httpsServer . createContext (  \"  / subscription / services / hostedservices / myservice \"  ,     (    s )     -  >     {", "Headers   headers    =    s . getResponseHeaders (  )  ;", "headers . add (  \" Content - Type \"  ,     \" text / xml ;    charset = UTF -  8  \"  )  ;", "XMLOutputFactory   xmlOutputFactory    =    XMLOutputFactory . newFactory (  )  ;", "xmlOutputFactory . setProperty ( XMLOutputFactory . IS _ REPAIRING _ NAMESPACES ,    true )  ;", "StringWriter   out    =    new   StringWriter (  )  ;", "XMLStreamWriter   sw ;", "try    {", "sw    =    xmlOutputFactory . createXMLStreamWriter ( out )  ;", "sw . writeStartDocument (  )  ;", "String   namespace    =     \" http :  /  / schemas . microsoft . com / windowsazure \"  ;", "sw . setDefaultNamespace ( namespace )  ;", "sw . writeStartElement ( XMLConstants . DEFAULT _ NS _ PREFIX ,     \" HostedService \"  ,    namespace )  ;", "{", "sw . writeStartElement (  \" Deployments \"  )  ;", "{", "Path [  ]    files    =    FileSystemUtils . files ( AzureDiscoveryClusterFormationTests . logDir )  ;", "for    ( int   i    =     0  ;    i    <     ( files . length )  ;    i +  +  )     {", "Path   resolve    =    files [ i ]  . resolve (  \" transport . ports \"  )  ;", "if    ( Files . exists ( resolve )  )     {", "List < String >    addresses    =    Files . readAllLines ( resolve )  ;", "Collections . shuffle ( addresses ,    random (  )  )  ;", "String   address    =    addresses . get (  0  )  ;", "int   indexOfLastColon    =    address . lastIndexOf (  '  :  '  )  ;", "String   host    =    address . substring (  0  ,    indexOfLastColon )  ;", "String   port    =    address . substring (  ( indexOfLastColon    +     1  )  )  ;", "sw . writeStartElement (  \" Deployment \"  )  ;", "{", "sw . writeStartElement (  \" Name \"  )  ;", "sw . writeCharacters (  \" mydeployment \"  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" DeploymentSlot \"  )  ;", "sw . writeCharacters ( Production . name (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" Status \"  )  ;", "sw . writeCharacters ( Running . name (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" RoleInstanceList \"  )  ;", "{", "sw . writeStartElement (  \" RoleInstance \"  )  ;", "{", "sw . writeStartElement (  \" RoleName \"  )  ;", "sw . writeCharacters ( UUID . randomUUID (  )  . toString (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" IpAddress \"  )  ;", "sw . writeCharacters ( host )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" InstanceEndpoints \"  )  ;", "{", "sw . writeStartElement (  \" InstanceEndpoint \"  )  ;", "{", "sw . writeStartElement (  \" Name \"  )  ;", "sw . writeCharacters (  \" myendpoint \"  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" Vip \"  )  ;", "sw . writeCharacters ( host )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" PublicPort \"  )  ;", "sw . writeCharacters ( port )  ;", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "}", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "sw . writeEndDocument (  )  ;", "sw . flush (  )  ;", "final   byte [  ]    responseAsBytes    =    out . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  ;", "s . sendResponseHeaders (  2  0  0  ,    responseAsBytes . length )  ;", "OutputStream   responseBody    =    s . getResponseBody (  )  ;", "responseBody . write ( responseAsBytes )  ;", "responseBody . close (  )  ;", "}    catch    ( XMLStreamException   e )     {", "Loggers . getLogger ( AzureDiscoveryClusterFormationTests . class )  . error (  \" Failed   serializing   XML \"  ,    e )  ;", "throw   new   RuntimeException ( e )  ;", "}", "}  )  ;", "AzureDiscoveryClusterFormationTests . httpsServer . start (  )  ;", "}", "METHOD_END"], "methodName": ["startHttpd"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureDiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( internalCluster (  )  . size (  )  )  ;    i +  +  )     {", "internalCluster (  )  . stopRandomDataNode (  )  ;", "}", ". httpsServer . stop (  0  )  ;", ". httpsServer    =    null ;", ". logDir    =    null ;", "}", "METHOD_END"], "methodName": ["stopHttpd"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureDiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "ensureClusterSizeConsistency (  )  ;", "internalCluster (  )  . startDataOnlyNode (  )  ;", "ensureClusterSizeConsistency (  )  ;", "}", "METHOD_END"], "methodName": ["testJoin"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureDiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "final   String   node 1     =    internalCluster (  )  . startNode (  )  ;", "registerAzureNode ( node 1  )  ;", "expectThrows ( MasterNotDiscoveredException . class ,     (  )     -  >    client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setTimeout (  \"  1  0  0 ms \"  )  . get (  )  . getState (  )  . nodes (  )  . getId (  )  )  ;", "final   String   node 2     =    internalCluster (  )  . startNode (  )  ;", "registerAzureNode ( node 2  )  ;", "assertNotNull ( client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getId (  )  )  ;", "internalCluster (  )  . stopCurrent (  )  ;", "expectThrows ( MasterNotDiscoveredException . class ,     (  )     -  >    client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getId (  )  )  ;", "final   String   node 3     =    internalCluster (  )  . startNode (  )  ;", "registerAzureNode ( node 3  )  ;", "assertNotNull ( client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleOnlyMasterNodeElection"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureMinimumMasterNodesTests"}, {"methodBody": ["METHOD_START", "{", "Settings . Builder   settings    =    Settings . builder (  )  . put ( AzureComputeService . Management . SERVICE _ NAME _ SETTING . getKey (  )  ,     \" dummy \"  )  . put ( AzureComputeService . Discovery . HOST _ TYPE _ SETTING . getKey (  )  ,     \" private _ ip \"  )  ;", "final   String   node 1     =    internalCluster (  )  . startNode ( settings )  ;", "registerAzureNode ( node 1  )  ;", "assertNotNull ( client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setMasterNodeTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getMasterNodeId (  )  )  ;", "assertNumberOfNodes (  1  )  ;", "}", "METHOD_END"], "methodName": ["testOneNodeShouldRunUsingPrivateIp"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureSimpleTests"}, {"methodBody": ["METHOD_START", "{", "Settings . Builder   settings    =    Settings . builder (  )  . put ( AzureComputeService . Management . SERVICE _ NAME _ SETTING . getKey (  )  ,     \" dummy \"  )  . put ( AzureComputeService . Discovery . HOST _ TYPE _ SETTING . getKey (  )  ,     \" public _ ip \"  )  ;", "final   String   node 1     =    internalCluster (  )  . startNode ( settings )  ;", "registerAzureNode ( node 1  )  ;", "assertNotNull ( client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setMasterNodeTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getMasterNodeId (  )  )  ;", "assertNumberOfNodes (  1  )  ;", "}", "METHOD_END"], "methodName": ["testOneNodeShouldRunUsingPublicIp"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureSimpleTests"}, {"methodBody": ["METHOD_START", "{", "Settings . Builder   settings    =    Settings . builder (  )  . put ( AzureComputeService . Management . SERVICE _ NAME _ SETTING . getKey (  )  ,     \" dummy \"  )  . put ( AzureComputeService . Discovery . HOST _ TYPE _ SETTING . getKey (  )  ,     \" do _ not _ exist \"  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    internalCluster (  )  . startNode ( settings )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \" invalid   value   for   host   type    [ do _ not _ exist ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testOneNodeShouldRunUsingWrongSettings"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureSimpleTests"}, {"methodBody": ["METHOD_START", "{", "final   String   hostType    =    randomFrom ( AzureUnicastHostsProvider . HostType . values (  )  )  . getType (  )  ;", "logger . info (  (  \"  -  -  >    using   azure   host   type    \"     +    hostType )  )  ;", "final   Settings   settings    =    Settings . builder (  )  . put ( AzureComputeService . Management . SERVICE _ NAME _ SETTING . getKey (  )  ,     \" dummy \"  )  . put ( AzureComputeService . DHOST _ TYPE _ SETTING . getKey (  )  ,    hostType )  . build (  )  ;", "logger . info (  \"  -  -  >    start   first   node \"  )  ;", "final   String   node 1     =    internalCluster (  )  . startNode ( settings )  ;", "registerAzureNode ( node 1  )  ;", "assertNotNull ( client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setMasterNodeTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getMasterNodeId (  )  )  ;", "logger . info (  \"  -  -  >    start   another   node \"  )  ;", "final   String   node 2     =    internalCluster (  )  . startNode ( settings )  ;", "registerAzureNode ( node 2  )  ;", "assertNotNull ( client (  )  . admin (  )  . cluster (  )  . prepareState (  )  . setMasterNodeTimeout (  \"  1 s \"  )  . get (  )  . getState (  )  . nodes (  )  . getMasterNodeId (  )  )  ;", "assertNumberOfNodes (  2  )  ;", "}", "METHOD_END"], "methodName": ["testTwoNodesShouldRunUsingPrivateOrPublicIp"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureTwoStartedNodesTests"}, {"methodBody": ["METHOD_START", "{", "if    ( hostType    =  =     ( AzureUnicastHostsProvider . HostType . PRIVATE _ IP )  )     {", "final   InetAddress   privateIp    =    instance . getIPAddress (  )  ;", "if    ( privateIp    !  =    null )     {", "return   InetAddresses . toUriString ( privateIp )  ;", "} else    {", "logger . trace (  \" no   private   ip   provided .    ignoring    [  {  }  ]  .  .  .  \"  ,    instance . getInstanceName (  )  )  ;", "}", "} else", "if    ( hostType    =  =     ( AzureUnicastHostsProvider . HostType . PUBLIC _ IP )  )     {", "for    ( com . microsoft . windowsazure . management . compute . models . InstanceEndpoint   endpoint    :    instance . getInstanceEndpoints (  )  )     {", "if    ( publicEndpointName . equals ( endpoint . getName (  )  )  )     {", "return   format ( new   InetSocketAddress ( endpoint . getVirtualIPAddress (  )  ,    endpoint . getPort (  )  )  )  ;", "} else    {", "logger . trace (  \" ignoring   endpoint    [  {  }  ]    as   different   than    [  {  }  ]  \"  ,    endpoint . getName (  )  ,    publicEndpointName )  ;", "}", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["resolveInstanceAddress"], "fileName": "org.elasticsearch.discovery.azure.classic.AzureUnicastHostsProvider"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.discovery.azure.classic.DiscoveryAzureClassicClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "ClientConfiguration   clientConfiguration    =    new   ClientConfiguration (  )  ;", "clientConfiguration . setResponseMetadataCacheSize (  0  )  ;", "clientConfiguration . setProtocol (  . PROTOCOL _ SETTING . get ( settings )  )  ;", "if    (  . PROXY _ HOST _ SETTING . exists ( settings )  )     {", "String   proxyHost    =     . PROXY _ HOST _ SETTING . get ( settings )  ;", "Integer   proxyPort    =     . PROXY _ PORT _ SETTING . get ( settings )  ;", "try    ( SecureString   proxyUsername    =     . PROXY _ USERNAME _ SETTING . get ( settings )  ; SecureString   proxyPassword    =     . PROXY _ PASSWORD _ SETTING . get ( settings )  )     {", "clientConfiguration . withProxyHost ( proxyHost )  . withProxyPort ( proxyPort )  . withProxyUsername ( proxyUsername . toString (  )  )  . withProxyPassword ( proxyPassword . toString (  )  )  ;", "}", "}", "final   Random   rand    =    Randomness . get (  )  ;", "RetryPolicy   retryPolicy    =    new   RetryPolicy ( RetryCondition . NO _ RETRY _ CONDITION ,    new   RetryPolicy . BackoffStrategy (  )     {", "@ Override", "public   long   delayBeforeNextRetry ( AmazonWebServiceRequest   originalRequest ,    AmazonClientException   exception ,    int   retriesAttempted )     {", "logger . warn (  \" EC 2    API   request   failed ,    retry   again .    Reason   was :  \"  ,    exception )  ;", "return    1  0  0  0 L    *     (  ( long )     (  (  1  0  .  0     *     ( Math . pow (  2  ,     ( retriesAttempted    /     2  .  0  )  )  )  )     *     (  1  .  0     +     ( rand . nextDouble (  )  )  )  )  )  ;", "}", "}  ,     1  0  ,    false )  ;", "clientConfiguration . setRetryPolicy ( retryPolicy )  ;", "clientConfiguration . setSocketTimeout (  (  ( int )     (  . READ _ TIMEOUT _ SETTING . get ( settings )  . millis (  )  )  )  )  ;", "return   clientConfiguration ;", "}", "METHOD_END"], "methodName": ["buildConfiguration"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImpl"}, {"methodBody": ["METHOD_START", "{", "AWSCredentialsProvider   credentials ;", "try    ( SecureString   key    =     . ACCESS _ KEY _ SETTING . get ( settings )  ; SecureString   secret    =     . SECRET _ KEY _ SETTING . get ( settings )  )     {", "if    (  (  ( key . length (  )  )     =  =     0  )     &  &     (  ( secret . length (  )  )     =  =     0  )  )     {", "logger . debug (  \" Using   either   environment   variables ,    system   properties   or   instance   profile   credentials \"  )  ;", "credentials    =    new   DefaultAWSCredentialsProviderChain (  )  ;", "} else    {", "logger . debug (  \" Using   basic   key / secret   credentials \"  )  ;", "credentials    =    new   StaticCredentialsProvider ( new   com . amazonaws . auth . BasicAWSCredentials ( key . toString (  )  ,    secret . toString (  )  )  )  ;", "}", "}", "return   credentials ;", "}", "METHOD_END"], "methodName": ["buildCredentials"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImpl"}, {"methodBody": ["METHOD_START", "{", "String   endpoint    =    null ;", "if    (  . ENDPOINT _ SETTING . exists ( settings )  )     {", "endpoint    =     . ENDPOINT _ SETTING . get ( settings )  ;", "logger . debug (  \" using   explicit   ec 2    endpoint    [  {  }  ]  \"  ,    endpoint )  ;", "}", "return   endpoint ;", "}", "METHOD_END"], "methodName": ["findEndpoint"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImpl"}, {"methodBody": ["METHOD_START", "{", "ClientConfiguration   configuration    =    AwsEc 2 ServiceImpl . buildConfiguration ( logger ,    settings )  ;", "assertThat ( configuration . getResponseMetadataCacheSize (  )  ,    is (  0  )  )  ;", "assertThat ( configuration . getProtocol (  )  ,    is ( expectedProtocol )  )  ;", "assertThat ( configuration . getProxyHost (  )  ,    is ( expectedProxyHost )  )  ;", "assertThat ( configuration . getProxyPort (  )  ,    is ( expectedProxyPort )  )  ;", "assertThat ( configuration . getProxyUsername (  )  ,    is ( expectedProxyUsername )  )  ;", "assertThat ( configuration . getProxyPassword (  )  ,    is ( expectedProxyPassword )  )  ;", "assertThat ( configuration . getSocketTimeout (  )  ,    is ( expectedReadTimeout )  )  ;", "}", "METHOD_END"], "methodName": ["launchAWSConfigurationTest"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "AWSCredentials   credentials    =    AwsEc 2 ServiceImpl . buildCredentials ( logger ,    settings )  . getCredentials (  )  ;", "assertThat ( credentials . getAWSAccessKeyId (  )  ,    is ( expectedKey )  )  ;", "assertThat ( credentials . getAWSSecretKey (  )  ,    is ( expectedSecret )  )  ;", "}", "METHOD_END"], "methodName": ["launchAWSCredentialsWithElasticsearchSettingsTest"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  \" proxy . username \"  ,     \" aws _ proxy _ username \"  )  ;", "secureSettings . setString (  \" proxy . password \"  ,     \" aws _ proxy _ password \"  )  ;", "Settings   settings    =    Settings . builder (  )  . put (  \" protocol \"  ,     \" http \"  )  . put (  \" proxy . host \"  ,     \" aws _ proxy _ host \"  )  . put (  \" proxy . port \"  ,     8  0  8  0  )  . put (  \" read _ timeout \"  ,     \"  1  0 s \"  )  . setSecureSettings ( secureSettings )  . build (  )  ;", "launchAWSConfigurationTest ( settings ,    HTTP ,     \" aws _ proxy _ host \"  ,     8  0  8  0  ,     \" aws _ proxy _ username \"  ,     \" aws _ proxy _ password \"  ,     1  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testAWSConfigurationWithAwsSettings"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  \" access _ key \"  ,     \" aws _ key \"  )  ;", "secureSettings . setString (  \" secret _ key \"  ,     \" aws _ secret \"  )  ;", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( secureSettings )  . build (  )  ;", "launchAWSCredentialsWithElasticsearchSettingsTest ( settings ,     \" aws _ key \"  ,     \" aws _ secret \"  )  ;", "}", "METHOD_END"], "methodName": ["testAWSCredentialsWithElasticsearchAwsSettings"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "AWSCredentialsProvider   credentialsProvider    =    AwsEc 2 ServiceImpl . buildCredentials ( logger ,    EMPTY )  ;", "assertThat ( credentialsProvider ,    instanceOf ( DefaultAWSCredentialsProviderChain . class )  )  ;", "}", "METHOD_END"], "methodName": ["testAWSCredentialsWithSystemProviders"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "launchAWSConfigurationTest ( EMPTY ,    HTTPS ,    null ,     (  -  1  )  ,    null ,    null ,    DEFAULT _ SOCKET _ TIMEOUT )  ;", "}", "METHOD_END"], "methodName": ["testAWSDefaultConfiguration"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "String   endpoint    =    AwsEc 2 ServiceImpl . findEndpoint ( logger ,    EMPTY )  ;", "assertThat ( endpoint ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultEndpoint"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put ( AwsEc 2 Service . ENDPOINT _ SETTING . getKey (  )  ,     \" ec 2  . endpoint \"  )  . build (  )  ;", "String   endpoint    =     . findEndpoint ( logger ,    settings )  ;", "assertThat ( endpoint ,    is (  \" ec 2  . endpoint \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSpecificEndpoint"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "DescribeInstancesRequest   describeInstancesRequest    =    new   DescribeInstancesRequest (  )  . withFilters ( new   Filter (  \" instance - state - name \"  )  . withValues (  \" running \"  ,     \" pending \"  )  )  ;", "for    ( Map . Entry < String ,    List < String >  >    tagFilter    :    tags . entrySet (  )  )     {", "describeInstancesRequest . withFilters ( new   Filter (  (  \" tag :  \"     +     ( tagFilter . getKey (  )  )  )  )  . withValues ( tagFilter . getValue (  )  )  )  ;", "}", "if    (  !  ( availabilityZones . isEmpty (  )  )  )     {", "describeInstancesRequest . withFilters ( new   Filter (  \" availability - zone \"  )  . withValues ( availabilityZones )  )  ;", "}", "return   describeInstancesRequest ;", "}", "METHOD_END"], "methodName": ["buildDescribeInstancesRequest"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider"}, {"methodBody": ["METHOD_START", "{", "List < DiscoveryNode >    discoNodes    =    new   ArrayList <  >  (  )  ;", "DescribeInstancesResult   descInstances ;", "try    {", "descInstances    =    SocketAccess . doPrivileged (  (  )     -  >    client . describeInstances ( buildDescribeInstancesRequest (  )  )  )  ;", "}    catch    ( AmazonClientException   e )     {", "logger . info (  \" Exception   while   retrieving   instance   list   from   AWS   API :     {  }  \"  ,    e . getMessage (  )  )  ;", "logger . debug (  \" Full   exception :  \"  ,    e )  ;", "return   discoNodes ;", "}", "logger . trace (  \" building   dynamic   unicast   discovery   nodes .  .  .  \"  )  ;", "for    ( Reservation   reservation    :    descInstances . getReservations (  )  )     {", "for    ( Instance   instance    :    reservation . getInstances (  )  )     {", "if    (  !  ( groups . isEmpty (  )  )  )     {", "List < GroupIdentifier >    instanceSecurityGroups    =    instance . getSecurityGroups (  )  ;", "List < String >    securityGroupNames    =    new   ArrayList ( instanceSecurityGroups . size (  )  )  ;", "List < String >    securityGroupIds    =    new   ArrayList ( instanceSecurityGroups . size (  )  )  ;", "for    ( GroupIdentifier   sg    :    instanceSecurityGroups )     {", "securityGroupNames . add ( sg . getGroupName (  )  )  ;", "securityGroupIds . add ( sg . getGroupId (  )  )  ;", "}", "if    ( bindAnyGroup )     {", "if    (  ( Collections . disjoint ( securityGroupNames ,    groups )  )     &  &     ( Collections . disjoint ( securityGroupIds ,    groups )  )  )     {", "logger . trace (  \" filtering   out   instance    {  }    based   on   groups    {  }  ,    not   part   of    {  }  \"  ,    instance . getInstanceId (  )  ,    instanceSecurityGroups ,    groups )  ;", "continue ;", "}", "} else    {", "if    (  !  (  ( securityGroupNames . containsAll ( groups )  )     |  |     ( securityGroupIds . containsAll ( groups )  )  )  )     {", "logger . trace (  \" filtering   out   instance    {  }    based   on   groups    {  }  ,    does   not   include   all   of    {  }  \"  ,    instance . getInstanceId (  )  ,    instanceSecurityGroups ,    groups )  ;", "continue ;", "}", "}", "}", "String   address    =    null ;", "if    ( hostType . equals ( AwsEc 2 Service . HostType . PRIVATE _ DNS )  )     {", "address    =    instance . getPrivateDnsName (  )  ;", "} else", "if    ( hostType . equals ( AwsEc 2 Service . HostType . PRIVATE _ IP )  )     {", "address    =    instance . getPrivateIpAddress (  )  ;", "} else", "if    ( hostType . equals ( AwsEc 2 Service . HostType . PUBLIC _ DNS )  )     {", "address    =    instance . getPublicDnsName (  )  ;", "} else", "if    ( hostType . equals ( AwsEc 2 Service . HostType . PUBLIC _ IP )  )     {", "address    =    instance . getPublicIpAddress (  )  ;", "} else", "if    ( hostType . startsWith ( AwsEc 2 Service . HostType . TAG _ PREFIX )  )     {", "String   tagName    =    hostType . substring ( AwsEc 2 Service . HostType . TAG _ PREFIX . length (  )  )  ;", "logger . debug (  \" reading   hostname   from    [  {  }  ]    instance   tag \"  ,    tagName )  ;", "List < com . amazonaws . services . ec 2  . model . Tag >    tags    =    instance . getTags (  )  ;", "for    ( com . amazonaws . services . ec 2  . model . Tag   tag    :    tags )     {", "if    ( tag . getKey (  )  . equals ( tagName )  )     {", "address    =    tag . getValue (  )  ;", "logger . debug (  \" using    [  {  }  ]    as   the   instance   address \"  ,    address )  ;", "}", "}", "} else    {", "throw   new   IllegalArgumentException (  (  ( hostType )     +     \"    is   unknown   for   host _ type \"  )  )  ;", "}", "if    ( address    !  =    null )     {", "try    {", "TransportAddress [  ]    addresses    =    transportService . addressesFromString ( address ,     1  )  ;", "for    ( int   i    =     0  ;    i    <     ( addresses . length )  ;    i +  +  )     {", "logger . trace (  \" adding    {  }  ,    address    {  }  ,    transport _ address    {  }  \"  ,    instance . getInstanceId (  )  ,    address ,    addresses [ i ]  )  ;", "discoNodes . add ( new   DiscoveryNode ( instance . getInstanceId (  )  ,     (  (  (  \"  # cloud -  \"     +     ( instance . getInstanceId (  )  )  )     +     \"  -  \"  )     +    i )  ,    addresses [ i ]  ,    Collections . emptyMap (  )  ,    Collections . emptySet (  )  ,    CURRENT . minimumCompatibilityVersion (  )  )  )  ;", "}", "}    catch    ( Exception   e )     {", "final   String   finalAddress    =    address ;", "logger . warn (  (  ( Supplier <  ?  >  )     (  (  )     -  >    new   ParameterizedMessage (  \" failed   to   add    {  }  ,    address    {  }  \"  ,    instance . getInstanceId (  )  ,    finalAddress )  )  )  ,    e )  ;", "}", "} else    {", "logger . trace (  \" not   adding    {  }  ,    address   is   null ,    host _ type    {  }  \"  ,    instance . getInstanceId (  )  ,    hostType )  ;", "}", "}", "}", "logger . debug (  \" using   dynamic   discovery   nodes    {  }  \"  ,    discoNodes )  ;", "return   discoNodes ;", "}", "METHOD_END"], "methodName": ["fetchDynamicNodes"], "fileName": "org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.discovery.ec2.CloudAwsClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "Ec 2 DiscoveryClusterFormationTests . logDir    =    createTempDir (  )  ;", "Ec 2 DiscoveryClusterFormationTests . httpServer    =    MockHttpServer . createHttp ( new   InetSocketAddress ( InetAddress . getLoopbackAddress (  )  . getHostAddress (  )  ,     0  )  ,     0  )  ;", "Ec 2 DiscoveryClusterFormationTests . httpServer . createContext (  \"  /  \"  ,     (    s )     -  >     {", "Headers   headers    =    s . getResponseHeaders (  )  ;", "headers . add (  \" Content - Type \"  ,     \" text / xml ;    charset = UTF -  8  \"  )  ;", "String   action    =    null ;", "for    ( NameValuePair   parse    :    URLEncodedUtils . parse ( IOUtils . toString ( s . getRequestBody (  )  )  ,    StandardCharsets . UTF _  8  )  )     {", "if    (  \" Action \"  . equals ( parse . getName (  )  )  )     {", "action    =    parse . getValue (  )  ;", "break ;", "}", "}", "assertThat ( action ,    equalTo (  \" DescribeInstances \"  )  )  ;", "XMLOutputFactory   xmlOutputFactory    =    XMLOutputFactory . newFactory (  )  ;", "xmlOutputFactory . setProperty ( XMLOutputFactory . IS _ REPAIRING _ NAMESPACES ,    true )  ;", "StringWriter   out    =    new   StringWriter (  )  ;", "XMLStreamWriter   sw ;", "try    {", "sw    =    xmlOutputFactory . createXMLStreamWriter ( out )  ;", "sw . writeStartDocument (  )  ;", "String   namespace    =     \" http :  /  / ec 2  . amazonaws . com / doc /  2  0  1  3  -  0  2  -  0  1  /  \"  ;", "sw . setDefaultNamespace ( namespace )  ;", "sw . writeStartElement ( XMLConstants . DEFAULT _ NS _ PREFIX ,     \" DescribeInstancesResponse \"  ,    namespace )  ;", "{", "sw . writeStartElement (  \" requestId \"  )  ;", "sw . writeCharacters ( UUID . randomUUID (  )  . toString (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" reservationSet \"  )  ;", "{", "Path [  ]    files    =    FileSystemUtils . files ( Ec 2 DiscoveryClusterFormationTests . logDir )  ;", "for    ( int   i    =     0  ;    i    <     ( files . length )  ;    i +  +  )     {", "Path   resolve    =    files [ i ]  . resolve (  \" transport . ports \"  )  ;", "if    ( Files . exists ( resolve )  )     {", "List < String >    addresses    =    Files . readAllLines ( resolve )  ;", "Collections . shuffle ( addresses ,    random (  )  )  ;", "sw . writeStartElement (  \" item \"  )  ;", "{", "sw . writeStartElement (  \" reservationId \"  )  ;", "sw . writeCharacters ( UUID . randomUUID (  )  . toString (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" instancesSet \"  )  ;", "{", "sw . writeStartElement (  \" item \"  )  ;", "{", "sw . writeStartElement (  \" instanceId \"  )  ;", "sw . writeCharacters ( UUID . randomUUID (  )  . toString (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" imageId \"  )  ;", "sw . writeCharacters ( UUID . randomUUID (  )  . toString (  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" instanceState \"  )  ;", "{", "sw . writeStartElement (  \" code \"  )  ;", "sw . writeCharacters (  \"  1  6  \"  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" name \"  )  ;", "sw . writeCharacters (  \" running \"  )  ;", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" privateDnsName \"  )  ;", "sw . writeCharacters ( addresses . get (  0  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" dnsName \"  )  ;", "sw . writeCharacters ( addresses . get (  0  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" instanceType \"  )  ;", "sw . writeCharacters (  \" m 1  . medium \"  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" placement \"  )  ;", "{", "sw . writeStartElement (  \" availabilityZone \"  )  ;", "sw . writeCharacters (  \" use - east -  1 e \"  )  ;", "sw . writeEndElement (  )  ;", "sw . writeEmptyElement (  \" groupName \"  )  ;", "sw . writeStartElement (  \" tenancy \"  )  ;", "sw . writeCharacters (  \" default \"  )  ;", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" privateIpAddress \"  )  ;", "sw . writeCharacters ( addresses . get (  0  )  )  ;", "sw . writeEndElement (  )  ;", "sw . writeStartElement (  \" ipAddress \"  )  ;", "sw . writeCharacters ( addresses . get (  0  )  )  ;", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "}", "}", "}", "sw . writeEndElement (  )  ;", "}", "sw . writeEndElement (  )  ;", "sw . writeEndDocument (  )  ;", "sw . flush (  )  ;", "final   byte [  ]    responseAsBytes    =    out . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  ;", "s . sendResponseHeaders (  2  0  0  ,    responseAsBytes . length )  ;", "OutputStream   responseBody    =    s . getResponseBody (  )  ;", "responseBody . write ( responseAsBytes )  ;", "responseBody . close (  )  ;", "}    catch    ( XMLStreamException   e )     {", "Loggers . getLogger ( Ec 2 DiscoveryClusterFormationTests . class )  . error (  \" Failed   serializing   XML \"  ,    e )  ;", "throw   new   RuntimeException ( e )  ;", "}", "}  )  ;", "Ec 2 DiscoveryClusterFormationTests . httpServer . start (  )  ;", "}", "METHOD_END"], "methodName": ["startHttpd"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( internalCluster (  )  . size (  )  )  ;    i +  +  )     {", "internalCluster (  )  . stopRandomDataNode (  )  ;", "}", ". httpServer . stop (  0  )  ;", ". httpServer    =    null ;", ". logDir    =    null ;", "}", "METHOD_END"], "methodName": ["stopHttpd"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "assertNoTimeout ( client (  )  . admin (  )  . cluster (  )  . prepareHealth (  )  . setWaitForNodes ( Integer . toString (  2  )  )  . get (  )  )  ;", "internal (  )  . startDataOnlyNode (  )  ;", "assertNoTimeout ( client (  )  . admin (  )  . cluster (  )  . prepareHealth (  )  . setWaitForNodes ( Integer . toString (  3  )  )  . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["testJoin"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryClusterFormationTests"}, {"methodBody": ["METHOD_START", "{", "if    (  ( AwsEc 2 Service . AUTO _ ATTRIBUTE _ SETTING . get ( settings )  )     =  =    false )     {", "return   Settings . EMPTY ;", "}", "Settings . Builder   attrs    =    Settings . builder (  )  ;", "final   URL   url ;", "final   URLConnection   urlConnection ;", "try    {", "url    =    new   URL ( azMetadataUrl )  ;", ". logger . debug (  \" obtaining   ec 2     [ placement / availability - zone ]    from   ec 2    meta - data   url    {  }  \"  ,    url )  ;", "urlConnection    =    SocketAccess . doPrivilegedIOException ( url :  : openConnection )  ;", "urlConnection . setConnectTimeout (  2  0  0  0  )  ;", "}    catch    ( IOException   e )     {", "throw   new   UncheckedIOException ( e )  ;", "}", "try    ( InputStream   in    =    SocketAccess . doPrivilegedIOException ( urlConnection :  : getInputStream )  ; BufferedReader   urlReader    =    new   BufferedReader ( new   InputStreamReader ( in ,    StandardCharsets . UTF _  8  )  )  )     {", "String   metadataResult    =    urlReader . readLine (  )  ;", "if    (  ( metadataResult    =  =    null )     |  |     (  ( metadataResult . length (  )  )     =  =     0  )  )     {", "throw   new   IllegalStateException (  (  \" no   ec 2    metadata   returned   from    \"     +    url )  )  ;", "} else    {", "attrs . put (  (  ( NODE _ ATTRIBUTES . getKey (  )  )     +     \" aws _ availability _ zone \"  )  ,    metadataResult )  ;", "}", "}    catch    ( IOException   e )     {", ". logger . error (  \" failed   to   get   metadata   for    [ placement / availability - zone ]  \"  ,    e )  ;", "}", "return   attrs . build (  )  ;", "}", "METHOD_END"], "methodName": ["getAvailabilityZoneNodeAttributes"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPlugin"}, {"methodBody": ["METHOD_START", "{", "Settings   additional    =    getNodeAttributes ( settings ,    url )  ;", "if    ( expected    =  =    null )     {", "assertTrue ( additional . isEmpty (  )  )  ;", "} else    {", "assertEquals ( expected ,    additional . get (  (  ( NODE _ ATTRIBUTES . getKey (  )  )     +     \" aws _ availability _ zone \"  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["assertNodeAttributes"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "Settings   realSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . AUTO _ ATTRIBUTE _ SETTING . getKey (  )  ,    true )  . put ( settings )  . build (  )  ;", "return    . getAvailabilityZoneNodeAttributes ( realSettings ,    url )  ;", "}", "METHOD_END"], "methodName": ["getNodeAttributes"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "Path   zoneUrl    =    createTempFile (  )  ;", "Files . write ( zoneUrl ,    Arrays . asList (  \" us - e -  1 c \"  )  )  ;", "assertNodeAttributes ( EMPTY ,    zoneUrl . toUri (  )  . toURL (  )  . toString (  )  ,     \" us - e -  1 c \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAttributes"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "UncheckedIOException   e    =    expectThrows ( UncheckedIOException . class ,     (  )     -  >    getNodeAttributes ( Settings . EMPTY ,     \" bogus \"  )  )  ;", "assertNotNull ( e . getCause (  )  )  ;", "String   msg    =    e . getCause (  )  . getMessage (  )  ;", "assertTrue ( msg ,    msg . contains (  \" no   protocol :    bogus \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAttributesBogusUrl"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put ( AwsEc 2 Service . AUTO _ ATTRIBUTE _ SETTING . getKey (  )  ,    false )  . build (  )  ;", "assertNodeAttributes ( settings ,     \" bogus \"  ,    null )  ;", "}", "METHOD_END"], "methodName": ["testNodeAttributesDisabled"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "Path   zoneUrl    =    createTempFile (  )  ;", "IllegalStateException   e    =    expectThrows ( IllegalStateException . cs ,     (  )     -  >    getNodeAttributes ( Settings . EMPTY ,    zoneUrl . toUri (  )  . toURL (  )  . toString (  )  )  )  ;", "assertTrue ( e . getMessage (  )  ,    e . getMessage (  )  . contains (  \" no   ec 2    metadata   returned \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAttributesEmpty"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "Path   dne    =    createTempDir (  )  . resolve (  \" dne \"  )  ;", "assertNodeAttributes ( EMPTY ,    dne . toUri (  )  . toURL (  )  . toString (  )  ,    null )  ;", "}", "METHOD_END"], "methodName": ["testNodeAttributesErrorLenient"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryPluginTests"}, {"methodBody": ["METHOD_START", "{", "return   buildDynamicNodes ( nodeSettings ,    nodes ,    null )  ;", "}", "METHOD_END"], "methodName": ["buildDynamicNodes"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "AwsEc 2 Service   awsEc 2 Service    =    new   AwsEc 2 ServiceMock ( nodeSettings ,    nodes ,    tagsList )  ;", "AwsEc 2 UnicastHostsProvider   provider    =    new   AwsEc 2 UnicastHostsProvider ( nodeSettings ,    transportService ,    awsEc 2 Service )  ;", "List < DiscoveryNode >    Nodes    =    provider . buildDynamicNodes (  )  ;", "logger . debug (  \"  -  -  >    nodes   found :     {  }  \"  ,    Nodes )  ;", "return   Nodes ;", "}", "METHOD_END"], "methodName": ["buildDynamicNodes"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Ec 2 DiscoveryTests . threadPool    =    new   TestThreadPool ( Ec 2 DiscoveryTests . class . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["createThreadPool"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "NamedWriteableRegistry   namedWriteableRegistry    =    new   NamedWriteableRegistry ( Collections . emptyList (  )  )  ;", "final   Transport   transport    =    new   MockTcpTransport ( Settings . EMPTY ,    Ec 2 DiscoveryTests . threadPool ,    BigArrays . NON _ RECYCLING _ INSTANCE ,    new   NoneCircuitBreakerService (  )  ,    namedWriteableRegistry ,    new   NetworkService ( Collections . emptyList (  )  )  ,    Version . CURRENT )     {", "@ Override", "public   TransportAddress [  ]    addressesFromString ( String   address ,    int   perAddressLimit )    throws   UnknownHostException    {", "return   new   TransportAddress [  ]  {    poorMansDNS . getOrDefault ( address ,    buildNewFakeTransportAddress (  )  )     }  ;", "}", "}  ;", "transportService    =    new   test . transport . MockTransportService ( Settings . EMPTY ,    transport ,    Ec 2 DiscoveryTests . threadPool ,    TransportService . NOOP _ TRANSPORT _ INTERCEPTOR ,    null )  ;", "}", "METHOD_END"], "methodName": ["createTransportService"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "if    (  ( Ec 2 DiscoveryTests . threadPool )     !  =    null )     {", "terminate ( Ec 2 DiscoveryTests . threadPool )  ;", "Ec 2 DiscoveryTests . threadPool    =    null ;", "}", "}", "METHOD_END"], "methodName": ["stopThreadPool"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomInt (  1  0  )  ;", "Settings   nodeSettings    =    Settings . builder (  )  . build (  )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes )  ;", "assertThat ( Nodes ,    hasSize ( nodes )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultSettings"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomIntBetween (  5  ,     1  0  )  ;", "Settings   nodeSettings    =    Settings . builder (  )  . putList (  (  ( this . TAG _ SETTING . getKey (  )  )     +     \" stage \"  )  ,     \" prod \"  ,     \" preprod \"  )  . build (  )  ;", "int   prodInstances    =     0  ;", "List < List < Tag >  >    tagsList    =    new   ArrayList <  >  (  )  ;", "for    ( int   node    =     0  ;    node    <    nodes ;    node +  +  )     {", "List < Tag >    tags    =    new   ArrayList <  >  (  )  ;", "if    ( randomBoolean (  )  )     {", "tags . add ( new   Tag (  \" stage \"  ,     \" prod \"  )  )  ;", "if    ( randomBoolean (  )  )     {", "tags . add ( new   Tag (  \" stage \"  ,     \" preprod \"  )  )  ;", "prodInstances +  +  ;", "}", "} else    {", "tags . add ( new   Tag (  \" stage \"  ,     \" dev \"  )  )  ;", "if    ( randomBoolean (  )  )     {", "tags . add ( new   Tag (  \" stage \"  ,     \" preprod \"  )  )  ;", "}", "}", "tagsList . add ( tags )  ;", "}", "logger . info (  \" started    [  {  }  ]    instances   with    [  {  }  ]    stage = prod   tag \"  ,    nodes ,    prodInstances )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes ,    tagsList )  ;", "assertThat ( Nodes ,    hasSize ( prodInstances )  )  ;", "}", "METHOD_END"], "methodName": ["testFilterByMultipleTags"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomIntBetween (  5  ,     1  0  )  ;", "Settings   nodeSettings    =    Settings . builder (  )  . put (  (  ( this . TAG _ SETTING . getKey (  )  )     +     \" stage \"  )  ,     \" prod \"  )  . build (  )  ;", "int   prodInstances    =     0  ;", "List < List < Tag >  >    tagsList    =    new   ArrayList <  >  (  )  ;", "for    ( int   node    =     0  ;    node    <    nodes ;    node +  +  )     {", "List < Tag >    tags    =    new   ArrayList <  >  (  )  ;", "if    ( randomBoolean (  )  )     {", "tags . add ( new   Tag (  \" stage \"  ,     \" prod \"  )  )  ;", "prodInstances +  +  ;", "} else    {", "tags . add ( new   Tag (  \" stage \"  ,     \" dev \"  )  )  ;", "}", "tagsList . add ( tags )  ;", "}", "logger . info (  \" started    [  {  }  ]    instances   with    [  {  }  ]    stage = prod   tag \"  ,    nodes ,    prodInstances )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes ,    tagsList )  ;", "assertThat ( Nodes ,    hasSize ( prodInstances )  )  ;", "}", "METHOD_END"], "methodName": ["testFilterByTags"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings . Builder   builder    =    Settings . builder (  )  . put ( AwsEc 2 Service . NODE _ CACHE _ TIME _ SETTING . getKey (  )  ,     \"  5  0  0 ms \"  )  ;", "AwsEc 2 Service   awsEc 2 Service    =    new   AwsEc 2 ServiceMock ( Settings . EMPTY ,     1  ,    null )  ;", ". DummyEc 2 HostProvider   provider    =    new    . DummyEc 2 HostProvider ( builder . build (  )  ,    transportService ,    awsEc 2 Service )     {", "@ Override", "protected   List < DiscoveryNode >    fetchDynamicNodes (  )     {", "( fetchCount )  +  +  ;", "return    . this . buildDynamicNodes ( EMPTY ,     1  )  ;", "}", "}  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "provider . buildDynamicNodes (  )  ;", "}", "assertThat ( provider . fetchCount ,    is (  1  )  )  ;", "Thread . sleep (  1  0  0  0 L )  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "provider . buildDynamicNodes (  )  ;", "}", "assertThat ( provider . fetchCount ,    is (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetNodeListCached"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "AwsEc 2 Service   awsEc 2 Service    =    new   AwsEc 2 ServiceMock ( Settings . EMPTY ,     1  ,    null )  ;", ". DummyEc 2 HostProvider   provider    =    new    . DummyEc 2 HostProvider ( Settings . EMPTY ,    transportService ,    awsEc 2 Service )     {", "@ Override", "protected   List < DiscoveryNode >    fetchDynamicNodes (  )     {", "( fetchCount )  +  +  ;", "return   new   ArrayList <  >  (  )  ;", "}", "}  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "provider . buildDynamicNodes (  )  ;", "}", "assertThat ( provider . fetchCount ,    is (  3  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetNodeListEmptyCache"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . HOST _ TYPE _ SETTING . getKey (  )  ,     \" does _ not _ exist \"  )  . build (  )  ;", "IllegalArgumentException   exception    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >     {", "buildDynamicNodes ( nodeSettings ,     1  )  ;", "}  )  ;", "assertThat ( exception . getMessage (  )  ,    containsString (  \" does _ not _ exist   is   unknown   for   host _ type \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidHostType"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomInt (  1  0  )  ;", "for    ( int   i    =     0  ;    i    <    nodes ;    i +  +  )     {", "String   instanceId    =     \" node \"     +     ( i    +     1  )  ;", "poorMansDNS . put (  (  (  ( AmazonEC 2 Mock . PREFIX _ PRIVATE _ DNS )     +    instanceId )     +     ( AmazonEC 2 Mock . SUFFIX _ PRIVATE _ DNS )  )  ,    buildNewFakeTransportAddress (  )  )  ;", "}", "Settings   nodeSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . HOST _ TYPE _ SETTING . getKey (  )  ,     \" private _ dns \"  )  . build (  )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes )  ;", "assertThat ( Nodes ,    hasSize ( nodes )  )  ;", "int   node    =     1  ;", "for    ( DiscoveryNode   Node    :    Nodes )     {", "String   instanceId    =     \" node \"     +     ( node +  +  )  ;", "TransportAddress   address    =    Node . getAddress (  )  ;", "TransportAddress   expected    =    poorMansDNS . get (  (  (  ( AmazonEC 2 Mock . PREFIX _ PRIVATE _ DNS )     +    instanceId )     +     ( AmazonEC 2 Mock . SUFFIX _ PRIVATE _ DNS )  )  )  ;", "assertEquals ( address ,    expected )  ;", "}", "}", "METHOD_END"], "methodName": ["testPrivateDns"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomInt (  1  0  )  ;", "for    ( int   i    =     0  ;    i    <    nodes ;    i +  +  )     {", "poorMansDNS . put (  (  ( AmazonEC 2 Mock . PREFIX _ PRIVATE _ IP )     +     ( i    +     1  )  )  ,    buildNewFakeTransportAddress (  )  )  ;", "}", "Settings   nodeSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . HOST _ TYPE _ SETTING . getKey (  )  ,     \" private _ ip \"  )  . build (  )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes )  ;", "assertThat ( Nodes ,    hasSize ( nodes )  )  ;", "int   node    =     1  ;", "for    ( DiscoveryNode   Node    :    Nodes )     {", "TransportAddress   address    =    Node . getAddress (  )  ;", "TransportAddress   expected    =    poorMansDNS . get (  (  ( AmazonEC 2 Mock . PREFIX _ PRIVATE _ IP )     +     ( node +  +  )  )  )  ;", "assertEquals ( address ,    expected )  ;", "}", "}", "METHOD_END"], "methodName": ["testPrivateIp"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomInt (  1  0  )  ;", "for    ( int   i    =     0  ;    i    <    nodes ;    i +  +  )     {", "String   instanceId    =     \" node \"     +     ( i    +     1  )  ;", "poorMansDNS . put (  (  (  ( AmazonEC 2 Mock . PREFIX _ PUBLIC _ DNS )     +    instanceId )     +     ( AmazonEC 2 Mock . SUFFIX _ PUBLIC _ DNS )  )  ,    buildNewFakeTransportAddress (  )  )  ;", "}", "Settings   nodeSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . HOST _ TYPE _ SETTING . getKey (  )  ,     \" public _ dns \"  )  . build (  )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes )  ;", "assertThat ( Nodes ,    hasSize ( nodes )  )  ;", "int   node    =     1  ;", "for    ( DiscoveryNode   Node    :    Nodes )     {", "String   instanceId    =     \" node \"     +     ( node +  +  )  ;", "TransportAddress   address    =    Node . getAddress (  )  ;", "TransportAddress   expected    =    poorMansDNS . get (  (  (  ( AmazonEC 2 Mock . PREFIX _ PUBLIC _ DNS )     +    instanceId )     +     ( AmazonEC 2 Mock . SUFFIX _ PUBLIC _ DNS )  )  )  ;", "assertEquals ( address ,    expected )  ;", "}", "}", "METHOD_END"], "methodName": ["testPublicDns"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomInt (  1  0  )  ;", "for    ( int   i    =     0  ;    i    <    nodes ;    i +  +  )     {", "poorMansDNS . put (  (  ( AmazonEC 2 Mock . PREFIX _ PUBLIC _ IP )     +     ( i    +     1  )  )  ,    buildNewFakeTransportAddress (  )  )  ;", "}", "Settings   nodeSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . HOST _ TYPE _ SETTING . getKey (  )  ,     \" public _ ip \"  )  . build (  )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes )  ;", "assertThat ( Nodes ,    hasSize ( nodes )  )  ;", "int   node    =     1  ;", "for    ( DiscoveryNode   Node    :    Nodes )     {", "TransportAddress   address    =    Node . getAddress (  )  ;", "TransportAddress   expected    =    poorMansDNS . get (  (  ( AmazonEC 2 Mock . PREFIX _ PUBLIC _ IP )     +     ( node +  +  )  )  )  ;", "assertEquals ( address ,    expected )  ;", "}", "}", "METHOD_END"], "methodName": ["testPublicIp"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "int   nodes    =    randomIntBetween (  5  ,     1  0  )  ;", "String [  ]    addresses    =    new   String [ nodes ]  ;", "for    ( int   node    =     0  ;    node    <    nodes ;    node +  +  )     {", "addresses [ node ]     =     \"  1  9  2  .  1  6  8  .  0  .  \"     +     ( node    +     1  )  ;", "poorMansDNS . put (  (  \" node \"     +     ( node    +     1  )  )  ,    new   TransportAddress ( InetAddress . getByName ( addresses [ node ]  )  ,     9  3  0  0  )  )  ;", "}", "Settings   nodeSettings    =    Settings . builder (  )  . put ( AwsEc 2 Service . HOST _ TYPE _ SETTING . getKey (  )  ,     \" tag : foo \"  )  . build (  )  ;", "List < List < Tag >  >    tagsList    =    new   ArrayList <  >  (  )  ;", "for    ( int   node    =     0  ;    node    <    nodes ;    node +  +  )     {", "List < Tag >    tags    =    new   ArrayList <  >  (  )  ;", "tags . add ( new   Tag (  \" foo \"  ,     (  \" node \"     +     ( node    +     1  )  )  )  )  ;", "tagsList . add ( tags )  ;", "}", "logger . info (  \" started    [  {  }  ]    instances \"  ,    nodes )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( nodeSettings ,    nodes ,    tagsList )  ;", "assertThat ( Nodes ,    hasSize ( nodes )  )  ;", "for    ( DiscoveryNode   Node    :    Nodes )     {", "TransportAddress   address    =    Node . getAddress (  )  ;", "TransportAddress   expected    =    poorMansDNS . get ( Node . getName (  )  )  ;", "assertEquals ( address ,    expected )  ;", "}", "}", "METHOD_END"], "methodName": ["testReadHostFromTag"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( DISCOVERY _ HOSTS _ PROVIDER _ SETTING . getKey (  )  ,     \" ec 2  \"  )  . build (  )  ;", "internalCluster (  )  . startNode ( nodeSettings )  ;", "ClusterResponse   response    =    client (  )  . admin (  )  . cluster (  )  . prepare (  )  . setPersistentSettings ( Settings . builder (  )  . put (  \" discovery . zen . minimum _ master _ nodes \"  ,     1  )  )  . setTransientSettings ( Settings . builder (  )  . put (  \" discovery . zen . minimum _ master _ nodes \"  ,     1  )  )  . get (  )  ;", "Integer   min    =    response . getPersistentSettings (  )  . getAsInt (  \" discovery . zen . minimum _ master _ nodes \"  ,    null )  ;", "assertThat ( min ,    is (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testMinimumMasterNodesStart"], "fileName": "org.elasticsearch.discovery.ec2.Ec2DiscoveryUpdateSettingsTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   in    =    null ;", "String   metadataUrl    =     ( AwsEc 2 ServiceImpl . EC 2  _ METADATA _ URL )     +     ( type . e )  ;", "try    {", "URL   url    =    new   URL ( metadataUrl )  ;", "logger . debug (  \" obtaining   ec 2    hostname   from   ec 2    meta - data   url    {  }  \"  ,    url )  ;", "URLConnection   urlConnection    =    SocketAccess . doPrivilegedIOException ( url :  : openConnection )  ;", "urlConnection . setConnectTimeout (  2  0  0  0  )  ;", "in    =    SocketAccess . doPrivilegedIOException ( urlConnection :  : getInputStream )  ;", "BufferedReader   urlReader    =    new   BufferedReader ( new   InputStreamReader ( in ,    StandardCharsets . UTF _  8  )  )  ;", "String   metadataResult    =    urlReader . readLine (  )  ;", "if    (  ( metadataResult    =  =    null )     |  |     (  ( metadataResult . length (  )  )     =  =     0  )  )     {", "throw   new   IOException (  (  (  (  (  \" no   gce   metadata   returned   from    [  \"     +    url )     +     \"  ]    for    [  \"  )     +     ( type . configName )  )     +     \"  ]  \"  )  )  ;", "}", "return   new   InetAddress [  ]  {    InetAddress . getByName ( metadataResult )     }  ;", "}    catch    ( IOException   e )     {", "throw   new   IOException (  (  (  \" IOException   caught   when   fetching   InetAddress   from    [  \"     +    metadataUrl )     +     \"  ]  \"  )  ,    e )  ;", "}    finally    {", "IOUtils . closeWhileHandlingException ( in )  ;", "}", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NameResolver"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ local _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "InetAddress [  ]    addresses    =    networkService . resolveBindHostAddresses ( null )  ;", "assertThat ( addresses ,    arrayContaining ( networkService . resolveBindHostAddresses ( new   String [  ]  {     \"  _ local _  \"     }  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testNetworkHostCoreLocal"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" local - ipv 4  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  : privateDns _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" local - hostname \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2PrivateDns"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  : privateIp _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" local - ipv 4  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2PrivateIp"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  : privateIpv 4  _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" local - ipv 4  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2PrivateIpv4"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  : publicDns _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" public - hostname \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2PublicDns"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  : publicIp _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" public - ipv 4  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2PublicIp"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,     \"  _ ec 2  : publicIpv 4  _  \"  )  . build (  )  ;", "Service   networkService    =    new   Service ( Collections . singletonList ( new   Ec 2 NameResolver ( nodeSettings )  )  )  ;", "try    {", "networkService . resolveBindHostAddresses ( null )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" public - ipv 4  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNetworkHostEc2PublicIpv4"], "fileName": "org.elasticsearch.discovery.ec2.Ec2NetworkTests"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "return   Controller . doPrivileged ( operation )  ;", "}", "METHOD_END"], "methodName": ["doPrivileged"], "fileName": "org.elasticsearch.discovery.ec2.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   Controller . doPrivileged ( operation )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedIOException"], "fileName": "org.elasticsearch.discovery.ec2.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.discovery.file.FileBasedDiscoveryClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "MockTcpTransport   transport    =    new   MockTcpTransport ( Settings . EMPTY ,    threadPool ,    BigArrays . NON _ RECYCLING _ INSTANCE ,    new   NoneCircuitBreakerService (  )  ,    new   NamedWriteableRegistry ( Collections . emptyList (  )  )  ,    new   NetworkService ( Collections . emptyList (  )  )  )     {", "@ Override", "public   BoundTransportAddress   boundAddress (  )     {", "return   new   BoundTransportAddress ( new   TransportAddress [  ]  {    new   TransportAddress ( InetAddress . getLoopbackAddress (  )  ,     9  3  0  0  )     }  ,    new   TransportAddress ( InetAddress . getLoopbackAddress (  )  ,     9  3  0  0  )  )  ;", "}", "}  ;", "transportService    =    new   test . transport . MockTransportService ( Settings . EMPTY ,    transport ,    threadPool ,    TransportService . NOOP _ TRANSPORT _ INTERCEPTOR ,    null )  ;", "}", "METHOD_END"], "methodName": ["createTransportSvc"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "super . setUp (  )  ;", "threadPool    =    new   TestThreadPool (  . class . getName (  )  )  ;", "executorService    =    Executors . newSingleThreadExecutor (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "final   Path   homeDir    =    createTempDir (  )  ;", "final   Settings   settings    =    Settings . builder (  )  . put ( PATH _ HOME _ SETTING . getKey (  )  ,    homeDir )  . build (  )  ;", "final   Path   configPath ;", "if    ( randomBoolean (  )  )     {", "configPath    =    homeDir . resolve (  \" config \"  )  ;", "} else    {", "configPath    =    createTempDir (  )  ;", "}", "final   Path   discoveryFilePath    =    configPath . resolve (  \" discovery - file \"  )  ;", "Files . createDirectories ( discoveryFilePath )  ;", "final   Path   unicastHostsPath    =    discoveryFilePath . resolve (  . UNICAST _ HOSTS _ FILE )  ;", "try    ( BufferedWriter   writer    =    Files . newBufferedWriter ( unicastHostsPath )  )     {", "writer . write ( String . join (  \"  \\ n \"  ,    hostEntries )  )  ;", "}", "return   new    ( new   Environment ( settings ,    configPath )  ,    transportService ,    executorService )  . buildDynamicNodes (  )  ;", "}", "METHOD_END"], "methodName": ["setupAndRunHostProvider"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "terminate ( executorService )  ;", "}    finally    {", "try    {", "terminate ( threadPool )  ;", "}    finally    {", "super . tDown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "final   List < String >    hostEntries    =    Arrays . asList (  \"  # comment ,    should   be   ignored \"  ,     \"  1  9  2  .  1  6  8  .  0  .  1  \"  ,     \"  1  9  2  .  1  6  8  .  0  .  2  :  9  3  0  5  \"  ,     \"  2  5  5  .  2  5  5  .  2  3  .  1  5  \"  )  ;", "final   List < DiscoveryNode >    nodes    =    setupAndRunHostProvider ( hostEntries )  ;", "assertEquals (  (  ( hostEntries . size (  )  )     -     1  )  ,    nodes . size (  )  )  ;", "assertEquals (  \"  1  9  2  .  1  6  8  .  0  .  1  \"  ,    nodes . get (  0  )  . getAddress (  )  . getAddress (  )  )  ;", "assertEquals (  9  3  0  0  ,    nodes . get (  0  )  . getAddress (  )  . getPort (  )  )  ;", "assertEquals (  (  (  . UNICAST _ HOST _ PREFIX )     +     \"  1  9  2  .  1  6  8  .  0  .  1  _  0  #  \"  )  ,    nodes . get (  0  )  . getId (  )  )  ;", "assertEquals (  \"  1  9  2  .  1  6  8  .  0  .  2  \"  ,    nodes . get (  1  )  . getAddress (  )  . getAddress (  )  )  ;", "assertEquals (  9  3  0  5  ,    nodes . get (  1  )  . getAddress (  )  . getPort (  )  )  ;", "assertEquals (  (  (  . UNICAST _ HOST _ PREFIX )     +     \"  1  9  2  .  1  6  8  .  0  .  2  :  9  3  0  5  _  0  #  \"  )  ,    nodes . get (  1  )  . getId (  )  )  ;", "assertEquals (  \"  2  5  5  .  2  5  5  .  2  3  .  1  5  \"  ,    nodes . get (  2  )  . getAddress (  )  . getAddress (  )  )  ;", "assertEquals (  9  3  0  0  ,    nodes . get (  2  )  . getAddress (  )  . getPort (  )  )  ;", "assertEquals (  (  (  . UNICAST _ HOST _ PREFIX )     +     \"  2  5  5  .  2  5  5  .  2  3  .  1  5  _  0  #  \"  )  ,    nodes . get (  2  )  . getId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildDynamicNodes"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "final   List < String >    hostEntries    =    Collections . emptyList (  )  ;", "final   List < DNode >    nodes    =    setupAndRunHostProvider ( hostEntries )  ;", "assertEquals (  0  ,    nodes . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyUnicastHostsFile"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "List < String >    hostEntries    =    Arrays . asList (  \"  1  9  2  .  1  6  8  .  0  .  1  :  9  3  0  0  :  9  3  0  0  \"  )  ;", "List < DNode >    nodes    =    setupAndRunHostProvider ( hostEntries )  ;", "assertEquals (  0  ,    nodes . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidHostEntries"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "List < String >    hostEntries    =    Arrays . asList (  \"  1  9  2  .  1  6  8  .  0  .  1  :  9  3  0  0  :  9  3  0  0  \"  ,     \"  1  9  2  .  1  6  8  .  0  .  1  :  9  3  0  1  \"  )  ;", "List < DNode >    nodes    =    setupAndRunHostProvider ( hostEntries )  ;", "assertEquals (  1  ,    nodes . size (  )  )  ;", "assertEquals (  \"  1  9  2  .  1  6  8  .  0  .  1  \"  ,    nodes . get (  0  )  . getAddress (  )  . getAddress (  )  )  ;", "assertEquals (  9  3  0  1  ,    nodes . get (  0  )  . getAddress (  )  . getPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSomeInvalidHostEntries"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "final   Settings   settings    =    Settings . builder (  )  . put ( PATH _ HOME _ SETTING . getKey (  )  ,    createTempDir (  )  )  . build (  )  ;", "final   Environment   environment    =    TestEnvironment . newEnvironment ( settings )  ;", "final      provider    =    new    ( environment ,    transportService ,    executorService )  ;", "final   List < DiscoveryNode >    nodes    =    provider . buildDynamicNodes (  )  ;", "assertEquals (  0  ,    nodes . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnicastHostsDoesNotExist"], "fileName": "org.elasticsearch.discovery.file.FileBasedUnicastHostsProviderTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.discovery.gce.DiscoveryGceClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( expected ,    client (  )  . admin (  )  . cluster (  )  . prepareNodesInfo (  )  . clear (  )  . get (  )  . getNodes (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertNumberOfNodes"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoverTests"}, {"methodBody": ["METHOD_START", "{", "GceDiscoverTests . nodes . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clearGceNodes"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoverTests"}, {"methodBody": ["METHOD_START", "{", "final   TransportService   transportService    =    internalCluster (  )  . getInstance ( TransportService . class ,    nodeName )  ;", "assertNotNull ( transportService )  ;", "final   DiscoveryNode   discoveryNode    =    transportService . getLocalNode (  )  ;", "assertNotNull ( discoveryNode )  ;", "if    (  (  . nodes . put ( discoveryNode . getName (  )  ,    discoveryNode )  )     !  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" Node    [  \"     +     ( discoveryNode . getName (  )  )  )     +     \"  ]    cannot   be   registered   twice \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["registerGceNode"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoverTests"}, {"methodBody": ["METHOD_START", "{", "final   String   masterNode    =    internalCluster (  )  . startMasterOnlyNode (  )  ;", ". registerGceNode ( masterNode )  ;", "ClusterStateResponse   clusterStateResponse    =    client ( masterNode )  . admin (  )  . cluster (  )  . prepareState (  )  . setMasterNodeTimeout (  \"  1 s \"  )  . clear (  )  . setNodes ( true )  . get (  )  ;", "assertNotNull ( clusterStateResponse . getState (  )  . nodes (  )  . getMasterNodeId (  )  )  ;", "final   String   secondNode    =    internalCluster (  )  . startNode (  )  ;", ". registerGceNode ( secondNode )  ;", "clusterStateResponse    =    client ( secondNode )  . admin (  )  . cluster (  )  . prepareState (  )  . setMasterNodeTimeout (  \"  1 s \"  )  . clear (  )  . setNodes ( true )  . setLocal ( true )  . get (  )  ;", "assertNotNull ( clusterStateResponse . getState (  )  . nodes (  )  . getMasterNodeId (  )  )  ;", "assertNoTimeout ( client (  )  . admin (  )  . cluster (  )  . prepareHealth (  )  . setWaitForNodes ( Integer . toString (  2  )  )  . get (  )  )  ;", ". assertNumberOfNodes (  2  )  ;", "final   String   thirdNode    =    internalCluster (  )  . startDataOnlyNode (  )  ;", ". registerGceNode ( thirdNode )  ;", "assertNoTimeout ( client (  )  . admin (  )  . cluster (  )  . prepareHealth (  )  . setWaitForNodes ( Integer . toString (  3  )  )  . get (  )  )  ;", ". assertNumberOfNodes (  3  )  ;", "}", "METHOD_END"], "methodName": ["testJoin"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoverTests"}, {"methodBody": ["METHOD_START", "{", "GceUnicastHostsProvider   provider    =    new   GceUnicastHostsProvider ( nodeSettings ,    gceInstancesService ,    transportService ,    new   NetworkService ( Collections . emptyList (  )  )  )  ;", "List < DiscoveryNode >    Nodes    =    provider . buildDynamicNodes (  )  ;", "logger . info (  \"  -  -  >    nodes   found :     {  }  \"  ,    Nodes )  ;", "return   Nodes ;", "}", "METHOD_END"], "methodName": ["buildDynamicNodes"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "GceDiscoveryTests . threadPool    =    new   TestThreadPool ( GceDiscoveryTests . class . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["createThreadPool"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "transportService    =    MockTransportService . createNewService ( EMPTY ,    CURRENT ,    GceDiscoveryTests . threadPool ,    null )  ;", "}", "METHOD_END"], "methodName": ["createTransportService"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "projectName    =    getTestName (  )  . toLowerCase ( Locale . ROOT )  ;", "if    ( projectName . startsWith (  \" test \"  )  )     {", "projectName    =    projectName . substring (  \" test \"  . length (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setProjectName"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "if    (  ( mock )     !  =    null )     {", "mock . clo (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stopGceComputeService"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "if    (  ( GceDiscoveryTests . threadPool )     !  =    null )     {", "GceDiscoveryTests . threadPool . shutdownNow (  )  ;", "GceDiscoveryTests . threadPool    =    null ;", "}", "}", "METHOD_END"], "methodName": ["stopThreadPool"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . EMPTY ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "try    {", "buildDynamicNodes ( mock ,    nodeSettings )  ;", "fail (  \" We   expect   an   IllegalArgumentException   for   incomplete   settings \"  )  ;", "}    catch    ( IllegalArgumentException   expected )     {", "assertThat ( expected . getMessage (  )  ,    containsString (  \" one   or   more   gce      settings   are   missing .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testIllegalSettingsMissingAllRequired"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . putList ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" us - central 1  - a \"  ,     \" us - central 1  - b \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "try    {", "buildDynamicNodes ( mock ,    nodeSettings )  ;", "fail (  \" We   expect   an   IllegalArgumentException   for   incomplete   settings \"  )  ;", "}    catch    ( IllegalArgumentException   expected )     {", "assertThat ( expected . getMessage (  )  ,    containsString (  \" one   or   more   gce      settings   are   missing .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testIllegalSettingsMissingProject"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "try    {", "buildDynamicNodes ( mock ,    nodeSettings )  ;", "fail (  \" We   expect   an   IllegalArgumentException   for   incomplete   settings \"  )  ;", "}    catch    ( IllegalArgumentException   expected )     {", "assertThat ( expected . getMessage (  )  ,    containsString (  \" one   or   more   gce      settings   are   missing .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testIllegalSettingsMissingZone"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . putList ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" us - central 1  - a \"  ,     \" europe - west 1  - b \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( Nodes ,    hasSize (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleZonesAndTwoNodesInDifferentZones"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . putList ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" us - central 1  - a \"  ,     \" europe - west 1  - b \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( Nodes ,    hasSize (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleZonesAndTwoNodesInSameZone"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . putList ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  ,     \" us - central 1  - a \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( Nodes ,    hasSize (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testNoRegionReturnsEmptyList"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . put ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( Nodes ,    hasSize (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesWithDifferentTagsAndNoTagSet"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . put ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  )  . putList ( GceUnicastHostsProvider . TAGS _ SETTING . getKey (  )  ,     \" elasticsearch \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    discoveryNodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( discoveryNodes ,    hasSize (  1  )  )  ;", "assertThat ( discoveryNodes . get (  0  )  . getId (  )  ,    is (  \"  # cloud - test 2  -  0  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesWithDifferentTagsAndOneTagSet"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . put ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  )  . putList ( GceUnicastHostsProvider . TAGS _ SETTING . getKey (  )  ,     \" elasticsearch \"  ,     \" dev \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    discoveryNodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( discoveryNodes ,    hasSize (  1  )  )  ;", "assertThat ( discoveryNodes . get (  0  )  . getId (  )  ,    is (  \"  # cloud - test 2  -  0  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesWithDifferentTagsAndTwoTagSet"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . put ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( Nodes ,    hasSize (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesWithSameTagsAndNoTagSet"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . put ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  )  . putList ( GceUnicastHostsProvider . TAGS _ SETTING . getKey (  )  ,     \" elasticsearch \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    discoveryNodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( discoveryNodes ,    hasSize (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesWithSameTagsAndOneTagSet"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . put ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" europe - west 1  - b \"  )  . putList ( GceUnicastHostsProvider . TAGS _ SETTING . getKey (  )  ,     \" elasticsearch \"  ,     \" dev \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    discoveryNodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( discoveryNodes ,    hasSize (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesWithSameTagsAndTwoTagsSet"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put ( GceInstancesServiceImpl . PROJECT _ SETTING . getKey (  )  ,    projectName )  . putList ( GceInstancesServiceImpl . ZONE _ SETTING . getKey (  )  ,     \" us - central 1  - a \"  ,     \" us - central 1  - b \"  )  . build (  )  ;", "mock    =    new   GceInstancesServiceMock ( nodeSettings )  ;", "List < DiscoveryNode >    Nodes    =    buildDynamicNodes ( mock ,    nodeSettings )  ;", "assertThat ( Nodes ,    hasSize (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testZeroNode43"], "fileName": "org.elasticsearch.discovery.gce.GceDiscoveryTests"}, {"methodBody": ["METHOD_START", "{", "return   new   MockHttpTransport (  )     {", "@ Override", "public   LowLevelHttpRequest   buildRequest ( String   method ,    final   String   url )    throws   IOException    {", "return   new   MockLowLevelHttpRequest (  )     {", "@ Override", "public   LowLevelHttpResponse   execute (  )    throws   IOException    {", "MockLowLevelHttpResponse   response    =    new   MockLowLevelHttpResponse (  )  ;", "response . setStatusCode (  2  0  0  )  ;", "response . setContentType ( MEDIA _ TYPE )  ;", "if    ( url . startsWith (  . GCE _ METADATA _ URL )  )     {", ". logger . info (  \"  -  -  >    Simulate   GCE   Auth / Metadata   response   for    [  {  }  ]  \"  ,    url )  ;", "response . setContent (  . readGoogleInternalJsonResponse ( url )  )  ;", "} else    {", ". logger . info (  \"  -  -  >    Simulate   GCE   API   response   for    [  {  }  ]  \"  ,    url )  ;", "response . setContent (  . readGoogleApiJsonResponse ( url )  )  ;", "}", "return   response ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["configureMock"], "fileName": "org.elasticsearch.discovery.gce.GceMockUtils"}, {"methodBody": ["METHOD_START", "{", "return   GceMockUtils . readJsonResponse ( url ,     \" https :  /  / www . googleapis . com /  \"  )  ;", "}", "METHOD_END"], "methodName": ["readGoogleApiJsonResponse"], "fileName": "org.elasticsearch.discovery.gce.GceMockUtils"}, {"methodBody": ["METHOD_START", "{", "return   GceMockUtils . readJsonResponse ( url ,     \" http :  /  / metadata . google . internal /  \"  )  ;", "}", "METHOD_END"], "methodName": ["readGoogleInternalJsonResponse"], "fileName": "org.elasticsearch.discovery.gce.GceMockUtils"}, {"methodBody": ["METHOD_START", "{", "String   mockFileName    =    Strings . replace ( url ,    urlRoot ,     \"  \"  )  ;", "URL   resource    =    GceMockUtils . class . getResource ( mockFileName )  ;", "if    ( resource    =  =    null )     {", "throw   new   IOException (  (  (  \" can ' t   read    [  \"     +    url )     +     \"  ]    in   src / test / resources / org /  / discovery / gce \"  )  )  ;", "}", "try    ( InputStream   is    =    FileSystemUtils . openFileURLStream ( resource )  )     {", "final   StringBuilder   sb    =    new   StringBuilder (  )  ;", "Streams . readAllLines ( is ,    sb :  : append )  ;", "return   sb . toString (  )  ;", "}", "}", "METHOD_END"], "methodName": ["readJsonResponse"], "fileName": "org.elasticsearch.discovery.gce.GceMockUtils"}, {"methodBody": ["METHOD_START", "{", "resolveGce (  \"  _ local _  \"  ,    new   NetworkService ( Collections . emptyList (  )  )  . resolveBindHostAddresses ( new   String [  ]  {    NetworkService . DEFAULT _ NETWORK _ HOST    }  )  )  ;", "}", "METHOD_END"], "methodName": ["networkHostCoreLocal"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "resolveGce ( gceNetworkSetting ,     ( expected    =  =    null    ?    null    :    new   InetAddress [  ]  {    expected    }  )  )  ;", "}", "METHOD_END"], "methodName": ["resolveGce"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "Settings   nodeSettings    =    Settings . builder (  )  . put (  \" network . host \"  ,    gceNetworkSetting )  . build (  )  ;", "GceMetadataServiceMock   mock    =    new   GceMetadataServiceMock ( nodeSettings )  ;", "NetworkService   networkService    =    new   NetworkService ( Collections . singletonList ( new   GceNameResolver ( nodeSettings ,    mock )  )  )  ;", "try    {", "InetAddress [  ]    addresses    =    networkService . resolveBindHostAddresses ( GLOBAL _ NETWORK _ BINDHOST _ SETTING . get ( nodeSettings )  . toArray ( EMPTY _ ARRAY )  )  ;", "if    ( expected    =  =    null )     {", "fail (  \" We   should   get   a   IllegalArgumentException   when   setting   network . host :     _ gce : doesnotexist _  \"  )  ;", "}", "assertThat ( addresses ,    arrayContaining ( expected )  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "if    ( expected    !  =    null )     {", "throw   e ;", "}", "assertThat ( e . getMessage (  )  ,    containsString (  \" is   not   one   of   the   supported   GCE   network . host   setting \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["resolveGce"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "resolveGce (  \"  _ gce _  \"  ,    InetAddress . getByName (  \"  1  0  .  2  4  0  .  0  .  2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNetworkHostGceDefault"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "resolveGce (  \"  _ gce : hostname _  \"  ,    InetAddress . getByName (  \" localhost \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNetworkHostPrivateDns"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "resolveGce (  \"  _ gce : privateIp _  \"  ,    InetAddress . getByName (  \"  1  0  .  2  4  0  .  0  .  2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNetworkHostPrivateIp"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "resolveGce (  \"  _ gce : privateIp :  0  _  \"  ,    InetAddress . getByName (  \"  1  0  .  2  4  0  .  0  .  2  \"  )  )  ;", "resolveGce (  \"  _ gce : privateIp :  1  _  \"  ,    InetAddress . getByName (  \"  1  0  .  1  5  0  .  0  .  1  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNetworkHostPrivateIpInterface"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "resolveGce (  \"  _ gce : doesnotexist _  \"  ,     (  ( InetAddress )     ( null )  )  )  ;", "}", "METHOD_END"], "methodName": ["testNetworkHostWrongSetting"], "fileName": "org.elasticsearch.discovery.gce.GceNetworkTests"}, {"methodBody": ["METHOD_START", "{", "return   Access . doPrivileged ( MockGoogleCredential . Builder :  : new )  ;", "}", "METHOD_END"], "methodName": ["newMockCredentialBuilder"], "fileName": "org.elasticsearch.discovery.gce.RetryHttpInitializerWrapper"}, {"methodBody": ["METHOD_START", "{", "RetryHttpInitializerWrapperTests . FailThenSuccessBackoffTransport   fakeTransport    =    new   RetryHttpInitializerWrapperTests . FailThenSuccessBackoffTransport ( HttpStatusCodes . STATUS _ CODE _ SERVER _ ERROR ,     1  ,    true )  ;", "MockGoogleCredential   credential    =    RetryHttpInitializerWrapper . newMockCredentialBuilder (  )  . build (  )  ;", "MockSleeper   mockSleeper    =    new   MockSleeper (  )  ;", "RetryHttpInitializerWrapper   retryHttpInitializerWrapper    =    new   RetryHttpInitializerWrapper ( credential ,    mockSleeper ,    TimeValue . timeValueSeconds (  3  0 L )  )  ;", "Compute   client    =    new   Compute . Builder ( fakeTransport ,    new   JacksonFactory (  )  ,    null )  . setHttpRequestInitializer ( retryHttpInitializerWrapper )  . setApplicationName (  \" test \"  )  . build (  )  ;", "HttpRequest   request    =    client . getRequestFactory (  )  . buildRequest (  \" Get \"  ,    new   GenericUrl (  \" http :  /  / elasticsearch . com \"  )  ,    null )  ;", "HttpResponse   response    =    request . execute (  )  ;", "assertThat ( mockSleeper . getCount (  )  ,    equalTo (  1  )  )  ;", "assertThat ( response . getStatusCode (  )  ,    equalTo (  2  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testIOExceptionRetry"], "fileName": "org.elasticsearch.discovery.gce.RetryHttpInitializerWrapperTests"}, {"methodBody": ["METHOD_START", "{", "TimeValue   maxWaitTime    =    TimeValue . timeValueMillis (  1  0  )  ;", "int   maxRetryTimes    =     5  0  ;", ". FailThenSuccessBackoffTransport   fakeTransport    =    new    . FailThenSuccessBackoffTransport ( HttpStatusCodes . STATUS _ CODE _ SERVER _ ERROR ,    maxRetryTimes )  ;", "JsonFactory   jsonFactory    =    new   JacksonFactory (  )  ;", "MockGoogleCredential   credential    =    RetryHttpInitializerWrapper . newMockCredentialBuilder (  )  . build (  )  ;", "MockSleeper   oneTimeSleeper    =    new   MockSleeper (  )     {", "@ Override", "public   void   sleep ( long   millis )    throws   InterruptedException    {", "Thread . sleep ( maxWaitTime . getMillis (  )  )  ;", "super . sleep (  0  )  ;", "}", "}  ;", "RetryHttpInitializerWrapper   retryHttpInitializerWrapper    =    new   RetryHttpInitializerWrapper ( credential ,    oneTimeSleeper ,    maxWaitTime )  ;", "Compute   client    =    new   Compute . Builder ( fakeTransport ,    jsonFactory ,    null )  . setHttpRequestInitializer ( retryHttpInitializerWrapper )  . setApplicationName (  \" test \"  )  . build (  )  ;", "HttpRequest   request 1     =    client . getRequestFactory (  )  . buildRequest (  \" Get \"  ,    new   GenericUrl (  \" http :  /  / elasticsearch . com \"  )  ,    null )  ;", "try    {", "request 1  . execute (  )  ;", "fail (  \" Request   should   fail   if   wait   too   long \"  )  ;", "}    catch    ( HttpResponseException   e )     {", "assertThat ( e . getStatusCode (  )  ,    equalTo ( STATUS _ CODE _ SERVER _ ERROR )  )  ;", "assertThat ( oneTimeSleeper . getCount (  )  ,    lessThan ( maxRetryTimes )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRetryWaitTooLong"], "fileName": "org.elasticsearch.discovery.gce.RetryHttpInitializerWrapperTests"}, {"methodBody": ["METHOD_START", "{", "RetryHttpInitializerWrapperTests . FailThenSuccessBackoffTransport   fakeTransport    =    new   RetryHttpInitializerWrapperTests . FailThenSuccessBackoffTransport ( HttpStatusCodes . STATUS _ CODE _ SERVER _ ERROR ,     3  )  ;", "MockGoogleCredential   credential    =    RetryHttpInitializerWrapper . newMockCredentialBuilder (  )  . build (  )  ;", "MockSleeper   mockSleeper    =    new   MockSleeper (  )  ;", "RetryHttpInitializerWrapper   retryHttpInitializerWrapper    =    new   RetryHttpInitializerWrapper ( credential ,    mockSleeper ,    TimeValue . timeValueSeconds (  5  )  )  ;", "Compute   client    =    new   Compute . Builder ( fakeTransport ,    new   JacksonFactory (  )  ,    null )  . setHttpRequestInitializer ( retryHttpInitializerWrapper )  . setApplicationName (  \" test \"  )  . build (  )  ;", "HttpRequest   request    =    client . getRequestFactory (  )  . buildRequest (  \" Get \"  ,    new   GenericUrl (  \" http :  /  / elasticsearch . com \"  )  ,    null )  ;", "HttpResponse   response    =    request . execute (  )  ;", "assertThat ( mockSleeper . getCount (  )  ,    equalTo (  3  )  )  ;", "assertThat ( response . getStatusCode (  )  ,    equalTo (  2  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleRetry"], "fileName": "org.elasticsearch.discovery.gce.RetryHttpInitializerWrapperTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return   bool ;", "}", "METHOD_END"], "methodName": ["getBool"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsConfig"}, {"methodBody": ["METHOD_START", "{", "return   filtered ;", "}", "METHOD_END"], "methodName": ["getFiltered"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsConfig"}, {"methodBody": ["METHOD_START", "{", "return   list ;", "}", "METHOD_END"], "methodName": ["getList"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsConfig"}, {"methodBody": ["METHOD_START", "{", "return   simple ;", "}", "METHOD_END"], "methodName": ["getSimple"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsConfig"}, {"methodBody": ["METHOD_START", "{", "return   validated ;", "}", "METHOD_END"], "methodName": ["getValidated"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsConfig"}, {"methodBody": ["METHOD_START", "{", "final   String   expected    =    randomAlphaOfLengthBetween (  1  ,     5  )  ;", "final   String   actual    =     . VALIDATED _ SETTING . get ( Settings . builder (  )  . put (  . VALIDATED _ SETTING . getKey (  )  ,    expected )  . build (  )  )  ;", "assertEquals ( expected ,    actual )  ;", "final   IllegalArgumentException   exception    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    VALIDATED _ SETTING . get ( Settings . builder (  )  . put (  \" custom . validated \"  ,     \" it ' s   forbidden \"  )  . build (  )  )  )  ;", "assertEquals (  \" Setting   must   not   contain    [ forbidden ]  \"  ,    exception . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testValidatedSetting"], "fileName": "org.elasticsearch.example.customsettings.ExampleCustomSettingsConfigTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.example.expertscript.ExpertScriptClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return   this . privateMember ;", "}", "METHOD_END"], "methodName": ["getPrivateMemberAccessor"], "fileName": "org.elasticsearch.example.painlesswhitelist.ExampleWhitelistedClass"}, {"methodBody": ["METHOD_START", "{", "this . privateMember    =    privateMember ;", "}", "METHOD_END"], "methodName": ["setPrivateMemberAccessor"], "fileName": "org.elasticsearch.example.painlesswhitelist.ExampleWhitelistedClass"}, {"methodBody": ["METHOD_START", "{", "return   Integer . parseInt ( x )  ;", "}", "METHOD_END"], "methodName": ["toInt"], "fileName": "org.elasticsearch.example.painlesswhitelist.ExampleWhitelistedClass"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.example.painlesswhitelist.PainlessWhitelistClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return   factor ;", "}", "METHOD_END"], "methodName": ["factor"], "fileName": "org.elasticsearch.example.rescore.ExampleRescoreBuilder"}, {"methodBody": ["METHOD_START", "{", "return   factorField ;", "}", "METHOD_END"], "methodName": ["factorField"], "fileName": "org.elasticsearch.example.rescore.ExampleRescoreBuilder"}, {"methodBody": ["METHOD_START", "{", "return   ExampleRescoreBuilder . PARSER . apply ( parser ,    null )  ;", "}", "METHOD_END"], "methodName": ["fromXContent"], "fileName": "org.elasticsearch.example.rescore.ExampleRescoreBuilder"}, {"methodBody": ["METHOD_START", "{", "float   factor    =     (  ( float )     ( randomDoubleBetween (  1  .  0  ,    Float . MAX _ VALUE ,    false )  )  )  ;", "String   fieldFactor    =    null ;", "builder    =    new    ( factor ,    fieldFactor )  . windowSize (  2  )  ;", "RescoreContext   context    =    builder . buildContext ( null )  ;", "TopDocs   docs    =    new   TopDocs (  1  0  ,    new   ScoreDoc [  3  ]  ,     0  )  ;", "docs . scoreDocs [  0  ]     =    new   ScoreDoc (  0  ,     1  .  0 F )  ;", "docs . scoreDocs [  1  ]     =    new   ScoreDoc (  1  ,     1  .  0 F )  ;", "docs . scoreDocs [  2  ]     =    new   ScoreDoc (  2  ,     1  .  0 F )  ;", "context . rescorer (  )  . rescore ( docs ,    null ,    context )  ;", "assertEquals ( factor ,    docs . scoreDocs [  0  ]  . score ,     0  .  0 F )  ;", "assertEquals ( factor ,    docs . scoreDocs [  1  ]  . score ,     0  .  0 F )  ;", "assertEquals (  1  .  0 F ,    docs . scoreDocs [  2  ]  . score ,     0  .  0 F )  ;", "}", "METHOD_END"], "methodName": ["testRescore"], "fileName": "org.elasticsearch.example.rescore.ExampleRescoreBuilderTests"}, {"methodBody": ["METHOD_START", "{", "ExampleRescoreBuilder   builder    =    createTestInstance (  )  ;", "assertSame ( builder ,    builder . rewrite ( null )  )  ;", "}", "METHOD_END"], "methodName": ["testRewrite"], "fileName": "org.elasticsearch.example.rescore.ExampleRescoreBuilderTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.example.rescore.ExampleRescoreClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return    \"  /  _ cat / example \\ n \"  ;", "}", "METHOD_END"], "methodName": ["documentation"], "fileName": "org.elasticsearch.example.resthandler.ExampleCatAction"}, {"methodBody": ["METHOD_START", "{", "final   String   stringAddress    =    Objects . requireNonNull ( System . getProperty (  \" external . address \"  )  )  ;", "final   URL   url    =    new   URL (  (  \" http :  /  /  \"     +    stringAddress )  )  ;", "final   InetAddress   address    =    InetAddress . getByName ( url . getHost (  )  )  ;", "try    ( Socket   socket    =    new   MockSocket ( address ,    url . getPort (  )  )  ; BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( socket . getInputStream (  )  ,    StandardCharsets . UTF _  8  )  )  )     {", "assertEquals (  \" TEST \"  ,    reader . readLine (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testExample"], "fileName": "org.elasticsearch.example.resthandler.ExampleFixtureIT"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.example.resthandler.ExampleRestHandlerClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put ( SETTING _ VERSION _ CREATED ,    CURRENT )  . put ( SETTING _ NUMBER _ OF _ REPLICAS ,     0  )  . put ( SETTING _ NUMBER _ OF _ SHARDS ,     1  )  . put ( SETTING _ INDEX _ UUID ,    UUIDs . randomBase 6  4 UUID (  )  )  . put ( PATH _ HOME _ SETTING . getKey (  )  ,    createTempDir (  )  . toString (  )  )  . build (  )  ;", "Environment   environment    =    TestEnvironment . newEnvironment ( settings )  ;", "IndexMetaData   metaData    =    IndexMetaData . builder ( IndexMetaData . INDEX _ UUID _ NA _ VALUE )  . settings ( settings )  . build (  )  ;", "IndexSettings   indexSettings    =    new   IndexSettings ( metaData ,    Settings . EMPTY )  ;", "testThreadSafety ( new   PolishStemTokenFilter ( indexSettings ,    environment ,     \" stempelpolishstem \"  ,    settings )  )  ;", "}", "METHOD_END"], "methodName": ["testThreadSafety"], "fileName": "org.elasticsearch.index.analysis.AnalysisPolishFactoryTests"}, {"methodBody": ["METHOD_START", "{", "final   Analyzer   analyzer    =    new   Analyzer (  )     {", "@ Override", "protected   TokenStreamComponents   createComponents ( String   fieldName )     {", "Tokenizer   tokenizer    =    new   MockTokenizer (  )  ;", "return   new   TokenStreamComponents ( tokenizer ,    f . create ( tokenizer )  )  ;", "}", "}  ;", "BaseTokenStreamTestCase . checkRandomData ( random (  )  ,    analyzer ,     1  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testThreadSafety"], "fileName": "org.elasticsearch.index.analysis.AnalysisPolishFactoryTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.analysis.IcuClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "String   unicodeSetFilter    =    settings . get (  \" unicodeSetFilter \"  )  ;", "if    ( unicodeSetFilter    !  =    null )     {", "UnicodeSet   unicodeSet    =    new   UnicodeSet ( unicodeSetFilter )  ;", "unicodeSet . freeze (  )  ;", "return   new   com . ibm . icu . text . Filtered 2  ( normalizer ,    unicodeSet )  ;", "}", "return   normalizer ;", "}", "METHOD_END"], "methodName": ["wrapWithUnicodeSetFilter"], "fileName": "org.elasticsearch.index.analysis.IcuNormalizerTokenFilterFactory"}, {"methodBody": ["METHOD_START", "{", "Map < Integer ,    String >    tailored    =    new   HashMap <  >  (  )  ;", "try    {", "List < String >    ruleFiles    =    settings . getAsList (  . RULE _ FILES )  ;", "for    ( String   scriptAndResourcePath    :    ruleFiles )     {", "int   colonPos    =    scriptAndResourcePath . indexOf (  \"  :  \"  )  ;", "if    (  ( colonPos    =  =     (  -  1  )  )     |  |     ( colonPos    =  =     (  ( scriptAndResourcePath . length (  )  )     -     1  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  . RULE _ FILES )     +     \"    should   contain   comma - separated    \\  \" code : rulefile \\  \"    pairs \"  )  )  ;", "}", "String   scriptCode    =    scriptAndResourcePath . substring (  0  ,    colonPos )  . trim (  )  ;", "String   resourcePath    =    scriptAndResourcePath . substring (  ( colonPos    +     1  )  )  . trim (  )  ;", "tailored . put ( UCharacter . getPropertyValueEnum ( SCRIPT ,    scriptCode )  ,    resourcePath )  ;", "}", "if    ( tailored . isEmpty (  )  )     {", "return   null ;", "} else    {", "final   BreakIterator [  ]    breakers    =    new   BreakIterator [ UScript . CODE _ LIMIT ]  ;", "for    ( Map . Entry < Integer ,    String >    entry    :    tailored . entrySet (  )  )     {", "int   code    =    entry . getKey (  )  ;", "String   resourcePath    =    entry . getValue (  )  ;", "breakers [ code ]     =    parseRules ( resourcePath ,    env )  ;", "}", "ICUTokenizerConfig   config    =    new   DefaultICUTokenizerConfig ( true ,    true )     {", "@ Override", "public   BreakIterator   getBreakIterator ( int   script )     {", "if    (  ( breakers [ script ]  )     !  =    null )     {", "return    (  ( BreakIterator )     ( breakers [ script ]  . clone (  )  )  )  ;", "} else    {", "return   super . getBreakIterator ( script )  ;", "}", "}", "}  ;", "return   config ;", "}", "}    catch    ( Exception   e )     {", "throw   new   ElasticsearchException (  \" failed   to   load   ICU   rule   files \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getIcuConfig"], "fileName": "org.elasticsearch.index.analysis.IcuTokenizerFactory"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    env . configFile (  )  . resolve ( filename )  ;", "String   rules    =    Files . readAllLines ( path )  . stream (  )  . filter (  (    v )     -  >     ( v . startsWith (  \"  #  \"  )  )     =  =    false )  . collect ( Colles . joining (  \"  \\ n \"  )  )  ;", "return   new   RuleBasedBreakIterator ( rules . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["parseRules"], "fileName": "org.elasticsearch.index.analysis.IcuTokenizerFactory"}, {"methodBody": ["METHOD_START", "{", "InputStream   keywords    =    IcuTokenizerFactoryTests . class . getResourceAsStream (  \" KeywordTokenizer . rbbi \"  )  ;", "InputStream   latin    =    IcuTokenizerFactoryTests . class . getResourceAsStream (  \" Latin - dont - break - on - hyphens . rbbi \"  )  ;", "Path   home    =    createTempDir (  )  ;", "Path   config    =    home . resolve (  \" config \"  )  ;", "Files . createDirectory ( config )  ;", "Files . copy ( keywords ,    config . resolve (  \" KeywordTokenizer . rbbi \"  )  )  ;", "Files . copy ( latin ,    config . resolve (  \" Latin - dont - break - on - hyphens . rbbi \"  )  )  ;", "String   json    =     \"  / org / elasticsearch / index / analysis / icu _ analysis . json \"  ;", "Settings   settings    =    Settings . builder (  )  . loadFromStream ( json ,    IcuTokenizerFactoryTests . class . getResourceAsStream ( json )  ,    false )  . put ( SETTING _ VERSION _ CREATED ,    CURRENT )  . build (  )  ;", "Settings   nodeSettings    =    Settings . builder (  )  . put ( PATH _ HOME _ SETTING . getKey (  )  ,    home )  . build (  )  ;", "return   IcuTokenizerFactoryTests . createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    nodeSettings ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "}", "METHOD_END"], "methodName": ["createTestAnalysis"], "fileName": "org.elasticsearch.index.analysis.IcuTokenizerFactoryTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    IcuTokenizerFactoryTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" user _ rule _ tokenizer \"  )  ;", "ICUTokenizer   tokenizer    =     (  ( ICUTokenizer )     ( tokenizerFactory . create (  )  )  )  ;", "Reader   reader    =    new   StringReader (  \" One - two   punch .       Brang -  ,    not   brung - it .       This   one -  - not   that   one -  - is   the   right   one ,     - ish .  \"  )  ;", "tokenizer . setReader ( reader )  ;", "BaseTokenStreamTestCase . assertTokenStreamContents ( tokenizer ,    new   String [  ]  {     \" One - two \"  ,     \" punch \"  ,     \" Brang \"  ,     \" not \"  ,     \" brung - it \"  ,     \" This \"  ,     \" one \"  ,     \" not \"  ,     \" that \"  ,     \" one \"  ,     \" is \"  ,     \" the \"  ,     \" right \"  ,     \" one \"  ,     \" ish \"     }  )  ;", "}", "METHOD_END"], "methodName": ["testIcuCustomizeRuleFile"], "fileName": "org.elasticsearch.index.analysis.IcuTokenizerFactoryTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    IcuTokenizerFactoryTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" multi _ rule _ tokenizer \"  )  ;", "ICUTokenizer   tokenizer    =     (  ( ICUTokenizer )     ( tokenizerFactory . create (  )  )  )  ;", "StringReader   reader    =    new   StringReader (  \" Some   English .        \u00a7\u00af  \u00a7\u00d6  \u00a7\u00de  \u00a7\u00df  \u00a7\u00e0  \u00a7\u00d4  \u00a7\u00e0     \u00a7\u00e2  \u00a7\u00e5  \u00a7\u00e3  \u00a7\u00e3  \u00a7\u00dc  \u00a7\u00da  \u00a7\u00db  .        ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?     ?     ?  ?  ?  ?     ?       More   English .  \"  )  ;", "tokenizer . setReader ( reader )  ;", "BaseTokenStreamTestCase . assertTokenStreamContents ( tokenizer ,    new   String [  ]  {     \" Some \"  ,     \" English \"  ,     \"  \u00a7\u00af  \u00a7\u00d6  \u00a7\u00de  \u00a7\u00df  \u00a7\u00e0  \u00a7\u00d4  \u00a7\u00e0     \u00a7\u00e2  \u00a7\u00e5  \u00a7\u00e3  \u00a7\u00e3  \u00a7\u00dc  \u00a7\u00da  \u00a7\u00db  .        \"  ,     \"  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?     ?     ?  ?  ?  ?     ?        \"  ,     \" More \"  ,     \" English \"     }  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleIcuCustomizeRuleFiles"], "fileName": "org.elasticsearch.index.analysis.IcuTokenizerFactoryTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    IcuTokenizerFactoryTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" icu _ tokenizer \"  )  ;", "ICUTokenizer   tokenizer    =     (  ( ICUTokenizer )     ( tokenizerFactory . create (  )  )  )  ;", "Reader   reader    =    new   StringReader (  \"  \u00cf\u00f2  \u00c8\u00d5  \u00bf\u00fb  ,    one - two \"  )  ;", "tokenizer . setReader ( reader )  ;", "BaseTokenStreamTestCase . assertTokenStreamContents ( tokenizer ,    new   String [  ]  {     \"  \u00cf\u00f2  \u00c8\u00d5  \u00bf\u00fb  \"  ,     \" one \"  ,     \" two \"     }  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleIcuTokenizer"], "fileName": "org.elasticsearch.index.analysis.IcuTokenizerFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assert   outputLength    =  =     ( IndexableBinaryStringTools . getDecodedLength ( inputArray ,    inputOffset ,    inputLength )  )  ;", "final   int   numInputChars    =    inputLength    -     1  ;", "final   int   numOutputBytes    =    outputLength ;", "if    ( numOutputBytes    >     0  )     {", "int   caseNum    =     0  ;", "int   outputByteNum    =    outputOffset ;", "int   inputCharNum    =    inputOffset ;", "short   inputChar ;", "IndexableBinaryStringTools . CodingCase   codingCase ;", "for    (  ;    inputCharNum    <     ( numInputChars    -     1  )  ;     +  + inputCharNum )     {", "codingCase    =    IndexableBinaryStringTools . CODING _ CASES [ caseNum ]  ;", "inputChar    =     (  ( short )     ( inputArray [ inputCharNum ]  )  )  ;", "if    (  2     =  =     ( codingCase . numBytes )  )     {", "if    (  0     =  =    caseNum )     {", "outputArray [ outputByteNum ]     =     (  ( byte )     ( inputChar    >  >  >     ( codingCase . initialShift )  )  )  ;", "} else    {", "outputArray [ outputByteNum ]     +  =     (  ( byte )     ( inputChar    >  >  >     ( codingCase . initialShift )  )  )  ;", "}", "outputArray [  ( outputByteNum    +     1  )  ]     =     (  ( byte )     (  ( inputChar    &     ( codingCase . finalMask )  )     <  <     ( codingCase . finalShift )  )  )  ;", "} else    {", "outputArray [ outputByteNum ]     +  =     (  ( byte )     ( inputChar    >  >  >     ( codingCase . initialShift )  )  )  ;", "outputArray [  ( outputByteNum    +     1  )  ]     =     (  ( byte )     (  ( inputChar    &     ( codingCase . middleMask )  )     >  >  >     ( codingCase . middleShift )  )  )  ;", "outputArray [  ( outputByteNum    +     2  )  ]     =     (  ( byte )     (  ( inputChar    &     ( codingCase . finalMask )  )     <  <     ( codingCase . finalShift )  )  )  ;", "}", "outputByteNum    +  =    codingCase . advanceBytes ;", "if    (  (  +  + caseNum )     =  =     ( IndexableBinaryStringTools . CODING _ CASES . length )  )     {", "caseNum    =     0  ;", "}", "}", "inputChar    =     (  ( short )     ( inputArray [ inputCharNum ]  )  )  ;", "codingCase    =    IndexableBinaryStringTools . CODING _ CASES [ caseNum ]  ;", "if    (  0     =  =    caseNum )     {", "outputArray [ outputByteNum ]     =     0  ;", "}", "outputArray [ outputByteNum ]     +  =     (  ( byte )     ( inputChar    >  >  >     ( codingCase . initialShift )  )  )  ;", "final   int   bytesLeft    =    numOutputBytes    -    outputByteNum ;", "if    ( bytesLeft    >     1  )     {", "if    (  2     =  =     ( codingCase . numBytes )  )     {", "outputArray [  ( outputByteNum    +     1  )  ]     =     (  ( byte )     (  ( inputChar    &     ( codingCase . finalMask )  )     >  >  >     ( codingCase . finalShift )  )  )  ;", "} else    {", "outputArray [  ( outputByteNum    +     1  )  ]     =     (  ( byte )     (  ( inputChar    &     ( codingCase . middleMask )  )     >  >  >     ( codingCase . middleShift )  )  )  ;", "if    ( bytesLeft    >     2  )     {", "outputArray [  ( outputByteNum    +     2  )  ]     =     (  ( byte )     (  ( inputChar    &     ( codingCase . finalMask )  )     <  <     ( codingCase . finalShift )  )  )  ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["decode"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringTools"}, {"methodBody": ["METHOD_START", "{", "assert   outputLength    =  =     ( IndexableBinaryStringTools . getEncodedLength ( inputArray ,    inputOffset ,    inputLength )  )  ;", "if    ( inputLength    >     0  )     {", "int   inputByteNum    =    inputOffset ;", "int   caseNum    =     0  ;", "int   outputCharNum    =    outputOffset ;", "IndexableBinaryStringTools . CodingCase   codingCase ;", "for    (  ;     ( inputByteNum    +     ( IndexableBinaryStringTools . CODING _ CASES [ caseNum ]  . numBytes )  )     <  =    inputLength ;     +  + outputCharNum )     {", "codingCase    =    IndexableBinaryStringTools . CODING _ CASES [ caseNum ]  ;", "if    (  2     =  =     ( codingCase . numBytes )  )     {", "outputArray [ outputCharNum ]     =     (  ( char )     (  (  (  (  ( inputArray [ inputByteNum ]  )     &     2  5  5  )     <  <     ( codingCase . initialShift )  )     +     (  (  (  ( inputArray [  ( inputByteNum    +     1  )  ]  )     &     2  5  5  )     >  >  >     ( codingCase . finalShift )  )     &     ( codingCase . finalMask )  )  )     &     (  ( short )     (  3  2  7  6  7  )  )  )  )  ;", "} else    {", "outputArray [ outputCharNum ]     =     (  ( char )     (  (  (  (  (  ( inputArray [ inputByteNum ]  )     &     2  5  5  )     <  <     ( codingCase . initialShift )  )     +     (  (  ( inputArray [  ( inputByteNum    +     1  )  ]  )     &     2  5  5  )     <  <     ( codingCase . middleShift )  )  )     +     (  (  (  ( inputArray [  ( inputByteNum    +     2  )  ]  )     &     2  5  5  )     >  >  >     ( codingCase . finalShift )  )     &     ( codingCase . finalMask )  )  )     &     (  ( short )     (  3  2  7  6  7  )  )  )  )  ;", "}", "inputByteNum    +  =    codingCase . advanceBytes ;", "if    (  (  +  + caseNum )     =  =     ( IndexableBinaryStringTools . CODING _ CASES . length )  )     {", "caseNum    =     0  ;", "}", "}", "codingCase    =    IndexableBinaryStringTools . CODING _ CASES [ caseNum ]  ;", "if    (  ( inputByteNum    +     1  )     <    inputLength )     {", "outputArray [  ( outputCharNum +  +  )  ]     =     (  ( char )     (  (  (  (  ( inputArray [ inputByteNum ]  )     &     2  5  5  )     <  <     ( codingCase . initialShift )  )     +     (  (  ( inputArray [  ( inputByteNum    +     1  )  ]  )     &     2  5  5  )     <  <     ( codingCase . middleShift )  )  )     &     (  ( short )     (  3  2  7  6  7  )  )  )  )  ;", "outputArray [  ( outputCharNum +  +  )  ]     =     (  ( char )     (  1  )  )  ;", "} else", "if    ( inputByteNum    <    inputLength )     {", "outputArray [  ( outputCharNum +  +  )  ]     =     (  ( char )     (  (  (  ( inputArray [ inputByteNum ]  )     &     2  5  5  )     <  <     ( codingCase . initialShift )  )     &     (  ( short )     (  3  2  7  6  7  )  )  )  )  ;", "outputArray [  ( outputCharNum +  +  )  ]     =     ( caseNum    =  =     0  )     ?     (  ( char )     (  1  )  )     :     (  ( char )     (  0  )  )  ;", "} else    {", "outputArray [  ( outputCharNum +  +  )  ]     =     (  ( char )     (  1  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["encode"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringTools"}, {"methodBody": ["METHOD_START", "{", "final   int   numChars    =    length    -     1  ;", "if    ( numChars    <  =     0  )     {", "return    0  ;", "} else    {", "final   long   numFullBytesInFinalChar    =    encoded [  (  ( offset    +    length )     -     1  )  ]  ;", "final   long   numEncodedChars    =    numChars    -     1  ;", "return    (  ( int )     (  (  (  ( numEncodedChars    *     1  5 L )     +     7 L )     /     8 L )     +    numFullBytesInFinalChar )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getDecodedLength"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringTools"}, {"methodBody": ["METHOD_START", "{", "return    (  ( int )     (  (  (  8 L    *    inputLength )     +     1  4 L )     /     1  5 L )  )     +     1  ;", "}", "METHOD_END"], "methodName": ["getEncodedLength"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringTools"}, {"methodBody": ["METHOD_START", "{", "IndexableBinaryStringToolsTests . NUM _ RANDOM _ TESTS    =    atLeast (  2  0  0  )  ;", "IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH    =    atLeast (  3  0  0  )  ;", "}", "METHOD_END"], "methodName": ["beforeClass"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   buf    =    new   StringBuilder (  )  ;", "for    ( int   byteNum    =     0  ;    byteNum    <    numBytes ;     +  + byteNum )     {", "String   hex    =    Integer . toHexString (  (  ( binary [ byteNum ]  )     &     2  5  5  )  )  ;", "if    (  ( hex . length (  )  )     =  =     1  )     {", "buf . append (  '  0  '  )  ;", "}", "buf . append ( hex . toUpperCase ( Locale . ROOT )  )  ;", "if    ( byteNum    <     ( numBytes    -     1  )  )     {", "buf . append (  '     '  )  ;", "}", "}", "return   buf . toString (  )  ;", "}", "METHOD_END"], "methodName": ["binaryDump"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   buf    =    new   StringBuilder (  )  ;", "for    ( int   charNum    =     0  ;    charNum    <    numBytes ;     +  + charNum )     {", "String   hex    =    Integer . toHexString ( charArray [ charNum ]  )  ;", "for    ( int   digit    =     0  ;    digit    <     (  4     -     ( hex . length (  )  )  )  ;     +  + digit )     {", "buf . append (  '  0  '  )  ;", "}", "buf . append ( hex . toUpperCase ( Locale . ROOT )  )  ;", "if    ( charNum    <     ( numBytes    -     1  )  )     {", "buf . append (  '     '  )  ;", "}", "}", "return   buf . toString (  )  ;", "}", "METHOD_END"], "methodName": ["charArrayDump"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    binary    =    new   byte [  ]  {     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0     }  ;", "int   encodedLen    =    IndexableBinaryStringTools . getEncodedLength ( binary ,     0  ,    binary . length )  ;", "char [  ]    encoded    =    new   char [ encodedLen ]  ;", "IndexableBinaryStringTools . encode ( binary ,     0  ,    binary . length ,    encoded ,     0  ,    encoded . length )  ;", "int   decodedLen    =    IndexableBinaryStringTools . getDecodedLength ( encoded ,     0  ,    encoded . length )  ;", "byte [  ]    decoded    =    new   byte [ decodedLen ]  ;", "IndexableBinaryStringTools . decode ( encoded ,     0  ,    encoded . length ,    decoded ,     0  ,    decoded . length )  ;", "assertEquals (  (  (  (  (  (  (  \" Round   trip   decode / decode   returned   different   results :  \"     +     (  . LINE _ SEPARATOR )  )     +     \"       original :     \"  )     +     ( binaryDump ( binary ,    binary . length )  )  )     +     (  . LINE _ SEPARATOR )  )     +     \" decodedBuf :     \"  )     +     ( binaryDump ( decoded ,    decoded . length )  )  )  ,    binaryDump ( binary ,    binary . length )  ,    binaryDump ( decoded ,    decoded . length )  )  ;", "}", "METHOD_END"], "methodName": ["testAllNullInput"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    binary    =    new   byte [  0  ]  ;", "int   encodedLen    =     . getEncodedLength ( binary ,     0  ,    binary . length )  ;", "char [  ]    encoded    =    new   char [ encodedLen ]  ;", ". encode ( binary ,     0  ,    binary . length ,    encoded ,     0  ,    encoded . length )  ;", "int   decodedLen    =     . getDecodedLength ( encoded ,     0  ,    encoded . length )  ;", "byte [  ]    decoded    =    new   byte [ decodedLen ]  ;", ". decode ( encoded ,     0  ,    encoded . length ,    decoded ,     0  ,    decoded . length )  ;", "assertEquals (  \" decoded   empty   input   was   not   empty \"  ,    decoded . length ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyInput"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    originalArray 1     =    new   byte [ IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH ]  ;", "char [  ]    originalString 1     =    new   char [ IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH ]  ;", "char [  ]    encoded 1     =    new   char [  ( IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH )     *     1  0  ]  ;", "byte [  ]    original 2     =    new   byte [ IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH ]  ;", "char [  ]    originalString 2     =    new   char [ IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH ]  ;", "char [  ]    encoded 2     =    new   char [  ( IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH )     *     1  0  ]  ;", "for    ( int   testNum    =     0  ;    testNum    <     ( IndexableBinaryStringToolsTests . NUM _ RANDOM _ TESTS )  ;     +  + testNum )     {", "int   numBytes 1     =     ( random (  )  . nextInt (  (  ( IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH )     -     1  )  )  )     +     1  ;", "for    ( int   byteNum    =     0  ;    byteNum    <    numBytes 1  ;     +  + byteNum )     {", "int   randomInt    =    random (  )  . nextInt (  2  5  6  )  ;", "originalArray 1  [ byteNum ]     =     (  ( byte )     ( randomInt )  )  ;", "originalString 1  [ byteNum ]     =     (  ( char )     ( randomInt )  )  ;", "}", "int   numBytes 2     =     ( random (  )  . nextInt (  (  ( IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH )     -     1  )  )  )     +     1  ;", "for    ( int   byteNum    =     0  ;    byteNum    <    numBytes 2  ;     +  + byteNum )     {", "int   randomInt    =    random (  )  . nextInt (  2  5  6  )  ;", "original 2  [ byteNum ]     =     (  ( byte )     ( randomInt )  )  ;", "originalString 2  [ byteNum ]     =     (  ( char )     ( randomInt )  )  ;", "}", "int   originalComparison    =    new   String ( originalString 1  ,     0  ,    numBytes 1  )  . compareTo ( new   String ( originalString 2  ,     0  ,    numBytes 2  )  )  ;", "originalComparison    =     ( originalComparison    <     0  )     ?     -  1     :    originalComparison    >     0     ?     1     :     0  ;", "int   encodedLen 1     =    IndexableBinaryStringTools . getEncodedLength ( originalArray 1  ,     0  ,    numBytes 1  )  ;", "if    ( encodedLen 1     >     ( encoded 1  . length )  )", "encoded 1     =    new   char [ oversize ( encodedLen 1  ,    Character . BYTES )  ]  ;", "IndexableBinaryStringTools . encode ( originalArray 1  ,     0  ,    numBytes 1  ,    encoded 1  ,     0  ,    encodedLen 1  )  ;", "int   encodedLen 2     =    IndexableBinaryStringTools . getEncodedLength ( original 2  ,     0  ,    numBytes 2  )  ;", "if    ( encodedLen 2     >     ( encoded 2  . length )  )", "encoded 2     =    new   char [ oversize ( encodedLen 2  ,    Character . BYTES )  ]  ;", "IndexableBinaryStringTools . encode ( original 2  ,     0  ,    numBytes 2  ,    encoded 2  ,     0  ,    encodedLen 2  )  ;", "int   encodedComparison    =    new   String ( encoded 1  ,     0  ,    encodedLen 1  )  . compareTo ( new   String ( encoded 2  ,     0  ,    encodedLen 2  )  )  ;", "encodedComparison    =     ( encodedComparison    <     0  )     ?     -  1     :    encodedComparison    >     0     ?     1     :     0  ;", "assertEquals (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" Test    #  \"     +     ( testNum    +     1  )  )     +     \"  :    Original   bytes   and   encoded   chars   compare   differently :  \"  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \"    binary    1  :     \"  )     +     ( binaryDump ( originalArray 1  ,    numBytes 1  )  )  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \"    binary    2  :     \"  )     +     ( binaryDump ( original 2  ,    numBytes 2  )  )  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \" encoded    1  :     \"  )     +     ( charArrayDump ( encoded 1  ,    encodedLen 1  )  )  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \" encoded    2  :     \"  )     +     ( charArrayDump ( encoded 2  ,    encodedLen 2  )  )  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )  ,    originalComparison ,    encodedComparison )  ;", "}", "}", "METHOD_END"], "methodName": ["testEncodedSortability"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    binary    =    new   byte [ IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH ]  ;", "char [  ]    encoded    =    new   char [  ( IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH )     *     1  0  ]  ;", "byte [  ]    decoded    =    new   byte [ IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH ]  ;", "for    ( int   testNum    =     0  ;    testNum    <     ( IndexableBinaryStringToolsTests . NUM _ RANDOM _ TESTS )  ;     +  + testNum )     {", "int   numBytes    =     ( random (  )  . nextInt (  (  ( IndexableBinaryStringToolsTests . MAX _ RANDOM _ BINARY _ LENGTH )     -     1  )  )  )     +     1  ;", "for    ( int   byteNum    =     0  ;    byteNum    <    numBytes ;     +  + byteNum )     {", "binary [ byteNum ]     =     (  ( byte )     ( random (  )  . nextInt (  2  5  6  )  )  )  ;", "}", "int   encodedLen    =    IndexableBinaryStringTools . getEncodedLength ( binary ,     0  ,    numBytes )  ;", "if    (  ( encoded . length )     <    encodedLen )", "encoded    =    new   char [ oversize ( encodedLen ,    Character . BYTES )  ]  ;", "IndexableBinaryStringTools . encode ( binary ,     0  ,    numBytes ,    encoded ,     0  ,    encodedLen )  ;", "int   decodedLen    =    IndexableBinaryStringTools . getDecodedLength ( encoded ,     0  ,    encodedLen )  ;", "IndexableBinaryStringTools . decode ( encoded ,     0  ,    encodedLen ,    decoded ,     0  ,    decodedLen )  ;", "assertEquals (  (  (  (  (  (  (  (  (  (  (  (  \" Test    #  \"     +     ( testNum    +     1  )  )     +     \"  :    Round   trip   decode / decode   returned   different   results :  \"  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \"       original :     \"  )     +     ( binaryDump ( binary ,    numBytes )  )  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \" encodedBuf :     \"  )     +     ( charArrayDump ( encoded ,    encodedLen )  )  )     +     ( IndexableBinaryStringToolsTests . LINE _ SEPARATOR )  )     +     \" decodedBuf :     \"  )     +     ( binaryDump ( decoded ,    decodedLen )  )  )  ,    binaryDump ( binary ,    numBytes )  ,    binaryDump ( decoded ,    decodedLen )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRandomBinaryRoundTrip"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    binary    =    new   byte [  ]  {     (  ( byte )     (  3  5  )  )  ,     (  ( byte )     (  1  5  2  )  )  ,     (  ( byte )     (  1  9  )  )  ,     (  ( byte )     (  2  2  8  )  )  ,     (  ( byte )     (  1  1  8  )  )  ,     (  ( byte )     (  6  5  )  )  ,     (  ( byte )     (  1  7  8  )  )  ,     (  ( byte )     (  2  0  1  )  )  ,     (  ( byte )     (  1  2  7  )  )  ,     (  ( byte )     (  1  0  )  )  ,     (  ( byte )     (  1  6  6  )  )  ,     (  ( byte )     (  2  1  6  )  )     }  ;", "int   encodedLen    =    IndexableBinaryStringTools . getEncodedLength ( binary ,     0  ,    binary . length )  ;", "char [  ]    encoded    =    new   char [ encodedLen ]  ;", "IndexableBinaryStringTools . encode ( binary ,     0  ,    binary . length ,    encoded ,     0  ,    encoded . length )  ;", "int   decodedLen    =    IndexableBinaryStringTools . getDecodedLength ( encoded ,     0  ,    encoded . length )  ;", "byte [  ]    decoded    =    new   byte [ decodedLen ]  ;", "IndexableBinaryStringTools . decode ( encoded ,     0  ,    encoded . length ,    decoded ,     0  ,    decoded . length )  ;", "assertEquals (  (  (  (  (  (  (  (  (  (  \" Round   trip   decode / decode   returned   different   results :  \"     +     (  . LINE _ SEPARATOR )  )     +     \" original :     \"  )     +     ( binaryDump ( binary ,    binary . length )  )  )     +     (  . LINE _ SEPARATOR )  )     +     \"    encoded :     \"  )     +     ( charArrayDump ( encoded ,    encoded . length )  )  )     +     (  . LINE _ SEPARATOR )  )     +     \"    decoded :     \"  )     +     ( binaryDump ( decoded ,    decoded . length )  )  )  ,    binaryDump ( binary ,    binary . length )  ,    binaryDump ( decoded ,    decoded . length )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleBinaryRoundTrip"], "fileName": "org.elasticsearch.index.analysis.IndexableBinaryStringToolsTests"}, {"methodBody": ["METHOD_START", "{", "return   ignoreCase ;", "}", "METHOD_END"], "methodName": ["ignoreCase"], "fileName": "org.elasticsearch.index.analysis.JapaneseStopTokenFilterFactory"}, {"methodBody": ["METHOD_START", "{", "return   stopWords ;", "}", "METHOD_END"], "methodName": ["stopWords"], "fileName": "org.elasticsearch.index.analysis.JapaneseStopTokenFilterFactory"}, {"methodBody": ["METHOD_START", "{", "String   actual    =    readFully ( filtered )  ;", "assertThat ( actual ,    equalTo ( expected )  )  ;", "}", "METHOD_END"], "methodName": ["assertCharFilterEquals"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "stream . reset (  )  ;", "CharTermAttribute   termAttr    =    stream . getAttribute ( CharTermAttribute . class )  ;", "assertThat ( termAttr ,    notNullValue (  )  )  ;", "int   i    =     0  ;", "while    ( stream . incrementToken (  )  )     {", "assertThat ( expected . length ,    greaterThan ( i )  )  ;", "assertThat (  (  \" expected   different   term   at       \"     +    i )  ,    termAttr . toString (  )  ,    equalTo ( expected [  ( i +  +  )  ]  )  )  ;", "}", "assertThat (  \" not   all   tokens   produced \"  ,    i ,    equalTo ( expected . length )  )  ;", "}", "METHOD_END"], "methodName": ["assertSimpleTSOutput"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   empty _ dict    =    KuromojiAnalysisTests . class . getResourceAsStream (  \" empty _ user _ dict . txt \"  )  ;", "InputStream   dict    =    KuromojiAnalysisTests . class . getResourceAsStream (  \" user _ dict . txt \"  )  ;", "Path   home    =    createTempDir (  )  ;", "Path   config    =    home . resolve (  \" config \"  )  ;", "Files . createDirectory ( config )  ;", "Files . copy ( empty _ dict ,    config . resolve (  \" empty _ user _ dict . txt \"  )  )  ;", "Files . copy ( dict ,    config . resolve (  \" user _ dict . txt \"  )  )  ;", "String   json    =     \"  / org / elasticsearch / index / analysis / kuromoji _ analysis . json \"  ;", "Settings   settings    =    Settings . builder (  )  . loadFromStream ( json ,    KuromojiAnalysisTests . class . getResourceAsStream ( json )  ,    false )  . put ( SETTING _ VERSION _ CREATED ,    CURRENT )  . build (  )  ;", "Settings   nodeSettings    =    Settings . builder (  )  . put ( PATH _ HOME _ SETTING . getKey (  )  ,    home )  . build (  )  ;", "return   KuromojiAnalysisTests . createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    nodeSettings ,    settings ,    new   AnalysisKuromojiPlugin (  )  )  ;", "}", "METHOD_END"], "methodName": ["createTestAnalysis"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   buffer    =    new   StringBuilder (  )  ;", "int   ch ;", "while    (  ( ch    =    reader . read (  )  )     !  =     (  -  1  )  )     {", "buffer . append (  (  ( char )     ( ch )  )  )  ;", "}", "return   buffer . toString (  )  ;", "}", "METHOD_END"], "methodName": ["readFully"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenFilterFactory   tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ pos \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiPartOfSpeechFilterFactory . class )  )  ;", "String   source    =     \"  \u00cb\u00bd  \u00a4\u00cf  \u00d6\u00c6  \u00cf\u00de  \u00a5\u00b9  \u00a5\u00d4  \u00a9`  \u00a5\u00c9  \u00a4\u00f2  \u00b3\u00ac  \u00a4\u00a8  \u00a4\u00eb  \u00a1\u00a3  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00cb\u00bd  \"  ,     \"  \u00a4\u00cf  \"  ,     \"  \u00d6\u00c6  \u00cf\u00de  \"  ,     \"  \u00a5\u00b9  \u00a5\u00d4  \u00a9`  \u00a5\u00c9  \"  ,     \"  \u00a4\u00f2  \"     }  ;", "Tokenizer   tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testBaseFormFilterFactory"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" kuromoji _ tokenizer \"  )  ;", "assertThat ( tokenizerFactory ,    instanceOf ( KuromojiTokenizerFactory . class )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" kuromoji _ part _ of _ speech \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( KuromojiPartOfSpeechFilterFactory . class )  )  ;", "filterFactory    =    analysis . tokenFilter . get (  \" kuromoji _ readingform \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( KuromojiReadingFormFilterFactory . class )  )  ;", "filterFactory    =    analysis . tokenFilter . get (  \" kuromoji _ baseform \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( KuromojiBaseFormFilterFactory . class )  )  ;", "filterFactory    =    analysis . tokenFilter . get (  \" kuromoji _ stemmer \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( KuromojiKatakanaStemmerFactory . class )  )  ;", "filterFactory    =    analysis . tokenFilter . get (  \" ja _ stop \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( JapaneseStopTokenFilterFactory . class )  )  ;", "filterFactory    =    analysis . tokenFilter . get (  \" kuromoji _ number \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( KuromojiNumberFilterFactory . class )  )  ;", "IndexAnalyzers   indexAnalyzers    =    analysis . indexAnalyzers ;", "NamedAnalyzer   analyzer    =    indexAnalyzers . get (  \" kuromoji \"  )  ;", "assertThat ( analyzer . analyzer (  )  ,    instanceOf ( JapaneseAnalyzer . class )  )  ;", "analyzer    =    indexAnalyzers . get (  \" my _ analyzer \"  )  ;", "assertThat ( analyzer . analyzer (  )  ,    instanceOf ( CustomAnalyzer . class )  )  ;", "assertThat ( analyzer . analyzer (  )  . tokenStream ( null ,    new   StringReader (  \"  \"  )  )  ,    instanceOf ( JapaneseTokenizer . class )  )  ;", "CharFilterFactory   charFilterFactory    =    analysis . charFilter . get (  \" kuromoji _ iteration _ mark \"  )  ;", "assertThat ( charFilterFactory ,    instanceOf ( KuromojiIterationMarkCharFilterFactory . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultsKuromojiAnalysis"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "CharFilterFactory   charFilterFactory    =    analysis . charFilter . get (  \" kuromoji _ im _ only _ kanji \"  )  ;", "assertNotNull ( charFilterFactory )  ;", "assertThat ( charFilterFactory ,    instanceOf ( KuromojiIterationMarkCharFilterFactory . class )  )  ;", "String   source    =     \"  \u00a4\u00c8  \u00a4\u00b3  \u00a4\u00ed  \u00a9g  \u00a9f  \u00a9f  \u00a1\u00a2  \u00a5\u00b8  \u00a9d  \u00a4\u00ac  \u00a1\u00a2  \u2022r  \u00a1\u00a9  \u00a1\u00a2  \u00f1R  \u00c2\u00b9  \u00a1\u00a9  \u00a1\u00a9  \u00a4\u00b7  \u00a4\u00a4  \"  ;", "String   expected    =     \"  \u00a4\u00c8  \u00a4\u00b3  \u00a4\u00ed  \u00a9g  \u00a9f  \u00a9f  \u00a1\u00a2  \u00a5\u00b8  \u00a9d  \u00a4\u00ac  \u00a1\u00a2  \u2022r  \u2022r  \u00a1\u00a2  \u00f1R  \u00c2\u00b9  \u00f1R  \u00c2\u00b9  \u00a4\u00b7  \u00a4\u00a4  \"  ;", "assertCharFilterEquals ( charFilterFactory . create ( new   StringReader ( source )  )  ,    expected )  ;", "charFilterFactory    =    analysis . charFilter . get (  \" kuromoji _ im _ only _ kana \"  )  ;", "assertNotNull ( charFilterFactory )  ;", "assertThat ( charFilterFactory ,    instanceOf ( KuromojiIterationMarkCharFilterFactory . class )  )  ;", "expected    =     \"  \u00a4\u00c8  \u00a4\u00b3  \u00a4\u00ed  \u00a4\u00c9  \u00a4\u00b3  \u00a4\u00ed  \u00a1\u00a2  \u00a5\u00b8  \u00a5\u00b8  \u00a4\u00ac  \u00a1\u00a2  \u2022r  \u00a1\u00a9  \u00a1\u00a2  \u00f1R  \u00c2\u00b9  \u00a1\u00a9  \u00a1\u00a9  \u00a4\u00b7  \u00a4\u00a4  \"  ;", "assertCharFilterEquals ( charFilterFactory . create ( new   StringReader ( source )  )  ,    expected )  ;", "charFilterFactory    =    analysis . charFilter . get (  \" kuromoji _ im _ default \"  )  ;", "assertNotNull ( charFilterFactory )  ;", "assertThat ( charFilterFactory ,    instanceOf ( KuromojiIterationMarkCharFilterFactory . class )  )  ;", "expected    =     \"  \u00a4\u00c8  \u00a4\u00b3  \u00a4\u00ed  \u00a4\u00c9  \u00a4\u00b3  \u00a4\u00ed  \u00a1\u00a2  \u00a5\u00b8  \u00a5\u00b8  \u00a4\u00ac  \u00a1\u00a2  \u2022r  \u2022r  \u00a1\u00a2  \u00f1R  \u00c2\u00b9  \u00f1R  \u00c2\u00b9  \u00a4\u00b7  \u00a4\u00a4  \"  ;", "assertCharFilterEquals ( charFilterFactory . create ( new   StringReader ( source )  )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testIterationMarkCharFilter"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenFilterFactory   tokenFilter    =    analysis . tokenFilter . get (  \" ja _ stop \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( JapaneseStopTokenFilterFactory . class )  )  ;", "String   source    =     \"  \u00cb\u00bd  \u00a4\u00cf  \u00d6\u00c6  \u00cf\u00de  \u00a5\u00b9  \u00a5\u00d4  \u00a9`  \u00a5\u00c9  \u00a4\u00f2  \u00b3\u00ac  \u00a4\u00a8  \u00a4\u00eb  \u00a1\u00a3  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00cb\u00bd  \"  ,     \"  \u00d6\u00c6  \u00cf\u00de  \"  ,     \"  \u00b3\u00ac  \u00a4\u00a8  \u00a4\u00eb  \"     }  ;", "Tokenizer   tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testJapaneseStopFilterFactory"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenFilterFactory   tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ stemmer \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiKatakanaStemmerFactory . class )  )  ;", "String   source    =     \"  \u00c3\u00f7  \u00e1\u00e1  \u00c8\u00d5  \u00a5\u00d1  \u00a9`  \u00a5\u00c6  \u00a5\u00a3  \u00a9`  \u00a4\u00cb  \u00d0\u00d0  \u00a4\u00af  \u00d3\u00e8  \u00b6\u00a8  \u00a4\u00ac  \u00a4\u00a2  \u00a4\u00eb  \u00a1\u00a3  \u2021\u00ed  \u2022\u00f8  \u00f0^  \u00a4\u00c7  \u00d9Y  \u00c1\u00cf  \u00a4\u00f2  \u00a5\u00b3  \u00a5\u00d4  \u00a9`  \u00a4\u00b7  \u00a4\u00de  \u00a4\u00b7  \u00a4\u00bf  \u00a1\u00a3  \"  ;", "Tokenizer   tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "String [  ]    expected _ tokens _ katakana    =    new   String [  ]  {     \"  \u00c3\u00f7  \u00e1\u00e1  \u00c8\u00d5  \"  ,     \"  \u00a5\u00d1  \u00a9`  \u00a5\u00c6  \u00a5\u00a3  \"  ,     \"  \u00a4\u00cb  \"  ,     \"  \u00d0\u00d0  \u00a4\u00af  \"  ,     \"  \u00d3\u00e8  \u00b6\u00a8  \"  ,     \"  \u00a4\u00ac  \"  ,     \"  \u00a4\u00a2  \u00a4\u00eb  \"  ,     \"  \u2021\u00ed  \u2022\u00f8  \u00f0^  \"  ,     \"  \u00a4\u00c7  \"  ,     \"  \u00d9Y  \u00c1\u00cf  \"  ,     \"  \u00a4\u00f2  \"  ,     \"  \u00a5\u00b3  \u00a5\u00d4  \u00a9`  \"  ,     \"  \u00a4\u00b7  \"  ,     \"  \u00a4\u00de  \u00a4\u00b7  \"  ,     \"  \u00a4\u00bf  \"     }  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected _ tokens _ katakana )  ;", "tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ ks \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiKatakanaStemmerFactory . class )  )  ;", "tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "expected _ tokens _ katakana    =    new   String [  ]  {     \"  \u00c3\u00f7  \u00e1\u00e1  \u00c8\u00d5  \"  ,     \"  \u00a5\u00d1  \u00a9`  \u00a5\u00c6  \u00a5\u00a3  \u00a9`  \"  ,     \"  \u00a4\u00cb  \"  ,     \"  \u00d0\u00d0  \u00a4\u00af  \"  ,     \"  \u00d3\u00e8  \u00b6\u00a8  \"  ,     \"  \u00a4\u00ac  \"  ,     \"  \u00a4\u00a2  \u00a4\u00eb  \"  ,     \"  \u2021\u00ed  \u2022\u00f8  \u00f0^  \"  ,     \"  \u00a4\u00c7  \"  ,     \"  \u00d9Y  \u00c1\u00cf  \"  ,     \"  \u00a4\u00f2  \"  ,     \"  \u00a5\u00b3  \u00a5\u00d4  \u00a9`  \"  ,     \"  \u00a4\u00b7  \"  ,     \"  \u00a4\u00de  \u00a4\u00b7  \"  ,     \"  \u00a4\u00bf  \"     }  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected _ tokens _ katakana )  ;", "}", "METHOD_END"], "methodName": ["testKatakanaStemFilter"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" kuromoji _ empty _ user _ dict \"  )  ;", "assertThat ( tokenizerFactory ,    instanceOf ( KuromojiTokenizerFactory . class )  )  ;", "}", "METHOD_END"], "methodName": ["testKuromojiEmptyUserDict"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" kuromoji _ user _ dict \"  )  ;", "String   source    =     \"  \u00cb\u00bd  \u00a4\u00cf  \u00d6\u00c6  \u00cf\u00de  \u00a5\u00b9  \u00a5\u00d4  \u00a9`  \u00a5\u00c9  \u00a4\u00f2  \u00b3\u00ac  \u00a4\u00a8  \u00a4\u00eb  \u00a1\u00a3  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00cb\u00bd  \"  ,     \"  \u00a4\u00cf  \"  ,     \"  \u00d6\u00c6  \u00cf\u00de  \u00a5\u00b9  \u00a5\u00d4  \u00a9`  \u00a5\u00c9  \"  ,     \"  \u00a4\u00f2  \"  ,     \"  \u00b3\u00ac  \u00a4\u00a8  \u00a4\u00eb  \"     }  ;", "Tokenizer   tokenizer    =    tokenizerFactory . create (  )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenizer ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testKuromojiUserDict"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" kuromoji _ nbest _ both \"  )  ;", "String   source    =     \"  \u00f8F  \u00c9\u00bd  \u00b7e  \u00a4\u00df  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00f8F  \"  ,     \"  \u00f8F  \u00c9\u00bd  \"  ,     \"  \u00c9\u00bd  \u00b7e  \u00a4\u00df  \"  ,     \"  \u00b7e  \u00a4\u00df  \"     }  ;", "Tokenizer   tokenizer    =    tokenizerFactory . create (  )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenizer ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testNbestBothOptions"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" kuromoji _ nbest _ cost \"  )  ;", "String   source    =     \"  \u00f8F  \u00c9\u00bd  \u00b7e  \u00a4\u00df  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00f8F  \"  ,     \"  \u00f8F  \u00c9\u00bd  \"  ,     \"  \u00c9\u00bd  \u00b7e  \u00a4\u00df  \"  ,     \"  \u00b7e  \u00a4\u00df  \"     }  ;", "Tokenizer   tokenizer    =    tokenizerFactory . create (  )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenizer ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testNbestCost"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" kuromoji _ nbest _ examples \"  )  ;", "String   source    =     \"  \u00f8F  \u00c9\u00bd  \u00b7e  \u00a4\u00df  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00f8F  \"  ,     \"  \u00f8F  \u00c9\u00bd  \"  ,     \"  \u00c9\u00bd  \u00b7e  \u00a4\u00df  \"  ,     \"  \u00b7e  \u00a4\u00df  \"     }  ;", "Tokenizer   tokenizer    =    tokenizerFactory . create (  )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenizer ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testNbestExample"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenFilterFactory   tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ number \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiNumberFilterFactory . class )  )  ;", "String   source    =     \"  \u00b1\u00be  \u00c8\u00d5  \u00ca\u00ae  \u00cd\u00f2  \u00b6\u00fe  \u00c7\u00a7  \u00ce\u00e5  \u00b0\u00d9  \u0192\u00d2  \u00a4\u00ce  \u00a5\u00ef  \u00a5\u00a4  \u00a5\u00f3  \u00a4\u00f2  \u00d9I  \u00a4\u00c3  \u00a4\u00bf  \"  ;", "String [  ]    expected    =    new   String [  ]  {     \"  \u00b1\u00be  \u00c8\u00d5  \"  ,     \"  1  0  2  5  0  0  \"  ,     \"  \u0192\u00d2  \"  ,     \"  \u00a4\u00ce  \"  ,     \"  \u00a5\u00ef  \u00a5\u00a4  \u00a5\u00f3  \"  ,     \"  \u00a4\u00f2  \"  ,     \"  \u00d9I  \u00a4\u00c3  \"  ,     \"  \u00a4\u00bf  \"     }  ;", "Tokenizer   tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testNumberFilterFactory"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenFilterFactory   tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ part _ of _ speech \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiPartOfSpeechFilterFactory . class )  )  ;", "String   source    =     \"  \u00ca\u00d9  \u00cb\u00be  \u00a4\u00ac  \u00a4\u00aa  \u00a4\u00a4  \u00a4\u00b7  \u00a4\u00a4  \u00a4\u00cd  \"  ;", "String [  ]    expected _ tokens    =    new   String [  ]  {     \"  \u00ca\u00d9  \u00cb\u00be  \"  ,     \"  \u00a4\u00aa  \u00a4\u00a4  \u00a4\u00b7  \u00a4\u00a4  \"     }  ;", "Tokenizer   tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected _ tokens )  ;", "}", "METHOD_END"], "methodName": ["testPartOfSpeechFilter"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    KuromojiAnalysisTests . createTestAnalysis (  )  ;", "TokenFilterFactory   tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ rf \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiReadingFormFilterFactory . class )  )  ;", "String   source    =     \"  \u00bd\u00f1  \u00d2\u00b9  \u00a4\u00cf  \u00a5\u00ed  \u00a5\u00d0  \u00a9`  \u00a5\u00c8  \u00cf\u00c8  \u00c9\u00fa  \u00a4\u00c8  \u00d4\u2019  \u00a4\u00b7  \u00a4\u00bf  \"  ;", "String [  ]    expected _ tokens _ romaji    =    new   String [  ]  {     \" kon ' ya \"  ,     \" ha \"  ,     \" robato \"  ,     \" sensei \"  ,     \" to \"  ,     \" hanashi \"  ,     \" ta \"     }  ;", "Tokenizer   tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected _ tokens _ romaji )  ;", "tokenizer    =    new   JapaneseTokenizer ( null ,    true ,    Mode . SEARCH )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "String [  ]    expected _ tokens _ katakana    =    new   String [  ]  {     \"  \u00a5\u00b3  \u00a5\u00f3  \u00a5\u00e4  \"  ,     \"  \u00a5\u00cf  \"  ,     \"  \u00a5\u00ed  \u00a5\u00d0  \u00a9`  \u00a5\u00c8  \"  ,     \"  \u00a5\u00bb  \u00a5\u00f3  \u00a5\u00bb  \u00a5\u00a4  \"  ,     \"  \u00a5\u00c8  \"  ,     \"  \u00a5\u00cf  \u00a5\u00ca  \u00a5\u00b7  \"  ,     \"  \u00a5\u00bf  \"     }  ;", "tokenFilter    =    analysis . tokenFilter . get (  \" kuromoji _ readingform \"  )  ;", "assertThat ( tokenFilter ,    instanceOf ( KuromojiReadingFormFilterFactory . class )  )  ;", "KuromojiAnalysisTests . assertSimpleTSOutput ( tokenFilter . create ( tokenizer )  ,    expected _ tokens _ katakana )  ;", "}", "METHOD_END"], "methodName": ["testReadingFormFilterFactory"], "fileName": "org.elasticsearch.index.analysis.KuromojiAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.analysis.KuromojiClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "JapaneseTokenizer . Mode   mode    =    JapaneseTokenizer . DEFAULT _ MODE ;", "String   modeSetting    =    settings . get (  \" mode \"  ,    null )  ;", "if    ( modeSetting    !  =    null )     {", "if    (  \" search \"  . equalsIgnoreCase ( modeSetting )  )     {", "mode    =    Mode . SEARCH ;", "} else", "if    (  \" normal \"  . equalsIgnoreCase ( modeSetting )  )     {", "mode    =    Mode . NORMAL ;", "} else", "if    (  \" extended \"  . equalsIgnoreCase ( modeSetting )  )     {", "mode    =    Mode . EXTENDED ;", "}", "}", "return   mode ;", "}", "METHOD_END"], "methodName": ["getMode"], "fileName": "org.elasticsearch.index.analysis.KuromojiTokenizerFactory"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   Reader   reader    =    Analysis . getReaderFromFile ( env ,    settings ,     . USER _ DICT _ OPTION )  ;", "if    ( reader    =  =    null )     {", "return   null ;", "} else    {", "try    {", "return   UserDictionary . open ( reader )  ;", "}    finally    {", "reader . close (  )  ;", "}", "}", "}    catch    ( IOException   e )     {", "throw   new   ElasticsearchException (  \" failed   to   load   kuromoji   user   dictionary \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getUserDictionary"], "fileName": "org.elasticsearch.index.analysis.KuromojiTokenizerFactory"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.analysis.PhoneticClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "final   TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    EMPTY ,    new   AnalysisStempelPlugin (  )  )  ;", "TokenFilterFactory   tokenizerFactory    =    analysis . tokenFilter . get (  \" polish _ stem \"  )  ;", "MatcherAssert . assertThat ( tokenizerFactory ,    instanceOf ( PolishStemTokenFilterFactory . class )  )  ;", "Analyzer   analyzer    =    analysis . indexAnalyzers . get (  \" polish \"  )  . analyzer (  )  ;", "MatcherAssert . assertThat ( analyzer ,    instanceOf ( zer . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultsPolishAnalysis"], "fileName": "org.elasticsearch.index.analysis.PolishAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    EMPTY ,    new   AnalysisICUPlugin (  )  )  ;", "TokenizerFactory   tokenizerFactory    =    tokenizer . get (  \" icu _ tokenizer \"  )  ;", "assertThat ( tokenizerFactory ,    instanceOf ( IcuTokenizerFactory . class )  )  ;", "TokenFilterFactory   filterFactory    =    tokenFilter . get (  \" icu _ normalizer \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( IcuNormalizerTokenFilterFactory . class )  )  ;", "filterFactory    =    tokenFilter . get (  \" icu _ folding \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( IcuFoldingTokenFilterFactory . class )  )  ;", "filterFactory    =    tokenFilter . get (  \" icu _ collation \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( IcuCollationTokenFilterFactory . class )  )  ;", "filterFactory    =    tokenFilter . get (  \" icu _ transform \"  )  ;", "assertThat ( filterFactory ,    instanceOf ( IcuTransformTokenFilterFactory . class )  )  ;", "CharFilterFactory   charFilterFactory    =    charFilter . get (  \" icu _ normalizer \"  )  ;", "assertThat ( charFilterFactory ,    instanceOf ( IcuNormalizerCharFilterFactory . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultsIcuAnalysis"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "assertCollation ( factory ,    string 1  ,    string 2  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["assertCollatesToSame"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "CharTermAttribute   term 1     =    stream 1  . addAttribute ( CharTermAttribute . class )  ;", "CharTermAttribute   term 2     =    stream 2  . addAttribute ( CharTermAttribute . class )  ;", "stream 1  . reset (  )  ;", "stream 2  . reset (  )  ;", "assertThat ( stream 1  . increment (  )  ,    equalTo ( true )  )  ;", "assertThat ( stream 2  . increment (  )  ,    equalTo ( true )  )  ;", "assertThat ( Integer . signum ( term 1  . toString (  )  . compareTo ( term 2  . toString (  )  )  )  ,    equalTo ( Integer . signum ( comparison )  )  )  ;", "assertThat ( stream 1  . increment (  )  ,    equalTo ( false )  )  ;", "assertThat ( stream 2  . increment (  )  ,    equalTo ( false )  )  ;", "stream 1  . end (  )  ;", "stream 2  . end (  )  ;", "stream 1  . close (  )  ;", "stream 2  . close (  )  ;", "}", "METHOD_END"], "methodName": ["assertCollation"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Tokenizer   tokenizer    =    new   KeywordTokenizer (  )  ;", "tokenizer . setReader ( new   StringReader ( string 1  )  )  ;", "TokenStream   stream 1     =    factory . create ( tokenizer )  ;", "tokenizer    =    new   KeywordTokenizer (  )  ;", "tokenizer . setReader ( new   StringReader ( string 2  )  )  ;", "TokenStream   stream 2     =    factory . create ( tokenizer )  ;", "assert ( stream 1  ,    stream 2  ,    comparison )  ;", "}", "METHOD_END"], "methodName": ["assertCollation"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . rules \"  ,     \"  & a    <    g \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollation ( filterFactory ,     \" green \"  ,     \" bird \"  ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testBasicCustomRules"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" tr \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" primary \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" I   W ? LL   USE   TURK ? SH   CASING \"  ,     \"  ?    will   use   turkish   cas ? ng \"  )  ;", "}", "METHOD_END"], "methodName": ["testBasicUsage"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "RuleBasedCollator   baseCollator    =     (  ( RuleBasedCollator )     ( Collator . getInstance ( new   ULocale (  \" de _ DE \"  )  )  )  )  ;", "String   DIN 5  0  0  7  _  2  _ tailorings    =     \"  &    ae    ,    a \\ u 0  3  0  8     &    AE    ,    A \\ u 0  3  0  8  \"     +     (  \"  &    oe    ,    o \\ u 0  3  0  8     &    OE    ,    O \\ u 0  3  0  8  \"     +     \"  &    ue    ,    u \\ u 0  3  0  8     &    UE    ,    u \\ u 0  3  0  8  \"  )  ;", "RuleBasedCollator   tailoredCollator    =    new   RuleBasedCollator (  (  ( baseCollator . getRules (  )  )     +    DIN 5  0  0  7  _  2  _ tailorings )  )  ;", "String   tailoredRules    =    tailoredCollator . getRules (  )  ;", "Settings   settings    =    Settings . builder (  )  . put (  \" filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" filter . myCollator . rules \"  ,    tailoredRules )  . put (  \" filter . myCollator . strength \"  ,     \" primary \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" T ? ne \"  ,     \" Toene \"  )  ;", "}", "METHOD_END"], "methodName": ["testCustomRules"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" primary \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" FOO \"  ,     \" foo \"  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultUsage"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" en \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" primary \"  )  . put (  \" index . analysis . filter . myCollator . caseLevel \"  ,     \" true \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" r \u00a8\u00a6 sum \u00a8\u00a6  \"  ,     \" resume \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" R \u00a8\u00a6 sum \u00a8\u00a6  \"  ,     \" Resume \"  )  ;", "assertCollation ( filterFactory ,     \" resume \"  ,     \" Resume \"  ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testIgnoreAccentsButNotCase"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" en \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" primary \"  )  . put (  \" index . analysis . filter . myCollator . alternate \"  ,     \" shifted \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" foo - bar \"  ,     \" foo   bar \"  )  ;", "}", "METHOD_END"], "methodName": ["testIgnorePunctuation"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" en \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" primary \"  )  . put (  \" index . analysis . filter . myCollator . alternate \"  ,     \" shifted \"  )  . put (  \" index . analysis . filter . myCollator . variableTop \"  ,     \"     \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" foo   bar \"  ,     \" foobar \"  )  ;", "assertCollation ( filterFactory ,     \" foo - bar \"  ,     \" foo   bar \"  ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testIgnoreWhitespace"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" tr \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" primary \"  )  . put (  \" index . analysis . filter . myCollator . decomposition \"  ,     \" canonical \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" I   WI \\ u 0  3  0  7 LL   USE   TURK \\ u 0  1  3  0 SH   CASING \"  ,     \"  ?    will   use   turkish   cas ? ng \"  )  ;", "}", "METHOD_END"], "methodName": ["testNormalization"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" en \"  )  . put (  \" index . analysis . filter . myCollator . numeric \"  ,     \" true \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollation ( filterFactory ,     \" foobar -  9  \"  ,     \" foobar -  1  0  \"  ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testNumerics"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" en \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" secondary \"  )  . put (  \" index . analysis . filter . myCollator . decomposition \"  ,     \" no \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollatesToSame ( filterFactory ,     \" TESTING \"  ,     \" testing \"  )  ;", "}", "METHOD_END"], "methodName": ["testSecondaryStrength"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . filter . myCollator . type \"  ,     \" icu _ collation \"  )  . put (  \" index . analysis . filter . myCollator . language \"  ,     \" en \"  )  . put (  \" index . analysis . filter . myCollator . strength \"  ,     \" tertiary \"  )  . put (  \" index . analysis . filter . myCollator . caseFirst \"  ,     \" upper \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myCollator \"  )  ;", "assertCollation ( filterFactory ,     \" Resume \"  ,     \" resume \"  ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testUpperCaseFirst"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuCollationTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . char _ filter . myNormalizerChar . type \"  ,     \" icu _ normalizer \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "CharFilterFactory   charFilterFactory    =    analysis . charFilter . get (  \" myNormalizerChar \"  )  ;", "String   input    =     \"  ?  ?  ?  5  \u00a1\u00e6  \u00a1\u00ed  \u00a9Z  ?  \u00a3\u00ac  \u00a5\u00d0  \u00a5\u00c3  \u00a5\u00d5  \u00a5\u00a1  \u00a9`  \u00a4\u00ce  \u00d5\u00fd  \u00d2\u017d  \u00bb\u00af  \u00a4\u00ce  \u00a5\u00c6  \u00a5\u00b9  \u00a5\u00c8  \u00a3\u00ae  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ? g ?  ?  /  ?  ?  ?  ?  ?  ?  ? chk ?  ?  ?  ?  ?  \"  ;", "Normalizer 2    normalizer    =    Normalizer 2  . getInstance ( null ,     \" nfkc _ cf \"  ,    COMPOSE )  ;", "String   expectedOutput    =    normalizer . normalize ( input )  ;", "CharFilter   inputReader    =     (  ( CharFilter )     ( charFilterFactory . create ( new   StringReader ( input )  )  )  )  ;", "char [  ]    tempBuff    =    new   char [  1  0  ]  ;", "StringBuilder   output    =    new   StringBuilder (  )  ;", "while    ( true )     {", "int   length    =    inputReader . read ( tempBuff )  ;", "if    ( length    =  =     (  -  1  )  )", "break ;", "output . append ( tempBuff ,     0  ,    length )  ;", "assertEquals ( output . toString (  )  ,    normalizer . normalize ( input . substring (  0  ,    inputReader . correctOffset ( output . length (  )  )  )  )  )  ;", "}", "assertEquals ( expectedOutput ,    output . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultSetting"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuNormalizerCharFilterTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" index . analysis . char _ filter . myNormalizerChar . type \"  ,     \" icu _ normalizer \"  )  . put (  \" index . analysis . char _ filter . myNormalizerChar . name \"  ,     \" nfkc \"  )  . put (  \" index . analysis . char _ filter . myNormalizerChar . mode \"  ,     \" decompose \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisICUPlugin (  )  )  ;", "CharFilterFactory   charFilterFactory    =    analysis . charFilter . get (  \" myNormalizerChar \"  )  ;", "String   input    =     \"  ?  ?  ?  5  \u00a1\u00e6  \u00a1\u00ed  \u00a9Z  ?  \u00a3\u00ac  \u00a5\u00d0  \u00a5\u00c3  \u00a5\u00d5  \u00a5\u00a1  \u00a9`  \u00a4\u00ce  \u00d5\u00fd  \u00d2\u017d  \u00bb\u00af  \u00a4\u00ce  \u00a5\u00c6  \u00a5\u00b9  \u00a5\u00c8  \u00a3\u00ae  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ? g ?  ?  /  ?  ?  ?  ?  ?  ?  ? chk ?  ?  ?  ?  ?  \"  ;", "Normalizer 2    normalizer    =    Normalizer 2  . getInstance ( null ,     \" nfkc \"  ,    DECOMPOSE )  ;", "String   expectedOutput    =    normalizer . normalize ( input )  ;", "CharFilter   inputReader    =     (  ( CharFilter )     ( charFilterFactory . create ( new   StringReader ( input )  )  )  )  ;", "char [  ]    tempBuff    =    new   char [  1  0  ]  ;", "StringBuilder   output    =    new   StringBuilder (  )  ;", "while    ( true )     {", "int   length    =    inputReader . read ( tempBuff )  ;", "if    ( length    =  =     (  -  1  )  )", "break ;", "output . append ( tempBuff ,     0  ,    length )  ;", "assertEquals ( output . toString (  )  ,    normalizer . normalize ( input . substring (  0  ,    inputReader . correctOffset ( output . length (  )  )  )  )  )  ;", "}", "assertEquals ( expectedOutput ,    output . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNameAndModeSetting"], "fileName": "org.elasticsearch.index.analysis.SimpleIcuNormalizerCharFilterTests"}, {"methodBody": ["METHOD_START", "{", "String   yaml    =     \"  / org / elasticsearch / index / analysis / phonetic -  1  . yml \"  ;", "Settings   settings    =    Settings . builder (  )  . loadFromStream ( yaml ,    getClass (  )  . getResourceAsStream ( yaml )  ,    false )  . put ( SETTING _ VERSION _ CREATED ,    CURRENT )  . build (  )  ;", "this . analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    settings ,    new   AnalysisPhoneticPlugin (  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.elasticsearch.index.analysis.SimplePhoneticAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" beidermorsefilter \"  )  ;", "Tokenizer   tokenizer    =    new   WhitespaceTokenizer (  )  ;", "tokenizer . setReader ( new   StringReader (  \" ABADIAS \"  )  )  ;", "String [  ]    expected    =    new   String [  ]  {     \" abYdias \"  ,     \" abYdios \"  ,     \" abadia \"  ,     \" abadiaS \"  ,     \" abadias \"  ,     \" abadio \"  ,     \" abadioS \"  ,     \" abadios \"  ,     \" abodia \"  ,     \" abodiaS \"  ,     \" abodias \"  ,     \" abodio \"  ,     \" abodioS \"  ,     \" abodios \"  ,     \" avadias \"  ,     \" avadios \"  ,     \" avodias \"  ,     \" avodios \"  ,     \" obadia \"  ,     \" obadiaS \"  ,     \" obadias \"  ,     \" obadio \"  ,     \" obadioS \"  ,     \" obadios \"  ,     \" obodia \"  ,     \" obodiaS \"  ,     \" obodias \"  ,     \" obodioS \"     }  ;", "BaseTokenStreamTestCase . assertTokenStreamContents ( filterFactory . create ( tokenizer )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testPhoneticTokenFilterBeiderMorseNoLanguage"], "fileName": "org.elasticsearch.index.analysis.SimplePhoneticAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" beidermorsefilterfrench \"  )  ;", "Tokenizer   tokenizer    =    new   WhitespaceTokenizer (  )  ;", "tokenizer . setReader ( new   StringReader (  \" Rimbault \"  )  )  ;", "String [  ]    expected    =    new   String [  ]  {     \" rimbD \"  ,     \" rimbDlt \"  ,     \" rimba \"  ,     \" rimbalt \"  ,     \" rimbo \"  ,     \" rimbolt \"  ,     \" rimbu \"  ,     \" rimbult \"  ,     \" rmbD \"  ,     \" rmbDlt \"  ,     \" rmba \"  ,     \" rmbalt \"  ,     \" rmbo \"  ,     \" rmbolt \"  ,     \" rmbu \"  ,     \" rmbult \"     }  ;", "BaseTokenStreamTestCase . assertTokenStreamContents ( filterFactory . create ( tokenizer )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testPhoneticTokenFilterBeiderMorseWithLanguage"], "fileName": "org.elasticsearch.index.analysis.SimplePhoneticAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" daitch _ mokotoff \"  )  ;", "Tokenizer   tokenizer    =    new   WhitespaceTokenizer (  )  ;", "tokenizer . setReader ( new   StringReader (  \" chauptman \"  )  )  ;", "String [  ]    expected    =    new   String [  ]  {     \"  4  7  3  6  6  0  \"  ,     \"  5  7  3  6  6  0  \"     }  ;", "assertThat ( filterFactory . create ( tokenizer )  ,    instanceOf ( DaitchMokotoffSoundexFilter . class )  )  ;", "BaseTokenStreamTestCase . assertTokenStreamContents ( filterFactory . create ( tokenizer )  ,    expected )  ;", "}", "METHOD_END"], "methodName": ["testPhoneticTokenFilterDaitchMotokoff"], "fileName": "org.elasticsearch.index.analysis.SimplePhoneticAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" phonetic \"  )  ;", "MatcherAssert . assertThat ( filterFactory ,    instanceOf ( PhoneticTokenFilterFactory . class )  )  ;", "}", "METHOD_END"], "methodName": ["testPhoneticTokenFilterFactory"], "fileName": "org.elasticsearch.index.analysis.SimplePhoneticAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    EMPTY ,    new   AnalysisStempelPlugin (  )  )  ;", "Analyzer   analyzer    =    indexAnalyzers . get (  \" polish \"  )  . analyzer (  )  ;", "TokenStream   ts    =    analyzer . tokenStream (  \" test \"  ,    source )  ;", "CharTermAttribute   term 1     =    ts . addAttribute ( CharTermAttribute . class )  ;", "ts . reset (  )  ;", "for    ( String   expected    :    expected _ terms )     {", "assertThat ( ts . incrementToken (  )  ,    equalTo ( true )  )  ;", "assertThat ( term 1  . toString (  )  ,    equalTo ( expected )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAnalyzer"], "fileName": "org.elasticsearch.index.analysis.SimplePolishTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "testToken (  \" kwiaty \"  ,     \" kw ?  \"  )  ;", "testToken (  \" canona \"  ,     \"  ?  \"  )  ;", "testToken (  \" wirtualna \"  ,     \" wirtualny \"  )  ;", "testToken (  \" polska \"  ,     \" polski \"  )  ;", "testAnalyzer (  \" wirtualna   polska \"  ,     \" wirtualny \"  ,     \" polski \"  )  ;", "}", "METHOD_END"], "methodName": ["testBasicUsage"], "fileName": "org.elasticsearch.index.analysis.SimplePolishTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "Index   index    =    new   Index (  \" test \"  ,     \"  _ na _  \"  )  ;", "Settings   settings    =    Settings . builder (  )  . put (  \" filter . myStemmer . type \"  ,     \" polish _ stem \"  )  . build (  )  ;", "TestAnalysis   analysis    =    createTestAnalysis ( index ,    settings ,    new   AnalysisStempelPlugin (  )  )  ;", "TokenFilterFactory   filterFactory    =    analysis . tokenFilter . get (  \" myStemmer \"  )  ;", "Tokenizer   tokenizer    =    new   KeywordTokenizer (  )  ;", "tokenizer . setReader ( new   StringReader ( source )  )  ;", "TokenStream   ts    =    filterFactory . create ( tokenizer )  ;", "CharTermAttribute   term 1     =    ts . addAttribute ( CharTermAttribute . class )  ;", "ts . reset (  )  ;", "assertThat ( ts . incrementToken (  )  ,    equalTo ( true )  )  ;", "assertThat ( term 1  . toString (  )  ,    equalTo ( expected )  )  ;", "}", "METHOD_END"], "methodName": ["testToken"], "fileName": "org.elasticsearch.index.analysis.SimplePolishTokenFilterTests"}, {"methodBody": ["METHOD_START", "{", "final   TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    EMPTY ,    new   AnalysisSmartChinesePlugin (  )  )  ;", "TokenizerFactory   tokenizerFactory    =    analysis . tokenizer . get (  \" smartcn _ tokenizer \"  )  ;", "MatcherAssert . assertThat ( tokenizerFactory ,    instanceOf ( SmartChineseTokenizerTokenizerFactory . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultsIcuAnalysis"], "fileName": "org.elasticsearch.index.analysis.SimpleSmartChineseAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    EMPTY ,    new   AnalysisUkrainianPlugin (  )  )  ;", "Analyzer   analyzer    =    analysis . indexAnalyzers . get (  \" ukrainian \"  )  . analyzer (  )  ;", "TokenStream   ts    =    analyzer . tokenStream (  \" test \"  ,    source )  ;", "CharTermAttribute   term 1     =    ts . addAttribute ( CharTermAttribute . class )  ;", "ts . reset (  )  ;", "for    ( String   expected    :    expected _ terms )     {", "assertThat ( ts . incrementToken (  )  ,    equalTo ( true )  )  ;", "assertThat ( term 1  . toString (  )  ,    equalTo ( expected )  )  ;", "}", "assertThat ( ts . incrementToken (  )  ,    equalTo ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testAnalyzer"], "fileName": "org.elasticsearch.index.analysis.SimpleUkrainianAnalyzerTests"}, {"methodBody": ["METHOD_START", "{", "SimpleUkrainianAnalyzerTests . testAnalyzer (  \"  \u00a7\u00e9  \u00a7\u00d6  \u00a7\u00e2  \u00a7\u00d4  \u00a7\u00e5  \"  ,     \"  \u00a7\u00e9  \u00a7\u00d6  \u00a7\u00e2  \u00a7\u00d4  \u00a7\u00d1  \"  )  ;", "SimpleUkrainianAnalyzerTests . testAnalyzer (  \"  \u00a7\u00e2  \u00a7\u00e5  \u00a7\u00e7  \u00a7\u00d1  ?  \u00a7\u00e4  \u00a7\u00ee  \u00a7\u00e3  \u00a7\u00f1  \"  ,     \"  \u00a7\u00e2  \u00a7\u00e5  \u00a7\u00e7  \u00a7\u00d1  \u00a7\u00e4  \u00a7\u00da  \u00a7\u00e3  \u00a7\u00f1  \"  )  ;", "SimpleUkrainianAnalyzerTests . testAnalyzer (  \"  \u00a7\u00dc  \u00a7\u00e0  \u00a7\u00dd  \u00a7\u00e5  \"  ,     \"  \u00a7\u00dc  \u00a7\u00e0  \u00a7\u00dd  \u00a7\u00d1  \"  ,     \"  \u00a7\u00dc  \u00a7\u00e0  \u00a7\u00dd  \u00a7\u00e0  \"  ,     \"  \u00a7\u00dc  ?  \u00a7\u00dd  \"  )  ;", "SimpleUkrainianAnalyzerTests . testAnalyzer (  \"  \u00a7\u00b8  \u00a7\u00f1     \u00a7\u00e1  '  ?  \u00a7\u00e3  \u00a7\u00d1     \u00a7\u00e5     \u00a7\u00e3  \u00a7\u00d3  \u00a7\u00e0  \u00a7\u00f0     \u00a7\u00e9  \u00a7\u00d6  \u00a7\u00e2  \u00a7\u00d4  \u00a7\u00e5     \u00a7\u00e2  \u00a7\u00e5  \u00a7\u00e7  \u00a7\u00d1  ?  \u00a7\u00e4  \u00a7\u00ee  \u00a7\u00e3  \u00a7\u00f1     \u00a7\u00e1  \u00a7\u00e0     \u00a7\u00dc  \u00a7\u00e0  \u00a7\u00dd  \u00a7\u00e5  .  \"  ,     \"  \u00a7\u00e1  '  ?  \u00a7\u00e3  \u00a7\u00d1  \"  ,     \"  \u00a7\u00e9  \u00a7\u00d6  \u00a7\u00e2  \u00a7\u00d4  \u00a7\u00d1  \"  ,     \"  \u00a7\u00e2  \u00a7\u00e5  \u00a7\u00e7  \u00a7\u00d1  \u00a7\u00e4  \u00a7\u00da  \u00a7\u00e3  \u00a7\u00f1  \"  ,     \"  \u00a7\u00dc  \u00a7\u00e0  \u00a7\u00dd  \u00a7\u00d1  \"  ,     \"  \u00a7\u00dc  \u00a7\u00e0  \u00a7\u00dd  \u00a7\u00e0  \"  ,     \"  \u00a7\u00dc  ?  \u00a7\u00dd  \"  )  ;", "}", "METHOD_END"], "methodName": ["testBasicUsage"], "fileName": "org.elasticsearch.index.analysis.SimpleUkrainianAnalyzerTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.analysis.SmartCNClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.analysis.StempelClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "final   TestAnalysis   analysis    =    createTestAnalysis ( new   Index (  \" test \"  ,     \"  _ na _  \"  )  ,    EMPTY ,    new   AnalysisUkrainianPlugin (  )  )  ;", "Analyzer   analyzer    =    analysis . indexAnalyzers . get (  \" ukrainian \"  )  . analyzer (  )  ;", "MatcherAssert . assertThat ( analyzer ,    instanceOf ( UkrainianMorfologikAnalyzer . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultsUkranianAnalysis"], "fileName": "org.elasticsearch.index.analysis.UkrainianAnalysisTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.analysis.UkrainianClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "List < String >    parts    =    partition ( str )  ;", "String [  ]    codes    =    new   String [ parts . size (  )  ]  ;", "int   i    =     0  ;", "for    ( String   s    :    parts )     {", "codes [  ( i +  +  )  ]     =    subtute ( s )  ;", "}", "return   codes ;", "}", "METHOD_END"], "methodName": ["code"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "return   str . replaceAll (  \"  \\ u 0  0 c 4  \"  ,     \" AE \"  )  . replaceAll (  \"  \\ u 0  0 d 6  \"  ,     \" OE \"  )  . replaceAll (  \"  \\ u 0  0 dc \"  ,     \" UE \"  )  ;", "}", "METHOD_END"], "methodName": ["expandUmlauts"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "return    '  0  '  ;", "}", "METHOD_END"], "methodName": ["getCode"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "return   KoelnerPhonetik . POSTEL _ VARIATIONS _ PATTERNS ;", "}", "METHOD_END"], "methodName": ["getPatterns"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "String [  ]    kopho 1     =    code ( expandUmlauts ( o 1  . toString (  )  . toUpperCase ( Locale . GERMANY )  )  )  ;", "String [  ]    kopho 2     =    code ( expandUmlauts ( o 2  . toString (  )  . toUpperCase ( Locale . GERMANY )  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( kopho 1  . length )  ;    i +  +  )     {", "for    ( int   ii    =     0  ;    ii    <     ( kopho 2  . length )  ;    ii +  +  )     {", "if    ( kopho 1  [ i ]  . equals ( kopho 2  [ ii ]  )  )     {", "return    1  ;", "}", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["getRelativeValue"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "return   KoelnerPhonetik . POSTEL _ VARIATIONS _ REPLACEMENTS ;", "}", "METHOD_END"], "methodName": ["getReplacements"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "int   position    =     0  ;", "List < String >    variations    =    new   ArrayList <  >  (  )  ;", "variations . add (  \"  \"  )  ;", "while    ( position    <     ( str . length (  )  )  )     {", "int   i    =     0  ;", "int   substPos    =     -  1  ;", "while    (  ( substPos    <    position )     &  &     ( i    <     ( getPatterns (  )  . length )  )  )     {", "Matcher   m    =    variationsPatterns [ i ]  . matcher ( str )  ;", "while    (  ( substPos    <    position )     &  &     ( m . f (  )  )  )     {", "substPos    =    m . start (  )  ;", "}", "i +  +  ;", "}", "if    ( substPos    >  =    position )     {", "i -  -  ;", "List < String >    varNew    =    new   ArrayList <  >  (  )  ;", "String   prevPart    =    str . substring ( position ,    substPos )  ;", "for    ( int   ii    =     0  ;    ii    <     ( variations . size (  )  )  ;    ii +  +  )     {", "String   tmp    =    variations . get ( ii )  ;", "varNew . add ( tmp . concat (  ( prevPart    +     ( getReplacements (  )  [ i ]  )  )  )  )  ;", "variations . set ( ii ,     (  (  ( variations . get ( ii )  )     +    prevPart )     +     ( getPatterns (  )  [ i ]  )  )  )  ;", "}", "variations . addAll ( varNew )  ;", "position    =    substPos    +     ( getPatterns (  )  [ i ]  . length (  )  )  ;", "} else    {", "for    ( int   ii    =     0  ;    ii    <     ( variations . size (  )  )  ;    ii +  +  )     {", "variations . set ( ii ,     (  ( variations . get ( ii )  )     +     ( str . substring ( position ,    str . length (  )  )  )  )  )  ;", "}", "position    =    str . length (  )  ;", "}", "}", "return   variations ;", "}", "METHOD_END"], "methodName": ["getVariations"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "this . variationsPatterns    =    new   Pattern [ getPatterns (  )  . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( getPatterns (  )  . length )  ;    i +  +  )     {", "this . variationsPatterns [ i ]     =    Pattern . compile ( getPatterns (  )  [ i ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "String   primaryForm    =    str ;", "List < String >    parts    =    new   ArrayList <  >  (  )  ;", "parts . add ( primaryForm . replaceAll (  \"  [  ^  \\  \\ p { L }  \\  \\ p { N }  ]  \"  ,     \"  \"  )  )  ;", "if    (  !  ( primary )  )     {", "List < String >    tmpParts    =    new   ArrayList <  >  ( Arrays . asList ( str . split (  \"  [  \\  \\ p { Z }  \\  \\ p { C }  \\  \\ p { P }  ]  \"  )  )  )  ;", "int   numberOfParts    =    tmpParts . size (  )  ;", "while    (  ( tmpParts . size (  )  )     >     0  )     {", "StringBuilder   part    =    new   StringBuilder (  )  ;", "for    ( int   i    =     0  ;    i    <     ( tmpParts . size (  )  )  ;    i +  +  )     {", "part . append ( tmpParts . get ( i )  )  ;", "if    (  !  (  ( i    +     1  )     =  =    numberOfParts )  )     {", "parts . add ( part . toString (  )  )  ;", "}", "}", "tmpParts . remove (  0  )  ;", "}", "}", "List < String >    variations    =    new   ArrayList <  >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( parts . size (  )  )  ;    i +  +  )     {", "List < String >    variation    =    getVariations ( parts . get ( i )  )  ;", "if    ( variation    !  =    null )     {", "variations . addAll ( variation )  ;", "}", "}", "return   variations ;", "}", "METHOD_END"], "methodName": ["partition"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "if    (  ( str    =  =    null )     |  |     (  ( str . length (  )  )     =  =     0  )  )     {", "return    \"  \"  ;", "}", "int   i    =     0  ;", "int   j    =     0  ;", "StringBuilder   sb    =    new   StringBuilder (  )  . append ( str . charAt (  ( i +  +  )  )  )  ;", "char   c ;", "while    ( i    <     ( str . length (  )  )  )     {", "c    =    str . charAt ( i )  ;", "if    ( c    !  =     ( sb . charAt ( j )  )  )     {", "sb . append ( c )  ;", "j +  +  ;", "}", "i +  +  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["removeSequences"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "String   s    =    expandUmlauts ( str . toUpperCase ( Locale . GERMAN )  )  ;", "s    =    removeSequences ( s )  ;", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    ( int   i    =     0  ;    i    <     ( s . length (  )  )  ;    i +  +  )     {", "char   current    =    s . charAt ( i )  ;", "char   next    =     (  ( i    +     1  )     <     ( s . length (  )  )  )     ?    s . charAt (  ( i    +     1  )  )     :     '  _  '  ;", "char   prev    =     ( i    >     0  )     ?    s . charAt (  ( i    -     1  )  )     :     '  _  '  ;", "switch    ( current )     {", "case    ' A '     :", "case    ' E '     :", "case    ' I '     :", "case    ' J '     :", "case    ' Y '     :", "case    ' O '     :", "case    ' U '     :", "if    (  ( i    =  =     0  )     |  |     (  ( i    =  =     1  )     &  &     ( prev    =  =     ' H '  )  )  )     {", "sb . append ( getCode (  )  )  ;", "}", "break ;", "case    ' P '     :", "sb . append (  ( next    =  =     ' H '     ?     \"  3  3  \"     :     '  1  '  )  )  ;", "break ;", "case    ' B '     :", "sb . append (  '  1  '  )  ;", "break ;", "case    ' D '     :", "case    ' T '     :", "sb . append (  ( csz . contains ( next )     ?     '  8  '     :     '  2  '  )  )  ;", "break ;", "case    ' F '     :", "case    ' V '     :", "case    ' W '     :", "sb . append (  '  3  '  )  ;", "break ;", "case    ' G '     :", "case    ' K '     :", "case    ' Q '     :", "sb . append (  '  4  '  )  ;", "break ;", "case    ' C '     :", "if    ( i    =  =     0  )     {", "sb . append (  ( ahkloqrux . contains ( next )     ?     '  4  '     :     '  8  '  )  )  ;", "} else    {", "sb . append (  ( aouhkxq . contains ( next )     ?     '  4  '     :     '  8  '  )  )  ;", "}", "if    (  (  ( sb . length (  )  )     >  =     2  )     &  &     (  ( sb . charAt (  (  ( sb . length (  )  )     -     2  )  )  )     =  =     '  8  '  )  )     {", "sb . setCharAt (  (  ( sb . length (  )  )     -     1  )  ,     '  8  '  )  ;", "}", "break ;", "case    ' X '     :", "sb . append (  (  ( i    <     1  )     |  |     (  !  ( ckq . contains ( prev )  )  )     ?     \"  4  8  \"     :     '  8  '  )  )  ;", "break ;", "case    ' L '     :", "sb . append (  '  5  '  )  ;", "break ;", "case    ' M '     :", "case    ' N '     :", "sb . append (  '  6  '  )  ;", "break ;", "case    ' R '     :", "sb . append (  '  7  '  )  ;", "break ;", "case    ' S '     :", "case    ' Z '     :", "sb . append (  '  8  '  )  ;", "break ;", "case    ' H '     :", "break ;", "}", "}", "s    =    sb . toString (  )  ;", "s    =    removeSequences ( s )  ;", "return   s ;", "}", "METHOD_END"], "methodName": ["substitute"], "fileName": "org.elasticsearch.index.analysis.phonetic.KoelnerPhonetik"}, {"methodBody": ["METHOD_START", "{", "if    (  ( str    =  =    null )     |  |     (  ( str . length (  )  )     =  =     0  )  )     {", "return   str ;", "}", "int   len    =    str . length (  )  ;", "char [  ]    chars    =    new   char [ len ]  ;", "int   count    =     0  ;", "for    ( int   i    =     0  ;    i    <    len ;    i +  +  )     {", "if    ( Character . isLetter ( str . charAt ( i )  )  )     {", "chars [  ( count +  +  )  ]     =    str . charAt ( i )  ;", "}", "}", "if    ( count    =  =    len )     {", "return   str . toUpperCase ( Locale . ENGLISH )  ;", "}", "return   new   String ( chars ,     0  ,    count )  . toUpperCase ( Locale . ENGLISH )  ;", "}", "METHOD_END"], "methodName": ["clean"], "fileName": "org.elasticsearch.index.analysis.phonetic.Nysiis"}, {"methodBody": ["METHOD_START", "{", "return   this . strict ;", "}", "METHOD_END"], "methodName": ["isStrict"], "fileName": "org.elasticsearch.index.analysis.phonetic.Nysiis"}, {"methodBody": ["METHOD_START", "{", "return    (  (  (  ( c    =  =     ' A '  )     |  |     ( c    =  =     ' E '  )  )     |  |     ( c    =  =     ' I '  )  )     |  |     ( c    =  =     ' O '  )  )     |  |     ( c    =  =     ' U '  )  ;", "}", "METHOD_END"], "methodName": ["isVowel"], "fileName": "org.elasticsearch.index.analysis.phonetic.Nysiis"}, {"methodBody": ["METHOD_START", "{", "if    ( str    =  =    null )     {", "return   null ;", "}", "str    =     . clean ( str )  ;", "if    (  ( str . length (  )  )     =  =     0  )     {", "return   str ;", "}", "str    =     . PAT _ MAC . matcher ( str )  . replaceFirst (  \" MCC \"  )  ;", "str    =     . PAT _ KN . matcher ( str )  . replaceFirst (  \" NN \"  )  ;", "str    =     . PAT _ K . matcher ( str )  . replaceFirst (  \" C \"  )  ;", "str    =     . PAT _ PH _ PF . matcher ( str )  . replaceFirst (  \" FF \"  )  ;", "str    =     . PAT _ SCH . matcher ( str )  . replaceFirst (  \" SSS \"  )  ;", "str    =     . PAT _ EE _ IE . matcher ( str )  . replaceFirst (  \" Y \"  )  ;", "str    =     . PAT _ DT _ ETC . matcher ( str )  . replaceFirst (  \" D \"  )  ;", "StringBuilder   key    =    new   StringBuilder ( str . length (  )  )  ;", "key . append ( str . charAt (  0  )  )  ;", "final   char [  ]    chars    =    str . toCharArray (  )  ;", "final   int   len    =    chars . length ;", "for    ( int   i    =     1  ;    i    <    len ;    i +  +  )     {", "final   char   next    =     ( i    <     ( len    -     1  )  )     ?    chars [  ( i    +     1  )  ]     :     . SPACE ;", "final   char   aNext    =     ( i    <     ( len    -     2  )  )     ?    chars [  ( i    +     2  )  ]     :     . SPACE ;", "final   char [  ]    transcoded    =     . transcodeRemaining ( chars [  ( i    -     1  )  ]  ,    chars [ i ]  ,    next ,    aNext )  ;", "System . arraycopy ( transcoded ,     0  ,    chars ,    i ,    transcoded . length )  ;", "if    (  ( chars [ i ]  )     !  =     ( chars [  ( i    -     1  )  ]  )  )     {", "key . append ( chars [ i ]  )  ;", "}", "}", "if    (  ( key . length (  )  )     >     1  )     {", "char   lastChar    =    key . charAt (  (  ( key . length (  )  )     -     1  )  )  ;", "if    ( lastChar    =  =     ' S '  )     {", "key . deleteCharAt (  (  ( key . length (  )  )     -     1  )  )  ;", "lastChar    =    key . charAt (  (  ( key . length (  )  )     -     1  )  )  ;", "}", "if    (  ( key . length (  )  )     >     2  )     {", "final   char   last 2 Char    =    key . charAt (  (  ( key . length (  )  )     -     2  )  )  ;", "if    (  ( last 2 Char    =  =     ' A '  )     &  &     ( lastChar    =  =     ' Y '  )  )     {", "key . deleteCharAt (  (  ( key . length (  )  )     -     2  )  )  ;", "}", "}", "if    ( lastChar    =  =     ' A '  )     {", "key . deleteCharAt (  (  ( key . length (  )  )     -     1  )  )  ;", "}", "}", "final   String   string    =    key . toString (  )  ;", "return   this . isStrict (  )     ?    string . substring (  0  ,    Math . min (  . TRUE _ LENGTH ,    string . length (  )  )  )     :    string ;", "}", "METHOD_END"], "methodName": ["nysiis"], "fileName": "org.elasticsearch.index.analysis.phonetic.Nysiis"}, {"methodBody": ["METHOD_START", "{", "if    (  ( curr    =  =     ' E '  )     &  &     ( next    =  =     ' V '  )  )     {", "return    . CHARS _ AF ;", "}", "if    (  . isVowel ( curr )  )     {", "return    . CHARS _ A ;", "}", "if    ( curr    =  =     ' Q '  )     {", "return    . CHARS _ G ;", "} else", "if    ( curr    =  =     ' Z '  )     {", "return    . CHARS _ S ;", "} else", "if    ( curr    =  =     ' M '  )     {", "return    . CHARS _ N ;", "}", "if    ( curr    =  =     ' K '  )     {", "if    ( next    =  =     ' N '  )     {", "return    . CHARS _ NN ;", "} else    {", "return    . CHARS _ C ;", "}", "}", "if    (  (  ( curr    =  =     ' S '  )     &  &     ( next    =  =     ' C '  )  )     &  &     ( aNext    =  =     ' H '  )  )     {", "return    . CHARS _ SSS ;", "}", "if    (  ( curr    =  =     ' P '  )     &  &     ( next    =  =     ' H '  )  )     {", "return    . CHARS _ FF ;", "}", "if    (  ( curr    =  =     ' H '  )     &  &     (  (  !  (  . isVowel ( prev )  )  )     |  |     (  !  (  . isVowel ( next )  )  )  )  )     {", "return   new   char [  ]  {    prev    }  ;", "}", "if    (  ( curr    =  =     ' W '  )     &  &     (  . isVowel ( prev )  )  )     {", "return   new   char [  ]  {    prev    }  ;", "}", "return   new   char [  ]  {    curr    }  ;", "}", "METHOD_END"], "methodName": ["transcodeRemaining"], "fileName": "org.elasticsearch.index.analysis.phonetic.Nysiis"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   ft    =    createDefaultFieldType (  )  ;", "ft . setName (  \" field \"  )  ;", "ft . setIndexOptions ( DOCS )  ;", "expectThrows ( UnsupportedOperationException . class ,     (  )     -  >    ft . fuzzyQuery (  \" foo \"  ,    Fuzziness . fromEdits (  2  )  ,     1  ,     5  0  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["testFuzzyQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "ICUCollationKeywordFieldMapper . CollationFieldType   ft    =    new   ICUCollationKeywordFieldMapper . CollationFieldType (  )  ;", "assertEquals ( INTERSECTS ,    ft . isFieldWithinQuery ( null ,    RandomStrings . randomAsciiOfLengthBetween ( random (  )  ,     0  ,     5  )  ,    RandomStrings . randomAsciiOfLengthBetween ( random (  )  ,     0  ,     5  )  ,    randomBoolean (  )  ,    randomBoolean (  )  ,    null ,    null ,    null )  )  ;", "}", "METHOD_END"], "methodName": ["testIsFieldWithinQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   ft    =    createDefaultFieldType (  )  ;", "ft . setName (  \" field \"  )  ;", "ft . setIndexOptions ( DOCS )  ;", "expectThrows ( UnsupportedOperationException . class ,     (  )     -  >    ft . prefixQuery (  \" prefix \"  ,    null ,    null )  )  ;", "}", "METHOD_END"], "methodName": ["testPrefixQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   ft    =    createDefaultFieldType (  )  ;", "ft . setName (  \" field \"  )  ;", "ft . setIndexOptions ( DOCS )  ;", "Collator   collator    =    Collator . getInstance ( ROOT )  . freeze (  )  ;", "(  ( ICUCollationKeywordFieldM )     ( ft )  )  . setCollator ( collator )  ;", "RawCollationKey   aKey    =    collator . getRawCollationKey (  \" a \"  ,    null )  ;", "RawCollationKey   bKey    =    collator . getRawCollationKey (  \" b \"  ,    null )  ;", "TermRangeQuery   expected    =    new   TermRangeQuery (  \" field \"  ,    new   BytesRef ( aKey . bytes ,     0  ,    aKey . size )  ,    new   BytesRef ( bKey . bytes ,     0  ,    bKey . size )  ,    false ,    false )  ;", "assertEquals ( expected ,    ft . rangeQuery (  \" a \"  ,     \" b \"  ,    false ,    false ,    null ,    null ,    null ,    null )  )  ;", "ft . setIndexOptions ( NONE )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    ft . rangeQuery (  \" a \"  ,     \" b \"  ,    false ,    false ,    null ,    null ,    null ,    null )  )  ;", "assertEquals (  \" Cannot   search   on   field    [ field ]    since   it   is   not   indexed .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRangeQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   ft    =    createDefaultFieldType (  )  ;", "ft . setName (  \" field \"  )  ;", "ft . setIndexOptions ( DOCS )  ;", "expectThrows ( UnsupportedOperationException . class ,     (  )     -  >    ft . regexpQuery (  \" foo .  *  \"  ,     0  ,     1  0  ,    null ,    null )  )  ;", "}", "METHOD_END"], "methodName": ["testRegexpQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   ft    =    createDefaultFieldType (  )  ;", "ft . setName (  \" field \"  )  ;", "ft . setIndexOptions ( DOCS )  ;", "Collator   collator    =    Collator . getInstance ( new   ULocale (  \" tr \"  )  )  ;", "collator . setStrength ( PRIMARY )  ;", "collator . freeze (  )  ;", "(  ( ICUCollationKeywordFieldM )     ( ft )  )  . setCollator ( collator )  ;", "RawCollationKey   key    =    collator . getRawCollationKey (  \"  ?    will   use   turkish   cas ? ng \"  ,    null )  ;", "BytesRef   expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "assertEquals ( new   TermQuery ( new   Term (  \" field \"  ,    expected )  )  ,    ft . termQuery (  \" I   W ? LL   USE   TURK ? SH   CASING \"  ,    null )  )  ;", "ft . setIndexOptions ( NONE )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    ft . termQuery (  \" bar \"  ,    null )  )  ;", "assertEquals (  \" Cannot   search   on   field    [ field ]    since   it   is   not   indexed .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testTermQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   ft    =    createDefaultFieldType (  )  ;", "ft . setName (  \" field \"  )  ;", "ft . setIndexOptions ( DOCS )  ;", "Collator   collator    =    Collator . getInstance ( ROOT )  . freeze (  )  ;", "(  ( ICUCollationKeywordFieldM )     ( ft )  )  . setCollator ( collator )  ;", "RawCollationKey   fooKey    =    collator . getRawCollationKey (  \" foo \"  ,    null )  ;", "RawCollationKey   barKey    =    collator . getRawCollationKey (  \" bar \"  ,    null )  ;", "List < BytesRef >    terms    =    new   ArrayList <  >  (  )  ;", "terms . add ( new   BytesRef ( fooKey . bytes ,     0  ,    fooKey . size )  )  ;", "terms . add ( new   BytesRef ( barKey . bytes ,     0  ,    barKey . size )  )  ;", "assertEquals ( new   TermInSetQuery (  \" field \"  ,    terms )  ,    ft . termsQuery ( Arrays . asList (  \" foo \"  ,     \" bar \"  )  ,    null )  )  ;", "ft . setIndexOptions ( NONE )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    ft . termsQuery ( Arrays . asList (  \" foo \"  ,     \" bar \"  )  ,    null )  )  ;", "assertEquals (  \" Cannot   search   on   field    [ field ]    since   it   is   not   indexed .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testTermsQuery"], "fileName": "org.elasticsearch.index.mapper.CollationFieldTypeTests"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "String [  ]    equilavent    =    new   String [  ]  {     \" I   W ? LL   USE   TURK ? SH   CASING \"  ,     \"  ?    will   use   turkish   cas ? ng \"     }  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" tr \"  )  . field (  \" strength \"  ,     \" primary \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  0  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  1  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     ( randomBoolean (  )     ?    equilavent [  0  ]     :    equilavent [  1  ]  )  )  )  . sort (  \" collate \"  )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testBasicUsage"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "RuleBasedCollator   baseCollator    =     (  ( RuleBasedCollator )     ( Collator . getInstance ( new   ULocale (  \" de _ DE \"  )  )  )  )  ;", "String   DIN 5  0  0  7  _  2  _ tailorings    =     \"  &    ae    ,    a \\ u 0  3  0  8     &    AE    ,    A \\ u 0  3  0  8  \"     +     (  \"  &    oe    ,    o \\ u 0  3  0  8     &    OE    ,    O \\ u 0  3  0  8  \"     +     \"  &    ue    ,    u \\ u 0  3  0  8     &    UE    ,    u \\ u 0  3  0  8  \"  )  ;", "RuleBasedCollator   tailoredCollator    =    new   RuleBasedCollator (  (  ( baseCollator . getRules (  )  )     +    DIN 5  0  0  7  _  2  _ tailorings )  )  ;", "String   tailoredRules    =    tailoredCollator . getRules (  )  ;", "String [  ]    equilavent    =    new   String [  ]  {     \" T ? ne \"  ,     \" Toene \"     }  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" rules \"  ,    tailoredRules )  . field (  \" strength \"  ,     \" primary \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  0  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  1  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     ( randomBoolean (  )     ?    equilavent [  0  ]     :    equilavent [  1  ]  )  )  )  . sort (  \" collate \"  ,    ASC )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testCustomRules"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . field (  \" strength \"  ,     \" primary \"  )  . field (  \" case _ level \"  ,    true )  . field (  \" index \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" r \\ u 0  0 e 9 sum \\ u 0  0 e 9  \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" Resume \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  3  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" resume \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  4  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" R \\ u 0  0 e 9 sum \\ u 0  0 e 9  \\  \"  }  \"  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . sort (  \" collate \"  ,    ASC )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     4 L )  ;", "assertOrderedSearchHits ( response ,     \"  3  \"  ,     \"  1  \"  ,     \"  4  \"  ,     \"  2  \"  )  ;", "}", "METHOD_END"], "methodName": ["testIgnoreAccentsButNotCase"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "String [  ]    equilavent    =    new   String [  ]  {     \" foo - bar \"  ,     \" foo   bar \"     }  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . field (  \" strength \"  ,     \" primary \"  )  . field (  \" alternate \"  ,     \" shifted \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  0  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  1  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     ( randomBoolean (  )     ?    equilavent [  0  ]     :    equilavent [  1  ]  )  )  )  . sort (  \" collate \"  )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testIgnorePunctuation"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . field (  \" strength \"  ,     \" primary \"  )  . field (  \" alternate \"  ,     \" shifted \"  )  . field (  \" variable _ top \"  ,     \"     \"  )  . field (  \" index \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" foo   bar \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" foobar \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  3  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" foo - bar \\  \"  }  \"  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . sort (  \" collate \"  ,    ASC )  . sort (  \"  _ id \"  ,    ASC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     3 L )  ;", "assertOrderedSearchHits ( response ,     \"  3  \"  ,     \"  1  \"  ,     \"  2  \"  )  ;", "}", "METHOD_END"], "methodName": ["testIgnoreWhitespace"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "String [  ]    equilavent    =    new   String [  ]  {     \" a \"  ,     \" C \"  ,     \" a \"  ,     \" B \"     }  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  (  (  (  (  \"  {  \\  \" collate \\  \"  :  [  \\  \"  \"     +     ( equilavent [  0  ]  )  )     +     \"  \\  \"  ,     \\  \"  \"  )     +     ( equilavent [  1  ]  )  )     +     \"  \\  \"  ]  }  \"  )  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  2  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     \" a \"  )  )  . sort ( SortBuilders . fieldSort (  \" collate \"  )  . sortMode ( MAX )  . order ( DESC )  )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  1  \"  ,     \"  2  \"  )  ;", "request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     \" a \"  )  )  . sort ( SortBuilders . fieldSort (  \" collate \"  )  . sortMode ( MIN )  . order ( DESC )  )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleValues"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "String [  ]    equilavent    =    new   String [  ]  {     \" I   WI \\ u 0  3  0  7 LL   USE   TURK \\ u 0  1  3  0 SH   CASING \"  ,     \"  ?    will   use   turkish   cas ? ng \"     }  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" tr \"  )  . field (  \" strength \"  ,     \" primary \"  )  . field (  \" decomposition \"  ,     \" canonical \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  0  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  1  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     ( randomBoolean (  )     ?    equilavent [  0  ]     :    equilavent [  1  ]  )  )  )  . sort (  \" collate \"  )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNormalization"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . field (  \" numeric \"  ,    true )  . field (  \" index \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" foobar -  1  0  \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" foobar -  9  \\  \"  }  \"  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . sort (  \" collate \"  ,    ASC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNumerics"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "String [  ]    equilavent    =    new   String [  ]  {     \" TESTING \"  ,     \" testing \"     }  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . field (  \" strength \"  ,     \" secondary \"  )  . field (  \" decomposition \"  ,     \" no \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  0  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  (  (  \"  {  \\  \" collate \\  \"  :  \\  \"  \"     +     ( equilavent [  1  ]  )  )     +     \"  \\  \"  }  \"  )  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . query ( QueryBuilders . termQuery (  \" collate \"  ,     ( randomBoolean (  )     ?    equilavent [  0  ]     :    equilavent [  1  ]  )  )  )  . sort (  \" collate \"  )  . sort (  \"  _ id \"  ,    DESC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testSecondaryStrength"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" collate \"  )  . field (  \" type \"  ,     \" icu _ c _ keyword \"  )  . field (  \" language \"  ,     \" en \"  )  . field (  \" strength \"  ,     \" tertiary \"  )  . field (  \" case _ first \"  ,     \" upper \"  )  . field (  \" index \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "indexRandom ( true ,    client (  )  . prepareIndex ( index ,    type ,     \"  1  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" resume \\  \"  }  \"  ,    JSON )  ,    client (  )  . prepareIndex ( index ,    type ,     \"  2  \"  )  . setSource (  \"  {  \\  \" collate \\  \"  :  \\  \" Resume \\  \"  }  \"  ,    JSON )  )  ;", "SearchRequest   request    =    new   SearchRequest (  )  . indices ( index )  . types ( type )  . source ( new   SearchSourceBuilder (  )  . fetchSource ( false )  . sort (  \" collate \"  ,    ASC )  )  ;", "SearchResponse   response    =    client (  )  . search ( request )  . actionGet (  )  ;", "assertNoFailures ( response )  ;", "assertHitCount ( response ,     2 L )  ;", "assertOrderedSearchHits ( response ,     \"  2  \"  ,     \"  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testUpperCaseFirst"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperIT"}, {"methodBody": ["METHOD_START", "{", "indexService    =    createIndex (  \" test \"  )  ;", "parser    =    indexServiceService (  )  . documentMapperParser (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "indexService    =    createIndex (  \" oldindex \"  ,    Settings . builder (  )  . put (  \" index . version . created \"  ,    V _  5  _  5  _  0  )  . build (  )  )  ;", "parser    =    indexService . mapperService (  )  . documentMapperParser (  )  ;", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,     . FIELD _ TYPE )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" oldindex \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "Collator   collator    =    Collator . getInstance ( ROOT )  ;", "RawCollationKey   key    =    collator . getRawCollationKey (  \"  1  2  3  4  \"  ,    null )  ;", "BytesRef   expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "assertEquals ( expected ,    fields [  0  ]  . binaryValue (  )  )  ;", "IndexableFieldType   fieldType    =    fields [  0  ]  . fieldType (  )  ;", "assertThat ( fieldType . omitNorms (  )  ,    equalTo ( true )  )  ;", "assertFalse ( fieldType . tokenized (  )  )  ;", "assertFalse ( fieldType . stored (  )  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( DOCS )  )  ;", "assertThat ( fieldType . storeTermVectors (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorOffsets (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPositions (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPayloads (  )  ,    equalTo ( false )  )  ;", "assertEquals ( NONE ,    fieldType . docValuesType (  )  )  ;", "assertEquals ( expected ,    fields [  1  ]  . binaryValue (  )  )  ;", "fieldType    =    fields [  1  ]  . fieldType (  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( IndexOptions . NONE )  )  ;", "assertEquals ( SORTED ,    fieldType . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBackCompat"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" language \"  ,     \" tr \"  )  . field (  \" strength \"  ,     \" primary \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \" I   W ? LL   USE   TURK ? SH   CASING \"  )  . endObject (  )  )  ,    JSON )  )  ;", "Collator   collator    =    Collator . getInstance ( new   ULocale (  \" tr \"  )  )  ;", "collator . setStrength ( PRIMARY )  ;", "RawCollationKey   key    =    collator . getRawCollationKey (  \"  ?    will   use   turkish   cas ? ng \"  ,    null )  ;", "BytesRef   expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "assertEquals ( expected ,    fields [  0  ]  . binaryValue (  )  )  ;", "IndexableFieldType   fieldType    =    fields [  0  ]  . fieldType (  )  ;", "assertThat ( fieldType . omitNorms (  )  ,    equalTo ( true )  )  ;", "assertFalse ( fieldType . tokenized (  )  )  ;", "assertFalse ( fieldType . stored (  )  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( DOCS )  )  ;", "assertThat ( fieldType . storeTermVectors (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorOffsets (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPositions (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPayloads (  )  ,    equalTo ( false )  )  ;", "assertEquals ( NONE ,    fieldType . docValuesType (  )  )  ;", "assertEquals ( expected ,    fields [  1  ]  . binaryValue (  )  )  ;", "fieldType    =    fields [  1  ]  . fieldType (  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( IndexOptions . NONE )  )  ;", "assertEquals ( SORTED _ SET ,    fieldType . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCollator"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "Collator   collator    =    Collator . getInstance ( ROOT )  ;", "RawCollationKey   key    =    collator . getRawCollationKey (  \"  1  2  3  4  \"  ,    null )  ;", "BytesRef   expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "assertEquals ( expected ,    fields [  0  ]  . binaryValue (  )  )  ;", "IndexableFieldType   fieldType    =    fields [  0  ]  . fieldType (  )  ;", "assertThat ( fieldType . omitNorms (  )  ,    equalTo ( true )  )  ;", "assertFalse ( fieldType . tokenized (  )  )  ;", "assertFalse ( fieldType . stored (  )  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( DOCS )  )  ;", "assertThat ( fieldType . storeTermVectors (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorOffsets (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPositions (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPayloads (  )  ,    equalTo ( false )  )  ;", "assertEquals ( NONE ,    fieldType . docValuesType (  )  )  ;", "assertEquals ( expected ,    fields [  1  ]  . binaryValue (  )  )  ;", "fieldType    =    fields [  1  ]  . fieldType (  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( IndexOptions . NONE )  )  ;", "assertEquals ( SORTED _ SET ,    fieldType . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaults"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" doc _ values \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  1  ,    fields . length )  ;", "assertEquals ( NONE ,    fields [  0  ]  . fieldType (  )  . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDisableDocValues"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" index \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  1  ,    fields . length )  ;", "assertEquals ( NONE ,    fields [  0  ]  . fieldType (  )  . indexOptions (  )  )  ;", "assertEquals ( SORTED _ SET ,    fields [  0  ]  . fieldType (  )  . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDisableIndex"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" norms \"  ,    true )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "assertFalse ( fields [  0  ]  . fieldType (  )  . omitNorms (  )  )  ;", "}", "METHOD_END"], "methodName": ["testEnableNorms"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" store \"  ,    true )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "assertTrue ( fields [  0  ]  . fieldType (  )  . stored (  )  )  ;", "}", "METHOD_END"], "methodName": ["testEnableStore"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" index _ options \"  ,     \" freqs \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "assertEquals ( DOCS _ AND _ FREQS ,    fields [  0  ]  . fieldType (  )  . indexOptions (  )  )  ;", "for    ( String   indexOptions    :    Arrays . asList (  \" positions \"  ,     \" offsets \"  )  )     {", "final   String   mapping 2     =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" index _ options \"  ,    indexOptions )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping 2  )  )  )  ;", "assertEquals (  (  (  (  \" The    [  \"     +     ( ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  )     +     \"  ]    field   does   not   support   positions ,    got    [ index _ options ]  =  \"  )     +    indexOptions )  ,    e . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testIndexOptions"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,    Arrays . asList (  \"  1  2  3  4  \"  ,     \"  5  6  7  8  \"  )  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  4  ,    fields . length )  ;", "Collator   collator    =    Collator . getInstance ( ROOT )  ;", "RawCollationKey   key    =    collator . getRawCollationKey (  \"  1  2  3  4  \"  ,    null )  ;", "BytesRef   expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "assertEquals ( expected ,    fields [  0  ]  . binaryValue (  )  )  ;", "IndexableFieldType   fieldType    =    fields [  0  ]  . fieldType (  )  ;", "assertThat ( fieldType . omitNorms (  )  ,    equalTo ( true )  )  ;", "assertFalse ( fieldType . tokenized (  )  )  ;", "assertFalse ( fieldType . stored (  )  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( DOCS )  )  ;", "assertThat ( fieldType . storeTermVectors (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorOffsets (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPositions (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPayloads (  )  ,    equalTo ( false )  )  ;", "assertEquals ( NONE ,    fieldType . docValuesType (  )  )  ;", "assertEquals ( expected ,    fields [  1  ]  . binaryValue (  )  )  ;", "fieldType    =    fields [  1  ]  . fieldType (  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( IndexOptions . NONE )  )  ;", "assertEquals ( SORTED _ SET ,    fieldType . docValuesType (  )  )  ;", "collator    =    Collator . getInstance ( ROOT )  ;", "key    =    collator . getRawCollationKey (  \"  5  6  7  8  \"  ,    null )  ;", "expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "assertEquals ( expected ,    fields [  2  ]  . binaryValue (  )  )  ;", "fieldType    =    fields [  2  ]  . fieldType (  )  ;", "assertThat ( fieldType . omitNorms (  )  ,    equalTo ( true )  )  ;", "assertFalse ( fieldType . tokenized (  )  )  ;", "assertFalse ( fieldType . stored (  )  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( DOCS )  )  ;", "assertThat ( fieldType . storeTermVectors (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorOffsets (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPositions (  )  ,    equalTo ( false )  )  ;", "assertThat ( fieldType . storeTermVectorPayloads (  )  ,    equalTo ( false )  )  ;", "assertEquals ( NONE ,    fieldType . docValuesType (  )  )  ;", "assertEquals ( expected ,    fields [  3  ]  . binaryValue (  )  )  ;", "fieldType    =    fields [  3  ]  . fieldType (  )  ;", "assertThat ( fieldType . indexOptions (  )  ,    equalTo ( IndexOptions . NONE )  )  ;", "assertEquals ( SORTED _ SET ,    fieldType . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleValues"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "ParsedDocument   doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . nullField (  \" field \"  )  . endObject (  )  )  ,    JSON )  )  ;", "assertArrayEquals ( new   IndexableField [  0  ]  ,    doc . rootDoc (  )  . getFields (  \" field \"  )  )  ;", "mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" null _ value \"  ,     \"  1  2  3  4  \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "assertEquals ( mapping ,    mapper . mappingSource (  )  . toString (  )  )  ;", "doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  0  ,    fields . length )  ;", "doc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . nullField (  \" field \"  )  . endObject (  )  )  ,    JSON )  )  ;", "Collator   collator    =    Collator . getInstance ( ROOT )  ;", "RawCollationKey   key    =    collator . getRawCollationKey (  \"  1  2  3  4  \"  ,    null )  ;", "BytesRef   expected    =    new   BytesRef ( key . bytes ,     0  ,    key . size )  ;", "fields    =    doc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertEquals (  2  ,    fields . length )  ;", "assertEquals ( expected ,    fields [  0  ]  . binaryValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNullValue"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" language \"  ,     \" tr \"  )  . field (  \" strength \"  ,     \" primary \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "indexService . mapperService (  )  . merge (  \" type \"  ,    new   CompressedXContent ( mapping )  ,    MAPPING _ UPDATE )  ;", "String   mapping 2     =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,    ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  . field (  \" language \"  ,     \" en \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    indexService . mapperService (  )  . merge (  \" type \"  ,    new   CompressedXContent ( mapping 2  )  ,    MergeReason . MAPPING _ UPDATE )  )  ;", "assertEquals (  (  (  (  (  \" Can ' t   merge   because   of   conflicts :     [ Cannot   update   language   setting   for    [  \"     +     ( ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  )     +     \"  ]  ,    Cannot   update   strength   setting   for    [  \"  )     +     ( ICUCollationKeywordFieldMapperTests . FIELD _ TYPE )  )     +     \"  ]  ]  \"  )  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUpdateCollator"], "fileName": "org.elasticsearch.index.mapper.ICUCollationKeywordFieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "return   createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.mapper.murmur3.MapperMurmur3ClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "indexService    =    createIndex (  \" test \"  )  ;", "mapperRegistry    =    new   MapperRegistry ( Collections . singletonMap ( Murmur 3 FieldMapper . CONTENT _ TYPE ,    new   Murmur 3 FieldMapper . TypeParser (  )  )  ,    Collections . emptyMap (  )  ,    MapperPlugin . NOOP _ FIELD _ FILTER )  ;", "Supplier < QueryShardContext >    queryShardContext    =     (  )     -  >     {", "return   indexService . newQueryShardContext (  0  ,    null ,     (  )     -  >     {", "throw   new   UnsupportedOperationException (  )  ;", "}  ,    null )  ;", "}  ;", "parser    =    new   DocumentMapperParser ( indexService . getIndexSettings (  )  ,    indexService . mapperService (  )  ,    indexService . getIndexAnalyzers (  )  ,    indexService . xContentRegistry (  )  ,    indexService . similarityService (  )  ,    mapperRegistry ,    queryShardContext )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.elasticsearch.index.mapper.murmur3.Murmur3FieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,     \" murmur 3  \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "DocumentMapper   mapper    =    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "ParsedDocument   parsedDoc    =    mapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    BytesReference . bytes ( jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \" value \"  )  . endObject (  )  )  ,    JSON )  )  ;", "IndexableField [  ]    fields    =    parsedDoc . rootDoc (  )  . getFields (  \" field \"  )  ;", "assertNotNull ( fields )  ;", "assertEquals ( Arrays . toString ( fields )  ,     1  ,    fields . length )  ;", "IndexableField   field    =    fields [  0  ]  ;", "assertEquals ( NONE ,    field . fieldType (  )  . indexOptions (  )  )  ;", "assertEquals ( SORTED _ NUMERIC ,    field . fieldType (  )  . docValuesType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaults"], "fileName": "org.elasticsearch.index.mapper.murmur3.Murmur3FieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,     \" murmur 3  \"  )  . field (  \" doc _ values \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "try    {", "parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "fail (  \" expected   a   mapper   parsing   exception \"  )  ;", "}    catch    ( MapperParsingException   e )     {", "assertTrue ( e . getMessage (  )  . contains (  \" Setting    [ doc _ values ]    cannot   be   modified \"  )  )  ;", "}", "mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,     \" murmur 3  \"  )  . field (  \" doc _ values \"  ,    true )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "try    {", "parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "fail (  \" expected   a   mapper   parsing   exception \"  )  ;", "}    catch    ( MapperParsingException   e )     {", "assertTrue ( e . getMessage (  )  . contains (  \" Setting    [ doc _ values ]    cannot   be   modified \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDocValuesSettingNotAllowed"], "fileName": "org.elasticsearch.index.mapper.murmur3.Murmur3FieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \"  \"  )  . field (  \" type \"  ,     \" murmur 3  \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \" name   cannot   be   empty   string \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyName"], "fileName": "org.elasticsearch.index.mapper.murmur3.Murmur3FieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "String   mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,     \" murmur 3  \"  )  . field (  \" index \"  ,     \" not _ analyzed \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "try    {", "parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "fail (  \" expected   a   mapper   parsing   exception \"  )  ;", "}    catch    ( MapperParsingException   e )     {", "assertTrue ( e . getMessage (  )  . contains (  \" Setting    [ index ]    cannot   be   modified \"  )  )  ;", "}", "mapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \" properties \"  )  . startObject (  \" field \"  )  . field (  \" type \"  ,     \" murmur 3  \"  )  . field (  \" index \"  ,     \" no \"  )  . endObject (  )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "try    {", "parser . parse (  \" type \"  ,    new   CompressedXContent ( mapping )  )  ;", "fail (  \" expected   a   mapper   parsing   exception \"  )  ;", "}    catch    ( MapperParsingException   e )     {", "assertTrue ( e . getMessage (  )  . contains (  \" Setting    [ index ]    cannot   be   modified \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testIndexSettingNotAllowed"], "fileName": "org.elasticsearch.index.mapper.murmur3.Murmur3FieldMapperTests"}, {"methodBody": ["METHOD_START", "{", "return   createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.mapper.size.MapperSizeClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "MappedFieldType   defaultFieldType    =    SizeFieldMapper . Defaults . SIZE _ FIELD _ TYPE . clone (  )  ;", "defaultFieldType . setHasDocValues ( true )  ;", "return   defaultFieldType ;", "}", "METHOD_END"], "methodName": ["defaultFieldType"], "fileName": "org.elasticsearch.index.mapper.size.SizeFieldMapper"}, {"methodBody": ["METHOD_START", "{", "return   this . enabledState . enabled ;", "}", "METHOD_END"], "methodName": ["enabled"], "fileName": "org.elasticsearch.index.mapper.size.SizeFieldMapper"}, {"methodBody": ["METHOD_START", "{", "String   errMsg    =    String . format ( Locale . ROOT ,     (  (  \" Expected   size   field   mapping   to   be    \"     +     ( enabled    ?     \" enabled \"     :     \" disabled \"  )  )     +     \"    for    % s /  % s \"  )  ,    index ,    type )  ;", "GetsResponse   getsResponse    =    client (  )  . admin (  )  . indices (  )  . prepareGets ( index )  . addTypes ( type )  . get (  )  ;", "Map < String ,    Object >    mappingSource    =    getsResponse . gets (  )  . get ( index )  . get ( type )  . getSourceAsMap (  )  ;", "assertThat ( errMsg ,    mappingSource ,    hasKey (  \"  _ size \"  )  )  ;", "String   sizeAsString    =    mappingSource . get (  \"  _ size \"  )  . toString (  )  ;", "assertThat ( sizeAsString ,    is ( notNullValue (  )  )  )  ;", "assertThat ( errMsg ,    sizeAsString ,    is (  (  (  \"  { enabled =  \"     +    enabled )     +     \"  }  \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["assertSizeMappingEnabled"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingIT"}, {"methodBody": ["METHOD_START", "{", "assertAcked ( prepareCreate (  \" test \"  )  . addMapping (  \" type \"  ,     \"  _ size \"  ,     \" enabled = true \"  )  )  ;", "final   String   source    =     \"  {  \\  \" f \\  \"  :  1  0  }  \"  ;", "indexRandom ( true ,    client (  )  . prepareIndex (  \" test \"  ,     \" type \"  ,     \"  1  \"  )  . setSource ( source ,    JSON )  )  ;", "GetResponse   getResponse    =    client (  )  . prepareGet (  \" test \"  ,     \" type \"  ,     \"  1  \"  )  . setStoredFields (  \"  _ size \"  )  . get (  )  ;", "assertNotNull ( getResponse . getField (  \"  _ size \"  )  )  ;", "assertEquals ( source . length (  )  ,     (  ( int )     ( getResponse . getField (  \"  _ size \"  )  . getValue (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testBasic"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \"  _ size \"  )  . field (  \" enabled \"  ,    true )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "assertEnabled ( index ,    type ,    true )  ;", "XContentBuilder   updateMappingBuilder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \"  _ size \"  )  . field (  \" enabled \"  ,    false )  . endObject (  )  . endObject (  )  ;", "PutMappingResponse   putMappingResponse    =    client (  )  . admin (  )  . indices (  )  . preparePutMapping ( index )  . setType ( type )  . setSource ( updateMappingBuilder )  . get (  )  ;", "assertAcked ( putMappingResponse )  ;", "assertEnabled ( index ,    type ,    false )  ;", "}", "METHOD_END"], "methodName": ["testThatSizeCanBeSwitchedOnAndOff"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingIT"}, {"methodBody": ["METHOD_START", "{", "String   index    =     \" foo \"  ;", "String   type    =     \" mytype \"  ;", "XContentBuilder   builder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \"  _ size \"  )  . field (  \" enabled \"  ,    true )  . endObject (  )  . endObject (  )  ;", "assertAcked ( client (  )  . admin (  )  . indices (  )  . prepareCreate ( index )  . addMapping ( type ,    builder )  )  ;", "assertEnabled ( index ,    type ,    true )  ;", "XContentBuilder   updateMappingBuilder    =    jsonBuilder (  )  . startObject (  )  . startObject (  \" properties \"  )  . startObject (  \" otherField \"  )  . field (  \" type \"  ,     \" text \"  )  . endObject (  )  . endObject (  )  . endObject (  )  ;", "PutMappingResponse   putMappingResponse    =    client (  )  . admin (  )  . indices (  )  . preparePutMapping ( index )  . setType ( type )  . setSource ( updateMappingBuilder )  . get (  )  ;", "assertAcked ( putMappingResponse )  ;", "assertEnabled ( index ,    type ,    true )  ;", "}", "METHOD_END"], "methodName": ["testThatUpdatingMappingShouldNotRemoveSizeMappingConfiguration"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingIT"}, {"methodBody": ["METHOD_START", "{", "IndexService   service    =    createIndex (  \" test \"  ,    EMPTY ,     \" type \"  ,     \"  _ size \"  ,     \" enabled = false \"  )  ;", "DocumentMapper   docMapper    =    serviceService (  )  . documentMapper (  \" type \"  )  ;", "BytesReference   source    =    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \" value \"  )  . endObject (  )  )  ;", "ParsedDocument   doc    =    docMapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    source ,    JSON )  )  ;", "assertThat ( doc . rootDoc (  )  . getField (  \"  _ size \"  )  ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSizeDisabled"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingTests"}, {"methodBody": ["METHOD_START", "{", "IndexService   service    =    createIndex (  \" test \"  ,    EMPTY ,     \" type \"  ,     \"  _ size \"  ,     \" enabled = true \"  )  ;", "DocumentMapper   docMapper    =    serviceService (  )  . documentMapper (  \" type \"  )  ;", "BytesReference   source    =    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \" value \"  )  . endObject (  )  )  ;", "ParsedDocument   doc    =    docMapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    source ,    JSON )  )  ;", "boolean   stored    =    false ;", "boolean   points    =    false ;", "for    ( IndexableField   field    :    doc . rootDoc (  )  . getFields (  \"  _ size \"  )  )     {", "stored    |  =    field . fieldType (  )  . stored (  )  ;", "points    |  =     ( field . fieldType (  )  . pointDimensionCount (  )  )     >     0  ;", "}", "assertTrue ( stored )  ;", "assertTrue ( points )  ;", "}", "METHOD_END"], "methodName": ["testSizeEnabled"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingTests"}, {"methodBody": ["METHOD_START", "{", "IndexService   service    =    createIndex (  \" test \"  ,    EMPTY ,     \" type \"  )  ;", "DocumentMapper   docMapper    =    serviceService (  )  . documentMapper (  \" type \"  )  ;", "BytesReference   source    =    BytesReference . bytes ( XContentFactory . jsonBuilder (  )  . startObject (  )  . field (  \" field \"  ,     \" value \"  )  . endObject (  )  )  ;", "ParsedDocument   doc    =    docMapper . parse ( SourceToParse . source (  \" test \"  ,     \" type \"  ,     \"  1  \"  ,    source ,    JSON )  )  ;", "assertThat ( doc . rootDoc (  )  . getField (  \"  _ size \"  )  ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSizeNotSet"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingTests"}, {"methodBody": ["METHOD_START", "{", "IndexService   service    =    createIndex (  \" test \"  ,    EMPTY ,     \" type \"  ,     \"  _ size \"  ,     \" enabled = true \"  )  ;", "DocumentMapper   docMapper    =    serviceService (  )  . documentMapper (  \" type \"  )  ;", "assertThat ( docMapper . metadataMapper ( SizeFieldMapper . class )  . enabled (  )  ,    is ( true )  )  ;", "String   disabledMapping    =    Strings . toString ( XContentFactory . jsonBuilder (  )  . startObject (  )  . startObject (  \" type \"  )  . startObject (  \"  _ size \"  )  . field (  \" enabled \"  ,    false )  . endObject (  )  . endObject (  )  . endObject (  )  )  ;", "docMapper    =    serviceService (  )  . merge (  \" type \"  ,    new   CompressedXContent ( disabledMapping )  ,    MAPPING _ UPDATE )  ;", "assertThat ( docMapper . metadataMapper ( SizeFieldMapper . class )  . enabled (  )  ,    is ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testThatDisablingWorksWhenMerging"], "fileName": "org.elasticsearch.index.mapper.size.SizeMappingTests"}, {"methodBody": ["METHOD_START", "{", "createIndex (  \" test \"  )  ;", "long   nbDocs    =    randomIntBetween (  1  0  ,     1  0  0  0  )  ;", "for    ( long   i    =     0  ;    i    <    nbDocs ;    i +  +  )     {", "(  \" test \"  ,     \" doc \"  ,     (  \"  \"     +    i )  ,     \" foo \"  ,     \" bar \"  )  ;", "}", "refresh (  )  ;", "SearchResponse   response    =    client (  )  . prepareSearch (  \" test \"  )  . get (  )  ;", "assertThat ( response . getHits (  )  . getTotalHits (  )  ,    is ( nbDocs )  )  ;", "}", "METHOD_END"], "methodName": ["testAzureFs"], "fileName": "org.elasticsearch.index.store.AbstractAzureFsTestCase"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.index.store.StoreSmbClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return   field ;", "}", "METHOD_END"], "methodName": ["getField"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   indexedChars ;", "}", "METHOD_END"], "methodName": ["getIndexedChars"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   properties ;", "}", "METHOD_END"], "methodName": ["getProperties"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   targetField ;", "}", "METHOD_END"], "methodName": ["getTargetField"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   ignoreMissing ;", "}", "METHOD_END"], "methodName": ["isIgnoreMissing"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessor"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" attachment \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance (  . Factory . DEFAULT _ PROPERTIES )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildDefaults"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "Set < AttachmentProcessor . Property >    properties    =    EnumSet . noneOf ( AttachmentProcessor . Property . class )  ;", "List < String >    fieldNames    =    new   ArrayList <  >  (  )  ;", "int   numFields    =    scaledRandomIntBetween (  1  ,    AttachmentProcessor . Property . values (  )  . length )  ;", "for    ( int   i    =     0  ;    i    <    numFields ;    i +  +  )     {", "AttachmentProcessor . Property   property    =    AttachmentProcessor . Property . values (  )  [ i ]  ;", "properties . add ( property )  ;", "fieldNames . add ( property . name (  )  . toLowerCase ( Locale . ROOT )  )  ;", "}", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,    fieldNames )  ;", "AttachmentProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    equalTo ( properties )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildFields"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,    Collections . singletonList (  \" invalid \"  )  )  ;", "try    {", "factory . create ( null ,    null ,    config )  ;", "fail (  \" exception   expected \"  )  ;", "}    catch    ( ElasticsearchParseException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \"  [ properties ]    illegal   field   option    [ invalid ]  \"  )  )  ;", "for    (  . Property   property    :     . Property . values (  )  )     {", "assertThat ( e . getMessage (  )  ,    containsString ( property . name (  )  )  )  ;", "}", "}", "config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,     \" invalid \"  )  ;", "try    {", "factory . create ( null ,    null ,    config )  ;", "fail (  \" exception   expected \"  )  ;", "}    catch    ( ElasticsearchParseException   e )     {", "assertThat ( e . getMessage (  )  ,    equalTo (  \"  [ properties ]    property   isn ' t   a   list ,    but   of   type    [ String ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testBuildIllegalFieldOption"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" target _ field \"  ,     \"  _ field \"  )  ;", "processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildTargetField"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "int   indexedChars    =    randomIntBetween (  1  ,     1  0  0  0  0  0  )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" indexed _ chars \"  ,    indexedChars )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getIndexedChars (  )  ,    is ( indexedChars )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConfigureIndexedChars"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" ignore _ missing \"  ,    true )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" attachment \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance (  . Factory . DEFAULT _ PROPERTIES )  )  ;", "assertTrue ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testIgnoreMissing"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "processor    =    new   AttachmentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    EnumSet . allOf ( AttachmentProcessor . Property . class )  ,     1  0  0  0  0  ,    false ,    null )  ;", "}", "METHOD_END"], "methodName": ["createStandardProcessor"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "String   path    =     \"  / org / elasticsearch / ingest / attachment / test / sample - files /  \"     +    filename ;", "try    ( InputStream   is    =     . class . getResourceAsStream ( path )  )     {", "byte [  ]    bytes    =    IOUtils . toByteArray ( is )  ;", "return   Base 6  4  . getEncoder (  )  . encodeToString ( bytes )  ;", "}", "}", "METHOD_END"], "methodName": ["getAsBase64"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "return   parseDocument ( file ,    processor ,    new   HashMap <  >  (  )  )  ;", "}", "METHOD_END"], "methodName": ["parseDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,    getAsBase 6  4  ( file )  )  ;", "document . putAll ( optionalFields )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    Data    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "return   Data ;", "}", "METHOD_END"], "methodName": ["parseDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" asciidoc . asciidoc \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content _ type \"  ,     \" content \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testAsciidocDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" text - empty . txt \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    not ( hasItem (  \" language \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyTextDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "ElasticsearchParseException   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    parseDocument (  \" encrypted . pdf \"  ,    processor )  )  ;", "assertThat ( e . getDetailedMessage (  )  ,    containsString (  \" document   is   encrypted \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testEncryptedPdf"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" text - in - english . txt \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \"  \\  \" God   Save   the   Queen \\  \"     ( alternatively    \\  \" God   Save   the   King \\  \"  \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testEnglishTextDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" testEPUB . epub \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" author \"  ,     \" title \"  ,     \" content _ type \"  ,     \" content _ length \"  ,     \" date \"  ,     \" keywords \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" application / epub + zip \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testEpubDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" text - in - french . txt \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    hasItem (  \" language \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" fr \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testFrenchTextDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" htmlWithEmptyDateMeta . html \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" author \"  ,     \" keywords \"  ,     \" title \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" author \"  )  ,    is (  \" kimchy \"  )  )  ;", "assertThat ( attachmentData . get (  \" keywords \"  )  ,    is (  \"  , cool , bonsai \"  )  )  ;", "assertThat ( attachmentData . get (  \" title \"  )  ,    is (  \" Hello \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / html \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testHtmlDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "ArrayList < AttachmentProcessor . Property >    fieldsList    =    new   ArrayList <  >  ( EnumSet . complementOf ( EnumSet . of ( AttachmentProcessor . Property . DATE )  )  )  ;", "Set < AttachmentProcessor . Property >    selectedProperties    =    new   HashSet <  >  (  )  ;", "int   numFields    =    randomIntBetween (  1  ,    fieldsList . size (  )  )  ;", "String [  ]    selectedFieldNames    =    new   String [ numFields ]  ;", "for    ( int   i    =     0  ;    i    <    numFields ;    i +  +  )     {", "AttachmentProcessor . Property   property ;", "do    {", "property    =    randomFrom ( fieldsList )  ;", "}    while    (  ( selectedProperties . add ( property )  )     =  =    false    )  ;", "selectedFieldNames [ i ]     =    property . toLowerCase (  )  ;", "}", "if    ( randomBoolean (  )  )     {", "selectedProperties . add ( AttachmentProcessor . Property . DATE )  ;", "}", "processor    =    new   AttachmentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    selectedProperties ,     1  0  0  0  0  ,    false ,    null )  ;", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" htmlWithEmptyDateMeta . html \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    hasSize ( selectedFieldNames . length )  )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder ( selectedFieldNames )  )  ;", "}", "METHOD_END"], "methodName": ["testHtmlDocumentWithRandomFields"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "processor    =    new   AttachmentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    EnumSet . allOf ( AttachmentProcessor . Property . class )  ,     1  9  ,    false ,    null )  ;", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" text - in - english . txt \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \"  \\  \" God   Save   the   Queen \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is (  1  9 L )  )  ;", "processor    =    new   AttachmentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    EnumSet . allOf ( AttachmentProcessor . Property . class )  ,     1  9  ,    false ,     \" max _ length \"  )  ;", "attachmentData    =    parseDocument (  \" text - in - english . txt \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \"  \\  \" God   Save   the   Queen \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is (  1  9 L )  )  ;", "attachmentData    =    parseDocument (  \" text - in - english . txt \"  ,    processor ,    Collections . singletonMap (  \" max _ length \"  ,     1  0  )  )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" sk \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \"  \\  \" God   Save \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is (  1  0 L )  )  ;", "attachmentData    =    parseDocument (  \" text - in - english . txt \"  ,    processor ,    Collections . singletonMap (  \" max _ length \"  ,     1  0  0  )  )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \"  \\  \" God   Save   the   Queen \\  \"     ( alternatively    \\  \" God   Save   the   King \\  \"  \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is (  5  6 L )  )  ;", "}", "METHOD_END"], "methodName": ["testIndexedChars"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" issue -  2  2  0  7  7  . doc \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" content \"  ,     \" language \"  ,     \" date \"  ,     \" author \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  . toString (  )  ,    containsString (  \" Table   of   Contents \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" date \"  )  ,    is (  \"  2  0  1  6  -  1  2  -  1  6 T 1  5  :  0  4  :  0  0 Z \"  )  )  ;", "assertThat ( attachmentData . get (  \" author \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    is (  \" application / msword \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testLegacyWordDocumentWithVisioSchema"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . emptyMap (  )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Processor   processor    =    new    ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" randomTarget \"  ,    null ,     1  0  ,    true ,    null )  ;", "processor . execute ( ingestDocument )  ;", "IngestDocumentMatcher . assertIngestDocument ( originalIngestDocument ,    ingestDocument )  ;", "}", "METHOD_END"], "methodName": ["testNonExistentWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . emptyMap (  )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Processor   processor    =    new    ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" randomTarget \"  ,    null ,     1  0  ,    false ,    null )  ;", "Exception   exception    =    expectThrows ( Exception . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( exception . getMessage (  )  ,    equalTo (  \" field    [ source _ field ]    not   present   as   part   of   path    [ source _ field ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNonExistentWithoutIgnoreMissing"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . singletonMap (  \" source _ field \"  ,    null )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Processor   processor    =    new    ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" randomTarget \"  ,    null ,     1  0  ,    true ,    null )  ;", "processor . execute ( ingestDocument )  ;", "IngestDocumentMatcher . assertIngestDocument ( originalIngestDocument ,    ingestDocument )  ;", "}", "METHOD_END"], "methodName": ["testNullValueWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . singletonMap (  \" source _ field \"  ,    null )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Processor   processor    =    new    ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" randomTarget \"  ,    null ,     1  0  ,    false ,    null )  ;", "Exception   exception    =    expectThrows ( Exception . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( exception . getMessage (  )  ,    equalTo (  \" field    [ source _ field ]    is   null ,    cannot   parse .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNullWithoutIgnoreMissing"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "String   path    =     \"  / org / elasticsearch / ingest / attachment / test / sample - files / text - in - english . txt \"  ;", "byte [  ]    bytes ;", "try    ( InputStream   is    =     . class . getResourceAsStream ( path )  )     {", "bytes    =    IOUtils . toByteArray ( is )  ;", "}", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,    bytes )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    attachmentData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \"  \\  \" God   Save   the   Queen \\  \"     ( alternatively    \\  \" God   Save   the   King \\  \"  \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" text / plain \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testParseAsBytesArray"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" test . pdf \"  ,    processor )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is (  \" This   is   a   test ,    with   umlauts ,    from   M \\ u 0  0 fcnchen \\ n \\ nAlso   contains   newlines   for   testing .  \\ n \\ nAnd   one   more .  \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    is (  \" application / pdf \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testPdf"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" text - gibberish . txt \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    hasItem (  \" language \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" lt \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnknownLanguageDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" issue -  2  2  0  7  7  . vsdx \"  ,    processor )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    nullValue (  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  ,    is (  \" application / vnd . ms - visio . drawing \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is (  0 L )  )  ;", "}", "METHOD_END"], "methodName": ["testVisioIsExcluded"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" issue -  1  0  4  . docx \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" content \"  ,     \" language \"  ,     \" date \"  ,     \" author \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" date \"  )  ,    is (  \"  2  0  1  2  -  1  0  -  1  2 T 1  1  :  1  7  :  0  0 Z \"  )  )  ;", "assertThat ( attachmentData . get (  \" author \"  )  ,    is (  \" Windows   User \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    is (  \" application / vnd . openxmlformats - officedocument . wordprocessingml . document \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testWordDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" issue -  2  2  0  7  7  . docx \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" content \"  ,     \" language \"  ,     \" date \"  ,     \" author \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" content \"  )  . toString (  )  ,    containsString (  \" Table   of   Contents \"  )  )  ;", "assertThat ( attachmentData . get (  \" language \"  )  ,    is (  \" en \"  )  )  ;", "assertThat ( attachmentData . get (  \" date \"  )  ,    is (  \"  2  0  1  5  -  0  1  -  0  6 T 1  8  :  0  7  :  0  0 Z \"  )  )  ;", "assertThat ( attachmentData . get (  \" author \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" content _ length \"  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    is (  \" application / vnd . openxmlformats - officedocument . wordprocessingml . document \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testWordDocumentWithVisioSchema"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    attachmentData    =    parseDocument (  \" testXHTML . html \"  ,    processor )  ;", "assertThat ( attachmentData . keySet (  )  ,    containsInAnyOrder (  \" language \"  ,     \" content \"  ,     \" author \"  ,     \" title \"  ,     \" content _ type \"  ,     \" content _ length \"  )  )  ;", "assertThat ( attachmentData . get (  \" content _ type \"  )  . toString (  )  ,    containsString (  \" application / xhtml + xml \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testXHtmlDocument"], "fileName": "org.elasticsearch.ingest.attachment.AttachmentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.ingest.attachment.IngestAttachmentClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "try    {", "byte [  ]    bytes    =    Files . readAllBytes ( fileName )  ;", "String   parsedContent    =    Impl . parse ( bytes ,    new   Metadata (  )  ,     (  -  1  )  )  ;", "assertNotNull ( parsedContent )  ;", "assertFalse ( parsedContent . isEmpty (  )  )  ;", "logger . debug (  \" extracted   content :     {  }  \"  ,    parsedContent )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException (  (  (  \" parsing   of   filename :     \"     +     ( fileName . getFileName (  )  )  )     +     \"    failed \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["assertParseable"], "fileName": "org.elasticsearch.ingest.attachment.TikaDocTests"}, {"methodBody": ["METHOD_START", "{", "Path   tmp    =    createTempDir (  )  ;", "logger . debug (  \" unzipping   all   tika   sample   files \"  )  ;", "try    ( DirectoryStream < Path >    stream    =    Files . newDirectoryStream ( PathUtils . get ( getClass (  )  . getResource (  . TIKA _ FILES )  . toURI (  )  )  )  )     {", "for    ( Path   doc    :    stream )     {", "String   filename    =    doc . getFileName (  )  . toString (  )  ;", "TestUtil . unzip ( getClass (  )  . getResourceAsStream (  (  (  . TIKA _ FILES )     +    filename )  )  ,    tmp )  ;", "}", "}", "try    ( DirectoryStream < Path >    stream    =    Files . newDirectoryStream ( tmp )  )     {", "for    ( Path   doc    :    stream )     {", "logger . debug (  \" parsing :     {  }  \"  ,    doc )  ;", "assertParseable ( doc )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFiles"], "fileName": "org.elasticsearch.ingest.attachment.TikaDocTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "for    ( URL   url    :    resources )     {", "Path   path    =    PathUtils . get ( url . toURI (  )  )  ;", "if    ( Files . isDirectory ( path )  )     {", "FilePermissionUtils . addDirectoryPath ( perms ,     \" cs . path \"  ,    path ,     \" read , readlink \"  )  ;", "} else    {", "FilePermissionUtils . addSingleFilePath ( perms ,    path ,     \" read , readlink \"  )  ;", "}", "}", "}    catch    ( URISyntaxException   bogus )     {", "throw   new   RuntimeException ( bogus )  ;", "}", "}", "METHOD_END"], "methodName": ["addReadPermissions"], "fileName": "org.elasticsearch.ingest.attachment.TikaImpl"}, {"methodBody": ["METHOD_START", "{", "Permissions   perms    =    new   Permissions (  )  ;", "perms . add ( new   PropertyPermission (  \"  *  \"  ,     \" read \"  )  )  ;", "perms . add ( new   RuntimePermission (  \" getenv . TIKA _ CONFIG \"  )  )  ;", "try    {", ". addReadPermissions ( perms ,    JarHell . parseClassPath (  )  )  ;", "if    (  (  . class . getClassLoader (  )  )    instanceof   URLClassLoader )     {", "URL [  ]    urls    =     (  ( URLClassLoader )     (  . class . getClassLoader (  )  )  )  . getURLs (  )  ;", "Set < URL >    set    =    new   LinkedHashSet <  >  ( Arrays . asList ( urls )  )  ;", "if    (  ( set . size (  )  )     !  =     ( urls . length )  )     {", "throw   new   AssertionError (  (  \" duplicate   jars :     \"     +     ( Arrays . toString ( urls )  )  )  )  ;", "}", ". addReadPermissions ( perms ,    set )  ;", "}", "FilePermissionUtils . addDirectoryPath ( perms ,     \" tmpdir \"  ,    PathUtils . get ( System . getProperty (  \" tmpdir \"  )  )  ,     \" read , readlink , write , delete \"  )  ;", "}    catch    ( IOException   e )     {", "throw   new   UncheckedIOException ( e )  ;", "}", "perms . add ( new   SecurityPermission (  \" putProviderProperty . BC \"  )  )  ;", "perms . add ( new   SecurityPermission (  \" insertProvider \"  )  )  ;", "perms . add ( new   ReflectPermission (  \" suppressAccessChecks \"  )  )  ;", "perms . add ( new   RuntimePermission (  \" getClassLoader \"  )  )  ;", "if    (  ( JavaVersion . current (  )  . compareTo ( JavaVersion . parse (  \"  1  0  \"  )  )  )     >  =     0  )     {", "assert    ( JavaVersion . current (  )  . compareTo ( JavaVersion . parse (  \"  1  1  \"  )  )  )     <     0  ;", "perms . add ( new   RuntimePermission (  \" accessDeclaredMembers \"  )  )  ;", "}", "perms . setReadOnly (  )  ;", "return   perms ;", "}", "METHOD_END"], "methodName": ["getRestrictedPermissions"], "fileName": "org.elasticsearch.ingest.attachment.TikaImpl"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   AccessController . doPrivileged (  (  ( PrivilegedExceptionAction < String >  )     (  (  )     -  >     . TIKA _ INSTANCE . parseToString ( new   ByteArrayInputStream ( content )  ,    metadata ,    limit )  )  )  ,     . RESTRICTED _ CONTEXT )  ;", "}    catch    ( PrivilegedActionException   e )     {", "Throwable   cause    =    e . getCause (  )  ;", "if    ( cause   instanceof   TikaException )     {", "throw    (  ( TikaException )     ( cause )  )  ;", "} else", "if    ( cause   instanceof   IOException )     {", "throw    (  ( IOException )     ( cause )  )  ;", "} else    {", "throw   new   AssertionError ( cause )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.elasticsearch.ingest.attachment.TikaImpl"}, {"methodBody": ["METHOD_START", "{", "Class . forName (  \" TikaImpl \"  )  ;", "}", "METHOD_END"], "methodName": ["testTikaLoads"], "fileName": "org.elasticsearch.ingest.attachment.TikaImplTests"}, {"methodBody": ["METHOD_START", "{", "if    (  ( databaseReader . get (  )  )     =  =    null )     {", "databaseReader . set ( loader . get (  )  )  ;", ". LOGGER . debug (  \" Loaded    [  {  }  ]    geoip   database \"  ,    databaseFileName )  ;", "}", "return   databaseReader . get (  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.elasticsearch.ingest.geoip.DatabaseReaderLazyLoader"}, {"methodBody": ["METHOD_START", "{", "GeoIpCache   cache    =    new   GeoIpCache (  1  )  ;", "final   NodeCache . Loader   loader    =     (    key )     -  >    new   IntNode ( key )  ;", "JsonNode   jsonNode 1     =    cache . get (  1  ,    loader )  ;", "assertSame ( jsonNode 1  ,    cache . get (  1  ,    loader )  )  ;", "cache . get (  2  ,    loader )  ;", "assertNotSame ( jsonNode 1  ,    cache . get (  1  ,    loader )  )  ;", "}", "METHOD_END"], "methodName": ["testCachesAndEvictsResults"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpCacheTests"}, {"methodBody": ["METHOD_START", "{", "GeoIpCache   cache    =    new   GeoIpCache (  1  )  ;", "NodeCache . Loader   loader    =     ( int   key )     -  >     {", "throw   new   IllegalArgumentException (  \" Illegal   key \"  )  ;", "}  ;", "EException   ex    =    expectThrows ( EException . class ,     (  )     -  >    cache . get (  1  ,    loader )  )  ;", "assertTrue (  (  (  \" Expected   cause   to   be   of   type   IllegalArgumentException   but   was    [  \"     +     ( ex . getCause (  )  . getClass (  )  )  )     +     \"  ]  \"  )  ,     (  ( ex . getCause (  )  )    instanceof   IllegalArgumentException )  )  ;", "assertEquals (  \" Illegal   key \"  ,    ex . getCause (  )  . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testThrowsElasticsearchException"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpCacheTests"}, {"methodBody": ["METHOD_START", "{", "return   dbReader ;", "}", "METHOD_END"], "methodName": ["getDbReader"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "return   field ;", "}", "METHOD_END"], "methodName": ["getField"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "return   properties ;", "}", "METHOD_END"], "methodName": ["getProperties"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "return   targetField ;", "}", "METHOD_END"], "methodName": ["getTargetField"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "return   ignoreMissing ;", "}", "METHOD_END"], "methodName": ["isIgnoreMissing"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "AsnResponse   response    =    AccessController . doPrivileged (  (  ( PrivilegedAction < AsnResponse >  )     (  (  )     -  >     {", "try    {", "return   dbReader . asn ( ipAddress )  ;", "}    catch    ( AddressNotFoundException   e )     {", "throw   new    . AddressNotFoundRuntimeException ( e )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}  )  )  )  ;", "Integer   asn    =    response . getAutonomousSystemNumber (  )  ;", "String   organization _ name    =    response . getAutonomousSystemOrganization (  )  ;", "Map < String ,    Object >    geoData    =    new   HashMap <  >  (  )  ;", "for    (  . Property   property    :    this . properties )     {", "switch    ( property )     {", "case   IP    :", "geoData . put (  \" ip \"  ,    NetworkAddress . format ( ipAddress )  )  ;", "break ;", "case   ASN    :", "if    ( asn    !  =    null )     {", "geoData . put (  \" asn \"  ,    asn )  ;", "}", "break ;", "case   ORGANIZATION _ NAME    :", "if    ( organization _ name    !  =    null )     {", "geoData . put (  \" organization _ name \"  ,    organization _ name )  ;", "}", "break ;", "}", "}", "return   geoData ;", "}", "METHOD_END"], "methodName": ["retrieveAsnGeoData"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "CityResponse   response    =    AccessController . doPrivileged (  (  ( PrivilegedAction < CityResponse >  )     (  (  )     -  >     {", "try    {", "return   dbReader . city ( ipAddress )  ;", "}    catch    ( AddressNotFoundException   e )     {", "throw   new    . AddressNotFoundRuntimeException ( e )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}  )  )  )  ;", "Country   country    =    response . getCountry (  )  ;", "City   city    =    response . getCity (  )  ;", "Location   location    =    response . getLocation (  )  ;", "Continent   continent    =    response . getContinent (  )  ;", "Subdivision   subdivision    =    response . getMostSpecificSubdivision (  )  ;", "Map < String ,    Object >    geoData    =    new   HashMap <  >  (  )  ;", "for    (  . Property   property    :    this . properties )     {", "switch    ( property )     {", "case   IP    :", "geoData . put (  \" ip \"  ,    NetworkAddress . format ( ipAddress )  )  ;", "break ;", "case   COUNTRY _ ISO _ CODE    :", "String   countryIsoCode    =    country . getIsoCode (  )  ;", "if    ( countryIsoCode    !  =    null )     {", "geoData . put (  \" country _ iso _ code \"  ,    countryIsoCode )  ;", "}", "break ;", "case   COUNTRY _ NAME    :", "String   countryName    =    country . getName (  )  ;", "if    ( countryName    !  =    null )     {", "geoData . put (  \" country _ name \"  ,    countryName )  ;", "}", "break ;", "case   CONTINENT _ NAME    :", "String   continentName    =    continent . getName (  )  ;", "if    ( continentName    !  =    null )     {", "geoData . put (  \" continent _ name \"  ,    continentName )  ;", "}", "break ;", "case   REGION _ NAME    :", "String   subdivisionName    =    subdivision . getName (  )  ;", "if    ( subdivisionName    !  =    null )     {", "geoData . put (  \" region _ name \"  ,    subdivisionName )  ;", "}", "break ;", "case   CITY _ NAME    :", "String   cityName    =    city . getName (  )  ;", "if    ( cityName    !  =    null )     {", "geoData . put (  \" city _ name \"  ,    cityName )  ;", "}", "break ;", "case   TIMEZONE    :", "String   locationTimeZone    =    location . getTimeZone (  )  ;", "if    ( locationTimeZone    !  =    null )     {", "geoData . put (  \" timezone \"  ,    locationTimeZone )  ;", "}", "break ;", "case   LOCATION    :", "Double   latitude    =    location . getLatitude (  )  ;", "Double   longitude    =    location . getLongitude (  )  ;", "if    (  ( latitude    !  =    null )     &  &     ( longitude    !  =    null )  )     {", "Map < String ,    Object >    locationObject    =    new   HashMap <  >  (  )  ;", "locationObject . put (  \" lat \"  ,    latitude )  ;", "locationObject . put (  \" lon \"  ,    longitude )  ;", "geoData . put (  \" location \"  ,    locationObject )  ;", "}", "break ;", "}", "}", "return   geoData ;", "}", "METHOD_END"], "methodName": ["retrieveCityGeoData"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "CountryResponse   response    =    AccessController . doPrivileged (  (  ( PrivilegedAction < CountryResponse >  )     (  (  )     -  >     {", "try    {", "return   dbReader . country ( ipAddress )  ;", "}    catch    ( AddressNotFoundException   e )     {", "throw   new    . AddressNotFoundRuntimeException ( e )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}  )  )  )  ;", "Country   country    =    response . getCountry (  )  ;", "Continent   continent    =    response . getContinent (  )  ;", "Map < String ,    Object >    geoData    =    new   HashMap <  >  (  )  ;", "for    (  . Property   property    :    this . properties )     {", "switch    ( property )     {", "case   IP    :", "geoData . put (  \" ip \"  ,    NetworkAddress . format ( ipAddress )  )  ;", "break ;", "case   COUNTRY _ ISO _ CODE    :", "String   countryIsoCode    =    country . getIsoCode (  )  ;", "if    ( countryIsoCode    !  =    null )     {", "geoData . put (  \" country _ iso _ code \"  ,    countryIsoCode )  ;", "}", "break ;", "case   COUNTRY _ NAME    :", "String   countryName    =    country . getName (  )  ;", "if    ( countryName    !  =    null )     {", "geoData . put (  \" country _ name \"  ,    countryName )  ;", "}", "break ;", "case   CONTINENT _ NAME    :", "String   continentName    =    continent . getName (  )  ;", "if    ( continentName    !  =    null )     {", "geoData . put (  \" continent _ name \"  ,    continentName )  ;", "}", "break ;", "}", "}", "return   geoData ;", "}", "METHOD_END"], "methodName": ["retrieveCountryGeoData"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessor"}, {"methodBody": ["METHOD_START", "{", "if    ( Constants . WINDOWS )     {", "return ;", "}", "for    ( DatabaseReaderLazyLoader   reader    :     . databaseReaders . values (  )  )     {", "reader . close (  )  ;", "}", ". databaseReaders    =    null ;", "}", "METHOD_END"], "methodName": ["closeDatabaseReaders"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "if    ( Constants . WINDOWS )     {", "return ;", "}", "Path   configDir    =    createTempDir (  )  ;", "Path   geoIpConfigDir    =    configDir . resolve (  \" ingest - geoip \"  )  ;", "Files . createDirectories ( geoIpConfigDir )  ;", "Files . copy ( new   ByteArrayInputStream ( StreamsUtils . copyToBytesFromClasspath (  \"  / GeoLite 2  - City . mmdb \"  )  )  ,    geoIpConfigDir . resolve (  \" GeoLite 2  - City . mmdb \"  )  )  ;", "Files . copy ( new   ByteArrayInputStream ( StreamsUtils . copyToBytesFromClasspath (  \"  / GeoLite 2  - Country . mmdb \"  )  )  ,    geoIpConfigDir . resolve (  \" GeoLite 2  - Country . mmdb \"  )  )  ;", "Files . copy ( new   ByteArrayInputStream ( StreamsUtils . copyToBytesFromClasspath (  \"  / GeoLite 2  - ASN . mmdb \"  )  )  ,    geoIpConfigDir . resolve (  \" GeoLite 2  - ASN . mmdb \"  )  )  ;", "NodeCache   cache    =    randomFrom ( NoCache . getInstance (  )  ,    new   GeoIpCache ( randomNonNegativeLong (  )  )  )  ;", ". databaseReaders    =    IngestGeoIpPlugin . loadDatabaseReaders ( geoIpConfigDir ,    cache )  ;", "}", "METHOD_END"], "methodName": ["loadDatabaseReaders"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - ASN . mmdb \"  )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" geoip \"  )  )  ;", "assertThat ( processor . getDbReader (  )  . getMetadata (  )  . getDatabaseType (  )  ,    equalTo (  \" GeoLite 2  - ASN \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance ( GeoIpProcessor . Factory . DEFAULT _ ASN _ PROPERTIES )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAsnBuildDefaults"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - Country . mmdb \"  )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" geoip \"  )  )  ;", "assertThat ( processor . getDbReader (  )  . getMetadata (  )  . getDatabaseType (  )  ,    equalTo (  \" GeoLite 2  - Country \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance ( GeoIpProcessor . Factory . DEFAULT _ COUNTRY _ PROPERTIES )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildDbFile"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" geoip \"  )  )  ;", "assertThat ( processor . getDbReader (  )  . getMetadata (  )  . getDatabaseType (  )  ,    equalTo (  \" GeoLite 2  - City \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance ( GeoIpProcessor . Factory . DEFAULT _ CITY _ PROPERTIES )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildDefaults"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Set < GeoIpProcessor . Property >    properties    =    EnumSet . noneOf ( GeoIpProcessor . Property . class )  ;", "List < String >    fieldNames    =    new   ArrayList <  >  (  )  ;", "int   counter    =     0  ;", "int   numFields    =    scaledRandomIntBetween (  1  ,    GeoIpProcessor . Property . values (  )  . length )  ;", "for    ( GeoIpProcessor . Property   property    :    GeoIpProcessor . Property . ALL _ CITY _ PROPERTIES )     {", "properties . add ( property )  ;", "fieldNames . add ( property . name (  )  . toLowerCase ( Locale . ROOT )  )  ;", "if    (  (  +  + counter )     >  =    numFields )     {", "break ;", "}", "}", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,    fieldNames )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    equalTo ( properties )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildFields"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config 1     =    new   HashMap <  >  (  )  ;", "config 1  . put (  \" field \"  ,     \"  _ field \"  )  ;", "config 1  . put (  \" properties \"  ,    Collections . singletonList (  \" invalid \"  )  )  ;", "Exception   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config 1  )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  (  \"  [ properties ]    illegal   property   value    [ invalid ]  .    valid   values   are    [ IP ,    COUNTRY _ ISO _ CODE ,     \"     +     \" COUNTRY _ NAME ,    CONTINENT _ NAME ,    REGION _ NAME ,    CITY _ NAME ,    TIMEZONE ,    LOCATION ]  \"  )  )  )  ;", "Map < String ,    Object >    config 2     =    new   HashMap <  >  (  )  ;", "config 2  . put (  \" field \"  ,     \"  _ field \"  )  ;", "config 2  . put (  \" properties \"  ,     \" invalid \"  )  ;", "e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config 2  )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  \"  [ properties ]    property   isn ' t   a   list ,    but   of   type    [ String ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildIllegalFieldOption"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" does - not - exist . mmdb \"  )  ;", "Exception   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  \"  [ database _ file ]    database   file    [ does - not - exist . mmdb ]    doesn ' t   exist \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildNonExistingDbFile"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" target _ field \"  ,     \"  _ field \"  )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildTargetField"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - ASN . mmdb \"  )  ;", "EnumSet < GeoIpProcessor . Property >    cityOnlyProperties    =    EnumSet . copyOf ( GeoIpProcessor . Property . ALL _ CITY _ PROPERTIES )  ;", "cityOnlyProperties . remove ( GeoIpProcessor . Property . IP )  ;", "String   cityProperty    =    RandomPicks . randomFrom ( Randomness . get (  )  ,    cityOnlyProperties )  . toString (  )  ;", "config . put (  \" properties \"  ,    Collections . singletonList ( cityProperty )  )  ;", "Exception   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  (  (  \"  [ properties ]    illegal   property   value    [  \"     +    cityProperty )     +     \"  ]  .    valid   values   are    [ IP ,    ASN ,    ORGANIZATION _ NAME ]  \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildWithAsnDbAndCityFields"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - Country . mmdb \"  )  ;", "EnumSet < GeoIpProcessor . Property >    asnOnlyProperties    =    EnumSet . copyOf ( GeoIpProcessor . Property . ALL _ ASN _ PROPERTIES )  ;", "asnOnlyProperties . remove ( GeoIpProcessor . Property . IP )  ;", "String   asnProperty    =    RandomPicks . randomFrom ( Randomness . get (  )  ,    asnOnlyProperties )  . toString (  )  ;", "config . put (  \" properties \"  ,    Collections . singletonList ( asnProperty )  )  ;", "Exception   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  (  (  \"  [ properties ]    illegal   property   value    [  \"     +    asnProperty )     +     \"  ]  .    valid   values   are    [ IP ,    COUNTRY _ ISO _ CODE ,    COUNTRY _ NAME ,    CONTINENT _ NAME ]  \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildWithCountryDbAndAsnFields"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - Country . mmdb \"  )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" geoip \"  )  )  ;", "assertThat ( processor . getDbReader (  )  . getMetadata (  )  . getDatabaseType (  )  ,    equalTo (  \" GeoLite 2  - Country \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance ( GeoIpProcessor . Factory . DEFAULT _ COUNTRY _ PROPERTIES )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCountryBuildDefaults"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "Path   configDir    =    createTempDir (  )  ;", "Path   geoIpConfigDir    =    configDir . resolve (  \" ingest - geoip \"  )  ;", "Files . createDirectories ( geoIpConfigDir )  ;", "Files . copy ( new   ByteArrayInputStream ( StreamsUtils . copyToBytesFromClasspath (  \"  / GeoLite 2  - City . mmdb \"  )  )  ,    geoIpConfigDir . resolve (  \" GeoLite 2  - City . mmdb \"  )  )  ;", "Files . copy ( new   ByteArrayInputStream ( StreamsUtils . copyToBytesFromClasspath (  \"  / GeoLite 2  - Country . mmdb \"  )  )  ,    geoIpConfigDir . resolve (  \" GeoLite 2  - Country . mmdb \"  )  )  ;", "Files . copy ( new   ByteArrayInputStream ( StreamsUtils . copyToBytesFromClasspath (  \"  / GeoLite 2  - ASN . mmdb \"  )  )  ,    geoIpConfigDir . resolve (  \" GeoLite 2  - ASN . mmdb \"  )  )  ;", "Map < String ,    DatabaseReaderLazyLoader >    databaseReaders    =    IngestGeoIpPlugin . loadDatabaseReaders ( geoIpConfigDir ,    NoCache . getInstance (  )  )  ;", ". Factory   factory    =    new    . Factory ( databaseReaders )  ;", "for    ( DatabaseReaderLazyLoader   lazyLoader    :    databaseReaders . values (  )  )     {", "assertNull ( lazyLoader . databaseReader . get (  )  )  ;", "}", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - City . mmdb \"  )  ;", "factory . create ( null ,     \"  _ tag \"  ,    config )  ;", "config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - Country . mmdb \"  )  ;", "factory . create ( null ,     \"  _ tag \"  ,    config )  ;", "config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" database _ file \"  ,     \" GeoLite 2  - ASN . mmdb \"  )  ;", "factory . create ( null ,     \"  _ tag \"  ,    config )  ;", "for    ( DatabaseReaderLazyLoader   lazyLoader    :    databaseReaders . values (  )  )     {", "assertNotNull ( lazyLoader . databaseReader . get (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testLazyLoading"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "assumeFalse (  \" windows   deletion   behavior   is   asinine \"  ,    WINDOWS )  ;", "GeoIpProcessor . Factory   factory    =    new   GeoIpProcessor . Factory (  . databaseReaders )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" ignore _ missing \"  ,    true )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "GeoIpProcessor   processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" geoip \"  )  )  ;", "assertThat ( processor . getDbReader (  )  . getMetadata (  )  . getDatabaseType (  )  ,    equalTo (  \" GeoLite 2  - City \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    sameInstance ( GeoIpProcessor . Factory . DEFAULT _ CITY _ PROPERTIES )  )  ;", "assertTrue ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetIgnoreMissing"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "return   GeoIpProcessor . class . getResourceAsStream ( path )  ;", "}", "METHOD_END"], "methodName": ["getDatabaseFileInputStream"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . containsKey (  \" target _ field \"  )  ,    is ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testAddressIsNotInTheDatabase"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "String   ip    =     \"  8  2  .  1  7  0  .  2  1  3  .  7  9  \"  ;", "InputStream   database    =     . getDatabaseFileInputStream (  \"  / GeoLite 2  - ASN . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,    ip )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . get (  \" source _ field \"  )  ,    equalTo ( ip )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    geoData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( geoData . size (  )  ,    equalTo (  3  )  )  ;", "assertThat ( geoData . get (  \" ip \"  )  ,    equalTo ( ip )  )  ;", "assertThat ( geoData . get (  \" asn \"  )  ,    equalTo (  5  6  1  5  )  )  ;", "assertThat ( geoData . get (  \" organization _ name \"  )  ,    equalTo (  \" KPN   B . V .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testAsn"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \"  8  .  8  .  8  .  8  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . get (  \" source _ field \"  )  ,    equalTo (  \"  8  .  8  .  8  .  8  \"  )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    geoData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( geoData . size (  )  ,    equalTo (  5  )  )  ;", "assertThat ( geoData . get (  \" ip \"  )  ,    equalTo (  \"  8  .  8  .  8  .  8  \"  )  )  ;", "assertThat ( geoData . get (  \" country _ iso _ code \"  )  ,    equalTo (  \" US \"  )  )  ;", "assertThat ( geoData . get (  \" country _ name \"  )  ,    equalTo (  \" United   States \"  )  )  ;", "assertThat ( geoData . get (  \" continent _ name \"  )  ,    equalTo (  \" North   America \"  )  )  ;", "Map < String ,    Object >    location    =    new   HashMap <  >  (  )  ;", "location . put (  \" lat \"  ,     3  7  .  7  5  1  )  ;", "location . put (  \" lon \"  ,     (  -  9  7  .  8  2  2  )  )  ;", "assertThat ( geoData . get (  \" location \"  )  ,    equalTo ( location )  )  ;", "}", "METHOD_END"], "methodName": ["testCity"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \"  8  0  .  2  3  1  .  5  .  0  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . get (  \" source _ field \"  )  ,    equalTo (  \"  8  0  .  2  3  1  .  5  .  0  \"  )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    geoData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( geoData . size (  )  ,    equalTo (  1  )  )  ;", "assertThat ( geoData . get (  \" ip \"  )  ,    equalTo (  \"  8  0  .  2  3  1  .  5  .  0  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testCityWithMissingLocation"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "String   address    =     \"  2  6  0  2  :  3  0  6  :  3  3 d 3  :  8  0  0  0  :  :  3  2  5  7  :  9  6  5  2  \"  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,    address )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . get (  \" source _ field \"  )  ,    equalTo ( address )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    geoData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( geoData . size (  )  ,    equalTo (  8  )  )  ;", "assertThat ( geoData . get (  \" ip \"  )  ,    equalTo ( address )  )  ;", "assertThat ( geoData . get (  \" country _ iso _ code \"  )  ,    equalTo (  \" US \"  )  )  ;", "assertThat ( geoData . get (  \" country _ name \"  )  ,    equalTo (  \" United   States \"  )  )  ;", "assertThat ( geoData . get (  \" continent _ name \"  )  ,    equalTo (  \" North   America \"  )  )  ;", "assertThat ( geoData . get (  \" region _ name \"  )  ,    equalTo (  \" Florida \"  )  )  ;", "assertThat ( geoData . get (  \" city _ name \"  )  ,    equalTo (  \" Hollywood \"  )  )  ;", "assertThat ( geoData . get (  \" timezone \"  )  ,    equalTo (  \" America / New _ York \"  )  )  ;", "Map < String ,    Object >    location    =    new   HashMap <  >  (  )  ;", "location . put (  \" lat \"  ,     2  6  .  0  2  5  2  )  ;", "location . put (  \" lon \"  ,     (  -  8  0  .  2  9  6  )  )  ;", "assertThat ( geoData . get (  \" location \"  )  ,    equalTo ( location )  )  ;", "}", "METHOD_END"], "methodName": ["testCity_withIpV6"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - Country . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \"  8  2  .  1  7  0  .  2  1  3  .  7  9  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . get (  \" source _ field \"  )  ,    equalTo (  \"  8  2  .  1  7  0  .  2  1  3  .  7  9  \"  )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    geoData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( geoData . size (  )  ,    equalTo (  4  )  )  ;", "assertThat ( geoData . get (  \" ip \"  )  ,    equalTo (  \"  8  2  .  1  7  0  .  2  1  3  .  7  9  \"  )  )  ;", "assertThat ( geoData . get (  \" country _ iso _ code \"  )  ,    equalTo (  \" NL \"  )  )  ;", "assertThat ( geoData . get (  \" country _ name \"  )  ,    equalTo (  \" Netherlands \"  )  )  ;", "assertThat ( geoData . get (  \" continent _ name \"  )  ,    equalTo (  \" Europe \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testCountry"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - Country . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \"  8  0  .  2  3  1  .  5  .  0  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "processor . execute ( ingestDocument )  ;", "assertThat ( ingestDocument . getSourceAndMetadata (  )  . get (  \" source _ field \"  )  ,    equalTo (  \"  8  0  .  2  3  1  .  5  .  0  \"  )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    geoData    =     (  ( Map < String ,    Object >  )     ( ingestDocument . getSourceAndMetadata (  )  . get (  \" target _ field \"  )  )  )  ;", "assertThat ( geoData . size (  )  ,    equalTo (  1  )  )  ;", "assertThat ( geoData . get (  \" ip \"  )  ,    equalTo (  \"  8  0  .  2  3  1  .  5  .  0  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testCountryWithMissingLocation"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \" www . google . com \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", "Exception   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \" not   an   IP   string   literal \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalid"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    true )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . emptyMap (  )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "processor . execute ( ingestDocument )  ;", "IngestDocumentMatcher . assertIngestDocument ( originalIngestDocument ,    ingestDocument )  ;", "}", "METHOD_END"], "methodName": ["testNonExistentWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . emptyMap (  )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Exception   exception    =    expectThrows ( Exception . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( exception . getMessage (  )  ,    equalTo (  \" field    [ source _ field ]    not   present   as   part   of   path    [ source _ field ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNonExistentWithoutIgnoreMissing"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    true )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . singletonMap (  \" source _ field \"  ,    null )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "processor . execute ( ingestDocument )  ;", "IngestDocumentMatcher . assertIngestDocument ( originalIngestDocument ,    ingestDocument )  ;", "}", "METHOD_END"], "methodName": ["testNullValueWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   database    =    GeoIpProcessorTests . getDatabaseFileInputStream (  \"  / GeoLite 2  - City . mmdb \"  )  ;", "GeoIpProcessor   processor    =    new   GeoIpProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,    new   DatabaseReader . Builder ( database )  . build (  )  ,     \" target _ field \"  ,    EnumSet . allOf ( GeoIpProcessor . Property . class )  ,    false )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . singletonMap (  \" source _ field \"  ,    null )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Exception   exception    =    expectThrows ( Exception . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( exception . getMessage (  )  ,    equalTo (  \" field    [ source _ field ]    is   null ,    cannot   extract   geoip   information .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNullWithoutIgnoreMissing"], "fileName": "org.elasticsearch.ingest.geoip.GeoIpProcessorTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.ingest.geoip.IngestGeoIpClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "return   new   DatabaseReader . Builder ( databasePath . toFile (  )  )  ;", "}", "METHOD_END"], "methodName": ["createDatabaseBuilder"], "fileName": "org.elasticsearch.ingest.geoip.IngestGeoIpPlugin"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( Files . exists ( geoIpConfigDirectory )  )     =  =    false )     &  &     ( Files . isDirectory ( geoIpConfigDirectory )  )  )     {", "throw   new   IllegalStateException (  (  (  \" thedirectory    [  \"     +    geoIpConfigDirectory )     +     \"  ]    containing   databases   doesn ' t   exist \"  )  )  ;", "}", "boolean   loadDatabaseOnHeap    =    Booleans . parseBoolean ( System . getProperty (  \" esload _ db _ on _ heap \"  ,     \" false \"  )  )  ;", "Map < String ,    DatabaseReaderLazyLoader >    databaseReaders    =    new   HashMap <  >  (  )  ;", "try    ( Stream < Path >    databaseFiles    =    Files . list ( geoIpConfigDirectory )  )     {", "PathMatcher   pathMatcher    =    geoIpConfigDirectory . getFileSystem (  )  . getPathMatcher (  \" glob :  *  *  . mmdb \"  )  ;", "Iterator < Path >    iterator    =    databaseFiles . iterator (  )  ;", "while    ( iterator . hasNext (  )  )     {", "Path   databasePath    =    iterator . next (  )  ;", "if    (  ( Files . isRegularFile ( databasePath )  )     &  &     ( pathMatcher . matches ( databasePath )  )  )     {", "String   databaseFileName    =    databasePath . getFileName (  )  . toString (  )  ;", "DatabaseReaderLazyLoader   holder    =    new   DatabaseReaderLazyLoader ( databaseFileName ,     (  )     -  >     {", "DatabaseReader . Builder   builder    =    createDatabaseBuilder ( databasePath )  . withCache ( cache )  ;", "if    ( loadDatabaseOnHeap )     {", "builder . fileMode ( Reader . FileMode . MEMORY )  ;", "} else    {", "builder . fileMode ( Reader . FileMode . MEMORY _ MAPPED )  ;", "}", "return   builder . build (  )  ;", "}  )  ;", "databaseReaders . put ( databaseFileName ,    holder )  ;", "}", "}", "}", "return   Collections . unmodifiableMap ( databaseReaders )  ;", "}", "METHOD_END"], "methodName": ["loadDatabaseReaders"], "fileName": "org.elasticsearch.ingest.geoip.IngestGeoIpPlugin"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.ingest.useragent.IngestUserAgentClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    UserAgentParser >    userAgentParsers    =    new   HashMap <  >  (  )  ;", "UserAgentParser   defaultParser    =    new   UserAgentParser (  . DEFAULT _ PARSER _ NAME ,     . class . getResourceAsStream (  \"  / regexes . yml \"  )  ,    cache )  ;", "userAgentParsers . put (  . DEFAULT _ PARSER _ NAME ,    defaultParser )  ;", "if    (  ( Files . exists ( userAgentConfigDirectory )  )     &  &     ( Files . isDirectory ( userAgentConfigDirectory )  )  )     {", "PathMatcher   pathMatcher    =    userAgentConfigDirectory . getFileSystem (  )  . getPathMatcher (  \" glob :  *  *  . yml \"  )  ;", "try    ( Stream < Path >    regexFiles    =    Files . find ( userAgentConfigDirectory ,     1  ,     (    path ,    attr )     -  >     ( attr . isRegularFile (  )  )     &  &     ( pathMatcher . matches ( path )  )  )  )     {", "Iterable < Path >    iterable    =    regexFiles :  : iterator ;", "for    ( Path   path    :    iterable )     {", "String   parserName    =    path . getFileName (  )  . toString (  )  ;", "try    ( InputStream   regexStream    =    Files . newInputStream ( path ,    StandardOpenOption . READ )  )     {", "userAgentParsers . put ( parserName ,    new   UserAgentParser ( parserName ,    regexStream ,    cache )  )  ;", "}", "}", "}", "}", "return   Collections . unmodifiableMap ( userAgentParsers )  ;", "}", "METHOD_END"], "methodName": ["createUserAgentParsers"], "fileName": "org.elasticsearch.ingest.useragent.IngestUserAgentPlugin"}, {"methodBody": ["METHOD_START", "{", "return   cache . get ( new   UserAgentCache . CompositeCacheKey ( parserName ,    userAgent )  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentCache"}, {"methodBody": ["METHOD_START", "{", "cache . put ( new   UserAgentCache . CompositeCacheKey ( parserName ,    userAgent )  ,    details )  ;", "}", "METHOD_END"], "methodName": ["put"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentCache"}, {"methodBody": ["METHOD_START", "{", "if    (  ( regex _ flag    !  =    null )     &  &     ( regex _ flag . equals (  \" i \"  )  )  )     {", "return   Pattern . compile ( regex ,    Pattern . CASE _ INSENSITIVE )  ;", "} else    {", "return   Pattern . compile ( regex )  ;", "}", "}", "METHOD_END"], "methodName": ["compilePattern"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "UserAgentParser . VersionedName   name ;", "for    ( UserAgentParser . UserAgentSubpattern   pattern    :    possiblePatterns )     {", "name    =    pattern . match ( agentString )  ;", "if    ( name    !  =    null )     {", "return   name ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["findMatch"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "return   devicePatterns ;", "}", "METHOD_END"], "methodName": ["getDevicePatterns"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "return   osPatterns ;", "}", "METHOD_END"], "methodName": ["getOsPatterns"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "return   uaPatterns ;", "}", "METHOD_END"], "methodName": ["getUaPatterns"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "XContentParser   yamlParser    =    XContentFactory . xContent ( YAML )  . createParser ( EMPTY ,    INSTANCE ,    regexStream )  ;", "XContentParser . Token   token    =    yamlParser . nextToken (  )  ;", "if    ( token    =  =     ( Token . START _ OBJECT )  )     {", "token    =    yamlParser . nextToken (  )  ;", "for    (  ;    token    !  =    null ;    token    =    yamlParser . nextToken (  )  )     {", "if    (  ( token    =  =     ( Token . FIELD _ NAME )  )     &  &     ( yamlParser . currentName (  )  . equals (  \" user _ agent _ parsers \"  )  )  )     {", "List < Map < String ,    String >  >    parserConfigurations    =    readParserConfigurations ( yamlParser )  ;", "for    ( Map < String ,    String >    map    :    parserConfigurations )     {", "uaPatterns . add ( new    . UserAgentSubpattern ( compilePattern ( map . get (  \" regex \"  )  ,    map . get (  \" regex _ flag \"  )  )  ,    map . get (  \" family _ replacement \"  )  ,    map . get (  \" v 1  _ replacement \"  )  ,    map . get (  \" v 2  _ replacement \"  )  ,    map . get (  \" v 3  _ replacement \"  )  ,    map . get (  \" v 4  _ replacement \"  )  )  )  ;", "}", "} else", "if    (  ( token    =  =     ( Token . FIELD _ NAME )  )     &  &     ( yamlParser . currentName (  )  . equals (  \" os _ parsers \"  )  )  )     {", "List < Map < String ,    String >  >    parserConfigurations    =    readParserConfigurations ( yamlParser )  ;", "for    ( Map < String ,    String >    map    :    parserConfigurations )     {", "osPatterns . add ( new    . UserAgentSubpattern ( compilePattern ( map . get (  \" regex \"  )  ,    map . get (  \" regex _ flag \"  )  )  ,    map . get (  \" os _ replacement \"  )  ,    map . get (  \" os _ v 1  _ replacement \"  )  ,    map . get (  \" os _ v 2  _ replacement \"  )  ,    map . get (  \" os _ v 3  _ replacement \"  )  ,    map . get (  \" os _ v 4  _ replacement \"  )  )  )  ;", "}", "} else", "if    (  ( token    =  =     ( Token . FIELD _ NAME )  )     &  &     ( yamlParser . currentName (  )  . equals (  \" device _ parsers \"  )  )  )     {", "List < Map < String ,    String >  >    parserConfigurations    =    readParserConfigurations ( yamlParser )  ;", "for    ( Map < String ,    String >    map    :    parserConfigurations )     {", "devicePatterns . add ( new    . UserAgentSubpattern ( compilePattern ( map . get (  \" regex \"  )  ,    map . get (  \" regex _ flag \"  )  )  ,    map . get (  \" device _ replacement \"  )  ,    null ,    null ,    null ,    null )  )  ;", "}", "}", "}", "}", "if    (  (  ( uaPatterns . isEmpty (  )  )     &  &     ( osPatterns . isEmpty (  )  )  )     &  &     ( devicePatterns . isEmpty (  )  )  )     {", "throw   new   ElasticsearchParseException (  \" not   a   valid   regular   expression   file \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "UserAgentParser . Details   details    =    cache . get ( name ,    agentString )  ;", "if    ( details    =  =    null )     {", "UserAgentParser . VersionedName   userAgent    =    findMatch ( uaPatterns ,    agentString )  ;", "UserAgentParser . VersionedName   operatingSystem    =    findMatch ( osPatterns ,    agentString )  ;", "UserAgentParser . VersionedName   device    =    findMatch ( devicePatterns ,    agentString )  ;", "details    =    new   UserAgentParser . Details ( userAgent ,    operatingSystem ,    device )  ;", "cache . put ( name ,    agentString ,    details )  ;", "}", "return   details ;", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "List < Map < String ,    String >  >    patternList    =    new   ArrayList <  >  (  )  ;", "XContentParser . Token   token    =    yamlParser . nextToken (  )  ;", "if    ( token    !  =     ( Token . START _ ARRAY )  )     {", "throw   new   EParseException (  \" malformed   regular   expression   file ,    should   continue   with    ' array '    after    ' object '  \"  )  ;", "}", "token    =    yamlParser . nextToken (  )  ;", "if    ( token    !  =     ( Token . START _ OBJECT )  )     {", "throw   new   EParseException (  \" malformed   regular   expression   file ,    expecting    ' object '  \"  )  ;", "}", "while    ( token    =  =     ( Token . START _ OBJECT )  )     {", "token    =    yamlParser . nextToken (  )  ;", "if    ( token    !  =     ( Token . FIELD _ NAME )  )     {", "throw   new   EParseException (  \" malformed   regular   expression   file ,    should   continue   with    ' field _ name '    after    ' array '  \"  )  ;", "}", "Map < String ,    String >    regexMap    =    new   HashMap <  >  (  )  ;", "for    (  ;    token    =  =     ( Token . FIELD _ NAME )  ;    token    =    yamlParser . nextToken (  )  )     {", "String   fieldName    =    yamlParser . currentName (  )  ;", "token    =    yamlParser . nextToken (  )  ;", "String   fieldValue    =    yamlParser . text (  )  ;", "regexMap . put ( fieldName ,    fieldValue )  ;", "}", "patternList . add ( regexMap )  ;", "token    =    yamlParser . nextToken (  )  ;", "}", "return   patternList ;", "}", "METHOD_END"], "methodName": ["readParserConfigurations"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentParser"}, {"methodBody": ["METHOD_START", "{", "if    (  ( operatingSystem    =  =    null )     |  |     (  ( operatingSystem . name )     =  =    null )  )     {", "return   null ;", "}", "StringBuilder   sb    =    new   StringBuilder ( operatingSystem . name )  ;", "if    (  ( operatingSystem . major )     !  =    null )     {", "sb . append (  \"     \"  )  ;", "sb . append ( operatingSystem . major )  ;", "if    (  ( operatingSystem . minor )     !  =    null )     {", "sb . append (  \"  .  \"  )  ;", "sb . append ( operatingSystem . minor )  ;", "if    (  ( operatingSystem . patch )     !  =    null )     {", "sb . append (  \"  .  \"  )  ;", "sb . append ( operatingSystem . patch )  ;", "if    (  ( operatingSystem . build )     !  =    null )     {", "sb . append (  \"  .  \"  )  ;", "sb . append ( operatingSystem . build )  ;", "}", "}", "}", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["buildFullOSName"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   field ;", "}", "METHOD_END"], "methodName": ["getField"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   properties ;", "}", "METHOD_END"], "methodName": ["getProperties"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   targetField ;", "}", "METHOD_END"], "methodName": ["getTargetField"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   parser ;", "}", "METHOD_END"], "methodName": ["getUaParser"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessor"}, {"methodBody": ["METHOD_START", "{", "return   ignoreMissing ;", "}", "METHOD_END"], "methodName": ["isIgnoreMissing"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessor"}, {"methodBody": ["METHOD_START", "{", "Path   configDir    =    createTempDir (  )  ;", ". userAgentConfigDir    =    configDir . resolve (  \" ingest - user - agent \"  )  ;", "Files . createDirectories (  . userAgentConfigDir )  ;", "try    ( BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( UserAgentProcessor . class . getResourceAsStream (  \"  / regexes . yml \"  )  ,    StandardCharsets . UTF _  8  )  )  ; BufferedWriter   writer    =    Files . newBufferedWriter (  . userAgentConfigDir . resolve (  . regexWithoutDevicesFilename )  )  )     {", "String   line ;", "while    (  ( line    =    reader . readLine (  )  )     !  =    null )     {", "if    ( line . startsWith (  \" device _ parsers :  \"  )  )     {", "break ;", "}", "writer . write ( line )  ;", "writer . newLine (  )  ;", "}", "}", ". userAgentParsers    =    IngestUserAgentPlugin . createUserAgentParsers (  . userAgentConfigDir ,    new   UserAgentCache (  1  0  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["createUserAgentParsers"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "UserAgentProcessor   processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" user _ agent \"  )  )  ;", "assertThat ( processor . getUaParser (  )  . getUaPatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getUaParser (  )  . getOsPatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getUaParser (  )  . getDevicePatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getProperties (  )  ,    equalTo ( EnumSet . allOf ( UserAgentProcessor . Property . class )  )  )  ;", "assertFalse ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildDefaults"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Set < UserAgentProcessor . Property >    properties    =    EnumSet . noneOf ( UserAgentProcessor . Property . class )  ;", "List < String >    fieldNames    =    new   ArrayList <  >  (  )  ;", "int   numFields    =    scaledRandomIntBetween (  1  ,    UserAgentProcessor . Property . values (  )  . length )  ;", "for    ( int   i    =     0  ;    i    <    numFields ;    i +  +  )     {", "UserAgentProcessor . Property   property    =    UserAgentProcessor . Property . values (  )  [ i ]  ;", "properties . add ( property )  ;", "fieldNames . add ( property . name (  )  . toLowerCase ( Locale . ROOT )  )  ;", "}", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,    fieldNames )  ;", "UserAgentProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getProperties (  )  ,    equalTo ( properties )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildFields"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" regex _ file \"  ,     \" does - not - exist . yml \"  )  ;", "ElasticsearchParseException   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  \"  [ regex _ file ]    regex   file    [ does - not - exist . yml ]    doesn ' t   exist    ( has   to   exist   at   node   startup )  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildNonExistingRegexFile"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" regex _ file \"  ,    UserAgentProcessorFactoryTests . regexWithoutDevicesFilename )  ;", "UserAgentProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getUaParser (  )  . getUaPatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getUaParser (  )  . getOsPatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getUaParser (  )  . getDevicePatterns (  )  . size (  )  ,    equalTo (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildRegexFile"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" target _ field \"  ,     \"  _ target _ field \"  )  ;", "UserAgentProcessor   processor    =    factory . create ( null ,    null ,    config )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \"  _ target _ field \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildTargetField"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" ignore _ missing \"  ,    true )  ;", "String   processorTag    =    randomAlphaOfLength (  1  0  )  ;", "UserAgentProcessor   processor    =    factory . create ( null ,    processorTag ,    config )  ;", "assertThat ( processor . getTag (  )  ,    equalTo ( processorTag )  )  ;", "assertThat ( processor . getField (  )  ,    equalTo (  \"  _ field \"  )  )  ;", "assertThat ( processor . getTargetField (  )  ,    equalTo (  \" user _ agent \"  )  )  ;", "assertThat ( processor . getUaParser (  )  . getUaPatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getUaParser (  )  . getOsPatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getUaParser (  )  . getDevicePatterns (  )  . size (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( processor . getProperties (  )  ,    equalTo ( EnumSet . allOf ( UserAgentProcessor . Property . class )  )  )  ;", "assertTrue ( processor . isIgnoreMissing (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBuildWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,     \" invalid \"  )  ;", "ElasticsearchParseException   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  \"  [ properties ]    property   isn ' t   a   list ,    but   of   type    [ String ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidPropertiesType"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor . Factory   factory    =    new   UserAgentProcessor . Factory ( UserAgentProcessorFactoryTests . userAgentParsers )  ;", "Map < String ,    Object >    config    =    new   HashMap <  >  (  )  ;", "config . put (  \" field \"  ,     \"  _ field \"  )  ;", "config . put (  \" properties \"  ,    Collections . singletonList (  \" invalid \"  )  )  ;", "ElasticsearchParseException   e    =    expectThrows ( ElasticsearchParseException . class ,     (  )     -  >    factory . create ( null ,    null ,    config )  )  ;", "assertThat ( e . getMessage (  )  ,    equalTo (  (  \"  [ properties ]    illegal   property   value    [ invalid ]  .    valid   values   are    [ NAME ,    MAJOR ,    MINOR ,     \"     +     \" PATCH ,    OS ,    OS _ NAME ,    OS _ MAJOR ,    OS _ MINOR ,    DEVICE ,    BUILD ]  \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidProperty"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorFactoryTests"}, {"methodBody": ["METHOD_START", "{", "InputStream   regexStream    =    UserAgentProcessor . class . getResourceAsStream (  \"  / regexes . yml \"  )  ;", "assertNotNull ( regexStream )  ;", "UserAgentParser   parser    =    new   UserAgentParser ( randomAlphaOfLength (  1  0  )  ,    regexStream ,    new   UserAgentCache (  1  0  0  0  )  )  ;", ". processor    =    new   UserAgentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    parser ,    EnumSet . allOf ( UserAgentProcessor . Property . class )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["setupProcessor"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \" Mozilla /  5  .  0     ( Macintosh ;    Intel   Mac   OS   X    1  0  _  9  _  2  )    AppleWebKit /  5  3  7  .  3  6     ( KHTML ,    like   Gecko )    Chrome /  3  3  .  0  .  1  7  5  0  .  1  4  9    Safari /  5  3  7  .  3  6  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", ". processor . execute ( ingestDocument )  ;", "Map < String ,    Object >    data    =    ingestDocument . getSourceAndMetadata (  )  ;", "assertThat ( data ,    hasKey (  \" target _ field \"  )  )  ;", "Map < String ,    Object >    target    =     (  ( Map < String ,    Object >  )     ( data . get (  \" target _ field \"  )  )  )  ;", "assertThat ( target . get (  \" name \"  )  ,    is (  \" Chrome \"  )  )  ;", "assertThat ( target . get (  \" major \"  )  ,    is (  \"  3  3  \"  )  )  ;", "assertThat ( target . get (  \" minor \"  )  ,    is (  \"  0  \"  )  )  ;", "assertThat ( target . get (  \" patch \"  )  ,    is (  \"  1  7  5  0  \"  )  )  ;", "assertNull ( target . get (  \" build \"  )  )  ;", "assertThat ( target . get (  \" os \"  )  ,    is (  \" Mac   OS   X    1  0  .  9  .  2  \"  )  )  ;", "assertThat ( target . get (  \" os _ name \"  )  ,    is (  \" Mac   OS   X \"  )  )  ;", "assertThat ( target . get (  \" os _ major \"  )  ,    is (  \"  1  0  \"  )  )  ;", "assertThat ( target . get (  \" os _ minor \"  )  ,    is (  \"  9  \"  )  )  ;", "assertThat ( target . get (  \" device \"  )  ,    is (  \" Other \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testCommonBrowser"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor   processor    =    new   UserAgentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    null ,    EnumSet . allOf ( UserAgentProcessor . Property . class )  ,    true )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . emptyMap (  )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "processor . execute ( ingestDocument )  ;", "IngestDocumentMatcher . assertIngestDocument ( originalIngestDocument ,    ingestDocument )  ;", "}", "METHOD_END"], "methodName": ["testNonExistentWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor   processor    =    new   UserAgentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    null ,    EnumSet . allOf ( UserAgentProcessor . Property . class )  ,    false )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . emptyMap (  )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Exception   exception    =    expectThrows ( Exception . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( exception . getMessage (  )  ,    equalTo (  \" field    [ source _ field ]    not   present   as   part   of   path    [ source _ field ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNonExistentWithoutIgnoreMissing"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor   processor    =    new   UserAgentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    null ,    EnumSet . allOf ( UserAgentProcessor . Property . class )  ,    true )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . singletonMap (  \" source _ field \"  ,    null )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "processor . execute ( ingestDocument )  ;", "IngestDocumentMatcher . assertIngestDocument ( originalIngestDocument ,    ingestDocument )  ;", "}", "METHOD_END"], "methodName": ["testNullValueWithIgnoreMissing"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "UserAgentProcessor   processor    =    new   UserAgentProcessor ( randomAlphaOfLength (  1  0  )  ,     \" source _ field \"  ,     \" target _ field \"  ,    null ,    EnumSet . allOf ( UserAgentProcessor . Property . class )  ,    false )  ;", "IngestDocument   originalIngestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    Collections . singletonMap (  \" source _ field \"  ,    null )  )  ;", "IngestDocument   ingestDocument    =    new   IngestDocument ( originalIngestDocument )  ;", "Exception   exception    =    expectThrows ( Exception . class ,     (  )     -  >    processor . execute ( ingestDocument )  )  ;", "assertThat ( exception . getMessage (  )  ,    equalTo (  \" field    [ source _ field ]    is   null ,    cannot   parse   user - agent .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNullWithoutIgnoreMissing"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \" Mozilla /  5  .  0     ( compatible ;    EasouSpider ;     + http :  /  / www . easou . com / search / spider . html )  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", ". processor . execute ( ingestDocument )  ;", "Map < String ,    Object >    data    =    ingestDocument . getSourceAndMetadata (  )  ;", "assertThat ( data ,    hasKey (  \" target _ field \"  )  )  ;", "Map < String ,    Object >    target    =     (  ( Map < String ,    Object >  )     ( data . get (  \" target _ field \"  )  )  )  ;", "assertThat ( target . get (  \" name \"  )  ,    is (  \" EasouSpider \"  )  )  ;", "assertNull ( target . get (  \" major \"  )  )  ;", "assertNull ( target . get (  \" minor \"  )  )  ;", "assertNull ( target . get (  \" patch \"  )  )  ;", "assertNull ( target . get (  \" build \"  )  )  ;", "assertThat ( target . get (  \" os \"  )  ,    is (  \" Other \"  )  )  ;", "assertThat ( target . get (  \" os _ name \"  )  ,    is (  \" Other \"  )  )  ;", "assertNull ( target . get (  \" os _ major \"  )  )  ;", "assertNull ( target . get (  \" os _ minor \"  )  )  ;", "assertThat ( target . get (  \" device \"  )  ,    is (  \" Spider \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSpider"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     (  \" Mozilla /  5  .  0     ( Linux ;    U ;    Android    3  .  0  ;    en - us ;    Xoom   Build / HRI 3  9  )    AppleWebKit /  5  2  5  .  1  0  +     \"     +     \"  ( KHTML ,    like   Gecko )    Version /  3  .  0  .  4    Mobile   Safari /  5  2  3  .  1  2  .  2  \"  )  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", ". processor . execute ( ingestDocument )  ;", "Map < String ,    Object >    data    =    ingestDocument . getSourceAndMetadata (  )  ;", "assertThat ( data ,    hasKey (  \" target _ field \"  )  )  ;", "Map < String ,    Object >    target    =     (  ( Map < String ,    Object >  )     ( data . get (  \" target _ field \"  )  )  )  ;", "assertThat ( target . get (  \" name \"  )  ,    is (  \" Android \"  )  )  ;", "assertThat ( target . get (  \" major \"  )  ,    is (  \"  3  \"  )  )  ;", "assertThat ( target . get (  \" minor \"  )  ,    is (  \"  0  \"  )  )  ;", "assertNull ( target . get (  \" patch \"  )  )  ;", "assertNull ( target . get (  \" build \"  )  )  ;", "assertThat ( target . get (  \" os \"  )  ,    is (  \" Android    3  .  0  \"  )  )  ;", "assertThat ( target . get (  \" os _ name \"  )  ,    is (  \" Android \"  )  )  ;", "assertThat ( target . get (  \" os _ major \"  )  ,    is (  \"  3  \"  )  )  ;", "assertThat ( target . get (  \" os _ minor \"  )  ,    is (  \"  0  \"  )  )  ;", "assertThat ( target . get (  \" device \"  )  ,    is (  \" Motorola   Xoom \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testUncommonDevice"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Object >    document    =    new   HashMap <  >  (  )  ;", "document . put (  \" source _ field \"  ,     \" Something   I   made   up   v 4  2  .  0  .  1  \"  )  ;", "IngestDocument   ingestDocument    =    RandomDocumentPicks . randomIngestDocument ( random (  )  ,    document )  ;", ". processor . execute ( ingestDocument )  ;", "Map < String ,    Object >    data    =    ingestDocument . getSourceAndMetadata (  )  ;", "assertThat ( data ,    hasKey (  \" target _ field \"  )  )  ;", "Map < String ,    Object >    target    =     (  ( Map < String ,    Object >  )     ( data . get (  \" target _ field \"  )  )  )  ;", "assertThat ( target . get (  \" name \"  )  ,    is (  \" Other \"  )  )  ;", "assertNull ( target . get (  \" major \"  )  )  ;", "assertNull ( target . get (  \" minor \"  )  )  ;", "assertNull ( target . get (  \" patch \"  )  )  ;", "assertNull ( target . get (  \" build \"  )  )  ;", "assertThat ( target . get (  \" os \"  )  ,    is (  \" Other \"  )  )  ;", "assertThat ( target . get (  \" os _ name \"  )  ,    is (  \" Other \"  )  )  ;", "assertNull ( target . get (  \" os _ major \"  )  )  ;", "assertNull ( target . get (  \" os _ minor \"  )  )  ;", "assertThat ( target . get (  \" device \"  )  ,    is (  \" Other \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnknown"], "fileName": "org.elasticsearch.ingest.useragent.UserAgentProcessorTests"}, {"methodBody": ["METHOD_START", "{", "return   new   AzureComputeServiceImpl ( settings )  ;", "}", "METHOD_END"], "methodName": ["createComputeService"], "fileName": "org.elasticsearch.plugin.discovery.azure.classic.AzureDiscoveryPlugin"}, {"methodBody": ["METHOD_START", "{", "return   new   AzureUnicastHostsProvider ( settings ,    azureComputeService ,    transportService ,    networkService )  ;", "}", "METHOD_END"], "methodName": ["createUnicastHostsProvider"], "fileName": "org.elasticsearch.plugin.discovery.azure.classic.AzureDiscoveryPlugin"}, {"methodBody": ["METHOD_START", "{", "return   new   GceInstancesServiceImpl ( settings )  ;", "}", "METHOD_END"], "methodName": ["createGceInstancesService"], "fileName": "org.elasticsearch.plugin.discovery.gce.GceDiscoveryPlugin"}, {"methodBody": ["METHOD_START", "{", "return    ( keyPath )     +     ( blobName    =  =    null    ?     \"  \"     :    blobName )  ;", "}", "METHOD_END"], "methodName": ["buildKey"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobContainer"}, {"methodBody": ["METHOD_START", "{", "return   this . client . blobExists ( this . clientName ,    this . locMode ,    container ,    blob )  ;", "}", "METHOD_END"], "methodName": ["blobExists"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "this . client . deleteBlob ( this . clientName ,    this . locMode ,    container ,    blob )  ;", "}", "METHOD_END"], "methodName": ["deleteBlob"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   this . client . doesContainerExist ( this . clientName ,    this . locMode ,    container )  ;", "}", "METHOD_END"], "methodName": ["doesContainerExist"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   this . client . getInputStream ( this . clientName ,    this . locMode ,    container ,    blob )  ;", "}", "METHOD_END"], "methodName": ["getInputStream"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   locMode ;", "}", "METHOD_END"], "methodName": ["getLocationMode"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   this . client . listBlobsByPrefix ( this . clientName ,    this . locMode ,    container ,    keyPath ,    prefix )  ;", "}", "METHOD_END"], "methodName": ["listBlobsByPrefix"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "this . client . moveBlob ( this . clientName ,    this . locMode ,    container ,    sourceBlob ,    targetBlob )  ;", "}", "METHOD_END"], "methodName": ["moveBlob"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "this . client . writeBlob ( this . clientName ,    this . locMode ,    container ,    blobName ,    inputStream ,    blobSize )  ;", "}", "METHOD_END"], "methodName": ["writeBlob"], "fileName": "org.elasticsearch.repositories.azure.AzureBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   new   AzureStorageServiceImpl ( settings ,    clientsSettings )  ;", "}", "METHOD_END"], "methodName": ["createStorageService"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositoryPlugin"}, {"methodBody": ["METHOD_START", "{", "Settings   internalSettings    =    Settings . builder (  )  . put ( PATH _ HOME _ SETTING . getKey (  )  ,    createTempDir (  )  . toAbsolutePath (  )  )  . putList ( PATH _ DATA _ SETTING . getKey (  )  ,    tmpPaths (  )  )  . put ( settings )  . build (  )  ;", "return   new   AzureRepository ( new   cluster . metadata . RepositoryMetaData (  \" foo \"  ,     \" azure \"  ,    internalSettings )  ,    TestEnvironment . newEnvironment ( internalSettings )  ,    NamedXContentRegistry . EMPTY ,    null )  ;", "}", "METHOD_END"], "methodName": ["azureRepository"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "AzureRepository   azureRepository    =    azureRepository ( EMPTY )  ;", "assertEquals ( AzureStorageService . MAX _ CHUNK _ SIZE ,    azureRepository . chunkSize (  )  )  ;", "int   size    =    randomIntBetween (  1  ,     6  4  )  ;", "azureRepository    =    azureRepository ( Settings . builder (  )  . put (  \" chunk _ size \"  ,     ( size    +     \" mb \"  )  )  . build (  )  )  ;", "assertEquals ( new   common . unit . ByteSizeValue ( size ,    ByteSizeUnit . MB )  ,    azureRepository . chunkSize (  )  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    azureRepository ( Settings . builder (  )  . put (  \" chunk _ size \"  ,     \"  0  \"  )  . build (  )  )  )  ;", "assertEquals (  \" failed   to   parse   value    [  0  ]    for   setting    [ chunk _ size ]  ,    must   be    >  =     [  1 b ]  \"  ,    e . getMessage (  )  )  ;", "e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    azureRepository ( Settings . builder (  )  . put (  \" chunk _ size \"  ,     \"  -  1  \"  )  . build (  )  )  )  ;", "assertEquals (  \" failed   to   parse   value    [  -  1  ]    for   setting    [ chunk _ size ]  ,    must   be    >  =     [  1 b ]  \"  ,    e . getMessage (  )  )  ;", "e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    azureRepository ( Settings . builder (  )  . put (  \" chunk _ size \"  ,     \"  6  5 mb \"  )  . build (  )  )  )  ;", "assertEquals (  \" failed   to   parse   value    [  6  5 mb ]    for   setting    [ chunk _ size ]  ,    must   be    <  =     [  6  4 mb ]  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testChunkSize"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( EMPTY )  . isReadOnly (  )  ,    is ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyDefault"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put (  \" readonly \"  ,    true )  . build (  )  )  . isReadOnly (  )  ,    is ( true )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyDefaultAndReadonlyOn"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,    PRIMARY _ THEN _ SECONDARY . name (  )  )  . put (  \" readonly \"  ,    false )  . build (  )  )  . isReadOnly (  )  ,    is ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyWithPrimaryAndSecondaryOnlyAndReadonlyOff"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,    PRIMARY _ THEN _ SECONDARY . name (  )  )  . put (  \" readonly \"  ,    true )  . build (  )  )  . isReadOnly (  )  ,    is ( true )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyWithPrimaryAndSecondaryOnlyAndReadonlyOn"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,    PRIMARY _ ONLY . name (  )  )  . build (  )  )  . isReadOnly (  )  ,    is ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyWithPrimaryOnly"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,    PRIMARY _ ONLY . name (  )  )  . put (  \" readonly \"  ,    true )  . build (  )  )  . isReadOnly (  )  ,    is ( true )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyWithPrimaryOnlyAndReadonlyOn"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,    SECONDARY _ ONLY . name (  )  )  . put (  \" readonly \"  ,    false )  . build (  )  )  . isReadOnly (  )  ,    is ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyWithSecondaryOnlyAndReadonlyOff"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( azureRepository ( Settings . builder (  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,    SECONDARY _ ONLY . name (  )  )  . put (  \" readonly \"  ,    true )  . build (  )  )  . isReadOnly (  )  ,    is ( true )  )  ;", "}", "METHOD_END"], "methodName": ["testReadonlyWithSecondaryOnlyAndReadonlyOn"], "fileName": "org.elasticsearch.repositories.azure.AzureRepositorySettingsTests"}, {"methodBody": ["METHOD_START", "{", "assertBusy (  (  )     -  >     {", "gettorageService (  )  . createContainer (  \" default \"  ,    LocationMode . PRIMARY _ ONLY ,    containerName )  ;", "}  ,     3  0  ,    TimeUnit . SECONDS )  ;", "}", "METHOD_END"], "methodName": ["createTestContainer"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "AzureSnapshotRestoreTests . createTestContainer ( AzureSnapshotRestoreTests . getContainerName (  )  )  ;", "AzureSnapshotRestoreTests . createTestContainer (  (  ( AzureSnapshotRestoreTests . getContainerName (  )  )     +     \"  -  1  \"  )  )  ;", "AzureSnapshotRestoreTests . createTestContainer (  (  ( AzureSnapshotRestoreTests . getContainerName (  )  )     +     \"  -  2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["createTestContainers"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "return   Settings . builder (  )  . setSecureSettings ( AzureTestUtils . generateMockSecureSettings (  )  )  ;", "}", "METHOD_END"], "methodName": ["generateMockSettings"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "return   new   AzureStorageServiceImpl ( AzureSnapshotRestoreTests . generateMockSettings (  )  . build (  )  ,    AzureStorageSettings . load ( AzureSnapshotRestoreTests . generateMockSettings (  )  . build (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getAzureStorageService"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "String   testName    =     \" snapshot - itest -  \"  . concat ( RandomizedTest . getContext (  )  . getRunnerSeedAsString (  )  . toLowerCase ( Locale . ROOT )  )  ;", "return   testName . contains (  \"     \"  )     ?    Strings . split ( testName ,     \"     \"  )  [  0  ]     :    testName ;", "}", "METHOD_END"], "methodName": ["getContainerName"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "String   testName    =     \" it -  \"     +     ( getTestName (  )  )  ;", "return   testName . contains (  \"     \"  )     ?    Strings . split ( testName ,     \"     \"  )  [  0  ]     :    testName ;", "}", "METHOD_END"], "methodName": ["getRepositoryPath"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "AzureSnapshotRestoreTests . removeTestContainer ( AzureSnapshotRestoreTests . getContainerName (  )  )  ;", "AzureSnapshotRestoreTests . removeTestContainer (  (  ( AzureSnapshotRestoreTests . getContainerName (  )  )     +     \"  -  1  \"  )  )  ;", "AzureSnapshotRestoreTests . removeTestContainer (  (  ( AzureSnapshotRestoreTests . getContainerName (  )  )     +     \"  -  2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["removeContainer"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "AzureSnapshotRestoreTests . getAzureStorageService (  )  . removeContainer (  \" default \"  ,    PRIMARY _ ONLY ,    containerName )  ;", "}", "METHOD_END"], "methodName": ["removeTestContainer"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "logger . info (  \"  -  -  >       creating   azure   primary   repository \"  )  ;", "PutRepositoryResponse   putRepositoryResponsePrimary    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" primary \"  )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  )  )  . get (  )  ;", "assertThat ( putRepositoryResponsePrimary . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "logger . info (  \"  -  -  >    start   get   snapshots   on   primary \"  )  ;", "long   startWait    =    System . currentTimeMillis (  )  ;", "client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" primary \"  )  . get (  )  ;", "long   endWait    =    System . currentTimeMillis (  )  ;", "assertThat (  ( endWait    -    startWait )  ,    lessThanOrEqualTo (  3  0  0  0  0 L )  )  ;", "logger . info (  \"  -  -  >       creating   azure   secondary   repository \"  )  ;", "PutRepositoryResponse   putRepositoryResponseSecondary    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" secondary \"  )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  )  . put ( AzureRepository . Repository . LOCATION _ MODE _ SETTING . getKey (  )  ,     \" secondary _ only \"  )  )  . get (  )  ;", "assertThat ( putRepositoryResponseSecondary . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "logger . info (  \"  -  -  >    start   get   snapshots   on   secondary \"  )  ;", "startWait    =    System . currentTimeMillis (  )  ;", "client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" secondary \"  )  . get (  )  ;", "endWait    =    System . currentTimeMillis (  )  ;", "logger . info (  \"  -  -  >    end   of   get   snapshots   on   secondary .    Took    {  }    ms \"  ,     ( endWait    -    startWait )  )  ;", "assertThat (  ( endWait    -    startWait )  ,    lessThanOrEqualTo (  3  0  0  0  0 L )  )  ;", "}", "METHOD_END"], "methodName": ["testGeoRedundantStorage"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "final   String   repositoryName    =     \" test - repo -  2  8  \"  ;", "ClusterAdminClient   client    =    client (  )  . admin (  )  . cluster (  )  ;", "logger . info (  \"  -  -  >       creating   azure   repository   without   any   path \"  )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . preparePutRepository ( repositoryName )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "try    {", "client . prepareGetSnapshots ( repositoryName )  . addSnapshots (  \" nonexistingsnapshotname \"  )  . get (  )  ;", "fail (  \" Shouldn ' t   be   here \"  )  ;", "}    catch    ( SnapshotMissingException   ex )     {", "}", "try    {", "client . prepareDeleteSnapshot ( repositoryName ,     \" nonexistingsnapshotname \"  )  . get (  )  ;", "fail (  \" Shouldn ' t   be   here \"  )  ;", "}    catch    ( SnapshotMissingException   ex )     {", "}", "}", "METHOD_END"], "methodName": ["testGetDeleteNonExistingSnapshot_28"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "final   String   repositoryName    =     \" test - repo -  2  6  \"  ;", "createIndex (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  ,     \" test - idx -  3  \"  )  ;", "ensureGreen (  )  ;", "logger . info (  \"  -  -  >    indexing   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "index (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" bar \"     +    i )  )  ;", "index (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "index (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "}", "refresh (  )  ;", "ClusterAdminClient   client    =    client (  )  . admin (  )  . cluster (  )  ;", "logger . info (  \"  -  -  >       creating   azure   repository   without   any   path \"  )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . preparePutRepository ( repositoryName )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "assertThat ( client . prepareGetSnapshots ( repositoryName )  . get (  )  . getSnapshots (  )  . size (  )  ,    equalTo (  0  )  )  ;", "logger . info (  \"  -  -  >    snapshot \"  )  ;", "CreateSnapshotResponse   createSnapshotResponse    =    client . prepareCreateSnapshot ( repositoryName ,     \" test - snap -  2  6  \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( client . prepareGetSnapshots ( repositoryName )  . get (  )  . getSnapshots (  )  . size (  )  ,    equalTo (  1  )  )  ;", "client . prepareDeleteSnapshot ( repositoryName ,     \" test - snap -  2  6  \"  )  . get (  )  ;", "client . prepareDeleteRepository ( repositoryName )  . get (  )  ;", "logger . info (  \"  -  -  >       creating   azure   repository   path    [  {  }  ]  \"  ,    getRepositoryPath (  )  )  ;", "putRepositoryResponse    =    client . preparePutRepository ( repositoryName )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  )  . put ( AzureRepository . Repository . BASE _ PATH _ SETTING . getKey (  )  ,    getRepositoryPath (  )  )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "assertThat ( client . prepareGetSnapshots ( repositoryName )  . get (  )  . getSnapshots (  )  . size (  )  ,    equalTo (  0  )  )  ;", "logger . info (  \"  -  -  >    snapshot \"  )  ;", "createSnapshotResponse    =    client . prepareCreateSnapshot ( repositoryName ,     \" test - snap -  2  6  \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( client . prepareGetSnapshots ( repositoryName )  . get (  )  . getSnapshots (  )  . size (  )  ,    equalTo (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testListBlobs_26"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "logger . info (  \"  -  -  >       creating   azure   repository   with   path    [  {  }  ]  \"  ,    getRepositoryPath (  )  )  ;", "PutRepositoryResponse   putRepositoryResponse 1     =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo 1  \"  )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  . concat (  \"  -  1  \"  )  )  . put ( AzureRepository . Repository . BASE _ PATH _ SETTING . getKey (  )  ,    getRepositoryPath (  )  )  . put ( AzureRepository . Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    randomIntBetween (  1  0  0  0  ,     1  0  0  0  0  )  ,    BYTES )  )  . get (  )  ;", "assertThat ( putRepositoryResponse 1  . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "PutRepositoryResponse   putRepositoryResponse 2     =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo 2  \"  )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  . concat (  \"  -  2  \"  )  )  . put ( AzureRepository . Repository . BASE _ PATH _ SETTING . getKey (  )  ,    getRepositoryPath (  )  )  . put ( AzureRepository . Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    randomIntBetween (  1  0  0  0  ,     1  0  0  0  0  )  ,    BYTES )  )  . get (  )  ;", "assertThat ( putRepositoryResponse 2  . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "createIndex (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  ;", "ensureGreen (  )  ;", "logger . info (  \"  -  -  >    indexing   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "index (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" bar \"     +    i )  )  ;", "index (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "logger . info (  \"  -  -  >    snapshot    1  \"  )  ;", "CreateSnapshotResponse   createSnapshotResponse 1     =    client . admin (  )  . cluster (  )  . prepareCreateSnapshot (  \" test - repo 1  \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  1  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse 1  . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( createSnapshotResponse 1  . getSnapshotInfo (  )  . successfulShards (  )  ,    equalTo ( createSnapshotResponse 1  . getSnapshotInfo (  )  . totalShards (  )  )  )  ;", "logger . info (  \"  -  -  >    snapshot    2  \"  )  ;", "CreateSnapshotResponse   createSnapshotResponse 2     =    client . admin (  )  . cluster (  )  . prepareCreateSnapshot (  \" test - repo 2  \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  2  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse 2  . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( createSnapshotResponse 2  . getSnapshotInfo (  )  . successfulShards (  )  ,    equalTo ( createSnapshotResponse 2  . getSnapshotInfo (  )  . totalShards (  )  )  )  ;", "assertThat ( client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" test - repo 1  \"  )  . setSnapshots (  \" test - snap \"  )  . get (  )  . getSnapshots (  )  . get (  0  )  . state (  )  ,    equalTo ( SUCCESS )  )  ;", "assertThat ( client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" test - repo 2  \"  )  . setSnapshots (  \" test - snap \"  )  . get (  )  . getSnapshots (  )  . get (  0  )  . state (  )  ,    equalTo ( SUCCESS )  )  ;", "logger . info (  \"  -  -  >    delete   indices \"  )  ;", "cluster (  )  . wipeIndices (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  ;", "logger . info (  \"  -  -  >    restore   one   index   after   deletion   from   snapshot    1  \"  )  ;", "RestoreSnapshotResponse   restoreSnapshotResponse 1     =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo 1  \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  1  \"  )  . get (  )  ;", "assertThat ( restoreSnapshotResponse 1  . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "ClusterState   clusterState    =    client . admin (  )  . cluster (  )  . prepareState (  )  . get (  )  . getState (  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  1  \"  )  ,    equalTo ( true )  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  2  \"  )  ,    equalTo ( false )  )  ;", "logger . info (  \"  -  -  >    restore   other   index   after   deletion   from   snapshot    2  \"  )  ;", "RestoreSnapshotResponse   restoreSnapshotResponse 2     =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo 2  \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  2  \"  )  . get (  )  ;", "assertThat ( restoreSnapshotResponse 2  . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "clusterState    =    client . admin (  )  . cluster (  )  . prepareState (  )  . get (  )  . getState (  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  1  \"  )  ,    equalTo ( true )  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  2  \"  )  ,    equalTo ( true )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleRepositories"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "final   String   repositoryName    =     \" test - repo - test 2  3  \"  ;", "Client   client    =    client (  )  ;", "logger . info (  \"  -  -  >       creating   azure   repository   with   path    [  {  }  ]  \"  ,    getRepositoryPath (  )  )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository ( repositoryName )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,     . getContainerName (  )  )  . put ( AzureRepository . Repository . BASE _ PATH _ SETTING . getKey (  )  ,    getRepositoryPath (  )  )  . put ( AzureRepository . Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    randomIntBetween (  1  0  0  0  ,     1  0  0  0  0  )  ,    BYTES )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "logger . info (  \"  -  -  >    restore   non   existing   snapshot \"  )  ;", "try    {", "client . admin (  )  . cluster (  )  . prepareRestoreSnapshot ( repositoryName ,     \" no - existing - snapshot \"  )  . setWaitForCompletion ( true )  . get (  )  ;", "fail (  \" Shouldn ' t   be   here \"  )  ;", "}    catch    ( SnapshotRestoreException   ex )     {", "}", "}", "METHOD_END"], "methodName": ["testNonExistingRepo_23"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "final   String   container    =    AzureSnapshotRestoreTests . getContainerName (  )  . concat (  \"  - testremove \"  )  ;", "AzureSnapshotRestoreTests . createTestContainer ( container )  ;", "AzureSnapshotRestoreTests . removeTestContainer ( container )  ;", "ClusterAdminClient   client    =    client (  )  . admin (  )  . cluster (  )  ;", "logger . info (  \"  -  -  >       creating   azure   repository   while   container   is   being   removed \"  )  ;", "try    {", "client . preparePutRepository (  \" test - repo \"  )  . setType (  \" azure \"  )  . setSettings ( Settings . builder (  )  . put ( AzureRepository . Repository . CONTAINER _ SETTING . getKey (  )  ,    container )  )  . get (  )  ;", "fail (  \" we   should   get   a   RepositoryVerificationException \"  )  ;", "}    catch    ( RepositoryVerificationException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testRemoveAndCreateContainer"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "client (  )  . admin (  )  . cluster (  )  . prepareDeleteRy (  \"  *  \"  )  . get (  )  ;", "}    catch    ( RyMissingException   ignored )     {", "}", "}", "METHOD_END"], "methodName": ["wipeAzureRepositories"], "fileName": "org.elasticsearch.repositories.azure.AzureSnapshotRestoreTests"}, {"methodBody": ["METHOD_START", "{", "final   InetSocketAddress   inetSocketAddress    =     (  ( InetSocketAddress )     ( address )  )  ;", "if    (  ( inetSocketAddress . getAddress (  )  )    instanceof   Inet 6 Address )     {", "ren    (  (  \"  [  \"     +     ( inetSocketAddress . getHostString (  )  )  )     +     \"  ]  :  \"  )     +     ( inetSocketAddress . getPort (  )  )  ;", "} else    {", "ren    (  ( inetSocketAddress . getHostString (  )  )     +     \"  :  \"  )     +     ( inetSocketAddress . getPort (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addressToString"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageFixture"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args    =  =    null )     |  |     (  ( args . length )     !  =     2  )  )     {", "throw   new   IllegalArgumentException (  \"     < working   directory >     < container >  \"  )  ;", "}", "final   InetSocketAddress   socketAddress    =    new   InetSocketAddress ( InetAddress . getLoopbackAddress (  )  ,     0  )  ;", "final   HttpServer   httpServer    =    MockHttpServer . createHttp ( socketAddress ,     0  )  ;", "try    {", "final   Path   workingDirectory    =     . workingDir ( args [  0  ]  )  ;", ". writeFile ( workingDirectory ,     \" pid \"  ,    ManagementFactory . getRuntimeMXBean (  )  . getName (  )  . split (  \"  @  \"  )  [  0  ]  )  ;", "final   String   addressAndPort    =     . addressToString ( httpServer . getAddress (  )  )  ;", ". writeFile ( workingDirectory ,     \" ports \"  ,    addressAndPort )  ;", "final   String   storageUrl    =     \" http :  /  /  \"     +    addressAndPort ;", "final   AzureStorageTestServer   testServer    =    new   AzureStorageTestServer ( storageUrl )  ;", "testServer . createContainer ( args [  1  ]  )  ;", "httpServer . createContext (  \"  /  \"  ,    new    . ResponseHandler ( testServer )  )  ;", "httpServer . start (  )  ;", "Thread . sleep ( Long . MAX _ VALUE )  ;", "}    finally    {", "httpServer . stop (  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageFixture"}, {"methodBody": ["METHOD_START", "{", "return   Paths . get ( dir )  ;", "}", "METHOD_END"], "methodName": ["workingDir"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageFixture"}, {"methodBody": ["METHOD_START", "{", "final   Path   tempPidFile    =    Files . createTempFile ( dir ,    null ,    null )  ;", "Files . write ( tempPidFile ,    Collections . singleton ( content )  )  ;", "Files . move ( tempPidFile ,    dir . resolve ( fileName )  ,    StandardCopyOption . ATOMIC _ MOVE )  ;", "}", "METHOD_END"], "methodName": ["writeFile"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageFixture"}, {"methodBody": ["METHOD_START", "{", "turn   new   InputSam (  )     {", "@ Override", "public   intad (  )    throws   IOException    {", "turn   SocketAccess . doPrivilegedIOException ( sam : ad )  ;", "}", "@ Override", "public   intad ( byte [  ]    b )    throws   IOException    {", "turn   SocketAccess . doPrivilegedIOException (  (  )     -  >    samad ( b )  )  ;", "}", "@ Override", "public   intad ( byte [  ]    b ,    int   off ,    int   len )    throws   IOException    {", "turn   SocketAccess . doPrivilegedIOException (  (  )     -  >    samad ( b ,    off ,    len )  )  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["giveSocketPermissionsToStream"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageService"}, {"methodBody": ["METHOD_START", "{", "String   path    =    uri . getPath (  )  ;", "String [  ]    splits    =    path . split (  \"  /  \"  ,     3  )  ;", "return   splits [  2  ]  ;", "}", "METHOD_END"], "methodName": ["blobNameFromUri"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceImpl"}, {"methodBody": ["METHOD_START", "{", "try    {", "logger . trace (  \" creating   new   Azure   storage   client   using   account    [  {  }  ]  ,    key    [  {  }  ]  ,    endpoint   suffix    [  {  }  ]  \"  ,    attings . getAccount (  )  ,    attings . getKey (  )  ,    attings . getEndpointSuffix (  )  )  ;", "String   storageConnectionString    =     (  (  (  (  \" DefaultEndpointsProtocol = https ;  \"     +     \" AccountName =  \"  )     +     ( attings . getAccount (  )  )  )     +     \"  ;  \"  )     +     \" AccountKey =  \"  )     +     ( attings . getKey (  )  )  ;", "String   endpointSuffix    =    attings . getEndpointSuffix (  )  ;", "if    (  ( endpointSuffix    !  =    null )     &  &     (  !  ( endpointSuffix . isEmpty (  )  )  )  )     {", "storageConnectionString    +  =     \"  ; EndpointSuffix =  \"     +    endpointSuffix ;", "}", "CloudStorageAccount   storageAccount    =    CloudStorageAccount . parse ( storageConnectionString )  ;", "CloudBlobClient   client    =    storageAccount . createCloudBlobClient (  )  ;", "this . clients . put ( attings . getAccount (  )  ,    client )  ;", "}    catch    ( Exception   e )     {", "logger . error (  \" can   not   create   azure   storage   client :     {  }  \"  ,    e . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createClient"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceImpl"}, {"methodBody": ["METHOD_START", "{", "OperationContext   context    =    new   OperationContext (  )  ;", "ttings   azureStorageSettings    =    this . storageSettings . get ( clientName )  ;", "if    (  ( azureStorageSettings . getProxy (  )  )     !  =    null )     {", "context . setProxy ( azureStorageSettings . getProxy (  )  )  ;", "}", "return   context ;", "}", "METHOD_END"], "methodName": ["generateOperationContext"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceImpl"}, {"methodBody": ["METHOD_START", "{", "logger . trace (  \" selecting   a   client   named    [  {  }  ]  ,    mode    [  {  }  ]  \"  ,    clientName ,    mode . name (  )  )  ;", "ttings   azureStorageSettings    =    this . storageSettings . get ( clientName )  ;", "if    ( azureStorageSettings    =  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" Can   not   find   named   azure   client    [  \"     +    clientName )     +     \"  ]  .    Check   your   settings .  \"  )  )  ;", "}", "CloudBlobClient   client    =    this . clients . get ( azureStorageSettings . getAccount (  )  )  ;", "if    ( client    =  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" Can   not   find   an   azure   client   named    [  \"     +     ( azureStorageSettings . getAccount (  )  )  )     +     \"  ]  \"  )  )  ;", "}", "client . getDefaultRequestOptions (  )  . setLocationMode ( mode )  ;", "if    (  ( azureStorageSettings . getTimeout (  )  . getSeconds (  )  )     >     0  )     {", "try    {", "int   timeout    =     (  ( int )     ( azureStorageSettings . getTimeout (  )  . getMillis (  )  )  )  ;", "client . getDefaultRequestOptions (  )  . setTimeoutIntervalInMs ( timeout )  ;", "}    catch    ( ClassCastException   e )     {", "throw   new   IllegalArgumentException (  (  (  \" Can   not   convert    [  \"     +     ( azureStorageSettings . getTimeout (  )  )  )     +     \"  ]  .    It   can   not   be   longer   than    2  ,  1  4  7  ,  4  8  3  ,  6  4  7 ms .  \"  )  )  ;", "}", "}", "client . getDefaultRequestOptions (  )  . setRetryPolicyFactory ( new   com . microsoft . azure . storage . RetryExponentialRetry ( RetryPolicy . DEFAULT _ CLIENT _ BACKOFF ,    azureStorageSettings . getMaxRetries (  )  )  )  ;", "return   client ;", "}", "METHOD_END"], "methodName": ["getSelectedClient"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( str    =  =    null )     |  |     ( suffix    =  =    null )  )     {", "return   false ;", "}", "if    ( str . endsWith ( suffix )  )     {", "return   true ;", "}", "if    (  ( str . length (  )  )     <     ( suffix . length (  )  )  )     {", "return   false ;", "}", "String   lcStr    =    str . substring (  0  ,    suffix . length (  )  )  . toLowerCase ( Locale . ROOT )  ;", "String   lcPrefix    =    suffix . toLowerCase ( Locale . ROOT )  ;", "return   lcStr . equals ( lcPrefix )  ;", "}", "METHOD_END"], "methodName": ["endsWithIgnoreCase"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceMock"}, {"methodBody": ["METHOD_START", "{", "if    (  ( str    =  =    null )     |  |     ( prefix    =  =    null )  )     {", "return   false ;", "}", "if    ( str . startsWith ( prefix )  )     {", "return   true ;", "}", "if    (  ( str . length (  )  )     <     ( prefix . length (  )  )  )     {", "return   false ;", "}", "String   lcStr    =    str . substring (  0  ,    prefix . length (  )  )  . toLowerCase ( Locale . ROOT )  ;", "String   lcPrefix    =    prefix . toLowerCase ( Locale . ROOT )  ;", "return   lcStr . equals ( lcPrefix )  ;", "}", "METHOD_END"], "methodName": ["startsWithIgnoreCase"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceMock"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  . client 1  . account \"  ,     \" myaccount 1  \"  )  ;", "secureSettings . setString (  . client 1  . key \"  ,     \" mykey 1  \"  )  ;", "secureSettings . setString (  . client 2  . account \"  ,     \" myaccount 2  \"  )  ;", "secureSettings . setString (  . client 2  . key \"  ,     \" mykey 2  \"  )  ;", "secureSettings . setString (  . client 3  . account \"  ,     \" myaccount 3  \"  )  ;", "secureSettings . setString (  . client 3  . key \"  ,     \" mykey 3  \"  )  ;", "return   secureSettings ;", "}", "METHOD_END"], "methodName": ["buildSecureSettings"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . build (  )  ;", "return   settings ;", "}", "METHOD_END"], "methodName": ["buildSettings"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "String   name    =    AzureStorageServiceImpl . blobNameFromUri ( new   URI (  \" https :  /  / myservice . azure . net / container / path / to / myfile \"  )  )  ;", "assertThat ( name ,    is (  \" path / to / myfile \"  )  )  ;", "name    =    AzureStorageServiceImpl . blobNameFromUri ( new   URI (  \" http :  /  / myservice . azure . net / container / path / to / myfile \"  )  )  ;", "assertThat ( name ,    is (  \" path / to / myfile \"  )  )  ;", "name    =    AzureStorageServiceImpl . blobNameFromUri ( new   URI (  \" http :  /  /  1  2  7  .  0  .  0  .  1  / container / path / to / myfile \"  )  )  ;", "assertThat ( name ,    is (  \" path / to / myfile \"  )  )  ;", "name    =    AzureStorageServiceImpl . blobNameFromUri ( new   URI (  \" https :  /  /  1  2  7  .  0  .  0  .  1  / container / path / to / myfile \"  )  )  ;", "assertThat ( name ,    is (  \" path / to / myfile \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testBlobNameFromUri"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  \" azure . client . azure 1  . account \"  ,     \" myaccount 1  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 1  . key \"  ,    Base 6  4  . encode (  \" mykey 1  \"  . getBytes ( StandardCharsets . UTF _  8  )  )  )  ;", "secureSettings . setString (  \" azure . client . azure 2  . account \"  ,     \" myaccount 2  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 2  . key \"  ,    Base 6  4  . encode (  \" mykey 2  \"  . getBytes ( StandardCharsets . UTF _  8  )  )  )  ;", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( secureSettings )  . put (  \" azure . client . azure 1  . endpoint _ suffix \"  ,     \" my _ endpoint _ suffix \"  )  . build (  )  ;", "Impl   azureStorageService    =    new   Impl ( settings ,    AzureStorageSettings . load ( settings )  )  ;", "CloudBlobClient   client 1     =    azureStorageService . getSelectedClient (  \" azure 1  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 1  . getEndpoint (  )  . toString (  )  ,    equalTo (  \" https :  /  / myaccount 1  . blob . my _ endpoint _ suffix \"  )  )  ;", "CloudBlobClient   client 2     =    azureStorageService . getSelectedClient (  \" azure 2  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 2  . getEndpoint (  )  . toString (  )  ,    equalTo (  \" https :  /  / myaccount 2  . blob . core . windows . net \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateClientWithEndpointSuffix"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "AzureStorageServiceImpl   azureStorageService    =    new   AzureStorageServiceTests . AzureStorageServiceMockForSettings ( buildSettings (  )  )  ;", "CloudBlobClient   client 1     =    azureStorageService . getSelectedClient (  \" azure 1  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 1  . getDefaultRequestOptions (  )  . getRetryPolicyFactory (  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( client 1  . getDefaultRequestOptions (  )  . getRetryPolicyFactory (  )  ,    instanceOf ( RetryExponentialRetry . class )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSelectedClientBackoffPolicy"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   timeoutSettings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . max _ retries \"  ,     7  )  . build (  )  ;", "AzureStorageServiceImpl   azureStorageService    =    new    . AzureStorageServiceMockForSettings ( timeoutSettings )  ;", "CloudBlobClient   client 1     =    azureStorageService . getSelectedClient (  \" azure 1  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 1  . getDefaultRequestOptions (  )  . getRetryPolicyFactory (  )  ,    is ( notNullValue (  )  )  )  ;", "assertThat ( client 1  . getDefaultRequestOptions (  )  . getRetryPolicyFactory (  )  ,    instanceOf ( RetryExponentialRetry . class )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSelectedClientBackoffPolicyNbRetries"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   timeoutSettings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 3  . timeout \"  ,     \"  3  0 s \"  )  . build (  )  ;", "AzureStorageServiceImpl   azureStorageService    =    new    . AzureStorageServiceMockForSettings ( timeoutSettings )  ;", "CloudBlobClient   client 1     =    azureStorageService . getSelectedClient (  \" azure 1  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 1  . getDefaultRequestOptions (  )  . getTimeoutIntervalInMs (  )  ,    nullValue (  )  )  ;", "CloudBlobClient   client 3     =    azureStorageService . getSelectedClient (  \" azure 3  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 3  . getDefaultRequestOptions (  )  . getTimeoutIntervalInMs (  )  ,    is (  (  3  0     *     1  0  0  0  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSelectedClientDefaultTimeout"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "AzureStorageServiceImpl   azureStorageService    =    new   AzureStorageServiceTests . AzureStorageServiceMockForSettings ( buildSettings (  )  )  ;", "CloudBlobClient   client 1     =    azureStorageService . getSelectedClient (  \" azure 1  \"  ,    PRIMARY _ ONLY )  ;", "assertThat ( client 1  . getDefaultRequestOptions (  )  . getTimeoutIntervalInMs (  )  ,    is ( nullValue (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSelectedClientNoTimeout"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "AzureStorageServiceImpl   azureStorageService    =    new   AzureStorageServiceTests . AzureStorageServiceMockForSettings ( buildSettings (  )  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >     {", "azureStorageService . getSelectedClient (  \" azure 4  \"  ,    LocationMode . PRIMARY _ ONLY )  ;", "}  )  ;", "assertThat ( e . getMessage (  )  ,    is (  \" Can   not   find   named   azure   client    [ azure 4  ]  .    Check   your   settings .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSelectedClientNonExisting"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "new    . AzureStorageServiceMockForSettings ( Settings . EMPTY )  ;", "fail (  \" we   should   have   raised   an   IllegalArgumentException \"  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "assertThat ( e . getMessage (  )  ,    is (  \" If   you   want   to   use   an   azure   repository ,    you   need   to   define   a   client   configuration .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetSelectedClientWithNoPrimaryAndSecondary"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . host \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  . put (  \" azure . client . azure 1  . proxy . port \"  ,     8  0  8  0  )  . put (  \" azure . client . azure 1  . proxy . type \"  ,     \" http \"  )  . put (  \" azure . client . azure 2  . proxy . host \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  . put (  \" azure . client . azure 2  . proxy . port \"  ,     8  0  8  1  )  . put (  \" azure . client . azure 2  . proxy . type \"  ,     \" http \"  )  . build (  )  ;", ". AzureStorageServiceMockForSettings   mock    =    new    . AzureStorageServiceMockForSettings ( settings )  ;", "Proxy   azure 1 Proxy    =    mock . storageSettings . get (  \" azure 1  \"  )  . getProxy (  )  ;", "assertThat ( azure 1 Proxy ,    notNullValue (  )  )  ;", "assertThat ( azure 1 Proxy . type (  )  ,    is ( Proxy . Type . HTTP )  )  ;", "assertThat ( azure 1 Proxy . address (  )  ,    is ( new   InetSocketAddress ( InetAddress . getByName (  \"  1  2  7  .  0  .  0  .  1  \"  )  ,     8  0  8  0  )  )  )  ;", "Proxy   azure 2 Proxy    =    mock . storageSettings . get (  \" azure 2  \"  )  . getProxy (  )  ;", "assertThat ( azure 2 Proxy ,    notNullValue (  )  )  ;", "assertThat ( azure 2 Proxy . type (  )  ,    is ( Proxy . Type . HTTP )  )  ;", "assertThat ( azure 2 Proxy . address (  )  ,    is ( new   InetSocketAddress ( InetAddress . getByName (  \"  1  2  7  .  0  .  0  .  1  \"  )  ,     8  0  8  1  )  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 3  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleProxies"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . build (  )  ;", ". AzureStorageServiceMockForSettings   mock    =    new    . AzureStorageServiceMockForSettings ( settings )  ;", "assertThat ( mock . storageSettings . get (  \" azure 1  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 2  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 3  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNoProxy"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . host \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  . put (  \" azure . client . azure 1  . proxy . port \"  ,     8  0  8  0  )  . put (  \" azure . client . azure 1  . proxy . type \"  ,     \" http \"  )  . build (  )  ;", ". AzureStorageServiceMockForSettings   mock    =    new    . AzureStorageServiceMockForSettings ( settings )  ;", "Proxy   azure 1 Proxy    =    mock . storageSettings . get (  \" azure 1  \"  )  . getProxy (  )  ;", "assertThat ( azure 1 Proxy ,    notNullValue (  )  )  ;", "assertThat ( azure 1 Proxy . type (  )  ,    is ( Proxy . Type . HTTP )  )  ;", "assertThat ( azure 1 Proxy . address (  )  ,    is ( new   InetSocketAddress ( InetAddress . getByName (  \"  1  2  7  .  0  .  0  .  1  \"  )  ,     8  0  8  0  )  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 2  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 3  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxyHttp"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . port \"  ,     8  0  8  0  )  . put (  \" azure . client . azure 1  . proxy . type \"  ,    randomFrom (  \" socks \"  ,     \" http \"  )  )  . build (  )  ;", "SettingsException   e    =    expectThrows ( SettingsException . class ,     (  )     -  >    new   MockForSettings ( settings )  )  ;", "assertEquals (  \" Azure   Proxy   type   has   been   set   but   proxy   host   or   port   is   not   defined .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxyNoHost"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . host \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  . put (  \" azure . client . azure 1  . proxy . type \"  ,    randomFrom (  \" socks \"  ,     \" http \"  )  )  . build (  )  ;", "SettingsException   e    =    expectThrows ( SettingsException . class ,     (  )     -  >    new   MockForSettings ( settings )  )  ;", "assertEquals (  \" Azure   Proxy   type   has   been   set   but   proxy   host   or   port   is   not   defined .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxyNoPort"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . host \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  . put (  \" azure . client . azure 1  . proxy . port \"  ,     8  0  8  0  )  . build (  )  ;", "SettingsException   e    =    expectThrows ( SettingsException . class ,     (  )     -  >    new   MockForSettings ( settings )  )  ;", "assertEquals (  \" Azure   Proxy   port   or   host   have   been   set   but   proxy   type   is   not   defined .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxyNoType"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . host \"  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  . put (  \" azure . client . azure 1  . proxy . port \"  ,     8  0  8  0  )  . put (  \" azure . client . azure 1  . proxy . type \"  ,     \" socks \"  )  . build (  )  ;", ". AzureStorageServiceMockForSettings   mock    =    new    . AzureStorageServiceMockForSettings ( settings )  ;", "Proxy   azure 1 Proxy    =    mock . storageSettings . get (  \" azure 1  \"  )  . getProxy (  )  ;", "assertThat ( azure 1 Proxy ,    notNullValue (  )  )  ;", "assertThat ( azure 1 Proxy . type (  )  ,    is ( Proxy . Type . SOCKS )  )  ;", "assertThat ( azure 1 Proxy . address (  )  ,    is ( new   InetSocketAddress ( InetAddress . getByName (  \"  1  2  7  .  0  .  0  .  1  \"  )  ,     8  0  8  0  )  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 2  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "assertThat ( mock . storageSettings . get (  \" azure 3  \"  )  . getProxy (  )  ,    nullValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxySocks"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( buildSecureSettings (  )  )  . put (  \" azure . client . azure 1  . proxy . type \"  ,    randomFrom (  \" socks \"  ,     \" http \"  )  )  . put (  \" azure . client . azure 1  . proxy . host \"  ,     \" thisisnotavalidhostorwehavebeensuperunlucky \"  )  . put (  \" azure . client . azure 1  . proxy . port \"  ,     8  0  8  0  )  . build (  )  ;", "SettingsException   e    =    expectThrows ( SettingsException . class ,     (  )     -  >    new   MockForSettings ( settings )  )  ;", "assertEquals (  \" Azure   proxy   host   is   unknown .  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxyWrongHost"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  \" azure . client . azure 1  . account \"  ,     \" myaccount 1  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 1  . key \"  ,     \" mykey 1  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 2  . account \"  ,     \" myaccount 2  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 2  . key \"  ,     \" mykey 2  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 3  . account \"  ,     \" myaccount 3  \"  )  ;", "secureSettings . setString (  \" azure . client . azure 3  . key \"  ,     \" mykey 3  \"  )  ;", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( secureSettings )  . put (  \" azure . client . azure 3  . endpoint _ suffix \"  ,     \" my _ endpoint _ suffix \"  )  . build (  )  ;", "Map < String ,    ttings >    loadedSettings    =    ttings . load ( settings )  ;", "assertThat ( loadedSettings . keySet (  )  ,    containsInAnyOrder (  \" azure 1  \"  ,     \" azure 2  \"  ,     \" azure 3  \"  ,     \" default \"  )  )  ;", "assertThat ( loadedSettings . get (  \" azure 1  \"  )  . getEndpointSuffix (  )  ,    isEmptyString (  )  )  ;", "assertThat ( loadedSettings . get (  \" azure 2  \"  )  . getEndpointSuffix (  )  ,    isEmptyString (  )  )  ;", "assertThat ( loadedSettings . get (  \" azure 3  \"  )  . getEndpointSuffix (  )  ,    equalTo (  \" my _ endpoint _ suffix \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testReadSecuredSettings"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "return   account ;", "}", "METHOD_END"], "methodName": ["getAccount"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "try    ( SecureString   account    =    getConfigValue ( settings ,    clientName ,    AzureStorageSettings . ACCOUNT _ SETTING )  ; SecureString   key    =    getConfigValue ( settings ,    clientName ,    AzureStorageSettings . KEY _ SETTING )  )     {", "return   new   AzureStorageSettings ( account . toString (  )  ,    key . toString (  )  ,    AzureStorageSettings . getValue ( settings ,    clientName ,    AzureStorageSettings . ENDPOINT _ SUFFIX _ SETTING )  ,    AzureStorageSettings . getValue ( settings ,    clientName ,    AzureStorageSettings . TIMEOUT _ SETTING )  ,    AzureStorageSettings . getValue ( settings ,    clientName ,    AzureStorageSettings . MAX _ RETRIES _ SETTING )  ,    AzureStorageSettings . getValue ( settings ,    clientName ,    AzureStorageSettings . PROXY _ TYPE _ SETTING )  ,    AzureStorageSettings . getValue ( settings ,    clientName ,    AzureStorageSettings . PROXY _ HOST _ SETTING )  ,    AzureStorageSettings . getValue ( settings ,    clientName ,    AzureStorageSettings . PROXY _ PORT _ SETTING )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getClientSettings"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "Setting < T >    concreteSetting    =    clientSetting . getConcreteSettingForNamespace ( clientName )  ;", "return   concreteSetting . get ( settings )  ;", "}", "METHOD_END"], "methodName": ["getConfigValue"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "return   endpointSuffix ;", "}", "METHOD_END"], "methodName": ["getEndpointSuffix"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "return   key ;", "}", "METHOD_END"], "methodName": ["getKey"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "return   maxRetries ;", "}", "METHOD_END"], "methodName": ["getMaxRetries"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "return   proxy ;", "}", "METHOD_END"], "methodName": ["getProxy"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "return   timeout ;", "}", "METHOD_END"], "methodName": ["getTimeout"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "Setting . AffixKey   k    =     (  ( Setting . AffixKey )     ( setting . getRawKey (  )  )  )  ;", "String   fullKey    =    k . toConcreteKey ( groupName )  . toString (  )  ;", "return   setting . getConcret ( fullKey )  . get ( settings )  ;", "}", "METHOD_END"], "methodName": ["getValue"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    AzureStorageSettings >    storageSettings    =    new   HashMap <  >  (  )  ;", "for    ( String   clientName    :    AzureStorageSettings . ACCOUNT _ SETTING . getNamespaces ( settings )  )     {", "storageSettings . put ( clientName ,    AzureStorageSettings . getClientSettings ( settings ,    clientName )  )  ;", "}", "if    (  (  ( storageSettings . containsKey (  \" default \"  )  )     =  =    false )     &  &     (  ( storageSettings . isEmpty (  )  )     =  =    false )  )     {", "AzureStorageSettings   defaultSettings    =    storageSettings . values (  )  . iterator (  )  . next (  )  ;", "storageSettings . put (  \" default \"  ,    defaultSettings )  ;", "}", "return   Collections . unmodifiableMap ( storageSettings )  ;", "}", "METHOD_END"], "methodName": ["load"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageSettings"}, {"methodBody": ["METHOD_START", "{", "containers . put ( containerName ,    new   AzureStorageTestServer . Container ( containerName )  )  ;", "}", "METHOD_END"], "methodName": ["createContainer"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   PathTrie < AzureStorageTestServer . RequestHandler >    handlers    =    new   PathTrie ( RestUtils . REST _ DECODER )  ;", "AzureStorageTestServer . objectsPaths (  (  (  \" HEAD    \"     +    endpoint )     +     \"  /  { container }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    requestId )     -  >     {", "final   String   containerName    =    params . get (  \" container \"  )  ;", "final   Container   container    =    containers . get ( containerName )  ;", "if    ( container    =  =    null )     {", "return   newContainerNotFoundError ( requestId )  ;", "}", "final   String   blobName    =    objectName ( params )  ;", "for    ( Entry < String ,    byte [  ]  >    object    :    container . objects . entrySet (  )  )     {", "if    ( object . getKey (  )  . equals ( blobName )  )     {", "Map < String ,    String >    responseHeaders    =    new   HashMap <  >  (  )  ;", "responseHeaders . put (  \" x - ms - blob - content - length \"  ,    String . valueOf ( object . getValue (  )  . length )  )  ;", "responseHeaders . put (  \" x - ms - blob - type \"  ,     \" blockblob \"  )  ;", "return   new   Response ( RestStatus . OK ,    responseHeaders ,     \" text / plain \"  ,    AzureStorageTestServer . EMPTY _ BYTE )  ;", "}", "}", "return   newBlobNotFoundError ( requestId )  ;", "}  )  )  ;", "AzureStorageTestServer . objectsPaths (  (  (  \" PUT    \"     +    endpoint )     +     \"  /  { container }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    requestId )     -  >     {", "final   String   destContainerName    =    params . get (  \" container \"  )  ;", "final   Container   destContainer    =    containers . get ( destContainerName )  ;", "if    ( destContainer    =  =    null )     {", "return   newContainerNotFoundError ( requestId )  ;", "}", "final   String   destBlobName    =    objectName ( params )  ;", "List < String >    headerCopySource    =    headers . getOrDefault (  \" x - ms - copy - source \"  ,    emptyList (  )  )  ;", "if    (  ( headerCopySource . isEmpty (  )  )     =  =    false )     {", "String   srcBlobName    =    headerCopySource . get (  0  )  ;", "Container   srcContainer    =    null ;", "for    ( Container   container    :    containers . values (  )  )     {", "String   prefix    =     (  ( endpoint    +     \"  /  \"  )     +    container . name )     +     \"  /  \"  ;", "if    ( srcBlobName . startsWith ( prefix )  )     {", "srcBlobName    =    srcBlobName . replaceFirst ( prefix ,     \"  \"  )  ;", "srcContainer    =    container ;", "break ;", "}", "}", "if    (  ( srcContainer    =  =    null )     |  |     (  ( srcContainer . objects . containsKey ( srcBlobName )  )     =  =    false )  )     {", "return   newBlobNotFoundError ( requestId )  ;", "}", "byte [  ]    bytes    =    srcContainer . objects . get ( srcBlobName )  ;", "if    ( bytes    !  =    null )     {", "destContainer . objects . put ( destBlobName ,    bytes )  ;", "return   new   Response ( RestStatus . ACCEPTED ,    singletonMap (  \" x - ms - copy - status \"  ,     \" success \"  )  ,     \" text / plain \"  ,    AzureStorageTestServer . EMPTY _ BYTE )  ;", "} else    {", "return   newBlobNotFoundError ( requestId )  ;", "}", "} else    {", "destContainer . objects . put ( destBlobName ,    body )  ;", "}", "return   new   Response ( RestStatus . CREATED ,    emptyMap (  )  ,     \" text / plain \"  ,    AzureStorageTestServer . EMPTY _ BYTE )  ;", "}  )  )  ;", "AzureStorageTestServer . objectsPaths (  (  (  \" GET    \"     +    endpoint )     +     \"  /  { container }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    requestId )     -  >     {", "final   String   containerName    =    params . get (  \" container \"  )  ;", "final   Container   container    =    containers . get ( containerName )  ;", "if    ( container    =  =    null )     {", "return   newContainerNotFoundError ( requestId )  ;", "}", "final   String   blobName    =    objectName ( params )  ;", "if    ( container . objects . containsKey ( blobName )  )     {", "Map < String ,    String >    responseHeaders    =    new   HashMap <  >  (  )  ;", "responseHeaders . put (  \" x - ms - copy - status \"  ,     \" success \"  )  ;", "responseHeaders . put (  \" x - ms - blob - type \"  ,     \" blockblob \"  )  ;", "return   new   Response ( RestStatus . OK ,    responseHeaders ,     \" application / octet - stream \"  ,    container . objects . get ( blobName )  )  ;", "}", "return   newBlobNotFoundError ( requestId )  ;", "}  )  )  ;", "AzureStorageTestServer . objectsPaths (  (  (  \" DELETE    \"     +    endpoint )     +     \"  /  { container }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    requestId )     -  >     {", "final   String   containerName    =    params . get (  \" container \"  )  ;", "final   Container   container    =    containers . get ( containerName )  ;", "if    ( container    =  =    null )     {", "return   newContainerNotFoundError ( requestId )  ;", "}", "final   String   blobName    =    objectName ( params )  ;", "if    (  ( container . objects . remove ( blobName )  )     !  =    null )     {", "return   new   Response ( RestStatus . ACCEPTED ,    emptyMap (  )  ,     \" text / plain \"  ,    AzureStorageTestServer . EMPTY _ BYTE )  ;", "}", "return   newBlobNotFoundError ( requestId )  ;", "}  )  )  ;", "handlers . insert (  (  (  \" GET    \"     +    endpoint )     +     \"  /  { container }  /  \"  )  ,     (    params ,    headers ,    body ,    requestId )     -  >     {", "final   String   containerName    =    params . get (  \" container \"  )  ;", "final   Container   container    =    containers . get ( containerName )  ;", "if    ( container    =  =    null )     {", "return   newContainerNotFoundError ( requestId )  ;", "}", "final   String   prefix    =    params . get (  \" prefix \"  )  ;", "return   newEnumerationResultsResponse ( requestId ,    container ,    prefix )  ;", "}  )  ;", "handlers . insert (  (  (  \" HEAD    \"     +    endpoint )     +     \"  /  { container }  \"  )  ,     (    params ,    headers ,    body ,    requestId )     -  >     {", "String   container    =    params . get (  \" container \"  )  ;", "if    (  ( Strings . hasText ( container )  )     &  &     ( containers . containsKey ( container )  )  )     {", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" text / plain \"  ,    AzureStorageTestServer . EMPTY _ BYTE )  ;", "} else    {", "return   newContainerNotFoundError ( requestId )  ;", "}", "}  )  ;", "return   handlers ;", "}", "METHOD_END"], "methodName": ["defaultHandlers"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   endpoint ;", "}", "METHOD_END"], "methodName": ["getEndpoint"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   long   requestId    =    requests . incrementAndGet (  )  ;", "final   Map < String ,    String >    params    =    new   HashMap <  >  (  )  ;", "if    ( query    !  =    null )     {", "RestUtils . decodeQueryString ( query ,     0  ,    params )  ;", "}", "final    . RequestHandler   handler    =    handlers . retrieve (  (  ( method    +     \"     \"  )     +    path )  ,    params )  ;", "if    ( handler    !  =    null )     {", "return   handler . execute ( params ,    headers ,    body ,    requestId )  ;", "} else    {", "return    . newInternalError ( requestId )  ;", "}", "}", "METHOD_END"], "methodName": ["handle"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   AzureStorageTestServer . newError ( requestId ,    NOT _ FOUND ,     \" BlobNotFound \"  ,     \" The   specified   blob   does   not   exist \"  )  ;", "}", "METHOD_END"], "methodName": ["newBlobNotFoundError"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   AzureStorageTestServer . newError ( requestId ,    NOT _ FOUND ,     \" ContainerNotFound \"  ,     \" The   specified   container   does   not   exist \"  )  ;", "}", "METHOD_END"], "methodName": ["newContainerNotFoundError"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   String   id    =    Long . toString ( requestId )  ;", "final   StringBuilder   response    =    new   StringBuilder (  )  ;", "response . append (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" utf -  8  \\  \"  ?  >  \"  )  ;", "response . append (  \"  < EnumerationResults   ServiceEndpoint =  \\  \" http :  /  / myaccount . blob . core . windows . net /  \\  \"  \"  )  ;", "response . append (  \"    ContainerName =  \\  \"  \"  )  . append ( container . name )  . append (  \"  \\  \"  >  \"  )  ;", "if    ( prefix    !  =    null )     {", "response . append (  \"  < Prefix >  \"  )  . append ( prefix )  . append (  \"  <  / Prefix >  \"  )  ;", "} else    {", "response . append (  \"  < Prefix /  >  \"  )  ;", "}", "response . append (  \"  < MaxResults >  \"  )  . append ( container . objects . size (  )  )  . append (  \"  <  / MaxResults >  \"  )  ;", "response . append (  \"  < Blobs >  \"  )  ;", "int   count    =     0  ;", "for    ( Map . Entry < String ,    byte [  ]  >    object    :    container . objects . entrySet (  )  )     {", "String   objectName    =    object . getKey (  )  ;", "if    (  ( prefix    =  =    null )     |  |     ( objectName . startsWith ( prefix )  )  )     {", "response . append (  \"  < Blob >  \"  )  ;", "response . append (  \"  < Name >  \"  )  . append ( objectName )  . append (  \"  <  / Name >  \"  )  ;", "response . append (  \"  < Properties >  \"  )  ;", "response . append (  \"  < Content - Length >  \"  )  . append ( object . getValue (  )  . length )  . append (  \"  <  / Content - Length >  \"  )  ;", "response . append (  \"  < CopyId >  \"  )  . append (  ( count +  +  )  )  . append (  \"  <  / CopyId >  \"  )  ;", "response . append (  \"  < CopyStatus > success <  / CopyStatus >  \"  )  ;", "response . append (  \"  < BlobType > BlockBlob <  / BlobType >  \"  )  ;", "response . append (  \"  <  / Properties >  \"  )  ;", "response . append (  \"  <  / Blob >  \"  )  ;", "}", "}", "response . append (  \"  <  / Blobs >  \"  )  ;", "response . append (  \"  < NextMarker    /  >  \"  )  ;", "response . append (  \"  <  / EnumerationResults >  \"  )  ;", "return   new    . Response ( RestStatus . OK ,    Collections . singletonMap (  \" x - amz - request - id \"  ,    id )  ,     \" application / xml \"  ,    response . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["newEnumerationResultsResponse"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   StringBuilder   response    =    new   StringBuilder (  )  ;", "response . append (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" UTF -  8  \\  \"  ?  >  \"  )  ;", "response . append (  \"  < Error >  \"  )  ;", "response . append (  \"  < Code >  \"  )  . append ( code )  . append (  \"  <  / Code >  \"  )  ;", "response . append (  \"  < Message >  \"  )  . append ( message )  . append (  \"  <  / Message >  \"  )  ;", "response . append (  \"  <  / Error >  \"  )  ;", "final   Map < String ,    String >    headers    =    new   HashMap <  >  (  2  )  ;", "headers . put (  \" x - ms - request - id \"  ,    String . valueOf ( requestId )  )  ;", "headers . put (  \" x - ms - error - code \"  ,    code )  ;", "return   new    . Response ( status ,    headers ,     \" application / xml \"  ,    response . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["newError"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   AzureStorageTestServer . newError ( requestId ,    INTERNAL _ SERVER _ ERROR ,     \" InternalError \"  ,     \" The   server   encountered   an   internal   error \"  )  ;", "}", "METHOD_END"], "methodName": ["newInternalError"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   StringBuilder   name    =    new   StringBuilder (  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "String   value    =    params . getOrDefault (  (  \" path \"     +    i )  ,    null )  ;", "if    ( value    !  =    null )     {", "if    (  ( name . length (  )  )     >     0  )     {", "name . append (  '  /  '  )  ;", "}", "name . append ( value )  ;", "}", "}", "return   name . toString (  )  ;", "}", "METHOD_END"], "methodName": ["objectName"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   List < String >    paths    =    new   ArrayList <  >  (  )  ;", "String   p    =    path ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "p    =     (  ( p    +     \"  /  { path \"  )     +    i )     +     \"  }  \"  ;", "pathdd ( p )  ;", "}", "return   paths ;", "}", "METHOD_END"], "methodName": ["objectsPaths"], "fileName": "org.elasticsearch.repositories.azure.AzureStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "if    (  ( Strings . isEmpty ( System . getProperty (  \" testaccount \"  )  )  )     |  |     ( Strings . isEmpty ( System . getProperty (  \" testkey \"  )  )  )  )     {", "throw   new   IllegalStateException (  (  \" to   run   integration   tests ,    you   need   to   set    - Dtests . thirdparty = true   and    \"     +     \"  - Dtestaccount = azure - account    - Dtestkey = azure - key \"  )  )  ;", "}", "secureSettings . setString (  \" azure . client . default . account \"  ,    System . getProperty (  \" testaccount \"  )  )  ;", "secureSettings . setString (  \" azure . client . default . key \"  ,    System . getProperty (  \" testkey \"  )  )  ;", "return   secureSettings ;", "}", "METHOD_END"], "methodName": ["generateMockSecureSettings"], "fileName": "org.elasticsearch.repositories.azure.AzureTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.repositories.azure.RepositoryAzureClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   Controller . doPrivileged ( operation )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( StorageException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedException"], "fileName": "org.elasticsearch.repositories.azure.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   Controller . doPrivileged ( operation )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedIOException"], "fileName": "org.elasticsearch.repositories.azure.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "Controller . doPrivileged (  (  ( PrivilegedExceptionAction < Void >  )     (  (  )     -  >     {", "action . executeCouldThrow (  )  ;", "return   null ;", "}  )  )  )  ;", "}    catch    ( PrivilegedActionException   e )     {", "Throwable   cause    =    e . getCause (  )  ;", "if    ( cause   instanceof   StorageException )     {", "throw    (  ( StorageException )     ( cause )  )  ;", "} else    {", "throw    (  ( URISyntaxException )     ( cause )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedVoidException"], "fileName": "org.elasticsearch.repositories.azure.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "assert   blobName    !  =    null ;", "return    ( path )     +    blobName ;", "}", "METHOD_END"], "methodName": ["buildKey"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobContainer"}, {"methodBody": ["METHOD_START", "{", "try    {", "Object   blob    =    SocketAccess . doPrivilegedIOException (  (  )     -  >    client . objects (  )  . get ( bucket ,    blobName )  . execute (  )  )  ;", "if    ( blob    !  =    null )     {", "return   Strings . hasText ( blob . getId (  )  )  ;", "}", "}    catch    ( GoogleJsonResponseException   e )     {", "GoogleJsonError   error    =    e . getDetails (  )  ;", "if    (  (  ( e . getStatusCode (  )  )     =  =     ( HttpURLConnection . HTTP _ NOT _ FOUND )  )     |  |     (  ( error    !  =    null )     &  &     (  ( error . getCode (  )  )     =  =     ( HttpURLConnection . HTTP _ NOT _ FOUND )  )  )  )     {", "return   false ;", "}", "throw   e ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["blobExists"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   StreamSupport . stream ( new   GoogleCloudStorageBlobStore . StorageObjectsSpliterator ( client ,    bucketName ,    prefix ,    pageSize )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["blobsStream"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "assert   s    !  =    null ;", "return   keyPath    +    s ;", "}", "METHOD_END"], "methodName": ["buildKey"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( blobExists ( blobName )  )  )     {", "throw   new   NoSuchFileException (  (  (  \"     [  \"     +    blobName )     +     \"  ]    does   not   exist \"  )  )  ;", "}", "SocketAccess . doPrivilegedIOException (  (  )     -  >    client . objects (  )  . delete ( bucket ,    blobName )  . execute (  )  )  ;", "}", "METHOD_END"], "methodName": ["deleteBlob"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( blobNames    =  =    null )     |  |     ( blobNames . isEmpty (  )  )  )     {", "return ;", "}", "if    (  ( blobNames . size (  )  )     =  =     1  )     {", "deleteBlob ( blobNames . iterator (  )  . next (  )  )  ;", "return ;", "}", "final   List < Storage . Objects . Delete >    deletions    =    new   ArrayList <  >  ( Math . min (  . MAX _ BATCHING _ REQUESTS ,    blobNames . size (  )  )  )  ;", "final   Iterator < String >    blobs    =    blobNames . iterator (  )  ;", "SocketAccess . doPrivilegedVoidIOException (  (  )     -  >     {", "while    ( blobs . hasNext (  )  )     {", "deletions . add ( client . objects (  )  . delete ( bucket ,    blobs . next (  )  )  )  ;", "if    (  (  ( blobs . hasNext (  )  )     =  =    false )     |  |     (  ( deletions . size (  )  )     =  =     (  . MAX _ BATCHING _ REQUESTS )  )  )     {", "try    {", "BatchRequest   batch    =    client . batch (  )  ;", "CountDown   countDown    =    new   CountDown ( deletions . size (  )  )  ;", "for    ( Storage . Objects . Delete   delete    :    deletions )     {", "delete . queue ( batch ,    new   JsonBatchCallback < Void >  (  )     {", "@ Override", "public   void   onFailure ( GoogleJsonError   e ,    HttpHeaders   responseHeaders )    throws   IOException    {", "logger . error (  \" failed   to   delete   blob    [  {  }  ]    in   bucket    [  {  }  ]  :     {  }  \"  ,    delete . getObject (  )  ,    delete . getBucket (  )  ,    e . getMessage (  )  )  ;", "}", "@ Override", "public   void   onSuccess ( Void   aVoid ,    HttpHeaders   responseHeaders )    throws   IOException    {", "countDown . countDown (  )  ;", "}", "}  )  ;", "}", "batch . execute (  )  ;", "if    (  ( countDown . isCountedDown (  )  )     =  =    false )     {", "throw   new   IOException (  (  (  \" Failed   to   delete   all    [  \"     +     ( deletions . size (  )  )  )     +     \"  ]    blobs \"  )  )  ;", "}", "}    finally    {", "deletions . clear (  )  ;", "}", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["deleteBlobs"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "deleteBlobs ( listBlobsByPath ( bucket ,    prefix ,    null )  . keySet (  )  )  ;", "}", "METHOD_END"], "methodName": ["deleteBlobsByPrefix"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   SocketAccess . doPrivilegedIOException (  (  )     -  >     {", "try    {", "Bucket   bucket    =    client . buckets (  )  . get ( bucketName )  . execute (  )  ;", "if    ( bucket    !  =    null )     {", "return   Strings . hasText ( bucket . getId (  )  )  ;", "}", "}    catch    ( GoogleJsonResponseException   e )     {", "GoogleJsonError   error    =    e . getDetails (  )  ;", "if    (  (  ( e . getStatusCode (  )  )     =  =     ( HttpURLConnection . HTTP _ NOT _ FOUND )  )     |  |     (  ( error    !  =    null )     &  &     (  ( error . getCode (  )  )     =  =     ( HttpURLConnection . HTTP _ NOT _ FOUND )  )  )  )     {", "return   false ;", "}", "throw   e ;", "}", "return   false ;", "}  )  ;", "}    catch    ( IOException   e )     {", "throw   new   Exception (  (  (  \" Unable   to   check   if   bucket    [  \"     +    bucketName )     +     \"  ]    exists \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["doesBucketExist"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   SocketAccess . doPrivilegedIOException (  (  )     -  >    listBlobsByPath ( bucket ,    path ,    path )  )  ;", "}", "METHOD_END"], "methodName": ["listBlobs"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   GoogleCloudStorageBlobStore . blobsStream ( client ,    bucketName ,    path ,    GoogleCloudStorageBlobStore . MAX _ BATCHING _ REQUESTS )  . map ( new   GoogleCloudStorageBlobStore . BlobMetaDataConverter ( pathToRemove )  )  . collect ( Collectors . toMap ( PlainBlobMetaData :  : name ,    Function . identity (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["listBlobsByPath"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   SocketAccess . doPrivilegedIOException (  (  )     -  >    listBlobsByPath ( bucket ,    buildKey ( path ,    prefix )  ,    path )  )  ;", "}", "METHOD_END"], "methodName": ["listBlobsByPrefix"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "SocketAccess . doPrivilegedIOException (  (  )     -  >     {", "client . objects (  )  . copy ( bucket ,    sourc ,    bucket ,    targetBlob ,    null )  . execute (  )  ;", "client . objects (  )  . delete ( bucket ,    sourc )  . execute (  )  ;", "return   null ;", "}  )  ;", "}", "METHOD_END"], "methodName": ["moveBlob"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   SocketAccess . doPrivilegedIOException (  (  )     -  >     {", ". Objects . Get   object    =    client . objects (  )  . get ( bucket ,    blobName )  ;", "return   object . executeMediaAsInputStream (  )  ;", "}  )  ;", "}    catch    ( GoogleJsonResponseException   e )     {", "GoogleJsonError   error    =    e . getDetails (  )  ;", "if    (  (  ( e . getStatusCode (  )  )     =  =     ( HttpURLConnection . HTTP _ NOT _ FOUND )  )     |  |     (  ( error    !  =    null )     &  &     (  ( error . getCode (  )  )     =  =     ( HttpURLConnection . HTTP _ NOT _ FOUND )  )  )  )     {", "throw   new   NoSuchFileException ( e . getMessage (  )  )  ;", "}", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["readBlob"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "SocketAccess . doPrivilegedVoidIOException (  (  )     -  >     {", "InputStreamContent   stream    =    new   InputStreamContent ( null ,    inputStream )  ;", "stream . setLength ( blobSize )  ;", ". Objects . Insert   insert    =    client . objects (  )  . insert ( bucket ,    null ,    stream )  ;", "insert . setName ( blobName )  ;", "insert . execute (  )  ;", "}  )  ;", "}", "METHOD_END"], "methodName": ["writeBlob"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStore"}, {"methodBody": ["METHOD_START", "{", "RepositoryMetaData   repositoryMetaData    =    new   RepositoryMetaData (  \" repo \"  ,    GoogleCloudStorageRepository . TYPE ,    Settings . EMPTY )  ;", "ByteSizeValue   chunkSize    =    GoogleCloudStorageRepository . getSetting ( GoogleCloudStorageRepository . CHUNK _ SIZE ,    repositoryMetaData )  ;", "assertEquals ( GoogleCloudStorageRepository . MAX _ CHUNK _ SIZE ,    chunkSize )  ;", "int   size    =    randomIntBetween (  1  ,     1  0  0  )  ;", "repositoryMetaData    =    new   RepositoryMetaData (  \" repo \"  ,    GoogleCloudStorageRepository . TYPE ,    Settings . builder (  )  . put (  \" chunk _ size \"  ,     ( size    +     \" mb \"  )  )  . build (  )  )  ;", "chunkSize    =    GoogleCloudStorageRepository . getSetting ( GoogleCloudStorageRepository . CHUNK _ SIZE ,    repositoryMetaData )  ;", "assertEquals ( new   ByteSizeValue ( size ,    ByteSizeUnit . MB )  ,    chunkSize )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >     {", "RepositoryMetaData   repoMetaData    =    new   RepositoryMetaData (  \" repo \"  ,    GoogleCloudStorageRepository . TYPE ,    Settings . builder (  )  . put (  \" chunk _ size \"  ,     \"  0  \"  )  . build (  )  )  ;", "GoogleCloudStorageRepository . getSetting ( GoogleCloudStorageRepository . CHUNK _ SIZE ,    repoMetaData )  ;", "}  )  ;", "assertEquals (  \" failed   to   parse   value    [  0  ]    for   setting    [ chunk _ size ]  ,    must   be    >  =     [  1 b ]  \"  ,    e . getMessage (  )  )  ;", "e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >     {", "RepositoryMetaData   repoMetaData    =    new   RepositoryMetaData (  \" repo \"  ,    GoogleCloudStorageRepository . TYPE ,    Settings . builder (  )  . put (  \" chunk _ size \"  ,     \"  -  1  \"  )  . build (  )  )  ;", "GoogleCloudStorageRepository . getSetting ( GoogleCloudStorageRepository . CHUNK _ SIZE ,    repoMetaData )  ;", "}  )  ;", "assertEquals (  \" failed   to   parse   value    [  -  1  ]    for   setting    [ chunk _ size ]  ,    must   be    >  =     [  1 b ]  \"  ,    e . getMessage (  )  )  ;", "e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >     {", "RepositoryMetaData   repoMetaData    =    new   RepositoryMetaData (  \" repo \"  ,    GoogleCloudStorageRepository . TYPE ,    Settings . builder (  )  . put (  \" chunk _ size \"  ,     \"  1  0  1 mb \"  )  . build (  )  )  ;", "GoogleCloudStorageRepository . getSetting ( GoogleCloudStorageRepository . CHUNK _ SIZE ,    repoMetaData )  ;", "}  )  ;", "assertEquals (  \" failed   to   parse   value    [  1  0  1 mb ]    for   setting    [ chunk _ size ]  ,    must   be    <  =     [  1  0  0 mb ]  \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testChunkSize"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStoreRepositoryTests"}, {"methodBody": ["METHOD_START", "{", "GoogleCloudStorageBlobStoreRepositoryTests . blobs . clear (  )  ;", "}", "METHOD_END"], "methodName": ["wipeRepository"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageBlobStoreRepositoryTests"}, {"methodBody": ["METHOD_START", "{", "return   applicationName ;", "}", "METHOD_END"], "methodName": ["getApplicationName"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "return   new   GoogleCloudStorageClientSettings ( GoogleCloudStorageClientSettings . loadCredential ( settings ,    clientName )  ,    getConfigValue ( settings ,    clientName ,    ENDPOINT _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    CONNECT _ TIMEOUT _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    READ _ TIMEOUT _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    APPLICATION _ NAME _ SETTING )  )  ;", "}", "METHOD_END"], "methodName": ["getClientSettings"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "Setting < T >    concreteSetting    =    clientSetting . getConcreteSettingForNamespace ( clientName )  ;", "return   concreteSetting . get ( settings )  ;", "}", "METHOD_END"], "methodName": ["getConfigValue"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "return   connectTimeout ;", "}", "METHOD_END"], "methodName": ["getConnectTimeout"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "return   credential ;", "}", "METHOD_END"], "methodName": ["getCredential"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "return   endpoint ;", "}", "METHOD_END"], "methodName": ["getEndpoint"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "return   readTimeout ;", "}", "METHOD_END"], "methodName": ["getReadTimeout"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "final   Map < String ,    GoogleCloudStorageClientSettings >    clients    =    new   HashMap <  >  (  )  ;", "for    ( String   clientName    :    settings . getGroups ( GoogleCloudStorageClientSettings . PREFIX )  . keySet (  )  )     {", "clients . put ( clientName ,    GoogleCloudStorageClientSettings . getClientSettings ( settings ,    clientName )  )  ;", "}", "if    (  ( clients . containsKey (  \" default \"  )  )     =  =    false )     {", "clients . put (  \" default \"  ,    GoogleCloudStorageClientSettings . getClientSettings ( settings ,     \" default \"  )  )  ;", "}", "return   Collections . unmodifiableMap ( clients )  ;", "}", "METHOD_END"], "methodName": ["load"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  ( CREDENTIALS _ FILE _ SETTING . getConcreteForNamespace ( clientName )  . exists ( settings )  )     =  =    false )     {", "return   null ;", "}", "try    ( InputStream   credStream    =    CREDENTIALS _ FILE _ SETTING . getConcreteForNamespace ( clientName )  . get ( settings )  )     {", "GoogleCredential   credential    =    GoogleCredential . fromStream ( credStream )  ;", "if    ( credential . createScopedRequired (  )  )     {", "credential    =    credential . createScoped ( Collections . singleton ( DEVSTORAGE _ FULL _ CONTROL )  )  ;", "}", "return   credential ;", "}", "}    catch    ( IOException   e )     {", "throw   new   UncheckedIOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["loadCredential"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettings"}, {"methodBody": ["METHOD_START", "{", "if    ( expected    !  =    null )     {", "assertEquals ( expected . gerviceAccountUser (  )  ,    actual . gerviceAccountUser (  )  )  ;", "assertEquals ( expected . gerviceAccountId (  )  ,    actual . gerviceAccountId (  )  )  ;", "assertEquals ( expected . gerviceAccountProjectId (  )  ,    actual . gerviceAccountProjectId (  )  )  ;", "assertEquals ( expected . gerviceAccountScopesAsString (  )  ,    actual . gerviceAccountScopesAsString (  )  )  ;", "assertEquals ( expected . gerviceAccountPrivateKey (  )  ,    actual . gerviceAccountPrivateKey (  )  )  ;", "assertEquals ( expected . gerviceAccountPrivateKeyId (  )  ,    actual . gerviceAccountPrivateKeyId (  )  )  ;", "} else    {", "assertNull ( actual )  ;", "}", "}", "METHOD_END"], "methodName": ["assertGoogleCredential"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "Tuple < GoogleCredential ,    byte [  ]  >    credentials    =    GoogleCloudStorageClientSettingsTests . randomCredential ( clientName )  ;", "GoogleCredential   credential    =    credentials . v 1  (  )  ;", "secureSettings . setFile ( GoogleCloudStorageClientSettings . CREDENTIALS _ FILE _ SETTING . getConcreteSettingForNamespace ( clientName )  . getKey (  )  ,    credentials . v 2  (  )  )  ;", "String   endpoint ;", "if    ( randomBoolean (  )  )     {", "endpoint    =    randomAlphaOfLength (  5  )  ;", "settings . put ( GoogleCloudStorageClientSettings . ENDPOINT _ SETTING . getConcreteSettingForNamespace ( clientName )  . getKey (  )  ,    endpoint )  ;", "} else    {", "endpoint    =    GoogleCloudStorageClientSettings . ENDPOINT _ SETTING . getDefault ( EMPTY )  ;", "}", "TimeValue   connectTimeout ;", "if    ( randomBoolean (  )  )     {", "connectTimeout    =    GoogleCloudStorageClientSettingsTests . randomTimeout (  )  ;", "settings . put ( GoogleCloudStorageClientSettings . CONNECT _ TIMEOUT _ SETTING . getConcreteSettingForNamespace ( clientName )  . getKey (  )  ,    connectTimeout . getStringRep (  )  )  ;", "} else    {", "connectTimeout    =    GoogleCloudStorageClientSettings . CONNECT _ TIMEOUT _ SETTING . getDefault ( EMPTY )  ;", "}", "TimeValue   readTimeout ;", "if    ( randomBoolean (  )  )     {", "readTimeout    =    GoogleCloudStorageClientSettingsTests . randomTimeout (  )  ;", "settings . put ( GoogleCloudStorageClientSettings . READ _ TIMEOUT _ SETTING . getConcreteSettingForNamespace ( clientName )  . getKey (  )  ,    readTimeout . getStringRep (  )  )  ;", "} else    {", "readTimeout    =    GoogleCloudStorageClientSettings . READ _ TIMEOUT _ SETTING . getDefault ( EMPTY )  ;", "}", "String   applicationName ;", "if    ( randomBoolean (  )  )     {", "applicationName    =    randomAlphaOfLength (  5  )  ;", "settings . put ( GoogleCloudStorageClientSettings . APPLICATION _ NAME _ SETTING . getConcreteSettingForNamespace ( clientName )  . getKey (  )  ,    applicationName )  ;", "} else    {", "applicationName    =    GoogleCloudStorageClientSettings . APPLICATION _ NAME _ SETTING . getDefault ( EMPTY )  ;", "}", "return   new   GoogleCloudStorageClientSettings ( credential ,    endpoint ,    connectTimeout ,    readTimeout ,    applicationName )  ;", "}", "METHOD_END"], "methodName": ["randomClient"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "final   Map < String ,    GoogleCloudStorageClientSettings >    expectedClients    =    new   HashMap <  >  (  )  ;", "expectedClients . put (  \" default \"  ,    GoogleCloudStorageClientSettings . getClientSettings ( EMPTY ,     \" default \"  )  )  ;", "final   Settings . Builder   settings    =    Settings . builder (  )  ;", "final   MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "for    ( int   i    =     0  ;    i    <    nbClients ;    i +  +  )     {", "String   clientName    =    randomAlphaOfLength (  5  )  . toLowerCase ( Locale . ROOT )  ;", "GoogleCloudStorageClientSettings   clientSettings    =     . randomClient ( clientName ,    settings ,    secureSettings )  ;", "expectedClients . put ( clientName ,    clientSettings )  ;", "}", "if    ( randomBoolean (  )  )     {", "GoogleCloudStorageClientSettings   clientSettings    =     . randomClient (  \" default \"  ,    settings ,    secureSettings )  ;", "expectedClients . put (  \" default \"  ,    clientSettings )  ;", "}", "return   Tuple . tuple ( expectedClients ,    settings . setSecureSettings ( secureSettings )  . build (  )  )  ;", "}", "METHOD_END"], "methodName": ["randomClients"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "KeyPair   keyPair    =    KeyPairGenerator . getInstance (  \" RSA \"  )  . generateKeyPair (  )  ;", "redential . Builder   credentialBuilder    =    new   redential . Builder (  )  ;", "credentialBuilder . setServiceAccountId ( clientName )  ;", "credentialBuilder . setServiceAccountProjectId (  (  \" project _ id _  \"     +    clientName )  )  ;", "credentialBuilder . setServiceAccountScopes ( Collections . singleton ( DEVSTORAGE _ FULL _ CONTROL )  )  ;", "credentialBuilder . setServiceAccountPrivateKey ( keyPair . getPrivate (  )  )  ;", "credentialBuilder . setServiceAccountPrivateKeyId (  (  \" private _ key _ id _  \"     +    clientName )  )  ;", "String   encodedPrivateKey    =    Base 6  4  . getEncoder (  )  . encodeToString ( keyPair . getPrivate (  )  . getEncoded (  )  )  ;", "String   serviceAccount    =     (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \"  {  \\  \" type \\  \"  :  \\  \" service _ account \\  \"  ,  \"     +     \"  \\  \" project _ id \\  \"  :  \\  \" project _ id _  \"  )     +    clientName )     +     \"  \\  \"  ,  \"  )     +     \"  \\  \" private _ key _ id \\  \"  :  \\  \" private _ key _ id _  \"  )     +    clientName )     +     \"  \\  \"  ,  \"  )     +     \"  \\  \" private _ key \\  \"  :  \\  \"  -  -  -  -  - BEGIN   PRIVATE   KEY -  -  -  -  -  \\  \\ n \"  )     +    encodedPrivateKey )     +     \"  \\  \\ n -  -  -  -  - END   PRIVATE   KEY -  -  -  -  -  \\  \\ n \\  \"  ,  \"  )     +     \"  \\  \" client _ email \\  \"  :  \\  \"  \"  )     +    clientName )     +     \"  \\  \"  ,  \"  )     +     \"  \\  \" client _ id \\  \"  :  \\  \" id _  \"  )     +    clientName )     +     \"  \\  \"  ,  \"  )     +     \"  \\  \" auth _ uri \\  \"  :  \\  \" https :  /  / accounts . google . com / o / oauth 2  / auth \\  \"  ,  \"  )     +     \"  \\  \" token _ uri \\  \"  :  \\  \" https :  /  / accounts . google . com / o / oauth 2  / token \\  \"  ,  \"  )     +     \"  \\  \" auth _ provider _ x 5  0  9  _ cert _ url \\  \"  :  \\  \" https :  /  / www . googleapis . com / oauth 2  / v 1  / certs \\  \"  ,  \"  )     +     \"  \\  \" client _ x 5  0  9  _ cert _ url \\  \"  :  \\  \" https :  /  / www . googleapis . com / robot / v 1  / metadata / x 5  0  9  /  \"  )     +    clientName )     +     \"  %  4  0 appspot . gserviceaccount . com \\  \"  }  \"  ;", "return   Tuple . tuple ( credentialBuilder . build (  )  ,    serviceAccount . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["randomCredential"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "return   randomFrom ( MINUS _ ONE ,    ZERO ,    TimeValue . parseTimeValue ( randomPositiveTimeValue (  )  ,     \" test \"  )  )  ;", "}", "METHOD_END"], "methodName": ["randomTimeout"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "final   int   nbClients    =    randomIntBetween (  1  ,     5  )  ;", "final   Tuple < Map < String ,    GoogleCloudStorageClientSettings >  ,    Settings >    randomClients    =    randomClients ( nbClients )  ;", "final   Map < String ,    GoogleCloudStorageClientSettings >    expectedClientsSettings    =    randomClients . v 1  (  )  ;", "Map < String ,    GoogleCloudStorageClientSettings >    actualClientsSettings    =    GoogleCloudStorageClientSettings . load ( randomClients . v 2  (  )  )  ;", "assertEquals ( expectedClientsSettings . size (  )  ,    actualClientsSettings . size (  )  )  ;", "for    ( String   clientName    :    expectedClientsSettings . keySet (  )  )     {", "GoogleCloudStorageClientSettings   actualClientSettings    =    actualClientsSettings . get ( clientName )  ;", "assertNotNull ( actualClientSettings )  ;", "GoogleCloudStorageClientSettings   expectedClientSettings    =    expectedClientsSettings . get ( clientName )  ;", "assertNotNull ( expectedClientSettings )  ;", ". assertGoogleCredential ( expectedClientSettings . getCredential (  )  ,    actualClientSettings . getCredential (  )  )  ;", "assertEquals ( expectedClientSettings . getEndpoint (  )  ,    actualClientSettings . getEndpoint (  )  )  ;", "assertEquals ( expectedClientSettings . getConnectTimeout (  )  ,    actualClientSettings . getConnectTimeout (  )  )  ;", "assertEquals ( expectedClientSettings . getReadTimeout (  )  ,    actualClientSettings . getReadTimeout (  )  )  ;", "assertEquals ( expectedClientSettings . getApplicationName (  )  ,    actualClientSettings . getApplicationName (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testLoad"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "Tuple < Map < String ,    GoogleCloudStorageClientSettings >  ,    Settings >    randomClient    =    randomClients (  1  )  ;", "GoogleCloudStorageClientSettings   expectedClientSettings    =    randomClient . v 1  (  )  . values (  )  . iterator (  )  . next (  )  ;", "String   clientName    =    randomClient . v 1  (  )  . keySet (  )  . iterator (  )  . next (  )  ;", ". assertGoogleCredential ( expectedClientSettings . getCredential (  )  ,    GoogleCloudStorageClientSettings . loadCredential ( randomClient . v 2  (  )  ,    clientName )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadCredential"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    GoogleCloudStorageClientSettings >    clientsSettings    =    GoogleCloudStorageClientSettings . load ( EMPTY )  ;", "assertEquals (  1  ,    clientsSettings . size (  )  )  ;", "assertNotNull ( clientsSettings . get (  \" default \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadWithEmptySettings"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageClientSettingsTests"}, {"methodBody": ["METHOD_START", "{", "final   InetSocketAddress   inetSocketAddress    =     (  ( InetSocketAddress )     ( address )  )  ;", "if    (  ( inetSocketAddresetAddress (  )  )    instanceof   Inet 6 Address )     {", "return    (  (  \"  [  \"     +     ( inetSocketAddresetHostString (  )  )  )     +     \"  ]  :  \"  )     +     ( inetSocketAddresetPort (  )  )  ;", "} else    {", "return    (  ( inetSocketAddresetHostString (  )  )     +     \"  :  \"  )     +     ( inetSocketAddresetPort (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addressToString"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageFixture"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args    =  =    null )     |  |     (  ( args . length )     !  =     2  )  )     {", "throw   new   IllegalArgumentException (  \"     < working   directory >     < bucket >  \"  )  ;", "}", "final   InetSocketAddress   socketAddress    =    new   InetSocketAddress ( InetAddress . getLoopbackAddress (  )  ,     0  )  ;", "final   HttpServer   httpServer    =    MockHttpServer . createHttp ( socketAddress ,     0  )  ;", "try    {", "final   Path   workingDirectory    =     . workingDir ( args [  0  ]  )  ;", ". writeFile ( workingDirectory ,     \" pid \"  ,    ManagementFactory . getRuntimeMXBean (  )  . getName (  )  . split (  \"  @  \"  )  [  0  ]  )  ;", "final   String   addressAndPort    =     . addressToString ( httpServer . getAddress (  )  )  ;", ". writeFile ( workingDirectory ,     \" ports \"  ,    addressAndPort )  ;", "final   String   storageUrl    =     \" http :  /  /  \"     +    addressAndPort ;", "final   GoogleCloudStorageTestServer   storageTestServer    =    new   GoogleCloudStorageTestServer ( storageUrl )  ;", "storageTestServer . createBucket ( args [  1  ]  )  ;", "httpServer . createContext (  \"  /  \"  ,    new    . ResponseHandler ( storageTestServer )  )  ;", "httpServer . start (  )  ;", "Thread . sleep ( Long . MAX _ VALUE )  ;", "}    finally    {", "httpServer . stop (  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageFixture"}, {"methodBody": ["METHOD_START", "{", "return   Paths . get ( dir )  ;", "}", "METHOD_END"], "methodName": ["workingDir"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageFixture"}, {"methodBody": ["METHOD_START", "{", "final   Path   tempPidFile    =    Files . createTempFile ( dir ,    null ,    null )  ;", "Files . write ( tempPidFile ,    Collections . singleton ( content )  )  ;", "Files . move ( tempPidFile ,    dir . resolve ( fileName )  ,    StandardCopyOption . ATOMIC _ MOVE )  ;", "}", "METHOD_END"], "methodName": ["writeFile"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageFixture"}, {"methodBody": ["METHOD_START", "{", "return   new   GoogleCloudStorageService ( environment ,    clientsSettings )  ;", "}", "METHOD_END"], "methodName": ["createStorageService"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStoragePlugin"}, {"methodBody": ["METHOD_START", "{", "return   clientsSettings ;", "}", "METHOD_END"], "methodName": ["getClientsSettings"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStoragePlugin"}, {"methodBody": ["METHOD_START", "{", "T   value    =    setting . get ( metadata . settings (  )  )  ;", "if    ( value    =  =    null )     {", "throw   new   RepositoryException ( metadata . name (  )  ,     (  (  \" Setting    [  \"     +     ( setting . getKey (  )  )  )     +     \"  ]    is   not   defined   for   repository \"  )  )  ;", "}", "if    (  ( value   instanceof   String )     &  &     (  ( Strings . hasText (  (  ( String )     ( value )  )  )  )     =  =    false )  )     {", "throw   new   RepositoryException ( metadata . name (  )  ,     (  (  \" Setting    [  \"     +     ( setting . getKey (  )  )  )     +     \"  ]    is   empty   for   repository \"  )  )  ;", "}", "return   value ;", "}", "METHOD_END"], "methodName": ["getSetting"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageRepository"}, {"methodBody": ["METHOD_START", "{", "final   GoogleCloudStorageClientSettings   clientSettings    =    clientsSettings . get ( clientName )  ;", "if    ( clientSettings    =  =    null )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Unknown   client   name    [  \"     +    clientName )     +     \"  ]  .    Existing   client   configs :     \"  )     +     ( Strings . collectionToDelimitedString ( clientsSettings . keySet (  )  ,     \"  ,  \"  )  )  )  )  ;", "}", "HttpTransport   transport    =    GoogleNetHttpTransport . newTrustedTransport (  )  ;", "HttpRequestInitializer   requestInitializer    =     . createRequestInitializer ( clientSettings )  ;", "Storage . Builder   storage    =    new   Storage . Builder ( transport ,    JacksonFactory . getDefaultInstance (  )  ,    requestInitializer )  ;", "if    ( Strings . hasLength ( clientSettings . getApplicationName (  )  )  )     {", "storage . setApplicationName ( clientSettings . getApplicationName (  )  )  ;", "}", "if    ( Strings . hasLength ( clientSettings . getEndpoint (  )  )  )     {", "storage . setRootUrl ( clientSettings . getEndpoint (  )  )  ;", "}", "return   storage . build (  )  ;", "}", "METHOD_END"], "methodName": ["createClient"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageService"}, {"methodBody": ["METHOD_START", "{", "GoogleCredential   credential    =    settings . getCredential (  )  ;", "if    ( credential    =  =    null )     {", "credential    =    GoogleCredential . getApplicationDefault (  )  ;", "}", "return   new    . DefaultHttpRequestInitializer ( credential ,     . toTimeout ( settings . getConnectTimeout (  )  )  ,     . toTimeout ( settings . getReadTimeout (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["createRequestInitializer"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( timeout    =  =    null )     |  |     ( ZERO . equals ( timeout )  )  )     {", "return   null ;", "}", "if    ( MINUS _ ONE . equals ( timeout )  )     {", "return    0  ;", "}", "return   Math . toIntExact ( timeout . getMillis (  )  )  ;", "}", "METHOD_END"], "methodName": ["toTimeout"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageService"}, {"methodBody": ["METHOD_START", "{", "final   Environment   environment    =    mock ( Environment . class )  ;", "when ( environment . settings (  )  )  . thenReturn ( EMPTY )  ;", "final   GoogleCredential   credential    =    mock ( GoogleCredential . class )  ;", "when ( credential . handleResponse ( any ( HttpRequest . class )  ,    any ( HttpResponse . class )  ,    anyBoolean (  )  )  )  . thenReturn ( false )  ;", "final   TimeValue   readTimeout    =    TimeValue . timeValueSeconds ( randomIntBetween (  1  ,     1  2  0  )  )  ;", "final   TimeValue   connectTimeout    =    TimeValue . timeValueSeconds ( randomIntBetween (  1  ,     1  2  0  )  )  ;", "final   String   endpoint    =     ( randomBoolean (  )  )     ?    randomAlphaOfLength (  1  0  )     :    null ;", "final   String   applicationName    =     ( randomBoolean (  )  )     ?    randomAlphaOfLength (  1  0  )     :    null ;", "final   GoogleCloudStorageClientSettings   clientSettings    =    new   GoogleCloudStorageClientSettings ( credential ,    endpoint ,    connectTimeout ,    readTimeout ,    applicationName )  ;", "final   HttpRequestInitializer   initializer    =     . createRequestInitializer ( clientSettings )  ;", "final   HttpRequestFactory   requestFactory    =    new   MockHttpTransport (  )  . createRequestFactory ( initializer )  ;", "final   HttpRequest   request 1     =    requestFactory . buildGetRequest ( new   GenericUrl (  )  )  ;", "assertEquals (  (  ( int )     ( connectTimeout . millis (  )  )  )  ,    request 1  . getConnectTimeout (  )  )  ;", "assertEquals (  (  ( int )     ( readTimeout . millis (  )  )  )  ,    request 1  . getReadTimeout (  )  )  ;", "assertSame ( credential ,    request 1  . getInterceptor (  )  )  ;", "assertNotNull ( request 1  . getIOExceptionHandler (  )  )  ;", "assertNotNull ( request 1  . getUnsuccessfulResponseHandler (  )  )  ;", "final   HttpRequest   request 2     =    requestFactory . buildGetRequest ( new   GenericUrl (  )  )  ;", "assertEquals (  (  ( int )     ( connectTimeout . millis (  )  )  )  ,    request 2  . getConnectTimeout (  )  )  ;", "assertEquals (  (  ( int )     ( readTimeout . millis (  )  )  )  ,    request 2  . getReadTimeout (  )  )  ;", "assertSame ( request 1  . getInterceptor (  )  ,    request 2  . getInterceptor (  )  )  ;", "assertNotNull ( request 2  . getIOExceptionHandler (  )  )  ;", "assertNotSame ( request 1  . getIOExceptionHandler (  )  ,    request 2  . getIOExceptionHandler (  )  )  ;", "assertNotNull ( request 2  . getUnsuccessfulResponseHandler (  )  )  ;", "assertNotSame ( request 1  . getUnsuccessfulResponseHandler (  )  ,    request 2  . getUnsuccessfulResponseHandler (  )  )  ;", "request 1  . getUnsuccessfulResponseHandler (  )  . handleResponse ( null ,    null ,    false )  ;", "verify ( credential ,    times (  1  )  )  . handleResponse ( any ( HttpRequest . class )  ,    any ( HttpResponse . class )  ,    anyBoolean (  )  )  ;", "request 2  . getUnsuccessfulResponseHandler (  )  . handleResponse ( null ,    null ,    false )  ;", "verify ( credential ,    times (  2  )  )  . handleResponse ( any ( HttpRequest . class )  ,    any ( HttpResponse . class )  ,    anyBoolean (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultHttpRequestInitializer"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "assertNull ( GoogleCloudStorageService . toTimeout ( null )  )  ;", "assertNull ( GoogleCloudStorageService . toTimeout ( ZERO )  )  ;", "assertEquals (  0  ,    GoogleCloudStorageService . toTimeout ( MINUS _ ONE )  . intValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testToTimeout"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageServiceTests"}, {"methodBody": ["METHOD_START", "{", "return   jsonBuilder (  )  . startObject (  )  . field (  \" kind \"  ,     \" storage # bucket \"  )  . field (  \" id \"  ,    name )  . endObject (  )  ;", "}", "METHOD_END"], "methodName": ["buildBucketResource"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   GoogleCloudStorageTestServer . buildObjectResource ( jsonBuilder (  )  ,    bucket ,    name ,    bytes )  ;", "}", "METHOD_END"], "methodName": ["buildObjectResource"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   builder . startObject (  )  . field (  \" kind \"  ,     \" storage # object \"  )  . field (  \" id \"  ,    String . join (  \"  /  \"  ,    bucket ,    name )  )  . field (  \" name \"  ,    name )  . field (  \" size \"  ,    String . valueOf ( bytes . length )  )  . endObject (  )  ;", "}", "METHOD_END"], "methodName": ["buildObjectResource"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "buckets . put ( bucketName ,    new   GoogleCloudStorageTestServer . Bucket ( bucketName )  )  ;", "}", "METHOD_END"], "methodName": ["createBucket"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   PathTrie < GoogleCloudStorageTestServer . RequestHandler >    handlers    =    new   PathTrie ( RestUtils . REST _ DECODER )  ;", "handlers . insert (  (  (  \" GET    \"     +    endpoint )     +     \"  / storage / v 1  / b /  { bucket }  \"  )  ,     (    params ,    headers ,    body )     -  >     {", "String   name    =    params . get (  \" bucket \"  )  ;", "if    (  ( Strings . hasText ( name )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" bucket   name   is   missing \"  )  ;", "}", "if    ( buckets . containsKey ( name )  )     {", "return   newResponse ( RestStatus . OK ,    emptyMap (  )  ,    buildBucketResource ( name )  )  ;", "} else    {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "}  )  ;", "handlers . insert (  (  (  \" GET    \"     +    endpoint )     +     \"  / storage / v 1  / b /  { bucket }  / o /  { object }  \"  )  ,     (    params ,    headers ,    body )     -  >     {", "String   objectName    =    params . get (  \" object \"  )  ;", "if    (  ( Strings . hasText ( objectName )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" object   name   is   missing \"  )  ;", "}", "final   Bucket   bucket    =    buckets . get ( params . get (  \" bucket \"  )  )  ;", "if    ( bucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "for    ( Entry < String ,    byte [  ]  >    object    :    bucket . objects . entrySet (  )  )     {", "if    ( object . getKey (  )  . equals ( objectName )  )     {", "return   newResponse ( RestStatus . OK ,    emptyMap (  )  ,    buildObjectResource ( bucket . name ,    objectName ,    object . getValue (  )  )  )  ;", "}", "}", "return   newError ( RestStatus . NOT _ FOUND ,     \" object   not   found \"  )  ;", "}  )  ;", "handlers . insert (  (  (  \" DELETE    \"     +    endpoint )     +     \"  / storage / v 1  / b /  { bucket }  / o /  { object }  \"  )  ,     (    params ,    headers ,    body )     -  >     {", "String   objectName    =    params . get (  \" object \"  )  ;", "if    (  ( Strings . hasText ( objectName )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" object   name   is   missing \"  )  ;", "}", "final   Bucket   bucket    =    buckets . get ( params . get (  \" bucket \"  )  )  ;", "if    ( bucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "final   byte [  ]    bytes    =    bucket . objects . remove ( objectName )  ;", "if    ( bytes    !  =    null )     {", "return   new   Response ( RestStatus . NO _ CONTENT ,    emptyMap (  )  ,    XContentType . JSON . mediaType (  )  ,    GoogleCloudStorageTestServer . EMPTY _ BYTE )  ;", "}", "return   newError ( RestStatus . NOT _ FOUND ,     \" object   not   found \"  )  ;", "}  )  ;", "handlers . insert (  (  (  \" POST    \"     +    endpoint )     +     \"  / upload / storage / v 1  / b /  { bucket }  / o \"  )  ,     (    params ,    headers ,    body )     -  >     {", "if    (  (  \" resumable \"  . equals ( params . get (  \" uploadType \"  )  )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" upload   type   must   be   resumable \"  )  ;", "}", "final   String   objectName    =    params . get (  \" name \"  )  ;", "if    (  ( Strings . hasText ( objectName )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" object   name   is   missing \"  )  ;", "}", "final   Bucket   bucket    =    buckets . get ( params . get (  \" bucket \"  )  )  ;", "if    ( bucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "if    (  ( bucket . objects . put ( objectName ,    GoogleCloudStorageTestServer . EMPTY _ BYTE )  )     =  =    null )     {", "String   location    =     (  (  ( endpoint    +     \"  / upload / storage / v 1  / b /  \"  )     +    bucket . name )     +     \"  / o ? uploadType = resumable & upload _ id =  \"  )     +    objectName ;", "return   new   Response ( RestStatus . CREATED ,    singletonMap (  \" Location \"  ,    location )  ,    XContentType . JSON . mediaType (  )  ,    GoogleCloudStorageTestServer . EMPTY _ BYTE )  ;", "} else    {", "return   newError ( RestStatus . CONFLICT ,     \" object   already   exist \"  )  ;", "}", "}  )  ;", "handlers . insert (  (  (  \" PUT    \"     +    endpoint )     +     \"  / upload / storage / v 1  / b /  { bucket }  / o \"  )  ,     (    params ,    headers ,    body )     -  >     {", "String   objectId    =    params . get (  \" upload _ id \"  )  ;", "if    (  ( Strings . hasText ( objectId )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" upload   id   is   missing \"  )  ;", "}", "final   Bucket   bucket    =    buckets . get ( params . get (  \" bucket \"  )  )  ;", "if    ( bucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "if    (  ( bucket . objects . containsKey ( objectId )  )     =  =    false )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" object   name   not   found \"  )  ;", "}", "bucket . objects . put ( objectId ,    body )  ;", "return   newResponse ( RestStatus . OK ,    emptyMap (  )  ,    buildObjectResource ( bucket . name ,    objectId ,    body )  )  ;", "}  )  ;", "handlers . insert (  (  (  \" POST    \"     +    endpoint )     +     \"  / storage / v 1  / b /  { srcBucket }  / o /  { src }  / copyTo / b /  { destBucket }  / o /  { dest }  \"  )  ,     (    params ,    headers ,    body )     -  >     {", "String   source    =    params . get (  \" src \"  )  ;", "if    (  ( Strings . hasText ( source )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" source   object   name   is   missing \"  )  ;", "}", "final   Bucket   srcBucket    =    buckets . get ( params . get (  \" srcBucket \"  )  )  ;", "if    ( srcBucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" source   bucket   not   found \"  )  ;", "}", "String   dest    =    params . get (  \" dest \"  )  ;", "if    (  ( Strings . hasText ( dest )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" destination   object   name   is   missing \"  )  ;", "}", "final   Bucket   destBucket    =    buckets . get ( params . get (  \" destBucket \"  )  )  ;", "if    ( destBucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" destination   bucket   not   found \"  )  ;", "}", "final   byte [  ]    sourceBytes    =    srcBucket . objects . get ( source )  ;", "if    ( sourceBytes    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" source   object   not   found \"  )  ;", "}", "destBucket . objects . put ( dest ,    sourceBytes )  ;", "return   newResponse ( RestStatus . OK ,    emptyMap (  )  ,    buildObjectResource ( destBucket . name ,    dest ,    sourceBytes )  )  ;", "}  )  ;", "handlers . insert (  (  (  \" GET    \"     +    endpoint )     +     \"  / storage / v 1  / b /  { bucket }  / o \"  )  ,     (    params ,    headers ,    body )     -  >     {", "final   Bucket   bucket    =    buckets . get ( params . get (  \" bucket \"  )  )  ;", "if    ( bucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "final   XContentBuilder   builder    =    jsonBuilder (  )  ;", "builder . startObject (  )  ;", "builder . field (  \" kind \"  ,     \" storage # objects \"  )  ;", "{", "builder . startArray (  \" items \"  )  ;", "final   String   prefixParam    =    params . get (  \" prefix \"  )  ;", "for    ( Entry < String ,    byte [  ]  >    object    :    bucket . objects . entrySet (  )  )     {", "if    (  ( prefixParam    !  =    null )     &  &     (  ( object . getKey (  )  . startsWith ( prefixParam )  )     =  =    false )  )     {", "continue ;", "}", "buildObjectResource ( builder ,    bucket . name ,    object . getKey (  )  ,    object . getValue (  )  )  ;", "}", "builder . endArray (  )  ;", "}", "builder . endObject (  )  ;", "return   newResponse ( RestStatus . OK ,    emptyMap (  )  ,    builder )  ;", "}  )  ;", "handlers . insert (  (  (  \" GET    \"     +    endpoint )     +     \"  / download / storage / v 1  / b /  { bucket }  / o /  { object }  \"  )  ,     (    params ,    headers ,    body )     -  >     {", "String   object    =    params . get (  \" object \"  )  ;", "if    (  ( Strings . hasText ( object )  )     =  =    false )     {", "return   newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,     \" object   id   is   missing \"  )  ;", "}", "final   Bucket   bucket    =    buckets . get ( params . get (  \" bucket \"  )  )  ;", "if    ( bucket    =  =    null )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" bucket   not   found \"  )  ;", "}", "if    (  ( bucket . objects . containsKey ( object )  )     =  =    false )     {", "return   newError ( RestStatus . NOT _ FOUND ,     \" object   name   not   found \"  )  ;", "}", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" application / octet - stream \"  ,    bucket . objects . get ( object )  )  ;", "}  )  ;", "handlers . insert (  (  (  \" POST    \"     +    endpoint )     +     \"  / batch \"  )  ,     (    params ,    headers ,    body )     -  >     {", "final   List < Response >    batchedResponses    =    new   ArrayList <  >  (  )  ;", "String   boundary    =     \"  _  _ END _ OF _ PART _  _  \"  ;", "final   List < String >    contentTypes    =    headers . getOrDefault (  \" Content - Type \"  ,    headers . get (  \" Content - type \"  )  )  ;", "if    ( contentTypes    !  =    null )     {", "final   String   contentType    =    contentTypes . get (  0  )  ;", "if    (  ( contentType    !  =    null )     &  &     ( contentType . contains (  \" multipart / mixed ;    boundary =  \"  )  )  )     {", "boundary    =    contentType . replace (  \" multipart / mixed ;    boundary =  \"  ,     \"  \"  )  ;", "}", "}", "try    ( BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( new   ByteArrayInputStream ( body )  ,    StandardCharsets . UTF _  8  )  )  )     {", "String   line ;", "while    (  ( line    =    reader . readLine (  )  )     !  =    null )     {", "if    ( line . equals (  (  \"  -  -  \"     +    boundary )  )  )     {", "Map < String ,    List < String >  >    batchedHeaders    =    new   HashMap <  >  (  )  ;", "while    (  ( line    =    reader . readLine (  )  )     !  =    null )     {", "if    (  ( line . equals (  \"  \\ r \\ n \"  )  )     |  |     (  ( line . length (  )  )     =  =     0  )  )     {", "break ;", "} else    {", "String [  ]    header    =    line . split (  \"  :  \"  ,     2  )  ;", "batchedHeaders . put ( header [  0  ]  ,    singletonList ( header [  1  ]  )  )  ;", "}", "}", "line    =    reader . readLine (  )  ;", "String   batchedUrl    =    line . substring (  0  ,    line . lastIndexOf (  '     '  )  )  ;", "final   Map < String ,    String >    batchedParams    =    new   HashMap <  >  (  )  ;", "int   questionMark    =    batchedUrl . indexOf (  '  ?  '  )  ;", "if    ( questionMark    !  =     (  -  1  )  )     {", "RestUtils . decodeQueryString ( batchedUrl . substring (  ( questionMark    +     1  )  )  ,     0  ,    batchedParams )  ;", "}", "line    =    reader . readLine (  )  ;", "byte [  ]    batchedBody    =    new   byte [  0  ]  ;", "if    (  ( line    !  =    null )     |  |     (  ( line . startsWith (  (  \"  -  -  \"     +    boundary )  )  )     =  =    false )  )     {", "batchedBody    =    line . getBytes ( StandardCharsets . UTF _  8  )  ;", "}", "RequestHandler   handler    =    handlers . retrieve ( batchedUrl ,    batchedParams )  ;", "if    ( handler    !  =    null )     {", "try    {", "batchedResponses . add ( handler . execute ( batchedParams ,    batchedHeaders ,    batchedBody )  )  ;", "}    catch    (    e )     {", "batchedResponses . add ( newError ( RestStatus . INTERNAL _ SERVER _ ERROR ,    e . getMessage (  )  )  )  ;", "}", "}", "}", "}", "}", "String   sep    =     \"  -  -  \"  ;", "String   line    =     \"  \\ r \\ n \"  ;", "StringBuilder   builder    =    new   StringBuilder (  )  ;", "for    ( Response   response    :    batchedResponses )     {", "builder . append ( sep )  . append ( boundary )  . append ( line )  ;", "builder . append (  \" Content - Type :    application / http \"  )  . append ( line )  ;", "builder . append ( line )  ;", "builder . append (  \" HTTP /  1  .  1     \"  )  . append ( response . status . getStatus (  )  )  . append (  '     '  )  . append ( response . status . toString (  )  )  . append ( line )  ;", "builder . append (  \" Content - Length :     \"  )  . append ( response . body . length )  . append ( line )  ;", "builder . append (  \" Content - Type :     \"  )  . append ( response . contentType )  . append ( line )  ;", "response . headers . forEach (  (    k ,    v )     -  >    builder . append ( k )  . append (  \"  :     \"  )  . append ( v )  . append ( line )  )  ;", "builder . append ( line )  ;", "builder . append ( new   String ( response . body ,    StandardCharsets . UTF _  8  )  )  . append ( line )  ;", "builder . append ( line )  ;", "}", "builder . append ( line )  ;", "builder . append ( sep )  . append ( boundary )  . append ( sep )  ;", "byte [  ]    content    =    builder . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  ;", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     (  \" multipart / mixed ;    boundary =  \"     +    boundary )  ,    content )  ;", "}  )  ;", "handlers . insert (  (  (  \" POST    \"     +    endpoint )     +     \"  / o / oauth 2  / token \"  )  ,     (    url ,    params ,    req )     -  >    newResponse ( RestStatus . OK ,    emptyMap (  )  ,    jsonBuilder (  )  . startObject (  )  . field (  \" access _ token \"  ,     \" unknown \"  )  . field (  \" token _ type \"  ,     \" Bearer \"  )  . field (  \" expires _ in \"  ,     3  6  0  0  )  . endObject (  )  )  )  ;", "return   handlers ;", "}", "METHOD_END"], "methodName": ["defaultHandlers"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "return   endpoint ;", "}", "METHOD_END"], "methodName": ["getEndpoint"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   Map < String ,    String >    params    =    new   HashMap <  >  (  )  ;", "if    ( query    !  =    null )     {", "RestUtils . decodeQueryString ( query ,     0  ,    params )  ;", "}", "final    . RequestHandler   handler    =    handlers . retrieve (  (  ( method    +     \"     \"  )     +    path )  ,    params )  ;", "if    ( handler    !  =    null )     {", "return   handler . execute ( params ,    headers ,    body )  ;", "} else    {", "return    . newError ( INTERNAL _ SERVER _ ERROR ,     (  (  (  (  \" No   handler   defined   for   request    [ method :     \"     +    method )     +     \"  ,    path :     \"  )     +    path )     +     \"  ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["handle"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "final   int   questionMark    =    url . indexOf (  '  ?  '  )  ;", "if    ( questionMark    =  =     (  -  1  )  )     {", "return   handle ( method ,    url ,    null ,    headers ,    body )  ;", "}", "return   handle ( method ,    url . substring (  0  ,    questionMark )  ,    url . substring (  ( questionMark    +     1  )  )  ,    headers ,    body )  ;", "}", "METHOD_END"], "methodName": ["handle"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "try    ( ByteArrayOutputStream   out    =    new   ByteArrayOutputStream (  )  )     {", "try    ( XContentBuilder   builder    =    jsonBuilder (  )  )     {", "builder . startObject (  )  . startObject (  \" error \"  )  . field (  \" code \"  ,    status . getStatus (  )  )  . field (  \" message \"  ,    message )  . startArray (  \" errors \"  )  . startObject (  )  . field (  \" domain \"  ,     \" global \"  )  . field (  \" reason \"  ,    status . toString (  )  )  . field (  \" message \"  ,    message )  . endObject (  )  . endArray (  )  . endObject (  )  . endObject (  )  ;", "BytesReference . bytes ( builder )  . writeTo ( out )  ;", "}", "return   new    . Response ( status ,    Collections . emptyMap (  )  ,    JSON . mediaType (  )  ,    out . toByteArray (  )  )  ;", "}    catch    ( IOException   e )     {", "byte [  ]    bytes    =     ( message    !  =    null    ?    message    :     \" something   went   wrong \"  )  . getBytes ( StandardCharsets . UTF _  8  )  ;", "return   new    . Response ( RestStatus . INTERNAL _ SERVER _ ERROR ,    Collections . emptyMap (  )  ,     \"    text / plain \"  ,    bytes )  ;", "}", "}", "METHOD_END"], "methodName": ["newError"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "try    ( ByteArrayOutputStream   out    =    new   ByteArrayOutputStream (  )  )     {", "BytesReference . bytes ( xContentBuilder )  . writeTo ( out )  ;", "return   new    . Response ( status ,    headers ,    JSON . mediaType (  )  ,    out . toByteArray (  )  )  ;", "}    catch    ( IOException   e )     {", "return    . newError ( INTERNAL _ SERVER _ ERROR ,    e . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["newResponse"], "fileName": "org.elasticsearch.repositories.gcs.GoogleCloudStorageTestServer"}, {"methodBody": ["METHOD_START", "{", "HttpResponseException . Builder   builder    =    new   HttpResponseException . Builder (  4  0  4  ,     (  \" Bucket   not   found :     \"     +    bucket )  ,    new   HttpHeaders (  )  )  ;", "return   new   com . google . api . client . googleapis . json . GoogleJsonResponseException ( builder ,    new   GoogleJsonError (  )  )  ;", "}", "METHOD_END"], "methodName": ["newBucketNotFoundException"], "fileName": "org.elasticsearch.repositories.gcs.MockStorage"}, {"methodBody": ["METHOD_START", "{", "HttpResponseException . Builder   builder    =    new   HttpResponseException . Builder (  4  0  4  ,     (  \" Object   not   found :     \"     +    object )  ,    new   HttpHeaders (  )  )  ;", "return   new   com . google . api . client . googleapis . json . GoogleJsonResponseException ( builder ,    new   GoogleJsonError (  )  )  ;", "}", "METHOD_END"], "methodName": ["newObjectNotFoundException"], "fileName": "org.elasticsearch.repositories.gcs.MockStorage"}, {"methodBody": ["METHOD_START", "{", "return   createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.repositories.gcs.RepositoryGcsClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   Controller . doPrivileged ( operation )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedIOException"], "fileName": "org.elasticsearch.repositories.gcs.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "Controller . doPrivileged (  (  ( PrivilegedExceptionAction < Void >  )     (  (  )     -  >     {", "action . run (  )  ;", "return   null ;", "}  )  )  )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedVoidIOException"], "fileName": "org.elasticsearch.repositories.gcs.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "logger . info (  \" Swapping   active   namenodes :     [  {  }  ]    to   standby   and    [  {  }  ]    to   active \"  ,    from ,    to )  ;", "try    {", "AccessController . doPrivileged (  (  ( PrivilegedExceptionAction < Void >  )     (  (  )     -  >     {", ". CloseableHAAdmin   haAdmin    =    new    . CloseableHAAdmin (  )  ;", "haAdmin . setConf ( configuration )  ;", "try    {", "haAdmin . transitionToStandby ( from )  ;", "haAdmin . transitionToActive ( to )  ;", "}    finally    {", "haAdmin . close (  )  ;", "}", "return   null ;", "}  )  )  )  ;", "}    catch    ( PrivilegedActionException   pae )     {", "throw   new   IOException (  \" Unable   to   perform   namenode   failover \"  ,    pae )  ;", "}", "}", "METHOD_END"], "methodName": ["failoverHDFS"], "fileName": "org.elasticsearch.repositories.hdfs.HaHdfsFailoverTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "if    ( securityEnabled )     {", "return    (  (  \"  \\  \" security . principal \\  \"  :     \\  \"  \"     +    kerberosPrincipal )     +     \"  \\  \"  ,  \"  )     +     \"  \\  \" conf . data . transfer . protection \\  \"  :     \\  \" authentication \\  \"  ,  \"  ;", "} else    {", "return    \"  \"  ;", "}", "}", "METHOD_END"], "methodName": ["securityCredentials"], "fileName": "org.elasticsearch.repositories.hdfs.HaHdfsFailoverTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "RestClient   client    =    client (  )  ;", "Map < String ,    String >    emptyParams    =    Collections . emptyMap (  )  ;", "Header   contentHeader    =    new   BasicHeader (  \" Content - Type \"  ,     \" application / json \"  )  ;", "String   esKerberosPrincipal    =    System . getProperty (  \" test . krb 5  . principal . es \"  )  ;", "String   hdfsKerberosPrincipal    =    System . getProperty (  \" test . krb 5  . principal . hdfs \"  )  ;", "String   kerberosKeytabLocation    =    System . getProperty (  \" test . krb 5  . keytab . hdfs \"  )  ;", "boolean   securityEnabled    =    hdfsKerberosPrincipal    !  =    null ;", "Configuration   hdfsConfiguration    =    new   Configuration (  )  ;", "hdfsConfiguration . set (  \" dfs . nameservices \"  ,     \" ha - hdfs \"  )  ;", "hdfsConfiguration . set (  \" dfs . ha . namenodes . ha - hdfs \"  ,     \" nn 1  , nn 2  \"  )  ;", "hdfsConfiguration . set (  \" dfs . namenode . rpc - address . ha - hdfs . nn 1  \"  ,     \" localhost :  1  0  0  0  1  \"  )  ;", "hdfsConfiguration . set (  \" dfs . namenode . rpc - address . ha - hdfs . nn 2  \"  ,     \" localhost :  1  0  0  0  2  \"  )  ;", "hdfsConfiguration . set (  \" dfs . client . failover . proxy . provider . ha - hdfs \"  ,     \" ConfiguredFailoverProxyProvider \"  )  ;", "AccessController . doPrivileged (  (  ( PrivilegedExceptionAction < Void >  )     (  (  )     -  >     {", "if    ( securityEnabled )     {", "Path   kt    =    PathUtils . get ( kerberosKeytabLocation )  ;", "if    (  ( Files . exists ( kt )  )     =  =    false )     {", "throw   new   IllegalStateException (  (  \" Could   not   locate   keytab   at    \"     +    kerberosKeytabLocation )  )  ;", "}", "if    (  ( Files . isReadable ( kt )  )     !  =    true )     {", "throw   new   IllegalStateException (  (  \" Could   not   read   keytab   at    \"     +    kerberosKeytabLocation )  )  ;", "}", "logger . info (  (  \" Keytab   Length :     \"     +     ( Files . readAllBytes ( kt )  . length )  )  )  ;", "hdfsConfiguration . set (  \" dfs . namenode . kerberos . principal \"  ,    hdfsKerberosPrincipal )  ;", "hdfsConfiguration . set (  \" dfs . datanode . kerberos . principal \"  ,    hdfsKerberosPrincipal )  ;", "hdfsConfiguration . set (  \" dfs . data . transfer . protection \"  ,     \" authentication \"  )  ;", "SecurityUtil . setAuthenticationMethod ( KERBEROS ,    hdfsConfiguration )  ;", "UserGroupInformation . setConfiguration ( hdfsConfiguration )  ;", "UserGroupInformation . loginUserFromKeytab ( hdfsKerberosPrincipal ,    kerberosKeytabLocation )  ;", "} else    {", "SecurityUtil . setAuthenticationMethod ( SIMPLE ,    hdfsConfiguration )  ;", "UserGroupInformation . setConfiguration ( hdfsConfiguration )  ;", "UserGroupInformation . getCurrentUser (  )  ;", "}", "return   null ;", "}  )  )  )  ;", "{", "Response   response    =    client . performRequest (  \" PUT \"  ,     \"  /  _ snapshot / hdfs _ ha _ repo _ read \"  ,    emptyParams ,    new   NStringEntity (  (  (  (  (  (  (  (  (  (  (  \"  {  \"     +     (  (  (  (  \"  \\  \" type \\  \"  :  \\  \" hdfs \\  \"  ,  \"     +     \"  \\  \" settings \\  \"  :  {  \"  )     +     \"  \\  \" uri \\  \"  :     \\  \" hdfs :  /  / ha - hdfs /  \\  \"  ,  \\ n \"  )     +     \"  \\  \" path \\  \"  :     \\  \"  / user /  / existing / readonly - repository \\  \"  ,  \"  )     +     \"  \\  \" readonly \\  \"  :     \\  \" true \\  \"  ,  \"  )  )     +     ( securityCredentials ( securityEnabled ,    esKerberosPrincipal )  )  )     +     \"  \\  \" conf . dfs . nameservices \\  \"  :     \\  \" ha - hdfs \\  \"  ,  \"  )     +     \"  \\  \" conf . dfs . ha . namenodes . ha - hdfs \\  \"  :     \\  \" nn 1  , nn 2  \\  \"  ,  \"  )     +     \"  \\  \" conf . dfs . namenode . rpc - address . ha - hdfs . nn 1  \\  \"  :     \\  \" localhost :  1  0  0  0  1  \\  \"  ,  \"  )     +     \"  \\  \" conf . dfs . namenode . rpc - address . ha - hdfs . nn 2  \\  \"  :     \\  \" localhost :  1  0  0  0  2  \\  \"  ,  \"  )     +     \"  \\  \" conf . dfs . client . failover . proxy . provider . ha - hdfs \\  \"  :     \"  )     +     \"  \\  \" ConfiguredFailoverProxyProvider \\  \"  \"  )     +     \"  }  \"  )     +     \"  }  \"  )  ,    Charset . defaultCharset (  )  )  ,    contentHeader )  ;", "Assert . assertEquals (  2  0  0  ,    response . getStatusLine (  )  . getStatusCode (  )  )  ;", "}", "{", "Response   response    =    client . performRequest (  \" GET \"  ,     \"  /  _ snapshot / hdfs _ ha _ repo _ read /  _ all \"  ,    emptyParams )  ;", "Assert . assertEquals (  2  0  0  ,    response . getStatusLine (  )  . getStatusCode (  )  )  ;", "}", "failoverHDFS (  \" nn 1  \"  ,     \" nn 2  \"  ,    hdfsConfiguration )  ;", "{", "Response   response    =    client . performRequest (  \" GET \"  ,     \"  /  _ snapshot / hdfs _ ha _ repo _ read /  _ all \"  ,    emptyParams )  ;", "Assert . assertEquals (  2  0  0  ,    response . getStatusLine (  )  . getStatusCode (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testHAFailoverWithRepository"], "fileName": "org.elasticsearch.repositories.hdfs.HaHdfsFailoverTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    translateToHdfsPath ( blobPath )  ;", "if    (  !  ( readOnly )  )     {", "try    {", "mkdirs ( path )  ;", "}    catch    ( FileAlreadyExistsException   ok )     {", "}    catch    ( IOException   ex )     {", "throw   new   EException (  \" failed   to   create   blob   container \"  ,    ex )  ;", "}", "}", "return   path ;", "}", "METHOD_END"], "methodName": ["buildHdfsPath"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStore"}, {"methodBody": ["METHOD_START", "{", "if    ( closed )     {", "throw   new   AlreadyClosedException (  (  \"    is   closed :     \"     +     ( this )  )  )  ;", "}", "return   securityContext . doPrivilegedOrThrow (  (  )     -  >     {", "securityContext . ensureLogin (  )  ;", "return   operation . run ( fileContext )  ;", "}  )  ;", "}", "METHOD_END"], "methodName": ["execute"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStore"}, {"methodBody": ["METHOD_START", "{", "execute (  (  ( HdfsBlobStore . Operation < Void >  )     (  (    fileContext )     -  >     {", "fileContext . mkdir ( path ,    null ,    true )  ;", "return   null ;", "}  )  )  )  ;", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStore"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    root ;", "for    ( String   p    :    bPath )     {", "path    =    new   Path ( path ,    p )  ;", "}", "return   path ;", "}", "METHOD_END"], "methodName": ["translateToHdfsPath"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStore"}, {"methodBody": ["METHOD_START", "{", "Configuration   cfg    =    new   Configuration ( true )  ;", "cfg . setClassLoader ( HdfsRy . class . getClassLoader (  )  )  ;", "cfg . reloadConfiguration (  )  ;", "Constructor <  ?  >    ctor ;", "Subject   subject ;", "try    {", "Class <  ?  >    clazz    =    Class . forName (  \" User \"  )  ;", "ctor    =    clazz . getConstructor ( String . class )  ;", "ctor . setAccessible ( true )  ;", "}    catch    ( ClassNotFoundException    |    NoSuchMethodException   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "try    {", "Principal   principal    =     (  ( Principal )     ( ctor . newInstance ( System . getProperty (  \" user . name \"  )  )  )  )  ;", "subject    =    new   Subject ( false ,    Collections . singleton ( principal )  ,    Collections . emptySet (  )  ,    Collections . emptySet (  )  )  ;", "}    catch    ( InstantiationException    |    IllegalAccessException    |    InvocationTargetException   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "cfg . setBoolean (  \" fs . hdfs . impl . disable . cache \"  ,    true )  ;", "cfg . set (  (  (  \" fs . AbstractFileSystem .  \"     +     ( uri . getScheme (  )  )  )     +     \"  . impl \"  )  ,    TestingFs . class . getName (  )  )  ;", "return   Subject . doAs ( subject ,     (  ( PrivilegedAction < FileContext >  )     (  (  )     -  >     {", "try    {", "TestingFs   fs    =     (  ( TestingFs )     ( AbstractFileSystem . get ( uri ,    cfg )  )  )  ;", "return   FileContext . getFileContext ( fs ,    cfg )  ;", "}    catch    ( UnsupportedFileSystemException   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}  )  )  )  ;", "}", "METHOD_END"], "methodName": ["createContext"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "FileContext   fileContext ;", "try    {", "fileContext    =    AccessController . doPrivileged (  (  ( PrivilegedExceptionAction < FileContext >  )     (  (  )     -  >    createContext ( new   URI (  \" hdfs :  /  /  /  \"  )  )  )  )  )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw   new   RuntimeException ( e . getCause (  )  )  ;", "}", "return   fileContext ;", "}", "METHOD_END"], "methodName": ["createTestContext"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "FileContext   fileContext    =    createTestContext (  )  ;", "hdfsBlobStore    =    new    ( fileContext ,     \" dir \"  ,     1  0  2  4  ,    true )  ;", "FileContext . Util   util    =    fileContext . util (  )  ;", "Path   root    =    fileContext . makeQualified ( new   Path (  \" dir \"  )  )  ;", "assertFalse ( util . exists ( root )  )  ;", "BlobPath   blobPath    =    BlobPath . cleanPath (  )  . add (  \" path \"  )  ;", "hdfsBlobStore . blobContainer ( blobPath )  ;", "Path   hdfsPath    =    root ;", "for    ( String   p    :    blobPath )     {", "hdfsPath    =    new   Path ( hdfsPath ,    p )  ;", "}", "assertFalse ( util . exists ( hdfsPath )  )  ;", "hdfsBlobStore    =    new    ( fileContext ,     \" dir \"  ,     1  0  2  4  ,    false )  ;", "assertTrue ( util . exists ( root )  )  ;", "BlobContainer   container    =    hdfsBlobStore . blobContainer ( blobPath )  ;", "assertTrue ( util . exists ( hdfsPath )  )  ;", "byte [  ]    data    =    ESBlobStoreTestCase . randomBytes ( randomIntBetween (  1  0  ,    scaledRandomIntBetween (  1  0  2  4  ,     (  1     <  <     1  6  )  )  )  )  ;", "writeBlob ( container ,     \" foo \"  ,    new   BytesArray ( data )  )  ;", "assertArrayEquals ( ESBlobStoreTestCase . readBlobFully ( container ,     \" foo \"  ,    data . length )  ,    data )  ;", "assertTrue ( container . blobExists (  \" foo \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testReadOnly"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsBlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "ClassLoader   oldCCL    =    Thread . currentThread (  )  . getContextClassLoader (  )  ;", "try    {", "Thread . currentThread (  )  . setContextClassLoader ( HdfsRy . class . getClassLoader (  )  )  ;", "KerberosInfo   info    =    SecurityUtil . getKerberosInfo ( ClientNamenodeProtocolPB . class ,    null )  ;", "if    ( info    =  =    null )     {", "throw   new   RuntimeException (  (  \" Could   not   initialize   SecurityUtil :     \"     +     \" Unable   to   find   services   for    [ SecurityInfo ]  \"  )  )  ;", "}", "}    finally    {", "Thread . currentThread (  )  . setContextClassLoader ( oldCCL )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["eagerInit"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsPlugin"}, {"methodBody": ["METHOD_START", "{", "Path   hadoopHome    =    null ;", "String   oldValue    =    null ;", "try    {", "hadoopHome    =    Files . createTempDirectory (  \" hadoop \"  )  . toAbsolutePath (  )  ;", "oldValue    =    System . setProperty (  \" hadoop . home . dir \"  ,    hadoopHome . toString (  )  )  ;", "Class . forName (  \" apache . hadoop . security . UserGroupInformation \"  )  ;", "Class . forName (  \" apache . hadoop . util . StringUtils \"  )  ;", "Class . forName (  \" apache . hadoop . util . ShutdownHookManager \"  )  ;", "Class . forName (  \" apache . hadoop . conf . Configuration \"  )  ;", "}    catch    ( ClassNotFoundException    |    IOException   e )     {", "throw   new   RuntimeException ( e )  ;", "}    finally    {", "if    ( oldValue    =  =    null )     {", "System . clearProperty (  \" hadoop . home . dir \"  )  ;", "} else    {", "System . setProperty (  \" hadoop . home . dir \"  ,    oldValue )  ;", "}", "try    {", "if    ( hadoopHome    !  =    null )     {", "Files . delete ( hadoopHome )  ;", "}", "}    catch    ( IOException   thisIsBestEffort )     {", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["evilHadoopInit"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsPlugin"}, {"methodBody": ["METHOD_START", "{", "Configuration   hadoopConfiguration    =    new   Configuration ( repositorySettings . getAsBoolean (  \" load _ defaults \"  ,    true )  )  ;", "hadoopConfiguration . setClassLoader (  . class . getClassLoader (  )  )  ;", "hadoopConfiguration . reloadConfiguration (  )  ;", "final   Settings   confSettings    =    repositorySettings . getByPrefix (  \" conf .  \"  )  ;", "for    ( String   key    :    confSettings . keySet (  )  )     {", ". LOGGER . debug (  \" Adding   configuration   to   HDFS   Client   Configuration    :     {  }     =     {  }  \"  ,    key ,    confSettings . get ( key )  )  ;", "hadoopConfiguration . set ( key ,    confSettings . get ( key )  )  ;", "}", "hadoopConfiguration . setBoolean (  \" fs . hdfs . impl . disable . cache \"  ,    true )  ;", "UserGroupInformation   ugi    =    login ( hadoopConfiguration ,    repositorySettings )  ;", "String   host    =    uri . getHost (  )  ;", "String   configKey    =     (  ( Failover . PROXY _ PROVIDER _ KEY _ PREFIX )     +     \"  .  \"  )     +    host ;", "Class <  ?  >    ret    =    hadoopConfiguration . getClass ( configKey ,    null ,    FailoverProxyProvider . class )  ;", "boolean   haEnabled    =    ret    !  =    null ;", "int   bufferSize    =    repositorySettings . getAsBytesSize (  \" buffer _ size \"  ,     . DEFAULT _ BUFFER _ SIZE )  . bytesAsInt (  )  ;", "FileContext   fileContext    =    ugi . doAs (  (  ( PrivilegedAction < FileContext >  )     (  (  )     -  >     {", "try    {", "AbstractFileSystem   fs    =    AbstractFileSystem . get ( uri ,    hadoopConfiguration )  ;", "return   FileContext . getFileContext ( fs ,    hadoopConfiguration )  ;", "}    catch    ( UnsupportedFileSystemException   e )     {", "throw   new   UncheckedIOException ( e )  ;", "}", "}  )  )  )  ;", "logger . debug (  \" Using   file - system    [  {  }  ]    for   URI    [  {  }  ]  ,    path    [  {  }  ]  \"  ,    fileContext . getDefaultFileSystem (  )  ,    fileContext . getDefaultFileSystem (  )  . getUri (  )  ,    path )  ;", "try    {", "return   new   HdfsBlobStore ( fileContext ,    path ,    bufferSize ,    isReadOnly (  )  ,    haEnabled )  ;", "}    catch    ( IOException   e )     {", "throw   new   UncheckedIOException ( String . format ( Locale . ROOT ,     \" Cannot   create   HDFS   repository   for   uri    [  % s ]  \"  ,    uri )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["createBlobstore"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsRepository"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   InetAddress . getLocalHost (  )  . getCanonicalHostName (  )  ;", "}    catch    ( UnknownHostException   e )     {", "throw   new   RuntimeException (  \" Could   not   locate   host   information \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getHostName"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsRepository"}, {"methodBody": ["METHOD_START", "{", "AuthenticationMethod   authMethod    =    SecurityUtil . getAuthenticationMethod ( hadoopConfiguration )  ;", "if    (  (  ( authMethod . equals ( SIMPLE )  )     =  =    false )     &  &     (  ( authMethod . equals ( KERBEROS )  )     =  =    false )  )     {", "throw   new   RuntimeException (  (  (  \" Unsupported   authorization   mode    [  \"     +    authMethod )     +     \"  ]  \"  )  )  ;", "}", "String   kerberosPrincipal    =    repositorySettings . get (  . CONF _ SECURITY _ PRINCIPAL )  ;", "if    (  ( kerberosPrincipal    !  =    null )     &  &     ( authMethod . equals ( SIMPLE )  )  )     {", ". LOGGER . warn (  (  \" Hadoop   authentication   method   is   set   to    [ SIMPLE ]  ,    but   a   Kerberos   principal   is    \"     +     \" specified .    Continuing   with    [ KERBEROS ]    authentication .  \"  )  )  ;", "SecurityUtil . setAuthenticationMethod ( KERBEROS ,    hadoopConfiguration )  ;", "} else", "if    (  ( kerberosPrincipal    =  =    null )     &  &     ( authMethod . equals ( KERBEROS )  )  )     {", "throw   new   RuntimeException (  (  (  (  \" HDFS   Repository   does   not   support    [ KERBEROS ]    authentication   without    \"     +     \" a   valid   Kerberos   principal   and   keytab .    Please   specify   a   principal   in   the   repository   settings   with    [  \"  )     +     (  . CONF _ SECURITY _ PRINCIPAL )  )     +     \"  ]  .  \"  )  )  ;", "}", "UserGroupInformation . setConfiguration ( hadoopConfiguration )  ;", ". LOGGER . debug (  \" Hadoop   security   enabled :     [  {  }  ]  \"  ,    UserGroupInformation . isSecurityEnabled (  )  )  ;", ". LOGGER . debug (  \" Using   Hadoop   authentication   method :     [  {  }  ]  \"  ,    SecurityUtil . getAuthenticationMethod ( hadoopConfiguration )  )  ;", "try    {", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "String   principal    =     . preparePrincipal ( kerberosPrincipal )  ;", "String   keytab    =    HdfsSecurityContext . locateKeytabFile ( environment )  . toString (  )  ;", ". LOGGER . debug (  \" Using   kerberos   principal    [  {  }  ]    and   keytab   located   at    [  {  }  ]  \"  ,    principal ,    keytab )  ;", "return   UserGroupInformation . loginUserFromKeytabAndReturnUGI ( principal ,    keytab )  ;", "}", "return   UserGroupInformation . getCurrentUser (  )  ;", "}    catch    ( IOException   e )     {", "throw   new   UncheckedIOException (  \" Could   not   retrieve   the   current   user   information \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["login"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsRepository"}, {"methodBody": ["METHOD_START", "{", "String   finalPrincipal    =    originalPrincipal ;", "if    ( originalPrincipal . contains (  \"  _ HOST \"  )  )     {", "try    {", "finalPrincipal    =    SecurityUtil . getServerPrincipal ( originalPrincipal ,     . getHostName (  )  )  ;", "}    catch    ( IOException   e )     {", "throw   new   UncheckedIOException ( e )  ;", "}", "if    (  ( originalPrincipal . equals ( finalPrincipal )  )     =  =    false )     {", ". LOGGER . debug (  \" Found   service   principal .    Converted   original   principal   name    [  {  }  ]    to   server   principal    [  {  }  ]  \"  ,    originalPrincipal ,    finalPrincipal )  ;", "}", "}", "return   finalPrincipal ;", "}", "METHOD_END"], "methodName": ["preparePrincipal"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsRepository"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "if    ( restrictPermissions )     {", "return   Accessroller . doPrivileged ( action ,    null ,    this . getRestrictedExecutionPermissions (  )  )  ;", "} else    {", "return   Accessroller . doPrivileged ( action )  ;", "}", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedOrThrow"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsSecurityContext"}, {"methodBody": ["METHOD_START", "{", "if    ( ugi . isFromKeytab (  )  )     {", "try    {", "ugi . checkTGTAndReloginFromKeytab (  )  ;", "}    catch    ( IOException   ioe )     {", "throw   new   UncheckedIOException (  \" Could   not   re - authenate \"  ,    ioe )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["ensureLogin"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsSecurityContext"}, {"methodBody": ["METHOD_START", "{", "return   restrictedExecutionPermissions ;", "}", "METHOD_END"], "methodName": ["getRestrictedExecutionPermissions"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsSecurityContext"}, {"methodBody": ["METHOD_START", "{", "Path   keytabPath    =    environment . configFile (  )  . resolve (  \" repository - hdfs \"  )  . resolve (  \" krb 5  . keytab \"  )  ;", "try    {", "if    (  ( Files . exists ( keytabPath )  )     =  =    false )     {", "throw   new   RuntimeException (  (  (  \" Could   not   locate   keytab   at    [  \"     +    keytabPath )     +     \"  ]  .  \"  )  )  ;", "}", "}    catch    ( SecurityException   se )     {", "throw   new   RuntimeException (  (  (  \" Could   not   locate   keytab   at    [  \"     +    keytabPath )     +     \"  ]  \"  )  ,    se )  ;", "}", "return   keytabPath ;", "}", "METHOD_END"], "methodName": ["locateKeytabFile"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsSecurityContext"}, {"methodBody": ["METHOD_START", "{", "Permission [  ]    permissions ;", "if    ( ugi . isFromKeytab (  )  )     {", "int   permlen    =     (  . KERBEROS _ AUTH _ PERMISSIONS . length )     +     1  ;", "permissions    =    new   Permission [ permlen ]  ;", "System . arraycopy (  . KERBEROS _ AUTH _ PERMISSIONS ,     0  ,    permissions ,     0  ,     . KERBEROS _ AUTH _ PERMISSIONS . length )  ;", "permissions [  (  ( permissions . length )     -     1  )  ]     =    new   ServicePermission ( ugi . getUserName (  )  ,     \" initiate \"  )  ;", "} else    {", "permissions    =    Arrays . copyOf (  . SIMPLE _ AUTH _ PERMISSIONS ,     . SIMPLE _ AUTH _ PERMISSIONS . length )  ;", "}", "return   permissions ;", "}", "METHOD_END"], "methodName": ["renderPermissions"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsSecurityContext"}, {"methodBody": ["METHOD_START", "{", "return   client . prepareSearch ( index )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ;", "}", "METHOD_END"], "methodName": ["count"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "client (  )  . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" hdfs \"  )  . setSettings ( Settings . builder (  )  . put (  \" uri \"  ,     \"  / path \"  )  . build (  )  )  . get (  )  ;", "fail (  )  ;", "}    catch    ( RyException   e )     {", "assertTrue (  (  ( e . getCause (  )  )    instanceof   IllegalArgumentException )  )  ;", "assertTrue ( e . getCause (  )  . getMessage (  )  ,    e . getCause (  )  . getMessage (  )  . contains (  \" Invalid   scheme    [ null ]    specified   in   uri    [  / path ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testEmptyUri"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "client (  )  . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" hdfs \"  )  . setSettings ( Settings . builder (  )  . put (  \" uri \"  ,     \" hdfs :  /  /  /  \"  )  . build (  )  )  . get (  )  ;", "fail (  )  ;", "}    catch    ( RyException   e )     {", "assertTrue (  (  ( e . getCause (  )  )    instanceof   IllegalArgumentException )  )  ;", "assertTrue ( e . getCause (  )  . getMessage (  )  . contains (  \" No    ' path '    defined   for   hdfs \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMissingPath"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "client (  )  . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" hdfs \"  )  . setSettings ( EMPTY )  . get (  )  ;", "fail (  )  ;", "}    catch    ( RyException   e )     {", "assertTrue (  (  ( e . getCause (  )  )    instanceof   IllegalArgumentException )  )  ;", "assertTrue ( e . getCause (  )  . getMessage (  )  . contains (  \" No    ' uri '    defined   for   hdfs \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMissingUri"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "client (  )  . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" hdfs \"  )  . setSettings ( Settings . builder (  )  . put (  \" uri \"  ,     \" file :  /  /  /  \"  )  . build (  )  )  . get (  )  ;", "fail (  )  ;", "}    catch    ( RyException   e )     {", "assertTrue (  (  ( e . getCause (  )  )    instanceof   IllegalArgumentException )  )  ;", "assertTrue ( e . getCause (  )  . getMessage (  )  . contains (  \" Invalid   scheme    [ file ]    specified   in   uri    [ file :  /  /  /  ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonHdfsUri"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "client (  )  . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" hdfs \"  )  . setSettings ( Settings . builder (  )  . put (  \" uri \"  ,     \" hdfs :  /  /  / some / path \"  )  . build (  )  )  . get (  )  ;", "fail (  )  ;", "}    catch    ( RyException   e )     {", "assertTrue (  (  ( e . getCause (  )  )    instanceof   IllegalArgumentException )  )  ;", "assertTrue ( e . getCause (  )  . getMessage (  )  . contains (  \" Use    ' path '    option   to   specify   a   path    [  / some / path ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPathSpecifiedInHdfs"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "PutRyResponse   putRyResponse    =    client . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" hdfs \"  )  . setSettings ( Settings . builder (  )  . put (  \" uri \"  ,     \" hdfs :  /  /  /  \"  )  . put (  \" conf . fs . AbstractFileSystem . hdfs . impl \"  ,    TestingFs . class . getName (  )  )  . put (  \" path \"  ,     \" foo \"  )  . put (  \" chunk _ size \"  ,     (  ( randomIntBetween (  1  0  0  ,     1  0  0  0  )  )     +     \" k \"  )  )  . put (  \" compress \"  ,    randomBoolean (  )  )  )  . get (  )  ;", "assertThat ( putRyResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "createIndex (  \" test - idx -  1  \"  )  ;", "createIndex (  \" test - idx -  2  \"  )  ;", "createIndex (  \" test - idx -  3  \"  )  ;", "ensureGreen (  )  ;", "logger . info (  \"  -  -  >    indexing   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "client (  )  . prepareIndex (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . setSource (  \" foo \"  ,     (  \" bar \"     +    i )  )  . get (  )  ;", "client (  )  . prepareIndex (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . setSource (  \" foo \"  ,     (  \" bar \"     +    i )  )  . get (  )  ;", "client (  )  . prepareIndex (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . setSource (  \" foo \"  ,     (  \" bar \"     +    i )  )  . get (  )  ;", "}", "client (  )  . admin (  )  . indices (  )  . prepareRefresh (  )  . get (  )  ;", "assertThat ( count ( client ,     \" test - idx -  1  \"  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( count ( client ,     \" test - idx -  2  \"  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( count ( client ,     \" test - idx -  3  \"  )  ,    equalTo (  1  0  0 L )  )  ;", "logger . info (  \"  -  -  >    snapshot \"  )  ;", "CreateSnapshotResponse   createSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareCreateSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  ,     \"  - test - idx -  3  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    equalTo ( createSnapshotResponse . getSnapshotInfo (  )  . totalShards (  )  )  )  ;", "assertThat ( client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" test - repo \"  )  . setSnapshots (  \" test - snap \"  )  . get (  )  . getSnapshots (  )  . get (  0  )  . state (  )  ,    equalTo ( SUCCESS )  )  ;", "logger . info (  \"  -  -  >    delete   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     5  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "for    ( int   i    =     5  0  ;    i    <     1  0  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i    +  =     2  )     {", "client . prepareDelete (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "client (  )  . admin (  )  . indices (  )  . prepareRefresh (  )  . get (  )  ;", "assertThat ( count ( client ,     \" test - idx -  1  \"  )  ,    equalTo (  5  0 L )  )  ;", "assertThat ( count ( client ,     \" test - idx -  2  \"  )  ,    equalTo (  5  0 L )  )  ;", "assertThat ( count ( client ,     \" test - idx -  3  \"  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    close   indices \"  )  ;", "client . admin (  )  . indices (  )  . prepareClose (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  . get (  )  ;", "logger . info (  \"  -  -  >    restore   all   indices   from   the   snapshot \"  )  ;", "RestoreSnapshotResponse   restoreSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreSnapshotResponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( count ( client ,     \" test - idx -  1  \"  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( count ( client ,     \" test - idx -  2  \"  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( count ( client ,     \" test - idx -  3  \"  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    delete   indices \"  )  ;", "client (  )  . admin (  )  . indices (  )  . prepareDelete (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  . get (  )  ;", "logger . info (  \"  -  -  >    restore   one   index   after   deletion \"  )  ;", "restoreSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  ,     \"  - test - idx -  2  \"  )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreSnapshotResponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( count ( client ,     \" test - idx -  1  \"  )  ,    equalTo (  1  0  0 L )  )  ;", "ClusterState   clusterState    =    client . admin (  )  . cluster (  )  . prepareState (  )  . get (  )  . getState (  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  1  \"  )  ,    equalTo ( true )  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  2  \"  )  ,    equalTo ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleWorkflow"], "fileName": "org.elasticsearch.repositories.hdfs.HdfsTests"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.repositories.hdfs.RepositoryHdfsClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "final   FileSystemProvider   baseProvider    =    base . getFileSystem (  )  . provider (  )  ;", "return   new   RawLocalFileSystem (  )     {", "private   Path   box ( Path   path )     {", "return   new   apache . hadoop . fs . Path ( path . toUri (  )  )  ;", "}", "private   Path   unbox ( apache . hadoop . fs . Path   path )     {", "return   baseProvider . getPath ( path . toUri (  )  )  ;", "}", "@ Override", "protected   Path   getInitialWorkingDirectory (  )     {", "return   box ( base )  ;", "}", "@ Override", "public   void   setPermission ( apache . hadoop . fs . Path   path ,    FsPermission   permission )     {", "}", "@ Override", "public   boolean   supportsSymlinks (  )     {", "return   false ;", "}", "@ Override", "public   FileStatus   getFileLinkStatus ( apache . hadoop . fs . Path   path )    throws   IOException    {", "return   getFileStatus ( path )  ;", "}", "@ Override", "public   Path   getLinkTarget ( apache . hadoop . fs . Path   path )    throws   IOException    {", "return   path ;", "}", "@ Override", "public   FileStatus   getFileStatus ( apache . hadoop . fs . Path   path )    throws   IOException    {", "BasicFileAttributes   attributes ;", "try    {", "attributes    =    Files . readAttributes ( unbox ( path )  ,    BasicFileAttributes . class )  ;", "}    catch    ( NoSuchFileException   e )     {", "FileNotFoundException   fnfe    =    new   FileNotFoundException (  (  (  \" File    \"     +    path )     +     \"    does   not   exist \"  )  )  ;", "fnfe . initCause ( e )  ;", "throw   fnfe ;", "}", "long   length    =    attributes . size (  )  ;", "boolean   isDir    =    attributes . isDirectory (  )  ;", "int   blockReplication    =     1  ;", "long   blockSize    =    getDefaultBlockSize ( path )  ;", "long   modificationTime    =    attributes . creationTime (  )  . toMillis (  )  ;", "return   new   FileStatus ( length ,    isDir ,    blockReplication ,    blockSize ,    modificationTime ,    path )  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["wrap"], "fileName": "org.elasticsearch.repositories.hdfs.TestingFs"}, {"methodBody": ["METHOD_START", "{", "createIndex (  \" test - idx -  1  \"  )  ;", "ensureGreen (  )  ;", "logger . info (  \"  -  -  >    indexing   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "index (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" bar \"     +    i )  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "logger . info (  \"  -  -  >    snapshot \"  )  ;", "Createponse   createponse    =    client . admin (  )  . cluster (  )  . prepareCreateSnapshot ( repository ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  )  . get (  )  ;", "assertThat ( createponse . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( createponse . getSnapshotInfo (  )  . successfulShards (  )  ,    equalTo ( createponse . getSnapshotInfo (  )  . totalShards (  )  )  )  ;", "assertThat ( client . admin (  )  . cluster (  )  . prepareGetSnapshots ( repository )  . setSnapshots (  \" test - snap \"  )  . get (  )  . getSnapshots (  )  . get (  0  )  . state (  )  ,    equalTo ( SUCCESS )  )  ;", "logger . info (  \"  -  -  >    delete   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     5  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    close   indices \"  )  ;", "client . admin (  )  . indices (  )  . prepareClose (  \" test - idx -  1  \"  )  . get (  )  ;", "logger . info (  \"  -  -  >    restore   all   indices   from   the   snapshot \"  )  ;", "Restoreponse   restoreponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot ( repository ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "}", "METHOD_END"], "methodName": ["assertRepositoryIsOperational"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    internalCluster (  )  . getInstance ( Settings . class )  ;", "Settings [  ]    buckets    =    new   Settings [  ]  {    settings . getByPrefix (  \"  \"  )  ,    settings . getByPrefix (  \" private - bucket .  \"  )  ,    settings . getByPrefix (  \" remote - bucket .  \"  )  ,    settings . getByPrefix (  \" external - bucket .  \"  )     }  ;", "for    ( Settings   bucket    :    buckets )     {", "String   bucketName    =    bucket . get (  \" bucket \"  )  ;", "assertThat (  \" Your   settings   in   elasticsearch . yml   are   incorrect .    Check   README   file .  \"  ,    bucketName ,    notNullValue (  )  )  ;", "AmazonS 3    client    =    internalCluster (  )  . getInstance ( AwsS 3 Service . class )  . client ( EMPTY )  ;", "try    {", "ObjectListing   prevListing    =    null ;", "DeleteObjectsRequest   multiObjectDeleteRequest    =    null ;", "ArrayList < DeleteObjectsRequest . KeyVersion >    keys    =    new   ArrayList <  >  (  )  ;", "while    ( true )     {", "ObjectListing   list ;", "if    ( prevListing    !  =    null )     {", "list    =    client . listNextBatchOfObjects ( prevListing )  ;", "} else    {", "list    =    client . listObjects ( bucketName ,    basePath )  ;", "multiObjectDeleteRequest    =    new   DeleteObjectsRequest ( list . getBucketName (  )  )  ;", "}", "for    ( S 3 ObjectSummary   summary    :    list . getObjectSummaries (  )  )     {", "keys . add ( new   DeleteObjectsRequest . KeyVersion ( summary . getKey (  )  )  )  ;", "if    (  ( keys . size (  )  )     >     5  0  0  )     {", "multiObjectDeleteRequest . setKeys ( keys )  ;", "client . deleteObjects ( multiObjectDeleteRequest )  ;", "multiObjectDeleteRequest    =    new   DeleteObjectsRequest ( list . getBucketName (  )  )  ;", "keys . clear (  )  ;", "}", "}", "if    ( list . isTruncated (  )  )     {", "prevListing    =    list ;", "} else    {", "break ;", "}", "}", "if    (  !  ( keys . isEmpty (  )  )  )     {", "multiObjectDeleteRequest . setKeys ( keys )  ;", "client . deleteObjects ( multiObjectDeleteRequest )  ;", "}", "}    catch    ( Exception   ex )     {", "logger . warn (  (  ( Supplier <  ?  >  )     (  (  )     -  >    new   ParameterizedMessage (  \" Failed   to   delete   S 3    repository    [  {  }  ]  \"  ,    bucketName )  )  )  ,    ex )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["cleanRepositoryFiles"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    internalCluster (  )  . getInstance ( Settings . class )  . get (  \" bucket \"  )  ,    basePath )  ;", "Settings   repositorySettings    =    Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  . put ( S 3 Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    randomIntBetween (  1  0  0  0  ,     1  0  0  0  0  )  )  . put ( S 3 Repository . SERVER _ SIDE _ ENCRYPTION _ SETTING . getKey (  )  ,    true )  . build (  )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( repositorySettings )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "createIndex (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  ,     \" test - idx -  3  \"  )  ;", "ensureGreen (  )  ;", "logger . info (  \"  -  -  >    indexing   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "index (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" bar \"     +    i )  )  ;", "index (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "index (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  3  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "logger . info (  \"  -  -  >    snapshot \"  )  ;", "CreateSnapshotResponse   createSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareCreateSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  ,     \"  - test - idx -  3  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    equalTo ( createSnapshotResponse . getSnapshotInfo (  )  . totalShards (  )  )  )  ;", "assertThat ( client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" test - repo \"  )  . setSnapshots (  \" test - snap \"  )  . get (  )  . getSnapshots (  )  . get (  0  )  . state (  )  ,    equalTo ( SUCCESS )  )  ;", "Settings   settings    =    internalCluster (  )  . getInstance ( Settings . class )  ;", "Settings   bucket    =    settings . getByPrefix (  \"  \"  )  ;", "AmazonS 3    s 3 Client    =    internalCluster (  )  . getInstance ( AwsS 3 Service . class )  . client ( repositorySettings )  ;", "String   bucketName    =    bucket . get (  \" bucket \"  )  ;", "logger . info (  \"  -  -  >    verify   encryption   for   bucket    [  {  }  ]  ,    prefix    [  {  }  ]  \"  ,    bucketName ,    basePath )  ;", "List < S 3 ObjectSummary >    summaries    =    s 3 Client . listObjects ( bucketName ,    basePath )  . getObjectSummaries (  )  ;", "for    ( S 3 ObjectSummary   summary    :    summaries )     {", "assertThat ( s 3 Client . getObjectMetadata ( bucketName ,    summary . getKey (  )  )  . getSSEAlgorithm (  )  ,    equalTo (  \" AES 2  5  6  \"  )  )  ;", "}", "logger . info (  \"  -  -  >    delete   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     5  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "for    ( int   i    =     5  0  ;    i    <     1  0  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i    +  =     2  )     {", "client . prepareDelete (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  3  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    close   indices \"  )  ;", "client . admin (  )  . indices (  )  . prepareClose (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  . get (  )  ;", "logger . info (  \"  -  -  >    restore   all   indices   from   the   snapshot \"  )  ;", "RestoreSnapshotResponse   restoreSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreSnapshotResponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  3  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    delete   indices \"  )  ;", "cluster (  )  . wipeIndices (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  ;", "logger . info (  \"  -  -  >    restore   one   index   after   deletion \"  )  ;", "restoreSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  ,     \"  - test - idx -  2  \"  )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreSnapshotResponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "ClusterState   clusterState    =    client . admin (  )  . cluster (  )  . prepareState (  )  . get (  )  . getState (  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  1  \"  )  ,    equalTo ( true )  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  2  \"  )  ,    equalTo ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testEncryption"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "ClusterAdminClient   client    =    client (  )  . admin (  )  . cluster (  )  ;", "logger . info (  \"  -  -  >       creating   s 3    y   without   any   path \"  )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "try    {", "client . prepareGetSnapshots (  \" test - repo \"  )  . addSnapshots (  \" no - existing - snapshot \"  )  . get (  )  ;", "fail (  \" Shouldn ' t   be   here \"  )  ;", "}    catch    ( SnapshotMissingException   ex )     {", "}", "try    {", "client . prepareDeleteSnapshot (  \" test - repo \"  ,     \" no - existing - snapshot \"  )  . get (  )  ;", "fail (  \" Shouldn ' t   be   here \"  )  ;", "}    catch    ( SnapshotMissingException   ex )     {", "}", "}", "METHOD_END"], "methodName": ["testGetDeleteNonExistingSnapshot86"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    internalCluster (  )  . getInstance ( Settings . class )  . get (  \" bucket \"  )  ,    basePath )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "logger . info (  \"  -  -  >    restore   non   existing   snapshot \"  )  ;", "try    {", "client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" no - existing - snapshot \"  )  . setWaitForCompletion ( true )  . execute (  )  . actionGet (  )  ;", "fail (  \" Shouldn ' t   be   here \"  )  ;", "}    catch    ( SnapshotMissingException   ex )     {", "}", "}", "METHOD_END"], "methodName": ["testNonExistingRepo86"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "Settings   settings    =    internalCluster (  )  . getInstance ( Settings . class )  ;", "Settings   bucketSettings    =    settings . getByPrefix (  \" remote - bucket .  \"  )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    bucketSettings . get (  \" bucket \"  )  ,    basePath )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  . put ( S 3 Repository . BUCKET _ SETTING . getKey (  )  ,    bucketSettings . get (  \" bucket \"  )  )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "assertRepositoryIsOperational ( client ,     \" test - repo \"  )  ;", "}", "METHOD_END"], "methodName": ["testRepositoryInRemoteRegion"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "Settings   bucketSettings    =    internalCluster (  )  . getInstance ( Settings . class )  . getByPrefix (  \" remote - bucket .  \"  )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    bucketSettings . get (  \" bucket \"  )  ,    basePath )  ;", "try    {", "client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  . put ( S 3 Repository . BUCKET _ SETTING . getKey (  )  ,    bucketSettings . get (  \" bucket \"  )  )  )  . get (  )  ;", "fail (  \" repository   verification   should   have   raise   an   exception !  \"  )  ;", "}    catch    ( RepositoryVerificationException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testRepositoryInRemoteRegionIsRemote"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "PutRyResponse   putRyResponse    =    client . admin (  )  . cluster (  )  . preparePutRy (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Ry . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  )  . get (  )  ;", "assertThat ( putRyResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "assertRyIsOperational ( client ,     \" test - repo \"  )  ;", "}", "METHOD_END"], "methodName": ["testRepositoryWithBasePath"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "Settings   bucketSettings    =    internalCluster (  )  . getInstance ( Settings . class )  . getByPrefix (  \" private - bucket .  \"  )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    bucketSettings . get (  \" bucket \"  )  ,    basePath )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  . put ( S 3 Repository . BUCKET _ SETTING . getKey (  )  ,    bucketSettings . get (  \" bucket \"  )  )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "assertRepositoryIsOperational ( client ,     \" test - repo \"  )  ;", "}", "METHOD_END"], "methodName": ["testRepositoryWithCustomCredentials"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "Settings   bucketSettings    =    internalCluster (  )  . getInstance ( Settings . class )  . getByPrefix (  \" private - bucket .  \"  )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    bucketSettings . get (  \" bucket \"  )  ,    basePath )  ;", "try    {", "client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  . put ( S 3 Repository . BUCKET _ SETTING . getKey (  )  ,    bucketSettings . get (  \" bucket \"  )  )  )  . get (  )  ;", "fail (  \" repository   verification   should   have   raise   an   exception !  \"  )  ;", "}    catch    ( RepositoryVerificationException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testRepositoryWithCustomCredentialsIsNotAccessibleByDefaultCredentials"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "Settings   bucketSettings    =    internalCluster (  )  . getInstance ( Settings . class )  . getByPrefix (  \" external - bucket .  \"  )  ;", "logger . info (  \"  -  -  >    creating   s 3    repostoriy   with   endpoint    [  {  }  ]  ,    bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    bucketSettings . get (  \" endpoint \"  )  ,    bucketSettings . get (  \" bucket \"  )  ,    basePath )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( Settings . builder (  )  . put ( S 3 Repository . BUCKET _ SETTING . getKey (  )  ,    bucketSettings . get (  \" bucket \"  )  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "assertRepositoryIsOperational ( client ,     \" test - repo \"  )  ;", "}", "METHOD_END"], "methodName": ["testRepositoryWithCustomEndpointProtocol"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "Client   client    =    client (  )  ;", "Settings . Builder   settings    =    Settings . builder (  )  . put ( S 3 Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    randomIntBetween (  1  0  0  0  ,     1  0  0  0  0  )  )  ;", "settings . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,    basePath )  ;", "logger . info (  \"  -  -  >       creating   s 3    repository   with   bucket [  {  }  ]    and   path    [  {  }  ]  \"  ,    internalCluster (  )  . getInstance ( Settings . class )  . get (  \" bucket \"  )  ,    basePath )  ;", "PutRepositoryResponse   putRepositoryResponse    =    client . admin (  )  . cluster (  )  . preparePutRepository (  \" test - repo \"  )  . setType (  \" s 3  \"  )  . setSettings ( settings )  . get (  )  ;", "assertThat ( putRepositoryResponse . isAcknowledged (  )  ,    equalTo ( true )  )  ;", "createIndex (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  ,     \" test - idx -  3  \"  )  ;", "ensureGreen (  )  ;", "logger . info (  \"  -  -  >    indexing   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "index (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" bar \"     +    i )  )  ;", "index (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "index (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  ,     \" foo \"  ,     (  \" baz \"     +    i )  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  3  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "logger . info (  \"  -  -  >    snapshot \"  )  ;", "CreateSnapshotResponse   createSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareCreateSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  ,     \"  - test - idx -  3  \"  )  . get (  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    greaterThan (  0  )  )  ;", "assertThat ( createSnapshotResponse . getSnapshotInfo (  )  . successfulShards (  )  ,    equalTo ( createSnapshotResponse . getSnapshotInfo (  )  . totalShards (  )  )  )  ;", "assertThat ( client . admin (  )  . cluster (  )  . prepareGetSnapshots (  \" test - repo \"  )  . setSnapshots (  \" test - snap \"  )  . get (  )  . getSnapshots (  )  . get (  0  )  . state (  )  ,    equalTo ( SUCCESS )  )  ;", "logger . info (  \"  -  -  >    delete   some   data \"  )  ;", "for    ( int   i    =     0  ;    i    <     5  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  1  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "for    ( int   i    =     5  0  ;    i    <     1  0  0  ;    i +  +  )     {", "client . prepareDelete (  \" test - idx -  2  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i    +  =     2  )     {", "client . prepareDelete (  \" test - idx -  3  \"  ,     \" doc \"  ,    Integer . toString ( i )  )  . get (  )  ;", "}", "refresh (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  3  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    close   indices \"  )  ;", "client . admin (  )  . indices (  )  . prepareClose (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  . get (  )  ;", "logger . info (  \"  -  -  >    restore   all   indices   from   the   snapshot \"  )  ;", "RestoreSnapshotResponse   restoreSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreSnapshotResponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  2  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  3  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  5  0 L )  )  ;", "logger . info (  \"  -  -  >    delete   indices \"  )  ;", "cluster (  )  . wipeIndices (  \" test - idx -  1  \"  ,     \" test - idx -  2  \"  )  ;", "logger . info (  \"  -  -  >    restore   one   index   after   deletion \"  )  ;", "restoreSnapshotResponse    =    client . admin (  )  . cluster (  )  . prepareRestoreSnapshot (  \" test - repo \"  ,     \" test - snap \"  )  . setWaitForCompletion ( true )  . setIndices (  \" test - idx -  *  \"  ,     \"  - test - idx -  2  \"  )  . execute (  )  . actionGet (  )  ;", "assertThat ( restoreSnapshotResponse . getRestoreInfo (  )  . totalShards (  )  ,    greaterThan (  0  )  )  ;", "ensureGreen (  )  ;", "assertThat ( client . prepareSearch (  \" test - idx -  1  \"  )  . setSize (  0  )  . get (  )  . getHits (  )  . getTotalHits (  )  ,    equalTo (  1  0  0 L )  )  ;", "ClusterState   clusterState    =    client . admin (  )  . cluster (  )  . prepareState (  )  . get (  )  . getState (  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  1  \"  )  ,    equalTo ( true )  )  ;", "assertThat ( clusterState . getMetaData (  )  . hasIndex (  \" test - idx -  2  \"  )  ,    equalTo ( false )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleWorkflow"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "AbstractS 3 SnapshotRestoreTest . wipeRepositories (  )  ;", "cleanRepositoryFiles ( basePath )  ;", "}", "METHOD_END"], "methodName": ["wipeAfter"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "AbstractS 3 SnapshotRestoreTest . wipeRepositories (  )  ;", "basePath    =     \" repo -  \"     +     ( randomInt (  )  )  ;", "cleanRepositoryFiles ( basePath )  ;", "}", "METHOD_END"], "methodName": ["wipeBefore"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "if    (  ( repositories . length )     =  =     0  )     {", "repositories    =    new   String [  ]  {     \"  *  \"     }  ;", "}", "for    ( String   repository    :    repositories )     {", "try    {", "client (  )  . admin (  )  . cluster (  )  . prepareDeleteRepository ( repository )  . execute (  )  . actionGet (  )  ;", "}    catch    ( RepositoryMissingException   ex )     {", "}", "}", "}", "METHOD_END"], "methodName": ["wipeRepositories"], "fileName": "org.elasticsearch.repositories.s3.AbstractS3SnapshotRestoreTest"}, {"methodBody": ["METHOD_START", "{", "final   InetSocketAddress   inetSocketAddress    =     (  ( InetSocketAddress )     ( address )  )  ;", "if    (  ( inetSocketAddress . getAddress (  )  )    instanceof   Inet 6 Address )     {", "ren    (  (  \"  [  \"     +     ( inetSocketAddress . getHostString (  )  )  )     +     \"  ]  :  \"  )     +     ( inetSocketAddress . getPort (  )  )  ;", "} else    {", "ren    (  ( inetSocketAddress . getHostString (  )  )     +     \"  :  \"  )     +     ( inetSocketAddress . getPort (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addressToString"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3Fixture"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args    =  =    null )     |  |     (  ( args . length )     !  =     2  )  )     {", "throw   new   IllegalArgumentException (  \"     < working   directory >     < bucket >  \"  )  ;", "}", "final   InetSocketAddress   socketAddress    =    new   InetSocketAddress ( InetAddress . getLoopbackAddress (  )  ,     0  )  ;", "final   HttpServer   httpServer    =    MockHttpServer . createHttp ( socketAddress ,     0  )  ;", "try    {", "final   Path   workingDirectory    =     . workingDir ( args [  0  ]  )  ;", ". writeFile ( workingDirectory ,     \" pid \"  ,    ManagementFactory . getRuntimeMXBean (  )  . getName (  )  . split (  \"  @  \"  )  [  0  ]  )  ;", "final   String   addressAndPort    =     . addressToString ( httpServer . getAddress (  )  )  ;", ". writeFile ( workingDirectory ,     \" ports \"  ,    addressAndPort )  ;", "final   String   storageUrl    =     \" http :  /  /  \"     +    addressAndPort ;", "final   AmazonS 3 TestServer   storageTestServer    =    new   AmazonS 3 TestServer ( storageUrl )  ;", "storageTestServer . createBucket ( args [  1  ]  )  ;", "httpServer . createContext (  \"  /  \"  ,    new    . ResponseHandler ( storageTestServer )  )  ;", "httpServer . start (  )  ;", "Thread . sleep ( Long . MAX _ VALUE )  ;", "}    finally    {", "httpServer . stop (  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3Fixture"}, {"methodBody": ["METHOD_START", "{", "return   Paths . get ( dir )  ;", "}", "METHOD_END"], "methodName": ["workingDir"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3Fixture"}, {"methodBody": ["METHOD_START", "{", "final   Path   tempPidFile    =    Files . createTempFile ( dir ,    null ,    null )  ;", "Files . write ( tempPidFile ,    Collections . singleton ( content )  )  ;", "Files . move ( tempPidFile ,    dir . resolve ( fileName )  ,    StandardCopyOption . ATOMIC _ MOVE )  ;", "}", "METHOD_END"], "methodName": ["writeFile"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3Fixture"}, {"methodBody": ["METHOD_START", "{", "buckets . put ( bucketName ,    new   AmazonS 3 TestServer . Bucket ( bucketName )  )  ;", "}", "METHOD_END"], "methodName": ["createBucket"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   PathTrie < AmazonS 3 TestServer . RequestHandler >    handlers    =    new   PathTrie ( RestUtils . REST _ DECODER )  ;", "AmazonS 3 TestServer . objectsPaths (  (  (  \" HEAD    \"     +    endpoint )     +     \"  /  { bucket }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    id )     -  >     {", "final   String   bucketName    =    params . get (  \" bucket \"  )  ;", "final   Bucket   bucket    =    buckets . get ( bucketName )  ;", "if    ( bucket    =  =    null )     {", "return   newBucketNotFoundError ( id ,    bucketName )  ;", "}", "final   String   objectName    =    objectName ( params )  ;", "for    ( Entry < String ,    byte [  ]  >    object    :    bucket . objects . entrySet (  )  )     {", "if    ( object . getKey (  )  . equals ( objectName )  )     {", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" text / plain \"  ,    AmazonS 3 TestServer . EMPTY _ BYTE )  ;", "}", "}", "return   newObjectNotFoundError ( id ,    objectName )  ;", "}  )  )  ;", "AmazonS 3 TestServer . objectsPaths (  (  (  \" PUT    \"     +    endpoint )     +     \"  /  { bucket }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    id )     -  >     {", "final   String   destBucketName    =    params . get (  \" bucket \"  )  ;", "final   Bucket   destBucket    =    buckets . get ( destBucketName )  ;", "if    ( destBucket    =  =    null )     {", "return   newBucketNotFoundError ( id ,    destBucketName )  ;", "}", "final   String   destObjectName    =    objectName ( params )  ;", "List < String >    headerCopySource    =    headers . getOrDefault (  \" x - amz - copy - source \"  ,    emptyList (  )  )  ;", "if    (  ( headerCopySource . isEmpty (  )  )     =  =    false )     {", "String   srcObjectName    =    headerCopySource . get (  0  )  ;", "Bucket   srcBucket    =    null ;", "for    ( Bucket   bucket    :    buckets . values (  )  )     {", "String   prefix    =     (  \"  /  \"     +    bucket . name )     +     \"  /  \"  ;", "if    ( srcObjectName . startsWith ( prefix )  )     {", "srcObjectName    =    srcObjectName . replaceFirst ( prefix ,     \"  \"  )  ;", "srcBucket    =    bucket ;", "break ;", "}", "}", "if    (  ( srcBucket    =  =    null )     |  |     (  ( srcBucket . objects . containsKey ( srcObjectName )  )     =  =    false )  )     {", "return   newObjectNotFoundError ( id ,    srcObjectName )  ;", "}", "byte [  ]    bytes    =    srcBucket . objects . get ( srcObjectName )  ;", "if    ( bytes    !  =    null )     {", "destBucket . objects . put ( destObjectName ,    bytes )  ;", "return   newCopyResultResponse ( id )  ;", "} else    {", "return   newObjectNotFoundError ( id ,    srcObjectName )  ;", "}", "} else    {", "List < String >    headerDecodedContentLength    =    headers . getOrDefault (  \" X - amz - decoded - content - length \"  ,    emptyList (  )  )  ;", "if    (  ( headerDecodedContentLength . size (  )  )     =  =     1  )     {", "int   contentLength    =    Integer . valueOf ( headerDecodedContentLength . get (  0  )  )  ;", "try    ( BufferedInputStream   inputStream    =    new   BufferedInputStream ( new   ByteArrayInputStream ( body )  )  )     {", "int   b ;", "while    (  ( b    =    inputStream . read (  )  )     !  =     (  -  1  )  )     {", "if    ( b    =  =     '  \\ n '  )     {", "break ;", "}", "}", "final   byte [  ]    bytes    =    new   byte [ contentLength ]  ;", "inputStream . read ( bytes ,     0  ,    contentLength )  ;", "destBucket . objects . put ( destObjectName ,    bytes )  ;", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" text / plain \"  ,    AmazonS 3 TestServer . EMPTY _ BYTE )  ;", "}", "}", "}", "return   newInternalError ( id ,     \" Something   is   wrong   with   this   PUT   request \"  )  ;", "}  )  )  ;", "AmazonS 3 TestServer . objectsPaths (  (  (  \" DELETE    \"     +    endpoint )     +     \"  /  { bucket }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    id )     -  >     {", "final   String   bucketName    =    params . get (  \" bucket \"  )  ;", "final   Bucket   bucket    =    buckets . get ( bucketName )  ;", "if    ( bucket    =  =    null )     {", "return   newBucketNotFoundError ( id ,    bucketName )  ;", "}", "final   String   objectName    =    objectName ( params )  ;", "if    (  ( bucket . objects . remove ( objectName )  )     !  =    null )     {", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" text / plain \"  ,    AmazonS 3 TestServer . EMPTY _ BYTE )  ;", "}", "return   newObjectNotFoundError ( id ,    objectName )  ;", "}  )  )  ;", "AmazonS 3 TestServer . objectsPaths (  (  (  \" GET    \"     +    endpoint )     +     \"  /  { bucket }  \"  )  )  . forEach (  (    path )     -  >    handlers . insert ( path ,     (    params ,    headers ,    body ,    id )     -  >     {", "final   String   bucketName    =    params . get (  \" bucket \"  )  ;", "final   Bucket   bucket    =    buckets . get ( bucketName )  ;", "if    ( bucket    =  =    null )     {", "return   newBucketNotFoundError ( id ,    bucketName )  ;", "}", "final   String   objectName    =    objectName ( params )  ;", "if    ( bucket . objects . containsKey ( objectName )  )     {", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" application / octet - stream \"  ,    bucket . objects . get ( objectName )  )  ;", "}", "return   newObjectNotFoundError ( id ,    objectName )  ;", "}  )  )  ;", "handlers . insert (  (  (  \" HEAD    \"     +    endpoint )     +     \"  /  { bucket }  \"  )  ,     (    params ,    headers ,    body ,    id )     -  >     {", "String   bucket    =    params . get (  \" bucket \"  )  ;", "if    (  ( Strings . hasText ( bucket )  )     &  &     ( buckets . containsKey ( bucket )  )  )     {", "return   new   Response ( RestStatus . OK ,    emptyMap (  )  ,     \" text / plain \"  ,    AmazonS 3 TestServer . EMPTY _ BYTE )  ;", "} else    {", "return   newBucketNotFoundError ( id ,    bucket )  ;", "}", "}  )  ;", "handlers . insert (  (  (  \" GET    \"     +    endpoint )     +     \"  /  { bucket }  /  \"  )  ,     (    params ,    headers ,    body ,    id )     -  >     {", "final   String   bucketName    =    params . get (  \" bucket \"  )  ;", "final   Bucket   bucket    =    buckets . get ( bucketName )  ;", "if    ( bucket    =  =    null )     {", "return   newBucketNotFoundError ( id ,    bucketName )  ;", "}", "String   prefix    =    params . get (  \" prefix \"  )  ;", "if    ( prefix    =  =    null )     {", "List < String >    prefixes    =    headers . get (  \" Prefix \"  )  ;", "if    (  ( prefixes    !  =    null )     &  &     (  ( prefixes . size (  )  )     =  =     1  )  )     {", "prefix    =    prefixes . get (  0  )  ;", "}", "}", "return   newListBucketResultResponse ( id ,    bucket ,    prefix )  ;", "}  )  ;", "handlers . insert (  (  (  \" POST    \"     +    endpoint )     +     \"  /  \"  )  ,     (    params ,    headers ,    body ,    id )     -  >     {", "final   List < String >    deletes    =    new   ArrayList <  >  (  )  ;", "final   List < String >    errors    =    new   ArrayList <  >  (  )  ;", "if    ( params . containsKey (  \" delete \"  )  )     {", "String   request    =    Streams . copyToString ( new   InputStreamReader ( new   ByteArrayInputStream ( body )  ,    StandardCharsets . UTF _  8  )  )  ;", "if    ( request . startsWith (  \"  < Delete >  \"  )  )     {", "final   String   startMarker    =     \"  < Key >  \"  ;", "final   String   endMarker    =     \"  <  / Key >  \"  ;", "int   offset    =     0  ;", "while    ( offset    !  =     (  -  1  )  )     {", "offset    =    request . indexOf ( startMarker ,    offset )  ;", "if    ( offset    >     0  )     {", "int   closingOffset    =    request . indexOf ( endMarker ,    offset )  ;", "if    ( closingOffset    !  =     (  -  1  )  )     {", "offset    =    offset    +     ( startMarker . length (  )  )  ;", "final   String   objectName    =    request . substring ( offset ,    closingOffset )  ;", "boolean   found    =    false ;", "for    ( Bucket   bucket    :    buckets . values (  )  )     {", "if    (  ( bucket . objects . remove ( objectName )  )     !  =    null )     {", "found    =    true ;", "}", "}", "if    ( found )     {", "deletes . add ( objectName )  ;", "} else    {", "errors . add ( objectName )  ;", "}", "}", "}", "}", "return   newDeleteResultResponse ( id ,    deletes ,    errors )  ;", "}", "}", "return   newInternalError ( id ,     \" Something   is   wrong   with   this   POST   multiple   deletes   request \"  )  ;", "}  )  ;", "return   handlers ;", "}", "METHOD_END"], "methodName": ["defaultHandlers"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "return   endpoint ;", "}", "METHOD_END"], "methodName": ["getEndpoint"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   long   requestId    =    requests . incrementAndGet (  )  ;", "final   Map < String ,    String >    params    =    new   HashMap <  >  (  )  ;", "if    ( query    !  =    null )     {", "RestUtils . decodeQueryString ( query ,     0  ,    params )  ;", "}", "final   List < String >    authorizations    =    headers . get (  \" Authorization \"  )  ;", "if    (  ( authorizations    =  =    null )     |  |     (  (  ( authorizations . isEmpty (  )  )     =  =    false )     &     (  ( authorizations . get (  0  )  . contains (  \" s 3  _ integration _ test _ access _ key \"  )  )     =  =    false )  )  )     {", "return    . newError ( requestId ,    FORBIDDEN ,     \" AccessDenied \"  ,     \" Access   Denied \"  ,     \"  \"  )  ;", "}", "final    . RequestHandler   handler    =    handlers . retrieve (  (  ( method    +     \"     \"  )     +    path )  ,    params )  ;", "if    ( handler    !  =    null )     {", "return   handler . execute ( params ,    headers ,    body ,    requestId )  ;", "} else    {", "return    . newInternalError ( requestId ,     (  (  (  (  \" No   handler   defined   for   request    [ method :     \"     +    method )     +     \"  ,    path :     \"  )     +    path )     +     \"  ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["handle"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "return   AmazonS 3 TestServer . newError ( requestId ,    NOT _ FOUND ,     \" NoSuchBucket \"  ,     \" The   specified   bucket   does   not   exist \"  ,    bucket )  ;", "}", "METHOD_END"], "methodName": ["newBucketNotFoundError"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   String   id    =    Long . toString ( requestId )  ;", "final   StringBuilder   response    =    new   StringBuilder (  )  ;", "response . append (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" UTF -  8  \\  \"  ?  >  \"  )  ;", "response . append (  \"  < CopyObjectResult >  \"  )  ;", "response . append (  \"  < LastModified >  \"  )  . append ( DateUtils . formatISO 8  6  0  1 Date ( new   Date (  )  )  )  . append (  \"  <  / LastModified >  \"  )  ;", "response . append (  \"  < ETag >  \"  )  . append ( requestId )  . append (  \"  <  / ETag >  \"  )  ;", "response . append (  \"  <  / CopyObjectResult >  \"  )  ;", "return   new    . Response ( RestStatus . OK ,    Collections . singletonMap (  \" x - amz - request - id \"  ,    id )  ,     \" application / xml \"  ,    response . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["newCopyResultResponse"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   String   id    =    Long . toString ( requestId )  ;", "final   StringBuilder   response    =    new   StringBuilder (  )  ;", "response . append (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" UTF -  8  \\  \"  ?  >  \"  )  ;", "response . append (  \"  < DeleteResult   xmlns =  \\  \" http :  /  / s 3  . amazonaws . com / doc /  2  0  0  6  -  0  3  -  0  1  /  \\  \"  >  \"  )  ;", "for    ( String   deletedObject    :    deletedObjects )     {", "response . append (  \"  < Deleted >  \"  )  ;", "response . append (  \"  < Key >  \"  )  . append ( deletedObject )  . append (  \"  <  / Key >  \"  )  ;", "response . append (  \"  <  / Deleted >  \"  )  ;", "}", "for    ( String   ignoredObject    :    ignoredObjects )     {", "response . append (  \"  < Error >  \"  )  ;", "response . append (  \"  < Key >  \"  )  . append ( ignoredObject )  . append (  \"  <  / Key >  \"  )  ;", "response . append (  \"  < Code > NoSuchKey <  / Code >  \"  )  ;", "response . append (  \"  <  / Error >  \"  )  ;", "}", "response . append (  \"  <  / DeleteResult >  \"  )  ;", "return   new    . Response ( RestStatus . OK ,    Collections . singletonMap (  \" x - amz - request - id \"  ,    id )  ,     \" application / xml \"  ,    response . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["newDeleteResultResponse"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   String   id    =    Long . toString ( requestId )  ;", "final   StringBuilder   response    =    new   StringBuilder (  )  ;", "response . append (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" UTF -  8  \\  \"  ?  >  \"  )  ;", "response . append (  \"  < Error >  \"  )  ;", "response . append (  \"  < Code >  \"  )  . append ( code )  . append (  \"  <  / Code >  \"  )  ;", "response . append (  \"  < Message >  \"  )  . append ( message )  . append (  \"  <  / Message >  \"  )  ;", "response . append (  \"  < Resource >  \"  )  . append ( resource )  . append (  \"  <  / Resource >  \"  )  ;", "response . append (  \"  < RequestId >  \"  )  . append ( id )  . append (  \"  <  / RequestId >  \"  )  ;", "response . append (  \"  <  / Error >  \"  )  ;", "return   new    . Response ( status ,    Collections . singletonMap (  \" x - amz - request - id \"  ,    id )  ,     \" application / xml \"  ,    response . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["newError"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "return   AmazonS 3 TestServer . newError ( requestId ,    INTERNAL _ SERVER _ ERROR ,     \" InternalError \"  ,     \" We   encountered   an   internal   error \"  ,    resource )  ;", "}", "METHOD_END"], "methodName": ["newInternalError"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   String   id    =    Long . toString ( requestId )  ;", "final   StringBuilder   response    =    new   StringBuilder (  )  ;", "response . append (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" UTF -  8  \\  \"  ?  >  \"  )  ;", "response . append (  \"  < ListBucketResult   xmlns =  \\  \" http :  /  / s 3  . amazonaws . com / doc /  2  0  0  6  -  0  3  -  0  1  /  \\  \"  >  \"  )  ;", "response . append (  \"  < Prefix >  \"  )  ;", "if    ( prefix    !  =    null )     {", "response . append ( prefix )  ;", "}", "response . append (  \"  <  / Prefix >  \"  )  ;", "response . append (  \"  < Marker /  >  \"  )  ;", "response . append (  \"  < MaxKeys >  1  0  0  0  <  / MaxKeys >  \"  )  ;", "response . append (  \"  < IsTruncated > false <  / IsTruncated >  \"  )  ;", "int   count    =     0  ;", "for    ( Map . Entry < String ,    byte [  ]  >    object    :    bucket . objects . entrySet (  )  )     {", "String   objectName    =    object . getKey (  )  ;", "if    (  ( prefix    =  =    null )     |  |     ( objectName . startsWith ( prefix )  )  )     {", "response . append (  \"  < Contents >  \"  )  ;", "response . append (  \"  < Key >  \"  )  . append ( objectName )  . append (  \"  <  / Key >  \"  )  ;", "response . append (  \"  < LastModified >  \"  )  . append ( DateUtils . formatISO 8  6  0  1 Date ( new   Date (  )  )  )  . append (  \"  <  / LastModified >  \"  )  ;", "response . append (  \"  < ETag >  & quot ;  \"  )  . append (  ( count +  +  )  )  . append (  \"  & quot ;  <  / ETag >  \"  )  ;", "response . append (  \"  < Size >  \"  )  . append ( object . getValue (  )  . length )  . append (  \"  <  / Size >  \"  )  ;", "response . append (  \"  <  / Contents >  \"  )  ;", "}", "}", "response . append (  \"  <  / ListBucketResult >  \"  )  ;", "return   new    . Response ( RestStatus . OK ,    Collections . singletonMap (  \" x - amz - request - id \"  ,    id )  ,     \" application / xml \"  ,    response . toString (  )  . getBytes ( StandardCharsets . UTF _  8  )  )  ;", "}", "METHOD_END"], "methodName": ["newListBucketResultResponse"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "return   AmazonS 3 TestServer . newError ( requestId ,    NOT _ FOUND ,     \" NoSuchKey \"  ,     \" The   specified   key   does   not   exist \"  ,    object )  ;", "}", "METHOD_END"], "methodName": ["newObjectNotFoundError"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   StringBuilder   name    =    new   StringBuilder (  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "String   value    =    params . getOrDefault (  (  \" path \"     +    i )  ,    null )  ;", "if    ( value    !  =    null )     {", "if    (  ( name . length (  )  )     >     0  )     {", "name . append (  '  /  '  )  ;", "}", "name . append ( value )  ;", "}", "}", "return   name . toString (  )  ;", "}", "METHOD_END"], "methodName": ["objectName"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "final   List < String >    paths    =    new   ArrayList <  >  (  )  ;", "String   p    =    path ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "p    =     (  ( p    +     \"  /  { path \"  )     +    i )     +     \"  }  \"  ;", "paths . add ( p )  ;", "}", "return   paths ;", "}", "METHOD_END"], "methodName": ["objectsPaths"], "fileName": "org.elasticsearch.repositories.s3.AmazonS3TestServer"}, {"methodBody": ["METHOD_START", "{", "String   configName    =    InternalAwsS 3 Service . CLIENT _ NAME . get ( singleRepositorySettings )  ;", "S 3 ClientSettings   clientSettings    =    S 3 ClientSettings . getClientSettings ( settings ,    configName )  ;", "AWSCredentials   credentials    =    InternalAwsS 3 Service . buildCredentials ( logger ,    deprecationLogger ,    clientSettings ,    singleRepositorySettings )  . getCredentials (  )  ;", "assertThat ( credentials . getAWSAccessKeyId (  )  ,    is ( expectedKey )  )  ;", "assertThat ( credentials . getAWSSecretKey (  )  ,    is ( expectedSecret )  )  ;", "}", "METHOD_END"], "methodName": ["assertCredentials"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "String   configName    =    InternalAwsS 3 Service . CLIENT _ NAME . get ( repositorySettings )  ;", "S 3 ClientSettings   clientSettings    =    S 3 ClientSettings . getClientSettings ( settings ,    configName )  ;", "assertThat ( clientSettings . endpoint ,    is ( expectedEndpoint )  )  ;", "}", "METHOD_END"], "methodName": ["assertEndpoint"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "S 3 ClientSettings   clientSettings    =    S 3 ClientSettings . getClientSettings ( settings ,     \" default \"  )  ;", "ClientConfiguration   configuration    =    Internal . buildConfiguration ( clientSettings )  ;", "assertThat ( configuration . getResponseMetadataCacheSize (  )  ,    is (  0  )  )  ;", "assertThat ( configuration . getProtocol (  )  ,    is ( expectedProtocol )  )  ;", "assertThat ( configuration . getProxyHost (  )  ,    is ( expectedProxyHost )  )  ;", "assertThat ( configuration . getProxyPort (  )  ,    is ( expectedProxyPort )  )  ;", "assertThat ( configuration . getProxyUsername (  )  ,    is ( expectedProxyUsername )  )  ;", "assertThat ( configuration . getProxyPassword (  )  ,    is ( expectedProxyPassword )  )  ;", "assertThat ( configuration . getMaxErrorRetry (  )  ,    is ( expectedMaxRetries )  )  ;", "assertThat ( configuration . useThrottledRetries (  )  ,    is ( expectedUseThrottleRetries )  )  ;", "assertThat ( configuration . getSocketTimeout (  )  ,    is ( expectedReadTimeout )  )  ;", "}", "METHOD_END"], "methodName": ["launchAWSConfigurationTest"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  \" s 3  . client . default . proxy . username \"  ,     \" aws _ proxy _ username \"  )  ;", "secureSettings . setString (  \" s 3  . client . default . proxy . password \"  ,     \" aws _ proxy _ password \"  )  ;", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( secureSettings )  . put (  \" s 3  . client . default . protocol \"  ,     \" http \"  )  . put (  \" s 3  . client . default . proxy . host \"  ,     \" aws _ proxy _ host \"  )  . put (  \" s 3  . client . default . proxy . port \"  ,     8  0  8  0  )  . put (  \" s 3  . client . default . read _ timeout \"  ,     \"  1  0 s \"  )  . build (  )  ;", "launchAWSConfiguration ( settings ,    HTTP ,     \" aws _ proxy _ host \"  ,     8  0  8  0  ,     \" aws _ proxy _ username \"  ,     \" aws _ proxy _ password \"  ,     3  ,    DEFAULT _ THROTTLE _ RETRIES ,     1  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testAWSConfigurationWithAwsSettings"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "S 3 ClientSettings   clientSettings    =    S 3 ClientSettings . getClientSettings ( EMPTY ,     \" default \"  )  ;", "AWSCredentialsProvider   credentialsProvider    =    Internal . buildCredentials ( logger ,    deprecationLogger ,    clientSettings ,    EMPTY )  ;", "assertThat ( credentialsProvider ,    instanceOf ( Internal . PrivilegedInstanceProfileCredentialsProvider . class )  )  ;", "}", "METHOD_END"], "methodName": ["testAWSCredentialsWithSystemProviders"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "launchAWSConfigurationTest ( EMPTY ,    HTTPS ,    null ,     (  -  1  )  ,    null ,    null ,     3  ,    DEFAULT _ THROTTLE _ RETRIES ,    DEFAULT _ SOCKET _ TIMEOUT )  ;", "}", "METHOD_END"], "methodName": ["testAWSDefaultConfiguration"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "MockSecureSettingecureSettings    =    new   MockSecureSettings (  )  ;", "secureSettingetString (  \" s 3  . client . default . access _ key \"  ,     \" aws _ key \"  )  ;", "secureSettingetString (  \" s 3  . client . default . secret _ key \"  ,     \" awecret \"  )  ;", "Settingettings    =    Settings . builder (  )  . setSecureSettingecureSettings )  . build (  )  ;", "assertCredentials ( EMPTY ,    settings ,     \" aws _ key \"  ,     \" awecret \"  )  ;", "}", "METHOD_END"], "methodName": ["testAwsCredsDefaultSettings"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   repositorySettings    =    Settings . builder (  )  . put ( InternalAwsS 3 Service . CLIENT _ NAME . getKey (  )  ,     \" myconfig \"  )  . build (  )  ;", "MockSecureSettings   secureSettings    =    new   MockSecureSettings (  )  ;", "secureSettings . setString (  \" s 3  . client . myconfig . access _ key \"  ,     \" aws _ key \"  )  ;", "secureSettings . setString (  \" s 3  . client . myconfig . secret _ key \"  ,     \" aws _ secret \"  )  ;", "secureSettings . setString (  \" s 3  . client . default . access _ key \"  ,     \" wrong _ key \"  )  ;", "secureSettings . setString (  \" s 3  . client . default . secret _ key \"  ,     \" wrong _ secret \"  )  ;", "Settings   settings    =    Settings . builder (  )  . setSecureSettings ( secureSettings )  . build (  )  ;", "assertCredentials ( repositorySettings ,    settings ,     \" aws _ key \"  ,     \" aws _ secret \"  )  ;", "}", "METHOD_END"], "methodName": ["testAwsCredsExplicitConfigSettings"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" s 3  . client . default . endpoint \"  ,     \" s 3  . endpoint \"  )  . build (  )  ;", "assertEndpoint ( EMPTY ,    settings ,     \" s 3  . endpoint \"  )  ;", "}", "METHOD_END"], "methodName": ["testEndpointSetting"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   settings    =    Settings . builder (  )  . put (  \" s 3  . client . default . max _ retries \"  ,     5  )  . build (  )  ;", "launchAWSConfigurationTest ( settings ,    HTTPS ,    null ,     (  -  1  )  ,    null ,    null ,     5  ,    DEFAULT _ THROTTLE _ RETRIES ,     5  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testRepositoryMaxRetries"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   repositorySettings    =    Settings . builder (  )  . put ( S 3 Repository . ACCESS _ KEY _ SETTING . getKey (  )  ,     \" aws _ key \"  )  . put ( S 3 Repository . SECRET _ KEY _ SETTING . getKey (  )  ,     \" aws _ secret \"  )  . build (  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    assertCredentials ( repositorySettings ,    Settings . EMPTY ,     \" aws _ key \"  ,     \" aws _ secret \"  )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \" Setting    [ access _ key ]    is   insecure \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRepositorySettingsCredentialsDisallowed"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   repositorySettings    =    Settings . builder (  )  . put ( S 3 Repository . SECRET _ KEY _ SETTING . getKey (  )  ,     \" aws _ secret \"  )  . build (  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    assertCredentials ( repositorySettings ,    Settings . EMPTY ,     \" aws _ key \"  ,     \" aws _ secret \"  )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \" must   be   accompanied   by   setting    [ access _ key ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRepositorySettingsCredentialsMissingKey"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "Settings   repositorySettings    =    Settings . builder (  )  . put ( S 3 Repository . ACCESS _ KEY _ SETTING . getKey (  )  ,     \" aws _ key \"  )  . build (  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    assertCredentials ( repositorySettings ,    Settings . EMPTY ,     \" aws _ key \"  ,     \" aws _ secret \"  )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \" must   be   accompanied   by   setting    [ secret _ key ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRepositorySettingsCredentialsMissingSecret"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "final   boolean   throttling    =    randomBoolean (  )  ;", "Settings   settings    =    Settings . builder (  )  . put (  \" s 3  . client . default . use _ throttle _ ret \"  ,    throttling )  . build (  )  ;", "launchAWSConfigurationTest ( settings ,    HTTPS ,    null ,     (  -  1  )  ,    null ,    null ,     3  ,    throttling ,     5  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testRepositoryThrottleRetries"], "fileName": "org.elasticsearch.repositories.s3.AwsS3ServiceImplTests"}, {"methodBody": ["METHOD_START", "{", "ClientConfiguration   clientConfiguration    =    new   ClientConfiguration (  )  ;", "clientConfiguration . setResponseMetadataCacheSize (  0  )  ;", "clientConfiguration . setProtocol ( clientSettings . protocol )  ;", "if    ( Strings . hasText ( clientSettings . proxyHost )  )     {", "clientConfiguration . setProxyHost ( clientSettings . proxyHost )  ;", "clientConfiguration . setProxyPort ( clientSettings . proxyPort )  ;", "clientConfiguration . setProxyUsme ( clientSettings . proxyUsme )  ;", "clientConfiguration . setProxyPassword ( clientSettings . proxyPassword )  ;", "}", "clientConfiguration . setMaxErrorRetry ( clientSettings . maxRetries )  ;", "clientConfiguration . setUseThrottleRetries ( clientSettings . throttleRetries )  ;", "clientConfiguration . setSocketTimeout ( clientSettings . readTimeoutMillis )  ;", "return   clientConfiguration ;", "}", "METHOD_END"], "methodName": ["buildConfiguration"], "fileName": "org.elasticsearch.repositories.s3.InternalAwsS3Service"}, {"methodBody": ["METHOD_START", "{", "BasicAWSCredentials   credentials    =    clientSettings . credentials ;", "if    ( S 3 Repository . ACCESS _ KEY _ SETTING . exists ( repositorySettings )  )     {", "if    (  ( S 3 Repository . SECRET _ KEY _ SETTING . exists ( repositorySettings )  )     =  =    false )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Repository   setting    [  \"     +     ( S 3 Repository . ACCESS _ KEY _ SETTING . getKey (  )  )  )     +     \"    must   be   accompanied   by   setting    [  \"  )     +     ( S 3 Repository . SECRET _ KEY _ SETTING . getKey (  )  )  )     +     \"  ]  \"  )  )  ;", "}", "try    ( SecureString   key    =    S 3 Repository . ACCESS _ KEY _ SETTING . get ( repositorySettings )  ; SecureString   secret    =    S 3 Repository . SECRET _ KEY _ SETTING . get ( repositorySettings )  )     {", "credentials    =    new   BasicAWSCredentials ( key . toString (  )  ,    secret . toString (  )  )  ;", "}", "deprecationLogger . deprecated (  (  \" Using   s 3    access / secret   key   from   repository   settings .    Instead    \"     +     \" store   these   in   named   clients   and   the   elasticsearch   keystore   for   secure   settings .  \"  )  )  ;", "} else", "if    ( S 3 Repository . SECRET _ KEY _ SETTING . exists ( repositorySettings )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Repository   setting    [  \"     +     ( S 3 Repository . SECRET _ KEY _ SETTING . getKey (  )  )  )     +     \"    must   be   accompanied   by   setting    [  \"  )     +     ( S 3 Repository . ACCESS _ KEY _ SETTING . getKey (  )  )  )     +     \"  ]  \"  )  )  ;", "}", "if    ( credentials    =  =    null )     {", "logger . debug (  \" Using   instance   profile   credentials \"  )  ;", "return   new    . PrivilegedInstanceProfileCredentialsProvider (  )  ;", "} else    {", "logger . debug (  \" Using   basic   key / secret   credentials \"  )  ;", "return   new   com . amazonaws . internal . StaticCredentialsProvider ( credentials )  ;", "}", "}", "METHOD_END"], "methodName": ["buildCredentials"], "fileName": "org.elasticsearch.repositories.s3.InternalAwsS3Service"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.repositories.s3.RepositoryS3ClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "Settings   repositorySettings    =    Settings . builder (  )  . put ( S 3 Repository . ACCESS _ KEY _ SETTING . getKey (  )  ,     \" aws _ key \"  )  . put ( S 3 Repository . SECRET _ KEY _ SETTING . getKey (  )  ,     \" aws _ secret \"  )  . build (  )  ;", "AWSCredentials   credentials    =    InternalAwsS 3 Service . buildCredentials ( logger ,    deprecationLogger ,    S 3 ClientSettings . getClientSettings ( EMPTY ,     \" default \"  )  ,    repositorySettings )  . getCredentials (  )  ;", "assertEquals (  \" aws _ key \"  ,    credentials . getAWSAccessKeyId (  )  )  ;", "assertEquals (  \" aws _ secret \"  ,    credentials . getAWSSecretKey (  )  )  ;", "assertSettingDeprecationsAndWarnings ( new   Setting <  ?  >  [  ]  {    S 3 Repository . ACCESS _ KEY _ SETTING ,    S 3 Repository . SECRET _ KEY _ SETTING    }  ,     (  \" Using   s 3    access / secret   key   from   repository   settings .     \"     +     \" Instead   store   these   in   named   clients   and   the   elasticsearch   keystore   for   secure   settings .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRepositorySettingsCredentials"], "fileName": "org.elasticsearch.repositories.s3.RepositorySettingsCredentialsTests"}, {"methodBody": ["METHOD_START", "{", "return    ( keyPath )     +    blobName ;", "}", "METHOD_END"], "methodName": ["buildKey"], "fileName": "org.elasticsearch.repositories.s3.S3BlobContainer"}, {"methodBody": ["METHOD_START", "{", "if    ( blobSize    >     ( S 3 Repository . MAX _ FILE _ SIZE _ USING _ MULTIPART . getBytes (  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Multipart   upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   larger   than    \"  )     +     ( S 3 Repository . MAX _ FILE _ SIZE _ USING _ MULTIPART )  )  )  ;", "}", "if    ( blobSize    <     ( S 3 Repository . MIN _ PART _ SIZE _ USING _ MULTIPART . getBytes (  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Multipart   upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   smaller   than    \"  )     +     ( S 3 Repository . MIN _ PART _ SIZE _ USING _ MULTIPART )  )  )  ;", "}", "final   long   partSize    =    blobStore . bufferSizeInBytes (  )  ;", "final   Tuple < Long ,    Long >    multiparts    =     . numberOfMultiparts ( blobSize ,    partSize )  ;", "if    (  ( multiparts . v 1  (  )  )     >     ( Integer . MAX _ VALUE )  )     {", "throw   new   IllegalArgumentException (  \" Too   many   multipart   upload   requests ,    maybe   try   a   larger   buffer   size ?  \"  )  ;", "}", "final   int   nbParts    =    multiparts . v 1  (  )  . intValue (  )  ;", "final   long   lastPartSize    =    multiparts . v 2  (  )  ;", "assert   blobSize    =  =     (  (  ( nbParts    -     1  )     *    partSize )     +    lastPartSize )     :     \" blobSize   does   not   match   multipart   sizes \"  ;", "final   SetOnce < String >    uploadId    =    new   SetOnce (  )  ;", "final   String   bucketName    =    blobStore . bucket (  )  ;", "boolean   success    =    false ;", "try    {", "final   InitiateMultipartUploadRequest   initRequest    =    new   InitiateMultipartUploadRequest ( bucketName ,    blobName )  ;", "initRequest . setStorageClass ( blobStore . getStorageClass (  )  )  ;", "initRequest . setCannedACL ( blobStore . getCannedACL (  )  )  ;", "if    ( blobStore . serverSideEncryption (  )  )     {", "final   ObjectMetadata   md    =    new   ObjectMetadata (  )  ;", "md . setSSEAlgorithm ( AES _  2  5  6  _ SERVER _ SIDE _ ENCRYPTION )  ;", "initRequest . setObjectMetadata ( md )  ;", "}", "uploadId . set ( blobStore . client (  )  . initiateMultipartUpload ( initRequest )  . getUploadId (  )  )  ;", "if    ( Strings . isEmpty ( uploadId . get (  )  )  )     {", "throw   new   IOException (  (  \" Failed   to   initialize   multipart   upload    \"     +    blobName )  )  ;", "}", "final   List < PartETag >    parts    =    new   ArrayList <  >  (  )  ;", "long   bytesCount    =     0  ;", "for    ( int   i    =     1  ;    i    <  =    nbParts ;    i +  +  )     {", "final   UploadPartRequest   uploadRequest    =    new   UploadPartRequest (  )  ;", "uploadRequest . setBucketName ( bucketName )  ;", "uploadRequest . setKey ( blobName )  ;", "uploadRequest . setUploadId ( uploadId . get (  )  )  ;", "uploadRequest . setPartNumber ( i )  ;", "uploadRequest . setInputStream ( input )  ;", "if    ( i    <    nbParts )     {", "uploadRequest . setPartSize ( partSize )  ;", "uploadRequest . setLastPart ( false )  ;", "} else    {", "uploadRequest . setPartSize ( lastPartSize )  ;", "uploadRequest . setLastPart ( true )  ;", "}", "bytesCount    +  =    uploadRequest . getPartSize (  )  ;", "final   UploadPartResult   uploadResponse    =    blobStore . client (  )  . uploadPart ( uploadRequest )  ;", "parts . add ( uploadResponse . getPartETag (  )  )  ;", "}", "if    ( bytesCount    !  =    blobSize )     {", "throw   new   IOException (  (  (  (  (  (  \" Failed   to   execute   multipart   upload   for    [  \"     +    blobName )     +     \"  ]  ,    expected    \"  )     +    blobSize )     +     \" bytes   sent   but   got    \"  )     +    bytesCount )  )  ;", "}", "CompleteMultipartUploadRequest   complRequest    =    new   CompleteMultipartUploadRequest ( bucketName ,    blobName ,    uploadId . get (  )  ,    parts )  ;", "blobStore . client (  )  . completeMultipartUpload ( complRequest )  ;", "success    =    true ;", "}    catch    ( AmazonClientException   e )     {", "throw   new   IOException (  (  (  \" Unable   to   upload   object    [  \"     +    blobName )     +     \"  ]    using   multipart   upload \"  )  ,    e )  ;", "}    finally    {", "if    (  ( success    =  =    false )     &  &     ( Strings . hasLength ( uploadId . get (  )  )  )  )     {", "final   AbortMultipartUploadRequest   abortRequest    =    new   AbortMultipartUploadRequest ( bucketName ,    blobName ,    uploadId . get (  )  )  ;", "blobStore . client (  )  . abortMultipartUpload ( abortRequest )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["executeMultipartUpload"], "fileName": "org.elasticsearch.repositories.s3.S3BlobContainer"}, {"methodBody": ["METHOD_START", "{", "if    ( blobSize    >     ( S 3 Repository . MAX _ FILE _ SIZE . getBytes (  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   larger   than    \"  )     +     ( S 3 Repository . MAX _ FILE _ SIZE )  )  )  ;", "}", "if    ( blobSize    >     ( blobStore . bufferSizeInBytes (  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  \" Upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   larger   than   buffer   size \"  )  )  ;", "}", "try    {", "final   ObjectMetadata   md    =    new   ObjectMetadata (  )  ;", "md . setContentLength ( blobSize )  ;", "if    ( blobStore . serverSideEncryption (  )  )     {", "md . setSSEAlgorithm ( AES _  2  5  6  _ SERVER _ SIDE _ ENCRYPTION )  ;", "}", "final   PutObjectRequest   putRequest    =    new   PutObjectRequest ( blobStore . bucket (  )  ,    blobName ,    input ,    md )  ;", "putRequest . setStorageClass ( blobStore . getStorageClass (  )  )  ;", "putRequest . setCannedAcl ( blobStore . getCannedACL (  )  )  ;", "blobStore . client (  )  . putObject ( putRequest )  ;", "}    catch    ( AmazonClientException   e )     {", "throw   new   IOException (  (  (  \" Unable   to   upload   object    [  \"     +    blobName )     +     \"  ]    using   a   single   upload \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["executeSingleUpload"], "fileName": "org.elasticsearch.repositories.s3.S3BlobContainer"}, {"methodBody": ["METHOD_START", "{", "if    ( partSize    <  =     0  )     {", "throw   new   IllegalArgumentException (  \" Part   size   must   be   greater   than   zero \"  )  ;", "}", "if    (  ( totalSize    =  =     0 L )     |  |     ( totalSize    <  =    partSize )  )     {", "return   Tuple . tuple (  1 L ,    totalSize )  ;", "}", "final   long   parts    =    totalSize    /    partSize ;", "final   long   reming    =    totalSize    %    partSize ;", "if    ( reming    =  =     0  )     {", "return   Tuple . tuple ( parts ,    partSize )  ;", "} else    {", "return   Tuple . tuple (  ( parts    +     1  )  ,    reming )  ;", "}", "}", "METHOD_END"], "methodName": ["numberOfMultiparts"], "fileName": "org.elasticsearch.repositories.s3.S3BlobContainer"}, {"methodBody": ["METHOD_START", "{", "return   bucket ;", "}", "METHOD_END"], "methodName": ["bucket"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "return   bufferSize . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["bufferSizeInBytes"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "return   client ;", "}", "METHOD_END"], "methodName": ["client"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "return   cannedACL ;", "}", "METHOD_END"], "methodName": ["getCannedACL"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "return   storageClass ;", "}", "METHOD_END"], "methodName": ["getStorageClass"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( cannedACL    =  =    null )     |  |     ( cannedACL . equals (  \"  \"  )  )  )     {", "return   CannedAccessControlList . Private ;", "}", "for    ( CannedAccessControlList   cur    :    CannedAccessControlList . values (  )  )     {", "if    ( cur . toString (  )  . equalsIgnoreCase ( cannedACL )  )     {", "return   cur ;", "}", "}", "throw   new   Exception (  (  (  \" cannedACL   is   not   valid :     [  \"     +    cannedACL )     +     \"  ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["initCannedACL"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( storageClass    =  =    null )     |  |     ( storageClass . equals (  \"  \"  )  )  )     {", "return   StorageClass . Standard ;", "}", "try    {", "StorageClass    _ storageClass    =    StorageClass . fromValue ( storageClass . toUpperCase ( Locale . ENGLISH )  )  ;", "if    (  _ storageClass . equals ( Glacier )  )     {", "throw   new   Exception (  \" Glacier   storage   class   is   not   supported \"  )  ;", "}", "return    _ storageClass ;", "}    catch    ( IllegalArgumentException   illegalArgumentException )     {", "throw   new   Exception (  (  (  \"  `  \"     +    storageClass )     +     \"  `    is   not   a   valid   S 3    Storage   Class .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initStorageClass"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "return   serverSideEncryption ;", "}", "METHOD_END"], "methodName": ["serverSideEncryption"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStore"}, {"methodBody": ["METHOD_START", "{", "final   Tuple < Long ,    Long >    result    =    S 3 BlobContainer . numberOfMultiparts ( totalSize ,    partSize )  ;", "assertEquals (  (  (  (  (  \" Expected   number   of   parts    [  \"     +    expectedParts )     +     \"  ]    but   got    [  \"  )     +     ( result . v 1  (  )  )  )     +     \"  ]  \"  )  ,    expectedParts ,     (  ( long )     ( result . v 1  (  )  )  )  )  ;", "assertEquals (  (  (  (  (  \" Expected   remaining    [  \"     +    expectedRemaining )     +     \"  ]    but   got    [  \"  )     +     ( result . v 2  (  )  )  )     +     \"  ]  \"  )  ,    expectedRemaining ,     (  ( long )     ( result . v 2  (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["assertNumberOfMultiparts"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "return   S 3 BlobStoreTests . randomMockS 3 BlobStore (  )  ;", "}", "METHOD_END"], "methodName": ["newBlobStore"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   String   bucketName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "final   String   blobName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "final   BlobPath   blobPath    =    new   BlobPath (  )  ;", "if    ( randomBoolean (  )  )     {", "IntStream . of ( randomIntBetween (  1  ,     5  )  )  . forEach (  (    value )     -  >    blobPath . add (  (  \" path _  \"     +    value )  )  )  ;", "}", "final   long   blobSize    =    GB . toBytes ( randomIntBetween (  1  ,     1  2  8  )  )  ;", "final   long   bufferSize    =    MB . toBytes ( randomIntBetween (  5  ,     1  0  2  4  )  )  ;", "final      blobStore    =    mock (  . class )  ;", "when ( blobStore . bucket (  )  )  . thenReturn ( bucketName )  ;", "when ( blobStore . bufferSizeInBytes (  )  )  . thenReturn ( bufferSize )  ;", "final   boolean   serverSideEncryption    =    randomBoolean (  )  ;", "when ( blobStore . serverSideEncryption (  )  )  . thenReturn ( serverSideEncryption )  ;", "final   StorageClass   storageClass    =    randomFrom ( StorageClass . values (  )  )  ;", "when ( blobStore . getStorageClass (  )  )  . thenReturn ( storageClass )  ;", "final   CannedAccessControlList   cannedAccessControlList    =     ( randomBoolean (  )  )     ?    randomFrom ( CannedAccessControlList . values (  )  )     :    null ;", "if    ( cannedAccessControlList    !  =    null )     {", "when ( blobStore . getCannedACL (  )  )  . thenReturn ( cannedAccessControlList )  ;", "}", "final   AmazonS 3    client    =    mock ( AmazonS 3  . class )  ;", "when ( blobStore . client (  )  )  . thenReturn ( client )  ;", "final   ArgumentCaptor < InitiateMultipartUploadRequest >    initArgCaptor    =    ArgumentCaptor . forClass ( InitiateMultipartUploadRequest . class )  ;", "final   InitiateMultipartUploadResult   initResult    =    new   InitiateMultipartUploadResult (  )  ;", "initResult . setUploadId ( randomAlphaOfLength (  1  0  )  )  ;", "when ( client . initiateMultipartUpload ( initArgCaptor . capture (  )  )  )  . thenReturn ( initResult )  ;", "final   ArgumentCaptor < UploadPartRequest >    uploadArgCaptor    =    ArgumentCaptor . forClass ( UploadPartRequest . class )  ;", "final   List < String >    expectedEtags    =    new   ArrayList <  >  (  )  ;", "long   partSize    =    Math . min ( bufferSize ,    blobSize )  ;", "long   totalBytes    =     0  ;", "do    {", "expectedEtags . add ( randomAlphaOfLength (  5  0  )  )  ;", "totalBytes    +  =    partSize ;", "}    while    ( totalBytes    <    blobSize    )  ;", "when ( client . uploadPart ( uploadArgCaptor . capture (  )  )  )  . thenAnswer (  (    invocationOnMock )     -  >     {", "final   UploadPartRequest   request    =     (  ( UploadPartRequest )     ( invocationOnMock . getArguments (  )  [  0  ]  )  )  ;", "final   UploadPartResult   response    =    new   UploadPartResult (  )  ;", "response . setPartNumber ( request . getPartNumber (  )  )  ;", "response . setETag ( expectedEtags . get (  (  ( request . getPartNumber (  )  )     -     1  )  )  )  ;", "return   response ;", "}  )  ;", "final   ArgumentCaptor < CompleteMultipartUploadRequest >    compArgCaptor    =    ArgumentCaptor . forClass ( CompleteMultipartUploadRequest . class )  ;", "when ( client . completeMultipartUpload ( compArgCaptor . capture (  )  )  )  . thenReturn ( new   CompleteMultipartUploadResult (  )  )  ;", "final   ByteArrayInputStream   inputStream    =    new   ByteArrayInputStream ( new   byte [  0  ]  )  ;", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( blobPath ,    blobStore )  ;", "blobContainer . executeMultipartUpload ( blobStore ,    blobName ,    inputStream ,    blobSize )  ;", "final   InitiateMultipartUploadRequest   initRequest    =    initArgCaptor . getValue (  )  ;", "assertEquals ( bucketName ,    initRequest . getBucketName (  )  )  ;", "assertEquals (  (  ( blobPath . buildAsString (  )  )     +    blobName )  ,    initRequest . getKey (  )  )  ;", "assertEquals ( storageClass ,    initRequest . getStorageClass (  )  )  ;", "assertEquals ( cannedAccessControlList ,    initRequest . getCannedACL (  )  )  ;", "if    ( serverSideEncryption )     {", "assertEquals ( AES _  2  5  6  _ SERVER _ SIDE _ ENCRYPTION ,    initRequest . getObjectMetadata (  )  . getSSEAlgorithm (  )  )  ;", "}", "final   Tuple < Long ,    Long >    numberOfParts    =    S 3 BlobContainer . numberOfMultiparts ( blobSize ,    bufferSize )  ;", "final   List < UploadPartRequest >    uploadRequests    =    uploadArgCaptor . getAllValues (  )  ;", "assertEquals ( numberOfParts . v 1  (  )  . intValue (  )  ,    uploadRequests . size (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( uploadRequests . size (  )  )  ;    i +  +  )     {", "UploadPartRequest   uploadRequest    =    uploadRequests . get ( i )  ;", "assertEquals ( bucketName ,    uploadRequest . getBucketName (  )  )  ;", "assertEquals (  (  ( blobPath . buildAsString (  )  )     +    blobName )  ,    uploadRequest . getKey (  )  )  ;", "assertEquals ( initResult . getUploadId (  )  ,    uploadRequest . getUploadId (  )  )  ;", "assertEquals (  ( i    +     1  )  ,    uploadRequest . getPartNumber (  )  )  ;", "assertEquals ( inputStream ,    uploadRequest . getInputStream (  )  )  ;", "if    ( i    =  =     (  ( uploadRequests . size (  )  )     -     1  )  )     {", "assertTrue ( uploadRequest . isLastPart (  )  )  ;", "assertEquals ( numberOfParts . v 2  (  )  . longValue (  )  ,    uploadRequest . getPartSize (  )  )  ;", "} else    {", "assertFalse ( uploadRequest . isLastPart (  )  )  ;", "assertEquals ( bufferSize ,    uploadRequest . getPartSize (  )  )  ;", "}", "}", "final   CompleteMultipartUploadRequest   compRequest    =    compArgCaptor . getValue (  )  ;", "assertEquals ( bucketName ,    compRequest . getBucketName (  )  )  ;", "assertEquals (  (  ( blobPath . buildAsString (  )  )     +    blobName )  ,    compRequest . getKey (  )  )  ;", "assertEquals ( initResult . getUploadId (  )  ,    compRequest . getUploadId (  )  )  ;", "List < String >    actualETags    =    compRequest . getPartETags (  )  . stream (  )  . map ( PartETag :  : getETag )  . collect ( Collectors . toList (  )  )  ;", "assertEquals ( expectedEtags ,    actualETags )  ;", "}", "METHOD_END"], "methodName": ["testExecuteMultipartUpload"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   String   bucketName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "final   String   blobName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "final   BlobPath   blobPath    =    new   BlobPath (  )  ;", "final   long   blobSize    =    MB . toBytes (  7  6  5  )  ;", "final   long   bufferSize    =    MB . toBytes (  1  5  0  )  ;", "final      blobStore    =    mock (  . class )  ;", "when ( blobStore . bucket (  )  )  . thenReturn ( bucketName )  ;", "when ( blobStore . bufferSizeInBytes (  )  )  . thenReturn ( bufferSize )  ;", "when ( blobStore . getStorageClass (  )  )  . thenReturn ( randomFrom ( StorageClass . values (  )  )  )  ;", "final   AmazonS 3    client    =    mock ( AmazonS 3  . class )  ;", "when ( blobStore . client (  )  )  . thenReturn ( client )  ;", "final   String   uploadId    =    randomAlphaOfLength (  2  5  )  ;", "final   int   stage    =    randomInt (  2  )  ;", "final   List < AmazonClientException >    exceptions    =    Arrays . asList ( new   AmazonClientException (  \" Expected   initialization   request   to   fail \"  )  ,    new   AmazonClientException (  \" Expected   upload   part   request   to   fail \"  )  ,    new   AmazonClientException (  \" Expected   completion   request   to   fail \"  )  )  ;", "if    ( stage    =  =     0  )     {", "when ( client . initiateMultipartUpload ( any ( InitiateMultipartUploadRequest . class )  )  )  . thenThrow ( exceptions . get ( stage )  )  ;", "} else", "if    ( stage    =  =     1  )     {", "final   com . amazonaws . services . s 3  . model . InitiateMultipartUploadResult   initResult    =    new   com . amazonaws . services . s 3  . model . InitiateMultipartUploadResult (  )  ;", "initResult . setUploadId ( uploadId )  ;", "when ( client . initiateMultipartUpload ( any ( InitiateMultipartUploadRequest . class )  )  )  . thenReturn ( initResult )  ;", "when ( client . uploadPart ( any ( UploadPartRequest . class )  )  )  . thenThrow ( exceptions . get ( stage )  )  ;", "} else    {", "final   com . amazonaws . services . s 3  . model . InitiateMultipartUploadResult   initResult    =    new   com . amazonaws . services . s 3  . model . InitiateMultipartUploadResult (  )  ;", "initResult . setUploadId ( uploadId )  ;", "when ( client . initiateMultipartUpload ( any ( InitiateMultipartUploadRequest . class )  )  )  . thenReturn ( initResult )  ;", "when ( client . uploadPart ( any ( UploadPartRequest . class )  )  )  . thenAnswer (  (    invocationOnMock )     -  >     {", "final   UploadPartRequest   request    =     (  ( UploadPartRequest )     ( invocationOnMock . getArguments (  )  [  0  ]  )  )  ;", "final   com . amazonaws . services . s 3  . model . UploadPartResult   response    =    new   com . amazonaws . services . s 3  . model . UploadPartResult (  )  ;", "response . setPartNumber ( request . getPartNumber (  )  )  ;", "response . setETag ( randomAlphaOfLength (  2  0  )  )  ;", "return   response ;", "}  )  ;", "when ( client . completeMultipartUpload ( any ( CompleteMultipartUploadRequest . class )  )  )  . thenThrow ( exceptions . get ( stage )  )  ;", "}", "final   ArgumentCaptor < AbortMultipartUploadRequest >    argumentCaptor    =    ArgumentCaptor . forClass ( AbortMultipartUploadRequest . class )  ;", "doNothing (  )  . when ( client )  . abortMultipartUpload ( argumentCaptor . capture (  )  )  ;", "final   IOException   e    =    expectThrows ( IOException . class ,     (  )     -  >     {", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( blobPath ,    blobStore )  ;", "blobContainer . executeMultipartUpload ( blobStore ,    blobName ,    new   ByteArrayInputStream ( new   byte [  0  ]  )  ,    blobSize )  ;", "}  )  ;", "assertEquals (  (  (  \" Unable   to   upload   object    [  \"     +    blobName )     +     \"  ]    using   multipart   upload \"  )  ,    e . getMessage (  )  )  ;", "assertThat ( e . getCause (  )  ,    instanceOf ( AmazonClientException . class )  )  ;", "assertEquals ( exceptions . get ( stage )  . getMessage (  )  ,    e . getCause (  )  . getMessage (  )  )  ;", "if    ( stage    =  =     0  )     {", "verify ( client ,    times (  1  )  )  . initiateMultipartUpload ( any ( InitiateMultipartUploadRequest . class )  )  ;", "verify ( client ,    times (  0  )  )  . uploadPart ( any ( UploadPartRequest . class )  )  ;", "verify ( client ,    times (  0  )  )  . completeMultipartUpload ( any ( CompleteMultipartUploadRequest . class )  )  ;", "verify ( client ,    times (  0  )  )  . abortMultipartUpload ( any ( AbortMultipartUploadRequest . class )  )  ;", "} else    {", "verify ( client ,    times (  1  )  )  . initiateMultipartUpload ( any ( InitiateMultipartUploadRequest . class )  )  ;", "if    ( stage    =  =     1  )     {", "verify ( client ,    times (  1  )  )  . uploadPart ( any ( UploadPartRequest . class )  )  ;", "verify ( client ,    times (  0  )  )  . completeMultipartUpload ( any ( CompleteMultipartUploadRequest . class )  )  ;", "} else    {", "verify ( client ,    times (  6  )  )  . uploadPart ( any ( UploadPartRequest . class )  )  ;", "verify ( client ,    times (  1  )  )  . completeMultipartUpload ( any ( CompleteMultipartUploadRequest . class )  )  ;", "}", "verify ( client ,    times (  1  )  )  . abortMultipartUpload ( any ( AbortMultipartUploadRequest . class )  )  ;", "final   AbortMultipartUploadRequest   abortRequest    =    argumentCaptor . getValue (  )  ;", "assertEquals ( bucketName ,    abortRequest . getBucketName (  )  )  ;", "assertEquals ( blobName ,    abortRequest . getKey (  )  )  ;", "assertEquals ( uploadId ,    abortRequest . getUploadId (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testExecuteMultipartUploadAborted"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   long   blobSize    =    TB . toBytes ( randomIntBetween (  6  ,     1  0  )  )  ;", "final      blobStore    =    mock (  . class )  ;", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( mock ( BlobPath . class )  ,    blobStore )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    blobContainer . executeMultipartUpload ( blobStore ,    randomAlphaOfLengthBetween (  1  ,     1  0  )  ,    null ,    blobSize )  )  ;", "assertEquals (  (  (  \" Multipart   upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   larger   than    5 tb \"  )  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExecuteMultipartUploadBlobSizeTooLarge"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   long   blobSize    =    MB . toBytes ( randomIntBetween (  1  ,     4  )  )  ;", "final      blobStore    =    mock (  . class )  ;", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( mock ( BlobPath . class )  ,    blobStore )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    blobContainer . executeMultipartUpload ( blobStore ,    randomAlphaOfLengthBetween (  1  ,     1  0  )  ,    null ,    blobSize )  )  ;", "assertEquals (  (  (  \" Multipart   upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   smaller   than    5 mb \"  )  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExecuteMultipartUploadBlobSizeTooSmall"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   String   bucketName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "final   String   blobName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "final   BlobPath   blobPath    =    new   BlobPath (  )  ;", "if    ( randomBoolean (  )  )     {", "IntStream . of ( randomIntBetween (  1  ,     5  )  )  . forEach (  (    value )     -  >    blobPath . add (  (  \" path _  \"     +    value )  )  )  ;", "}", "final   int   bufferSize    =    randomIntBetween (  1  0  2  4  ,     2  0  4  8  )  ;", "final   int   blobSize    =    randomIntBetween (  0  ,    bufferSize )  ;", "final      blobStore    =    mock (  . class )  ;", "when ( blobStore . bucket (  )  )  . thenReturn ( bucketName )  ;", "when ( blobStore . bufferSizeInBytes (  )  )  . thenReturn (  (  ( long )     ( bufferSize )  )  )  ;", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( blobPath ,    blobStore )  ;", "final   boolean   serverSideEncryption    =    randomBoolean (  )  ;", "when ( blobStore . serverSideEncryption (  )  )  . thenReturn ( serverSideEncryption )  ;", "final   StorageClass   storageClass    =    randomFrom ( StorageClass . values (  )  )  ;", "when ( blobStore . getStorageClass (  )  )  . thenReturn ( storageClass )  ;", "final   CannedAccessControlList   cannedAccessControlList    =     ( randomBoolean (  )  )     ?    randomFrom ( CannedAccessControlList . values (  )  )     :    null ;", "if    ( cannedAccessControlList    !  =    null )     {", "when ( blobStore . getCannedACL (  )  )  . thenReturn ( cannedAccessControlList )  ;", "}", "final   AmazonS 3    client    =    mock ( AmazonS 3  . class )  ;", "when ( blobStore . client (  )  )  . thenReturn ( client )  ;", "final   ArgumentCaptor < PutObjectRequest >    argumentCaptor    =    ArgumentCaptor . forClass ( PutObjectRequest . class )  ;", "when ( client . putObject ( argumentCaptor . capture (  )  )  )  . thenReturn ( new   PutObjectResult (  )  )  ;", "final   ByteArrayInputStream   inputStream    =    new   ByteArrayInputStream ( new   byte [ blobSize ]  )  ;", "blobContainer . executeSingleUpload ( blobStore ,    blobName ,    inputStream ,    blobSize )  ;", "final   PutObjectRequest   request    =    argumentCaptor . getValue (  )  ;", "assertEquals ( bucketName ,    request . getBucketName (  )  )  ;", "assertEquals (  (  ( blobPath . buildAsString (  )  )     +    blobName )  ,    request . getKey (  )  )  ;", "assertEquals ( inputStream ,    request . getInputStream (  )  )  ;", "assertEquals ( blobSize ,    request . getMetadata (  )  . getContentLength (  )  )  ;", "assertEquals ( storageClass . toString (  )  ,    request . getStorageClass (  )  )  ;", "assertEquals ( cannedAccessControlList ,    request . getCannedAcl (  )  )  ;", "if    ( serverSideEncryption )     {", "assertEquals ( AES _  2  5  6  _ SERVER _ SIDE _ ENCRYPTION ,    request . getMetadata (  )  . getSSEAlgorithm (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testExecuteSingleUpload"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   S 3 BlobStore   blobStore    =    mock ( S 3 BlobStore . class )  ;", "when ( blobStore . bufferSizeInBytes (  )  )  . thenReturn ( MB . toBytes (  1  )  )  ;", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( mock ( BlobPath . class )  ,    blobStore )  ;", "final   String   blobName    =    randomAlphaOfLengthBetween (  1  ,     1  0  )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    blobContainer . executeSingleUpload ( blobStore ,    blobName ,    new   ByteArrayInputStream ( new   byte [  0  ]  )  ,    ByteSizeUnit . MB . toBytes (  2  )  )  )  ;", "assertEquals (  \" Upload   request   size    [  2  0  9  7  1  5  2  ]    can ' t   be   larger   than   buffer   size \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExecuteSingleUploadBlobSizeLargerThanBufferSize"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   long   blobSize    =    GB . toBytes ( randomIntBetween (  6  ,     1  0  )  )  ;", "final      blobStore    =    mock (  . class )  ;", "final   S 3 BlobContainer   blobContainer    =    new   S 3 BlobContainer ( mock ( BlobPath . class )  ,    blobStore )  ;", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    blobContainer . executeSingleUpload ( blobStore ,    randomAlphaOfLengthBetween (  1  ,     1  0  )  ,    null ,    blobSize )  )  ;", "assertEquals (  (  (  \" Upload   request   size    [  \"     +    blobSize )     +     \"  ]    can ' t   be   larger   than    5 gb \"  )  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExecuteSingleUploadBlobSizeTooLarge"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "final   ByteSizeUnit   unit    =    randomFrom ( BYTES ,    KB ,    MB ,    GB )  ;", "final   long   size    =    unit . toBytes ( randomIntBetween (  2  ,     1  0  0  0  )  )  ;", "final   int   factor    =    randomIntBetween (  2  ,     1  0  )  ;", ". assertNumberOfMultiparts (  1  ,     0 L ,     0 L ,    size )  ;", ". assertNumberOfMultiparts (  1  ,    size ,    size ,    size )  ;", ". assertNumberOfMultiparts (  1  ,    size ,    size ,     ( size    *    factor )  )  ;", ". assertNumberOfMultiparts ( factor ,    size ,     ( size    *    factor )  ,    size )  ;", "final   long   remaining    =    randomIntBetween (  1  ,     ( size    >     ( Integer . MAX _ VALUE )     ?    Integer . MAX _ VALUE    :     (  ( int )     ( size )  )     -     1  )  )  ;", ". assertNumberOfMultiparts (  ( factor    +     1  )  ,    remaining ,     (  ( size    *    factor )     +    remaining )  ,    size )  ;", "}", "METHOD_END"], "methodName": ["testNumberOfMultiparts"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "IllegalArgumentException   e    =    expectThrows ( IllegalArgumentException . class ,     (  )     -  >    S 3 BlobContainer . numberOfMultiparts ( randomNonNegativeLong (  )  ,     0 L )  )  ;", "assertEquals (  \" Part   size   must   be   greater   than   zero \"  ,    e . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNumberOfMultipartsWithZeroPartSize"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreContainerTests"}, {"methodBody": ["METHOD_START", "{", "S 3 BlobStoreRepositoryTests . bucket    =    randomAlphaOfLength ( randomIntBetween (  1  ,     1  0  )  )  . toLowerCase ( Locale . ROOT )  ;", "S 3 BlobStoreRepositoryTests . client    =    randomAlphaOfLength ( randomIntBetween (  1  ,     1  0  )  )  . toLowerCase ( Locale . ROOT )  ;", "S 3 BlobStoreRepositoryTests . bufferSize    =    new   ByteSizeValue ( randomIntBetween (  5  ,     5  0  )  ,    ByteSizeUnit . MB )  ;", "S 3 BlobStoreRepositoryTests . serverSideEncryption    =    randomBoolean (  )  ;", "if    ( randomBoolean (  )  )     {", "S 3 BlobStoreRepositoryTests . cannedACL    =    randomFrom ( CannedAccessControlList . values (  )  )  . toString (  )  ;", "}", "if    ( randomBoolean (  )  )     {", "S 3 BlobStoreRepositoryTests . storageClass    =    randomValueOtherThan ( Glacier ,     (  )     -  >    randomFrom ( StorageClass . values (  )  )  )  . toString (  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUpRepositorySettings"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreRepositoryTests"}, {"methodBody": ["METHOD_START", "{", "S 3 BlobStoreRepositoryTests . blobs . clear (  )  ;", "}", "METHOD_END"], "methodName": ["wipeRepository"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreRepositoryTests"}, {"methodBody": ["METHOD_START", "{", "String   bucket    =    randomAlphaOfLength ( randomIntBetween (  1  ,     1  0  )  )  . toLowerCase ( Locale . ROOT )  ;", "ByteSizeValue   bufferSize    =    new   ByteSizeValue ( randomIntBetween (  5  ,     1  0  0  )  ,    ByteSizeUnit . MB )  ;", "boolean   serverSideEncryption    =    randomBoolean (  )  ;", "String   cannedACL    =    null ;", "if    ( randomBoolean (  )  )     {", "cannedACL    =    randomFrom ( CannedAccessControlList . values (  )  )  . toString (  )  ;", "}", "String   storageClass    =    null ;", "if    ( randomBoolean (  )  )     {", "storageClass    =    randomValueOtherThan ( Glacier ,     (  )     -  >    randomFrom ( StorageClass . values (  )  )  )  . toString (  )  ;", "}", "AmazonS 3    client    =    new   MockAmazonS 3  ( new   ConcurrentHashMap <  >  (  )  ,    bucket ,    serverSideEncryption ,    cannedACL ,    storageClass )  ;", "return   new    ( Settings . EMPTY ,    client ,    bucket ,    serverSideEncryption ,    bufferSize ,    cannedACL ,    storageClass )  ;", "}", "METHOD_END"], "methodName": ["randomMockS3BlobStore"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( S 3 BlobStore . initStorageClass (  \" sTandaRd \"  )  ,    equalTo ( Standard )  )  ;", "assertThat ( S 3 BlobStore . initStorageClass (  \" sTandaRd _ Ia \"  )  ,    equalTo ( StandardInfrequentAccess )  )  ;", "assertThat ( S 3 BlobStore . initStorageClass (  \" reduCED _ redundancy \"  )  ,    equalTo ( ReducedRedundancy )  )  ;", "}", "METHOD_END"], "methodName": ["testCaseInsensitiveStorageClass"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "String [  ]    aclList    =    new   String [  ]  {     \" private \"  ,     \" public - read \"  ,     \" public - read - write \"  ,     \" authenticated - read \"  ,     \" log - delivery - write \"  ,     \" bucket - owner - read \"  ,     \" bucket - owner - full - control \"     }  ;", "assertThat (  . initCannedACL ( null )  ,    equalTo ( Private )  )  ;", "assertThat (  . initCannedACL (  \"  \"  )  ,    equalTo ( Private )  )  ;", "for    ( String   aclString    :    aclList )     {", "CannedAccessControlList   acl    =     . initCannedACL ( aclString )  ;", "assertThat ( acl . toString (  )  ,    equalTo ( aclString )  )  ;", "}", "for    ( CannedAccessControlList   awsList    :    CannedAccessControlList . values (  )  )     {", "CannedAccessControlList   acl    =     . initCannedACL ( awsList . toString (  )  )  ;", "assertThat ( acl ,    equalTo ( awsList )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testInitCannedACL"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "assertThat ( S 3 BlobStore . initStorageClass ( null )  ,    equalTo ( Standard )  )  ;", "assertThat ( S 3 BlobStore . initStorageClass (  \"  \"  )  ,    equalTo ( Standard )  )  ;", "assertThat ( S 3 BlobStore . initStorageClass (  \" standard \"  )  ,    equalTo ( Standard )  )  ;", "assertThat ( S 3 BlobStore . initStorageClass (  \" standard _ ia \"  )  ,    equalTo ( StandardInfrequentAccess )  )  ;", "assertThat ( S 3 BlobStore . initStorageClass (  \" reduced _ redundancy \"  )  ,    equalTo ( ReducedRedundancy )  )  ;", "}", "METHOD_END"], "methodName": ["testInitStorageClass"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "BlobStoreException   ex    =    expectThrows ( BlobStoreException . class ,     (  )     -  >    S 3 BlobStore . initCannedACL (  \" test _ invalid \"  )  )  ;", "assertThat ( ex . getMessage (  )  ,    equalTo (  \" cannedACL   is   not   valid :     [ test _ invalid ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidCannedACL"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "BlobStoreException   ex    =    expectThrows ( BlobStoreException . class ,     (  )     -  >    S 3 BlobStore . initStorageClass (  \" whatever \"  )  )  ;", "assertThat ( ex . getMessage (  )  ,    equalTo (  \"  ` whatever `    is   not   a   valid   S 3    Storage   Class .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidStorageClass"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "BlobStoreException   ex    =    expectThrows ( BlobStoreException . class ,     (  )     -  >    S 3 BlobStore . initStorageClass (  \" glacier \"  )  )  ;", "assertThat ( ex . getMessage (  )  ,    equalTo (  \" Glacier   storage   class   is   not   supported \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRejectGlacierStorageClass"], "fileName": "org.elasticsearch.repositories.s3.S3BlobStoreTests"}, {"methodBody": ["METHOD_START", "{", "try    ( SecureString   accessKey    =    getConfigValue ( settings ,    clientName ,    ACCESS _ KEY _ SETTING )  ; SecureString   secretKey    =    getConfigValue ( settings ,    clientName ,    SECRET _ KEY _ SETTING )  ; SecureString   proxyUsername    =    getConfigValue ( settings ,    clientName ,    PROXY _ USERNAME _ SETTING )  ; SecureString   proxyPassword    =    getConfigValue ( settings ,    clientName ,    PROXY _ PASSWORD _ SETTING )  )     {", "BasicAWSCredentials   credentials    =    null ;", "if    (  ( accessKey . length (  )  )     !  =     0  )     {", "if    (  ( secretKey . length (  )  )     !  =     0  )     {", "credentials    =    new   BasicAWSCredentials ( accessKey . toString (  )  ,    secretKey . toString (  )  )  ;", "} else    {", "throw   new   IllegalArgumentException (  (  (  \" Missing   secret   key   for   s 3    client    [  \"     +    clientName )     +     \"  ]  \"  )  )  ;", "}", "} else", "if    (  ( secretKey . length (  )  )     !  =     0  )     {", "throw   new   IllegalArgumentException (  (  (  \" Missing   access   key   for   s 3    client    [  \"     +    clientName )     +     \"  ]  \"  )  )  ;", "}", "return   new    ( credentials ,    getConfigValue ( settings ,    clientName ,    ENDPOINT _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    PROTOCOL _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    PROXY _ HOST _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    PROXY _ PORT _ SETTING )  ,    proxyUsername . toString (  )  ,    proxyPassword . toString (  )  ,     (  ( int )     ( getConfigValue ( settings ,    clientName ,    READ _ TIMEOUT _ SETTING )  . millis (  )  )  )  ,    getConfigValue ( settings ,    clientName ,    MAX _ RETRIES _ SETTING )  ,    getConfigValue ( settings ,    clientName ,    USE _ THROTTLE _ RETRIES _ SETTING )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getClientSettings"], "fileName": "org.elasticsearch.repositories.s3.S3ClientSettings"}, {"methodBody": ["METHOD_START", "{", "Setting < T >    concreteSetting    =    clientSetting . getConcreteSettingForNamespace ( clientName )  ;", "return   concreteSetting . get ( settings )  ;", "}", "METHOD_END"], "methodName": ["getConfigValue"], "fileName": "org.elasticsearch.repositories.s3.S3ClientSettings"}, {"methodBody": ["METHOD_START", "{", "Set < String >    clientNames    =    settings . getGroups ( S 3 ClientSettings . PREFIX )  . keySet (  )  ;", "Map < String ,    S 3 ClientSettings >    clients    =    new   HashMap <  >  (  )  ;", "for    ( String   clientName    :    clientNames )     {", "clients . put ( clientName ,    S 3 ClientSettings . getClientSettings ( settings ,    clientName )  )  ;", "}", "if    (  ( clients . containsKey (  \" default \"  )  )     =  =    false )     {", "clients . put (  \" default \"  ,    S 3 ClientSettings . getClientSettings ( settings ,     \" default \"  )  )  ;", "}", "return   Collections . unmodifiableMap ( clients )  ;", "}", "METHOD_END"], "methodName": ["load"], "fileName": "org.elasticsearch.repositories.s3.S3ClientSettings"}, {"methodBody": ["METHOD_START", "{", "assumeTrue (  \" we   are   expecting   proxy   settings   in   elasticsearch . yml   file \"  ,    proxySet )  ;", "}", "METHOD_END"], "methodName": ["checkProxySettings"], "fileName": "org.elasticsearch.repositories.s3.S3ProxiedSnapshotRestoreOverHttpsTests"}, {"methodBody": ["METHOD_START", "{", "return   new   InternalAwsS 3 Service ( settings ,    clientsSettings )  ;", "}", "METHOD_END"], "methodName": ["createStorageService"], "fileName": "org.elasticsearch.repositories.s3.S3RepositoryPlugin"}, {"methodBody": ["METHOD_START", "{", "RepositoryMetaData   metadata    =    new   RepositoryMetaData (  \" dummy - repo \"  ,     \" mock \"  ,    Settings . builder (  )  . put ( S 3 Repository . BUFFER _ SIZE _ SETTING . getKey (  )  ,    new   ByteSizeValue ( bufferMB ,    ByteSizeUnit . MB )  . getStringRep (  )  )  . put ( S 3 Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    new   ByteSizeValue ( chunkMB ,    ByteSizeUnit . MB )  . getStringRep (  )  )  . build (  )  )  ;", "Exception   e    =    expectThrows ( clazz ,     (  )     -  >    new   S 3 Repository ( metadata ,    Settings . EMPTY ,    NamedXContentRegistry . EMPTY ,    new   DummyS 3 Service (  )  )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString ( msg )  )  ;", "}", "METHOD_END"], "methodName": ["assertInvalidBuffer"], "fileName": "org.elasticsearch.repositories.s3.S3RepositoryTests"}, {"methodBody": ["METHOD_START", "{", "RepositoryMetaData   metadata    =    new   RepositoryMetaData (  \" dummy - repo \"  ,     \" mock \"  ,    Settings . builder (  )  . put ( S 3 Repository . BUFFER _ SIZE _ SETTING . getKey (  )  ,    new   ByteSizeValue ( bufferMB ,    ByteSizeUnit . MB )  . getStringRep (  )  )  . put ( S 3 Repository . CHUNK _ SIZE _ SETTING . getKey (  )  ,    new   ByteSizeValue ( chunkMB ,    ByteSizeUnit . MB )  . getStringRep (  )  )  . build (  )  )  ;", "new   S 3 Repository ( metadata ,    Settings . EMPTY ,    NamedXContentRegistry . EMPTY ,    new   S 3 RepositoryTests . DummyS 3 Service (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertValidBuffer"], "fileName": "org.elasticsearch.repositories.s3.S3RepositoryTests"}, {"methodBody": ["METHOD_START", "{", "RepositoryMetaData   metadata    =    new   RepositoryMetaData (  \" dummy - repo \"  ,     \" mock \"  ,    Settings . builder (  )  . put ( S 3 Repository . BASE _ PATH _ SETTING . getKey (  )  ,     \" foo / bar \"  )  . build (  )  )  ;", "S 3 Repository   s 3 repo    =    new   S 3 Repository ( metadata ,    Settings . EMPTY ,    NamedXContentRegistry . EMPTY ,    new    . DummyS 3 Service (  )  )  ;", "assertEquals (  \" foo / bar /  \"  ,    s 3 repo . basePath (  )  . buildAsString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBasePathSetting"], "fileName": "org.elasticsearch.repositories.s3.S3RepositoryTests"}, {"methodBody": ["METHOD_START", "{", "ByteSizeValue   defaultBufferSize    =    S 3 Repository . BUFFER _ SIZE _ SETTING . get ( EMPTY )  ;", "assertThat ( defaultBufferSize ,    Matchers . lessThanOrEqualTo ( new   ByteSizeValue (  1  0  0  ,    ByteSizeUnit . MB )  )  )  ;", "assertThat ( defaultBufferSize ,    Matchers . greaterThanOrEqualTo ( new   ByteSizeValue (  5  ,    ByteSizeUnit . MB )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultBufferSize"], "fileName": "org.elasticsearch.repositories.s3.S3RepositoryTests"}, {"methodBody": ["METHOD_START", "{", "assertInvalidBuffer (  1  0  ,     5  ,    RepositoryException . class ,     \" chunk _ size    (  5 mb )    can ' t   be   lower   than   buffer _ size    (  1  0 mb )  .  \"  )  ;", "assertValidBuffer (  5  ,     1  0  )  ;", "assertValidBuffer (  5  ,     5  )  ;", "assertInvalidBuffer (  4  ,     1  0  ,    IllegalArgumentException . class ,     \" failed   to   parse   value    [  4 mb ]    for   setting    [ buffer _ size ]  ,    must   be    >  =     [  5 mb ]  \"  )  ;", "assertInvalidBuffer (  5  ,     6  0  0  0  0  0  0  ,    IllegalArgumentException . class ,     \" failed   to   parse   value    [  6  0  0  0  0  0  0 mb ]    for   setting    [ chunk _ size ]  ,    must   be    <  =     [  5 tb ]  \"  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidChunkBufferSizeSettings"], "fileName": "org.elasticsearch.repositories.s3.S3RepositoryTests"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "return   Controller . doPrivileged ( operation )  ;", "}", "METHOD_END"], "methodName": ["doPrivileged"], "fileName": "org.elasticsearch.repositories.s3.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "try    {", "return   Controller . doPrivileged ( operation )  ;", "}    catch    ( PrivilegedActionException   e )     {", "throw    (  ( IOException )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doPrivilegedIOException"], "fileName": "org.elasticsearch.repositories.s3.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "SpecialPermission . check (  )  ;", "Controller . doPrivileged (  (  ( PrivilegedAction < Void >  )     (  (  )     -  >     {", "action . run (  )  ;", "return   null ;", "}  )  )  )  ;", "}", "METHOD_END"], "methodName": ["doPrivilegedVoid"], "fileName": "org.elasticsearch.repositories.s3.SocketAccess"}, {"methodBody": ["METHOD_START", "{", "try    {", "MessageDigest   digest    =    MessageDigest . getInstance (  \" MD 5  \"  )  ;", "byte [  ]    bytes    =    digest . digest ( path . getBytes (  \" UTF -  8  \"  )  )  ;", "int   i    =     0  ;", "return    (  (  (  (  ( bytes [  ( i +  +  )  ]  )     &     2  5  5  )     <  <     2  4  )     |     (  (  ( bytes [  ( i +  +  )  ]  )     &     2  5  5  )     <  <     1  6  )  )     |     (  (  ( bytes [  ( i +  +  )  ]  )     &     2  5  5  )     <  <     8  )  )     |     (  ( bytes [  ( i +  +  )  ]  )     &     2  5  5  )  ;", "}    catch    ( UnsupportedEncodingException   ex )     {", "throw   new   EException (  \" cannot   calculate   hashcode \"  ,    ex )  ;", "}    catch    ( NoSuchAlgorithmException   ex )     {", "throw   new   EException (  \" cannot   calculate   hashcode \"  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["hashCode"], "fileName": "org.elasticsearch.repositories.s3.TestAmazonS3"}, {"methodBody": ["METHOD_START", "{", "AtomicLong   value    =    accessCounts . get ( path )  ;", "if    ( value    =  =    null )     {", "value    =    accessCounts . putIfAbsent ( path ,    new   AtomicLong (  1  )  )  ;", "}", "if    ( value    !  =    null )     {", "return   value . incrementAndGet (  )  ;", "}", "return    1  ;", "}", "METHOD_END"], "methodName": ["incrementAndGet"], "fileName": "org.elasticsearch.repositories.s3.TestAmazonS3"}, {"methodBody": ["METHOD_START", "{", "if    ( probability    >     0  .  0  )     {", "String   path    =     (  (  (  ( randomPrefix )     +     \"  -  \"  )     +    bucketName )     +     \"  +  \"  )     +    key ;", "path    +  =     \"  /  \"     +     ( incrementAndGet ( path )  )  ;", "return    ( Math . abs ( hashCode ( path )  )  )     <     (  ( Integer . MAX _ VALUE )     *    probability )  ;", "} else    {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["shouldFail"], "fileName": "org.elasticsearch.repositories.s3.TestAmazonS3"}, {"methodBody": ["METHOD_START", "{", "TestAmazonS 3    wrapper    =    clients . get ( client )  ;", "if    ( wrapper    =  =    null )     {", "wrapper    =    new   TestAmazonS 3  ( client ,    settings )  ;", "clients . put ( client ,    wrapper )  ;", "}", "return   wrapper ;", "}", "METHOD_END"], "methodName": ["cachedWrapper"], "fileName": "org.elasticsearch.repositories.s3.TestAwsS3Service"}, {"methodBody": ["METHOD_START", "{", "return   ESClientYamlSuiteTestCase . createParameters (  )  ;", "}", "METHOD_END"], "methodName": ["parameters"], "fileName": "org.elasticsearch.smoketest.SmokeTestPluginsClientYamlTestSuiteIT"}, {"methodBody": ["METHOD_START", "{", "serverAcceptedChannel (  (  ( TcpNioSocketChannel )     ( channel )  )  )  ;", "}", "METHOD_END"], "methodName": ["acceptChannel"], "fileName": "org.elasticsearch.transport.nio.NioTransport"}, {"methodBody": ["METHOD_START", "{", "return   new   NioTransport . TcpChannelFactoryImpl ( settings )  ;", "}", "METHOD_END"], "methodName": ["channelFactory"], "fileName": "org.elasticsearch.transport.nio.NioTransport"}, {"methodBody": ["METHOD_START", "{", "onException (  (  ( TcpChannel )     ( channel )  )  ,    exception )  ;", "}", "METHOD_END"], "methodName": ["exceptionCaught"], "fileName": "org.elasticsearch.transport.nio.NioTransport"}, {"methodBody": ["METHOD_START", "{", "Client   transportClient    =    internalCluster (  )  . transportClient (  )  ;", "ClusterHealthResponse   clusterIndexHealths    =    transportClient . admin (  )  . cluster (  )  . prepareHealth (  )  . get (  )  ;", "assertThat ( clusterIndexHealths . getStatus (  )  ,    is ( GREEN )  )  ;", "try    {", "transportClient . filterWithHeader ( Collections . singletonMap (  \" ERROR \"  ,     \" MY   MESSAGE \"  )  )  . admin (  )  . cluster (  )  . prepareHealth (  )  . get (  )  ;", "fail (  \" Expected   exception ,    but   didn ' t   happen \"  )  ;", "}    catch    ( ElasticsearchException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" MY   MESSAGE \"  )  )  ;", "assertThat (  . channelProfileName ,    is ( DEFAULT _ PROFILE )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testThatConnectionFailsAsIntended"], "fileName": "org.elasticsearch.transport.nio.NioTransportIT"}, {"methodBody": ["METHOD_START", "{", "NamedWriteableRegistry   namedWriteableRegistry    =    new   NamedWriteableRegistry ( Collections . emptyList (  )  )  ;", "NetworkService   networkService    =    new   NetworkService ( Collections . emptyList (  )  )  ;", "Transport   transport    =    new   NioTransport ( settings ,    threadPool ,    networkService ,    BigArrays . NON _ RECYCLING _ INSTANCE ,    new   common . util . MockPageCacheRecycler ( settings )  ,    namedWriteableRegistry ,    new   NoneCircuitBreakerService (  )  )     {", "@ Override", "protected   Version   executeHandshake ( DiscoveryNode   node ,    TcpChannel   channel ,    TimeValue   timeout )    throws   IOException ,    InterruptedException    {", "if    ( doHandshake )     {", "return   super . executeHandshake ( node ,    channel ,    timeout )  ;", "} else    {", "return   version . minimumCompatibilityVersion (  )  ;", "}", "}", "@ Override", "protected   Version   getCurrentVersion (  )     {", "return   version ;", "}", "}  ;", "MockTransportService   mockTransportService    =    MockTransportService . createNewService ( EMPTY ,    transport ,    version ,    threadPool ,    clusterSettings ,    Collections . emptySet (  )  )  ;", "mockTransportService . start (  )  ;", "return   mockTransportService ;", "}", "METHOD_END"], "methodName": ["nioFromThreadPool"], "fileName": "org.elasticsearch.transport.nio.SimpleNioTransportTests"}, {"methodBody": ["METHOD_START", "{", "int   port    =    serviceA . boundAddress (  )  . publishAddress (  )  . getPort (  )  ;", "Settings   settings    =    Settings . builder (  )  . put ( NODE _ NAME _ SETTING . getKey (  )  ,     \" foobar \"  )  . put ( TRACE _ LOG _ INCLUDE _ SETTING . getKey (  )  ,     \"  \"  )  . put ( TRACE _ LOG _ EXCLUDE _ SETTING . getKey (  )  ,     \" NOTHING \"  )  . put (  \" tcp . port \"  ,    port )  . build (  )  ;", "ClusterSettings   clusterSettings    =    new   ClusterSettings ( settings ,    ClusterSettings . BUILT _ IN _ CLUSTER _ SETTINGS )  ;", "BindTransportException   bindTransportException    =    expectThrows ( BindTransportException . class ,     (  )     -  >     {", "MockTransportService   ervice    =    nioFromThreadPool ( settings ,    threadPool ,    Version . CURRENT ,    clusterSettings ,    true )  ;", "try    {", "ervice . start (  )  ;", "}    finally    {", "ervice . stop (  )  ;", "ervice . close (  )  ;", "}", "}  )  ;", "assertEquals (  (  (  \" Failed   to   bind   to    [  \"     +    port )     +     \"  ]  \"  )  ,    bindTransportException . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBindUnavailableAddress"], "fileName": "org.elasticsearch.transport.nio.SimpleNioTransportTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "serviceA . connectToNode ( new   cluster . node . DiscoveryNode (  \" C \"  ,    new   TransportAddress ( InetAddress . getByName (  \" localhost \"  )  ,     9  8  7  6  )  ,    Collections . emptyMap (  )  ,    Collections . emptySet (  )  ,    Version . CURRENT )  )  ;", "fail (  \" Expected   ConnectTransportException \"  )  ;", "}    catch    ( ConnectTransportException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  \" connect _ exception \"  )  )  ;", "assertThat ( e . getMessage (  )  ,    containsString (  \"  [  1  2  7  .  0  .  0  .  1  :  9  8  7  6  ]  \"  )  )  ;", "Throwable   cause    =    e . getCause (  )  ;", "assertThat ( cause ,    instanceOf ( IOException . class )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testConnectException"], "fileName": "org.elasticsearch.transport.nio.SimpleNioTransportTests"}, {"methodBody": ["METHOD_START", "{", "getContext (  )  . sendMessage ( BytesReference . toByteBuffers ( reference )  ,    ActionListener . toBiConsumer ( listener )  )  ;", "}", "METHOD_END"], "methodName": ["sendMessage"], "fileName": "org.elasticsearch.transport.nio.TcpNioSocketChannel"}]