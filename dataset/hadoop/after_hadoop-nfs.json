[{"methodBody": ["METHOD_START", "{", "return   this . host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.mount.MountEntry"}, {"methodBody": ["METHOD_START", "{", "return   this . path ;", "}", "METHOD_END"], "methodName": ["getPath"], "fileName": "org.apache.hadoop.mount.MountEntry"}, {"methodBody": ["METHOD_START", "{", "assert    ( exports . size (  )  )     =  =     ( hostMatcher . size (  )  )  ;", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "for    ( int   i    =     0  ;    i    <     ( exports . size (  )  )  ;    i +  +  )     {", "xdr . writeBoolean ( true )  ;", "xdr . writeString ( exports . get ( i )  )  ;", "String [  ]    hostGroups    =    hostMatcher . get ( i )  . getHostGroupList (  )  ;", "if    (  ( hostGroups . length )     >     0  )     {", "for    ( int   j    =     0  ;    j    <     ( hostGroups . length )  ;    j +  +  )     {", "xdr . writeBoolean ( true )  ;", "xdr . writeVariableOpaque ( hostGroups [ j ]  . getBytes (  )  )  ;", "}", "}", "xdr . writeBoolean ( false )  ;", "}", "xdr . writeBoolean ( false )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["writeExportList"], "fileName": "org.apache.hadoop.mount.MountResponse"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "xdr . writeInt ( status )  ;", "if    ( status    =  =     (  . MNT _ OK )  )     {", "xdr . writeVariableOpaque ( handle )  ;", "xdr . writeInt (  1  )  ;", "xdr . writeInt ( RpcAuthInfo . AuthFlavor . AUTH _ SYS . getValue (  )  )  ;", "}", "return   xdr ;", "}", "METHOD_END"], "methodName": ["writeMNTResponse"], "fileName": "org.apache.hadoop.mount.MountResponse"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "for    ( MountEntry   Entry    :    s )     {", "xdr . writeBoolean ( true )  ;", "xdr . writeString ( Entry . getHost (  )  )  ;", "xdr . writeString ( Entry . getPath (  )  )  ;", "}", "xdr . writeBoolean ( false )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["writeMountList"], "fileName": "org.apache.hadoop.mount.MountResponse"}, {"methodBody": ["METHOD_START", "{", "return   rpcProgram ;", "}", "METHOD_END"], "methodName": ["getRpcProgram"], "fileName": "org.apache.hadoop.mount.MountdBase"}, {"methodBody": ["METHOD_START", "{", "startUDPServer (  )  ;", "startTCPServer (  )  ;", "if    ( register )     {", "ShutdownHookManager . get (  )  . addShutdownHook ( new    . Unregister (  )  ,     . SHUTDOWN _ HOOK _ PRIORITY )  ;", "try    {", "rpcProgram . register ( PortmapMapping . TRANSPORT _ UDP ,    udpBoundPort )  ;", "rpcProgram . register ( PortmapMapping . TRANSPORT _ TCP ,    tcpBoundPort )  ;", "}    catch    ( Throwable   e )     {", ". LOG . fatal (  \" Failed   to   start   the   server .    Cause :  \"  ,    e )  ;", "terminate (  1  ,    e )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mount.MountdBase"}, {"methodBody": ["METHOD_START", "{", "SimpleTcpServer   tcpServer    =    new   SimpleTcpServer ( rpcProgram . getPort (  )  ,    rpcProgram ,     1  )  ;", "rpcProgram . startDaemons (  )  ;", "tcpServer . run (  )  ;", "tcpBdPort    =    tcpServer . getBdPort (  )  ;", "}", "METHOD_END"], "methodName": ["startTCPServer"], "fileName": "org.apache.hadoop.mount.MountdBase"}, {"methodBody": ["METHOD_START", "{", "SimpleUdpServer   udpServer    =    new   SimpleUdpServer ( rpcProgram . getPort (  )  ,    rpcProgram ,     1  )  ;", "rpcProgram . startDaemons (  )  ;", "udpServer . run (  )  ;", "udpBdPort    =    udpServer . getBdPort (  )  ;", "}", "METHOD_END"], "methodName": ["startUDPServer"], "fileName": "org.apache.hadoop.mount.MountdBase"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . nanoTime (  )  ;", ". AccessCacheEntry   newEntry    =    new    . AccessCacheEntry ( address ,    AccessPrivilege . NONE ,     ( now    +     ( this . cacheExpirationPeriod )  )  )  ;", ". AccessCacheEntry   cachedEntry    =    accessCache . get ( newEntry )  ;", "if    (  ( cachedEntry    !  =    null )     &  &     ( now    <     ( cachedEntry . expirationTime )  )  )     {", "return   cachedEntry . access ;", "} else    {", "for    (  . Match   match    :    mMatches )     {", "if    ( match . isIncluded ( address ,    hostname )  )     {", "if    (  ( match . accessPrivilege )     =  =     ( AccessPrivilege . READ _ ONLY )  )     {", "newEntry . access    =    AccessPrivilege . READ _ ONLY ;", "break ;", "} else", "if    (  ( match . accessPrivilege )     =  =     ( AccessPrivilege . READ _ WRITE )  )     {", "newEntry . access    =    AccessPrivilege . READ _ WRITE ;", "}", "}", "}", "accessCache . put ( newEntry )  ;", "return   newEntry . access ;", "}", "}", "METHOD_END"], "methodName": ["getAccessPrivilege"], "fileName": "org.apache.hadoop.nfs.NfsExports"}, {"methodBody": ["METHOD_START", "{", "return   getAccessPrivilege ( addr . getHostAddress (  )  ,    addr . getCanonicalHostName (  )  )  ;", "}", "METHOD_END"], "methodName": ["getAccessPrivilege"], "fileName": "org.apache.hadoop.nfs.NfsExports"}, {"methodBody": ["METHOD_START", "{", "int   listSize    =    mMatches . size (  )  ;", "String [  ]    hostGroups    =    new   String [ listSize ]  ;", "for    ( int   i    =     0  ;    i    <     ( mMatches . size (  )  )  ;    i +  +  )     {", "hostGroups [ i ]     =    mMatches . get ( i )  . getHostGroup (  )  ;", "}", "return   hostGroups ;", "}", "METHOD_END"], "methodName": ["getHostGroupList"], "fileName": "org.apache.hadoop.nfs.NfsExports"}, {"methodBody": ["METHOD_START", "{", "if    (  ( NfsExports . exports )     =  =    null )     {", "String   matchHosts    =    conf . get ( NFS _ EXPORTS _ ALLOWED _ HOSTS _ KEY ,    NFS _ EXPORTS _ ALLOWED _ HOSTS _ KEY _ DEFAULT )  ;", "int   cacheSize    =    conf . getInt ( Nfs 3 Constant . NFS _ EXPORTS _ CACHE _ SIZE _ KEY ,    Nfs 3 Constant . NFS _ EXPORTS _ CACHE _ SIZE _ DEFAULT )  ;", "long   expirationPeriodNano    =     (  ( conf . getLong ( Nfs 3 Constant . NFS _ EXPORTS _ CACHE _ EXPIRYTIME _ MILLIS _ KEY ,    Nfs 3 Constant . NFS _ EXPORTS _ CACHE _ EXPIRYTIME _ MILLIS _ DEFAULT )  )     *     1  0  0  0  )     *     1  0  0  0  ;", "try    {", "NfsExports . exports    =    new   NfsExports ( cacheSize ,    expirationPeriodNano ,    matchHosts )  ;", "}    catch    ( IllegalArgumentException   e )     {", "NfsExports . LOG . error (  \" Invalid   NFS   Exports   provided :     \"  ,    e )  ;", "return   NfsExports . exports ;", "}", "}", "return   NfsExports . exports ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.nfs.NfsExports"}, {"methodBody": ["METHOD_START", "{", "String [  ]    parts    =    line . split (  \"  \\  \\ s +  \"  )  ;", "final   String   host ;", "AccessPrivilege   privilege    =    AccessPrivilege . READ _ ONLY ;", "switch    ( parts . length )     {", "case    1     :", "host    =    parts [  0  ]  . toLowerCase (  )  . trim (  )  ;", "break ;", "case    2     :", "host    =    parts [  0  ]  . toLowerCase (  )  . trim (  )  ;", "String   option    =    parts [  1  ]  . trim (  )  ;", "if    (  \" rw \"  . equalsIgnoreCase ( option )  )     {", "privilege    =    AccessPrivilege . READ _ WRITE ;", "}", "break ;", "default    :", "throw   new   IllegalArgumentException (  (  (  \" Incorrectly   formatted   line    '  \"     +    line )     +     \"  '  \"  )  )  ;", "}", "if    ( host . equals (  \"  *  \"  )  )     {", "if    ( NfsExports . LOG . isDebugEnabled (  )  )     {", "NfsExports . LOG . debug (  (  (  (  \" Using   match   all   for    '  \"     +    host )     +     \"  '    and    \"  )     +    privilege )  )  ;", "}", "return   new   NfsExports . AnonymousMatch ( privilege )  ;", "} else", "if    ( NfsExports . CIDR _ FORMAT _ SHORT . matcher ( host )  . matches (  )  )     {", "if    ( NfsExports . LOG . isDebugEnabled (  )  )     {", "NfsExports . LOG . debug (  (  (  (  \" Using   CIDR   match   for    '  \"     +    host )     +     \"  '    and    \"  )     +    privilege )  )  ;", "}", "return   new   NfsExports . CIDRMatch ( privilege ,    new   commons . net . util . SubnetUtils ( host )  . getInfo (  )  )  ;", "} else", "if    ( NfsExports . CIDR _ FORMAT _ LONG . matcher ( host )  . matches (  )  )     {", "if    ( NfsExports . LOG . isDebugEnabled (  )  )     {", "NfsExports . LOG . debug (  (  (  (  \" Using   CIDR   match   for    '  \"     +    host )     +     \"  '    and    \"  )     +    privilege )  )  ;", "}", "String [  ]    pair    =    host . split (  \"  /  \"  )  ;", "return   new   NfsExports . CIDRMatch ( privilege ,    new   commons . net . util . SubnetUtils ( pair [  0  ]  ,    pair [  1  ]  )  . getInfo (  )  )  ;", "} else", "if    (  (  (  ( host . contains (  \"  *  \"  )  )     |  |     ( host . contains (  \"  ?  \"  )  )  )     |  |     ( host . contains (  \"  [  \"  )  )  )     |  |     ( host . contains (  \"  ]  \"  )  )  )     {", "if    ( NfsExports . LOG . isDebugEnabled (  )  )     {", "NfsExports . LOG . debug (  (  (  (  \" Using   Regex   match   for    '  \"     +    host )     +     \"  '    and    \"  )     +    privilege )  )  ;", "}", "return   new   NfsExports . RegexMatch ( privilege ,    host )  ;", "} else", "if    ( NfsExports . HOSTNAME _ FORMAT . matcher ( host )  . matches (  )  )     {", "if    ( NfsExports . LOG . isDebugEnabled (  )  )     {", "NfsExports . LOG . debug (  (  (  (  \" Using   exact   match   for    '  \"     +    host )     +     \"  '    and    \"  )     +    privilege )  )  ;", "}", "return   new   NfsExports . ExactMatch ( privilege ,    host )  ;", "} else    {", "throw   new   IllegalArgumentException (  (  (  \" Invalid   hostname   provided    '  \"     +    host )     +     \"  '  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getMatch"], "fileName": "org.apache.hadoop.nfs.NfsExports"}, {"methodBody": ["METHOD_START", "{", "return   value ;", "}", "METHOD_END"], "methodName": ["toValue"], "fileName": "org.apache.hadoop.nfs.NfsFileType"}, {"methodBody": ["METHOD_START", "{", "return   new   NfsTime ( xdr . readInt (  )  ,    xdr . readInt (  )  )  ;", "}", "METHOD_END"], "methodName": ["deserialize"], "fileName": "org.apache.hadoop.nfs.NfsTime"}, {"methodBody": ["METHOD_START", "{", "return    (  (  ( long )     ( seconds )  )     *     1  0  0  0  )     +     (  (  ( long )     ( nseconds )  )     /     1  0  0  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["getMilliSeconds"], "fileName": "org.apache.hadoop.nfs.NfsTime"}, {"methodBody": ["METHOD_START", "{", "return   nseconds ;", "}", "METHOD_END"], "methodName": ["getNseconds"], "fileName": "org.apache.hadoop.nfs.NfsTime"}, {"methodBody": ["METHOD_START", "{", "return   seconds ;", "}", "METHOD_END"], "methodName": ["getSeconds"], "fileName": "org.apache.hadoop.nfs.NfsTime"}, {"methodBody": ["METHOD_START", "{", "xdr . writeInt ( getSeconds (  )  )  ;", "xdr . writeInt ( getNseconds (  )  )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.NfsTime"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  0  /  2  5  5  .  2  5  5  .  2  5  2  .  0  \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testCidrLongRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  0  /  2  5  5  .  2  5  5  .  2  5  2  .  0    rw \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testCidrLongRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  0  /  2  2  \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testCidrShortRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  0  /  2  2    rw \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testCidrShortRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,    address 1  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testExactAddressRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     (  ( address 1  )     +     \"    rw \"  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertFalse (  (  ( AccessPrivilege . READ _ WRITE )     =  =     ( matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testExactAddressRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,    hostname 1  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testExactHostRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     (  ( hostname 1  )     +     \"    rw \"  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testExactHostRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \" foo # bar \"  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidHost"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \" foo   ro    :    bar   rw \"  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidSeparator"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "long   shortExpirationPeriod    =     (  (  1     *     1  0  0  0  )     *     1  0  0  0  )     *     1  0  0  0  ;", "NfsExports   matcher    =    new   NfsExports (  . CacheSize ,    shortExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  [  0  -  9  ]  +  ;  [ a - z ]  +  . b . com   rw \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 2  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    address 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 2  )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "AccessPrivilege   ap ;", "long   startNanos    =    System . nanoTime (  )  ;", "do    {", "ap    =    matcher . getAccessPrivilege ( address 2  ,    address 2  )  ;", "if    ( ap    =  =     ( AccessPrivilege . NONE )  )     {", "break ;", "}", "Thread . sleep (  5  0  0  )  ;", "}    while    (  (  (  ( System . nanoTime (  )  )     -    startNanos )     /     (  . NanosPerMillis )  )     <     5  0  0  0     )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    ap )  ;", "}", "METHOD_END"], "methodName": ["testMultiMatchers"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  [ a - z ]  +  . b . com \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegexHostRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  [ a - z ]  +  . b . com   rw \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegexHostRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  [  0  -  9  ]  +  \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegexIPRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  1  9  2  .  1  6  8  .  0  .  [  0  -  9  ]  +    rw \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "Assert . assertEquals ( AccessPrivilege . NONE ,    matcher . getAccessPrivilege ( address 2  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegexIPRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  *    ro \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ ONLY ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testWildcardRO"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsExports   matcher    =    new   NfsExports ( TestNfsExports . CacheSize ,    TestNfsExports . ExpirationPeriod ,     \"  *    rw \"  )  ;", "Assert . assertEquals ( AccessPrivilege . READ _ WRITE ,    matcher . getAccessPrivilege ( address 1  ,    hostname 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testWildcardRW"], "fileName": "org.apache.hadoop.nfs.TestNfsExports"}, {"methodBody": ["METHOD_START", "{", "NfsTime   nfstime    =    new   NfsTime (  1  0  0  1  )  ;", "Assert . assertEquals (  1  ,    nfstime . getSeconds (  )  )  ;", "Assert . assertEquals (  1  0  0  0  0  0  0  ,    nfstime . getNseconds (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConstructor"], "fileName": "org.apache.hadoop.nfs.TestNfsTime"}, {"methodBody": ["METHOD_START", "{", "NfsTime   t 1     =    new   NfsTime (  1  0  0  1  )  ;", "XDR   xdr    =    new   XDR (  )  ;", "t 1  . serialize ( xdr )  ;", "NfsTime   t 2     =    NfsTime . deserialize ( xdr . asReadOnlyWrap (  )  )  ;", "Assert . assertEquals ( t 1  ,    t 2  )  ;", "}", "METHOD_END"], "methodName": ["testSerializeDeserialize"], "fileName": "org.apache.hadoop.nfs.TestNfsTime"}, {"methodBody": ["METHOD_START", "{", "ByteBuffer   buffer    =    ByteBuffer . allocate (  8  )  ;", "for    ( int   i    =     0  ;    i    <     8  ;    i +  +  )     {", "buffer . put ( data [ i ]  )  ;", "}", "buffer . flip (  )  ;", "return   buffer . getLong (  )  ;", "}", "METHOD_END"], "methodName": ["bytesToLong"], "fileName": "org.apache.hadoop.nfs.nfs3.FileHandle"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( XDR . verifyLength ( xdr ,     3  2  )  )  )     {", "return   false ;", "}", "int   size    =    xdr . readInt (  )  ;", "h    =    xdr . readFixedOpaque ( size )  ;", "fileId    =    bytesToLong ( h )  ;", "return   true ;", "}", "METHOD_END"], "methodName": ["deserialize"], "fileName": "org.apache.hadoop.nfs.nfs3.FileHandle"}, {"methodBody": ["METHOD_START", "{", "return   handle . clone (  )  ;", "}", "METHOD_END"], "methodName": ["getContent"], "fileName": "org.apache.hadoop.nfs.nfs3.FileHandle"}, {"methodBody": ["METHOD_START", "{", "return   fileId ;", "}", "METHOD_END"], "methodName": ["getFileId"], "fileName": "org.apache.hadoop.nfs.nfs3.FileHandle"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   strBuilder    =    new   StringBuilder (  )  ;", "strBuilder . append (  . HEXES . charAt (  (  ( b    &     2  4  0  )     >  >     4  )  )  )  . append (  . HEXES . charAt (  ( b    &     1  5  )  )  )  ;", "return   strBuilder . toString (  )  ;", "}", "METHOD_END"], "methodName": ["hex"], "fileName": "org.apache.hadoop.nfs.nfs3.FileHandle"}, {"methodBody": ["METHOD_START", "{", "out . writeInt ( handle . length )  ;", "out . writeFixedOpaque ( handle )  ;", "return   true ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.nfs3.FileHandle"}, {"methodBody": ["METHOD_START", "{", "if    ( isExpired (  )  )     {", ". LOG . info (  \" Update   cache   now \"  )  ;", "try    {", "updateMaps (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Can ' t   update   the   maps .    Will   use   the   old   ones ,  \"     +     \"    which   can   potentially   cause   problem .  \"  )  ,    e )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["checkAndUpdateMaps"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "checkAndUpdateMaps (  )  ;", "Integer   id    =    gidNameMap . inverse (  )  . get ( g )  ;", "if    ( id    =  =    null )     {", "throw   new   IOException (  (  \" No   such   g :  \"     +    g )  )  ;", "}", "return   id . intValue (  )  ;", "}", "METHOD_END"], "methodName": ["getGid"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "checkAndUpdateMaps (  )  ;", "int   gid ;", "try    {", "gid    =    getGid ( group )  ;", "}    catch    ( IOException   e )     {", "gid    =    group . hashCode (  )  ;", ". LOG . info (  (  (  (  \" Can ' t   map   group    \"     +    group )     +     \"  .    Use   its   string   hashcode :  \"  )     +    gid )  ,    e )  ;", "}", "return   gid ;", "}", "METHOD_END"], "methodName": ["getGidAllowingUnknown"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "checkAndUpdateMaps (  )  ;", "String   gname    =    gidNameMap . get ( gid )  ;", "if    ( gname    =  =    null )     {", ". LOG . warn (  (  (  (  \" Can ' t   find   group   name   for   gid    \"     +    gid )     +     \"  .    Use   default   group   name    \"  )     +    unknown )  )  ;", "gname    =    unknown ;", "}", "return   gname ;", "}", "METHOD_END"], "methodName": ["getGroupName"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "return   timeout ;", "}", "METHOD_END"], "methodName": ["getTimeout"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "checkAndUpdateMaps (  )  ;", "Integer   id    =    uidNameMap . inverse (  )  . get ( user )  ;", "if    ( id    =  =    null )     {", "throw   new   IOException (  (  \"    just   deleted ?  :  \"     +    user )  )  ;", "}", "return   id . intValue (  )  ;", "}", "METHOD_END"], "methodName": ["getUid"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "checkAndUpdateMaps (  )  ;", "int   uid ;", "try    {", "uid    =    getUid ( user )  ;", "}    catch    ( IOException   e )     {", "uid    =    user . hashCode (  )  ;", ". LOG . info (  (  (  (  \" Can ' t   map   user    \"     +    user )     +     \"  .    Use   its   string   hashcode :  \"  )     +    uid )  ,    e )  ;", "}", "return   uid ;", "}", "METHOD_END"], "methodName": ["getUidAllowingUnknown"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "checkAndUpdateMaps (  )  ;", "String   uname    =    uidNameMap . get ( uid )  ;", "if    ( uname    =  =    null )     {", ". LOG . warn (  (  (  (  \" Can ' t   find   user   name   for   uid    \"     +    uid )     +     \"  .    Use   default   user   name    \"  )     +    unknown )  )  ;", "uname    =    unknown ;", "}", "return   uname ;", "}", "METHOD_END"], "methodName": ["getUserName"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "return    (  ( Time . monotonicNow (  )  )     -     ( lastUpdateTime )  )     >     ( timeout )  ;", "}", "METHOD_END"], "methodName": ["isExpired"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "Long   longVal    =    Long . parseLong ( idStr )  ;", "int   intVal    =    longVal . intValue (  )  ;", "return   Integer . valueOf ( intVal )  ;", "}", "METHOD_END"], "methodName": ["parseId"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "Map < Integer ,    Integer >    uidMapping    =    new   HashMap < Integer ,    Integer >  (  )  ;", "Map < Integer ,    Integer >    gidMapping    =    new   HashMap < Integer ,    Integer >  (  )  ;", "BufferedReader   in    =    new   BufferedReader ( new   InputStreamReader ( new   FileInputStream ( staticMapFile )  )  )  ;", "try    {", "String   line    =    null ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "if    (  (  . EMPTY _ LINE . matcher ( line )  . matches (  )  )     |  |     (  . COMMENT _ LINE . matcher ( line )  . matches (  )  )  )     {", "continue ;", "}", "Matcher   lineMatcher    =     . MAPPING _ LINE . matcher ( line )  ;", "if    (  !  ( lineMatcher . matches (  )  )  )     {", ". LOG . warn (  (  (  (  (  \" Could   not   parse   line    '  \"     +    line )     +     \"  '  .    Lines   should   be   of    \"  )     +     \" the   form    '  [ uid | gid ]     [ remote   id ]     [ local   id ]  '  .    Blank   lines   and    \"  )     +     \" everything   following   a    '  #  '    on   a   line   will   be   ignored .  \"  )  )  ;", "continue ;", "}", "String   firstComponent    =    lineMatcher . group (  1  )  ;", "int   remoteId    =    Integer . parseInt ( lineMatcher . group (  2  )  )  ;", "int   localId    =    Integer . parseInt ( lineMatcher . group (  3  )  )  ;", "if    ( firstComponent . equals (  \" uid \"  )  )     {", "uidMapping . put ( localId ,    remoteId )  ;", "} else    {", "gidMapping . put ( localId ,    remoteId )  ;", "}", "}", "}    finally    {", "in . close (  )  ;", "}", "return   new    . StaticMapping ( uidMapping ,    gidMapping )  ;", "}", "METHOD_END"], "methodName": ["parseStaticMap"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "IdUserGroup . LOG . warn (  (  (  \"  \\ n \"     +    header )     +     ( String . format (  \" new   entry    (  % d ,     % s )  ,    existing   entry :     (  % d ,     % s )  .  \\ n % s \\ n % s \"  ,    key ,    value ,    ekey ,    evalue ,     \" The   new   entry   is   to   be   ignored   for   the   following   reason .  \"  ,    IdUserGroup . DUPLICATE _ NAME _ ID _ DEBUG _ INFO )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["reportDuplicateEntry"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   br    =    null ;", "try    {", "Process   process    =    Runtime . getRuntime (  )  . exec ( new   String [  ]  {     \" bash \"  ,     \"  - c \"  ,    command    }  )  ;", "br    =    new   BufferedReader ( new   InputStreamReader ( process . getInputStream (  )  )  )  ;", "String   line    =    null ;", "while    (  ( line    =    br . readLine (  )  )     !  =    null )     {", "String [  ]    nameId    =    line . split ( regex )  ;", "if    (  ( nameId    =  =    null )     |  |     (  ( nameId . length )     !  =     2  )  )     {", "throw   new   IOException (  (  (  (  \" Can ' t   parse    \"     +    mapName )     +     \"    list   entry :  \"  )     +    line )  )  ;", "}", ". LOG . debug (  (  (  (  (  (  \" add   to    \"     +    mapName )     +     \" map :  \"  )     +     ( nameId [  0  ]  )  )     +     \"    id :  \"  )     +     ( nameId [  1  ]  )  )  )  ;", "final   Integer   key    =    staticMapping . get (  . parseId ( nameId [  1  ]  )  )  ;", "final   String   value    =    nameId [  0  ]  ;", "if    ( map . containsKey ( key )  )     {", "final   String   prevValue    =    map . get ( key )  ;", "if    ( value . equals ( prevValue )  )     {", "continue ;", "}", ". reportDuplicateEntry (  \" Got   multiple   names   associated   with   the   same   id :     \"  ,    key ,    value ,    key ,    prevValue )  ;", "continue ;", "}", "if    ( map . containsValue ( value )  )     {", "final   Integer   prevKey    =    map . inverse (  )  . get ( value )  ;", ". reportDuplicateEntry (  \" Got   multiple   ids   associated   with   the   same   name :     \"  ,    key ,    value ,    prevKey ,    value )  ;", "continue ;", "}", "map . put ( key ,    value )  ;", "}", ". LOG . info (  (  (  (  \" Updated    \"     +    mapName )     +     \"    map   size :     \"  )     +     ( map . size (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  \" Can ' t   update    \"     +    mapName )     +     \"    map \"  )  )  ;", "throw   e ;", "}    finally    {", "if    ( br    !  =    null )     {", "try    {", "br . close (  )  ;", "}    catch    ( IOException   e 1  )     {", ". LOG . error (  \" Can ' t   close   BufferedReader   of   command   result \"  ,    e 1  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["updateMapInternal"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "BiMap < Integer ,    String >    uMap    =    HashBiMap . create (  )  ;", "BiMap < Integer ,    String >    gMap    =    HashBiMap . create (  )  ;", "if    (  (  !  (  . OS . startsWith (  \" Linux \"  )  )  )     &  &     (  !  (  . OS . startsWith (  \" Mac \"  )  )  )  )     {", ". LOG . error (  (  (  (  \" Platform   is   not   supported :  \"     +     (  . OS )  )     +     \"  .    Can ' t   update   user   map   and   group   map   and \"  )     +     \"     ' nobody '    will   be   used   for   any   user   and   group .  \"  )  )  ;", "return ;", "}", ". StaticMapping   staticMapping    =    new    . StaticMapping ( new   HashMap < Integer ,    Integer >  (  )  ,    new   HashMap < Integer ,    Integer >  (  )  )  ;", "if    ( staticMappingFile . exists (  )  )     {", ". LOG . info (  (  (  \" Using    '  \"     +     ( staticMappingFile )  )     +     \"  '    for   static   UID / GID   mapping .  .  .  \"  )  )  ;", "staticMapping    =     . parseStaticMap ( staticMappingFile )  ;", "} else    {", ". LOG . info (  (  (  \" Not   doing   static   UID / GID   mapping   because    '  \"     +     ( staticMappingFile )  )     +     \"  '    does   not   exist .  \"  )  )  ;", "}", "if    (  . OS . startsWith (  \" Linux \"  )  )     {", ". updateMapInternal ( uMap ,     \" user \"  ,     . LINUX _ GET _ ALL _ USERS _ CMD ,     \"  :  \"  ,    staticMapping . uidMapping )  ;", ". updateMapInternal ( gMap ,     \" group \"  ,     . LINUX _ GET _ ALL _ GROUPS _ CMD ,     \"  :  \"  ,    staticMapping . gidMapping )  ;", "} else    {", ". updateMapInternal ( uMap ,     \" user \"  ,     . MAC _ GET _ ALL _ USERS _ CMD ,     \"  \\  \\ s +  \"  ,    staticMapping . uidMapping )  ;", ". updateMapInternal ( gMap ,     \" group \"  ,     . MAC _ GET _ ALL _ GROUPS _ CMD ,     \"  \\  \\ s +  \"  ,    staticMapping . gidMapping )  ;", "}", "uidNameMap    =    uMap ;", "gidNameMap    =    gMap ;", "lastUpdateTime    =    Time . monotonicNow (  )  ;", "}", "METHOD_END"], "methodName": ["updateMaps"], "fileName": "org.apache.hadoop.nfs.nfs3.IdUserGroup"}, {"methodBody": ["METHOD_START", "{", "return   rpcProgram ;", "}", "METHOD_END"], "methodName": ["getRpcProgram"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3Base"}, {"methodBody": ["METHOD_START", "{", "startTCPServer (  )  ;", "if    ( register )     {", "ShutdownHookManager . get (  )  . addShutdownHook ( new    . Unregister (  )  ,     . SHUTDOWN _ HOOK _ PRIORITY )  ;", "try    {", "rpcProgram . register ( PortmapMapping . TRANSPORT _ TCP ,    nfsBoundPort )  ;", "}    catch    ( Throwable   e )     {", ". LOG . fatal (  \" Failed   to   start   the   server .    Cause :  \"  ,    e )  ;", "terminate (  1  ,    e )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3Base"}, {"methodBody": ["METHOD_START", "{", "SimpleTcpServer   tcpServer    =    new   SimpleTcpServer ( rpcProgram . getPort (  )  ,    rpcProgram ,     0  )  ;", "rpcProgram . startDaemons (  )  ;", "tcpServer . run (  )  ;", "BoundPort    =    tcpServer . getBoundPort (  )  ;", "}", "METHOD_END"], "methodName": ["startTCPServer"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3Base"}, {"methodBody": ["METHOD_START", "{", "Nfs 3 FileAttributes   attr    =    new   Nfs 3 FileAttributes (  )  ;", "attr . type    =    xdr . readInt (  )  ;", "attr . mode    =    xdr . readInt (  )  ;", "attr . nlink    =    xdr . readInt (  )  ;", "attr . uid    =    xdr . readInt (  )  ;", "attr . gid    =    xdr . readInt (  )  ;", "attr . size    =    xdr . readHyper (  )  ;", "attr . used    =    xdr . readHyper (  )  ;", "xdr . readInt (  )  ;", "xdr . readInt (  )  ;", "attr . rdev    =    new   Nfs 3 FileAttributes . Specdata 3  (  )  ;", "attr . fsid    =    xdr . readHyper (  )  ;", "attr . fileId    =    xdr . readHyper (  )  ;", "attr . atime    =    NfsTime . deserialize ( xdr )  ;", "attr . mtime    =    NfsTime . deserialize ( xdr )  ;", "attr . ctime    =    NfsTime . deserialize ( xdr )  ;", "return   attr ;", "}", "METHOD_END"], "methodName": ["deserialize"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   atime ;", "}", "METHOD_END"], "methodName": ["getAtime"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   ctime ;", "}", "METHOD_END"], "methodName": ["getCtime"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return    ( childNum    +     2  )     *     3  2  ;", "}", "METHOD_END"], "methodName": ["getDirSize"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   fileId ;", "}", "METHOD_END"], "methodName": ["getFileId"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   fsid ;", "}", "METHOD_END"], "methodName": ["getFsid"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   this . gid ;", "}", "METHOD_END"], "methodName": ["getGid"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   this . mode ;", "}", "METHOD_END"], "methodName": ["getMode"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   mtime ;", "}", "METHOD_END"], "methodName": ["getMtime"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   nlink ;", "}", "METHOD_END"], "methodName": ["getNlink"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   this . uid ;", "}", "METHOD_END"], "methodName": ["getUid"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   used ;", "}", "METHOD_END"], "methodName": ["getUsed"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "return   new   WccAttr ( size ,    mtime ,    ctime )  ;", "}", "METHOD_END"], "methodName": ["getWccAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "xdr . writeInt ( type )  ;", "xdr . writeInt ( mode )  ;", "xdr . writeInt ( nlink )  ;", "xdr . writeInt ( uid )  ;", "xdr . writeInt ( gid )  ;", "xdr . writeLongAsHyper ( size )  ;", "xdr . writeLongAsHyper ( used )  ;", "xdr . writeInt ( rdev . getSpecdata 1  (  )  )  ;", "xdr . writeInt ( rdev . getSpecdata 2  (  )  )  ;", "xdr . writeLongAsHyper ( fsid )  ;", "xdr . writeLongAsHyper ( fId )  ;", "atime . serialize ( xdr )  ;", "mtime . serialize ( xdr )  ;", "ctime . serialize ( xdr )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "this . size    =    size ;", "}", "METHOD_END"], "methodName": ["setSize"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "this . used    =    used ;", "}", "METHOD_END"], "methodName": ["setUsed"], "fileName": "org.apache.hadoop.nfs.nfs3.Nfs3FileAttributes"}, {"methodBody": ["METHOD_START", "{", "FileHandle   handle    =    new   FileHandle (  1  0  2  4  )  ;", "XDR   xdr    =    new   XDR (  )  ;", "handle . serialize ( xdr )  ;", "Assert . assertEquals ( handle . getFileId (  )  ,     1  0  2  4  )  ;", "FileHandle   handle 2     =    new   FileHandle (  )  ;", "handle 2  . deserialize ( xdr . asReadOnlyWrap (  )  )  ;", "Assert . assertEquals (  \" Failed :    Assert    1  0  2  4    is   id    \"  ,     1  0  2  4  ,    handle . getFileId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConstructor"], "fileName": "org.apache.hadoop.nfs.nfs3.TestFileHandle"}, {"methodBody": ["METHOD_START", "{", "String   GET _ ALL _ USERS _ CMD    =     \" echo    \\  \" root : x :  0  :  0  : root :  / root :  / bin / bash \\ n \"     +     (  (  (  (  (  (  (  (  \" hdfs : x :  1  1  5  0  1  :  1  0  7  8  7  : Grid   Distributed   File   System :  / home / hdfs :  / bin / bash \\ n \"     +     \" hdfs : x :  1  1  5  0  2  :  1  0  7  8  8  : Grid   Distributed   File   System :  / home / hdfs :  / bin / bash \\ n \"  )     +     \" hdfs 1  : x :  1  1  5  0  1  :  1  0  7  8  7  : Grid   Distributed   File   System :  / home / hdfs :  / bin / bash \\ n \"  )     +     \" hdfs 2  : x :  1  1  5  0  2  :  1  0  7  8  7  : Grid   Distributed   File   System :  / home / hdfs :  / bin / bash \\ n \"  )     +     \" bin : x :  2  :  2  : bin :  / bin :  / bin / sh \\ n \"  )     +     \" bin : x :  1  :  1  : bin :  / bin :  / sbin / nologin \\ n \"  )     +     \" daemon : x :  1  :  1  : daemon :  / usr / sbin :  / bin / sh \\ n \"  )     +     \" daemon : x :  2  :  2  : daemon :  / sbin :  / sbin / nologin \\  \"  \"  )     +     \"     |    cut    - d :     - f 1  ,  3  \"  )  ;", "String   GET _ ALL _ GROUPS _ CMD    =     \" echo    \\  \" hdfs :  *  :  1  1  5  0  1  : hrt _ hdfs \\ n \"     +     (  (  (  (  \" mapred : x :  4  9  7  \\ n \"     +     \" mapred 2  : x :  4  9  7  \\ n \"  )     +     \" mapred : x :  4  9  8  \\ n \"  )     +     \" mapred 3  : x :  4  9  8  \\  \"  \"  )     +     \"     |    cut    - d :     - f 1  ,  3  \"  )  ;", "BiMap < Integer ,    String >    uMap    =    HashBiMap . create (  )  ;", "BiMap < Integer ,    String >    gMap    =    HashBiMap . create (  )  ;", "IdUserGroup . updateMapInternal ( uMap ,     \" user \"  ,    GET _ ALL _ USERS _ CMD ,     \"  :  \"  ,     . EMPTY _ PASS _ THROUGH _ MAP )  ;", "assertEquals (  5  ,    uMap . size (  )  )  ;", "assertEquals (  \" root \"  ,    uMap . get (  0  )  )  ;", "assertEquals (  \" hdfs \"  ,    uMap . get (  1  1  5  0  1  )  )  ;", "assertEquals (  \" hdfs 2  \"  ,    uMap . get (  1  1  5  0  2  )  )  ;", "assertEquals (  \" bin \"  ,    uMap . get (  2  )  )  ;", "assertEquals (  \" daemon \"  ,    uMap . get (  1  )  )  ;", "IdUserGroup . updateMapInternal ( gMap ,     \" group \"  ,    GET _ ALL _ GROUPS _ CMD ,     \"  :  \"  ,     . EMPTY _ PASS _ THROUGH _ MAP )  ;", "assertTrue (  (  ( gMap . size (  )  )     =  =     3  )  )  ;", "assertEquals (  \" hdfs \"  ,    gMap . get (  1  1  5  0  1  )  )  ;", "assertEquals (  \" mapred \"  ,    gMap . get (  4  9  7  )  )  ;", "assertEquals (  \" mapred 3  \"  ,    gMap . get (  4  9  8  )  )  ;", "}", "METHOD_END"], "methodName": ["testDuplicates"], "fileName": "org.apache.hadoop.nfs.nfs3.TestIdUserGroup"}, {"methodBody": ["METHOD_START", "{", "String   GET _ ALL _ USERS _ CMD    =     \" echo    \\  \"  \"     +     (  (  (  (  (  (  (  \" nfsnobody : x :  4  2  9  4  9  6  7  2  9  4  :  4  2  9  4  9  6  7  2  9  4  : Anonymous   NFS   User :  / var / lib / nfs :  / sbin / nologin \\ n \"     +     \" nfsnobody 1  : x :  4  2  9  4  9  6  7  2  9  5  :  4  2  9  4  9  6  7  2  9  5  : Anonymous   NFS   User :  / var / lib / nfs 1  :  / sbin / nologin \\ n \"  )     +     \" maxint : x :  2  1  4  7  4  8  3  6  4  7  :  2  1  4  7  4  8  3  6  4  7  : Grid   Distributed   File   System :  / home / maxint :  / bin / bash \\ n \"  )     +     \" minint : x :  2  1  4  7  4  8  3  6  4  8  :  2  1  4  7  4  8  3  6  4  8  : Grid   Distributed   File   System :  / home / minint :  / bin / bash \\ n \"  )     +     \" archivebackup :  *  :  1  0  3  1  :  4  2  9  4  9  6  7  2  9  4  : Archive   Backup :  / home / users / archivebackup :  / bin / sh \\ n \"  )     +     \" hdfs : x :  1  1  5  0  1  :  1  0  7  8  7  : Grid   Distributed   File   System :  / home / hdfs :  / bin / bash \\ n \"  )     +     \" daemon : x :  2  :  2  : daemon :  / sbin :  / sbin / nologin \\  \"  \"  )     +     \"     |    cut    - d :     - f 1  ,  3  \"  )  ;", "String   GET _ ALL _ GROUPS _ CMD    =     \" echo    \\  \"  \"     +     (  (  (  (  (  (  (  \" hdfs :  *  :  1  1  5  0  1  : hrt _ hdfs \\ n \"     +     \" rpcuser :  *  :  2  9  :  \\ n \"  )     +     \" nfsnobody :  *  :  4  2  9  4  9  6  7  2  9  4  :  \\ n \"  )     +     \" nfsnobody 1  :  *  :  4  2  9  4  9  6  7  2  9  5  :  \\ n \"  )     +     \" maxint :  *  :  2  1  4  7  4  8  3  6  4  7  :  \\ n \"  )     +     \" minint :  *  :  2  1  4  7  4  8  3  6  4  8  :  \\ n \"  )     +     \" mapred 3  : x :  4  9  8  \\  \"  \"  )     +     \"     |    cut    - d :     - f 1  ,  3  \"  )  ;", "BiMap < Integer ,    String >    uMap    =    HashBiMap . create (  )  ;", "BiMap < Integer ,    String >    gMap    =    HashBiMap . create (  )  ;", "IdUserGroup . updateMapInternal ( uMap ,     \" user \"  ,    GET _ ALL _ USERS _ CMD ,     \"  :  \"  ,     . EMPTY _ PASS _ THROUGH _ MAP )  ;", "assertTrue (  (  ( uMap . size (  )  )     =  =     7  )  )  ;", "assertEquals (  \" nfsnobody \"  ,    uMap . get (  (  -  2  )  )  )  ;", "assertEquals (  \" nfsnobody 1  \"  ,    uMap . get (  (  -  1  )  )  )  ;", "assertEquals (  \" maxint \"  ,    uMap . get (  2  1  4  7  4  8  3  6  4  7  )  )  ;", "assertEquals (  \" minint \"  ,    uMap . get (  -  2  1  4  7  4  8  3  6  4  8  )  )  ;", "assertEquals (  \" archivebackup \"  ,    uMap . get (  1  0  3  1  )  )  ;", "assertEquals (  \" hdfs \"  ,    uMap . get (  1  1  5  0  1  )  )  ;", "assertEquals (  \" daemon \"  ,    uMap . get (  2  )  )  ;", "IdUserGroup . updateMapInternal ( gMap ,     \" group \"  ,    GET _ ALL _ GROUPS _ CMD ,     \"  :  \"  ,     . EMPTY _ PASS _ THROUGH _ MAP )  ;", "assertTrue (  (  ( gMap . size (  )  )     =  =     7  )  )  ;", "assertEquals (  \" hdfs \"  ,    gMap . get (  1  1  5  0  1  )  )  ;", "assertEquals (  \" rpcuser \"  ,    gMap . get (  2  9  )  )  ;", "assertEquals (  \" nfsnobody \"  ,    gMap . get (  (  -  2  )  )  )  ;", "assertEquals (  \" nfsnobody 1  \"  ,    gMap . get (  (  -  1  )  )  )  ;", "assertEquals (  \" maxint \"  ,    gMap . get (  2  1  4  7  4  8  3  6  4  7  )  )  ;", "assertEquals (  \" minint \"  ,    gMap . get (  -  2  1  4  7  4  8  3  6  4  8  )  )  ;", "assertEquals (  \" mapred 3  \"  ,    gMap . get (  4  9  8  )  )  ;", "}", "METHOD_END"], "methodName": ["testIdOutOfIntegerRange"], "fileName": "org.apache.hadoop.nfs.nfs3.TestIdUserGroup"}, {"methodBody": ["METHOD_START", "{", "File   tempStaticMapFile    =    File . createTempFile (  \" nfs -  \"  ,     \"  . map \"  )  ;", "final   String   staticMapFileContents    =     \" uid    1  0     1  0  0  \\ n \"     +     (  (  (  (  (  (  (  (  (  \" gid    1  0     2  0  0  \\ n \"     +     \" uid    1  1     2  0  1     #    comment   at   the   end   of   a   line \\ n \"  )     +     \" uid    1  2     3  0  1  \\ n \"  )     +     \"  #    Comment   at   the   beginning   of   a   line \\ n \"  )     +     \"              #    Comment   that   starts   late   in   the   line \\ n \"  )     +     \" uid    1  0  0  0  0     1  0  0  0  1  #    line   without   whitespace   before   comment \\ n \"  )     +     \" uid    1  3     3  0  2  \\ n \"  )     +     \" gid \\ t 1  1  \\ t 2  0  1  \\ n \"  )     +     \"  \\ n \"  )     +     \" gid    1  2     2  0  2  \"  )  ;", "OutputStream   out    =    new   FileOutputStream ( tempStaticMapFile )  ;", "out . write ( staticMapFileContents . getBytes (  )  )  ;", "out . close (  )  ;", ". StaticMapping   parsedMap    =     . parseStaticMap ( tempStaticMapFile )  ;", "assertEquals (  1  0  ,     (  ( int )     ( parsedMap . uidMapping . get (  1  0  0  )  )  )  )  ;", "assertEquals (  1  1  ,     (  ( int )     ( parsedMap . uidMapping . get (  2  0  1  )  )  )  )  ;", "assertEquals (  1  2  ,     (  ( int )     ( parsedMap . uidMapping . get (  3  0  1  )  )  )  )  ;", "assertEquals (  1  3  ,     (  ( int )     ( parsedMap . uidMapping . get (  3  0  2  )  )  )  )  ;", "assertEquals (  1  0  ,     (  ( int )     ( parsedMap . gidMapping . get (  2  0  0  )  )  )  )  ;", "assertEquals (  1  1  ,     (  ( int )     ( parsedMap . gidMapping . get (  2  0  1  )  )  )  )  ;", "assertEquals (  1  2  ,     (  ( int )     ( parsedMap . gidMapping . get (  2  0  2  )  )  )  )  ;", "assertEquals (  1  0  0  0  0  ,     (  ( int )     ( parsedMap . uidMapping . get (  1  0  0  0  1  )  )  )  )  ;", "assertEquals (  1  0  0  0  ,     (  ( int )     ( parsedMap . uidMapping . get (  1  0  0  0  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testStaticMapParsing"], "fileName": "org.apache.hadoop.nfs.nfs3.TestIdUserGroup"}, {"methodBody": ["METHOD_START", "{", "Map < Integer ,    Integer >    uidStaticMap    =    new   IdUserGroup . PassThroughMap < Integer >  (  )  ;", "Map < Integer ,    Integer >    gidStaticMap    =    new   IdUserGroup . PassThroughMap < Integer >  (  )  ;", "uidStaticMap . put (  1  1  5  0  1  ,     1  0  )  ;", "gidStaticMap . put (  4  9  7  ,     2  0  0  )  ;", "BiMap < Integer ,    String >    uMap    =    HashBiMap . create (  )  ;", "BiMap < Integer ,    String >    gMap    =    HashBiMap . create (  )  ;", "String   GET _ ALL _ USERS _ CMD    =     \" echo    \\  \" atm : x :  1  0  0  0  :  1  0  0  0  : Aaron   T .    Myers ,  ,  ,  :  / home / atm :  / bin / bash \\ n \"     +     (  \" hdfs : x :  1  1  5  0  1  :  1  0  7  8  7  : Grid   Distributed   File   System :  / home / hdfs :  / bin / bash \\  \"  \"     +     \"     |    cut    - d :     - f 1  ,  3  \"  )  ;", "String   GET _ ALL _ GROUPS _ CMD    =     \" echo    \\  \" hdfs :  *  :  1  1  5  0  1  : hrt _ hdfs \\ n \"     +     (  (  \" mapred : x :  4  9  7  \\ n \"     +     \" mapred 2  : x :  4  9  8  \\  \"  \"  )     +     \"     |    cut    - d :     - f 1  ,  3  \"  )  ;", "IdUserGroup . updateMapInternal ( uMap ,     \" user \"  ,    GET _ ALL _ USERS _ CMD ,     \"  :  \"  ,    uidStaticMap )  ;", "IdUserGroup . updateMapInternal ( gMap ,     \" group \"  ,    GET _ ALL _ GROUPS _ CMD ,     \"  :  \"  ,    gidStaticMap )  ;", "assertEquals (  \" hdfs \"  ,    uMap . get (  1  0  )  )  ;", "assertEquals (  1  0  ,     (  ( int )     ( uMap . inverse (  )  . get (  \" hdfs \"  )  )  )  )  ;", "assertEquals (  \" atm \"  ,    uMap . get (  1  0  0  0  )  )  ;", "assertEquals (  1  0  0  0  ,     (  ( int )     ( uMap . inverse (  )  . get (  \" atm \"  )  )  )  )  ;", "assertEquals (  \" hdfs \"  ,    gMap . get (  1  1  5  0  1  )  )  ;", "assertEquals (  1  1  5  0  1  ,     (  ( int )     ( gMap . inverse (  )  . get (  \" hdfs \"  )  )  )  )  ;", "assertEquals (  \" mapred \"  ,    gMap . get (  2  0  0  )  )  ;", "assertEquals (  2  0  0  ,     (  ( int )     ( gMap . inverse (  )  . get (  \" mapred \"  )  )  )  )  ;", "assertEquals (  \" mapred 2  \"  ,    gMap . get (  4  9  8  )  )  ;", "assertEquals (  4  9  8  ,     (  ( int )     ( gMap . inverse (  )  . get (  \" mapred 2  \"  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testStaticMapping"], "fileName": "org.apache.hadoop.nfs.nfs3.TestIdUserGroup"}, {"methodBody": ["METHOD_START", "{", "IdUserGroup   iug    =    new   IdUserGroup ( new   Configuration (  )  )  ;", "assertEquals ( iug . getTimeout (  )  ,    Nfs 3 Constant . NFS _ USERGROUP _ UPDATE _ MILLIS _ DEFAULT )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setLong ( Nfs 3 Constant . NFS _ USERGROUP _ UPDATE _ MILLIS _ KEY ,     0  )  ;", "iug    =    new   IdUserGroup ( conf )  ;", "assertEquals ( iug . getTimeout (  )  ,    Nfs 3 Constant . NFS _ USERGROUP _ UPDATE _ MILLIS _ MIN )  ;", "conf . setLong ( Nfs 3 Constant . NFS _ USERGROUP _ UPDATE _ MILLIS _ KEY ,     (  ( Nfs 3 Constant . NFS _ USERGROUP _ UPDATE _ MILLIS _ DEFAULT )     *     2  )  )  ;", "iug    =    new   IdUserGroup ( conf )  ;", "assertEquals ( iug . getTimeout (  )  ,     (  ( Nfs 3 Constant . NFS _ USERGROUP _ UPDATE _ MILLIS _ DEFAULT )     *     2  )  )  ;", "}", "METHOD_END"], "methodName": ["testUserUpdateSetting"], "fileName": "org.apache.hadoop.nfs.nfs3.TestIdUserGroup"}, {"methodBody": ["METHOD_START", "{", "return   this . count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.COMMIT3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . offset ;", "}", "METHOD_END"], "methodName": ["getOffset"], "fileName": "org.apache.hadoop.nfs.nfs3.request.COMMIT3Request"}, {"methodBody": ["METHOD_START", "{", "return   mode ;", "}", "METHOD_END"], "methodName": ["getMode"], "fileName": "org.apache.hadoop.nfs.nfs3.request.CREATE3Request"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.CREATE3Request"}, {"methodBody": ["METHOD_START", "{", "return   objAttr ;", "}", "METHOD_END"], "methodName": ["getObjAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.request.CREATE3Request"}, {"methodBody": ["METHOD_START", "{", "return   verf ;", "}", "METHOD_END"], "methodName": ["getVerf"], "fileName": "org.apache.hadoop.nfs.nfs3.request.CREATE3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.LOOKUP3Request"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.LOOKUP3Request"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.MKDIR3Request"}, {"methodBody": ["METHOD_START", "{", "return   objAttr ;", "}", "METHOD_END"], "methodName": ["getObjAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.request.MKDIR3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READ3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . offset ;", "}", "METHOD_END"], "methodName": ["getOffset"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READ3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . cookie ;", "}", "METHOD_END"], "methodName": ["getCookie"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIR3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . cookieVerf ;", "}", "METHOD_END"], "methodName": ["getCookieVerf"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIR3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIR3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . cookie ;", "}", "METHOD_END"], "methodName": ["getCookie"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIRPLUS3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . cookieVerf ;", "}", "METHOD_END"], "methodName": ["getCookieVerf"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIRPLUS3Request"}, {"methodBody": ["METHOD_START", "{", "return   dirCount ;", "}", "METHOD_END"], "methodName": ["getDirCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIRPLUS3Request"}, {"methodBody": ["METHOD_START", "{", "return   maxCount ;", "}", "METHOD_END"], "methodName": ["getMaxCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.READDIRPLUS3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.REMOVE3Request"}, {"methodBody": ["METHOD_START", "{", "return   fromDirHandle ;", "}", "METHOD_END"], "methodName": ["getFromDirHandle"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RENAME3Request"}, {"methodBody": ["METHOD_START", "{", "return   fromName ;", "}", "METHOD_END"], "methodName": ["getFromName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RENAME3Request"}, {"methodBody": ["METHOD_START", "{", "return   toDirHandle ;", "}", "METHOD_END"], "methodName": ["getToDirHandle"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RENAME3Request"}, {"methodBody": ["METHOD_START", "{", "return   toName ;", "}", "METHOD_END"], "methodName": ["getToName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RENAME3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RMDIR3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . handle ;", "}", "METHOD_END"], "methodName": ["getHandle"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RequestWithHandle"}, {"methodBody": ["METHOD_START", "{", "handle . serialize ( xdr )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.nfs3.request.RequestWithHandle"}, {"methodBody": ["METHOD_START", "{", "return   attr ;", "}", "METHOD_END"], "methodName": ["getAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SETATTR3Request"}, {"methodBody": ["METHOD_START", "{", "return   ctime ;", "}", "METHOD_END"], "methodName": ["getCtime"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SETATTR3Request"}, {"methodBody": ["METHOD_START", "{", "return   check ;", "}", "METHOD_END"], "methodName": ["isCheck"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SETATTR3Request"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SYMLINK3Request"}, {"methodBody": ["METHOD_START", "{", "return   symAttr ;", "}", "METHOD_END"], "methodName": ["getSymAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SYMLINK3Request"}, {"methodBody": ["METHOD_START", "{", "return   symData ;", "}", "METHOD_END"], "methodName": ["getSymData"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SYMLINK3Request"}, {"methodBody": ["METHOD_START", "{", "if    ( xdr . readBoolean (  )  )     {", "mode    =    xdr . readInt (  )  ;", "updateFields . add 3 Field . MODE )  ;", "}", "if    ( xdr . readBoolean (  )  )     {", "uid    =    xdr . readInt (  )  ;", "updateFields . add 3 Field . UID )  ;", "}", "if    ( xdr . readBoolean (  )  )     {", "gid    =    xdr . readInt (  )  ;", "updateFields . add 3 Field . GID )  ;", "}", "if    ( xdr . readBoolean (  )  )     {", "size    =    xdr . readHyper (  )  ;", "updateFields . add 3 Field . SIZE )  ;", "}", "int   timeSetHow    =    xdr . readInt (  )  ;", "if    ( timeSetHow    =  =     3  . TIME _ SET _ TO _ CLIENT _ TIME )  )     {", "atime    =    NfsTime . deserialize ( xdr )  ;", "updateFields . add 3 Field . ATIME )  ;", "} else", "if    ( timeSetHow    =  =     3  . TIME _ SET _ TO _ SERVER _ TIME )  )     {", "atime    =    new   NfsTime ( System . currentTimeMillis (  )  )  ;", "updateFields . add 3 Field . ATIME )  ;", "}", "timeSetHow    =    xdr . readInt (  )  ;", "if    ( timeSetHow    =  =     3  . TIME _ SET _ TO _ CLIENT _ TIME )  )     {", "mtime    =    NfsTime . deserialize ( xdr )  ;", "updateFields . add 3 Field . MTIME )  ;", "} else", "if    ( timeSetHow    =  =     3  . TIME _ SET _ TO _ SERVER _ TIME )  )     {", "mtime    =    new   NfsTime ( System . currentTimeMillis (  )  )  ;", "updateFields . add 3 Field . MTIME )  ;", "}", "}", "METHOD_END"], "methodName": ["deserialize"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   atime ;", "}", "METHOD_END"], "methodName": ["getAtime"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   gid ;", "}", "METHOD_END"], "methodName": ["getGid"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   mode ;", "}", "METHOD_END"], "methodName": ["getMode"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   mtime ;", "}", "METHOD_END"], "methodName": ["getMtime"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   uid ;", "}", "METHOD_END"], "methodName": ["getUid"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   updateFields ;", "}", "METHOD_END"], "methodName": ["getUpdateFields"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( updateFields . contains ( SetAttr 3  . SetAttrField . MODE )  )  )     {", "xdr . writeBoolean ( false )  ;", "} else    {", "xdr . writeBoolean ( true )  ;", "xdr . writeInt ( mode )  ;", "}", "if    (  !  ( updateFields . contains ( SetAttr 3  . SetAttrField . UID )  )  )     {", "xdr . writeBoolean ( false )  ;", "} else    {", "xdr . writeBoolean ( true )  ;", "xdr . writeInt ( uid )  ;", "}", "if    (  !  ( updateFields . contains ( SetAttr 3  . SetAttrField . GID )  )  )     {", "xdr . writeBoolean ( false )  ;", "} else    {", "xdr . writeBoolean ( true )  ;", "xdr . writeInt ( gid )  ;", "}", "if    (  !  ( updateFields . contains ( SetAttr 3  . SetAttrField . SIZE )  )  )     {", "xdr . writeBoolean ( false )  ;", "} else    {", "xdr . writeBoolean ( true )  ;", "xdr . writeLongAsHyper ( size )  ;", "}", "if    (  !  ( updateFields . contains ( SetAttr 3  . SetAttrField . ATIME )  )  )     {", "xdr . writeBoolean ( false )  ;", "} else    {", "xdr . writeBoolean ( true )  ;", "atime . serialize ( xdr )  ;", "}", "if    (  !  ( updateFields . contains ( SetAttr 3  . SetAttrField . MTIME )  )  )     {", "xdr . writeBoolean ( false )  ;", "} else    {", "xdr . writeBoolean ( true )  ;", "mtime . serialize ( xdr )  ;", "}", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "this . gid    =    gid ;", "}", "METHOD_END"], "methodName": ["setGid"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "this . updateFields    =    updateFields ;", "}", "METHOD_END"], "methodName": ["setUpdateFields"], "fileName": "org.apache.hadoop.nfs.nfs3.request.SetAttr3"}, {"methodBody": ["METHOD_START", "{", "return   this . count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.WRITE3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . data ;", "}", "METHOD_END"], "methodName": ["getData"], "fileName": "org.apache.hadoop.nfs.nfs3.request.WRITE3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . offset ;", "}", "METHOD_END"], "methodName": ["getOffset"], "fileName": "org.apache.hadoop.nfs.nfs3.request.WRITE3Request"}, {"methodBody": ["METHOD_START", "{", "return   this . stableHow ;", "}", "METHOD_END"], "methodName": ["getStableHow"], "fileName": "org.apache.hadoop.nfs.nfs3.request.WRITE3Request"}, {"methodBody": ["METHOD_START", "{", "this . count    =    count ;", "}", "METHOD_END"], "methodName": ["setCount"], "fileName": "org.apache.hadoop.nfs.nfs3.request.WRITE3Request"}, {"methodBody": ["METHOD_START", "{", "this . offset    =    offset ;", "}", "METHOD_END"], "methodName": ["setOffset"], "fileName": "org.apache.hadoop.nfs.nfs3.request.WRITE3Request"}, {"methodBody": ["METHOD_START", "{", "return   fileWcc ;", "}", "METHOD_END"], "methodName": ["getFileWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.COMMIT3Response"}, {"methodBody": ["METHOD_START", "{", "return   verf ;", "}", "METHOD_END"], "methodName": ["getVerf"], "fileName": "org.apache.hadoop.nfs.nfs3.response.COMMIT3Response"}, {"methodBody": ["METHOD_START", "{", "return   dirWcc ;", "}", "METHOD_END"], "methodName": ["getDirWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.CREATE3Response"}, {"methodBody": ["METHOD_START", "{", "return   objHandle ;", "}", "METHOD_END"], "methodName": ["getObjHandle"], "fileName": "org.apache.hadoop.nfs.nfs3.response.CREATE3Response"}, {"methodBody": ["METHOD_START", "{", "return   postOpObjAttr ;", "}", "METHOD_END"], "methodName": ["getPostOpObjAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.CREATE3Response"}, {"methodBody": ["METHOD_START", "{", "this . postOpAttr    =    postOpAttr ;", "}", "METHOD_END"], "methodName": ["setPostOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.GETATTR3Response"}, {"methodBody": ["METHOD_START", "{", "return   dirWcc ;", "}", "METHOD_END"], "methodName": ["getDirWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.MKDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   objAttr ;", "}", "METHOD_END"], "methodName": ["getObjAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.MKDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   objFileHandle ;", "}", "METHOD_END"], "methodName": ["getObjFileHandle"], "fileName": "org.apache.hadoop.nfs.nfs3.response.MKDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   this . status ;", "}", "METHOD_END"], "methodName": ["getStatus"], "fileName": "org.apache.hadoop.nfs.nfs3.response.NFS3Response"}, {"methodBody": ["METHOD_START", "{", "this . status    =    status ;", "}", "METHOD_END"], "methodName": ["setStatus"], "fileName": "org.apache.hadoop.nfs.nfs3.response.NFS3Response"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply   reply    =    RpcAcceptedReply . getAcceptInstance ( xid ,    verifier )  ;", "reply . write ( out )  ;", "out . writeInt ( this . getStatus (  )  )  ;", "return   out ;", "}", "METHOD_END"], "methodName": ["writeHeaderAndResponse"], "fileName": "org.apache.hadoop.nfs.nfs3.response.NFS3Response"}, {"methodBody": ["METHOD_START", "{", "return   count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READ3Response"}, {"methodBody": ["METHOD_START", "{", "return   data ;", "}", "METHOD_END"], "methodName": ["getData"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READ3Response"}, {"methodBody": ["METHOD_START", "{", "return   postOpAttr ;", "}", "METHOD_END"], "methodName": ["getPostOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READ3Response"}, {"methodBody": ["METHOD_START", "{", "return   eof ;", "}", "METHOD_END"], "methodName": ["isEof"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READ3Response"}, {"methodBody": ["METHOD_START", "{", "return   cookieVerf ;", "}", "METHOD_END"], "methodName": ["getCookieVerf"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   dirList ;", "}", "METHOD_END"], "methodName": ["getDirList"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   postOpDirAttr ;", "}", "METHOD_END"], "methodName": ["getPostOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   dirListPlus ;", "}", "METHOD_END"], "methodName": ["getDirListPlus"], "fileName": "org.apache.hadoop.nfs.nfs3.response.READDIRPLUS3Response"}, {"methodBody": ["METHOD_START", "{", "return   fromDirWcc ;", "}", "METHOD_END"], "methodName": ["getFromDirWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.RENAME3Response"}, {"methodBody": ["METHOD_START", "{", "return   toDirWcc ;", "}", "METHOD_END"], "methodName": ["getToDirWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.RENAME3Response"}, {"methodBody": ["METHOD_START", "{", "return   dirWcc ;", "}", "METHOD_END"], "methodName": ["getDirWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.RMDIR3Response"}, {"methodBody": ["METHOD_START", "{", "return   wccData ;", "}", "METHOD_END"], "methodName": ["getWccData"], "fileName": "org.apache.hadoop.nfs.nfs3.response.SETATTR3Response"}, {"methodBody": ["METHOD_START", "{", "return   dirWcc ;", "}", "METHOD_END"], "methodName": ["getDirWcc"], "fileName": "org.apache.hadoop.nfs.nfs3.response.SYMLINK3Response"}, {"methodBody": ["METHOD_START", "{", "return   objFileHandle ;", "}", "METHOD_END"], "methodName": ["getObjFileHandle"], "fileName": "org.apache.hadoop.nfs.nfs3.response.SYMLINK3Response"}, {"methodBody": ["METHOD_START", "{", "return   objPostOpAttr ;", "}", "METHOD_END"], "methodName": ["getObjPostOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.SYMLINK3Response"}, {"methodBody": ["METHOD_START", "{", "return   count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WRITE3Response"}, {"methodBody": ["METHOD_START", "{", "return   stableHow ;", "}", "METHOD_END"], "methodName": ["getStableHow"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WRITE3Response"}, {"methodBody": ["METHOD_START", "{", "return   verifer ;", "}", "METHOD_END"], "methodName": ["getVerifer"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WRITE3Response"}, {"methodBody": ["METHOD_START", "{", "return   ctime ;", "}", "METHOD_END"], "methodName": ["getCtime"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccAttr"}, {"methodBody": ["METHOD_START", "{", "return   mtime ;", "}", "METHOD_END"], "methodName": ["getMtime"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccAttr"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccAttr"}, {"methodBody": ["METHOD_START", "{", "out . writeLongAsHyper ( size )  ;", "if    (  ( mtime )     =  =    null )     {", "mtime    =    new   NfsTime (  0  )  ;", "}", "mtime . serialize ( out )  ;", "if    (  ( ctime )     =  =    null )     {", "ctime    =    new   NfsTime (  0  )  ;", "}", "ctime . serialize ( out )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccAttr"}, {"methodBody": ["METHOD_START", "{", "return   postOpAttr ;", "}", "METHOD_END"], "methodName": ["getPostOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccData"}, {"methodBody": ["METHOD_START", "{", "return   preOpAttr ;", "}", "METHOD_END"], "methodName": ["getPreOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccData"}, {"methodBody": ["METHOD_START", "{", "out . writeBoolean ( true )  ;", "preOpAttr . serialize ( out )  ;", "out . writeBoolean ( true )  ;", "postOpAttr . serialize ( out )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccData"}, {"methodBody": ["METHOD_START", "{", "this . postOpAttr    =    postOpAttr ;", "}", "METHOD_END"], "methodName": ["setPostOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccData"}, {"methodBody": ["METHOD_START", "{", "this . preOpAttr    =    preOpAttr ;", "}", "METHOD_END"], "methodName": ["setPreOpAttr"], "fileName": "org.apache.hadoop.nfs.nfs3.response.WccData"}, {"methodBody": ["METHOD_START", "{", "return   RpcAcceptedReply . getInstance ( xid ,    RpcAcceptedReply . AcceptState . SUCCESS ,    verifier )  ;", "}", "METHOD_END"], "methodName": ["getAcceptInstance"], "fileName": "org.apache.hadoop.oncrpc.RpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "return   acceptState ;", "}", "METHOD_END"], "methodName": ["getAcceptState"], "fileName": "org.apache.hadoop.oncrpc.RpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "return   new   RpcAcceptedReply ( xid ,    RpcReply . ReplyState . MSG _ ACCEPTED ,    verifier ,    state )  ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.oncrpc.RpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "Verifier   verifier    =    Verifier . readFlavorAndVerifier ( xdr )  ;", ". AcceptState   acceptState    =     . AcceptState . fromValue ( xdr . readInt (  )  )  ;", "return   new    ( xid ,    replyState ,    verifier ,    acceptState )  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.oncrpc.RpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "return   credentials ;", "}", "METHOD_END"], "methodName": ["getCredential"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   new   RpcCall ( xid ,    RpcMessage . Type . RPC _ CALL ,     2  ,    program ,    version ,    procedure ,    cred ,    verifier )  ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   procedure ;", "}", "METHOD_END"], "methodName": ["getProcedure"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   program ;", "}", "METHOD_END"], "methodName": ["getProgram"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   rpcVersion ;", "}", "METHOD_END"], "methodName": ["getRpcVersion"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   verifier ;", "}", "METHOD_END"], "methodName": ["getVerifier"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   version ;", "}", "METHOD_END"], "methodName": ["getVersion"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "return   new   RpcCall ( xdr . readInt (  )  ,    RpcMessage . Type . fromValue ( xdr . readInt (  )  )  ,    xdr . readInt (  )  ,    xdr . readInt (  )  ,    xdr . readInt (  )  ,    xdr . readInt (  )  ,    Credentials . readFlavorAndCredentials ( xdr )  ,    Verifier . readFlavorAndVerifier ( xdr )  )  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "validateMessageType ( RpcMessage . Type . RPC _ CALL )  ;", "validateRpcVersion (  )  ;", "}", "METHOD_END"], "methodName": ["validate"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rpcVersion )     !  =     ( RpcCall . RPC _ VERSION )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" RPC   version   is   expected   to   be    \"     +     ( RpcCall . RPC _ VERSION )  )     +     \"    but   got    \"  )     +     ( rpcVersion )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateRpcVersion"], "fileName": "org.apache.hadoop.oncrpc.RpcCall"}, {"methodBody": ["METHOD_START", "{", "RpcCallCache . ClientRequest   req    =    new   RpcCallCache . ClientRequest ( clientId ,    xid )  ;", "RpcCallCache . CacheEntry   e ;", "synchronized ( map )     {", "e    =    map . get ( req )  ;", "}", "e . response    =    response ;", "}", "METHOD_END"], "methodName": ["callCompleted"], "fileName": "org.apache.hadoop.oncrpc.RpcCallCache"}, {"methodBody": ["METHOD_START", "{", "RpcCallCache . ClientRequest   req    =    new   RpcCallCache . ClientRequest ( clientId ,    xid )  ;", "RpcCallCache . CacheEntry   e ;", "synchronized ( map )     {", "e    =    map . get ( req )  ;", "if    ( e    =  =    null )     {", "map . put ( req ,    new   RpcCallCache . CacheEntry (  )  )  ;", "}", "}", "return   e ;", "}", "METHOD_END"], "methodName": ["checkOrAddToCache"], "fileName": "org.apache.hadoop.oncrpc.RpcCallCache"}, {"methodBody": ["METHOD_START", "{", "return   program ;", "}", "METHOD_END"], "methodName": ["getProgram"], "fileName": "org.apache.hadoop.oncrpc.RpcCallCache"}, {"methodBody": ["METHOD_START", "{", "return   map . entrySet (  )  . iterator (  )  ;", "}", "METHOD_END"], "methodName": ["iterator"], "fileName": "org.apache.hadoop.oncrpc.RpcCallCache"}, {"methodBody": ["METHOD_START", "{", "return   map . size (  )  ;", "}", "METHOD_END"], "methodName": ["size"], "fileName": "org.apache.hadoop.oncrpc.RpcCallCache"}, {"methodBody": ["METHOD_START", "{", "return   rejectState ;", "}", "METHOD_END"], "methodName": ["getRejectState"], "fileName": "org.apache.hadoop.oncrpc.RpcDeniedReply"}, {"methodBody": ["METHOD_START", "{", "Verifier   verifier    =    Verifier . readFlavorAndVerifier ( xdr )  ;", ". RejectState   rejectState    =     . RejectState . fromValue ( xdr . readInt (  )  )  ;", "return   new    ( xid ,    replyState ,    rejectState ,    verifier )  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.oncrpc.RpcDeniedReply"}, {"methodBody": ["METHOD_START", "{", "return   channel ;", "}", "METHOD_END"], "methodName": ["channel"], "fileName": "org.apache.hadoop.oncrpc.RpcInfo"}, {"methodBody": ["METHOD_START", "{", "return   data ;", "}", "METHOD_END"], "methodName": ["data"], "fileName": "org.apache.hadoop.oncrpc.RpcInfo"}, {"methodBody": ["METHOD_START", "{", "return   header ;", "}", "METHOD_END"], "methodName": ["header"], "fileName": "org.apache.hadoop.oncrpc.RpcInfo"}, {"methodBody": ["METHOD_START", "{", "return   remoteAddress ;", "}", "METHOD_END"], "methodName": ["remoteAddress"], "fileName": "org.apache.hadoop.oncrpc.RpcInfo"}, {"methodBody": ["METHOD_START", "{", "return   messageType ;", "}", "METHOD_END"], "methodName": ["getMessageType"], "fileName": "org.apache.hadoop.oncrpc.RpcMessage"}, {"methodBody": ["METHOD_START", "{", "return   xid ;", "}", "METHOD_END"], "methodName": ["getXid"], "fileName": "org.apache.hadoop.oncrpc.RpcMessage"}, {"methodBody": ["METHOD_START", "{", "if    ( expected    !  =     ( messageType )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \"    type   is   expected   to   be    \"     +    expected )     +     \"    but   got    \"  )     +     ( messageType )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateMessageType"], "fileName": "org.apache.hadoop.oncrpc.RpcMessage"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( allowInsecurePorts )  )     {", "if    (  . LOG . isTraceEnabled (  )  )     {", ". LOG . trace (  (  \" Will   not   allow   connections   from   unprivileged   ports .     \"     +     \" Checking   for   valid   client   port .  .  .  \"  )  )  ;", "}", "if    ( remoteAddress   instanceof   InetSocketAddress )     {", "InetSocketAddress   inetRemoteAddress    =     (  ( InetSocketAddress )     ( remoteAddress )  )  ;", "if    (  ( inetRemoteAddress . getPort (  )  )     >     1  0  2  3  )     {", ". LOG . warn (  (  (  (  \" Connection   attempted   from    '  \"     +    inetRemoteAddress )     +     \"  '     \"  )     +     \" which   is   an   unprivileged   port .    Rejecting   connection .  \"  )  )  ;", "return   false ;", "}", "} else    {", ". LOG . warn (  (  (  \" Could   not   determine   remote   port   of   socket   address    '  \"     +    remoteAddress )     +     \"  '  .    Rejecting   connection .  \"  )  )  ;", "return   false ;", "}", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["doPortMonitoring"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "return   port ;", "}", "METHOD_END"], "methodName": ["getPort"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "if    ( boundPort    !  =     ( port )  )     {", ". LOG . info (  (  (  (  \" The   bound   port   is    \"     +    boundPort )     +     \"  ,    different   with   configured   port    \"  )     +     ( port )  )  )  ;", "port    =    boundPort ;", "}", "for    ( int   vers    =    lowProgVersion ;    vers    <  =     ( highProgVersion )  ;    vers +  +  )     {", "PortmapMapping   mapEntry    =    new   PortmapMapping ( progNumber ,    vers ,    transport ,    port )  ;", "register ( mapEntry ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["register"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "XDR   mappingRequest    =    PortmapRequest . create ( mapEntry ,    set )  ;", "SimpleUdpClient   registrationClient    =    new   SimpleUdpClient ( host ,     . RPCB _ PORT ,    mappingRequest ,    registrationSocket )  ;", "try    {", "registrationClient . run (  )  ;", "}    catch    ( IOException   e )     {", "String   request    =     ( set )     ?     \" Registration \"     :     \" Unregistration \"  ;", ". LOG . error (  (  (  (  (  (  ( request    +     \"    failure   with    \"  )     +     ( host )  )     +     \"  :  \"  )     +     ( port )  )     +     \"  ,    portmap   entry :     \"  )     +    mapEntry )  )  ;", "throw   new   RuntimeException (  ( request    +     \"    failure \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["register"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply   reply    =    RpcAcceptedReply . getInstance ( call . getXid (  )  ,    acceptState ,    Verifier . VERIFIER _ NONE )  ;", "XDR   out    =    new   XDR (  )  ;", "reply . write ( out )  ;", "if    ( acceptState    =  =     ( RpcAcceptedReply . AcceptState . PROG _ MISMATCH )  )     {", "out . writeInt ( lowVersion )  ;", "out . writeInt ( highVersion )  ;", "}", "ChannelBuffer   b    =    ChannelBuffers . wrappedBuffer ( out . asReadOnlyWrap (  )  . buffer (  )  )  ;", "RpcResponse   rsp    =    new   RpcResponse ( b ,    remoteAddress )  ;", "RpcUtil . sendRpcResponse ( ctx ,    rsp )  ;", "}", "METHOD_END"], "methodName": ["sendAcceptedReply"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "XDR   out    =    new   XDR (  )  ;", "DeniedReply   reply    =    new   DeniedReply ( call . getXid (  )  ,    Reply . ReplyState . MSG _ DENIED ,    DeniedReply . RejectState . AUTH _ ERROR ,    new   VerifierNone (  )  )  ;", "reply . write ( out )  ;", "ChannelBuffer   buf    =    ChannelBuffers . wrappedBuffer ( out . asReadOnlyWrap (  )  . buffer (  )  )  ;", "Response   rsp    =    new   Response ( buf ,    remoteAddress )  ;", "Util . sendResponse ( ctx ,    rsp )  ;", "}", "METHOD_END"], "methodName": ["sendRejectedReply"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "if    ( boundPort    !  =     ( port )  )     {", ". LOG . info (  (  (  (  \" The   bound   port   is    \"     +    boundPort )     +     \"  ,    different   with   configured   port    \"  )     +     ( port )  )  )  ;", "port    =    boundPort ;", "}", "for    ( int   vers    =    lowProgVersion ;    vers    <  =     ( highProgVersion )  ;    vers +  +  )     {", "PortmapMapping   mapEntry    =    new   PortmapMapping ( progNumber ,    vers ,    transport ,    port )  ;", "register ( mapEntry ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["unregister"], "fileName": "org.apache.hadoop.oncrpc.RpcProgram"}, {"methodBody": ["METHOD_START", "{", "return   replyState ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.oncrpc.RpcReply"}, {"methodBody": ["METHOD_START", "{", "return   verifier ;", "}", "METHOD_END"], "methodName": ["getVerifier"], "fileName": "org.apache.hadoop.oncrpc.RpcReply"}, {"methodBody": ["METHOD_START", "{", "int   xid    =    xdr . readInt (  )  ;", "final   RpcMessage . Type   messageType    =    RpcMessage . Type . fromValue ( xdr . readInt (  )  )  ;", "Preconditions . checkState (  ( messageType    =  =     ( RpcMessage . Type . RPC _ REPLY )  )  )  ;", ". ReplyState   stat    =     . ReplyState . fromValue ( xdr . readInt (  )  )  ;", "switch    ( stat )     {", "case   MSG _ ACCEPTED    :", "return   RpcAcceptedReply . read ( xid ,    stat ,    xdr )  ;", "case   MSG _ DENIED    :", "return   RpcDeniedReply . read ( xid ,    stat ,    xdr )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.oncrpc.RpcReply"}, {"methodBody": ["METHOD_START", "{", "return   data ;", "}", "METHOD_END"], "methodName": ["data"], "fileName": "org.apache.hadoop.oncrpc.RpcResponse"}, {"methodBody": ["METHOD_START", "{", "return   remoteAddress ;", "}", "METHOD_END"], "methodName": ["remoteAddress"], "fileName": "org.apache.hadoop.oncrpc.RpcResponse"}, {"methodBody": ["METHOD_START", "{", "return   new   RpcUtil . RpcFrameDecoder (  )  ;", "}", "METHOD_END"], "methodName": ["constructRpcFrameDecoder"], "fileName": "org.apache.hadoop.oncrpc.RpcUtil"}, {"methodBody": ["METHOD_START", "{", "return   RpcUtil . xid    =     (  +  +  ( RpcUtil . xid )  )     +     ( caller . hashCode (  )  )  ;", "}", "METHOD_END"], "methodName": ["getNewXid"], "fileName": "org.apache.hadoop.oncrpc.RpcUtil"}, {"methodBody": ["METHOD_START", "{", "Channels . fireMessageReceived ( ctx ,    response )  ;", "}", "METHOD_END"], "methodName": ["sendRpcResponse"], "fileName": "org.apache.hadoop.oncrpc.RpcUtil"}, {"methodBody": ["METHOD_START", "{", "ChannelFactory   factory    =    new   NioClientSocketChannelFactory ( Executors . newCachedThreadPool (  )  ,    Executors . newCachedThreadPool (  )  ,     1  ,     1  )  ;", "ClientBootstrap   bootstrap    =    new   ClientBootstrap ( factory )  ;", "bootstrap . setPipelineFactory ( setPipelineFactory (  )  )  ;", "bootstrap . setOption (  \" tcpNoDelay \"  ,    true )  ;", "bootstrap . setOption (  \" keepAlive \"  ,    true )  ;", "ChannelFuture   future    =    bootstrap . connect ( new   InetSocketAddress ( host ,    port )  )  ;", "if    ( oneShot )     {", "future . getChannel (  )  . getCloseFuture (  )  . awaitUninterruptibly (  )  ;", "bootstrap . releaseExternalResources (  )  ;", "}", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.oncrpc.SimpleTcpClient"}, {"methodBody": ["METHOD_START", "{", "this . pipelineFactory    =    new   ChannelPipelineFactory (  )     {", "@ Override", "public   ChannelPipeline   getPipeline (  )     {", "return   Channels . pipeline ( RpcUtil . constructRpcFrameDecoder (  )  ,    new   Handler ( request )  )  ;", "}", "}  ;", "return   this . pipelineFactory ;", "}", "METHOD_END"], "methodName": ["setPipelineFactory"], "fileName": "org.apache.hadoop.oncrpc.SimpleTcpClient"}, {"methodBody": ["METHOD_START", "{", "return   this . boundPort ;", "}", "METHOD_END"], "methodName": ["getBoundPort"], "fileName": "org.apache.hadoop.oncrpc.SimpleTcpServer"}, {"methodBody": ["METHOD_START", "{", "ChannelFactory   factory ;", "if    (  ( workerCount )     =  =     0  )     {", "factory    =    new   NioServerSocketChannelFactory ( Executors . newCachedThreadPool (  )  ,    Executors . newCachedThreadPool (  )  )  ;", "} else    {", "factory    =    new   NioServerSocketChannelFactory ( Executors . newCachedThreadPool (  )  ,    Executors . newCachedThreadPool (  )  ,    workerCount )  ;", "}", "ServerBootstrap   bootstrap    =    new   ServerBootstrap ( factory )  ;", "bootstrap . setPipelineFactory ( new   ChannelPipelineFactory (  )     {", "@ Override", "public   ChannelPipeline   getPipeline (  )    throws   Exception    {", "return   Channels . pipeline ( RpcUtil . constructRpcFrameDecoder (  )  ,    RpcUtil . STAGE _ RPC _ MESSAGE _ PARSER ,    rpcProgram ,    RpcUtil . STAGE _ RPC _ TCP _ RESPONSE )  ;", "}", "}  )  ;", "bootstrap . setOption (  \" child . tcpNoDelay \"  ,    true )  ;", "bootstrap . setOption (  \" child . keepAlive \"  ,    true )  ;", "Channel   ch    =    bootstrap . bind ( new   InetSocketAddress ( port )  )  ;", "InetSocketAddress   socketAddr    =     (  ( InetSocketAddress )     ( ch . getLocalAddress (  )  )  )  ;", "boundPort    =    socketAddr . getPort (  )  ;", ". LOG . info (  (  (  (  (  (  \" Started   listening   to   TCP   requests   at   port    \"     +     ( boundPort )  )     +     \"    for    \"  )     +     ( rpcProgram )  )     +     \"    with   workerCount    \"  )     +     ( workerCount )  )  )  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.oncrpc.SimpleTcpServer"}, {"methodBody": ["METHOD_START", "{", "InetAddress   IPAddress    =    InetAddress . getByName ( host )  ;", "byte [  ]    sendData    =    request . getBytes (  )  ;", "byte [  ]    receiveData    =    new   byte [  6  5  5  3  5  ]  ;", "DatagramSocket   socket    =     (  ( this . cSocket )     =  =    null )     ?    new   DatagramSocket (  )     :    this . cSocket ;", "try    {", "DatagramPacket   sendPacket    =    new   DatagramPacket ( sendData ,    sendData . length ,    IPAddress ,    port )  ;", "socket . send ( sendPacket )  ;", "socket . setSoTimeout (  5  0  0  )  ;", "DatagramPacket   receivePacket    =    new   DatagramPacket ( receiveData ,    receiveData . length )  ;", "socket . receive ( receivePacket )  ;", "XDR   xdr    =    new   XDR ( Arrays . copyOfRange ( receiveData ,     0  ,    receivePacket . getLength (  )  )  )  ;", "RpcReply   reply    =    RpcReply . read ( xdr )  ;", "if    (  ( reply . getState (  )  )     !  =     ( RpcReply . ReplyState . MSG _ ACCEPTED )  )     {", "throw   new   IOException (  (  \" Request   failed :     \"     +     ( reply . getState (  )  )  )  )  ;", "}", "}    finally    {", "if    (  ( this . cSocket )     =  =    null )     {", "socket . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.oncrpc.SimpleUdpClient"}, {"methodBody": ["METHOD_START", "{", "return   this . boundPort ;", "}", "METHOD_END"], "methodName": ["getBoundPort"], "fileName": "org.apache.hadoop.oncrpc.SimpleUdpServer"}, {"methodBody": ["METHOD_START", "{", "DatagramChannelFactory   f    =    new   NioDatagramChannelFactory ( Executors . newCachedThreadPool (  )  ,    workerCount )  ;", "ConnectionlessBootstrap   b    =    new   ConnectionlessBootstrap ( f )  ;", "b . setPipeline ( Channels . pipeline ( RpcUtil . STAGE _ RPC _ MESSAGE _ PARSER ,    rpcProgram ,    RpcUtil . STAGE _ RPC _ UDP _ RESPONSE )  )  ;", "b . setOption (  \" broadcast \"  ,     \" false \"  )  ;", "b . setOption (  \" sendBufferSize \"  ,    SEND _ BUFFER _ SIZE )  ;", "b . setOption (  \" receiveBufferSize \"  ,    RECEIVE _ BUFFER _ SIZE )  ;", "Channel   ch    =    b . bind ( new   InetSocketAddress ( port )  )  ;", "InetSocketAddress   socketAddr    =     (  ( InetSocketAddress )     ( ch . getLocalAddress (  )  )  )  ;", "boundPort    =    socketAddr . getPort (  )  ;", ". LOG . info (  (  (  (  (  (  \" Started   listening   to   UDP   requests   at   port    \"     +     ( boundPort )  )     +     \"    for    \"  )     +     ( rpcProgram )  )     +     \"    with   workerCount    \"  )     +     ( workerCount )  )  )  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.oncrpc.SimpleUdpServer"}, {"methodBody": ["METHOD_START", "{", "XDR   xdr _ out    =    new   XDR (  )  ;", ". createPortmapXDRheader ( xdr _ out ,     3  )  ;", "return   xdr _ out ;", "}", "METHOD_END"], "methodName": ["createGetportMount"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "RpcCall . getInstance (  0  ,     1  0  0  0  0  0  ,     2  ,    procedure ,    new   CredentialsNone (  )  ,    new   VerifierNone (  )  )  . write ( xdr _ out )  ;", "}", "METHOD_END"], "methodName": ["createPortmapXDRheader"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "Random   rand    =    new   Random (  )  ;", "int   serverPort    =     3  0  0  0  0     +     ( rand . nextInt (  1  0  0  0  0  )  )  ;", "int   retries    =     1  0  ;", "while    ( true )     {", "try    {", "RpcProgram   program    =    new    . TestRpcProgram (  \" TestRpcProgram \"  ,     \" localhost \"  ,    serverPort ,     1  0  0  0  0  0  ,     1  ,     2  ,    allowInsecurePorts )  ;", "SimpleTcpServer   tcpServer    =    new   SimpleTcpServer ( serverPort ,    program ,     1  )  ;", "tcpServer . run (  )  ;", "break ;", "}    catch    ( ChannelException   ce )     {", "if    (  ( retries -  -  )     >     0  )     {", "serverPort    +  =    rand . nextInt (  2  0  )  ;", "} else    {", "throw   ce ;", "}", "}", "}", "return   serverPort ;", "}", "METHOD_END"], "methodName": ["startRpcServer"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "int   serverPort    =    TestFrameDecoder . startRpcServer ( true )  ;", "XDR   xdrOut    =    TestFrameDecoder . createGetportMount (  )  ;", "int   headerSize    =    xdrOut . size (  )  ;", "int   bufsize    =     (  2     *     1  0  2  4  )     *     1  0  2  4  ;", "byte [  ]    buffer    =    new   byte [ bufsize ]  ;", "xdrOut . writeFixedOpaque ( buffer )  ;", "int   requestSize    =     ( xdrOut . size (  )  )     -    headerSize ;", "TestFrameDecoder . testRequest ( xdrOut ,    serverPort )  ;", "assertEquals ( requestSize ,    TestFrameDecoder . resultSize )  ;", "}", "METHOD_END"], "methodName": ["testFrames"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "RpcUtil . RpcFrameDecoder   decoder    =    new   RpcUtil . RpcFrameDecoder (  )  ;", "byte [  ]    fragment 1     =    new   byte [  4     +     1  0  ]  ;", "fragment 1  [  0  ]     =     0  ;", "fragment 1  [  1  ]     =     0  ;", "fragment 1  [  2  ]     =     0  ;", "fragment 1  [  3  ]     =     (  ( byte )     (  1  0  )  )  ;", "assertFalse ( XDR . isLastFragment ( fragment 1  )  )  ;", "assertTrue (  (  ( XDR . fragmentSize ( fragment 1  )  )     =  =     1  0  )  )  ;", "ByteBuffer   buffer    =    ByteBuffer . allocate (  (  4     +     1  0  )  )  ;", "buffer . put ( fragment 1  )  ;", "buffer . flip (  )  ;", "ChannelBuffer   buf    =    new   ByteBufferBackedChannelBuffer ( buffer )  ;", "ChannelBuffer   channelBuffer    =     (  ( ChannelBuffer )     ( decoder . decode ( Mockito . mock ( ChannelHandlerContext . class )  ,    Mockito . mock ( Channel . class )  ,    buf )  )  )  ;", "assertTrue (  ( channelBuffer    =  =    null )  )  ;", "byte [  ]    fragment 2     =    new   byte [  4     +     1  0  ]  ;", "fragment 2  [  0  ]     =     (  ( byte )     (  1     <  <     7  )  )  ;", "fragment 2  [  1  ]     =     0  ;", "fragment 2  [  2  ]     =     0  ;", "fragment 2  [  3  ]     =     (  ( byte )     (  1  0  )  )  ;", "assertTrue ( XDR . isLastFragment ( fragment 2  )  )  ;", "assertTrue (  (  ( XDR . fragmentSize ( fragment 2  )  )     =  =     1  0  )  )  ;", "buffer    =    ByteBuffer . allocate (  (  4     +     1  0  )  )  ;", "buffer . put ( fragment 2  )  ;", "buffer . flip (  )  ;", "buf    =    new   ByteBufferBackedChannelBuffer ( buffer )  ;", "channelBuffer    =     (  ( ChannelBuffer )     ( decoder . decode ( Mockito . mock ( ChannelHandlerContext . class )  ,    Mockito . mock ( Channel . class )  ,    buf )  )  )  ;", "assertTrue (  ( channelBuffer    !  =    null )  )  ;", "assertEquals (  2  0  ,    channelBuffer . readableBytes (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleFrames"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "TestFrameDecoder . resultSize    =     0  ;", "SimpleTcpClient   tcpClient    =    new   SimpleTcpClient (  \" localhost \"  ,    serverPort ,    request ,    true )  ;", "tcpClient . run (  )  ;", "}", "METHOD_END"], "methodName": ["testRequest"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "RpcUtil . RpcFrameDecoder   decoder    =    new   RpcUtil . RpcFrameDecoder (  )  ;", "ByteBuffer   buffer    =    ByteBuffer . allocate (  1  )  ;", "ChannelBuffer   buf    =    new   ByteBufferBackedChannelBuffer ( buffer )  ;", "ChannelBuffer   channelBuffer    =     (  ( ChannelBuffer )     ( decoder . decode ( Mockito . mock ( ChannelHandlerContext . class )  ,    Mockito . mock ( Channel . class )  ,    buf )  )  )  ;", "assertTrue (  ( channelBuffer    =  =    null )  )  ;", "byte [  ]    fragment    =    new   byte [  4     +     9  ]  ;", "fragment [  0  ]     =     (  ( byte )     (  1     <  <     7  )  )  ;", "fragment [  1  ]     =     0  ;", "fragment [  2  ]     =     0  ;", "fragment [  3  ]     =     (  ( byte )     (  1  0  )  )  ;", "assertTrue ( XDR . isLastFragment ( fragment )  )  ;", "assertTrue (  (  ( XDR . fragmentSize ( fragment )  )     =  =     1  0  )  )  ;", "buffer    =    ByteBuffer . allocate (  (  4     +     9  )  )  ;", "buffer . put ( fragment )  ;", "buffer . flip (  )  ;", "buf    =    new   ByteBufferBackedChannelBuffer ( buffer )  ;", "channelBuffer    =     (  ( ChannelBuffer )     ( decoder . decode ( Mockito . mock ( ChannelHandlerContext . class )  ,    Mockito . mock ( Channel . class )  ,    buf )  )  )  ;", "assertTrue (  ( channelBuffer    =  =    null )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleFrame"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "int   serverPort    =    TestFrameDecoder . startRpcServer ( false )  ;", "XDR   xdrOut    =    TestFrameDecoder . createGetportMount (  )  ;", "int   bufsize    =     (  2     *     1  0  2  4  )     *     1  0  2  4  ;", "byte [  ]    buffer    =    new   byte [ bufsize ]  ;", "xdrOut . writeFixedOpaque ( buffer )  ;", "TestFrameDecoder . testRequest ( xdrOut ,    serverPort )  ;", "assertEquals (  0  ,    TestFrameDecoder . resultSize )  ;", "xdrOut    =    new   XDR (  )  ;", "TestFrameDecoder . createPortmapXDRheader ( xdrOut ,     0  )  ;", "int   headerSize    =    xdrOut . size (  )  ;", "buffer    =    new   byte [ bufsize ]  ;", "xdrOut . writeFixedOpaque ( buffer )  ;", "int   requestSize    =     ( xdrOut . size (  )  )     -    headerSize ;", "TestFrameDecoder . testRequest ( xdrOut ,    serverPort )  ;", "assertEquals ( requestSize ,    TestFrameDecoder . resultSize )  ;", "}", "METHOD_END"], "methodName": ["testUnprivilegedPort"], "fileName": "org.apache.hadoop.oncrpc.TestFrameDecoder"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RpcAcceptedReply . AcceptState . SUCCESS ,    RpcAcceptedReply . AcceptState . fromValue (  0  )  )  ;", "assertEquals ( RpcAcceptedReply . AcceptState . PROG _ UNAVAIL ,    RpcAcceptedReply . AcceptState . fromValue (  1  )  )  ;", "assertEquals ( RpcAcceptedReply . AcceptState . PROG _ MISMATCH ,    RpcAcceptedReply . AcceptState . fromValue (  2  )  )  ;", "assertEquals ( RpcAcceptedReply . AcceptState . PROC _ UNAVAIL ,    RpcAcceptedReply . AcceptState . fromValue (  3  )  )  ;", "assertEquals ( RpcAcceptedReply . AcceptState . GARBAGE _ ARGS ,    RpcAcceptedReply . AcceptState . fromValue (  4  )  )  ;", "assertEquals ( RpcAcceptedReply . AcceptState . SYSTEM _ ERR ,    RpcAcceptedReply . AcceptState . fromValue (  5  )  )  ;", "}", "METHOD_END"], "methodName": ["testAcceptState"], "fileName": "org.apache.hadoop.oncrpc.TestRpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . AcceptState . fromValue (  6  )  ;", "}", "METHOD_END"], "methodName": ["testAcceptStateFromInvalidValue"], "fileName": "org.apache.hadoop.oncrpc.TestRpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "Verifier   verifier    =    new   VerifierNone (  )  ;", "reply    =    new    (  0  ,    RpcReply . ReplyState . MSG _ ACCEPTED ,    verifier ,     . AcceptState . SUCCESS )  ;", "assertEquals (  0  ,    reply . getXid (  )  )  ;", "assertEquals ( RpcMessage . Type . RPC _ REPLY ,    reply . getMessageType (  )  )  ;", "assertEquals ( RpcReply . ReplyState . MSG _ ACCEPTED ,    reply . getState (  )  )  ;", "assertEquals ( verifier ,    reply . getVerifier (  )  )  ;", "assertEquals (  . AcceptState . SUCCESS ,    reply . getAcceptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConstructor"], "fileName": "org.apache.hadoop.oncrpc.TestRpcAcceptedReply"}, {"methodBody": ["METHOD_START", "{", "Credentials   credential    =    new   CredentialsNone (  )  ;", "Verifier   verifier    =    new   VerifierNone (  )  ;", "int   rpcVersion    =     . RPC _ VERSION ;", "int   program    =     2  ;", "int   version    =     3  ;", "int   procedure    =     4  ;", "call    =    new    (  0  ,    RpcMessage . Type . RPC _ CALL ,    rpcVersion ,    program ,    version ,    procedure ,    credential ,    verifier )  ;", "assertEquals (  0  ,    call . getXid (  )  )  ;", "assertEquals ( RpcMessage . Type . RPC _ CALL ,    call . getMessageType (  )  )  ;", "assertEquals ( rpcVersion ,    call . getRpcVersion (  )  )  ;", "assertEquals ( program ,    call . getProgram (  )  )  ;", "assertEquals ( version ,    call . getVersion (  )  )  ;", "assertEquals ( procedure ,    call . getProcedure (  )  )  ;", "assertEquals ( credential ,    call . getCredential (  )  )  ;", "assertEquals ( verifier ,    call . getVerifier (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConstructor"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCall"}, {"methodBody": ["METHOD_START", "{", "RpcMessage . Type   invalidMessageType    =    RpcMessage . Type . RPC _ REPLY ;", "new    (  0  ,    invalidMessageType ,     . RPC _ VERSION ,     2  ,     3  ,     4  ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["testInvalidRpcMessageType"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCall"}, {"methodBody": ["METHOD_START", "{", "int   invalidRpcVersion    =     3  ;", "new    (  0  ,    RpcMessage . Type . RPC _ CALL ,    invalidRpcVersion ,     2  ,     3  ,     4  ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["testInvalidRpcVersion"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCall"}, {"methodBody": ["METHOD_START", "{", "RpcCallCache   cache    =    new   RpcCallCache (  \" test \"  ,     1  0  0  )  ;", "InetAddress   clientIp    =    InetAddress . getByName (  \"  1  .  1  .  1  .  1  \"  )  ;", "int   xid    =     1  0  0  ;", "RpcCallCache . CacheEntry   e    =    cache . checkOrAddToCache ( clientIp ,    xid )  ;", "assertNull ( e )  ;", "e    =    cache . checkOrAddToCache ( clientIp ,    xid )  ;", "validateInprogressCacheEntry ( e )  ;", "RpcResponse   response    =    mock ( RpcResponse . class )  ;", "cache . callCompleted ( clientIp ,    xid ,    response )  ;", "e    =    cache . checkOrAddToCache ( clientIp ,    xid )  ;", "validateCompletedCacheEntry ( e ,    response )  ;", "}", "METHOD_END"], "methodName": ["testAddRemoveEntries"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "RpcCallCache . CacheEntry   c    =    new   RpcCallCache . CacheEntry (  )  ;", "validateInprogressCacheEntry ( c )  ;", "assertTrue ( c . isInProgress (  )  )  ;", "assertFalse ( c . isCompleted (  )  )  ;", "assertNull ( c . getResponse (  )  )  ;", "RpcResponse   response    =    mock ( RpcResponse . class )  ;", "c . setResponse ( response )  ;", "validateCompletedCacheEntry ( c ,    response )  ;", "}", "METHOD_END"], "methodName": ["testCacheEntry"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "RpcCallCache   cache    =    new   RpcCallCache (  \" Test \"  ,     1  0  )  ;", "int   size    =     0  ;", "for    ( int   clientId    =     0  ;    clientId    <     2  0  ;    clientId +  +  )     {", "InetAddress   clientIp    =    InetAddress . getByName (  (  \"  1  .  1  .  1  .  \"     +    clientId )  )  ;", "System . out . println (  (  \" Adding    \"     +    clientIp )  )  ;", "cache . checkOrAddToCache ( clientIp ,     0  )  ;", "size    =    Math . min (  (  +  + size )  ,     1  0  )  ;", "System . out . println (  (  \" Cache   size    \"     +     ( cache . size (  )  )  )  )  ;", "assertEquals ( size ,    cache . size (  )  )  ;", "int   startEntry    =    Math . max (  (  ( clientId    -     1  0  )     +     1  )  ,     0  )  ;", "Iterator < Map . Entry < RpcCallCache . ClientRequest ,    RpcCallCache . CacheEntry >  >    iterator    =    cache . iterator (  )  ;", "for    ( int   i    =     0  ;    i    <    size ;    i +  +  )     {", "RpcCallCache . ClientRequest   key    =    iterator . next (  )  . getKey (  )  ;", "System . out . println (  (  \" Entry    \"     +     ( key . getClientId (  )  )  )  )  ;", "assertEquals ( InetAddress . getByName (  (  \"  1  .  1  .  1  .  \"     +     ( startEntry    +    i )  )  )  ,    key . getClientId (  )  )  ;", "}", "for    ( int   i    =     0  ;    i    <    size ;    i +  +  )     {", "RpcCallCache . CacheEntry   e    =    cache . checkOrAddToCache ( InetAddress . getByName (  (  \"  1  .  1  .  1  .  \"     +     ( startEntry    +    i )  )  )  ,     0  )  ;", "assertNotNull ( e )  ;", "assertTrue ( e . isInProgress (  )  )  ;", "assertFalse ( e . isCompleted (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCacheFunctionality"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "RpcCallCache   cache    =    new   RpcCallCache (  \" test \"  ,     1  0  0  )  ;", "assertEquals (  \" test \"  ,    cache . getProgram (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRpcCallCacheConstructor"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "new   RpcCallCache (  \" test \"  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testRpcCallCacheConstructorIllegalArgument0"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "new   RpcCallCache (  \" test \"  ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRpcCallCacheConstructorIllegalArgumentNegative"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "assertFalse ( c . isInProgress (  )  )  ;", "assertTrue ( c . isCompleted (  )  )  ;", "assertEquals ( response ,    c . getResponse (  )  )  ;", "}", "METHOD_END"], "methodName": ["validateCompletedCacheEntry"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "assertTrue ( c . isInProgress (  )  )  ;", "assertFalse ( c . isCompleted (  )  )  ;", "assertNull ( c . getResponse (  )  )  ;", "}", "METHOD_END"], "methodName": ["validateInprogressCacheEntry"], "fileName": "org.apache.hadoop.oncrpc.TestRpcCallCache"}, {"methodBody": ["METHOD_START", "{", "RpcDeniedReply   reply    =    new   RpcDeniedReply (  0  ,    RpcReply . ReplyState . MSG _ ACCEPTED ,    RpcDeniedReply . RejectState . AUTH _ ERROR ,    new   VerifierNone (  )  )  ;", "Assert . assertEquals (  0  ,    reply . getXid (  )  )  ;", "Assert . assertEquals ( RpcMessage . Type . RPC _ REPLY ,    reply . getMessageType (  )  )  ;", "Assert . assertEquals ( RpcReply . ReplyState . MSG _ ACCEPTED ,    reply . getState (  )  )  ;", "Assert . assertEquals ( RpcDeniedReply . RejectState . AUTH _ ERROR ,    reply . getRejectState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConstructor"], "fileName": "org.apache.hadoop.oncrpc.TestRpcDeniedReply"}, {"methodBody": ["METHOD_START", "{", "RpcDeniedReply . RejectState . fromValue (  2  )  ;", "}", "METHOD_END"], "methodName": ["testRejectStateFromInvalidValue1"], "fileName": "org.apache.hadoop.oncrpc.TestRpcDeniedReply"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( RpcDeniedReply . RejectState . RPC _ MISMATCH ,    RpcDeniedReply . RejectState . fromValue (  0  )  )  ;", "Assert . assertEquals ( RpcDeniedReply . RejectState . AUTH _ ERROR ,    RpcDeniedReply . RejectState . fromValue (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRejectStateFromValue"], "fileName": "org.apache.hadoop.oncrpc.TestRpcDeniedReply"}, {"methodBody": ["METHOD_START", "{", "return   new   RpcMessage ( xid ,    msgType )     {", "@ Override", "public   XDR   write ( XDR   xdr )     {", "return   null ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getRpcMessage"], "fileName": "org.apache.hadoop.oncrpc.TestRpcMessage"}, {"methodBody": ["METHOD_START", "{", "RpcMessage   msg    =    getRpcMessage (  0  ,    RpcMessage . Type . RPC _ CALL )  ;", "Assert . assertEquals (  0  ,    msg . getXid (  )  )  ;", "Assert . assertEquals ( RpcMessage . Type . RPC _ CALL ,    msg . getMessageType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRpcMessage"], "fileName": "org.apache.hadoop.oncrpc.TestRpcMessage"}, {"methodBody": ["METHOD_START", "{", "RpcMessage   msg    =    getRpcMessage (  0  ,    RpcMessage . Type . RPC _ CALL )  ;", "msg . validateMessageType ( RpcMessage . Type . RPC _ CALL )  ;", "}", "METHOD_END"], "methodName": ["testValidateMessage"], "fileName": "org.apache.hadoop.oncrpc.TestRpcMessage"}, {"methodBody": ["METHOD_START", "{", "RpcMessage   msg    =    getRpcMessage (  0  ,    RpcMessage . Type . RPC _ CALL )  ;", "msg . validateMessageType ( RpcMessage . Type . RPC _ REPLY )  ;", "}", "METHOD_END"], "methodName": ["testValidateMessageException"], "fileName": "org.apache.hadoop.oncrpc.TestRpcMessage"}, {"methodBody": ["METHOD_START", "{", "RpcReply . ReplyState . fromValue (  2  )  ;", "}", "METHOD_END"], "methodName": ["testReplyStateFromInvalidValue1"], "fileName": "org.apache.hadoop.oncrpc.TestRpcReply"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( RpcReply . ReplyState . MSG _ ACCEPTED ,    RpcReply . ReplyState . fromValue (  0  )  )  ;", "Assert . assertEquals ( RpcReply . ReplyState . MSG _ DENIED ,    RpcReply . ReplyState . fromValue (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testReplyStateFromValue"], "fileName": "org.apache.hadoop.oncrpc.TestRpcReply"}, {"methodBody": ["METHOD_START", "{", "RpcReply   reply    =    new   RpcReply (  0  ,    RpcReply . ReplyState . MSG _ ACCEPTED ,    new   VerifierNone (  )  )     {", "@ Override", "public   XDR   write ( XDR   xdr )     {", "return   null ;", "}", "}  ;", "Assert . assertEquals (  0  ,    reply . getXid (  )  )  ;", "Assert . assertEquals ( RpcMessage . Type . RPC _ REPLY ,    reply . getMessageType (  )  )  ;", "Assert . assertEquals ( RpcReply . ReplyState . MSG _ ACCEPTED ,    reply . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRpcReply"], "fileName": "org.apache.hadoop.oncrpc.TestRpcReply"}, {"methodBody": ["METHOD_START", "{", "XDR   w    =    new   XDR (  )  ;", "for    ( int   i    =     0  ;    i    <    times ;     +  + i )", "w . writeInt (  . WRITE _ VALUE )  ;", "XDR   r    =    w . asReadOnlyWrap (  )  ;", "for    ( int   i    =     0  ;    i    <    times ;     +  + i )", "assertEquals (  . WRITE _ VALUE ,    r . readInt (  )  )  ;", "}", "METHOD_END"], "methodName": ["serializeInt"], "fileName": "org.apache.hadoop.oncrpc.TestXDR"}, {"methodBody": ["METHOD_START", "{", "XDR   w    =    new   XDR (  )  ;", "for    ( int   i    =     0  ;    i    <    times ;     +  + i )", "w . writeLongAsHyper (  . WRITE _ VALUE )  ;", "XDR   r    =    w . asReadOnlyWrap (  )  ;", "for    ( int   i    =     0  ;    i    <    times ;     +  + i )", "assertEquals (  . WRITE _ VALUE ,    r . readHyper (  )  )  ;", "}", "METHOD_END"], "methodName": ["serializeLong"], "fileName": "org.apache.hadoop.oncrpc.TestXDR"}, {"methodBody": ["METHOD_START", "{", "final   int   TEST _ TIMES    =     8     <  <     2  0  ;", "serializeInt ( TEST _ TIMES )  ;", "serializeLg ( TEST _ TIMES )  ;", "}", "METHOD_END"], "methodName": ["testPerformance"], "fileName": "org.apache.hadoop.oncrpc.TestXDR"}, {"methodBody": ["METHOD_START", "{", "buf . position ( alignUp ( buf . position (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["alignPosition"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "return   length    +     ( pad ( length )  )  ;", "}", "METHOD_END"], "methodName": ["alignUp"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ByteBuffer   b    =    buf . asReadOnlyBuffer (  )  ;", "if    (  ( state )     =  =     (  . State . WRITING )  )     {", "b . flip (  )  ;", "}", "n    =    new    ( b ,     . State . READING )  ;", "return   n ;", "}", "METHOD_END"], "methodName": ["asReadOnlyWrap"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "return   buf . duplicate (  )  ;", "}", "METHOD_END"], "methodName": ["buffer"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . WRITING )  )  )  ;", "if    (  ( buf . remaining (  )  )     <    size )     {", "int   newCity    =     ( buf . city (  )  )     *     2  ;", "int   newRemaining    =     ( buf . city (  )  )     +     ( buf . remaining (  )  )  ;", "while    ( newRemaining    <    size )     {", "newRemaining    +  =    newCity ;", "newCity    *  =     2  ;", "}", "ByteBuffer   newbuf    =    ByteBuffer . allocate ( newCity )  ;", "buf . flip (  )  ;", "newbuf . put ( buf )  ;", "buf    =    newbuf ;", "}", "}", "METHOD_END"], "methodName": ["ensureFreeSpace"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ByteBuffer   b    =    ByteBuffer . wrap ( mark )  ;", "int   n    =    b . getInt (  )  ;", "return   n    &     2  1  4  7  4  8  3  6  4  7  ;", "}", "METHOD_END"], "methodName": ["fragmentSize"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ByteBuffer   d    =    asReadOnlyWrap (  )  . buffer (  )  ;", "byte [  ]    b    =    new   byte [ d . remaining (  )  ]  ;", "d . get ( b )  ;", "return   b ;", "}", "METHOD_END"], "methodName": ["getBytes"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ByteBuffer   b    =    ByteBuffer . wrap ( mark )  ;", "int   n    =    b . getInt (  )  ;", "return    ( n    &     -  2  1  4  7  4  8  3  6  4  8  )     !  =     0  ;", "}", "METHOD_END"], "methodName": ["isLastFragment"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "switch    ( length    %     4  )     {", "case    1     :", "return    3  ;", "case    2     :", "return    2  ;", "case    3     :", "return    1  ;", "default    :", "return    0  ;", "}", "}", "METHOD_END"], "methodName": ["pad"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . READING )  )  )  ;", "return    ( buf . getInt (  )  )     !  =     0  ;", "}", "METHOD_END"], "methodName": ["readBoolean"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . READING )  )  )  ;", "byte [  ]    r    =    new   byte [ size ]  ;", "buf . get ( r )  ;", "alignPosition (  )  ;", "return   r ;", "}", "METHOD_END"], "methodName": ["readFixedOpaque"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . READING )  )  )  ;", "return   buf . getLong (  )  ;", "}", "METHOD_END"], "methodName": ["readHyper"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . READING )  )  )  ;", "return   buf . getInt (  )  ;", "}", "METHOD_END"], "methodName": ["readInt"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "return   new   String ( readVariableOpaque (  )  )  ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . READING )  )  )  ;", "int   size    =    readInt (  )  ;", "return   readFixedOpaque ( size )  ;", "}", "METHOD_END"], "methodName": ["readVariableOpaque"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    b    =    new   byte [ XDR . SIZEOF _ INT ]  ;", "ByteBuffer   buf    =    ByteBuffer . wrap ( b )  ;", "buf . putInt (  (  ! last    ?    size    :    size    |     -  2  1  4  7  4  8  3  6  4  8  )  )  ;", "return   b ;", "}", "METHOD_END"], "methodName": ["recordMark"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "return    ( state )     =  =     ( XDR . State . READING )     ?    buf . limit (  )     :    buf . position (  )  ;", "}", "METHOD_END"], "methodName": ["size"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "return    ( xdr . buf . remaining (  )  )     >  =    len ;", "}", "METHOD_END"], "methodName": ["verifyLength"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ensureFreeSpace ( XDR . SIZEOF _ INT )  ;", "buf . putInt (  ( v    ?     1     :     0  )  )  ;", "}", "METHOD_END"], "methodName": ["writeBoolean"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "writeFixedOpaque ( src ,    src . length )  ;", "}", "METHOD_END"], "methodName": ["writeFixedOpaque"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ensureFreeSpace ( alignUp ( length )  )  ;", "buf . put ( src ,     0  ,    length )  ;", "writePadding (  )  ;", "}", "METHOD_END"], "methodName": ["writeFixedOpaque"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ensureFreeSpace ( XDR . SIZEOF _ INT )  ;", "buf . putInt ( v )  ;", "}", "METHOD_END"], "methodName": ["writeInt"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ensureFreeSpace ( XDR . SIZEOF _ LONG )  ;", "buf . putLong ( v )  ;", "}", "METHOD_END"], "methodName": ["writeLongAsHyper"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( request . state )     =  =     ( XDR . State . WRITING )  )  )  ;", "ByteBuffer   b    =    request . buf . duplicate (  )  ;", "b . flip (  )  ;", "byte [  ]    fragmentHeader    =    XDR . recordMark ( b . limit (  )  ,    last )  ;", "ByteBuffer   headerBuf    =    ByteBuffer . wrap ( fragmentHeader )  ;", "return   ChannelBuffers . copiedBuffer ( headerBuf ,    b )  ;", "}", "METHOD_END"], "methodName": ["writeMessageTcp"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( response . state )     =  =     ( XDR . State . READING )  )  )  ;", "return   ChannelBuffers . copiedBuffer ( response . buf )  ;", "}", "METHOD_END"], "methodName": ["writeMessageUdp"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkState (  (  ( state )     =  =     ( XDR . State . WRITING )  )  )  ;", "int   p    =    pad ( buf . position (  )  )  ;", "ensureFreeSpace ( p )  ;", "buf . put ( XDR . PADDING _ BYTES ,     0  ,    p )  ;", "}", "METHOD_END"], "methodName": ["writePadding"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "writeVariableOpaque ( s . getBytes (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "ensureFreeSpace (  (  ( XDR . SIZEOF _ INT )     +     ( alignUp ( src . length )  )  )  )  ;", "buf . putInt ( src . length )  ;", "writeFixedOpaque ( src )  ;", "}", "METHOD_END"], "methodName": ["writeVariableOpaque"], "fileName": "org.apache.hadoop.oncrpc.XDR"}, {"methodBody": ["METHOD_START", "{", "RpcAuthInfo . AuthFlavor   flavor    =    RpcAuthInfo . AuthFlavor . fromValue ( xdr . readInt (  )  )  ;", "final      credentials ;", "if    ( flavor    =  =     ( RpcAuthInfo . AuthFlavor . AUTH _ NONE )  )     {", "credentials    =    new   None (  )  ;", "} else", "if    ( flavor    =  =     ( RpcAuthInfo . AuthFlavor . AUTH _ SYS )  )     {", "credentials    =    new   Sys (  )  ;", "} else", "if    ( flavor    =  =     ( RpcAuthInfo . AuthFlavor . RPCSEC _ GSS )  )     {", "credentials    =    new   GSS (  )  ;", "} else    {", "throw   new   UnsupportedOperationException (  (  \" Unsupported      Flavor    \"     +    flavor )  )  ;", "}", "credentials . read ( xdr )  ;", "return   credentials ;", "}", "METHOD_END"], "methodName": ["readFlavorAndCredentials"], "fileName": "org.apache.hadoop.oncrpc.security.Credentials"}, {"methodBody": ["METHOD_START", "{", "if    ( cred   instanceof   CredentialsNone )     {", "xdr . writeInt ( RpcAuthInfo . AuthFlavor . AUTH _ NONE . getValue (  )  )  ;", "} else", "if    ( cred   instanceof   CredentialsSys )     {", "xdr . writeInt ( RpcAuthInfo . AuthFlavor . AUTH _ SYS . getValue (  )  )  ;", "} else", "if    ( cred   instanceof   CredentialsGSS )     {", "xdr . writeInt ( RpcAuthInfo . AuthFlavor . RPCSEC _ GSS . getValue (  )  )  ;", "} else    {", "throw   new   UnsupportedOperationException (  \" Cannot   recognize   the   verifier \"  )  ;", "}", "cred . write ( xdr )  ;", "}", "METHOD_END"], "methodName": ["writeFlavorAndCredentials"], "fileName": "org.apache.hadoop.oncrpc.security.Credentials"}, {"methodBody": ["METHOD_START", "{", "return   mAuxGIDs ;", "}", "METHOD_END"], "methodName": ["getAuxGIDs"], "fileName": "org.apache.hadoop.oncrpc.security.CredentialsSys"}, {"methodBody": ["METHOD_START", "{", "return   mGID ;", "}", "METHOD_END"], "methodName": ["getGID"], "fileName": "org.apache.hadoop.oncrpc.security.CredentialsSys"}, {"methodBody": ["METHOD_START", "{", "return   mUID ;", "}", "METHOD_END"], "methodName": ["getUID"], "fileName": "org.apache.hadoop.oncrpc.security.CredentialsSys"}, {"methodBody": ["METHOD_START", "{", "this . mGID    =    gid ;", "}", "METHOD_END"], "methodName": ["setGID"], "fileName": "org.apache.hadoop.oncrpc.security.CredentialsSys"}, {"methodBody": ["METHOD_START", "{", "this . mStamp    =    stamp ;", "}", "METHOD_END"], "methodName": ["setStamp"], "fileName": "org.apache.hadoop.oncrpc.security.CredentialsSys"}, {"methodBody": ["METHOD_START", "{", "this . mUID    =    uid ;", "}", "METHOD_END"], "methodName": ["setUID"], "fileName": "org.apache.hadoop.oncrpc.security.CredentialsSys"}, {"methodBody": ["METHOD_START", "{", "return   flavor ;", "}", "METHOD_END"], "methodName": ["getFlavor"], "fileName": "org.apache.hadoop.oncrpc.security.RpcAuthInfo"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  )  ;", "}", "METHOD_END"], "methodName": ["getAuxGids"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  )  ;", "}", "METHOD_END"], "methodName": ["getGid"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  )  ;", "}", "METHOD_END"], "methodName": ["getUid"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["isUnwrapRequired"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["isWrapRequired"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  )  ;", "}", "METHOD_END"], "methodName": ["unwrap"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  )  ;", "}", "METHOD_END"], "methodName": ["wrap"], "fileName": "org.apache.hadoop.oncrpc.security.SecurityHandler"}, {"methodBody": ["METHOD_START", "{", "CredentialsSys   credential    =    new   CredentialsSys (  )  ;", "credential . setUID (  0  )  ;", "credential . setGID (  1  )  ;", "XDR   xdr    =    new   XDR (  )  ;", "credential . write ( xdr )  ;", "CredentialsSys   newCredential    =    new   CredentialsSys (  )  ;", "newCredential . read ( xdr . asReadOnlyWrap (  )  )  ;", "assertEquals (  0  ,    newCredential . getUID (  )  )  ;", "assertEquals (  1  ,    newCredential . getGID (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReadWrite"], "fileName": "org.apache.hadoop.oncrpc.security.TestCredentialsSys"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RpcAuthInfo . AuthFlavor . AUTH _ NONE ,    RpcAuthInfo . AuthFlavor . fromValue (  0  )  )  ;", "assertEquals ( RpcAuthInfo . AuthFlavor . AUTH _ SYS ,    RpcAuthInfo . AuthFlavor . fromValue (  1  )  )  ;", "assertEquals ( RpcAuthInfo . AuthFlavor . AUTH _ SHORT ,    RpcAuthInfo . AuthFlavor . fromValue (  2  )  )  ;", "assertEquals ( RpcAuthInfo . AuthFlavor . AUTH _ DH ,    RpcAuthInfo . AuthFlavor . fromValue (  3  )  )  ;", "assertEquals ( RpcAuthInfo . AuthFlavor . RPCSEC _ GSS ,    RpcAuthInfo . AuthFlavor . fromValue (  6  )  )  ;", "}", "METHOD_END"], "methodName": ["testAuthFlavor"], "fileName": "org.apache.hadoop.oncrpc.security.TestRpcAuthInfo"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RpcAuthInfo . AuthFlavor . AUTH _ NONE ,    RpcAuthInfo . AuthFlavor . fromValue (  4  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidAuthFlavor"], "fileName": "org.apache.hadoop.oncrpc.security.TestRpcAuthInfo"}, {"methodBody": ["METHOD_START", "{", "RpcAuthInfo . AuthFlavor   flavor    =    RpcAuthInfo . AuthFlavor . fromValue ( xdr . readInt (  )  )  ;", "final      verifer ;", "if    ( flavor    =  =     ( RpcAuthInfo . AuthFlavor . AUTH _ NONE )  )     {", "verifer    =    new   None (  )  ;", "} else", "if    ( flavor    =  =     ( RpcAuthInfo . AuthFlavor . RPCSEC _ GSS )  )     {", "verifer    =    new   GSS (  )  ;", "} else    {", "throw   new   UnsupportedOperationException (  (  \" Unsupported   verifier   flavor \"     +    flavor )  )  ;", "}", "verifer . read ( xdr )  ;", "return   verifer ;", "}", "METHOD_END"], "methodName": ["readFlavorAndVerifier"], "fileName": "org.apache.hadoop.oncrpc.security.Verifier"}, {"methodBody": ["METHOD_START", "{", "if    ( verifier   instanceof   VerifierNone )     {", "xdr . writeInt ( RpcAuthInfo . AuthFlavor . AUTH _ NONE . getValue (  )  )  ;", "} else", "if    ( verifier   instanceof   VerifierGSS )     {", "xdr . writeInt ( RpcAuthInfo . AuthFlavor . RPCSEC _ GSS . getValue (  )  )  ;", "} else    {", "throw   new   UnsupportedOperationException (  \" Cannot   recognize   the   verifier \"  )  ;", "}", "verifier . write ( xdr )  ;", "}", "METHOD_END"], "methodName": ["writeFlavorAndVerifier"], "fileName": "org.apache.hadoop.oncrpc.security.Verifier"}, {"methodBody": ["METHOD_START", "{", "return   handler ;", "}", "METHOD_END"], "methodName": ["getHandler"], "fileName": "org.apache.hadoop.portmap.Portmap"}, {"methodBody": ["METHOD_START", "{", "return   tcpChannel . getLocalAddress (  )  ;", "}", "METHOD_END"], "methodName": ["getTcpServerLocalAddress"], "fileName": "org.apache.hadoop.portmap.Portmap"}, {"methodBody": ["METHOD_START", "{", "return   udpChannel . getLocalAddress (  )  ;", "}", "METHOD_END"], "methodName": ["getUdpServerLoAddress"], "fileName": "org.apache.hadoop.portmap.Portmap"}, {"methodBody": ["METHOD_START", "{", "StringUtils . startupShutdownMessage ( Portmap . class ,    args ,    Portmap . LOG )  ;", "final   int   port    =    RpcProgram . RPCB _ PORT ;", "Portmap   pm    =    new   Portmap (  )  ;", "try    {", "pm . start ( Portmap . DEFAULT _ IDLE _ TIME _ MILLISECONDS ,    new   InetSocketAddress ( port )  ,    new   InetSocketAddress ( port )  )  ;", "}    catch    ( Throwable   e )     {", "Portmap . LOG . fatal (  \" Failed   to   start   the   server .    Cause :  \"  ,    e )  ;", "pm . shutdown (  )  ;", "System . exit (  (  -  1  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.portmap.Portmap"}, {"methodBody": ["METHOD_START", "{", "allChannels . close (  )  . awaitUninterruptibly (  )  ;", "tcpServer . releaseExternalResources (  )  ;", "udpServer . releaseExternalResources (  )  ;", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.portmap.Portmap"}, {"methodBody": ["METHOD_START", "{", "tcpServer    =    new   ServerBootstrap ( new   NioServerSocketChannelFactory ( Executors . newCachedThreadPool (  )  ,    Executors . newCachedThreadPool (  )  )  )  ;", "tcpServer . setPipelineFactory ( new   ChannelPipelineFactory (  )     {", "private   final   HashedWheelTimer   timer    =    new   HashedWheelTimer (  )  ;", "private   final   IdleStateHandler   idleStateHandler    =    new   IdleStateHandler ( timer ,     0  ,     0  ,    idleTimeMilliSeconds ,    TimeUnit . MILLISECONDS )  ;", "@ Override", "public   ChannelPipeline   getPipeline (  )    throws   Exception    {", "return   Channels . pipeline ( RpcUtil . constructRpcFrameDecoder (  )  ,    RpcUtil . STAGE _ RPC _ MESSAGE _ PARSER ,    idleStateHandler ,    handler ,    RpcUtil . STAGE _ RPC _ TCP _ RESPONSE )  ;", "}", "}  )  ;", "udpServer    =    new   ConnectionlessBootstrap ( new   NioDatagramChannelFactory ( Executors . newCachedThreadPool (  )  )  )  ;", "udpServer . setPipeline ( Channels . pipeline ( RpcUtil . STAGE _ RPC _ MESSAGE _ PARSER ,    handler ,    RpcUtil . STAGE _ RPC _ UDP _ RESPONSE )  )  ;", "tcpChannel    =    tcpServer . bind ( tcpAddress )  ;", "udpChannel    =    udpServer . bind ( udpAddress )  ;", "allChannels . add ( tcpChannel )  ;", "allChannels . add ( udpChannel )  ;", "PLOG . info (  (  (  (  \" Pserver   started   at   tcp :  /  /  \"     +     ( tcpChannel . getLocalAddress (  )  )  )     +     \"  ,    udp :  /  /  \"  )     +     ( udpChannel . getLocalAddress (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.portmap.Portmap"}, {"methodBody": ["METHOD_START", "{", "return   new   PortmapMapping ( xdr . readInt (  )  ,    xdr . readInt (  )  ,    xdr . readInt (  )  ,    xdr . readInt (  )  )  ;", "}", "METHOD_END"], "methodName": ["deserialize"], "fileName": "org.apache.hadoop.portmap.PortmapMapping"}, {"methodBody": ["METHOD_START", "{", "return   port ;", "}", "METHOD_END"], "methodName": ["getPort"], "fileName": "org.apache.hadoop.portmap.PortmapMapping"}, {"methodBody": ["METHOD_START", "{", "return    (  (  (  ( mapping . program )     +     \"     \"  )     +     ( mapping . version )  )     +     \"     \"  )     +     ( mapping . transport )  ;", "}", "METHOD_END"], "methodName": ["key"], "fileName": "org.apache.hadoop.portmap.PortmapMapping"}, {"methodBody": ["METHOD_START", "{", "xdr . writeInt ( program )  ;", "xdr . writeInt ( version )  ;", "xdr . writeInt ( trans )  ;", "xdr . writeInt (  )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.portmap.PortmapMapping"}, {"methodBody": ["METHOD_START", "{", "XDR   request    =    new   XDR (  )  ;", "int   procedure    =     ( set )     ?    RpcProgramPMAPPROC _ SET    :    RpcProgramPMAPPROC _ UNSET ;", "RpcCall   call    =    RpcCall . getInstance ( RpcUtil . getNewXid ( String . valueOf ( RpcProgramPROGRAM )  )  ,    RpcProgramPROGRAM ,    RpcProgramPortmap . VERSION ,    procedure ,    new   CredentialsNone (  )  ,    new   VerifierNone (  )  )  ;", "call . write ( request )  ;", "return   mapping . serialize ( request )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.portmap.PortmapRequest"}, {"methodBody": ["METHOD_START", "{", "return   PortmapMapping . deserialize ( xdr )  ;", "}", "METHOD_END"], "methodName": ["mapping"], "fileName": "org.apache.hadoop.portmap.PortmapRequest"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "xdr . writeBoolean ( value )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["booleanReply"], "fileName": "org.apache.hadoop.portmap.PortmapResponse"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "xdr . writeInt ( value )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["intReply"], "fileName": "org.apache.hadoop.portmap.PortmapResponse"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "for    ( Mapping   mapping    :    list )     {", "xdr . writeBoolean ( true )  ;", "mapping . serialize ( xdr )  ;", "}", "xdr . writeBoolean ( false )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["pmapList"], "fileName": "org.apache.hadoop.portmap.PortmapResponse"}, {"methodBody": ["METHOD_START", "{", "RpcAcceptedReply . getAcceptInstance ( xid ,    new   VerifierNone (  )  )  . write ( xdr )  ;", "return   xdr ;", "}", "METHOD_END"], "methodName": ["voidReply"], "fileName": "org.apache.hadoop.portmap.PortmapResponse"}, {"methodBody": ["METHOD_START", "{", "PortmapMapping [  ]    pmapList    =    map . values (  )  . toArray ( new   PortmapMapping [  0  ]  )  ;", "return   PortmapResponse . pmapList ( out ,    xid ,    pmapList )  ;", "}", "METHOD_END"], "methodName": ["dump"], "fileName": "org.apache.hadoop.portmap.RpcProgramPortmap"}, {"methodBody": ["METHOD_START", "{", "PortmapMapping   mapping    =    PortmapRequest . mapping ( in )  ;", "String   key    =    PortmapMapping . key ( mapping )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Portmap   GETPORT   key =  \"     +    key )     +     \"     \"  )     +    mapping )  )  ;", "}", "PortmapMapping   value    =    map . get ( key )  ;", "int   res    =     0  ;", "if    ( value    !  =    null )     {", "res    =    value . getPort (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Found   mapping   for   key :     \"     +    key )     +     \"    port :  \"  )     +    res )  )  ;", "}", "} else    {", ". LOG . warn (  (  \" Warning ,    no   mapping   for   key :     \"     +    key )  )  ;", "}", "return   PortmapResponse . intReply ( out ,    xid ,    res )  ;", "}", "METHOD_END"], "methodName": ["getport"], "fileName": "org.apache.hadoop.portmap.RpcProgramPortmap"}, {"methodBody": ["METHOD_START", "{", "return   PortmapResponse . voidReply ( out ,    xid )  ;", "}", "METHOD_END"], "methodName": ["nullOp"], "fileName": "org.apache.hadoop.portmap.RpcProgramPortmap"}, {"methodBody": ["METHOD_START", "{", "PortmapMapping   mapping    =    PortmapRequest . mapping ( in )  ;", "String   key    =    PortmapMapping . key ( mapping )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Portmap   set   key =  \"     +    key )  )  ;", "}", "map . put ( key ,    mapping )  ;", "return   PortmapResponse . intReply ( out ,    xid ,    mapping . getPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["set"], "fileName": "org.apache.hadoop.portmap.RpcProgramPortmap"}, {"methodBody": ["METHOD_START", "{", "PortmapMapping   mapping    =    PortmapRequest . mapping ( in )  ;", "String   key    =    PortmapMapping . key ( mapping )  ;", "if    (  . LOG . isDebugEnabled (  )  )", ". LOG . debug (  (  \" Portmap   remove   key =  \"     +    key )  )  ;", "map . remove ( key )  ;", "return   PortmapResponse . booleanReply ( out ,    xid ,    true )  ;", "}", "METHOD_END"], "methodName": ["unset"], "fileName": "org.apache.hadoop.portmap.RpcProgramPortmap"}, {"methodBody": ["METHOD_START", "{", "TestPortmap . pm . start ( TestPortmap . SHORT _ TIMEOUT _ MILLISECONDS ,    new   InetSocketAddress (  \" localhost \"  ,     0  )  ,    new   InetSocketAddress (  \" localhost \"  ,     0  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.portmap.TestPortmap"}, {"methodBody": ["METHOD_START", "{", "TestPortmap . pm . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.portmap.TestPortmap"}, {"methodBody": ["METHOD_START", "{", "Socket   s    =    new   Socket (  )  ;", "try    {", "s . connect (  . pm . getTcpServerLocalAddress (  )  )  ;", "int   i    =     0  ;", "while    (  (  !  ( s . isConnected (  )  )  )     &  &     ( i    <     (  . RETRY _ TIMES )  )  )     {", "+  + i ;", "Thread . sleep (  . SHORT _ TIMEOUT _ MILLISECONDS )  ;", "}", "Assert . assertTrue (  \" Failed   to   connect   to   the   server \"  ,     (  ( s . isConnected (  )  )     &  &     ( i    <     (  . RETRY _ TIMES )  )  )  )  ;", "int   b    =    s . getInputStream (  )  . read (  )  ;", "Assert . assertTrue (  \" The   server   failed   to   disconnect \"  ,     ( b    =  =     (  -  1  )  )  )  ;", "}    finally    {", "s . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testIdle"], "fileName": "org.apache.hadoop.portmap.TestPortmap"}, {"methodBody": ["METHOD_START", "{", "XDR   req    =    new   XDR (  )  ;", "RpcCall . getInstance (  (  +  +  ( xid )  )  ,    RpcProgramPortmap . PROGRAM ,    RpcProgramPortmap . VERSION ,    RpcProgramPortmap . PMAPPROC _ SET ,    new   CredentialsNone (  )  ,    new   VerifierNone (  )  )  . write ( req )  ;", "PortmapMapping   sent    =    new   PortmapMapping (  9  0  0  0  0  ,     1  ,    PortmapMapping . TRANSPORT _ TCP ,     1  2  3  4  )  ;", "sent . serialize ( req )  ;", "byte [  ]    reqBuf    =    req . getBytes (  )  ;", "DatagramSocket   s    =    new   DatagramSocket (  )  ;", "DatagramPacket   p    =    new   DatagramPacket ( reqBuf ,    reqBuf . length ,     . pm . getUdpServerLoAddress (  )  )  ;", "try    {", "s . send ( p )  ;", "}    finally    {", "s . close (  )  ;", "}", "Thread . sleep (  1  0  0  )  ;", "boolean   found    =    false ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    PortmapMapping >    map    =     (  ( Map < String ,    PortmapMapping >  )     ( Whitebox . getInternalState (  . pm . getHandler (  )  ,     \" map \"  )  )  )  ;", "for    ( PortmapMapping   m    :    map . values (  )  )     {", "if    (  (  ( m . getPort (  )  )     =  =     ( sent . getPort (  )  )  )     &  &     ( PortmapMapping . key ( m )  . equals ( PortmapMapping . key ( sent )  )  )  )     {", "found    =    true ;", "break ;", "}", "}", "Assert . assertTrue (  \" Registration   failed \"  ,    found )  ;", "}", "METHOD_END"], "methodName": ["testRegistration"], "fileName": "org.apache.hadoop.portmap.TestPortmap"}]