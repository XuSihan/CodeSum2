[{"methodBody": ["METHOD_START", "{", "this . data . add ( item )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.contrib.utils.join.ArrayListBackedIterator"}, {"methodBody": ["METHOD_START", "{", "this . iter    =    null ;", "this . data    =    null ;", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.contrib.utils.join.ArrayListBackedIterator"}, {"methodBody": ["METHOD_START", "{", "return   this . iter . hasNext (  )  ;", "}", "METHOD_END"], "methodName": ["hasNext"], "fileName": "org.apache.hadoop.contrib.utils.join.ArrayListBackedIterator"}, {"methodBody": ["METHOD_START", "{", "return   this . iter . next (  )  ;", "}", "METHOD_END"], "methodName": ["next"], "fileName": "org.apache.hadoop.contrib.utils.join.ArrayListBackedIterator"}, {"methodBody": ["METHOD_START", "{", "this . iter    =    this . data . iterator (  )  ;", "}", "METHOD_END"], "methodName": ["reset"], "fileName": "org.apache.hadoop.contrib.utils.join.ArrayListBackedIterator"}, {"methodBody": ["METHOD_START", "{", "String   inputDir    =    args [  0  ]  ;", "String   outputDir    =    args [  1  ]  ;", "Class   inputFormat    =    SequenceFileInputFormat . class ;", "if    (  ( args [  2  ]  . compareToIgnoreCase (  \" text \"  )  )     !  =     0  )     {", "System . out . println (  (  \" Using   SequenceFileInputFormat :     \"     +     ( args [  2  ]  )  )  )  ;", "} else    {", "System . out . println (  (  \" Using   TextInputFormat :     \"     +     ( args [  2  ]  )  )  )  ;", "inputFormat    =    TextInputFormat . class ;", "}", "int   numOfReducers    =    Integer . parseInt ( args [  3  ]  )  ;", "Class   mapper    =     . getClassByName ( args [  4  ]  )  ;", "Class   reducer    =     . getClassByName ( args [  5  ]  )  ;", "Class   mapoutputValueClass    =     . getClassByName ( args [  6  ]  )  ;", "Class   outputFormat    =    TextOutputFormat . class ;", "Class   outputValueClass    =    Text . class ;", "if    (  ( args [  7  ]  . compareToIgnoreCase (  \" text \"  )  )     !  =     0  )     {", "System . out . println (  (  \" Using   SequenceFileOutputFormat :     \"     +     ( args [  7  ]  )  )  )  ;", "outputFormat    =    SequenceFileOutputFormat . class ;", "outputValueClass    =     . getClassByName ( args [  7  ]  )  ;", "} else    {", "System . out . println (  (  \" Using   TextOutputFormat :     \"     +     ( args [  7  ]  )  )  )  ;", "}", "long   maxNumOfValuesPerGroup    =     1  0  0  ;", "String   jobName    =     \"  \"  ;", "if    (  ( args . length )     >     8  )     {", "maxNumOfValuesPerGroup    =    Long . parseLong ( args [  8  ]  )  ;", "}", "if    (  ( args . length )     >     9  )     {", "jobName    =    args [  9  ]  ;", "}", "Configuration   defaults    =    new   Configuration (  )  ;", "JobConf   job    =    new   JobConf ( defaults ,     . class )  ;", "job . setJobName (  (  \"  :     \"     +    jobName )  )  ;", "FileSystem   fs    =    FileSystem . get ( defaults )  ;", "fs . delete ( new   Path ( outputDir )  ,    true )  ;", "FileInputFormat . setInputPaths ( job ,    inputDir )  ;", "job . setInputFormat ( inputFormat )  ;", "job . setMapperClass ( mapper )  ;", "FileOutputFormat . setOutputPath ( job ,    new   Path ( outputDir )  )  ;", "job . setOutputFormat ( outputFormat )  ;", "SequenceFileOutputFormat . setOutputCompressionType ( job ,    BLOCK )  ;", "job . setMapOutputKeyClass ( Text . class )  ;", "job . setMapOutputValueClass ( mapoutputValueClass )  ;", "job . setOutputKeyClass ( Text . class )  ;", "job . setOutputValueClass ( outputValueClass )  ;", "job . setReducerClass ( reducer )  ;", "job . setNumMapTasks (  1  )  ;", "job . setNumReduceTasks ( numOfReducers )  ;", "job . setLong (  \" datajoin . maxNumOfValuesPerGroup \"  ,    maxNumOfValuesPerGroup )  ;", "return   job ;", "}", "METHOD_END"], "methodName": ["createDataJoinJob"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinJob"}, {"methodBody": ["METHOD_START", "{", "Class   retv    =    null ;", "try    {", "ClassLoader   classLoader    =    Thread . currentThread (  )  . getCextClassLoader (  )  ;", "retv    =    Class . forName ( className ,    true ,    classLoader )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "return   retv ;", "}", "METHOD_END"], "methodName": ["getClassByName"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinJob"}, {"methodBody": ["METHOD_START", "{", "boolean   success ;", "if    (  (  ( args . length )     <     8  )     |  |     (  ( args . length )     >     1  0  )  )     {", "System . out . println (  (  \" usage :        \"     +     (  (  (  (  (  \" inputdirs   outputdir   map _ input _ file _ format    \"     +     \" numofParts    \"  )     +     \" mapper _ class    \"  )     +     \" reducer _ class    \"  )     +     \" map _ output _ value _ class    \"  )     +     \" output _ value _ class    [ maxNumOfValuesPerGroup    [ descriptionOfJob ]  ]  ]  \"  )  )  )  ;", "System . exit (  (  -  1  )  )  ;", "}", "try    {", "JobConf   job    =     . create ( args )  ;", "success    =     . runJob ( job )  ;", "if    (  ! success )     {", "System . out . println (  \" Job   failed \"  )  ;", "}", "}    catch    ( IOException   ioe )     {", "ioe . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinJob"}, {"methodBody": ["METHOD_START", "{", "JobClient   jc    =    new   JobClient ( job )  ;", "boolean   sucess    =    true ;", "RunningJob   running    =    null ;", "try    {", "running    =    jc . submitJob ( job )  ;", "JobID   jobId    =    running . getID (  )  ;", "System . out . println (  (  (  \" Job    \"     +    jobId )     +     \"    is   submitted \"  )  )  ;", "while    (  !  ( running . isComplete (  )  )  )     {", "System . out . println (  (  (  \" Job    \"     +    jobId )     +     \"    is   still   running .  \"  )  )  ;", "try    {", "Thread . sleep (  6  0  0  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "running    =    jc . getJob ( jobId )  ;", "}", "sucess    =    running . isSuccessful (  )  ;", "}    finally    {", "if    (  (  ! sucess )     &  &     ( running    !  =    null )  )     {", "running . killJob (  )  ;", "}", "jc . close (  )  ;", "}", "return   sucess ;", "}", "METHOD_END"], "methodName": ["runJob"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . reporter )     !  =    null )     {", "this . reporter . setStatus ( su . getReport (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinMapperBase"}, {"methodBody": ["METHOD_START", "{", "super . configure ( job )  ;", "this . job    =    job ;", "this . inputFile    =    job . get ( MAP _ INPUT _ FILE )  ;", "this . inputTag    =    generateInputTag ( this . inputFile )  ;", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinMapperBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . reporter )     =  =    null )     {", "this . reporter    =    reporter ;", "}", "addLongValue (  \" totalCount \"  ,     1  )  ;", "TaggedOutput   aRecord    =    generateTaggedOutput ( value )  ;", "if    ( aRecord    =  =    null )     {", "addLongValue (  \" discardedCount \"  ,     1  )  ;", "return ;", "}", "Text   groupKey    =    generateGroupKey ( aRecord )  ;", "if    ( groupKey    =  =    null )     {", "addLongValue (  \" nullGroupKeyCount \"  ,     1  )  ;", "return ;", "}", "output . collect ( groupKey ,    aRecord )  ;", "addLongValue (  \" collectedCount \"  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["map"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinMapperBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . reporter )     !  =    null )     {", "this . reporter . setStatus ( super . getReport (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "this . collected    +  =     1  ;", "addLongValue (  \" collectedCount \"  ,     1  )  ;", "if    ( aRecord    !  =    null )     {", "output . collect ( key ,    aRecord . get (  )  )  ;", "reporter . setStatus (  (  (  (  \" key :     \"     +     ( key . toString (  )  )  )     +     \"    collected :     \"  )     +     ( collected )  )  )  ;", "addLongValue (  \" actuallyCollectedCount \"  ,     1  )  ;", "}", "}", "METHOD_END"], "methodName": ["collect"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "super . configure ( job )  ;", "this . job    =    job ;", "this . maxNumOfValuesPerGroup    =    job . getLong (  \" datamaxNumOfValuesPerGroup \"  ,     1  0  0  )  ;", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "return   new   ArrayListBackedIterator (  )  ;", "}", "METHOD_END"], "methodName": ["createResetableIterator"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( values . length )     =  =    pos )     {", "TaggedMapOutput   combined    =    combine ( tags ,    partialList )  ;", "collect ( key ,    combined ,    output ,    reporter )  ;", "return ;", "}", "ResetableIterator   nextValues    =    values [ pos ]  ;", "nextValues . reset (  )  ;", "while    ( nextValues . hasNext (  )  )     {", "Object   v    =    nextValues . next (  )  ;", "partialList [ pos ]     =    v ;", "AndCollect ( tags ,    values ,     ( pos    +     1  )  ,    partialList ,    key ,    output ,    reporter )  ;", "}", "}", "METHOD_END"], "methodName": ["joinAndCollect"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( values . length )     <     1  )     {", "return ;", "}", "Object [  ]    partialList    =    new   Object [ values . length ]  ;", "AndCollect ( tags ,    values ,     0  ,    partialList ,    key ,    output ,    reporter )  ;", "}", "METHOD_END"], "methodName": ["joinAndCollect"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . reporter )     =  =    null )     {", "this . reporter    =    reporter ;", "}", "SortedMap < Object ,    ResetableIterator >    groups    =    regroup ( key ,    values ,    reporter )  ;", "Object [  ]    tags    =    groups . keySet (  )  . toArray (  )  ;", "ResetableIterator [  ]    groupValues    =    new   ResetableIterator [ tags . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( tags . length )  ;    i +  +  )     {", "groupValues [ i ]     =    groups . get ( tags [ i ]  )  ;", "}", "AndCollect ( tags ,    groupValues ,    key ,    output ,    reporter )  ;", "addLongValue (  \" groupCount \"  ,     1  )  ;", "for    ( int   i    =     0  ;    i    <     ( tags . length )  ;    i +  +  )     {", "groupValues [ i ]  . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["reduce"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "this . numOfValues    =     0  ;", "SortedMap < Object ,    ResetableIterator >    retv    =    new   TreeMap < Object ,    ResetableIterator >  (  )  ;", "TaggedMapOutput   aRecord    =    null ;", "while    ( arg 1  . hasNext (  )  )     {", "this . numOfValues    +  =     1  ;", "if    (  (  ( this . numOfValues )     %     1  0  0  )     =  =     0  )     {", "reporter . setStatus (  (  (  (  \" key :     \"     +     ( key . toString (  )  )  )     +     \"    numOfValues :     \"  )     +     ( this . numOfValues )  )  )  ;", "}", "if    (  ( this . numOfValues )     >     ( this . maxNumOfValuesPerGroup )  )     {", "inue ;", "}", "aRecord    =     (  ( TaggedMapOutput )     ( arg 1  . next (  )  )  )  . clone ( job )  ;", "Text   tag    =    aRecord . getTag (  )  ;", "ResetableIterator   data    =    retv . get ( tag )  ;", "if    ( data    =  =    null )     {", "data    =    createResetableIterator (  )  ;", "retv . put ( tag ,    data )  ;", "}", "data . add ( aRecord )  ;", "}", "if    (  ( this . numOfValues )     >     ( this . largestNumOfValues )  )     {", "this . largestNumOfValues    =    numOfValues ;", "JobBase . LOG . info (  (  (  (  \" key :     \"     +     ( key . toString (  )  )  )     +     \"    this . largestNumOfValues :     \"  )     +     ( this . largestNumOfValues )  )  )  ;", "}", "return   retv ;", "}", "METHOD_END"], "methodName": ["regroup"], "fileName": "org.apache.hadoop.contrib.utils.join.DataJoinReducerBase"}, {"methodBody": ["METHOD_START", "{", "Double   val    =    this . doubleCounters . get ( name )  ;", "Double   retv    =    null ;", "if    ( val    =  =    null )     {", "retv    =    new   Double ( inc )  ;", "} else    {", "retv    =    new   Double (  (  ( val . doubleValue (  )  )     +    inc )  )  ;", "}", "this . doubleCounters . put ( name ,    retv )  ;", "return   retv ;", "}", "METHOD_END"], "methodName": ["addDoubleValue"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "Long   val    =    this . longCounters . get ( name )  ;", "Long   retv    =    null ;", "if    ( val    =  =    null )     {", "retv    =    Long . valueOf ( inc )  ;", "} else    {", "retv    =    Long . valueOf (  (  ( val . longValue (  )  )     +    inc )  )  ;", "}", "this . longCounters . put ( name ,    retv )  ;", "return   retv ;", "}", "METHOD_END"], "methodName": ["addLongValue"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "this . longCounters    =    new   TreeMap < Object ,    Long >  (  )  ;", "this . doubleCounters    =    new   TreeMap < Object ,    Double >  (  )  ;", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "return   this . doubleCounters . get ( name )  ;", "}", "METHOD_END"], "methodName": ["getDoubleValue"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "return   this . longCounters . get ( name )  ;", "}", "METHOD_END"], "methodName": ["getLongValue"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "Iterator   iter    =    this . longCounters . entrySet (  )  . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry   e    =     (  ( Map . Entry )     ( iter . next (  )  )  )  ;", "sb . append ( e . getKey (  )  . toString (  )  )  . append (  \"  \\ t \"  )  . append ( e . getValue (  )  )  . append (  \"  \\ n \"  )  ;", "}", "iter    =    this . doubleCounters . entrySet (  )  . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry   e    =     (  ( Map . Entry )     ( iter . next (  )  )  )  ;", "sb . append ( e . getKey (  )  . toString (  )  )  . append (  \"  \\ t \"  )  . append ( e . getValue (  )  )  . append (  \"  \\ n \"  )  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getReport"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "JobBase . LOG . info ( getReport (  )  )  ;", "}", "METHOD_END"], "methodName": ["report"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "this . doubleCounters . put ( name ,    new   Double ( value )  )  ;", "}", "METHOD_END"], "methodName": ["setDoubleValue"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "this . longCounters . put ( name ,    Long . valueOf ( value )  )  ;", "}", "METHOD_END"], "methodName": ["setLongValue"], "fileName": "org.apache.hadoop.contrib.utils.join.JobBase"}, {"methodBody": ["METHOD_START", "{", "String   line    =     (  ( Text )     ( aRecord . getData (  )  )  )  . toString (  )  ;", "String   groupKey    =     \"  \"  ;", "String [  ]    tokens    =    line . split (  \"  \\  \\ t \"  ,     2  )  ;", "groupKey    =    tokens [  0  ]  ;", "return   new   Text ( groupKey )  ;", "}", "METHOD_END"], "methodName": ["generateGroupKey"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleDataJoinMapper"}, {"methodBody": ["METHOD_START", "{", "return   new   Text ( inputFile )  ;", "}", "METHOD_END"], "methodName": ["generateInputTag"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleDataJoinMapper"}, {"methodBody": ["METHOD_START", "{", "TaggedMapOutput   retv    =    new   SampleTaggedMapOutput (  (  ( Text )     ( value )  )  )  ;", "retv . setTag ( new   Text ( this . inputTag )  )  ;", "return   retv ;", "}", "METHOD_END"], "methodName": ["generateTaggedMapOutput"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleDataJoinMapper"}, {"methodBody": ["METHOD_START", "{", "if    (  ( tags . length )     <     2  )", "return   null ;", "String   joinedStr    =     \"  \"  ;", "for    ( int   i    =     0  ;    i    <     ( tags . length )  ;    i +  +  )     {", "if    ( i    >     0  )", "joinedStr    +  =     \"  \\ t \"  ;", "String   line    =     (  ( Text )     (  (  ( TaggedMapOutput )     ( values [ i ]  )  )  . getData (  )  )  )  . toString (  )  ;", "String [  ]    tokens    =    line . split (  \"  \\  \\ t \"  ,     2  )  ;", "joinedStr    +  =    tokens [  1  ]  ;", "}", "TaggedMapOutput   retv    =    new   TaggedMapOutput ( new   Text ( joinedStr )  )  ;", "retv . setTag (  (  ( Text )     ( tags [  0  ]  )  )  )  ;", "return   retv ;", "}", "METHOD_END"], "methodName": ["combine"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleDataJoinReducer"}, {"methodBody": ["METHOD_START", "{", "return   data ;", "}", "METHOD_END"], "methodName": ["getData"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput"}, {"methodBody": ["METHOD_START", "{", "this . tag . readFields ( in )  ;", "this . data . readFields ( in )  ;", "}", "METHOD_END"], "methodName": ["readFields"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput"}, {"methodBody": ["METHOD_START", "{", "this . tag . write ( out )  ;", "this . data . write ( out )  ;", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.contrib.utils.join.SampleTaggedMapOutput"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TaggedMapOutput )     ( WritableUtils . clone ( this ,    job )  )  )  ;", "}", "METHOD_END"], "methodName": ["clone"], "fileName": "org.apache.hadoop.contrib.utils.join.TaggedMapOutput"}, {"methodBody": ["METHOD_START", "{", "return   tag ;", "}", "METHOD_END"], "methodName": ["getTag"], "fileName": "org.apache.hadoop.contrib.utils.join.TaggedMapOutput"}, {"methodBody": ["METHOD_START", "{", "this . tag    =    tag ;", "}", "METHOD_END"], "methodName": ["setTag"], "fileName": "org.apache.hadoop.contrib.utils.join.TaggedMapOutput"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    out . getFileSystem ( job )  ;", "FileStatus [  ]    outlist    =    fs . listStatus ( out )  ;", "assertEquals (  1  ,    outlist . length )  ;", "assertTrue (  (  0     <     ( outlist [  0  ]  . getLen (  )  )  )  )  ;", "FSInputStream   in    =    fs . open ( outlist [  0  ]  . getPath (  )  )  ;", "LineRecordReader   rr    =    new   LineRecordReader ( in ,     0  ,    Integer . MAX _ VALUE ,    job )  ;", "LongWritable   k    =    new   LongWritable (  )  ;", "Text   v    =    new   Text (  )  ;", "int   count    =     0  ;", "while    ( rr . next ( k ,    v )  )     {", "String [  ]    vals    =    v . toString (  )  . split (  \"  \\ t \"  )  ;", "assertEquals (  ( srcs    +     1  )  ,    vals . length )  ;", "int [  ]    ivals    =    new   int [ vals . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( vals . length )  ;     +  + i )", "ivals [ i ]     =    Integer . parseInt ( vals [ i ]  )  ;", "assertEquals (  0  ,     (  ( ivals [  0  ]  )     %     ( srcs    *    srcs )  )  )  ;", "for    ( int   i    =     1  ;    i    <     ( vals . length )  ;     +  + i )     {", "assertEquals (  (  (  ( ivals [ i ]  )     -     ( i    -     1  )  )     *    srcs )  ,     (  1  0     *     ( ivals [  0  ]  )  )  )  ;", "}", "+  + count ;", "}", "assertEquals (  4  ,    count )  ;", "}", "METHOD_END"], "methodName": ["confirmOutput"], "fileName": "org.apache.hadoop.contrib.utils.join.TestDataJoin"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <    srcs ;     +  + i )     {", "src [ i ]     =    new   Path ( testdir ,    Integer . toSng (  ( i    +     1  0  )  ,     3  6  )  )  ;", "}", "SequenceFile [  ]    out    =    new   SequenceFile . Writer [ srcs ]  ;", "for    ( int   i    =     0  ;    i    <    srcs ;     +  + i )     {", "out [ i ]     =    new   SequenceFile . Writer ( testdir . getFileSystem ( conf )  ,    conf ,    src [ i ]  ,    Text . class ,    Text . class )  ;", "}", "return   out ;", "}", "METHOD_END"], "methodName": ["createWriters"], "fileName": "org.apache.hadoop.contrib.utils.join.TestDataJoin"}, {"methodBody": ["METHOD_START", "{", "TestSetup   setup    =    new   TestSetup ( new   TestSuite ( TestDataJoin . class )  )     {", "protected   void   setUp (  )    throws   Exception    {", "Configuration   conf    =    new   Configuration (  )  ;", "TestDataJoin . cluster    =    new   hdfs . MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "}", "protected   void   tearDown (  )    throws   Exception    {", "if    (  ( TestDataJoin . cluster )     !  =    null )     {", "TestDataJoin . cluster . shutdown (  )  ;", "}", "}", "}  ;", "return   setup ;", "}", "METHOD_END"], "methodName": ["suite"], "fileName": "org.apache.hadoop.contrib.utils.join.TestDataJoin"}, {"methodBody": ["METHOD_START", "{", "final   int   srcs    =     4  ;", "JobConf   job    =    new   JobConf (  )  ;", "job . setBoolean (  \" mapreduce . fileoutputcommitter . marksuccessfuljobs \"  ,    false )  ;", "Path   base    =     . cluster . getFileSystem (  )  . makeQualified ( new   Path (  \"  / inner \"  )  )  ;", "Path [  ]    src    =     . writeSimpleSrc ( base ,    job ,    srcs )  ;", "job . setInputFormat ( SequenceFileInputFormat . class )  ;", "Path   outdir    =    new   Path ( base ,     \" out \"  )  ;", "FileOutputFormat . setOutputPath ( job ,    outdir )  ;", "job . setMapperClass ( SampleDataJoinMapper . class )  ;", "job . setReducerClass ( SampleDataJoinReducer . class )  ;", "job . setMapOutputKeyClass ( Text . class )  ;", "job . setMapOutputValueClass ( SampleTaggedMapOutput . class )  ;", "job . setOutputKeyClass ( Text . class )  ;", "job . setOutputValueClass ( Text . class )  ;", "job . setOutputFormat ( TextOutputFormat . class )  ;", "job . setNumMapTasks (  1  )  ;", "job . setNumReduceTasks (  1  )  ;", "FileInputFormat . setInputPaths ( job ,    src )  ;", "try    {", "JobClient . runJob ( job )  ;", ". confirmOutput ( outdir ,    job ,    srcs )  ;", "}    finally    {", "base . getFileSystem ( job )  . delete ( base ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testDataJoin"], "fileName": "org.apache.hadoop.contrib.utils.join.TestDataJoin"}, {"methodBody": ["METHOD_START", "{", "SequenceFile [  ]    out    =    null ;", "Path [  ]    src    =    new   Path [ srcs ]  ;", "try    {", "out    =     . createWriters ( testdir ,    conf ,    srcs ,    src )  ;", "final   int   capacity    =     ( srcs    *     2  )     +     1  ;", "Text   key    =    new   Text (  )  ;", "key . set (  \" ignored \"  )  ;", "Text   val    =    new   Text (  )  ;", "for    ( int   k    =     0  ;    k    <    capacity ;     +  + k )     {", "for    ( int   i    =     0  ;    i    <    srcs ;     +  + i )     {", "val . set (  (  (  ( Integer . toString (  (  ( k    %    srcs )     =  =     0     ?    k    *    srcs    :     ( k    *    srcs )     +    i )  )  )     +     \"  \\ t \"  )     +     ( Integer . toString (  (  (  1  0     *    k )     +    i )  )  )  )  )  ;", "out [ i ]  . append ( key ,    val )  ;", "if    ( i    =  =    k )     {", "out [ i ]  . append ( key ,    val )  ;", "}", "}", "}", "}    finally    {", "if    ( out    !  =    null )     {", "for    ( int   i    =     0  ;    i    <    srcs ;     +  + i )     {", "if    (  ( out [ i ]  )     !  =    null )", "out [ i ]  . close (  )  ;", "}", "}", "}", "return   src ;", "}", "METHOD_END"], "methodName": ["writeSimpleSrc"], "fileName": "org.apache.hadoop.contrib.utils.join.TestDataJoin"}, {"methodBody": ["METHOD_START", "{", "return   getBlobReference ( blobKey )  . acquireLease (  6  0  ,    null )  ;", "}", "METHOD_END"], "methodName": ["acquireShortLease"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount . allMetrics . add ( record )  ;", "}", "METHOD_END"], "methodName": ["addRecord"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "conf . set (  \" fs . wasb . impl \"  ,     \" NativeAzureFileSystem \"  )  ;", "conf . set (  \" fs . wasbs . impl \"  ,     \" NativeAzureFileSystem \"  )  ;", "}", "METHOD_END"], "methodName": ["addWasbToConfiguration"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "if    (  (     !  =    null )     {", "close (  )  ;", "=    null ;", "}", "if    (  ( container )     !  =    null )     {", "container . deleteIfExists (  )  ;", "container    =    null ;", "}", "if    (  ( blob )     !  =    null )     {", "blob . delete (  )  ;", "blob    =    null ;", "}", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "if    (  (     !  =    null )     {", "close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["closeFileSystem"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . create (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . create ( containerNameSuffix ,    EnumSet . of ( AzureBlobStorageTestAccount . CreateOptions . CreateContainer )  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . create ( containerNameSuffix ,    createOptions ,    null )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem   fs    =    null ;", "CloudBlobContainer   container    =    null ;", "Configuration   conf    =     . createTestConfiguration ( initialConfiguration )  ;", "CloudStorageAccount   account    =     . createTestAccount ( conf )  ;", "if    ( account    =  =    null )     {", "return   null ;", "}", "fs    =    new   NativeAzureFileSystem (  )  ;", "String   containerName    =    String . format (  \" wasbtests -  % s -  % tQ % s \"  ,    System . getProperty (  \" user . name \"  )  ,    new   Date (  )  ,    containerNameSuffix )  ;", "container    =    account . createCloudBlobClient (  )  . getContainerReference ( containerName )  ;", "if    ( createOptions . contains (  . CreateOptions . CreateContainer )  )     {", "container . create (  )  ;", "}", "String   accountName    =    conf . get (  . TEST _ ACCOUNT _ NAME _ PROPERTY _ NAME )  ;", "if    ( createOptions . contains (  . CreateOptions . UseSas )  )     {", "String   sas    =     . generateSAS ( container ,    createOptions . contains (  . CreateOptions . Readonly )  )  ;", "if    (  !  ( createOptions . contains (  . CreateOptions . CreateContainer )  )  )     {", "container . delete (  )  ;", "}", "conf . set (  (  (  . ACCOUNT _ KEY _ PROPERTY _ NAME )     +    accountName )  ,     \"  \"  )  ;", "conf . set (  (  (  (  (  . SAS _ PROPERTY _ NAME )     +    containerName )     +     \"  .  \"  )     +    accountName )  ,    sas )  ;", "}", "if    ( createOptions . contains (  . CreateOptions . useThrottling )  )     {", "conf . setBoolean (  . KEY _ DISABLE _ THROTTLING ,    false )  ;", "} else    {", "conf . setBoolean (  . KEY _ DISABLE _ THROTTLING ,    true )  ;", "}", "URI   accountUri    =     . createAccountUri ( accountName ,    containerName )  ;", "fs . initialize ( accountUri ,    conf )  ;", "testAcct    =    new    ( fs ,    account ,    container )  ;", "return   testAcct ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . of ( AzureBlobStorageTestAccount . CreateOptions . CreateContainer )  ,    conf )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   new   URI (  (  (  (  (  ( AzureBlobStorageTestAccount . WASB _ SCHEME )     +     \"  :  \"  )     +     ( AzureBlobStorageTestAccount . PATH _ DELIMITER )  )     +     ( AzureBlobStorageTestAccount . PATH _ DELIMITER )  )     +    accountName )  )  ;", "}", "METHOD_END"], "methodName": ["createAccountUri"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   new   URI (  (  (  (  (  (  (  ( AzureBlobStorageTestAccount . WASB _ SCHEME )     +     \"  :  \"  )     +     ( AzureBlobStorageTestAccount . PATH _ DELIMITER )  )     +     ( AzureBlobStorageTestAccount . PATH _ DELIMITER )  )     +    containerName )     +     ( AzureBlobStorageTestAccount . WASB _ AUTHORITY _ DELIMITER )  )     +    accountName )  )  ;", "}", "METHOD_END"], "methodName": ["createAccountUri"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem   fs    =    null ;", "CloudBlobContainer   container    =    null ;", "Configuration   conf    =     . createTestConfiguration (  )  ;", "Configuration   noTestAccountConf    =    new   Configuration (  )  ;", "CloudStorageAccount   account    =     . createTestAccount ( conf )  ;", "if    ( account    =  =    null )     {", "return   null ;", "}", "CloudBlobClient   blobClient    =    account . createCloudBlobClient (  )  ;", "String   accountName    =    conf . get (  . TEST _ ACCOUNT _ NAME _ PROPERTY _ NAME )  ;", "String   containerName    =     . generateContainerName (  )  ;", ". primePublicContainer ( blobClient ,    accountName ,    containerName ,    blobName ,    fileSize )  ;", "container    =    blobClient . getContainerReference ( containerName )  ;", "if    (  ( null    =  =    container )     |  |     (  !  ( container . exists (  )  )  )  )     {", "final   String   errMsg    =    String . format (  \" Container    '  % s '    expected   but   not   found   while   creating   SAS   account .  \"  )  ;", "throw   new   Exception ( errMsg )  ;", "}", "URI   accountUri    =     . createAccountUri ( accountName ,    containerName )  ;", "fs    =    new   NativeAzureFileSystem (  )  ;", "fs . initialize ( accountUri ,    noTestAccountConf )  ;", "testAcct    =    new    ( fs ,    account ,    container )  ;", "return   testAcct ;", "}", "METHOD_END"], "methodName": ["createAnonymous"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem   fs    =    null ;", "CloudBlobContainer   container    =    null ;", "Configuration   conf    =     . createTestConfiguration (  )  ;", "if    (  !  ( conf . getBoolean (  . USE _ EMULATOR _ PROPERTY _ NAME ,    false )  )  )     {", "System . out . println (  (  \" Skipping   emulator   Azure   test   because   configuration    \"     +     (  \" doesn ' t   indicate   that   it ' s   running .  \"     +     \"    Please   see   README . txt   for   guidance .  \"  )  )  )  ;", "return   null ;", "}", "CloudStorageAccount   account    =    CloudStorageAccount . getDevelopmentStorageAccount (  )  ;", "fs    =    new   NativeAzureFileSystem (  )  ;", "String   containerName    =    String . format (  \" wasbtests -  % s -  % tQ \"  ,    System . getProperty (  \" user . name \"  )  ,    new   Date (  )  )  ;", "container    =    account . createCloudBlobClient (  )  . getContainerReference ( containerName )  ;", "container . create (  )  ;", "URI   accountUri    =     . createAccountUri ( AzureNativeFileSystemStore . DEFAULT _ STORAGE _ EMULATOR _ ACCOUNT _ NAME ,    containerName )  ;", "fs . initialize ( accountUri ,    conf )  ;", "testAcct    =    new    ( fs ,    account ,    container )  ;", "return   testAcct ;", "}", "METHOD_END"], "methodName": ["createForEmulator"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . createMock ( new   Configuration (  )  )  ;", "}", "METHOD_END"], "methodName": ["createMock"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "AzureNativeFileSystemStore   store    =    new   AzureNativeFileSystemStore (  )  ;", "MockStorageInterface   mockStorage    =    new   MockStorageInterface (  )  ;", "store . setAzureStorageInteractionLayer ( mockStorage )  ;", "NativeAzureFileSystem   fs    =    new   NativeAzureFileSystem ( store )  ;", ". addWasbToConfiguration ( conf )  ;", ". setMockAccountKey ( conf )  ;", "fs . initialize ( new   URI (  . MOCK _ WASB _ URI )  ,    conf )  ;", "testAcct    =    new    ( fs ,    mockStorage )  ;", "return   testAcct ;", "}", "METHOD_END"], "methodName": ["createMock"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "CloudBlobContainer   container    =    null ;", "Configuration   conf    =     . createTestConfiguration (  )  ;", "CloudStorageAccount   account    =     . createTestAccount ( conf )  ;", "if    ( null    =  =    account )     {", "return   null ;", "}", "String   containerName    =    String . format (  \" wasbtests -  % s -  % tQ \"  ,    System . getProperty (  \" user . name \"  )  ,    new   Date (  )  )  ;", "container    =    account . createCloudBlobClient (  )  . getContainerReference ( containerName )  ;", "container . create (  )  ;", "String   accountName    =    conf . get (  . TEST _ ACCOUNT _ NAME _ PROPERTY _ NAME )  ;", "conf . setBoolean (  . KEY _ DISABLE _ THROTTLING ,    true )  ;", "conf . setBoolean (  . KEY _ READ _ TOLERATE _ CONCURRENT _ APPEND ,    true )  ;", "URI   accountUri    =     . createAccountUri ( accountName ,    containerName )  ;", "AzureFileSystemMetricsSystem . fileSystemStarted (  )  ;", "String   sourceName    =    NativeAzureFileSystem . newMetricsSourceName (  )  ;", "String   sourceDesc    =     \" Azure   Storage   Volume   File   System   metrics \"  ;", "AzureFileSystemInstrumentation   instrumentation    =    new   AzureFileSystemInstrumentation ( conf )  ;", "AzureFileSystemMetricsSystem . registerSource ( sourceName ,    sourceDesc ,    instrumentation )  ;", "AzureNativeFileSystemStore   testStorage    =    new   AzureNativeFileSystemStore (  )  ;", "testStorage . initialize ( accountUri ,    conf ,    instrumentation )  ;", "testAcct    =    new    ( testStorage ,    account ,    container )  ;", "return   testAcct ;", "}", "METHOD_END"], "methodName": ["createOutOfBandStore"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem   fs    =    null ;", "CloudBlobContainer   container    =    null ;", "Configuration   conf    =     . createTestConfiguration (  )  ;", "CloudStorageAccount   account    =     . createTestAccount ( conf )  ;", "if    ( account    =  =    null )     {", "return   null ;", "}", "CloudBlobClient   blobClient    =    account . createCloudBlobClient (  )  ;", "String   accountName    =    conf . get (  . TEST _ ACCOUNT _ NAME _ PROPERTY _ NAME )  ;", "CloudBlockBlob   blobRoot    =     . primeRootContainer ( blobClient ,    accountName ,    blobName ,    fileSize )  ;", "container    =    blobClient . getContainerReference (  . AZURE _ ROOT _ CONTAINER )  ;", "if    (  ( null    =  =    container )     |  |     (  !  ( container . exists (  )  )  )  )     {", "final   String   errMsg    =    String . format (  \" Container    '  % s '    expected   but   not   found   while   creating   SAS   account .  \"  )  ;", "throw   new   Exception ( errMsg )  ;", "}", "URI   accountUri    =     . createAccountUri ( accountName )  ;", "fs    =    new   NativeAzureFileSystem (  )  ;", "fs . initialize ( accountUri ,    conf )  ;", "testAcct    =    new    ( fs ,    account ,    blobRoot )  ;", "return   testAcct ;", "}", "METHOD_END"], "methodName": ["createRoot"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "String   accountKey    =    AzureNativeFileSystemStore . getAccountKeyFromConfiguration ( accountName ,    conf )  ;", "StorageCredentials   credentials ;", "if    (  ( accountKey    =  =    null )     &  &    allowAnonymous )     {", "credentials    =    StorageCredentialsAnonymous . ANONYMOUS ;", "} else    {", "credentials    =    new   StorageCredentialsAccountAndKey ( accountName . split (  \"  \\  \\  .  \"  )  [  0  ]  ,    accountKey )  ;", "}", "if    ( credentials    =  =    null )     {", "return   null ;", "} else    {", "return   new   com . microsoft . windowsazure . storage . CloudStorageAccount ( credentials )  ;", "}", "}", "METHOD_END"], "methodName": ["createStorageAccount"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . createTestAccount ( AzureBlobStorageTestAccount . createTestConfiguration (  )  )  ;", "}", "METHOD_END"], "methodName": ["createTestAccount"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "String   testAccountName    =    conf . get ( AzureBlobStorageTestAccount . TEST _ ACCOUNT _ NAME _ PROPERTY _ NAME )  ;", "if    ( testAccountName    =  =    null )     {", "System . out . println (  (  \" Skipping   live   Azure   test   because   of   missing   test   account .  \"     +     \"    Please   see   README . txt   for   guidance .  \"  )  )  ;", "return   null ;", "}", "return   AzureBlobStorageTestAccount . createStorageAccount ( testAccountName ,    conf ,    false )  ;", "}", "METHOD_END"], "methodName": ["createTestAccount"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . createTestConfiguration ( null )  ;", "}", "METHOD_END"], "methodName": ["createTestConfiguration"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "if    ( conf    =  =    null )     {", "conf    =    new   Configuration (  )  ;", "}", "conf . addResource (  . TEST _ CONFIGURATION _ FILE _ NAME )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createTestConfiguration"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . of ( AzureBlobStorageTestAccount . CreateOptions . useThrottling ,    AzureBlobStorageTestAccount . CreateOptions . CreateContainer )  )  ;", "}", "METHOD_END"], "methodName": ["createThrottled"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "String   containerName    =    String . format (  \" wasbtests -  % s -  % tQ \"  ,    System . getProperty (  \" user . name \"  )  ,    new   Date (  )  )  ;", "return   containerName ;", "}", "METHOD_END"], "methodName": ["generateContainerName"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "container . createIfNotExists (  )  ;", "SharedAccessPolicy   sasPolicy    =    new   SharedAccessPolicy (  )  ;", "GregorianCalendar   calendar    =    new   GregorianCalendar ( TimeZone . getTimeZone (  \" UTC \"  )  )  ;", "calendar . setTime ( new   Date (  )  )  ;", "sasPolicy . setSharedAccessStartTime ( calendar . getTime (  )  )  ;", "calendar . add ( Calendar . HOUR ,     1  0  )  ;", "sasPolicy . setSharedAccessExpiryTime ( calendar . getTime (  )  )  ;", "if    ( readonly )     {", "sasPolicy . setPermissions ( EnumSet . of ( READ ,    LIST )  )  ;", "} else    {", "sasPolicy . setPermissions ( EnumSet . of ( READ ,    WRITE ,    LIST )  )  ;", "}", "ContainerPermissions   containerPermissions    =    new   ContainerPermissions (  )  ;", "containerPermissions . setPublicAccess ( OFF )  ;", "container . uploadPermissions ( containerPermissions )  ;", "String   sas    =    container . generateSharedAccessSignature ( sasPolicy ,    null )  ;", "Thread . sleep (  1  5  0  0  )  ;", "return   sas ;", "}", "METHOD_END"], "methodName": ["generateSAS"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   container . getBlockBlobReference ( String . format ( blobKey )  )  ;", "}", "METHOD_END"], "methodName": ["getBlobReference"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   fs ;", "}", "METHOD_END"], "methodName": ["getFileSystem"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "boolean   found    =    false ;", "Number   ret    =    null ;", "for    ( MetricsRecord   currentRecord    :     . allMetrics )     {", "if    ( wasGeneratedByMe ( currentRecord )  )     {", "for    ( AbstractMetric   currentMetric    :    currentRecord . metrics (  )  )     {", "if    ( currentMetric . name (  )  . equalsIgnoreCase ( metricName )  )     {", "found    =    true ;", "ret    =    currentMetric . value (  )  ;", "break ;", "}", "}", "}", "}", "if    (  ! found )     {", "if    ( defaultValue    !  =    null )     {", "return   defaultValue ;", "}", "throw   new   IndexOutOfBoundsException ( metricName )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getLatestMetricValue"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" http :  /  /  % s /  % s \"  ,    AzureBlobStorageTestAccount . MOCK _ ACCOUNT _ NAME ,    AzureBlobStorageTestAccount . MOCK _ CONTAINER _ NAME )  ;", "}", "METHOD_END"], "methodName": ["getMockContainerUri"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   mockStorage ;", "}", "METHOD_END"], "methodName": ["getMockStorage"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   account ;", "}", "METHOD_END"], "methodName": ["getRealAccount"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getRealContainer"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   this . storage ;", "}", "METHOD_END"], "methodName": ["getStore"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "CloudBlobContainer   container    =    blobClient . getContainerReference ( containerName )  ;", "container . createIfNotExists (  )  ;", "SharedAccessBlobPolicy   sasPolicy    =    new   SharedAccessBlobPolicy (  )  ;", "sasPolicy . setPermissions ( EnumSet . of ( READ ,    WRITE ,    LIST ,    DELETE )  )  ;", "BlobContainerPermissions   containerPermissions    =    new   BlobContainerPermissions (  )  ;", "containerPermissions . setPublicAccess ( CONTAINER )  ;", "containerPermissions . getSharedAccessPolicies (  )  . put (  \" testwasbpolicy \"  ,    sasPolicy )  ;", "container . uploadPermissions ( containerPermissions )  ;", "CloudBlockBlob   blob    =    container . getBlockBlobReference ( blobName )  ;", "BlobOutputStream   outputStream    =    blob . openOutputStream (  )  ;", "outputStream . write ( new   byte [ fileSize ]  )  ;", "outputStream . close (  )  ;", "}", "METHOD_END"], "methodName": ["primePublicContainer"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "CloudBlobContainer   container    =    blobClient . getContainerReference (  (  (  (  \" https :  /  /  \"     +    accountName )     +     \"  /  \"  )     +     \"  $ root \"  )  )  ;", "container . createIfNotExists (  )  ;", "CloudBlockBlob   blob    =    container . getBlockBlobReference ( blobName )  ;", "BlobOutputStream   outputStream    =    blob . openOutputStream (  )  ;", "outputStream . write ( new   byte [ fileSize ]  )  ;", "outputStream . close (  )  ;", "return   blob ;", "}", "METHOD_END"], "methodName": ["primeRootContainer"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "AccessCondition   accessCondition    =    new   AccessCondition (  )  ;", "accessCondition . setLeaseID ( leaseID )  ;", "getReference ( blobKey )  . releaseLease ( accessCondition )  ;", "}", "METHOD_END"], "methodName": ["releaseLease"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount . setMockAccountKey ( conf ,    AzureBlobStorageTestAccount . MOCK _ ACCOUNT _ NAME )  ;", "}", "METHOD_END"], "methodName": ["setMockAccountKey"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "conf . set (  (  ( AzureBlobStorageTestAccount . ACCOUNT _ KEY _ PROPERTY _ NAME )     +    accountName )  ,    Base 6  4  . encode ( new   byte [  ]  {     1  ,     2  ,     3     }  )  )  ;", "}", "METHOD_END"], "methodName": ["setMockAccountKey"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" http :  /  /  % s /  % s /  % s \"  ,    AzureBlobStorageTestAccount . MOCK _ ACCOUNT _ NAME ,    AzureBlobStorageTestAccount . MOCK _ CONTAINER _ NAME ,    path )  ;", "}", "METHOD_END"], "methodName": ["toMockUri"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "return   AzureBlobStorageTestAccount . toMockUri ( path . toUri (  )  . getRawPath (  )  . substring (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["toMockUri"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "String   myFsId    =    fs . getInstrumentation (  )  . getFileSystemInstanceId (  )  . toString (  )  ;", "for    ( MetricsTag   currentTag    :    currentRecord . tags (  )  )     {", "if    ( currentTag . name (  )  . equalsIgnoreCase (  \" wasbFileSystemId \"  )  )     {", "return   currentTag . value (  )  . equals ( myFsId )  ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["wasGeneratedByMe"], "fileName": "org.apache.hadoop.fs.azure.AzureBlobStorageTestAccount"}, {"methodBody": ["METHOD_START", "{", "this . testHookOperationContext    =    testHook ;", "}", "METHOD_END"], "methodName": ["addTestHookToOperationContext"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "LinkedList < Iterator < ListBlobItem >  >    dirIteratorStack    =    new   LinkedList < Iterator < ListBlobItem >  >  (  )  ;", "Iterable < ListBlobItem >    blobItems    =    aCloudBlobDirectory . listBlobs ( null ,    false ,    EnumSet . of ( METADATA )  ,    null ,    getInstrumentedContext (  )  )  ;", "Iterator < ListBlobItem >    blobItemIterator    =    blobItems . iterator (  )  ;", "if    (  (  0     =  =    maxListingDepth )     |  |     (  0     =  =    maxListingCount )  )     {", "return ;", "}", "final   boolean   isUnboundedDepth    =    maxListingDepth    <     0  ;", "int   listingDepth    =     1  ;", "while    (  ( null    !  =    blobItemIterator )     &  &     (  ( maxListingCount    <  =     0  )     |  |     (  ( aFileMetadataList . size (  )  )     <    maxListingCount )  )  )     {", "while    ( blobItemIterator . hasNext (  )  )     {", "if    (  (  0     <    maxListingCount )     &  &     (  ( aFileMetadataList . size (  )  )     >  =    maxListingCount )  )     {", "break ;", "}", "ListBlobItem   blobItem    =    blobItemIterator . next (  )  ;", "if    ( blobItem   instanceof   StorageInterface . CloudBlockBlobWrapper )     {", "String   blobKey    =    null ;", "StorageInterface . CloudBlockBlobWrapper   blob    =     (  ( StorageInterface . CloudBlockBlobWrapper )     ( blobItem )  )  ;", "BlobProperties   properties    =    blob . getProperties (  )  ;", "blobKey    =    normalizeKey ( blob )  ;", "FileMetadata   metadata ;", "if    (  . retrieveFolderAttribute ( blob )  )     {", "metadata    =    new   FileMetadata ( blobKey ,    properties . getLastModified (  )  . getTime (  )  ,    getPermissionStatus ( blob )  ,    BlobMaterialization . Explicit )  ;", "} else    {", "metadata    =    new   FileMetadata ( blobKey ,    properties . getLength (  )  ,    properties . getLastModified (  )  . getTime (  )  ,    getPermissionStatus ( blob )  )  ;", "}", "FileMetadata   existing    =     . getDirectoryInList ( aFileMetadataList ,    blobKey )  ;", "if    ( existing    !  =    null )     {", "aFileMetadataList . remove ( existing )  ;", "}", "aFileMetadataList . add ( metadata )  ;", "} else", "if    ( blobItem   instanceof   StorageInterface . CloudBlobDirectoryWrapper )     {", "StorageInterface . CloudBlobDirectoryWrapper   directory    =     (  ( StorageInterface . CloudBlobDirectoryWrapper )     ( blobItem )  )  ;", "if    ( isUnboundedDepth    |  |     ( maxListingDepth    >    listingDepth )  )     {", "dirIteratorStack . push ( blobItemIterator )  ;", "+  + listingDepth ;", "blobItems    =    directory . listBlobs ( null ,    false ,    EnumSet . noneOf ( com . microsoft . windowsazure . storage . blob . BlobListingDetails . class )  ,    null ,    getInstrumentedContext (  )  )  ;", "blobItemIterator    =    blobItems . iterator (  )  ;", "} else    {", "String   dirKey    =    normalizeKey ( directory )  ;", "if    (  (  . getDirectoryInList ( aFileMetadataList ,    dirKey )  )     =  =    null )     {", "FileMetadata   directoryMetadata    =    new   FileMetadata ( dirKey ,     0  ,     . defaultPermissionNoBlobMetadata (  )  ,    BlobMaterialization . Implicit )  ;", "aFileMetadataList . add ( directoryMetadata )  ;", "}", "}", "}", "}", "if    ( dirIteratorStack . isEmpty (  )  )     {", "blobItemIterator    =    null ;", "} else    {", "blobItemIterator    =    dirIteratorStack . pop (  )  ;", "-  - listingDepth ;", "if    ( listingDepth    <     0  )     {", "throw   new   AssertionError (  \" Non - negative   listing   depth   expected \"  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["buildUpList"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "synchronized ( containerStateLock )     {", "if    ( isOkContainerState ( accessType )  )     {", "return   currentKnownContainerState ;", "}", "if    (  ( currentKnownContainerState )     =  =     (  . ContainerState . ExistsAtWrongVersion )  )     {", "String   containerVersion    =     . retrieveVersionAttribute ( container )  ;", "throw   wrongVersionException ( containerVersion )  ;", "}", "if    (  ( currentKnownContainerState )     =  =     (  . ContainerState . ExistsAtRightVersion )  )     {", "throw   new   AssertionError (  (  \" Unexpected   state :     \"     +     ( currentKnownContainerState )  )  )  ;", "}", "try    {", "container . downloadAttributes ( getInstrumentedContext (  )  )  ;", "currentKnownContainerState    =     . ContainerState . Unknown ;", "}    catch    ( StorageException   ex )     {", "if    ( ex . getErrorCode (  )  . equals ( RESOURCE _ NOT _ FOUND . toString (  )  )  )     {", "currentKnownContainerState    =     . ContainerState . DoesntExist ;", "} else    {", "throw   ex ;", "}", "}", "if    (  ( currentKnownContainerState )     =  =     (  . ContainerState . DoesntExist )  )     {", "if    (  . needToCreateContainer ( accessType )  )     {", ". storeVersionAttribute ( container )  ;", "container . create ( getInstrumentedContext (  )  )  ;", "currentKnownContainerState    =     . ContainerState . ExistsAtRightVersion ;", "}", "} else    {", "String   containerVersion    =     . retrieveVersionAttribute ( container )  ;", "if    ( containerVersion    !  =    null )     {", "if    ( containerVersion . equals (  . FIRST _ WASB _ VERSION )  )     {", "if    ( needToStampVersion ( accessType )  )     {", ". storeVersionAttribute ( container )  ;", "container . uploadMetadata ( getInstrumentedContext (  )  )  ;", "}", "} else", "if    (  !  ( containerVersion . equals (  . CURRENT _ WASB _ VERSION )  )  )     {", "currentKnownContainerState    =     . ContainerState . ExistsAtWrongVersion ;", "throw   wrongVersionException ( containerVersion )  ;", "} else    {", "currentKnownContainerState    =     . ContainerState . ExistsAtRightVersion ;", "}", "} else    {", "currentKnownContainerState    =     . ContainerState . ExistsNoVersion ;", "if    ( needToStampVersion ( accessType )  )     {", ". storeVersionAttribute ( container )  ;", "container . uploadMetadata ( getInstrumentedContext (  )  )  ;", "currentKnownContainerState    =     . ContainerState . ExistsAtRightVersion ;", "}", "}", "}", "return   currentKnownContainerState ;", "}", "}", "METHOD_END"], "methodName": ["checkContainer"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( sessionUri )     =  =    null )     {", "throw   new   AssertionError (  \" Expected   a   non - null   session   URI   when   configuring   storage   session \"  )  ;", "}", "if    (  ( storageInteractionLayer )     =  =    null )     {", "throw   new   AssertionError ( String . format (  (  \" Cannot   configure   storage   session   for   URI    '  % s '     \"     +     \" if   storage   session   has   not   been   established .  \"  )  ,    sessionUri . toString (  )  )  )  ;", "}", "tolerateOobAppends    =    sessionConfiguration . getBoolean (  . KEY _ READ _ TOLERATE _ CONCURRENT _ APPEND ,     . DEFAULT _ READ _ TOLERATE _ CONCURRENT _ APPEND )  ;", "this . downloadBlockSizeBytes    =    sessionConfiguration . getInt (  . KEY _ STREAM _ MIN _ READ _ SIZE ,     . DEFAULT _ DOWNLOAD _ BLOCK _ SIZE )  ;", "this . uploadBlockSizeBytes    =    sessionConfiguration . getInt (  . KEY _ WRITE _ BLOCK _ SIZE ,     . DEFAULT _ UPLOAD _ BLOCK _ SIZE )  ;", "int   storageConnectionTimeout    =    sessionConfiguration . getInt (  . KEY _ STORAGE _ CONNECTION _ TIMEOUT ,     0  )  ;", "if    (  0     <    storageConnectionTimeout )     {", "storageInteractionLayer . setTimeoutInMs (  ( storageConnectionTimeout    *     1  0  0  0  )  )  ;", "}", "int   cpuCores    =     2     *     ( Runtime . getRuntime (  )  . availableProcessors (  )  )  ;", "concurrentWrites    =    sessionConfiguration . getInt (  . KEY _ CONCURRENT _ CONNECTION _ VALUE _ OUT ,    Math . min ( cpuCores ,     . DEFAULT _ CONCURRENT _ WRITES )  )  ;", "minBackoff    =    sessionConfiguration . getInt (  . KEY _ MIN _ BACKOFF _ INTERVAL ,     . DEFAULT _ MIN _ BACKOFF _ INTERVAL )  ;", "maxBackoff    =    sessionConfiguration . getInt (  . KEY _ MAX _ BACKOFF _ INTERVAL ,     . DEFAULT _ MAX _ BACKOFF _ INTERVAL )  ;", "deltaBackoff    =    sessionConfiguration . getInt (  . KEY _ BACKOFF _ INTERVAL ,     . DEFAULT _ BACKOFF _ INTERVAL )  ;", "maxRetries    =    sessionConfiguration . getInt (  . KEY _ MAX _ IO _ RETRIES ,     . DEFAULT _ MAX _ RETRY _ ATTEMPTS )  ;", "storageInteractionLayer . setRetryPolicyFactory ( new   RetryExponentialRetry ( minBackoff ,    deltaBackoff ,    maxBackoff ,    maxRetries )  )  ;", "selfThrottlingEnabled    =    sessionConfiguration . getBoolean (  . KEY _ SELF _ THROTTLE _ ENABLE ,     . DEFAULT _ SELF _ THROTTLE _ ENABLE )  ;", "selfThrottlingReadFactor    =    sessionConfiguration . getFloat (  . KEY _ SELF _ THROTTLE _ READ _ FACTOR ,     . DEFAULT _ SELF _ THROTTLE _ READ _ FACTOR )  ;", "selfThrottlingWriteFactor    =    sessionConfiguration . getFloat (  . KEY _ SELF _ THROTTLE _ WRITE _ FACTOR ,     . DEFAULT _ SELF _ THROTTLE _ WRITE _ FACTOR )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug ( String . format (  \"    init .    Settings =  % d ,  % b ,  % d ,  {  % d ,  % d ,  % d ,  % d }  ,  {  % b ,  % f ,  % f }  \"  ,    concurrentWrites ,    tolerateOobAppends ,     ( storageConnectionTimeout    >     0     ?    storageConnectionTimeout    :     . STORAGE _ CONNECTION _ TIMEOUT _ DEFAULT )  ,    minBackoff ,    deltaBackoff ,    maxBackoff ,    maxRetries ,    selfThrottlingEnabled ,    selfThrottlingReadFactor ,    selfThrottlingWriteFactor )  )  ;", "}", "}", "METHOD_END"], "methodName": ["configureAzureStorageSession"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   accountName    =    getAccountFromAuthority ( uri )  ;", "URI   storageUri    =    new   URI (  (  (  (  (  ( getHTTPScheme (  )  )     +     \"  :  \"  )     +     ( NativeAzur . PATH _ DELIMITER )  )     +     ( NativeAzur . PATH _ DELIMITER )  )     +    accountName )  )  ;", "String   containerName    =    getContainerFromAuthority ( uri )  ;", "storageInteractionLayer . createBlobClient ( storageUri )  ;", "suppressRetryPolicyInClientIfNeeded (  )  ;", "container    =    storageInteractionLayer . getContainerReference ( containerName )  ;", "rootDirectory    =    container . getDirectoryReference (  \"  \"  )  ;", "try    {", "if    (  !  ( container . exists ( getInstrumentedContext (  )  )  )  )     {", "throw   new   AzureException (  (  (  (  (  (  \" Container    \"     +    containerName )     +     \"    in   account    \"  )     +    accountName )     +     \"    not   found ,    and   we   can ' t   create    \"  )     +     \"    it   using   anoynomous   credentials .  \"  )  )  ;", "}", "}    catch    ( StorageException   ex )     {", "throw   new   AzureException (  (  (  (  (  (  \" Unable   to   access   container    \"     +    containerName )     +     \"    in   account    \"  )     +    accountName )     +     \"    using   anonymous   credentials ,    and   no   credentials   found   for   them    \"  )     +     \"    in   the   configuration .  \"  )  ,    ex )  ;", "}", "isAnonymousCredentials    =    true ;", "configureAzureStorageSession (  )  ;", "}", "METHOD_END"], "methodName": ["connectUsingAnonymousCredentials"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   rawAccountName    =    accountName . split (  \"  \\  \\  .  \"  )  [  0  ]  ;", "ageCredentials   credentials    =    new   ageCredentialsAccountAndKey ( rawAccountName ,    accountKey )  ;", "connectUsingCredentials ( accountName ,    credentials ,    containerName )  ;", "}", "METHOD_END"], "methodName": ["connectUsingConnectionStringCredentials"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    ( isStorageEmulatorAccount ( accountName )  )     {", "isStorageEmulator    =    true ;", "CloudStorageAccount   account    =    CloudStorageAccount . getDevelopmentStorageAccount (  )  ;", "storageInteractionLayer . createBlobClient ( account )  ;", "} else    {", "URI   blobEndPoint    =    new   URI (  (  (  ( getHTTPScheme (  )  )     +     \"  :  /  /  \"  )     +    accountName )  )  ;", "storageInteractionLayer . createBlobClient ( blobEndPoint ,    credentials )  ;", "}", "suppressRetryPolicyInClientIfNeeded (  )  ;", "container    =    storageInteractionLayer . getContainerReference ( containerName )  ;", "rootDirectory    =    container . getDirectoryReference (  \"  \"  )  ;", "canCreateOrModifyContainer    =    credentials   instanceof   StorageCredentialsAccountAndKey ;", "configureStorageSession (  )  ;", "}", "METHOD_END"], "methodName": ["connectUsingCredentials"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "StorageCredentials   credentials    =    new   StorageCredentialsSharedAccessSignature ( sas )  ;", "connectingUsingSAS    =    true ;", "connectUsingCredentials ( accountName ,    credentials ,    containerName )  ;", "}", "METHOD_END"], "methodName": ["connectUsingSASCredentials"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( null    =  =     ( sessionUri )  )     |  |     ( null    =  =     ( sessionConfiguration )  )  )     {", "throw   new   AzureException (  (  \" Filesystem   object   not   initialized   properly .  \"     +     \" Unable   to   start   session   with   Azure   Storage   server .  \"  )  )  ;", "}", "try    {", "if    (  ( getContainerFromAuthority ( sessionUri )  )     =  =    null )     {", "throw   new   AssertionError ( String . format (  \" Non - null   container   expected   from   session   URI :     % s .  \"  ,    sessionUri . toString (  )  )  )  ;", "}", "String   accountName    =    getAccountFromAuthority ( sessionUri )  ;", "if    ( null    =  =    accountName )     {", "final   String   errMsg    =    String . format (  (  \" Cannot   load   WASB   file   system   account   name   not \"     +     \"    specified   in   URI :     % s .  \"  )  ,    sessionUri . toString (  )  )  ;", "throw   new   AzureException ( errMsg )  ;", "}", "instrumentation . setAccountName ( accountName )  ;", "String   containerName    =    getContainerFromAuthority ( sessionUri )  ;", "instrumentation . setContainerName ( containerName )  ;", "if    ( isStorageEmulatorAccount ( accountName )  )     {", "connectUsingCredentials ( accountName ,    null ,    containerName )  ;", "return ;", "}", "String   propertyValue    =    sessionConfiguration . get (  (  (  (  (  . KEY _ ACCOUNT _ SAS _ PREFIX )     +    containerName )     +     \"  .  \"  )     +    accountName )  )  ;", "if    ( propertyValue    !  =    null )     {", "connectUsingSASCredentials ( accountName ,    containerName ,    propertyValue )  ;", "return ;", "}", "propertyValue    =     . getAccountKeyFromConfiguration ( accountName ,    sessionConfiguration )  ;", "if    ( propertyValue    !  =    null )     {", "connectUsingConnectionStringCredentials ( getAccountFromAuthority ( sessionUri )  ,    getContainerFromAuthority ( sessionUri )  ,    propertyValue )  ;", "return ;", "}", "connectUsingAnonymousCredentials ( sessionUri )  ;", "}    catch    ( Exception   e )     {", "throw   new   AzureException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["createAzureStorageSession"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "JSON   serializer    =    new   JSON (  )  ;", "serializer . addConvertor ( PermissionStatus . class ,    new    . PermissionStatusJsonSerializer (  )  )  ;", "return   serializer ;", "}", "METHOD_END"], "methodName": ["createPermissionJsonSerializer"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   new   PermissionStatus (  \"  \"  ,     \"  \"  ,    FsPermission . getDefault (  )  )  ;", "}", "METHOD_END"], "methodName": ["defaultPermissionNoBlobMetadata"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   authority    =    uri . getRawAuthority (  )  ;", "if    ( null    =  =    authority )     {", "throw   new   URISyntaxException ( uri . toString (  )  ,     \" Expected   URI   with   a   valid   authority \"  )  ;", "}", "if    (  !  ( authority . contains (  . WASB _ AUTHORITY _ DELIMITER )  )  )     {", "return   authority ;", "}", "String [  ]    authorityParts    =    authority . split (  . WASB _ AUTHORITY _ DELIMITER ,     2  )  ;", "if    (  (  ( authorityParts . length )     <     2  )     |  |     (  \"  \"  . equals ( authorityParts [  0  ]  )  )  )     {", "final   String   errMsg    =    String . format (  (  \" URI    '  % s '    has   a   malformed   WASB   authority ,    expected   container   name .     \"     +     \" Authority   takes   the   form   wasb :  /  /  [  < container   name >  @  ]  < account   name >  \"  )  ,    uri . toString (  )  )  ;", "throw   new   IllegalArgumentException ( errMsg )  ;", "}", "return   authorityParts [  1  ]  ;", "}", "METHOD_END"], "methodName": ["getAccountFromAuthority"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   key    =    null ;", "String   keyProviderClass    =    conf . get (  (  (  . KEY _ ACCOUNT _ KEYPROVIDER _ PREFIX )     +    accountName )  )  ;", "KeyProvider   keyProvider    =    null ;", "if    ( keyProviderClass    =  =    null )     {", "keyProvider    =    new   SimpleKeyProvider (  )  ;", "} else    {", "Object   keyProviderObject    =    null ;", "try    {", "Class <  ?  >    clazz    =    conf . getClassByName ( keyProviderClass )  ;", "keyProviderObject    =    clazz . newInstance (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   KeyProviderException (  \" Unable   to   load   key   provider   class .  \"  ,    e )  ;", "}", "if    (  !  ( keyProviderObject   instanceof   KeyProvider )  )     {", "throw   new   KeyProviderException (  ( keyProviderClass    +     \"    specified   in   config   is   not   a   valid   KeyProvider   class .  \"  )  )  ;", "}", "keyProvider    =     (  ( KeyProvider )     ( keyProviderObject )  )  ;", "}", "key    =    keyProvider . getStorageAccountKey ( accountName ,    conf )  ;", "return   key ;", "}", "METHOD_END"], "methodName": ["getAccountKeyFromConfiguration"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   bandwidthGaugeUpdater ;", "}", "METHOD_END"], "methodName": ["getBandwidthGaugeUpdater"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "StorageInterface . CloudBlockBlobWrapper   blob    =    this . container . getBlockBlobReference ( aKey )  ;", "blob . setStreamMinimumReadSizeInBytes ( downloadBlockSizeBytes )  ;", "blob . setWriteBlockSizeInBytes ( uploadBlockSizeBytes )  ;", "return   blob ;", "}", "METHOD_END"], "methodName": ["getBlobReference"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   authority    =    uri . getRawAuthority (  )  ;", "if    ( null    =  =    authority )     {", "throw   new   URISyntaxException ( uri . toString (  )  ,     \" Expected   URI   with   a   valid   authority \"  )  ;", "}", "if    (  !  ( authority . contains (  . WASB _ AUTHORITY _ DELIMITER )  )  )     {", "return    . AZURE _ ROOT _ CONTAINER ;", "}", "String [  ]    authorityParts    =    authority . split (  . WASB _ AUTHORITY _ DELIMITER ,     2  )  ;", "if    (  (  ( authorityParts . length )     <     2  )     |  |     (  \"  \"  . equals ( authorityParts [  0  ]  )  )  )     {", "final   String   errMsg    =    String . format (  (  \" URI    '  % s '    has   a   malformed   WASB   authority ,    expected   container   name .  \"     +     \" Authority   takes   the   form   wasb :  /  /  [  < container   name >  @  ]  < account   name >  \"  )  ,    uri . toString (  )  )  ;", "throw   new   IllegalArgumentException ( errMsg )  ;", "}", "return   authorityParts [  0  ]  ;", "}", "METHOD_END"], "methodName": ["getContainerFromAuthority"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "for    ( FileMetadata   current    :    list )     {", "if    (  ( current . isDir (  )  )     &  &     ( current . getKey (  )  . equals ( key )  )  )     {", "return   current ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getDirectoryInList"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "BlobRequestOptions   options    =    new   BlobRequestOptions (  )  ;", "options . setRetryPolicyFacy ( new   RetryExponentialRetry ( minBackoff ,    deltaBackoff ,    maxBackoff ,    maxRetries )  )  ;", "options . setUseTransactionalContentMD 5  ( getUseTransactionalContentMD 5  (  )  )  ;", "return   options ;", "}", "METHOD_END"], "methodName": ["getDownloadOptions"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   sessionScheme    =    sessionUri . getScheme (  )  ;", "if    (  ( sessionScheme    !  =    null )     &  &     (  ( sessionScheme . equalsIgnoreCase (  \" asvs \"  )  )     |  |     ( sessionScheme . equalsIgnoreCase (  \" wasbs \"  )  )  )  )     {", "return    . HTTPS _ SCHEME ;", "} else    {", "return    . HTTP _ SCHEME ;", "}", "}", "METHOD_END"], "methodName": ["getHTTPScheme"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   getInstrumentedContext ( false )  ;", "}", "METHOD_END"], "methodName": ["getInstrumentedContext"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "OperationContext   operationContext    =    new   OperationContext (  )  ;", "if    ( selfThrottlingEnabled )     {", "SelfThrottlingIntercept . hook ( operationContext ,    selfThrottlingReadFactor ,    selfThrottlingWriteFactor )  ;", "}", "ResponseReceivedMetricUpdater . hook ( operationContext ,    instrumentation ,    bandwidthGaugeUpdater )  ;", "if    ( bindConcurrentOOBIo )     {", "SendRequestIntercept . bind ( storageInteractionLayer . getCredentials (  )  ,    operationContext ,    true )  ;", "}", "if    (  ( testHookOperationContext )     !  =    null )     {", "operationContext    =    testHookOperationContext . modifyOperationContext ( operationContext )  ;", "}", "ErrorMetricUpdater . hook ( operationContext ,    instrumentation )  ;", "return   operationContext ;", "}", "METHOD_END"], "methodName": ["getInstrumentedContext"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   AzureNativeFileSystemStore . getMetadataAttribute ( blob ,    AzureNativeFileSystemStore . LINK _ BACK _ TO _ UPLOAD _ IN _ PROGRESS _ METADATA _ KEY ,    AzureNativeFileSystemStore . OLD _ LINK _ BACK _ TO _ UPLOAD _ IN _ PROGRESS _ METADATA _ KEY )  ;", "}", "METHOD_END"], "methodName": ["getLinkAttributeValue"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    metadata    =    blob . getMetadata (  )  ;", "if    ( null    =  =    metadata )     {", "return   null ;", "}", "for    ( String   key    :    keyAlterns )     {", "if    ( metadata . containsKey ( key )  )     {", "return   metadata . get ( key )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getMetadataAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   permissionMetadataValue    =    AzureNativeFileSystemStore . getMetadataAttribute ( blob ,    AzureNativeFileSystemStore . PERMISSION _ METADATA _ KEY ,    AzureNativeFileSystemStore . OLD _ PERMISSION _ METADATA _ KEY )  ;", "if    ( permissionMetadataValue    !  =    null )     {", "return   AzureNativeFileSystemStore . PermissionStatusJsonSerializer . fromJSONString ( permissionMetadataValue )  ;", "} else    {", "return   AzureNativeFileSystemStore . defaultPermissionNoBlobMetadata (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getPermissionStatus"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "BlobRequestOptions   options    =    new   BlobRequestOptions (  )  ;", "options . setStoreBlobContentMD 5  ( sessionConfiguration . getBoolean (  . KEY _ STORE _ BLOB _ MD 5  ,    false )  )  ;", "options . setUseTransactionalContentMD 5  ( getUseTransactionalContentMD 5  (  )  )  ;", "options . setConcurrentRequestCount ( concurrentWrites )  ;", "options . setRetryPolicyFactory ( new   RetryExponentialRetry ( minBackoff ,    deltaBackoff ,    maxBackoff ,    maxRetries )  )  ;", "return   options ;", "}", "METHOD_END"], "methodName": ["getUploadOptions"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   sessionConfiguration . getBoolean ( AzureNativeFileSystemStore . KEY _ CHECK _ BLOCK _ MD 5  ,    true )  ;", "}", "METHOD_END"], "methodName": ["getUseTransactionalContentMD5"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    ( isAnonymousCredentials )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["isAuthenticatedAccess"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   tolerateOobAppends ;", "}", "METHOD_END"], "methodName": ["isConcurrentOOBAppendAllowed"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "switch    ( currentKnownContainerState )     {", "case   Unknown    :", "return   connectingUsingSAS ;", "case   DoesntExist    :", "return   false ;", "case   ExistsAtRightVersion    :", "return   true ;", "case   ExistsAtWrongVersion    :", "return   false ;", "case   ExistsNoVersion    :", "return    !  ( needToStampVersion ( accessType )  )  ;", "default    :", "throw   new   AssertionError (  (  \" Unknown   access   type :     \"     +    accessType )  )  ;", "}", "}", "METHOD_END"], "methodName": ["isOkContainerState"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   accountName . equalsIgnoreCase ( sessionConfiguration . get ( AzureNativeFileSystemStore . STORAGE _ EMULATOR _ ACCOUNT _ NAME _ PROPERTY _ NAME ,    AzureNativeFileSystemStore . DEFAULT _ STORAGE _ EMULATOR _ ACCOUNT _ NAME )  )  ;", "}", "METHOD_END"], "methodName": ["isStorageEmulatorAccount"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "checkContainer (  . ContainerAccessType . PureRead )  ;", "if    (  (  0     <     ( prefix . length (  )  )  )     &  &     (  !  ( prefix . endsWith ( NativeAzureFileSystem . PATH _ DELIMITER )  )  )  )     {", "prefix    +  =    NativeAzureFileSystem . PATH _ DELIMITER ;", "}", "Iterable < ListBlobItem >    objects ;", "if    ( prefix . equals (  \"  /  \"  )  )     {", "objects    =    listRootBlobs ( true )  ;", "} else    {", "objects    =    listRootBlobs ( prefix ,    true )  ;", "}", "ArrayList < FileMetadata >    fileMetadata    =    new   ArrayList < FileMetadata >  (  )  ;", "for    ( ListBlobItem   blobItem    :    objects )     {", "if    (  (  0     <    maxListingCount )     &  &     (  ( fileMetadata . size (  )  )     >  =    maxListingCount )  )     {", "break ;", "}", "if    ( blobItem   instanceof   StorageInterface . CloudBlockBlobWrapper )     {", "String   blobKey    =    null ;", "StorageInterface . CloudBlockBlobWrapper   blob    =     (  ( StorageInterface . CloudBlockBlobWrapper )     ( blobItem )  )  ;", "BlobProperties   properties    =    blob . getProperties (  )  ;", "blobKey    =    normalizeKey ( blob )  ;", "FileMetadata   metadata ;", "if    (  . retrieveFolderAttribute ( blob )  )     {", "metadata    =    new   FileMetadata ( blobKey ,    properties . getLastModified (  )  . getTime (  )  ,    getPermissionStatus ( blob )  ,    BlobMaterialization . Explicit )  ;", "} else    {", "metadata    =    new   FileMetadata ( blobKey ,    properties . getLength (  )  ,    properties . getLastModified (  )  . getTime (  )  ,    getPermissionStatus ( blob )  )  ;", "}", "FileMetadata   existing    =     . getDirectoryInList ( fileMetadata ,    blobKey )  ;", "if    ( existing    !  =    null )     {", "fileMetadata . remove ( existing )  ;", "}", "fileMetadata . add ( metadata )  ;", "} else", "if    ( blobItem   instanceof   StorageInterface . CloudBlobDirectoryWrapper )     {", "StorageInterface . CloudBlobDirectoryWrapper   directory    =     (  ( StorageInterface . CloudBlobDirectoryWrapper )     ( blobItem )  )  ;", "String   dirKey    =    normalizeKey ( directory )  ;", "if    ( dirKey . endsWith ( NativeAzureFileSystem . PATH _ DELIMITER )  )     {", "dirKey    =    dirKey . substring (  0  ,     (  ( dirKey . length (  )  )     -     1  )  )  ;", "}", "FileMetadata   directoryMetadata    =    new   FileMetadata ( dirKey ,     0  ,     . defaultPermissionNoBlobMetadata (  )  ,    BlobMaterialization . Implicit )  ;", "if    (  (  . getDirectoryInList ( fileMetadata ,    dirKey )  )     =  =    null )     {", "fileMetadata . add ( directoryMetadata )  ;", "}", "buildUpList ( directory ,    fileMetadata ,    maxListingCount ,     ( maxListingDepth    -     1  )  )  ;", "}", "}", "priorLastKey    =    null ;", "return   new   PartialListing ( priorLastKey ,    fileMetadata . toArray ( new   FileMetadata [  ]  {        }  )  ,     (  0     =  =     ( fileMetadata . size (  )  )     ?    new   String [  ]  {        }     :    new   String [  ]  {    prefix    }  )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   AzureException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["list"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   rootDirectory . listBlobs ( null ,    false ,     ( includeMetadata    ?    EnumSet . of ( METADATA )     :    EnumSet . noneOf ( BlobListingDetails . class )  )  ,    null ,    getInstrumentedContext (  )  )  ;", "}", "METHOD_END"], "methodName": ["listRootBlobs"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   rootDirectory . listBlobs ( aPrefix ,    false ,     ( includeMetadata    ?    EnumSet . of ( METADATA )     :    EnumSet . noneOf ( BlobListingDetails . class )  )  ,    null ,    getInstrumentedContext (  )  )  ;", "}", "METHOD_END"], "methodName": ["listRootBlobs"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "StorageInterface . CloudBlobDirectoryWrapper   directory    =    this . container . getDirectoryReference ( aPrefix )  ;", "return   directory . listBlobs ( null ,    useFlatBlobListing ,    listingDetails ,    options ,    opContext )  ;", "}", "METHOD_END"], "methodName": ["listRootBlobs"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   accessType    =  =     ( AzureNativeFileSystemStore . ContainerAccessType . PureWrite )  ;", "}", "METHOD_END"], "methodName": ["needToCreateContainer"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return    ( accessType    !  =     ( AzureNativeFileSystemStore . ContainerAccessType . PureRead )  )     &  &     ( canCreateOrModifyContainer )  ;", "}", "METHOD_END"], "methodName": ["needToStampVersion"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   normKey ;", "int   parts    =     ( isageEmulator )     ?     4     :     3  ;", "normKey    =    keyUri . getPath (  )  . split (  \"  /  \"  ,    parts )  [  ( parts    -     1  )  ]  ;", "return   normKey ;", "}", "METHOD_END"], "methodName": ["normalizeKey"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   dirKey    =    normalizeKey ( directory . getUri (  )  )  ;", "if    ( dirKey . endsWith ( NativeAzur . PATH _ DELIMITER )  )     {", "dirKey    =    dirKey . substring (  0  ,     (  ( dirKey . length (  )  )     -     1  )  )  ;", "}", "return   dirKey ;", "}", "METHOD_END"], "methodName": ["normalizeKey"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   normalizeKey ( blob . getUri (  )  )  ;", "}", "METHOD_END"], "methodName": ["normalizeKey"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    metadata    =    blob . getMetadata (  )  ;", "if    ( metadata    !  =    null )     {", "metadata . remove ( key )  ;", "blob . setMetadata ( metadata )  ;", "}", "}", "METHOD_END"], "methodName": ["removeMetadataAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    metadata    =    blob . getMetadata (  )  ;", "return    ( null    !  =    metadata )     &  &     (  ( metadata . containsKey (  . IS _ FOLDER _ METADATA _ KEY )  )     |  |     ( metadata . containsKey (  . OLD _ IS _ FOLDER _ METADATA _ KEY )  )  )  ;", "}", "METHOD_END"], "methodName": ["retrieveFolderAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    metadata    =    container . getMetadata (  )  ;", "if    ( metadata    =  =    null )     {", "return   null ;", "} else", "if    ( metadata . containsKey (  . VERSION _ METADATA _ KEY )  )     {", "return   metadata . get (  . VERSION _ METADATA _ KEY )  ;", "} else", "if    ( metadata . containsKey (  . OLD _ VERSION _ METADATA _ KEY )  )     {", "return   metadata . get (  . OLD _ VERSION _ METADATA _ KEY )  ;", "} else    {", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["retrieveVersionAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "OperationContext   operationContext    =    getInstrumentedContext (  )  ;", "try    {", "blob . delete ( operationContext )  ;", "}    catch    ( StorageException   e )     {", "if    (  (  (  (  ( e . getErrorCode (  )  )     !  =    null )     &  &     ( e . getErrorCode (  )  . equals (  \" BlobNotFound \"  )  )  )     &  &     (  ( operationContext . getRequestResults (  )  . size (  )  )     >     1  )  )     &  &     (  ( operationContext . getRequestResults (  )  . get (  0  )  . getException (  )  )     !  =    null )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Swallowing   delete   exception   on   retry :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "return ;", "} else    {", "throw   e ;", "}", "}", "}", "METHOD_END"], "methodName": ["safeDelete"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "this . storageInteractionLayer    =    storageInteractionLayer ;", "}", "METHOD_END"], "methodName": ["setAzureStorageInteractionLayer"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "AzureNativeFileSystemStore . storeMetadataAttribute ( blob ,    AzureNativeFileSystemStore . IS _ FOLDER _ METADATA _ KEY ,     \" true \"  )  ;", "AzureNativeFileSystemStore . removeMetadataAttribute ( blob ,    AzureNativeFileSystemStore . OLD _ IS _ FOLDER _ METADATA _ KEY )  ;", "}", "METHOD_END"], "methodName": ["storeFolderAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "AzureNativeFileSystemStore . storeMetadataAttribute ( blob ,    AzureNativeFileSystemStore . LINK _ BACK _ TO _ UPLOAD _ IN _ PROGRESS _ METADATA _ KEY ,    linkTarget )  ;", "AzureNativeFileSystemStore . removeMetadataAttribute ( blob ,    AzureNativeFileSystemStore . OLD _ LINK _ BACK _ TO _ UPLOAD _ IN _ PROGRESS _ METADATA _ KEY )  ;", "}", "METHOD_END"], "methodName": ["storeLinkAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    metadata    =    blob . getMetadata (  )  ;", "if    ( null    =  =    metadata )     {", "metadata    =    new   HashMap < String ,    String >  (  )  ;", "}", "metadata . put ( key ,    value )  ;", "blob . setMetadata ( metadata )  ;", "}", "METHOD_END"], "methodName": ["storeMetadataAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "AzureNativeFileSystemStore . storeMetadataAttribute ( blob ,    AzureNativeFileSystemStore . PERMISSION _ METADATA _ KEY ,    AzureNativeFileSystemStore . PERMISSION _ JSON _ SERIALIZER . toJSON ( permissionStatus )  )  ;", "AzureNativeFileSystemStore . removeMetadataAttribute ( blob ,    AzureNativeFileSystemStore . OLD _ PERMISSION _ METADATA _ KEY )  ;", "}", "METHOD_END"], "methodName": ["storePermissionStatus"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    metadata    =    container . getMetadata (  )  ;", "if    ( null    =  =    metadata )     {", "metadata    =    new   HashMap < String ,    String >  (  )  ;", "}", "metadata . put (  . VERSION _ METADATA _ KEY ,     . CURRENT _ WASB _ VERSION )  ;", "if    ( metadata . containsKey (  . OLD _ VERSION _ METADATA _ KEY )  )     {", "metadata . remove (  . OLD _ VERSION _ METADATA _ KEY )  ;", "}", "container . setMetadata ( metadata )  ;", "}", "METHOD_END"], "methodName": ["storeVersionAttribute"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "suppressRetryPolicy    =    true ;", "}", "METHOD_END"], "methodName": ["suppressRetryPolicy"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    ( suppressRetryPolicy )     {", "sageInteractionLayer . setRetryPolicyFacy ( new   RetryNoRetry (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["suppressRetryPolicyInClientIfNeeded"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "boolean   copyInProgress    =    true ;", "int   exceptionCount    =     0  ;", "while    ( copyInProgress )     {", "try    {", "blob . downloadAttributes ( opContext )  ;", "}    catch    ( StorageException   se )     {", "exceptionCount +  +  ;", "if    ( exceptionCount    >     1  0  )     {", "throw   new   Exception (  \" Too   many   storage   exceptions   during   waitForCopyToComplete \"  ,    se )  ;", "}", "}", "copyInProgress    =     (  ( blob . getCopyState (  )  )     !  =    null )     &  &     (  ( blob . getCopyState (  )  . getStatus (  )  )     =  =     ( CopyStatus . PENDING )  )  ;", "if    ( copyInProgress )     {", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   ie )     {", "Thread . currentThread (  )  . interrupt (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["waitForCopyToComplete"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   new   AzureException (  (  (  (  (  (  \" The   container    \"     +     ( container . getName (  )  )  )     +     \"    is   at   an   unsupported   version :     \"  )     +    containerVersion )     +     \"  .    Current   supported   version :     \"  )     +     ( AzureNativeFileSystemStore . FIRST _ WASB _ VERSION )  )  )  ;", "}", "METHOD_END"], "methodName": ["wrongVersionException"], "fileName": "org.apache.hadoop.fs.azure.AzureNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   blobMaterialization ;", "}", "METHOD_END"], "methodName": ["getBlobMaterialization"], "fileName": "org.apache.hadoop.fs.azure.FileMetadata"}, {"methodBody": ["METHOD_START", "{", "return   key ;", "}", "METHOD_END"], "methodName": ["getKey"], "fileName": "org.apache.hadoop.fs.azure.FileMetadata"}, {"methodBody": ["METHOD_START", "{", "return   lastModified ;", "}", "METHOD_END"], "methodName": ["getLastModified"], "fileName": "org.apache.hadoop.fs.azure.FileMetadata"}, {"methodBody": ["METHOD_START", "{", "return   length ;", "}", "METHOD_END"], "methodName": ["getLength"], "fileName": "org.apache.hadoop.fs.azure.FileMetadata"}, {"methodBody": ["METHOD_START", "{", "return   permissionStatus ;", "}", "METHOD_END"], "methodName": ["getPermissionStatus"], "fileName": "org.apache.hadoop.fs.azure.FileMetadata"}, {"methodBody": ["METHOD_START", "{", "return   isDir ;", "}", "METHOD_END"], "methodName": ["isDir"], "fileName": "org.apache.hadoop.fs.azure.FileMetadata"}, {"methodBody": ["METHOD_START", "{", "blobs . put ( destKey ,    blobs . get ( sourceKey )  )  ;", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "blobs . remove ( key )  ;", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   blobs . containsKey ( key )  ;", "}", "METHOD_END"], "methodName": ["exists"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   containerMetadata ;", "}", "METHOD_END"], "methodName": ["getContainerMetadata"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   blobs . get ( key )  . content ;", "}", "METHOD_END"], "methodName": ["getContent"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ArrayList < String >  ( blobs . keySet (  )  )  ;", "}", "METHOD_END"], "methodName": ["getKeys"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "return    (  ( HashMap < String ,    String >  )     ( blobs . get ( key )  . metadata . clone (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getMetadata"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "ArrayList < InMemoryBlockBlobStore . ListBlobEntry >    list    =    new   ArrayList < InMemoryBlockBlobStore . ListBlobEntry >  (  )  ;", "for    ( Map . Entry < String ,    InMemoryBlockBlobStore . Entry >    entry    :    blobs . entrySet (  )  )     {", "if    ( entry . getKey (  )  . startsWith ( prefix )  )     {", "list . add ( new   InMemoryBlockBlobStore . ListBlobEntry ( entry . getKey (  )  ,     ( includeMetadata    ?    new   HashMap < String ,    String >  ( entry . getValue (  )  . metadata )     :    null )  ,    entry . getValue (  )  . content . length )  )  ;", "}", "}", "return   list ;", "}", "METHOD_END"], "methodName": ["listBlobs"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "containerMetadata    =    metadata ;", "}", "METHOD_END"], "methodName": ["setContainerMetadata"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "blobs . put ( key ,    new   InMemoryBlockBlobStore . Entry ( value ,     (  ( HashMap < String ,    String >  )     ( metadata . clone (  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setContent"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "setContent ( key ,    new   byte [  0  ]  ,    metadata )  ;", "return   new   ByteArrayOutputStream (  )     {", "@ Override", "public   void   flush (  )    throws   IOException    {", "super . flush (  )  ;", "setContent ( key ,    toByteArray (  )  ,    metadata )  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["upload"], "fileName": "org.apache.hadoop.fs.azure.InMemoryBlockBlobStore"}, {"methodBody": ["METHOD_START", "{", "preExistingContainers . add ( new   MockStorageInterface . PreExistingContainer ( uri ,    metadata )  )  ;", "}", "METHOD_END"], "methodName": ["addPreExistingContainer"], "fileName": "org.apache.hadoop.fs.azure.MockStorageInterface"}, {"methodBody": ["METHOD_START", "{", "return   backingStore ;", "}", "METHOD_END"], "methodName": ["getBackingStore"], "fileName": "org.apache.hadoop.fs.azure.MockStorageInterface"}, {"methodBody": ["METHOD_START", "{", "FsPermission   newPermission    =    new   FsPermission ( permission )  ;", "if    (  ( applyMode    =  =     (  . UMaskApplyMode . NewFile )  )     |  |     ( applyMode    =  =     (  . UMaskApplyMode . NewDirectory )  )  )     {", "newPermission    =    newPermission . applyUMask ( FsPermission . getUMask ( getConf (  )  )  )  ;", "}", "return   newPermission ;", "}", "METHOD_END"], "methodName": ["applyUMask"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   p . toUri (  )  . getPath (  )  . toString (  )  . contains (  \"  :  \"  )  ;", "}", "METHOD_END"], "methodName": ["containsColon"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "actualStore    =    new   AzureNativeFileSystemStore (  )  ;", "if    (  . suppressRetryPolicy )     {", "actualStore . suppressRetryPolicy (  )  ;", "}", "return   actualStore ;", "}", "METHOD_END"], "methodName": ["createDefaultStore"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   new   PermissionStatus ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    getConf (  )  . get ( NativeAzureFileSystem . AZURE _ DEFAULT _ GROUP _ PROPERTY _ NAME ,    NativeAzureFileSystem . AZURE _ DEFAULT _ GROUP _ DEFAULT )  ,    permission )  ;", "}", "METHOD_END"], "methodName": ["createPermissionStatus"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( NativeAzureFileSystem . LOG . isDebugEnabled (  )  )     {", "NativeAzureFileSystem . LOG . debug (  (  \" Deleting   files   with   dangling   temp   data   in    \"     +    root )  )  ;", "}", "handleFilesWithDanglingTempData ( root ,    new   NativeAzureFileSystem . DanglingFileDeleter (  )  )  ;", "}", "METHOD_END"], "methodName": ["deleteFilesWithDanglingTempData"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "String   fileName    =    aKey . substring (  (  ( aKey . lastIndexOf ( SEPARATOR )  )     +     1  )  ,    aKey . length (  )  )  ;", "String   filePrefix    =     (  (  . AZURE _ TEMP _ FOLDER )     +     ( Path . SEPARATOR )  )     +     ( UUID . randomUUID (  )  . toString (  )  )  ;", "String   randomizedKey    =    filePrefix    +    fileName ;", "return   randomizedKey ;", "}", "METHOD_END"], "methodName": ["encodeKey"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   instrumentation ;", "}", "METHOD_END"], "methodName": ["getInstrumentation"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   actualStore ;", "}", "METHOD_END"], "methodName": ["getStore"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "long   cutoffForDangling    =     ( new   Date (  )  . getTime (  )  )     -     (  ( getConf (  )  . getInt ( NativeAzureFileSystem . AZURE _ TEMP _ EXPIRY _ PROPERTY _ NAME ,    NativeAzureFileSystem . AZURE _ TEMP _ EXPIRY _ DEFAULT )  )     *     1  0  0  0  )  ;", "String   priorLastKey    =    null ;", "do    {", "PartialListing   listing    =    store . listAll ( pathToKey ( root )  ,    NativeAzureFileSystem . AZURE _ LIST _ ALL ,    NativeAzureFileSystem . AZURE _ UNBOUNDED _ DEPTH ,    priorLastKey )  ;", "for    ( FileMetadata   file    :    listing . getFiles (  )  )     {", "if    (  !  ( file . isDir (  )  )  )     {", "String   link    =    store . getLinkInFileMetadata ( file . getKey (  )  )  ;", "if    ( link    !  =    null )     {", "FileMetadata   linkMetadata    =    store . retrieveMetadata ( link )  ;", "if    (  ( linkMetadata    !  =    null )     &  &     (  ( linkMetadata . getLastModified (  )  )     >  =    cutoffForDangling )  )     {", "handler . handleFile ( file ,    linkMetadata )  ;", "}", "}", "}", "}", "priorLastKey    =    listing . getPriorLastKey (  )  ;", "}    while    ( priorLastKey    !  =    null    )  ;", "}", "METHOD_END"], "methodName": ["handleFilesWithDanglingTempData"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return    ( scheme    !  =    null )     &  &     (  (  (  ( scheme . equalsIgnoreCase (  \" asv \"  )  )     |  |     ( scheme . equalsIgnoreCase (  \" asvs \"  )  )  )     |  |     ( scheme . equalsIgnoreCase (  \" wasb \"  )  )  )     |  |     ( scheme . equalsIgnoreCase (  \" wasbs \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["isWasbScheme"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( key . equals (  \"  /  \"  )  )     {", "turn   new   Path (  \"  /  \"  )  ;", "}", "turn   new   Path (  (  \"  /  \"     +    key )  )  ;", "}", "METHOD_END"], "methodName": ["keyToPath"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( path . isAbsolute (  )  )     {", "return   path ;", "}", "return   new   Path ( workingDir ,    path )  ;", "}", "METHOD_END"], "methodName": ["makeAbsolute"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   new   FileStatus (  0  ,    true ,     1  ,    blockSize ,     ( meta    =  =    null    ?     0     :    meta . getLastModified (  )  )  ,     0  ,     ( meta    =  =    null    ?    FsPermission . getDefault (  )     :    meta . getPermissionStatus (  )  . getPermission (  )  )  ,     ( meta    =  =    null    ?     \"  \"     :    meta . getPermissionStatus (  )  . getUserName (  )  )  ,     ( meta    =  =    null    ?     \"  \"     :    meta . getPermissionStatus (  )  . getGroupName (  )  )  ,    path . makeQualified ( getUri (  )  ,    getWorkingDirectory (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["newDirectory"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   new   FileStatus ( meta . getLength (  )  ,    false ,     1  ,    blockSize ,    meta . getLastModified (  )  ,     0  ,    meta . getPermissionStatus (  )  . getPermission (  )  ,    meta . getPermissionStatus (  )  . getUserName (  )  ,    meta . getPermissionStatus (  )  . getGroupName (  )  ,    path . makeQualified ( getUri (  )  ,    getWorkingDirectory (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["newFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "int   number    =    NativeAzureFileSystem . metricsSourceNameCounter . incrementAndGet (  )  ;", "final   String   baseName    =     \" AzureFileSystemMetrics \"  ;", "if    ( number    =  =     1  )     {", "return   baseName ;", "} else    {", "return   baseName    +    number ;", "}", "}", "METHOD_END"], "methodName": ["newMetricsSourceName"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "URI   tmpUri    =    path . toUri (  )  ;", "String   pathUri    =    tmpUri . getPath (  )  ;", "Path   newPath    =    path ;", "if    (  \"  \"  . equals ( pathUri )  )     {", "newPath    =    new   Path (  (  ( tmpUri . toString (  )  )     +     ( Path . SEPARATOR )  )  )  ;", "}", "if    (  !  ( newPath . isAbsolute (  )  )  )     {", "throw   new   IllegalArgumentException (  (  \" Path   must   be   absolute :     \"     +    path )  )  ;", "}", "String   key    =    null ;", "key    =    newPath . toUri (  )  . getPath (  )  ;", "if    (  ( key . length (  )  )     =  =     1  )     {", "return   key ;", "} else    {", "return   key . substring (  1  )  ;", "}", "}", "METHOD_END"], "methodName": ["pathToKey"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( null    =  =     ( uri . getAuthority (  )  )  )     {", "URI   defaultUri    =    FileSystem . getDefaultUri ( conf )  ;", "if    (  ( defaultUri    !  =    null )     &  &     (  . isWasbScheme ( defaultUri . getScheme (  )  )  )  )     {", "try    {", "return   new   URI ( uri . getScheme (  )  ,    defaultUri . getAuthority (  )  ,    uri . getPath (  )  ,    uri . getQuery (  )  ,    uri . getFragment (  )  )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   Error (  \" Bad   URI   construction \"  ,    e )  ;", "}", "}", "}", "return   uri ;", "}", "METHOD_END"], "methodName": ["reconstructAuthorityIfNeeded"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( NativeAzureFileSystem . LOG . isDebugEnabled (  )  )     {", "NativeAzureFileSystem . LOG . debug (  (  \" Recovering   files   with   dangling   temp   data   in    \"     +    root )  )  ;", "}", "handleFilesWithDanglingTempData ( root ,    new   NativeAzureFileSystem . DanglingFileRecoverer ( destination )  )  ;", "}", "METHOD_END"], "methodName": ["recoverFilesWithDanglingTempData"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem . suppressRetryPolicy    =    false ;", "}", "METHOD_END"], "methodName": ["resumeRetryPolicy"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem . suppressRetryPolicy    =    true ;", "}", "METHOD_END"], "methodName": ["suppressRetryPolicy"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystem"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( NativeAzureFileSystemBaseTest . ignoreStickyBit ( expected )  ,    NativeAzureFileSystemBaseTest . ignoreStickyBit ( actual )  )  ;", "}", "METHOD_END"], "methodName": ["assertEqualsIgnoreStickyBit"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   outputStream    =    fs . create ( testFile ,    permission ,    true ,     4  0  9  6  ,     (  ( short )     (  1  )  )  ,     1  0  2  4  ,    null )  ;", "outputStream . close (  )  ;", "}", "METHOD_END"], "methodName": ["createEmptyFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   new   FsPermission ( original . getUserAction (  )  ,    original . getGroupAction (  )  ,    original . getOtherAction (  )  )  ;", "}", "METHOD_END"], "methodName": ["ignoreStickyBit"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( inputStream )  )  ;", "final   int   BUFFER _ SIZE    =     1  0  2  4  ;", "char [  ]    buffer    =    new   char [ BUFFER _ SIZE ]  ;", "int   count    =    reader . read ( buffer ,     0  ,    BUFFER _ SIZE )  ;", "if    ( count    >  =    BUFFER _ SIZE )     {", "throw   new   IOException (  \" Exceeded   buffer   size \"  )  ;", "}", "inputStream . close (  )  ;", "return   new   String ( buffer ,     0  ,    count )  ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FSDataInputStream   inputStream    =    fs . open ( testFile )  ;", "String   ret    =    readString ( inputStream )  ;", "inputStream . close (  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   readString ( fs ,    testFile )  ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    createTestAccount (  )  ;", "if    (  ( testAccount )     !  =    null )     {", "fs    =    testAccount . get (  )  ;", "}", "Assume . assumeNotNull ( testAccount )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "fs    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Assert . assertFalse ( fs . exists ( new   Path (  \"  / a \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testCheckingNonExistentOneLetterFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "fs . close (  )  ;", "fs . close (  )  ;", "}", "METHOD_END"], "methodName": ["testCloseFileSystemTwice"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   localFilePath    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \" azure _ test \"  )  )  ;", "localFs    =     . get ( new   Configuration (  )  )  ;", "localFs . delete ( localFilePath ,    true )  ;", "try    {", "writeString ( localFs ,    localFilePath ,     \" Testing \"  )  ;", "Path   dstPath    =    new   Path (  \" copiedFromLocal \"  )  ;", "Assert . assertTrue ( FileUtil . copy ( localFs ,    localFilePath ,    fs ,    dstPath ,    false ,    fs . getConf (  )  )  )  ;", "Assert . assertTrue ( fs . exists ( dstPath )  )  ;", "Assert . assertEquals (  \" Testing \"  ,    readString ( fs ,    dstPath )  )  ;", "fs . delete ( dstPath ,    true )  ;", "}    finally    {", "localFs . delete ( localFilePath ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopyFromLocalFileSystem"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   dir    =    new   Path (  \"  / x \"  )  ;", "Assert . assertTrue ( mkdirs ( dir )  )  ;", "try    {", "create ( dir )  . close (  )  ;", "Assert . assertTrue (  \" Should ' ve   thrown \"  ,    false )  ;", "}    catch    ( IOException   ex )     {", "Assert . assertEquals (  \" Cannot   create   file    / x ;    already   exists   as   a   directory .  \"  ,    ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCreatingFileOverDirectory"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \" deep / file / creation / test \"  )  ;", "FsPermission   permission    =    FsPermission . createImmutable (  (  ( short )     (  6  4  4  )  )  )  ;", "createEmptyFile ( testFile ,    permission )  ;", "Assert . assertTrue ( fs . exists ( testFile )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \" deep \"  )  )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \" deep / file / creation \"  )  )  )  ;", "FileStatus   ret    =    fs . getFileStatus ( new   Path (  \" deep / file \"  )  )  ;", "Assert . assertTrue ( ret . isDirectory (  )  )  ;", ". assertEqualsIgnoreStickyBit ( permission ,    ret . getPermission (  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \" deep \"  )  ,    true )  )  ;", "Assert . assertFalse ( fs . exists ( testFile )  )  ;", "}", "METHOD_END"], "methodName": ["testDeepFileCreation"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \" ownershipTestFile \"  )  ;", "writeString ( testFile ,     \" Testing \"  )  ;", "testOwnership ( testFile )  ;", "}", "METHOD_END"], "methodName": ["testFileOwnership"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \" permissionTestFile \"  )  ;", "FsPermission   permission    =    FsPermission . createImmutable (  (  ( short )     (  6  4  4  )  )  )  ;", "createEmptyFile ( testFile ,    permission )  ;", "FileStatus   ret    =    fs . getFileStatus ( testFile )  ;", ". assertEqualsIgnoreStickyBit ( permission ,    ret . getPermission (  )  )  ;", "fs . delete ( testFile ,    true )  ;", "}", "METHOD_END"], "methodName": ["testFilePermissions"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   parentFolder    =    new   Path (  \" testFolder \"  )  ;", "Path   innerFile    =    new   Path ( parentFolder ,     \" innerfile \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( parentFolder )  )  ;", "long   lastModifiedTime    =    fs . gettatus ( parentFolder )  . getModificationTime (  )  ;", "Thread . sleep (  (  ( modifiedTimeErrorMargin )     +     1  )  )  ;", "Assert . assertTrue ( fs . createNewFile ( innerFile )  )  ;", "Assert . assertFalse ( testModifiedTime ( parentFolder ,    lastModifiedTime )  )  ;", "testModifiedTime ( parentFolder )  ;", "lastModifiedTime    =    fs . gettatus ( parentFolder )  . getModificationTime (  )  ;", "Path   destFolder    =    new   Path (  \" testDestFolder \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( destFolder )  )  ;", "long   destLastModifiedTime    =    fs . gettatus ( destFolder )  . getModificationTime (  )  ;", "Thread . sleep (  (  ( modifiedTimeErrorMargin )     +     1  )  )  ;", "Path   destFile    =    new   Path ( destFolder ,     \" innerfile \"  )  ;", "Assert . assertTrue ( fs . rename ( innerFile ,    destFile )  )  ;", "Assert . assertFalse ( testModifiedTime ( parentFolder ,    lastModifiedTime )  )  ;", "Assert . assertFalse ( testModifiedTime ( destFolder ,    destLastModifiedTime )  )  ;", "testModifiedTime ( parentFolder )  ;", "testModifiedTime ( destFolder )  ;", "destLastModifiedTime    =    fs . gettatus ( destFolder )  . getModificationTime (  )  ;", "Thread . sleep (  (  ( modifiedTimeErrorMargin )     +     1  )  )  ;", "fs . delete ( destFile ,    false )  ;", "Assert . assertFalse ( testModifiedTime ( destFolder ,    destLastModifiedTime )  )  ;", "testModifiedTime ( destFolder )  ;", "}", "METHOD_END"], "methodName": ["testFolderLastModifiedTime"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder    =    new   Path (  \" ownershipTestFolder \"  )  ;", "fs . mkdirs ( testFolder )  ;", "testOwnership ( testFolder )  ;", "}", "METHOD_END"], "methodName": ["testFolderOwnership"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder    =    new   Path (  \" permissionTestFolder \"  )  ;", "FsPermission   permission    =    FsPermission . createImmutable (  (  ( short )     (  6  4  4  )  )  )  ;", "fs . mkdirs ( testFolder ,    permission )  ;", "FileStatus   ret    =    fs . getFileStatus ( testFolder )  ;", ". assertEqualsIgnoreStickyBit ( permission ,    ret . getPermission (  )  )  ;", "fs . delete ( testFolder ,    true )  ;", "}", "METHOD_END"], "methodName": ["testFolderPermissions"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   rootFolder    =    new   Path (  \" testingList \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( rootFolder )  )  ;", "tatus [  ]    listed    =    fs . listStatus ( rootFolder )  ;", "Assert . assertEquals (  0  ,    listed . length )  ;", "Path   innerFolder    =    new   Path ( rootFolder ,     \" inner \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( innerFolder )  )  ;", "listed    =    fs . listStatus ( rootFolder )  ;", "Assert . assertEquals (  1  ,    listed . length )  ;", "Assert . assertTrue ( listed [  0  ]  . isDirectory (  )  )  ;", "Path   innerFile    =    new   Path ( innerFolder ,     \" innerFile \"  )  ;", "writeString ( innerFile ,     \" testing \"  )  ;", "listed    =    fs . listStatus ( rootFolder )  ;", "Assert . assertEquals (  1  ,    listed . length )  ;", "Assert . assertTrue ( listed [  0  ]  . isDirectory (  )  )  ;", "listed    =    fs . listStatus ( innerFolder )  ;", "Assert . assertEquals (  1  ,    listed . length )  ;", "Assert . assertFalse ( listed [  0  ]  . isDirectory (  )  )  ;", "Assert . assertTrue ( fs . delete ( rootFolder ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["testListDirectory"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder    =    new   Path (  \"  / testFolder \"  )  ;", "Path   testFile    =    new   Path ( testFolder ,     \" testFile \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder )  )  ;", "Assert . assertTrue ( fs . createNewFile ( testFile )  )  ;", "tatus   status    =    fs . gettatus ( new   Path (  \"  / testFolder /  .  \"  )  )  ;", "Assert . assertNotNull ( status )  ;", "}", "METHOD_END"], "methodName": ["testListSlash"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Calendar   utc    =    Calendar . getInstance ( TimeZone . getTimeZone (  \" UTC \"  )  )  ;", "long   currentUtcTime    =    utc . getTime (  )  . getTime (  )  ;", "tatus   fileStatus    =    fs . gettatus ( testPath )  ;", "Assert . assertTrue (  (  (  (  \" Modification   time    \"     +     ( new   Date ( fileStatus . getModificationTime (  )  )  )  )     +     \"    is   not   close   to   now :     \"  )     +     ( utc . getTime (  )  )  )  ,    testModifiedTime ( testPath ,    currentUtcTime )  )  ;", "}", "METHOD_END"], "methodName": ["testModifiedTime"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FileStatus   fileStatus    =    fs . getFileStatus ( testPath )  ;", "final   long   errorMargin    =    modifiedTimeErrorMargin ;", "long   lastModified    =    fileStatus . getModificationTime (  )  ;", "return    ( lastModified    >     ( time    -    errorMargin )  )     &  &     ( lastModified    <     ( time    +    errorMargin )  )  ;", "}", "METHOD_END"], "methodName": ["testModifiedTime"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \" testFile \"  )  ;", "fs . create ( testFile )  . close (  )  ;", "testModifiedTime ( testFile )  ;", "}", "METHOD_END"], "methodName": ["testModifiedTimeForFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder    =    new   Path (  \" testFolder \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder )  )  ;", "testModifiedTime ( testFolder )  ;", "}", "METHOD_END"], "methodName": ["testModifiedTimeForFolder"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FileStatus   ret    =    fs . getFileStatus ( pathUnderTest )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . getCurrentUser (  )  ;", "Assert . assertEquals ( ret . getOwner (  )  ,    currentUser . getShortUserName (  )  )  ;", "fs . delete ( pathUnderTest ,    true )  ;", "}", "METHOD_END"], "methodName": ["testOwnership"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   dir    =    new   Path (  \"  / x \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( dir )  )  ;", "try    {", "fs . open ( dir )  . close (  )  ;", "Assert . assertTrue (  \" Should ' ve   thrown \"  ,    false )  ;", "}    catch    ( NotFoundException   ex )     {", "Assert . assertEquals (  \"  / x   is   a   directory   not   a   file .  \"  ,    ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testReadingDirectoryAsFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "for    ( NativeAzureFileSystemBaseTest . RenameVariation   variation    :    NativeAzureFileSystemBaseTest . RenameVariation . values (  )  )     {", "System . out . printf (  \" Rename   variation :     % s \\ n \"  ,    variation )  ;", "Path   originalFile ;", "switch    ( variation )     {", "case   NormalFileName    :", "originalFile    =    new   Path (  \" fileToRename \"  )  ;", "break ;", "case   SourceInAFolder    :", "originalFile    =    new   Path (  \" file / to / rename \"  )  ;", "break ;", "case   SourceWithSpace    :", "originalFile    =    new   Path (  \" file   to   rename \"  )  ;", "break ;", "case   SourceWithPlusAndPercent    :", "originalFile    =    new   Path (  \" file + to % rename \"  )  ;", "break ;", "default    :", "throw   new   Exception (  \" Unknown   variation \"  )  ;", "}", "Path   destinationFile    =    new   Path (  \" file / resting / destination \"  )  ;", "Assert . assertTrue ( fs . createNewFile ( originalFile )  )  ;", "Assert . assertTrue ( fs . exists ( originalFile )  )  ;", "Assert . assertFalse ( fs . rename ( originalFile ,    destinationFile )  )  ;", "Assert . assertTrue ( fs . mkdirs ( destinationFile . getParent (  )  )  )  ;", "Assert . assertTrue ( fs . rename ( originalFile ,    destinationFile )  )  ;", "Assert . assertTrue ( fs . exists ( destinationFile )  )  ;", "Assert . assertFalse ( fs . exists ( originalFile )  )  ;", "fs . delete ( destinationFile . getParent (  )  ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testRename"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "for    ( NativeAzureFileSystemBaseTest . RenameFolderVariation   variation    :    NativeAzureFileSystemBaseTest . RenameFolderVariation . values (  )  )     {", "Path   originalFolder    =    new   Path (  \" folderToRename \"  )  ;", "if    ( variation    !  =     ( NativeAzureFileSystemBaseTest . RenameFolderVariation . CreateJustInnerFile )  )     {", "Assert . assertTrue ( fs . mkdirs ( originalFolder )  )  ;", "}", "Path   innerFile    =    new   Path ( originalFolder ,     \" innerFile \"  )  ;", "if    ( variation    !  =     ( NativeAzureFileSystemBaseTest . RenameFolderVariation . CreateJustFolder )  )     {", "Assert . assertTrue ( fs . createNewFile ( innerFile )  )  ;", "}", "Path   destination    =    new   Path (  \" renamedFolder \"  )  ;", "Assert . assertTrue ( fs . rename ( originalFolder ,    destination )  )  ;", "Assert . assertTrue ( fs . exists ( destination )  )  ;", "if    ( variation    !  =     ( NativeAzureFileSystemBaseTest . RenameFolderVariation . CreateJustFolder )  )     {", "Assert . assertTrue ( fs . exists ( new   Path ( destination ,    innerFile . getName (  )  )  )  )  ;", "}", "Assert . assertFalse ( fs . exists ( originalFolder )  )  ;", "Assert . assertFalse ( fs . exists ( innerFile )  )  ;", "fs . delete ( destination ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testRenameFolder"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \" deep / file / rename / test \"  )  ;", "FsPermission   permission    =    FsPermission . createImmutable (  (  ( short )     (  6  4  4  )  )  )  ;", "createEmptyFile ( testFile ,    permission )  ;", "Assert . assertTrue ( fs . rename ( new   Path (  \" deep / file \"  )  ,    new   Path (  \" deep / renamed \"  )  )  )  ;", "Assert . assertFalse ( fs . exists ( testFile )  )  ;", "FileStatus   newStatus    =    fs . getFileStatus ( new   Path (  \" deep / renamed / rename / test \"  )  )  ;", "Assert . assertNotNull ( newStatus )  ;", ". assertEqualsIgnoreStickyBit ( permission ,    newStatus . getPermission (  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \" deep \"  )  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["testRenameImplicitFolder"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   newFile    =    new   Path (  \" testOwner \"  )  ;", "OutputStream   output    =    fs . create ( newFile )  ;", "output . write (  1  3  )  ;", "output . close (  )  ;", "fs . setOwner ( newFile ,     \" newUser \"  ,    null )  ;", "tatus   newStatus    =    fs . gettatus ( newFile )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals (  \" newUser \"  ,    newStatus . getOwner (  )  )  ;", "Assert . assertEquals (  \" supergroup \"  ,    newStatus . getGroup (  )  )  ;", "Assert . assertEquals (  1  ,    newStatus . getLen (  )  )  ;", "fs . setOwner ( newFile ,    null ,     \" newGroup \"  )  ;", "newStatus    =    fs . gettatus ( newFile )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals (  \" newUser \"  ,    newStatus . getOwner (  )  )  ;", "Assert . assertEquals (  \" newGroup \"  ,    newStatus . getGroup (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetOwnerOnFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   newFolder    =    new   Path (  \" testOwner \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( newFolder )  )  ;", "fs . setOwner ( newFolder ,     \" newUser \"  ,    null )  ;", "tatus   newStatus    =    fs . gettatus ( newFolder )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals (  \" newUser \"  ,    newStatus . getOwner (  )  )  ;", "Assert . assertTrue ( newStatus . isDirectory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetOwnerOnFolder"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   newFile    =    new   Path (  \" testPermission \"  )  ;", "OutputStream   output    =    fs . create ( newFile )  ;", "output . write (  1  3  )  ;", "output . close (  )  ;", "FsPermission   newPermission    =    new   FsPermission (  (  ( short )     (  4  4  8  )  )  )  ;", "fs . setPermission ( newFile ,    newPermission )  ;", "tatus   newStatus    =    fs . gettatus ( newFile )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals ( newPermission ,    newStatus . getPermission (  )  )  ;", "Assert . assertEquals (  \" supergroup \"  ,    newStatus . getGroup (  )  )  ;", "Assert . assertEquals ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    newStatus . getOwner (  )  )  ;", "Assert . assertEquals (  1  ,    newStatus . getLen (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetPermissionOnFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   newFolder    =    new   Path (  \" testPermission \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( newFolder )  )  ;", "FsPermission   newPermission    =    new   FsPermission (  (  ( short )     (  3  8  4  )  )  )  ;", "fs . setPermission ( newFolder ,    newPermission )  ;", "tatus   newStatus    =    fs . gettatus ( newFolder )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals ( newPermission ,    newStatus . getPermission (  )  )  ;", "Assert . assertTrue ( newStatus . isDirectory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetPermissionOnFolder"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FileSystem . clearStatistics (  )  ;", "FileSystem . Statistics   stats    =    FileSystem . getStatistics (  \" wasb \"  ,     . class )  ;", "Assert . assertEquals (  0  ,    stats . getBytesRead (  )  )  ;", "Assert . assertEquals (  0  ,    stats . getBytesWritten (  )  )  ;", "Path   newFile    =    new   Path (  \" testStats \"  )  ;", "writeString ( newFile ,     \"  1  2  3  4  5  6  7  8  \"  )  ;", "Assert . assertEquals (  8  ,    stats . getBytesWritten (  )  )  ;", "Assert . assertEquals (  0  ,    stats . getBytesRead (  )  )  ;", "String   readBack    =    readString ( newFile )  ;", "Assert . assertEquals (  \"  1  2  3  4  5  6  7  8  \"  ,    readBack )  ;", "Assert . assertEquals (  8  ,    stats . getBytesRead (  )  )  ;", "Assert . assertEquals (  8  ,    stats . getBytesWritten (  )  )  ;", "Assert . assertTrue ( fs . delete ( newFile ,    true )  )  ;", "Assert . assertEquals (  8  ,    stats . getBytesRead (  )  )  ;", "Assert . assertEquals (  8  ,    stats . getBytesWritten (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStatistics"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder    =    new   Path (  \" storeDeleteFolder \"  )  ;", "Assert . assertFalse ( fs . exists ( testFolder )  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder )  )  ;", "Assert . assertTrue ( fs . exists ( testFolder )  )  ;", "tatus   status    =    fs . gettatus ( testFolder )  ;", "Assert . assertNotNull ( status )  ;", "Assert . assertTrue ( status . isDirectory (  )  )  ;", "Assert . assertEquals ( new   FsPermission (  (  ( short )     (  4  9  3  )  )  )  ,    status . getPermission (  )  )  ;", "Path   innerFile    =    new   Path ( testFolder ,     \" innerFile \"  )  ;", "Assert . assertTrue ( fs . createNewFile ( innerFile )  )  ;", "Assert . assertTrue ( fs . exists ( innerFile )  )  ;", "Assert . assertTrue ( fs . delete ( testFolder ,    true )  )  ;", "Assert . assertFalse ( fs . exists ( innerFile )  )  ;", "Assert . assertFalse ( fs . exists ( testFolder )  )  ;", "}", "METHOD_END"], "methodName": ["testStoreDeleteFolder"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \" unit - test - file \"  )  ;", "writeString ( testFile ,     \" Testing \"  )  ;", "Assert . assertTrue ( fs . exists ( testFile )  )  ;", "tatus   status    =    fs . gettatus ( testFile )  ;", "Assert . assertNotNull ( status )  ;", "Assert . assertEquals ( new   FsPermission (  (  ( short )     (  4  2  0  )  )  )  ,    status . getPermission (  )  )  ;", "Assert . assertEquals (  \" Testing \"  ,    readString ( testFile )  )  ;", "fs . delete ( testFile ,    true )  ;", "}", "METHOD_END"], "methodName": ["testStoreRetrieveFile"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "fs . create ( new   Path (  \" p / t %  5 Fe \"  )  )  . close (  )  ;", "tatus [  ]    listing    =    fs . listStatus ( new   Path (  \" p \"  )  )  ;", "Assert . assertEquals (  1  ,    listing . length )  ;", "Assert . assertEquals (  \" t %  5 Fe \"  ,    listing [  0  ]  . getPath (  )  . getName (  )  )  ;", "Assert . assertTrue ( fs . rename ( new   Path (  \" p \"  )  ,    new   Path (  \" q \"  )  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \" q \"  )  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["testUriEncoding"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "String   fileName    =     \"  !  #  $  '  (  )  *  ;  =  [  ]  %  \"  ;", "String   directoryName    =     \"  *  ;  =  [  ]  %  !  #  $  '  (  )  \"  ;", "fs . create ( new   Path ( directoryName ,    fileName )  )  . close (  )  ;", "tatus [  ]    listing    =    fs . listStatus ( new   Path ( directoryName )  )  ;", "Assert . assertEquals (  1  ,    listing . length )  ;", "Assert . assertEquals ( fileName ,    listing [  0  ]  . getPath (  )  . getName (  )  )  ;", "tatus   status    =    fs . gettatus ( new   Path ( directoryName ,    fileName )  )  ;", "Assert . assertEquals ( fileName ,    status . getPath (  )  . getName (  )  )  ;", "InputStream   stream    =    fs . open ( new   Path ( directoryName ,    fileName )  )  ;", "Assert . assertNotNull ( stream )  ;", "stream . close (  )  ;", "Assert . assertTrue ( fs . delete ( new   Path ( directoryName ,    fileName )  ,    true )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path ( directoryName )  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["testUriEncodingMoreComplexCharacters"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "BufferedWriter   writer    =    new   BufferedWriter ( new   OutputStreamWriter ( outputStream )  )  ;", "writer . write ( value )  ;", "writer . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   outputStream    =    fs . create ( path ,    true )  ;", "writeString ( outputStream ,    value )  ;", "outputStream . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "writeString ( fs ,    path ,    value )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.fs.azure.NativeAzureFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   commonPrefixes ;", "}", "METHOD_END"], "methodName": ["getCommonPrefixes"], "fileName": "org.apache.hadoop.fs.azure.PartialListing"}, {"methodBody": ["METHOD_START", "{", "return   files ;", "}", "METHOD_END"], "methodName": ["getFiles"], "fileName": "org.apache.hadoop.fs.azure.PartialListing"}, {"methodBody": ["METHOD_START", "{", "return   priorLastKey ;", "}", "METHOD_END"], "methodName": ["getPriorLastKey"], "fileName": "org.apache.hadoop.fs.azure.PartialListing"}, {"methodBody": ["METHOD_START", "{", "SelfThrottlingIntercept   throttler    =    new   SelfThrottlingIntercept ( operationContext ,    readFactor ,    writeFactor )  ;", "SelfThrottlingIntercept . ResponseReceivedListener   responseListener    =    throttler . new   ResponseReceivedListener (  )  ;", "SelfThrottlingIntercept . SendingRequestListener   sendingListener    =    throttler . new   SendingRequestListener (  )  ;", "operationContext . getResponseReceivedEventHandler (  )  . addListener ( responseListener )  ;", "operationContext . getSendingRequestEventHandler (  )  . addListener ( sendingListener )  ;", "}", "METHOD_END"], "methodName": ["hook"], "fileName": "org.apache.hadoop.fs.azure.SelfThrottlingIntercept"}, {"methodBody": ["METHOD_START", "{", "RequestResult   result    =    event . getRequestResult (  )  ;", "Date   startDate    =    result . getStartDate (  )  ;", "Date   stopDate    =    result . getStopDate (  )  ;", "long   elapsed    =     ( stopDate . getTime (  )  )     -     ( startDate . getTime (  )  )  ;", "synchronized ( this )     {", "this . lastE 2 Elatency    =    elapsed ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", "int   statusCode    =    result . getStatusCode (  )  ;", "String   etag    =    result . getEtag (  )  ;", "HttpURLConnection   urlConnection    =     (  ( HttpURLConnection )     ( event . getConnectionObject (  )  )  )  ;", "int   contentLength    =    urlConnection . getContentLength (  )  ;", "String   requestMethod    =    urlConnection . getRequestMethod (  )  ;", "long   threadId    =    Thread . currentThread (  )  . getId (  )  ;", ". LOG . debug ( String . format (  \"  :  :    ResponseReceived :    threadId =  % d ,    Status =  % d ,    Elapsed ( ms )  =  % d ,    ETAG =  % s ,    contentLength =  % d ,    requestMethod =  % s \"  ,    threadId ,    statusCode ,    elapsed ,    etag ,    contentLength ,    requestMethod )  )  ;", "}", "}", "METHOD_END"], "methodName": ["responseReceived"], "fileName": "org.apache.hadoop.fs.azure.SelfThrottlingIntercept"}, {"methodBody": ["METHOD_START", "{", "long   lastLatency ;", "boolean   operationIsRead ;", "synchronized ( this )     {", "lastLatency    =    this . lastE 2 Elatency ;", "}", "float   sleepMultiple ;", "HttpURLConnection   urlConnection    =     (  ( HttpURLConnection )     ( sendEvent . getConnectionObject (  )  )  )  ;", "if    ( urlConnection . getRequestMethod (  )  . equalsIgnoreCase (  \" PUT \"  )  )     {", "operationIsRead    =    false ;", "sleepMultiple    =     (  1     /     ( writeFactor )  )     -     1  ;", "} else    {", "operationIsRead    =    true ;", "sleepMultiple    =     (  1     /     ( readFactor )  )     -     1  ;", "}", "long   sleepDuration    =     (  ( long )     ( sleepMultiple    *    lastLatency )  )  ;", "if    ( sleepDuration    <     0  )     {", "sleepDuration    =     0  ;", "}", "if    ( sleepDuration    >     0  )     {", "try    {", "Thread . sleep ( sleepDuration )  ;", "}    catch    ( InterruptedException   ie )     {", "Thread . currentThread (  )  . interrupt (  )  ;", "}", "sendEvent . getRequestResult (  )  . setStartDate ( new   Date (  )  )  ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", "boolean   isFirstRequest    =    lastLatency    =  =     0  ;", "long   threadId    =    Thread . currentThread (  )  . getId (  )  ;", ". LOG . debug ( String . format (  \"     :  :    SendingRequest :          threadId =  % d ,    requestType =  % s ,    isFirstRequest =  % b ,    sleepDuration =  % d \"  ,    threadId ,     ( operationIsRead    ?     \" read    \"     :     \" write \"  )  ,    isFirstRequest ,    sleepDuration )  )  ;", "}", "}", "METHOD_END"], "methodName": ["sendingRequest"], "fileName": "org.apache.hadoop.fs.azure.SelfThrottlingIntercept"}, {"methodBody": ["METHOD_START", "{", "SendRequestIntercept   sendListener    =    new   SendRequestIntercept ( storageCreds ,    allowConcurrentOOBIo ,    opContext )  ;", "opContext . getSendingRequestEventHandler (  )  . addListener ( sendListener )  ;", "}", "METHOD_END"], "methodName": ["bind"], "fileName": "org.apache.hadoop.fs.azure.SendRequestIntercept"}, {"methodBody": ["METHOD_START", "{", "return   storageCreds ;", "}", "METHOD_END"], "methodName": ["getCredentials"], "fileName": "org.apache.hadoop.fs.azure.SendRequestIntercept"}, {"methodBody": ["METHOD_START", "{", "return   opContext ;", "}", "METHOD_END"], "methodName": ["getOperationContext"], "fileName": "org.apache.hadoop.fs.azure.SendRequestIntercept"}, {"methodBody": ["METHOD_START", "{", "return   allowConcurrentOOBIo ;", "}", "METHOD_END"], "methodName": ["isOutOfBandIoAllowed"], "fileName": "org.apache.hadoop.fs.azure.SendRequestIntercept"}, {"methodBody": ["METHOD_START", "{", "return    ( SimpleKeyProvider . KEY _ ACCOUNT _ KEY _ PREFIX )     +    accountName ;", "}", "METHOD_END"], "methodName": ["getStorageAccountKeyName"], "fileName": "org.apache.hadoop.fs.azure.SimpleKeyProvider"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createOutOfBandStore ( TestAzureConcurrentOutOfBandIo . UPLOAD _ BLOCK _ SIZE ,    TestAzureConcurrentOutOfBandIo . DOWNLOAD _ BLOCK _ SIZE )  ;", "Assume . assumeNotNull ( testAccount )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestAzureConcurrentOutOfBandIo"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestAzureConcurrentOutOfBandIo"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    dataBlockWrite    =    new   byte [ TestAzureConcurrentOutOfBandIo . UPLOAD _ BLOCK _ SIZE ]  ;", "byte [  ]    dataBlockRead    =    new   byte [ TestAzureConcurrentOutOfBandIo . UPLOAD _ BLOCK _ SIZE ]  ;", "DataOutputStream   outputStream    =    testAccount . getStore (  )  . storefile (  \" WASB _ String . txt \"  ,    new   PermissionStatus (  \"  \"  ,     \"  \"  ,    FsPermission . getDefault (  )  )  )  ;", "Arrays . fill ( dataBlockWrite ,     (  ( byte )     (  2  5  5  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( TestAzureConcurrentOutOfBandIo . NUMBER _ OF _ BLOCKS )  ;    i +  +  )     {", "outputStream . write ( dataBlockWrite )  ;", "}", "outputStream . flush (  )  ;", "outputStream . close (  )  ;", "TestAzureConcurrentOutOfBandIo . DataBlockWriter   writeBlockTask    =    new   TestAzureConcurrentOutOfBandIo . DataBlockWriter ( testAccount ,     \" WASB _ String . txt \"  )  ;", "writeBlockTask . startWriting (  )  ;", "int   count    =     0  ;", "DataInputStream   inputStream    =    null ;", "for    ( int   i    =     0  ;    i    <     5  ;    i +  +  )     {", "try    {", "inputStream    =    testAccount . getStore (  )  . retrieve (  \" WASB _ String . txt \"  ,     0  )  ;", "count    =     0  ;", "int   c    =     0  ;", "while    ( c    >  =     0  )     {", "c    =    inputStream . read ( dataBlockRead ,     0  ,    TestAzureConcurrentOutOfBandIo . UPLOAD _ BLOCK _ SIZE )  ;", "if    ( c    <     0  )     {", "break ;", "}", "count    +  =    c ;", "}", "}    catch    ( IOException   e )     {", "System . out . println ( e . getCause (  )  . toString (  )  )  ;", "e . printStackTrace (  )  ;", "Assert . fail (  )  ;", "}", "if    ( null    !  =    inputStream )     {", "inputStream . close (  )  ;", "}", "}", "writeBlockTask . stopWriting (  )  ;", "Assert . assertEquals (  (  ( TestAzureConcurrentOutOfBandIo . NUMBER _ OF _ BLOCKS )     *     ( TestAzureConcurrentOutOfBandIo . UPLOAD _ BLOCK _ SIZE )  )  ,    count )  ;", "}", "METHOD_END"], "methodName": ["testReadOOBWrites"], "fileName": "org.apache.hadoop.fs.azure.TestAzureConcurrentOutOfBandIo"}, {"methodBody": ["METHOD_START", "{", "fs . getStore (  )  . addTestHookToOperationContext ( new   AzureNativeFileSystemStore . TestHookOperationContext (  )     {", "@ Override", "public   OperationContext   modifyOperationContext ( OperationContext   original )     {", "original . getSendingRequestEventHandler (  )  . addListener ( new    . TransientErrorInjector ( connectionRecognizer )  )  ;", "return   original ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["injectTransientError"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buffer    =    new   byte [ TestAzureFileSystemErrorConditions . ALL _ THREE _ FILE _ SIZE ]  ;", "InputStream   inStream    =    fs . open ( testFile )  ;", "Assert . assertEquals ( buffer . length ,    inStream . read ( buffer ,     0  ,    buffer . length )  )  ;", "inStream . close (  )  ;", "for    ( int   i    =     0  ;    i    <     ( buffer . length )  ;    i +  +  )     {", "Assert . assertEquals (  3  ,    buffer [ i ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["readAllThreeFile"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "AzureNativeFileSystemStore   store    =    new   AzureNativeFileSystemStore (  )  ;", "MockStorageInterface   mockStorage    =    new   MockStorageInterface (  )  ;", "store . setAzureStorageInteractionLayer ( mockStorage )  ;", "FileSystem   fs    =    new   Native ( store )  ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "AzureBlobStorageTestAccount . setMockAccountKey ( conf )  ;", "HashMap < String ,    String >    metadata    =    new   HashMap < String ,    String >  (  )  ;", "metadata . put ( AzureNativeFileSystemStore . VERSION _ METADATA _ KEY ,     \"  2  0  9  0  -  0  4  -  0  5  \"  )  ;", "mockStorage . addPreExistingContainer ( AzureBlobStorageTestAccount . getMockContainerUri (  )  ,    metadata )  ;", "boolean   passed    =    false ;", "try    {", "fs . initialize ( new   URI ( AzureBlobStorageTestAccount . MOCK _ WASB _ URI )  ,    conf )  ;", "fs . listStatus ( new   Path (  \"  /  \"  )  )  ;", "passed    =    true ;", "}    catch    ( AzureException   ex )     {", "Assert . assertTrue (  (  \" Unexpected   exception   message :     \"     +    ex )  ,    ex . getMessage (  )  . contains (  \" unsupported   version :     2  0  9  0  -  0  4  -  0  5  .  \"  )  )  ;", "}", "Assert . assertFalse (  \" Should ' ve   thrown   an   exception   because   of   the   wrong   version .  \"  ,    passed )  ;", "}    finally    {", "fs . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAccessContainerWithWrongVersion"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "AzureBlobStorageTestAccount . addWasbToConfiguration ( conf )  ;", "Path   noAccessPath    =    new   Path (  \" wasb :  /  / nonExistentContainer @ hopefullyNonExistentAccount / someFile \"  )  ;", "Native . suppressRetryPolicy (  )  ;", "try    {", "FileSystem . get ( noAccessPath . toUri (  )  ,    conf )  . open ( noAccessPath )  ;", "Assert . assertTrue (  \" Should ' ve   thrown .  \"  ,    false )  ;", "}    catch    ( AzureException   ex )     {", "Assert . assertTrue (  (  \" Unexpected   message   in   exception    \"     +    ex )  ,    ex . getMessage (  )  . contains (  (  \" Unable   to   access   container   nonExistentContainer   in   account \"     +     \"    hopefullyNonExistentAccount \"  )  )  )  ;", "}    finally    {", "Native . resumeRetryPolicy (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAccessUnauthorizedPublicContainer"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem   fs    =    AzureBlobStorageTestAccount . createMock (  )  . getFileSystem (  )  ;", "Path   testFile    =    new   Path (  \"  / testErrorDuringRetrieve \"  )  ;", "writeAllThreeFile ( fs ,    testFile )  ;", "FSDataInputStream   stream    =    fs . open ( testFile )  ;", "stream . seek ( Integer . MAX _ VALUE )  ;", "}", "METHOD_END"], "methodName": ["testErrorDuringRetrieve"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "AzureNativeFileSystemStore   store    =    new   AzureNativeFileSystemStore (  )  ;", "boolean   passed    =    false ;", "try    {", "store . retrieveMetadata (  \" foo \"  )  ;", "passed    =    true ;", "}    catch    ( AssertionError   e )     {", "}", "Assert . assertFalse (  \" Doing   an   operation   on   the   store   should   throw   if   not   initalized .  \"  ,    passed )  ;", "}", "METHOD_END"], "methodName": ["testNoInitialize"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount   testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "try    {", "NativeAzureFileSystem   fs    =    testAccount . getFileSystem (  )  ;", "injectTransientError ( fs ,    new    . ConnectionRecognizer (  )     {", "@ Override", "public   boolean   isTargetConnection ( HttpURLConnection   connection )     {", "return    ( connection . getRequestMethod (  )  . equals (  \" PUT \"  )  )     &  &     ( connection . getURL (  )  . getQuery (  )  . contains (  \" blocklist \"  )  )  ;", "}", "}  )  ;", "Path   testFile    =    new   Path (  \"  / a / b \"  )  ;", "writeAllThreeFile ( fs ,    testFile )  ;", "readAllThreeFile ( fs ,    testFile )  ;", "}    finally    {", "testAccount . cleanup (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testTransientErrorOnCommitBlockList"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount   testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "try    {", "NativeAzureFileSystem   fs    =    testAccount . getFileSystem (  )  ;", "injectTransientError ( fs ,    new    . ConnectionRecognizer (  )     {", "@ Override", "public   boolean   isTargetConnection ( HttpURLConnection   connection )     {", "return   connection . getRequestMethod (  )  . equals (  \" DELETE \"  )  ;", "}", "}  )  ;", "Path   testFile    =    new   Path (  \"  / a / b \"  )  ;", "Assert . assertTrue ( fs . createNewFile ( testFile )  )  ;", "Assert . assertTrue ( fs . rename ( testFile ,    new   Path (  \"  / x \"  )  )  )  ;", "}    finally    {", "testAccount . cleanup (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testTransientErrorOnDelete"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount   testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "try    {", "NativeAzureFileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Path   testFile    =    new   Path (  \"  / a / b \"  )  ;", "writeAllThreeFile ( fs ,    testFile )  ;", "injectTransientError ( fs ,    new    . ConnectionRecognizer (  )     {", "@ Override", "public   boolean   isTargetConnection ( HttpURLConnection   connection )     {", "return   connection . getRequestMethod (  )  . equals (  \" GET \"  )  ;", "}", "}  )  ;", "readAllThreeFile ( fs ,    testFile )  ;", "}    finally    {", "testAccount . cleanup (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testTransientErrorOnRead"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buffer    =    new   byte [ TestAzureFileSystemErrorConditions . ALL _ THREE _ FILE _ SIZE ]  ;", "Arrays . fill ( buffer ,     (  ( byte )     (  3  )  )  )  ;", "OutputStream   stream    =    fs . create ( testFile )  ;", "stream . write ( buffer )  ;", "stream . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeAllThreeFile"], "fileName": "org.apache.hadoop.fs.azure.TestAzureFileSystemErrorConditions"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "testStoreBlobMd 5  ( false )  ;", "}", "METHOD_END"], "methodName": ["testBlobMd5StoreOffByDefault"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "testCheckBlockMd 5  ( true )  ;", "}", "METHOD_END"], "methodName": ["testCheckBlockMd5"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeNotNull ( testAccount )  ;", "Path   testFilePath    =    new   Path (  \"  / testFile \"  )  ;", "testAccount . getFileSystem (  )  . getStore (  )  . addTestHookToOperationContext ( new   AzureNativeFileSystemStore . TestHookOperationContext (  )     {", "@ Override", "public   OperationContext   modifyOperationContext ( OperationContext   original )     {", "original . getResponseReceivedEventHandler (  )  . addListener ( new    . ContentMD 5 Checker ( expectMd 5 Checked )  )  ;", "return   original ;", "}", "}  )  ;", "OutputStream   outStream    =    testAccount . getFileSystem (  )  . create ( testFilePath )  ;", "outStream . write ( new   byte [  ]  {     5  ,     1  5     }  )  ;", "outStream . close (  )  ;", "InputStream   inStream    =    testAccount . getFileSystem (  )  . open ( testFilePath )  ;", "byte [  ]    inBuf    =    new   byte [  1  0  0  ]  ;", "while    (  ( inStream . read ( inBuf )  )     >     0  )     {", "}", "inStream . close (  )  ;", "}", "METHOD_END"], "methodName": ["testCheckBlockMd5"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setBoolean ( AzureNativeFileSystemStore . KEY _ CHECK _ BLOCK _ MD 5  ,    false )  ;", "testAccount    =    AzureBlobStorageTestAccount . create ( conf )  ;", "testCheckBlockMd 5  ( false )  ;", "}", "METHOD_END"], "methodName": ["testDontCheckBlockMd5"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setBoolean ( AzureNativeFileSystemStore . KEY _ STORE _ BLOB _ MD 5  ,    true )  ;", "testAccount    =    AzureBlobStorageTestAccount . create ( conf )  ;", "testStoreBlobMd 5  ( true )  ;", "}", "METHOD_END"], "methodName": ["testStoreBlobMd5"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeNotNull ( testAccount )  ;", "String   testFileKey    =     \" testFile \"  ;", "Path   testFilePath    =    new   Path (  (  \"  /  \"     +    testFileKey )  )  ;", "OutputStream   outStream    =    testAccount . getFileSystem (  )  . create ( testFilePath )  ;", "outStream . write ( new   byte [  ]  {     5  ,     1  5     }  )  ;", "outStream . close (  )  ;", "CloudBlockBlob   blob    =    testAccount . geReference ( testFileKey )  ;", "blob . downloadAttributes (  )  ;", "String   obtainedMd 5     =    blob . getProperties (  )  . getContentMD 5  (  )  ;", "if    ( expectMd 5 Stored )     {", "Assert . assertNotNull ( obtainedMd 5  )  ;", "} else    {", "Assert . assertNull (  (  \" Expected   no   MD 5  ,    found :     \"     +    obtainedMd 5  )  ,    obtainedMd 5  )  ;", "}", "String   newBlockId    =    Base 6  4  . encode ( new   byte [  ]  {     5  5  ,     4  4  ,     3  3  ,     2  2     }  )  ;", "blob . uploadBlock ( newBlockId ,    new   ByteArrayInputStream ( new   byte [  ]  {     6  ,     4  5     }  )  ,     2  )  ;", "blob . commitBlockList ( Arrays . asList ( new   BlockEntry [  ]  {    new   BlockEntry ( newBlockId ,    BlockSearchMode . UNCOMMITTED )     }  )  )  ;", "InputStream   inStream    =    testAccount . getFileSystem (  )  . open ( testFilePath )  ;", "try    {", "byte [  ]    inBuf    =    new   byte [  1  0  0  ]  ;", "while    (  ( inStream . read ( inBuf )  )     >     0  )     {", "}", "inStream . close (  )  ;", "if    ( expectMd 5 Stored )     {", "Assert . fail (  \" Should ' ve   thrown   because   of   data   corruption .  \"  )  ;", "}", "}    catch    ( IOException   ex )     {", "if    (  ! expectMd 5 Stored )     {", "throw   ex ;", "}", "StorageException   cause    =     (  ( StorageException )     ( ex . getCause (  )  )  )  ;", "Assert . assertNotNull ( cause )  ;", "Assert . assertTrue (  (  \" Unexpected   cause :     \"     +    cause )  ,    cause . getErrorCode (  )  . equals ( INVALID _ MD 5  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStoreBlobMd5"], "fileName": "org.apache.hadoop.fs.azure.TestBlobDataValidation"}, {"methodBody": ["METHOD_START", "{", "return   UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ;", "}", "METHOD_END"], "methodName": ["getExpectedOwner"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \"  {  \\  \" owner \\  \"  :  \\  \"  % s \\  \"  ,  \\  \" group \\  \"  :  \\  \"  % s \\  \"  ,  \\  \" permissions \\  \"  :  \\  \"  % s \\  \"  }  \"  ,    TestBlobMetadata . getExpectedOwner (  )  ,    NativeAzureFileSystem . AZURE _ DEFAULT _ GROUP _ DEFAULT ,    permissionString )  ;", "}", "METHOD_END"], "methodName": ["getExpectedPermissionString"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "fs    =    testAccount . getFileSystem (  )  ;", "backingStore    =    testAccount . getMockStorage (  )  . getBackingStore (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "testAccount . cleanup (  )  ;", "fs    =    null ;", "backingStore    =    null ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "fs . createNewFile ( new   Path (  \"  / foo \"  )  )  ;", "HashMap < String ,    String >    container    =    backingStore . getContainer (  )  ;", "Assert . assertNotNull ( container )  ;", "Assert . assertEquals ( AzureNativeFileSystemStore . CURRENT _ WASB _ VERSION ,    container . get ( AzureNativeFileSystemStore . VERSION _ METADATA _ KEY )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerVersionMetadata"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    String >    containerMetadata    =    new   HashMap < String ,    String >  (  )  ;", "containerMetadata . put ( AzureNativeFileSystemStore . OLD _ VERSION _ METADATA _ KEY ,    AzureNativeFileSystemStore . FIRST _ WASB _ VERSION )  ;", ". FsWithPreExistingContainer   fsWithContainer    =     . FsWithPreExistingContainer . create ( containerMetadata )  ;", "Assert . assertFalse ( fsWithContainer . getFs (  )  . exists ( new   Path (  \"  / IDontExist \"  )  )  )  ;", "Assert . assertEquals (  0  ,    fsWithContainer . getFs (  )  . listStatus ( new   Path (  \"  /  \"  )  )  . length )  ;", "Assert . assertEquals ( AzureNativeFileSystemStore . FIRST _ WASB _ VERSION ,    fsWithContainer . getContainerMetadata (  )  . get ( AzureNativeFileSystemStore . OLD _ VERSION _ METADATA _ KEY )  )  ;", "Assert . assertNull ( fsWithContainer . getContainerMetadata (  )  . get ( AzureNativeFileSystemStore . VERSION _ METADATA _ KEY )  )  ;", "fsWithContainer . getFs (  )  . mkdirs ( new   Path (  \"  / dir \"  )  )  ;", "Assert . assertEquals ( AzureNativeFileSystemStore . CURRENT _ WASB _ VERSION ,    fsWithContainer . getContainerMetadata (  )  . get ( AzureNativeFileSystemStore . VERSION _ METADATA _ KEY )  )  ;", "Assert . assertNull ( fsWithContainer . getContainerMetadata (  )  . get ( AzureNativeFileSystemStore . OLD _ VERSION _ METADATA _ KEY )  )  ;", "fsWithContainer . close (  )  ;", "}", "METHOD_END"], "methodName": ["testFirstContainerVersionMetadata"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "Path   folder    =    new   Path (  \"  / folder \"  )  ;", "FsPermission   justRead    =    new   FsPermission ( FsAction . READ ,    FsAction . READ ,    FsAction . READ )  ;", "fs . mkdirs ( folder ,    justRead )  ;", "HashMap < String ,    String >    metadata    =    backingStore . getMetadata ( AzureBlobStorageTestAccount . toMockUri ( folder )  )  ;", "Assert . assertNotNull ( metadata )  ;", "Assert . assertEquals (  \" true \"  ,    metadata . get (  \" hdi _ isfolder \"  )  )  ;", "Assert . assertEquals (  . getExpectedPermissionString (  \" r -  - r -  - r -  -  \"  )  ,    metadata . get (  \" hdi _ permission \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testFolderMetadata"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "Path   selfishFile    =    new   Path (  \"  / noOneElse \"  )  ;", "HashMap < String ,    String >    metadata    =    new   HashMap < String ,    String >  (  )  ;", "metadata . put (  \" asv _ permission \"  ,     . getExpectedPermissionString (  \" rw -  -  -  -  -  -  -  \"  )  )  ;", "backingStore . setContent ( AzureBlobStorageTestAccount . toMockUri ( selfishFile )  ,    new   byte [  ]  {        }  ,    metadata )  ;", "FsPermission   justMe    =    new   FsPermission ( FsAction . READ _ WRITE ,    FsAction . NONE ,    FsAction . NONE )  ;", "FileStatus   retrievedStatus    =    fs . getFileStatus ( selfishFile )  ;", "Assert . assertNotNull ( retrievedStatus )  ;", "Assert . assertEquals ( justMe ,    retrievedStatus . getPermission (  )  )  ;", "Assert . assertEquals (  . getExpectedOwner (  )  ,    retrievedStatus . getOwner (  )  )  ;", "Assert . assertEquals ( NativeAzureFileSystem . AZURE _ DEFAULT _ GROUP _ DEFAULT ,    retrievedStatus . getGroup (  )  )  ;", "FsPermission   meAndYou    =    new   FsPermission ( FsAction . READ _ WRITE ,    FsAction . READ _ WRITE ,    FsAction . NONE )  ;", "fs . setPermission ( selfishFile ,    meAndYou )  ;", "metadata    =    backingStore . getMetadata ( AzureBlobStorageTestAccount . toMockUri ( selfishFile )  )  ;", "Assert . assertNotNull ( metadata )  ;", "String   storedPermission    =    metadata . get (  \" hdi _ permission \"  )  ;", "Assert . assertEquals (  . getExpectedPermissionString (  \" rw - rw -  -  -  -  \"  )  ,    storedPermission )  ;", "Assert . assertNull ( metadata . get (  \" asv _ permission \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testOldPermissionMetadata"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "FsPermission   justMe    =    new   FsPermission ( FsAction . READ _ WRITE ,    FsAction . NONE ,    FsAction . NONE )  ;", "Path   selfishFile    =    new   Path (  \"  / noOneElse \"  )  ;", "fs . create ( selfishFile ,    justMe ,    true ,     4  0  9  6  ,    fs . getDefaultReplication (  )  ,    fs . getDefaultBlockSize (  )  ,    null )  . close (  )  ;", "HashMap < String ,    String >    metadata    =    backingStore . getMetadata ( AzureBlobStorageTestAccount . toMockUri ( selfishFile )  )  ;", "Assert . assertNotNull ( metadata )  ;", "String   storedPermission    =    metadata . get (  \" hdi _ permission \"  )  ;", "Assert . assertEquals (  . getExpectedPermissionString (  \" rw -  -  -  -  -  -  -  \"  )  ,    storedPermission )  ;", "FileStatus   retrievedStatus    =    fs . getFileStatus ( selfishFile )  ;", "Assert . assertNotNull ( retrievedStatus )  ;", "Assert . assertEquals ( justMe ,    retrievedStatus . getPermission (  )  )  ;", "Assert . assertEquals (  . getExpectedOwner (  )  ,    retrievedStatus . getOwner (  )  )  ;", "Assert . assertEquals ( NativeAzureFileSystem . AZURE _ DEFAULT _ GROUP _ DEFAULT ,    retrievedStatus . getGroup (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPermissionMetadata"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "TestBlobMetadata . FsWithPreExistingContainer   fsWithContainer    =    TestBlobMetadata . FsWithPreExistingContainer . create (  )  ;", "Assert . assertFalse ( fsWithContainer . getFs (  )  . exists ( new   Path (  \"  / IDontExist \"  )  )  )  ;", "Assert . assertEquals (  0  ,    fsWithContainer . getFs (  )  . listStatus ( new   Path (  \"  /  \"  )  )  . length )  ;", "Assert . assertNull ( fsWithContainer . getContainerMetadata (  )  )  ;", "fsWithContainer . getFs (  )  . mkdirs ( new   Path (  \"  / dir \"  )  )  ;", "Assert . assertNotNull ( fsWithContainer . getContainerMetadata (  )  )  ;", "Assert . assertEquals ( AzureNativeFileSystemStore . CURRENT _ WASB _ VERSION ,    fsWithContainer . getContainerMetadata (  )  . get ( AzureNativeFileSystemStore . VERSION _ METADATA _ KEY )  )  ;", "fsWithContainer . close (  )  ;", "}", "METHOD_END"], "methodName": ["testPreExistingContainerVersionMetadata"], "fileName": "org.apache.hadoop.fs.azure.TestBlobMetadata"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestContainerChecks"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . of ( AzureBlobStorageTestAccount . CreateOptions . UseSas )  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "CloudBlob   container    =    testAccount . getReal (  )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Assert . assertFalse ( container . exists (  )  )  ;", "try    {", "fs . createNewFile ( new   Path (  \"  / foo \"  )  )  ;", "Assert . assertFalse (  \" Should ' ve   thrown .  \"  ,    true )  ;", "}    catch    ( AzureException   ex )     {", "}", "Assert . assertFalse ( container . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerChecksWithSas"], "fileName": "org.apache.hadoop.fs.azure.TestContainerChecks"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . noneOf ( AzureBlobStorageTestAccount . CreateOptions . class )  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "CloudBlob   container    =    testAccount . getReal (  )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Assert . assertFalse ( container . exists (  )  )  ;", "try    {", "Assert . assertNull ( fs . listStatus ( new   Path (  \"  /  \"  )  )  )  ;", "Assert . assertTrue (  \" Should ' ve   thrown .  \"  ,    false )  ;", "}    catch    ( FileNotFoundException   ex )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +    ex )  ,    ex . getMessage (  )  . contains (  \" does   not   exist .  \"  )  )  ;", "}", "Assert . assertFalse ( container . exists (  )  )  ;", "container . create (  )  ;", "Assert . assertTrue ( fs . createNewFile ( new   Path (  \"  / foo \"  )  )  )  ;", "Assert . assertTrue ( container . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerCreateAfterDoesNotExist"], "fileName": "org.apache.hadoop.fs.azure.TestContainerChecks"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . noneOf ( AzureBlobStorageTestAccount . CreateOptions . class )  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "CloudBlob   container    =    testAccount . getReal (  )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Assert . assertFalse ( container . exists (  )  )  ;", "try    {", "fs . listStatus ( new   Path (  \"  /  \"  )  )  ;", "Assert . assertTrue (  \" Should ' ve   thrown .  \"  ,    false )  ;", "}    catch    ( FileNotFoundException   ex )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +    ex )  ,    ex . getMessage (  )  . contains (  \" does   not   exist .  \"  )  )  ;", "}", "Assert . assertFalse ( container . exists (  )  )  ;", "try    {", "fs . open ( new   Path (  \"  / foo \"  )  )  ;", "Assert . assertFalse (  \" Should ' ve   thrown .  \"  ,    true )  ;", "}    catch    ( FileNotFoundException   ex )     {", "}", "Assert . assertFalse ( container . exists (  )  )  ;", "Assert . assertFalse ( fs . rename ( new   Path (  \"  / foo \"  )  ,    new   Path (  \"  / bar \"  )  )  )  ;", "Assert . assertFalse ( container . exists (  )  )  ;", "Assert . assertTrue ( fs . createNewFile ( new   Path (  \"  / foo \"  )  )  )  ;", "Assert . assertTrue ( container . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerCreateOnWrite"], "fileName": "org.apache.hadoop.fs.azure.TestContainerChecks"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . noneOf ( AzureBlobStorageTestAccount . CreateOptions . class )  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "CloudBlob   container    =    testAccount . getReal (  )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Assert . assertFalse ( container . exists (  )  )  ;", "try    {", "fs . listStatus ( new   Path (  \"  /  \"  )  )  ;", "Assert . assertTrue (  \" Should ' ve   thrown .  \"  ,    false )  ;", "}    catch    ( FileNotFoundException   ex )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +    ex )  ,    ex . getMessage (  )  . contains (  \" does   not   exist .  \"  )  )  ;", "}", "Assert . assertFalse ( container . exists (  )  )  ;", "container . create (  )  ;", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  \" foo \"  )  ;", "BlobOutputStream   outputStream    =    blob . openOutputStream (  )  ;", "outputStream . write ( new   byte [  1  0  ]  )  ;", "outputStream . close (  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / foo \"  )  )  )  ;", "Assert . assertTrue ( container . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerExistAfterDoesNotExist"], "fileName": "org.apache.hadoop.fs.azure.TestContainerChecks"}, {"methodBody": ["METHOD_START", "{", "Path   testFile    =    new   Path (  \"  / testFile \"  )  ;", "OutputStream   outputStream    =    fs . create ( testFile )  ;", "outputStream . write ( new   byte [ size ]  )  ;", "outputStream . close (  )  ;", "return   testFile ;", "}", "METHOD_END"], "methodName": ["createTestFile"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "return   TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput ( fileSize ,    blockSize ,    start ,    len ,    null )  ;", "}", "METHOD_END"], "methodName": ["getBlockLocationsOutput"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NativeAzureFileSystem . AZURE _ BLOCK _ SIZE _ PROPERTY _ NAME ,     (  \"  \"     +    blockSize )  )  ;", "if    ( blockLocationHost    !  =    null )     {", "conf . set ( NativeAzureFileSystem . AZURE _ BLOCK _ LOCATION _ HOST _ PROPERTY _ NAME ,    blockLocationHost )  ;", "}", "AzureBlobStorageTestAccount   testAccount    =    AzureBlobStorageTestAccount . createMock ( conf )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Path   testFile    =     . createTestFile ( fs ,    fileSize )  ;", "FileStatus   stat    =    fs . getFileStatus ( testFile )  ;", "BlockLocation [  ]    locations    =    fs . getFileBlockLocations ( stat ,    start ,    len )  ;", "testAccount . cleanup (  )  ;", "return   locations ;", "}", "METHOD_END"], "methodName": ["getBlockLocationsOutput"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  1  0  0  ,     1  0  ,     0  ,     1  0  0  ,     \" myblobhost \"  )  ;", "Assert . assertEquals (  1  0  ,    locations . length )  ;", "Assert . assertEquals (  \" myblobhost \"  ,    locations [  0  ]  . getHosts (  )  [  0  ]  )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsDifferentLocationHost"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  0  ,     5  0  ,     0  ,     0  )  ;", "Assert . assertEquals (  0  ,    locations . length )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsEmptyFile"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  2  0  5  ,     1  0  ,     0  ,     0  )  ;", "Assert . assertEquals (  0  ,    locations . length )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsEmptySubsetOfFile"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  2  0  0  ,     5  0  ,     0  ,     2  0  0  )  ;", "Assert . assertEquals (  4  ,    locations . length )  ;", "Assert . assertEquals (  1  5  0  ,    locations [  3  ]  . getOffset (  )  )  ;", "Assert . assertEquals (  5  0  ,    locations [  3  ]  . getLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsExactBlockSizeMultiple"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  2  0  5  ,     1  0  ,     3  0  0  ,     1  0  )  ;", "Assert . assertEquals (  0  ,    locations . length )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsOutOfRangeSubsetOfFile"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  1  ,     5  0  ,     0  ,     1  )  ;", "Assert . assertEquals (  1  ,    locations . length )  ;", "Assert . assertEquals (  1  ,    locations [  0  ]  . getLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsSmallFile"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  2  0  5  ,     1  0  ,     1  5  ,     3  5  )  ;", "Assert . assertEquals (  4  ,    locations . length )  ;", "Assert . assertEquals (  1  0  ,    locations [  0  ]  . getLength (  )  )  ;", "Assert . assertEquals (  1  5  ,    locations [  0  ]  . getOffset (  )  )  ;", "Assert . assertEquals (  5  ,    locations [  3  ]  . getLength (  )  )  ;", "Assert . assertEquals (  4  5  ,    locations [  3  ]  . getOffset (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsSubsetOfFile"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "BlockLocation [  ]    locations    =    TestNativeAzureFileSystemBlockLocations . getBlockLocationsOutput (  2  1  0  ,     5  0  ,     0  ,     2  1  0  )  ;", "Assert . assertEquals (  5  ,    locations . length )  ;", "Assert . assertEquals (  \" localhost \"  ,    locations [  0  ]  . getHosts (  )  [  0  ]  )  ;", "Assert . assertEquals (  5  0  ,    locations [  0  ]  . getLength (  )  )  ;", "Assert . assertEquals (  1  0  ,    locations [  4  ]  . getLength (  )  )  ;", "Assert . assertEquals (  1  0  0  ,    locations [  2  ]  . getOffset (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBlockLocationsTypical"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NativeAzureFileSystem . AZURE _ BLOCK _ SIZE _ PROPERTY _ NAME ,     \"  5  0  0  \"  )  ;", "AzureBlobStorageTestAccount   testAccount    =    AzureBlobStorageTestAccount . createMock ( conf )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Path   testFile    =     . createTestFile ( fs ,     1  2  0  0  )  ;", "FileStatus   stat    =    fs . getFileStatus ( testFile )  ;", "Assert . assertEquals (  5  0  0  ,    stat . getBlockSize (  )  )  ;", "testAccount . cleanup (  )  ;", "}", "METHOD_END"], "methodName": ["testNumberOfBlocks"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemBlockLocations"}, {"methodBody": ["METHOD_START", "{", "return   new   Iterable < String >  (  )     {", "@ Override", "public   Iterator < String >    iterator (  )     {", "final   Iterator < Throwable >    exceptionIterator    =    collection . iterator (  )  ;", "return   new   Iterator < String >  (  )     {", "@ Override", "public   booleansNext (  )     {", "return   exceptionIteratorsNext (  )  ;", "}", "@ Override", "public   String   next (  )     {", "StringWriter   stringWriter    =    new   StringWriter (  )  ;", "PrintWriter   printWriter    =    new   PrintWriter ( stringWriter )  ;", "exceptionIterator . next (  )  . printStackTrace ( printWriter )  ;", "printWriter . close (  )  ;", "return   stringWriter . toString (  )  ;", "}", "@ Override", "public   void   remove (  )     {", "exceptionIterator . remove (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["selectToString"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "fs    =    testAccount . get (  )  ;", "backingStore    =    testAccount . getMockStorage (  )  . getBackingStore (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "testAccount . cleanup (  )  ;", "fs    =    null ;", "backingStore    =    null ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "Path   filePath    =    new   Path (  \"  / inProgress \"  )  ;", "FSDataOutputStream   outputStream    =    fs . create ( filePath )  ;", "HashMap < String ,    String >    metadata    =    backingStore . getMetadata ( AzureBlobStorageTestAccount . toMockUri ( filePath )  )  ;", "Assert . assertNotNull ( metadata )  ;", "String   linkValue    =    metadata . get ( AzureNativStore . LINK _ BACK _ TO _ UPLOAD _ IN _ PROGRESS _ METADATA _ KEY )  ;", "Assert . assertNotNull ( linkValue )  ;", "Assert . assertTrue ( backingStore . exists ( AzureBlobStorageTestAccount . toMockUri ( linkValue )  )  )  ;", "Assert . assertTrue ( fs . exists ( filePath )  )  ;", "outputStream . close (  )  ;", "metadata    =    backingStore . getMetadata ( AzureBlobStorageTestAccount . toMockUri ( filePath )  )  ;", "Assert . assertNull ( metadata . get ( AzureNativStore . LINK _ BACK _ TO _ UPLOAD _ IN _ PROGRESS _ METADATA _ KEY )  )  ;", "}", "METHOD_END"], "methodName": ["testLinkBlobs"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "for    ( int   iter    =     0  ;    iter    <     1  0  ;    iter +  +  )     {", "final   int   numThreads    =     2  0  ;", "Thread [  ]    threads    =    new   Thread [ numThreads ]  ;", "final   ConcurrentLinkedQueue < Throwable >    exceptionsEncountered    =    new   ConcurrentLinkedQueue < Throwable >  (  )  ;", "for    ( int   i    =     0  ;    i    <    numThreads ;    i +  +  )     {", "final   Path   threadLocalFile    =    new   Path (  (  \"  / myFile \"     +    i )  )  ;", "threads [ i ]     =    new   Thread ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "Assert . assertTrue (  (  !  ( fs . exists ( threadLocalFile )  )  )  )  ;", "OutputStream   output    =    fs . create ( threadLocalFile )  ;", "output . write (  5  )  ;", "output . close (  )  ;", "Assert . assertTrue ( fs . exists ( threadLocalFile )  )  ;", "Assert . assertTrue (  (  ( fs . listStatus ( new   Path (  \"  /  \"  )  )  . length )     >     0  )  )  ;", "}    catch    ( Throwable   ex )     {", "exceptionsEncountered . add ( ex )  ;", "}", "}", "}  )  ;", "}", "for    ( Thread   t    :    threads )     {", "t . start (  )  ;", "}", "for    ( Thread   t    :    threads )     {", "t . join (  )  ;", "}", "Assert . assertTrue (  (  \" Encountered   exceptions :     \"     +     ( StringUtils . join (  \"  \\ r \\ n \"  ,     . selectToString ( exceptionsEncountered )  )  )  )  ,    exceptionsEncountered . isEmpty (  )  )  ;", "tearDown (  )  ;", "setUp (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultiThreadedOperation"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "Path   filePath    =    new   Path (  \"  / inProgress \"  )  ;", "FSDataOutputStream   outputStream    =    fs . create ( filePath )  ;", "FileStatus [  ]    listOfRoot    =    fs . listStatus ( new   Path (  \"  /  \"  )  )  ;", "Assert . assertEquals (  (  \" Expected   one   file   listed ,    instead   got :     \"     +     (  . toString ( listOfRoot )  )  )  ,     1  ,    listOfRoot . length )  ;", "Assert . assertEquals ( fs . makeQualified ( filePath )  ,    listOfRoot [  0  ]  . getPath (  )  )  ;", "outputStream . close (  )  ;", "}", "METHOD_END"], "methodName": ["testNoTempBlobsVisible"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "String [  ]    asStrings    =    new   String [ list . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( list . length )  ;    i +  +  )     {", "asStrings [ i ]     =    list [ i ]  . getPath (  )  . toString (  )  ;", "}", "return   StringUtils . join (  \"  ,  \"  ,    asStrings )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "WasbFsck   fsck    =    new   WasbFsck ( fs . getConf (  )  )  ;", "fsck . setMockorTesting ( fs )  ;", "fsck . run ( new   String [  ]  {    p . toString (  )     }  )  ;", "return   fsck . getPathNameWarning (  )  ;", "}", "METHOD_END"], "methodName": ["runWasbFsck"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "fs    =    testAccount . get (  )  ;", "root    =    fs . getUri (  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "testAccount . cleanup (  )  ;", "root    =    null ;", "fs    =    null ;", "testAccount    =    null ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "Path   testFile 1     =    new   Path (  (  ( root )     +     \"  / testFile 1  \"  )  )  ;", "Assert . assertTrue ( fs . createNewFile ( testFile 1  )  )  ;", "Path   testFile 2     =    new   Path (  (  ( root )     +     \"  / testFile 2  :  2  \"  )  )  ;", "try    {", "fs . createNewFile ( testFile 2  )  ;", "Assert . fail (  \" Should ' ve   thrown .  \"  )  ;", "}    catch    ( IOException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testCreate"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder 1     =    new   Path (  (  ( root )     +     \"  / testFolder 1  \"  )  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder 1  )  )  ;", "Path   testFolder 2     =    new   Path (  (  ( root )     +     \"  / testFolder 2  :  2  \"  )  )  ;", "try    {", "Assert . assertTrue ( fs . mkdirs ( testFolder 2  )  )  ;", "Assert . fail (  \" Should ' ve   thrown .  \"  )  ;", "}    catch    ( IOException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testMkdirs"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "Path   testFile 1     =    new   Path (  (  ( root )     +     \"  / testFile 1  \"  )  )  ;", "Assert . assertTrue ( fs . createNewFile ( testFile 1  )  )  ;", "Path   testFile 2     =    new   Path (  (  ( root )     +     \"  / testFile 2  \"  )  )  ;", "fs . rename ( testFile 1  ,    testFile 2  )  ;", "Assert . assertTrue (  (  (  !  ( fs . exists ( testFile 1  )  )  )     &  &     ( fs . exists ( testFile 2  )  )  )  )  ;", "Path   testFile 3     =    new   Path (  (  ( root )     +     \"  / testFile 3  :  3  \"  )  )  ;", "try    {", "fs . rename ( testFile 2  ,    testFile 3  )  ;", "Assert . fail (  \" Should ' ve   thrown .  \"  )  ;", "}    catch    ( IOException   e )     {", "}", "Assert . assertTrue ( fs . exists ( testFile 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testRename"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "Path   testFolder 1     =    new   Path (  (  ( root )     +     \"  / testFolder 1  \"  )  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder 1  )  )  ;", "Path   testFolder 2     =    new   Path ( testFolder 1  ,     \" testFolder 2  \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder 2  )  )  ;", "Path   testFolder 3     =    new   Path ( testFolder 1  ,     \" testFolder 3  \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( testFolder 3  )  )  ;", "Path   testFile 1     =    new   Path ( testFolder 2  ,     \" testFile 1  \"  )  ;", "Assert . assertTrue ( fs . createNewFile ( testFile 1  )  )  ;", "Path   testFile 2     =    new   Path ( testFolder 1  ,     \" testFile 2  \"  )  ;", "Assert . assertTrue ( fs . createNewFile ( testFile 2  )  )  ;", "Assert . assertFalse ( runWasbFsck ( testFolder 1  )  )  ;", "InMemoryBlockBlobStore   backingStore    =    testAccount . getMockStorage (  )  . getBackingStore (  )  ;", "backingStore . setContent ( BlobStorageTestAccount . toMockUri (  \" testFolder 1  / testFolder 2  / test 2  :  2  \"  )  ,    new   byte [  ]  {     1  ,     2     }  ,    new   HashMap < String ,    String >  (  )  )  ;", "Assert . assertTrue ( runWasbFsck ( testFolder 1  )  )  ;", "}", "METHOD_END"], "methodName": ["testWasbFsck"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemFileNameCheck"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  \" Skipping   testListStatusThrowsExceptionForUnreadableDir   since   WASB \"     +     \"    doesn ' t   honor   directory   permissions .  \"  )  )  ;", "Assume . assumeTrue (  (  !  ( Path . WINDOWS )  )  )  ;", "}", "METHOD_END"], "methodName": ["testListStatusThrowsExceptionForUnreadableDir"], "fileName": "org.apache.hadoop.fs.azure.TestNativeAzureFileSystemOperationsMocked"}, {"methodBody": ["METHOD_START", "{", "backingStore . setContent ( AzureBlobStorageTestAccount . toMockUri ( path )  ,    new   byte [  ]  {     1  ,     2     }  ,    new   HashMap < String ,    String >  (  )  )  ;", "}", "METHOD_END"], "methodName": ["createEmptyBlobOutOfBand"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "fs    =    testAccount . getFileSystem (  )  ;", "backingStore    =    testAccount . getMockStorage (  )  . getBackingStore (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "testAccount . cleanup (  )  ;", "fs    =    null ;", "backingStore    =    null ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "for    ( TestOutOfBandAzureBlobOperations . DeepCreateTestVariation   variation    :    TestOutOfBandAzureBlobOperations . DeepCreateTestVariation . values (  )  )     {", "switch    ( variation )     {", "case   File    :", "Assert . assertTrue ( fs . createNewFile ( new   Path (  \"  / x / y / z \"  )  )  )  ;", "break ;", "case   Folder    :", "Assert . assertTrue ( fs . mkdirs ( new   Path (  \"  / x / y / z \"  )  )  )  ;", "break ;", "}", "Assert . assertTrue ( backingStore . exists ( AzureBlobStorageTestAccount . toMockUri (  \" x \"  )  )  )  ;", "Assert . assertTrue ( backingStore . exists ( AzureBlobStorageTestAccount . toMockUri (  \" x / y \"  )  )  )  ;", "fs . delete ( new   Path (  \"  / x \"  )  ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testCreatingDeepFileCreatesExplicitFolder"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "createEmptyBlobOutOfBand (  \" root / b \"  )  ;", "createEmptyBlobOutOfBand (  \" root / b / c \"  )  ;", "FileStatus [  ]    listResult    =    fs . listStatus ( new   Path (  \"  / root / b \"  )  )  ;", "Assert . assertEquals (  1  ,    listResult . length )  ;", "Assert . assertFalse ( listResult [  0  ]  . isDirectory (  )  )  ;", "try    {", "fs . delete ( new   Path (  \"  / root / b / c \"  )  ,    true )  ;", "Assert . assertTrue (  \" Should ' ve   thrown .  \"  ,    false )  ;", "}    catch    ( AzureException   e )     {", "Assert . assertEquals (  (  \" File    / root / b / c   has   a   parent   directory    / root / b \"     +     \"    which   is   also   a   file .    Can ' t   resolve .  \"  )  ,    e . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFileAndImplicitFolderSameName"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "createEmptyBlobOutOfBand (  \" root / b \"  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / root \"  )  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \"  / root / b \"  )  ,    true )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / root \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testFileInImplicitFolderDeleted"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "createEmptyBlobOutOfBand (  \" root / b \"  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / root \"  )  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \"  / root \"  )  ,    true )  )  ;", "Assert . assertFalse ( fs . exists ( new   Path (  \"  / root \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testImplicitFolderDeleted"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "createEmptyBlobOutOfBand (  \" root / b \"  )  ;", "FileStatus [  ]    obtained    =    fs . listStatus ( new   Path (  \"  / root / b \"  )  )  ;", "Assert . assertNotNull ( obtained )  ;", "Assert . assertEquals (  1  ,    obtained . length )  ;", "Assert . assertFalse ( obtained [  0  ]  . isDirectory (  )  )  ;", "Assert . assertEquals (  \"  / root / b \"  ,    obtained [  0  ]  . getPath (  )  . toUri (  )  . getPath (  )  )  ;", "obtained    =    fs . listStatus ( new   Path (  \"  / root \"  )  )  ;", "Assert . assertNotNull ( obtained )  ;", "Assert . assertEquals (  1  ,    obtained . length )  ;", "Assert . assertFalse ( obtained [  0  ]  . isDirectory (  )  )  ;", "Assert . assertEquals (  \"  / root / b \"  ,    obtained [  0  ]  . getPath (  )  . toUri (  )  . getPath (  )  )  ;", "FileStatus   dirStatus    =    fs . getFileStatus ( new   Path (  \"  / root \"  )  )  ;", "Assert . assertNotNull ( dirStatus )  ;", "Assert . assertTrue ( dirStatus . isDirectory (  )  )  ;", "Assert . assertEquals (  \"  / root \"  ,    dirStatus . getPath (  )  . toUri (  )  . getPath (  )  )  ;", "}", "METHOD_END"], "methodName": ["testImplicitFolderListed"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "createEmptyBlobOutOfBand (  \" root / b \"  )  ;", "fs . setOwner ( new   Path (  \"  / root \"  )  ,     \" newOwner \"  ,    null )  ;", "FileStatus   newStatus    =    fs . getFileStatus ( new   Path (  \"  / root \"  )  )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals (  \" newOwner \"  ,    newStatus . getOwner (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetOwnerOnImplicitFolder"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "createEmptyBlobOutOfBand (  \" root / b \"  )  ;", "FsPermission   newPermission    =    new   FsPermission (  (  ( short )     (  3  8  4  )  )  )  ;", "fs . setPermission ( new   Path (  \"  / root \"  )  ,    newPermission )  ;", "FileStatus   newStatus    =    fs . getFileStatus ( new   Path (  \"  / root \"  )  )  ;", "Assert . assertNotNull ( newStatus )  ;", "Assert . assertEquals ( newPermission ,    newStatus . getPermission (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetPermissionOnImplicitFolder"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperations"}, {"methodBody": ["METHOD_START", "{", "Path   targetFile    =    new   Path (  \"  / newInRoot \"  )  ;", "FSDataputStream   s 2     =    fs . create ( targetFile )  ;", "s 2  . close (  )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_create_rootDir"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  \" folderW / file \"  )  ;", "utputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / folderW \"  )  )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / folderW / file \"  )  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \"  / folderW \"  )  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_firstLevelFolderDelete"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "String   workingDir    =     (  \" user /  \"     +     ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  )     +     \"  /  \"  ;", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  ( workingDir    +     \" testFolder 2  / a / input / file \"  )  )  ;", "BlobOutputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \" testFolder 2  / a / input / file \"  )  )  )  ;", "Path   targetFolder    =    new   Path (  \" testFolder 2  / a / input \"  )  ;", "Assert . assertTrue ( fs . delete ( targetFolder ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_parentDelete"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "String   workingDir    =     (  \" user /  \"     +     ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  )     +     \"  /  \"  ;", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  ( workingDir    +     \" testFolder 4  / a / input / file \"  )  )  ;", "BlobOutputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Path   srcFilePath    =    new   Path (  \" testFolder 4  / a / input / file \"  )  ;", "Assert . assertTrue ( fs . exists ( srcFilePath )  )  ;", "Path   destFilePath    =    new   Path (  \" testFolder 4  / a / input / file 2  \"  )  ;", "fs . rename ( srcFilePath ,    destFilePath )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_rename"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  \" fileX \"  )  ;", "utputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Path   srcFilePath    =    new   Path (  \"  / fileX \"  )  ;", "Assert . assertTrue ( fs . exists ( srcFilePath )  )  ;", "Path   destFilePath    =    new   Path (  \"  / fileXrename \"  )  ;", "fs . rename ( srcFilePath ,    destFilePath )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_rename_rootLevelFiles"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  \" fileY \"  )  ;", "utputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \"  / fileY \"  )  )  )  ;", "Assert . assertTrue ( fs . delete ( new   Path (  \"  / fileY \"  )  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_rootFileDelete"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "String   workingDir    =     (  \" user /  \"     +     ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  )     +     \"  /  \"  ;", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  ( workingDir    +     \" testFolder 3  / a / input / file \"  )  )  ;", "BlobOutputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \" testFolder 3  / a / input / file \"  )  )  )  ;", "Path   targetFile    =    new   Path (  \" testFolder 3  / a / input / file 2  \"  )  ;", "FSDataOutputStream   s 2     =    fs . create ( targetFile )  ;", "s 2  . close (  )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_siblingCreate"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "String   workingDir    =     (  \" user /  \"     +     ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  )     +     \"  /  \"  ;", "CloudBlockBlob   blob    =    testAccount . getBlobReference (  ( workingDir    +     \" testFolder 1  / a / input / file \"  )  )  ;", "BlobOutputStream   s    =    blob . openOutputStream (  )  ;", "s . close (  )  ;", "Assert . assertTrue ( fs . exists ( new   Path (  \" testFolder 1  / a / input / file \"  )  )  )  ;", "Path   targetFolder    =    new   Path (  \" testFolder 1  / a / output \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( targetFolder )  )  ;", "}", "METHOD_END"], "methodName": ["outOfBandFolder_uncleMkdirs"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "if    (  ( testAccount )     !  =    null )     {", "fs    =    testAccount . getFileSystem (  )  ;", "}", "Assume . assumeNotNull ( testAccount )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "fs    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestOutOfBandAzureBlobOperationsLive"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( Shell . WINDOWS )  )     {", "return ;", "}", "ShellDecryptionKeyProvider   provider    =    new   ShellDecryptionKeyProvider (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "String   account    =     \" testacct \"  ;", "String   key    =     \" key \"  ;", "conf . set (  (  ( SimpleKeyProvider . KEY _ ACCOUNT _ KEY _ PREFIX )     +    account )  ,    key )  ;", "try    {", "provider . getStorageAccountKey ( account ,    conf )  ;", "Assert . fail (  \" fs . azure . shellkeyprovider . script   is   not   specified ,    we   should   throw \"  )  ;", "}    catch    ( KeyProviderException   e )     {", ". LOG . info (  (  \" Received   an   expected   exception :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testScriptPathNotSpecified"], "fileName": "org.apache.hadoop.fs.azure.TestShellDecryptionKeyProvider"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( Shell . WINDOWS )  )     {", "return ;", "}", "String   expectedResult    =     \" decretedKey \"  ;", "File   scriptFile    =    new   File (  . TEST _ ROOT _ DIR ,     \" testScript . cmd \"  )  ;", "FileUtils . writeStringToFile ( scriptFile ,     (  \"  @ echo    %  1     \"     +    expectedResult )  )  ;", "ShellDecryptionKeyProvider   provider    =    new   ShellDecryptionKeyProvider (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "String   account    =     \" testacct \"  ;", "String   key    =     \" key 1  \"  ;", "conf . set (  (  ( SimpleKeyProvider . KEY _ ACCOUNT _ KEY _ PREFIX )     +    account )  ,    key )  ;", "conf . set ( ShellDecryptionKeyProvider . KEY _ ACCOUNT _ SHELLKEYPROVIDER _ SCRIPT ,     (  \" cmd    / c    \"     +     ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "String   result    =    provider . getStorageAccountKey ( account ,    conf )  ;", "Assert . assertEquals (  (  ( key    +     \"     \"  )     +    expectedResult )  ,    result )  ;", "}", "METHOD_END"], "methodName": ["testValidScript"], "fileName": "org.apache.hadoop.fs.azure.TestShellDecryptionKeyProvider"}, {"methodBody": ["METHOD_START", "{", "int   count    =     0  ;", "for    ( String   key    :    backingStore . getKeys (  )  )     {", "if    ( key . contains ( NativeAFileSystem . AZURE _ TEMP _ FOLDER )  )     {", "count +  +  ;", "}", "}", "return   count ;", "}", "METHOD_END"], "methodName": ["getNumTempBlobs"], "fileName": "org.apache.hadoop.fs.azure.TestWasbFsck"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    fs . getConf (  )  ;", "conf . setInt ( NativeAzureFileSystem . AZURE _ TEMP _ EXPIRY _ PROPERTY _ NAME ,     0  )  ;", "fsck    =    new    ( conf )  ;", "fsck . setMockFileSystemForTesting ( fs )  ;", "fsck . run ( new   String [  ]  {    AzureBlobStorageTestAccount . MOCK _ WASB _ URI ,    command    }  )  ;", "}", "METHOD_END"], "methodName": ["runFsck"], "fileName": "org.apache.hadoop.fs.azure.TestWasbFsck"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "fs    =    testAccount . getFileSystem (  )  ;", "backingStore    =    testAccount . getMockStorage (  )  . getBackingStore (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.TestWasbFsck"}, {"methodBody": ["METHOD_START", "{", "testAccount . cleanup (  )  ;", "fs    =    null ;", "backingStore    =    null ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestWasbFsck"}, {"methodBody": ["METHOD_START", "{", "Path   danglingFile    =    new   Path (  \"  / crashedInTheMiddle \"  )  ;", "FSDataOutputStream   stream    =    fs . create ( danglingFile )  ;", "stream . write ( new   byte [  ]  {     1  ,     2  ,     3     }  )  ;", "stream . flush (  )  ;", "FileStatus   fileStatus    =    fs . getFileStatus ( danglingFile )  ;", "Assert . assertNotNull ( fileStatus )  ;", "Assert . assertEquals (  0  ,    fileStatus . getLen (  )  )  ;", "Assert . assertEquals (  1  ,    getNumTempBlobs (  )  )  ;", "run (  \"  - delete \"  )  ;", "Assert . assertEquals (  0  ,    getNumTempBlobs (  )  )  ;", "Assert . assertFalse ( fs . exists ( danglingFile )  )  ;", "}", "METHOD_END"], "methodName": ["testDelete"], "fileName": "org.apache.hadoop.fs.azure.TestWasbFsck"}, {"methodBody": ["METHOD_START", "{", "InputStream   inputStream    =    fs . open ( testFile )  ;", "int   byteRead    =    inputStream . read (  )  ;", "Assert . assertTrue (  (  \" File   unexpectedly   empty :     \"     +    testFile )  ,     ( byteRead    >  =     0  )  )  ;", "Assert . assertTrue (  (  \" File   has   more   than   a   single   byte :     \"     +    testFile )  ,     (  ( inputStream . read (  )  )     <     0  )  )  ;", "inputStream . close (  )  ;", "Assert . assertEquals (  (  \" Unxpected   content   in :     \"     +    testFile )  ,    expectedValue ,    byteRead )  ;", "}", "METHOD_END"], "methodName": ["assertSingleByteValue"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "InputStream   inputStream    =    fs . open ( filePath )  ;", "int   count    =     0  ;", "while    (  ( inputStream . read (  )  )     >  =     0  )     {", "count +  +  ;", "}", "inputStream . close (  )  ;", "return   count ;", "}", "METHOD_END"], "methodName": ["readInputStream"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "return   readInputStream ( fs ,    filePath )  ;", "}", "METHOD_END"], "methodName": ["readInputStream"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createForEmulator (  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "Assert . assertTrue ( validateIOStreams ( new   Path (  \"  / testFile \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testConnectToEmulator"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "AzureBlobStorageTestAccount . setMockAccountKey ( conf ,     \" mockAccount . mock . authority . net \"  )  ;", "AzureNativeFileSystemStore   store    =    new   AzureNativeFileSystemStore (  )  ;", "MockStorageInterface   mockStorage    =    new   MockStorageInterface (  )  ;", "store . setAzureStorageInteractionLayer ( mockStorage )  ;", "NativeAzureFileSystem   fs    =    new   NativeAzureFileSystem ( store )  ;", "fs . initialize ( new   URI (  \" wasb :  /  / mockContainer @ mockAccount . mock . authority . net \"  )  ,    conf )  ;", "fs . createNewFile ( new   Path (  \"  / x \"  )  )  ;", "Assert . assertTrue ( mockStorage . getBackingStore (  )  . exists (  \" http :  /  / mockAccount . mock . authority . net / mockContainer / x \"  )  )  ;", "fs . close (  )  ;", "}", "METHOD_END"], "methodName": ["testConnectToFullyQualifiedAccountMock"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "final   String   blobPrefix    =    String . format (  \" wasbtests -  % s -  % tQ - blob \"  ,    System . getProperty (  \" user . name \"  )  ,    new   Date (  )  )  ;", "final   String   inblobName    =     ( blobPrefix    +     \"  _ In \"  )     +     \"  . txt \"  ;", "final   String   outblobName    =     ( blobPrefix    +     \"  _ Out \"  )     +     \"  . txt \"  ;", "testAccount    =    AzureBlobStorageTestAccount . createRoot ( inblobName ,     . FILE _ SIZE )  ;", "Assume . assumeNotNull ( testAccount )  ;", "Assert . assertEquals (  . FILE _ SIZE ,    readInputStream ( new   Path (  (  (  . PATH _ DELIMITER )     +    inblobName )  )  )  )  ;", "try    {", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Path   outputPath    =    new   Path (  (  (  . PATH _ DELIMITER )     +    outblobName )  )  ;", "OutputStream   outputStream    =    fs . create ( outputPath )  ;", "Assert . fail (  \" Expected   an   AzureException   when   writing   to   root   folder .  \"  )  ;", "outputStream . write ( new   byte [  . FILE _ SIZE ]  )  ;", "outputStream . close (  )  ;", "}    catch    ( AzureException   e )     {", "Assert . assertTrue ( true )  ;", "}    catch    ( Exception   e )     {", "String   errMsg    =    String . format (  \" Expected   AzureException   but   got    % s   instead .  \"  ,    e )  ;", "Assert . assertTrue ( errMsg ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["testConnectToRoot"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createAnonymous (  \" testWasb . txt \"  ,    TestWasbUriAndConfiguration . FILE _ SIZE )  ;", "Assume . assumeNotNull ( testAccount )  ;", "Assert . assertEquals ( TestWasbUriAndConfiguration . FILE _ SIZE ,    readInputStream ( new   Path (  \"  / testWasb . txt \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testConnectUsingAnonymous"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "Assert . assertTrue ( validateIOStreams ( new   Path (  \"  / wasb _ scheme \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testConnectUsingKey"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . of ( AzureBlobStorageTestAccount . CreateOptions . UseSas ,    AzureBlobStorageTestAccount . CreateOptions . CreateContainer )  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "Assert . assertFalse ( testAccount . getFileSystem (  )  . exists ( new   Path (  \"  / IDontExist \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testConnectUsingSAS"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  \"  \"  ,    EnumSet . of ( AzureBlobStorageTestAccount . CreateOptions . UseSas ,    AzureBlobStorageTestAccount . CreateOptions . CreateContainer ,    AzureBlobStorageTestAccount . CreateOptions . Readonly )  )  ;", "Assume . assumeNotNull ( testAccount )  ;", "final   String   blobKey    =     \" blobForReadonly \"  ;", "CloudBlobContainer   container    =    testAccount . getRealContainer (  )  ;", "CloudBlockBlob   blob    =    container . getBlockBlobReference ( blobKey )  ;", "ByteArrayInputStream   inputStream    =    new   ByteArrayInputStream ( new   byte [  ]  {     1  ,     2  ,     3     }  )  ;", "blob . upload ( inputStream ,     3  )  ;", "inputStream . close (  )  ;", "Path   filePath    =    new   Path (  (  \"  /  \"     +    blobKey )  )  ;", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "Assert . assertTrue ( fs . exists ( filePath )  )  ;", "byte [  ]    obtained    =    new   byte [  3  ]  ;", "DataInputStream   obtainedInputStream    =    fs . open ( filePath )  ;", "obtainedInputStream . readFully ( obtained )  ;", "obtainedInputStream . close (  )  ;", "Assert . assertEquals (  3  ,    obtained [  2  ]  )  ;", "}", "METHOD_END"], "methodName": ["testConnectUsingSASReadonly"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . createThrottled (  )  ;", "Assert . assertTrue ( validateIOStreams ( new   Path (  \"  / wasb _ scheme \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testConnectWithThrottling"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "String   account    =     \" testacct \"  ;", "String   key    =     \" testkey \"  ;", "conf . set (  (  ( SimpleKeyProvider . KEY _ ACCOUNT _ KEY _ PREFIX )     +    account )  ,    key )  ;", "String   result    =    AzureNativeFileSystemStore . getAccountKeyFromConfiguration ( account ,    conf )  ;", "Assert . assertEquals ( key ,    result )  ;", "}", "METHOD_END"], "methodName": ["testDefaultKeyProvider"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "String   account    =     \" testacct \"  ;", "conf . set (  (  \" fs . azure . account . keyprovider .  \"     +    account )  ,     \" Class \"  )  ;", "try    {", "AzureNativeFileSystemStore . getAccountKeyFromConfiguration ( account ,    conf )  ;", "Assert . fail (  (  \" Nonexistant   key   provider   class   should   have   thrown   a    \"     +     \" KeyProviderException \"  )  )  ;", "}    catch    ( KeyProviderException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testInvalidKeyProviderNonexistantClass"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "String   account    =     \" testacct \"  ;", "conf . set (  (  \" fs . azure . account . keyprovider .  \"     +    account )  ,     \" String \"  )  ;", "try    {", "AzureNativeFileSystemStore . getAccountKeyFromConfiguration ( account ,    conf )  ;", "Assert . fail (  (  \" Key   provider   class   that   doesn ' t   implement   KeyProvider    \"     +     \" should   have   thrown   a   KeyProviderException \"  )  )  ;", "}    catch    ( KeyProviderException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testInvalidKeyProviderWrongClass"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount   firstAccount    =    AzureBlobStorageTestAccount . create (  \" first \"  )  ;", "AzureBlobStorageTestAccount   secondAccount    =    AzureBlobStorageTestAccount . create (  \" second \"  )  ;", "Assume . assumeNotNull ( firstAccount )  ;", "Assume . assumeNotNull ( secondAccount )  ;", "try    {", "FileSystem   firstFs    =    firstAccount . getFileSystem (  )  ;", "FileSystem   secondFs    =    secondAccount . getFileSystem (  )  ;", "Path   testFile    =    new   Path (  \"  / testWasb \"  )  ;", "Assert . assertTrue ( validateIOStreams ( firstFs ,    testFile )  )  ;", "Assert . assertTrue ( validateIOStreams ( secondFs ,    testFile )  )  ;", ". writeSingleByte ( firstFs ,    testFile ,     5  )  ;", ". writeSingleByte ( secondFs ,    testFile ,     7  )  ;", ". assertSingleByteValue ( firstFs ,    testFile ,     5  )  ;", ". assertSingleByteValue ( secondFs ,    testFile ,     7  )  ;", "}    finally    {", "firstAccount . cleanup (  )  ;", "secondAccount . cleanup (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultipleContainers"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "String [  ]    wasbAliases    =    new   String [  ]  {     \" wasb \"  ,     \" wasbs \"     }  ;", "for    ( String   defaultScheme    :    wasbAliases )     {", "for    ( String   wantedScheme    :    wasbAliases )     {", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "conf    =    testAccount . getFileSystem (  )  . getConf (  )  ;", "String   authority    =    testAccount . getFileSystem (  )  . getUri (  )  . getAuthority (  )  ;", "URI   defaultUri    =    new   URI ( defaultScheme ,    authority ,    null ,    null ,    null )  ;", "conf . set (  \" fs . default . name \"  ,    defaultUri . toString (  )  )  ;", "URI   wantedUri    =    new   URI (  ( wantedScheme    +     \"  :  /  /  / random / path \"  )  )  ;", "NativeAzureFileSystem   obtained    =     (  ( NativeAzureFileSystem )     ( FileSystem . get ( wantedUri ,    conf )  )  )  ;", "Assert . assertNotNull ( obtained )  ;", "Assert . assertEquals ( new   URI ( wantedScheme ,    authority ,    null ,    null ,    null )  ,    obtained . getUri (  )  )  ;", "Path   qualified    =    obtained . makeQualified ( new   Path ( wantedUri )  )  ;", "Assert . assertEquals ( new   URI ( wantedScheme ,    authority ,    wantedUri . getPath (  )  ,    null ,    null )  ,    qualified . toUri (  )  )  ;", "testAccount . cleanup (  )  ;", "FileSystem . closeAll (  )  ;", "}", "}", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "conf    =    testAccount . getFileSystem (  )  . getConf (  )  ;", "conf . set (  \" fs . default . name \"  ,     \" file :  /  /  /  \"  )  ;", "try    {", "FileSystem . get ( new   URI (  \" wasb :  /  /  / random / path \"  )  ,    conf )  ;", "Assert . fail (  \" Should ' ve   thrown .  \"  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testNoUriAuthority"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "String   account    =     \" testacct \"  ;", "String   key    =     \" testkey \"  ;", "conf . set (  (  ( SimpleKeyProvider . KEY _ ACCOUNT _ KEY _ PREFIX )     +    account )  ,    key )  ;", "conf . setClass (  (  \" fs . azure . account . keyprovider .  \"     +    account )  ,    SimpleKeyProvider . class ,    KeyProvider . class )  ;", "String   result    =    AzureNativeFileSystemStore . getAccountKeyFromConfiguration ( account ,    conf )  ;", "Assert . assertEquals ( key ,    result )  ;", "}", "METHOD_END"], "methodName": ["testValidKeyProvider"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "OutputStream   outputStream    =    fs . create ( filePath )  ;", "outputStream . write ( new   byte [  . FILE _ SIZE ]  )  ;", "outputStream . close (  )  ;", "return    (  . FILE _ SIZE )     =  =     ( readInputStream ( fs ,    filePath )  )  ;", "}", "METHOD_END"], "methodName": ["validateIOStreams"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    testAccount . getFileSystem (  )  ;", "return   validateIOStreams ( fs ,    filePath )  ;", "}", "METHOD_END"], "methodName": ["validateIOStreams"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "OutputStream   outputStream    =    fs . create ( testFile )  ;", "outputStream . write ( toWrite )  ;", "outputStream . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeSingleByte"], "fileName": "org.apache.hadoop.fs.azure.TestWasbUriAndConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   p . toUri (  )  . getPath (  )  . toString (  )  . contains (  \"  :  \"  )  ;", "}", "METHOD_END"], "methodName": ["containsColon"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "return   args . contains (  \"  - H \"  )  ;", "}", "METHOD_END"], "methodName": ["doPrintUsage"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "return   pathNameWarning ;", "}", "METHOD_END"], "methodName": ["getPathNameWarning"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "int   res    =    ToolRunner . run ( new   WasbFsck ( new   Configuration (  )  )  ,    args )  ;", "System . exit ( res )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  \" Usage :    WasbFSck    [  < path >  ]     [  - move    |     - delete ]  \"  )  ;", "System . out . println (  \"  \\ t < path >  \\ tstart   checking   from   this   path \"  )  ;", "System . out . println (  (  (  \"  \\ t - move \\ tmove   any   files   whose   upload   was   interrupted \"     +     \"    mid - stream   to    \"  )     +     (  . LOST _ AND _ FOUND _ PATH )  )  )  ;", "System . out . println (  (  \"  \\ t - delete \\ tdelete   any   files   whose   upload   was   interrupted \"     +     \"    mid - stream \"  )  )  ;", "ToolRunner . printGenericCommandUsage ( System . out )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "if    ( p    =  =    null )     {", "return   true ;", "}", "if    (  !  ( exists ( p )  )  )     {", "System . out . println (  (  (  \" Path    \"     +    p )     +     \"    does   not   exist !  \"  )  )  ;", "return   true ;", "}", "if    ( isFile ( p )  )     {", "if    ( containsColon ( p )  )     {", "System . out . println (  (  (  \" Warning :    file    \"     +    p )     +     \"    has   a   colon   in   its   name .  \"  )  )  ;", "return   false ;", "} else    {", "return   true ;", "}", "} else    {", "boolean   flag ;", "if    ( containsColon ( p )  )     {", "System . out . println (  (  (  \" Warning :    directory    \"     +    p )     +     \"    has   a   colon   in   its   name .  \"  )  )  ;", "flag    =    false ;", "} else    {", "flag    =    true ;", "}", "FileStatus [  ]    listed    =    listStatus ( p )  ;", "for    ( FileStatus   l    :    listed )     {", "if    (  !  ( recursiveCheckChildPathName (    l . getPath (  )  )  )  )     {", "flag    =    false ;", "}", "}", "return   flag ;", "}", "}", "METHOD_END"], "methodName": ["recursiveCheckChildPathName"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "this . mockFileSystemForTesting    =    fileSystem ;", "}", "METHOD_END"], "methodName": ["setMockFileSystemForTesting"], "fileName": "org.apache.hadoop.fs.azure.WasbFsck"}, {"methodBody": ["METHOD_START", "{", "currentBlockDownloadLatency . addPoint ( latency )  ;", "}", "METHOD_END"], "methodName": ["blockDownloaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "currentBlockUploadLatency . addPoint ( latency )  ;", "}", "METHOD_END"], "methodName": ["blockUploaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "clientErrors . incr (  )  ;", "}", "METHOD_END"], "methodName": ["clientErrorEncountered"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "if    ( bytesPerSecond    >     ( currentMaximumDownloadBytesPerSecond )  )     {", "currentMaximumDownloadBytesPerSecond    =    bytesPerSecond ;", "maximumDownloadBytesPerSecond . set ( bytesPerSecond )  ;", "}", "}", "METHOD_END"], "methodName": ["currentDownloadBytesPerSecond"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "if    ( bytesPerSecond    >     ( currentMaximumUploadBytesPerSecond )  )     {", "currentMaximumUploadBytesPerSecond    =    bytesPerSecond ;", "maximumUploadBytesPerSecond . set ( bytesPerSecond )  ;", "}", "}", "METHOD_END"], "methodName": ["currentUploadBytesPerSecond"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "numberOfDirectoriesCreated . incr (  )  ;", "}", "METHOD_END"], "methodName": ["directoryCreated"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "numberOfDirectoriesDeleted . incr (  )  ;", "}", "METHOD_END"], "methodName": ["directoryDeleted"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "numberOfFilesCreated . incr (  )  ;", "}", "METHOD_END"], "methodName": ["fileCreated"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "numberOfFilesDeleted . incr (  )  ;", "}", "METHOD_END"], "methodName": ["fileDeleted"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   currentBlockDownloadLatency . getCurrentAverage (  )  ;", "}", "METHOD_END"], "methodName": ["getBlockDownloadLatency"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   currentBlockUploadLatency . getCurrentAverage (  )  ;", "}", "METHOD_END"], "methodName": ["getBlockUploadLatency"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   currentMaximumDownloadBytesPerSecond ;", "}", "METHOD_END"], "methodName": ["getCurrentMaximumDownloadBandwidth"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   currentMaximumUploadBytesPerSecond ;", "}", "METHOD_END"], "methodName": ["getCurrentMaximumUploadBandwidth"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   inMemoryNumberOfWebResponses . get (  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentWebResponses"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   fileSystemInstanceId ;", "}", "METHOD_END"], "methodName": ["getFileSystemInstanceId"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   registry . info (  )  ;", "}", "METHOD_END"], "methodName": ["getMetricsRegistryInfo"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "rawBytesDownloaded . incr ( numberOfBytes )  ;", "}", "METHOD_END"], "methodName": ["rawBytesDownloaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "rawBytesUploaded . incr ( numberOfBytes )  ;", "}", "METHOD_END"], "methodName": ["rawBytesUploaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "serverErrors . incr (  )  ;", "}", "METHOD_END"], "methodName": ["serverErrorEncountered"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "registry . tag (  \" accountName \"  ,     \" Name   of   the   Azure   Storage   account   that   these   metrics   are   going   against \"  ,    accountName )  ;", "}", "METHOD_END"], "methodName": ["setAccountName"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "registry . tag (  \" containerName \"  ,     \" Name   of   the   Azure   Storage   container   that   these   metrics   are   going   against \"  ,    containerName )  ;", "}", "METHOD_END"], "methodName": ["setContainerName"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "bytesReadInLastSecond . set ( currentBytesRead )  ;", "}", "METHOD_END"], "methodName": ["updateBytesReadInLastSecond"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "bytesWrittenInLastSecond . set ( currentBytesWritten )  ;", "}", "METHOD_END"], "methodName": ["updateBytesWrittenInLastSecond"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "numberOfWebResponses . incr (  )  ;", "inMemoryNumberOfWebResponses . increAndGet (  )  ;", "}", "METHOD_END"], "methodName": ["webResponse"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "if    (  ( AzureFileSystemMetricsSystem . numFileSystems )     =  =     1  )     {", "AzureFileSystemMetricsSystem . instance . publishMetricsNow (  )  ;", "AzureFileSystemMetricsSystem . instance . stop (  )  ;", "AzureFileSystemMetricsSystem . instance . shutdown (  )  ;", "AzureFileSystemMetricsSystem . instance    =    null ;", "}", "( AzureFileSystemMetricsSystem . numFileSystems )  -  -  ;", "}", "METHOD_END"], "methodName": ["fileSystemClosed"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "if    (  ( AzureFileSystemMetricsSystem . numFileSystems )     =  =     0  )     {", "AzureFileSystemMetricsSystem . instance    =    new   MetricsSystemImpl (  )  ;", "AzureFileSystemMetricsSystem . instance . init (  \" azure - file - system \"  )  ;", "}", "( AzureFileSystemMetricsSystem . numFileSystems )  +  +  ;", "}", "METHOD_END"], "methodName": ["fileSystemStarted"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "AzureFileSystemMetricsSystem . instance . register ( name ,    desc ,    source )  ;", "}", "METHOD_END"], "methodName": ["registerSource"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "if    (  ( AzureFileSystemMetricsSystem . instance )     !  =    null )     {", "AzureFileSystemMetricsSystem . instance . publishMetricsNow (  )  ;", "AzureFileSystemMetricsSystem . instance . unregisterSource ( name )  ;", "}", "}", "METHOD_END"], "methodName": ["unregisterSource"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "return   AzureMetricsTestUtil . getLongGaugeValue ( instrumentation ,    AzureFileSystemInstrumentation . WASB _ BYTES _ READ )  ;", "}", "METHOD_END"], "methodName": ["getCurrentBytesRead"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "return   AzureMetricsTestUtil . getLongGaugeValue ( instrumentation ,    AzureFileSystemInstrumentation . WASB _ BYTES _ WRITTEN )  ;", "}", "METHOD_END"], "methodName": ["getCurrentBytesWritten"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "return   AzureMetricsTestUtil . getLongCounterValue ( instrumentation ,    AzureFileSystemInstrumentation . WASB _ RAW _ BYTES _ DOWNLOADED )  ;", "}", "METHOD_END"], "methodName": ["getCurrentTotalBytesRead"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "return   AzureMetricsTestUtil . getLongCounterValue ( instrumentation ,    AzureFileSystemInstrumentation . WASB _ RAW _ BYTES _ UPLOADED )  ;", "}", "METHOD_END"], "methodName": ["getCurrentTotalBytesWritten"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "return   getLongCounter ( AzureFileSystemInstrumentation . WASB _ WEB _ RESPONSES ,    getMetrics ( instrumentation )  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentWebResponses"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "return   getLongCounter ( counterName ,    getMetrics ( instrumentation )  )  ;", "}", "METHOD_END"], "methodName": ["getLongCounterValue"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "return   getLongGauge ( gaugeName ,    getMetrics ( instrumentation )  )  ;", "}", "METHOD_END"], "methodName": ["getLongGaugeValue"], "fileName": "org.apache.hadoop.fs.azure.metrics.AzureMetricsTestUtil"}, {"methodBody": ["METHOD_START", "{", "synchronized ( blocksReadLock )     {", "allBlocksRead . add ( new    . BlockTransferWindow ( startDate ,    endDate ,    length )  )  ;", "}", "}", "METHOD_END"], "methodName": ["blockDownloaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "synchronized ( blocksWrittenLock )     {", "allBlocksWritten . add ( new    . BlockTransferWindow ( startDate ,    endDate ,    length )  )  ;", "}", "}", "METHOD_END"], "methodName": ["blockUploaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "if    (  ( uploadBandwidthUpdater )     !  =    null )     {", "uploadBandwidthUpdater . interrupt (  )  ;", "try    {", "uploadBandwidthUpdater . join (  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "uploadBandwidthUpdater    =    null ;", "}", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "return   new   ArrayList < BandwidthGaugeUpdater . BlockTransferWindow >  ( BandwidthGaugeUpdater . PROCESS _ QUEUE _ INITIAL _ CAPACITY )  ;", "}", "METHOD_END"], "methodName": ["createNewToProcessQueue"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "suppressAutoUpdate    =    false ;", "}", "METHOD_END"], "methodName": ["resumeAutoUpdate"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "suppressAutoUpdate    =    true ;", "}", "METHOD_END"], "methodName": ["suppressAutoUpdate"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "ArrayList < BandwidthGaugeUpdater . BlockTransferWindow >    toProcess    =    null ;", "synchronized ( updateWrite    ?    blocksWrittenLock    :    blocksReadLock )     {", "if    ( updateWrite    &  &     (  !  ( allBlocksWritten . isEmpty (  )  )  )  )     {", "toProcess    =    allBlocksWritten ;", "allBlocksWritten    =    BandwidthGaugeUpdater . createNewToProcessQueue (  )  ;", "} else", "if    (  (  ! updateWrite )     &  &     (  !  ( allBlocksRead . isEmpty (  )  )  )  )     {", "toProcess    =    allBlocksRead ;", "allBlocksRead    =    BandwidthGaugeUpdater . createNewToProcessQueue (  )  ;", "}", "}", "if    ( toProcess    =  =    null )     {", "updateBytesTransferred ( updateWrite ,     0  )  ;", "updateBytesTransferRate ( updateWrite ,     0  )  ;", "return ;", "}", "long   cutoffTime    =     ( new   Date (  )  . getTime (  )  )     -     ( windowSizeMs )  ;", "long   maxSingleBlockTransferRate    =     0  ;", "long   bytesInLastSecond    =     0  ;", "for    ( BandwidthGaugeUpdater . BlockTransferWindow   currentWindow    :    toProcess )     {", "long   windowDuration    =     ( currentWindow . getEndDate (  )  . getTime (  )  )     -     ( currentWindow . getStartDate (  )  . getTime (  )  )  ;", "if    ( windowDuration    =  =     0  )     {", "windowDuration    =     1  ;", "}", "if    (  ( currentWindow . getStartDate (  )  . getTime (  )  )     >    cutoffTime )     {", "bytesInLastSecond    +  =    currentWindow . bytesTransferred ;", "} else", "if    (  ( currentWindow . getEndDate (  )  . getTime (  )  )     >    cutoffTime )     {", "long   adjustedBytes    =     (  ( currentWindow . getBytesTransferred (  )  )     *     (  ( currentWindow . getEndDate (  )  . getTime (  )  )     -    cutoffTime )  )     /    windowDuration ;", "bytesInLastSecond    +  =    adjustedBytes ;", "}", "long   currentBlockTransferRate    =     (  ( currentWindow . getBytesTransferred (  )  )     *     1  0  0  0  )     /    windowDuration ;", "maxSingleBlockTransferRate    =    Math . max ( maxSingleBlockTransferRate ,    currentBlockTransferRate )  ;", "}", "updateBytesTransferred ( updateWrite ,    bytesInLastSecond )  ;", "long   aggregateTransferRate    =    bytesInLastSecond ;", "long   maxObservedTransferRate    =    Math . max ( aggregateTransferRate ,    maxSingleBlockTransferRate )  ;", "updateBytesTransferRate ( updateWrite ,    maxObservedTransferRate )  ;", "}", "METHOD_END"], "methodName": ["triggerUpdate"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "if    ( updateWrite )     {", "instrumentation . currentUploadBytesPerSecond ( bytesPerSecond )  ;", "} else    {", "instrumentation . currentDownloadBytesPerSecond ( bytesPerSecond )  ;", "}", "}", "METHOD_END"], "methodName": ["updateBytesTransferRate"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "if    ( updateWrite )     {", "instrumentation . updateBytesWrittenInLastSecond ( bytes )  ;", "} else    {", "instrumentation . updateBytesReadInLastSecond ( bytes )  ;", "}", "}", "METHOD_END"], "methodName": ["updateBytesTransferred"], "fileName": "org.apache.hadoop.fs.azure.metrics.BandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "ErrorMetricUpdater   listener    =    new   ErrorMetricUpdater ( operationContext ,    instrumentation )  ;", "operationContext . getResponseReceivedEventHandler (  )  . addListener ( listener )  ;", "}", "METHOD_END"], "methodName": ["hook"], "fileName": "org.apache.hadoop.fs.azure.metrics.ErrorMetricUpdater"}, {"methodBody": ["METHOD_START", "{", "String   lengthString    =    connection . getRequestProperty ( CONTENT _ LENGTH )  ;", "if    ( lengthString    !  =    null )     {", "return   Long . parseLong ( lengthString )  ;", "} else    {", "return    0  ;", "}", "}", "METHOD_END"], "methodName": ["getRequestContentLength"], "fileName": "org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater"}, {"methodBody": ["METHOD_START", "{", "return   connection . getContentLength (  )  ;", "}", "METHOD_END"], "methodName": ["getResponseContentLength"], "fileName": "org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater"}, {"methodBody": ["METHOD_START", "{", "ResponseReceivedMetricUpdater   listener    =    new   ResponseReceivedMetricUpdater ( operationContext ,    instrumentation ,    blockUploadGaugeUpdater )  ;", "operationContext . getResponseReceivedEventHandler (  )  . addListener ( listener )  ;", "}", "METHOD_END"], "methodName": ["hook"], "fileName": "org.apache.hadoop.fs.azure.metrics.ResponseReceivedMetricUpdater"}, {"methodBody": ["METHOD_START", "{", "currentPoints . offer ( new   RollingWindowAverage . DataPoint ( new   Date (  )  ,    value )  )  ;", "cleanupOldPoints (  )  ;", "}", "METHOD_END"], "methodName": ["addPoint"], "fileName": "org.apache.hadoop.fs.azure.metrics.RollingWindowAverage"}, {"methodBody": ["METHOD_START", "{", "Date   cutoffTime    =    new   Date (  (  ( new   Date (  )  . getTime (  )  )     -     ( windowSizeMs )  )  )  ;", "while    (  (  !  ( currentPoints . isEmpty (  )  )  )     &  &     ( currentPoints . peekFirst (  )  . getEventTime (  )  . before ( cutoffTime )  )  )     {", "currentPoints . removeFirst (  )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanupOldPoints"], "fileName": "org.apache.hadoop.fs.azure.metrics.RollingWindowAverage"}, {"methodBody": ["METHOD_START", "{", "cleanupOldPoints (  )  ;", "if    ( currentPoints . isEmpty (  )  )     {", "return    0  ;", "}", "long   sum    =     0  ;", "for    (  . DataPoint   current    :    currentPoints )     {", "sum    +  =    current . getValue (  )  ;", "}", "return   sum    /     ( currentPoints . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentAverage"], "fileName": "org.apache.hadoop.fs.azure.metrics.RollingWindowAverage"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,    AzureFileSystemInstrumentation . WASB _ CLIENT _ ERRORS )  )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,    AzureFileSystemInstrumentation . WASB _ SERVER _ ERRORS )  )  ;", "}", "METHOD_END"], "methodName": ["assertNoErrors"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "assertCounter ( AzureFileSystemInstrumentation . WASB _ WEB _ RESPONSES ,     ( base    +    expected )  ,    getMyMetrics (  )  )  ;", "return   base    +    expected ;", "}", "METHOD_END"], "methodName": ["assertWebResponsesEquals"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   currentResponses    =    getCurrentWebResponses (  )  ;", "long   justOper    =    currentResponses    -    base ;", "Assert . assertTrue ( String . format (  \" Web   responses   expected   in   range    [  % d ,     % d ]  ,    but   was    % d .  \"  ,    inclusiveLowerLimit ,    inclusiveUpperLimit ,    justOper )  ,     (  ( justOper    >  =    inclusiveLowerLimit )     &  &     ( justOper    <  =    inclusiveUpperLimit )  )  )  ;", "return   currentResponses ;", "}", "METHOD_END"], "methodName": ["assertWebResponsesInRange"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "NativeAzureFileSystem   azureFs    =     (  ( NativeAzureFileSystem )     ( fs )  )  ;", "AzureNativeFileSystemStore   azureStore    =    azureFs . getStore (  )  ;", "return   azureStore . getBandwidthGaugeUpdater (  )  ;", "}", "METHOD_END"], "methodName": ["getBandwidthGaugeUpdater"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   assertWebResponsesEquals (  0  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["getBaseWebResponses"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   AzureMetricsTestUtil . getCurrentWebResponses ( getInstrumentation (  )  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentWebResponses"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NativeAzureFileSystem )     ( fs )  )  . getInstrumentation (  )  ;", "}", "METHOD_END"], "methodName": ["getInstrumentation"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "return   getMetrics ( getInstrumentation (  )  )  ;", "}", "METHOD_END"], "methodName": ["getMyMetrics"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  (  ( opName    +     \"    took    \"  )     +     (  ( getCurrentWebResponses (  )  )     -    base )  )     +     \"    web   responses   to   complete .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["logOpResponseCount"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    data    =    new   byte [ size ]  ;", "Arrays . fill ( data ,     (  ( byte )     (  5  )  )  )  ;", "return   data ;", "}", "METHOD_END"], "methodName": ["nonZeroByteArray"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "testAccount    =    AzureBlobStorageTestAccount . create (  )  ;", "if    (  ( testAccount )     !  =    null )     {", "fs    =    testAccount . get (  )  ;", "}", "Assume . assumeNotNull ( testAccount )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "if    (  ( testAccount )     !  =    null )     {", "testAccount . cleanup (  )  ;", "testAccount    =    null ;", "fs    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "String   directoryName    =     \" metricsTestDirectory _ ClientError \"  ;", "Path   directoryPath    =    new   Path (  (  \"  /  \"     +    directoryName )  )  ;", "Assert . assertTrue ( fs . mkdirs ( directoryPath )  )  ;", "String   leaseID    =    testAccount . acquireShortLease ( directoryName )  ;", "try    {", "try    {", "fs . delete ( directoryPath ,    true )  ;", "Assert . assertTrue (  \" Should ' ve   thrown .  \"  ,    false )  ;", "}    catch    ( AzureException   ex )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +    ex )  ,    ex . getMessage (  )  . contains (  \" lease \"  )  )  ;", "}", "Assert . assertEquals (  1  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ CLIENT _ ERRORS )  )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ SERVER _ ERRORS )  )  ;", "}    finally    {", "testAccount . releaseLease ( leaseID ,    directoryName )  ;", "}", "}", "METHOD_END"], "methodName": ["testClientErrorMetrics"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "String   accountName    =    testAccount . getRealAccount (  )  . getBlobEndpoint (  )  . getAuthority (  )  ;", "String   containerName    =    testAccount . getRealContainer (  )  . getName (  )  ;", "MetricsRecordBuilder   myMetrics    =    getMyMetrics (  )  ;", "verify ( myMetrics )  . add ( argThat ( new    . TagMatcher (  \" accountName \"  ,    accountName )  )  )  ;", "verify ( myMetrics )  . add ( argThat ( new    . TagMatcher (  \" containerName \"  ,    containerName )  )  )  ;", "verify ( myMetrics )  . add ( argThat ( new    . TagMatcher (  \" Context \"  ,     \" azureFileSystem \"  )  )  )  ;", "verify ( myMetrics )  . add ( argThat ( new    . TagExistsMatcher (  \" wasbFileSystemId \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testMetricTags"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   base    =    getBaseWebResponses (  )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getCurrentBytesWritten ( getInstrumentation (  )  )  )  ;", "Path   filePath    =    new   Path (  \"  / metricsTest _ webResponses \"  )  ;", "final   int   FILE _ SIZE    =     (  1  0  0     *     1  0  2  4  )     *     1  0  2  4  ;", "getBandwidthGaugeUpdater (  )  . suppressAutoUpdate (  )  ;", "OutputStream   outputStream    =    fs . create ( filePath )  ;", "outputStream . write ( new   byte [ FILE _ SIZE ]  )  ;", "outputStream . close (  )  ;", "logOpResponseCount (  \" Creating   a    1  0  0    MB   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     2  0  ,     5  0  )  ;", "getBandwidthGaugeUpdater (  )  . triggerUpdate ( true )  ;", "long   totalBytesWritten    =    AzureMetricsTestUtil . getCurrentTotalBytesWritten ( getInstrumentation (  )  )  ;", "Assert . assertTrue (  (  (  (  (  \" The   total   bytes   written       \"     +    totalBytesWritten )     +     \"    is   pretty   far   from   the   expected   range   of   around    \"  )     +    FILE _ SIZE )     +     \"    bytes   plus   a   little   overhead .  \"  )  ,     (  ( totalBytesWritten    >  =    FILE _ SIZE )     &  &     ( totalBytesWritten    <     ( FILE _ SIZE    *     2  )  )  )  )  ;", "long   uploadRate    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,     . WASB _ UPLOAD _ RATE )  ;", "System . out . println (  (  (  \" Upload   rate :     \"     +    uploadRate )     +     \"    bytes / second .  \"  )  )  ;", "long   uploadLatency    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,     . WASB _ UPLOAD _ LATENCY )  ;", "System . out . println (  (  \" Upload   latency :     \"     +    uploadLatency )  )  ;", "Assert . assertTrue (  (  (  \" The   upload   latency    \"     +    uploadLatency )     +     \"    should   be   greater   than   zero   now   that   I ' ve   just   uploaded   a   file .  \"  )  ,     ( uploadLatency    >     0  )  )  ;", "InputStream   inputStream    =    fs . open ( filePath )  ;", "int   count    =     0  ;", "while    (  ( inputStream . read (  )  )     >  =     0  )     {", "count +  +  ;", "}", "inputStream . close (  )  ;", "Assert . assertEquals ( FILE _ SIZE ,    count )  ;", "logOpResponseCount (  \" Reading   a    1  0  0    MB   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     2  0  ,     4  0  )  ;", "getBandwidthGaugeUpdater (  )  . triggerUpdate ( false )  ;", "long   totalBytesRead    =    AzureMetricsTestUtil . getCurrentTotalBytesRead ( getInstrumentation (  )  )  ;", "Assert . assertEquals ( FILE _ SIZE ,    totalBytesRead )  ;", "long   downloadRate    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,     . WASB _ DOWNLOAD _ RATE )  ;", "System . out . println (  (  (  \" Download   rate :     \"     +    downloadRate )     +     \"    bytes / second .  \"  )  )  ;", "long   downloadLatency    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,     . WASB _ DOWNLOAD _ LATENCY )  ;", "System . out . println (  (  \" Download   latency :     \"     +    downloadLatency )  )  ;", "Assert . assertTrue (  (  (  \" The   download   latency    \"     +    downloadLatency )     +     \"    should   be   greater   than   zero   now   that   I ' ve   just   downloaded   a   file .  \"  )  ,     ( downloadLatency    >     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsOnBigFileCreateRead"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   base    =    getBaseWebResponses (  )  ;", "Path   originalDirName    =    new   Path (  \"  / TestDirectory _ RenameStart \"  )  ;", "Path   innerFileName    =    new   Path ( originalDirName ,     \" innerFile \"  )  ;", "Path   destDirName    =    new   Path (  \"  / TestDirectory _ RenameFinal \"  )  ;", "Assert . assertTrue ( fs . mkdirs ( originalDirName )  )  ;", "base    =    getCurrentWebResponses (  )  ;", "Assert . assertTrue ( fs . createNewFile ( innerFileName )  )  ;", "base    =    getCurrentWebResponses (  )  ;", "Assert . assertTrue ( fs . rename ( originalDirName ,    destDirName )  )  ;", "logOpResponseCount (  \" Renaming   a   directory \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     1  ,     2  0  )  ;", "assertNoErrors (  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsOnDirRename"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   base    =    getBaseWebResponses (  )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getCurrentBytesWritten ( getInstrumentation (  )  )  )  ;", "Path   filePath    =    new   Path (  \"  / metricsTest _ webResponses \"  )  ;", "final   int   FILE _ SIZE    =     1  0  0  0  ;", "getBandwidthGaugeUpdater (  )  . suppressAutoUpdate (  )  ;", "Date   start    =    new   Date (  )  ;", "OutputStream   outputStream    =    fs . create ( filePath )  ;", "outputStream . write (  . nonZeroByteArray ( FILE _ SIZE )  )  ;", "outputStream . close (  )  ;", "long   uploadDurationMs    =     ( new   Date (  )  . getTime (  )  )     -     ( start . getTime (  )  )  ;", "logOpResponseCount (  \" Creating   a    1 K   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     2  ,     1  5  )  ;", "getBandwidthGaugeUpdater (  )  . triggerUpdate ( true )  ;", "long   bytesWritten    =    AzureMetricsTestUtil . getCurrentBytesWritten ( getInstrumentation (  )  )  ;", "Assert . assertTrue (  (  (  (  (  \" The   bytes   written   in   the   last   second    \"     +    bytesWritten )     +     \"    is   pretty   far   from   the   expected   range   of   around    \"  )     +    FILE _ SIZE )     +     \"    bytes   plus   a   little   overhead .  \"  )  ,     (  ( bytesWritten    >     ( FILE _ SIZE    /     2  )  )     &  &     ( bytesWritten    <     ( FILE _ SIZE    *     2  )  )  )  )  ;", "long   totalBytesWritten    =    AzureMetricsTestUtil . getCurrentTotalBytesWritten ( getInstrumentation (  )  )  ;", "Assert . assertTrue (  (  (  (  (  \" The   total   bytes   written       \"     +    totalBytesWritten )     +     \"    is   pretty   far   from   the   expected   range   of   around    \"  )     +    FILE _ SIZE )     +     \"    bytes   plus   a   little   overhead .  \"  )  ,     (  ( totalBytesWritten    >  =    FILE _ SIZE )     &  &     ( totalBytesWritten    <     ( FILE _ SIZE    *     2  )  )  )  )  ;", "long   uploadRate    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,    AzureFileSystemInstrumentation . WASB _ UPLOAD _ RATE )  ;", "System . out . println (  (  (  \" Upload   rate :     \"     +    uploadRate )     +     \"    bytes / second .  \"  )  )  ;", "long   expectedRate    =     ( FILE _ SIZE    *     1  0  0  0 L )     /    uploadDurationMs ;", "Assert . assertTrue (  (  (  (  (  (  (  \" The   upload   rate    \"     +    uploadRate )     +     \"    is   below   the   expected   range   of   around    \"  )     +    expectedRate )     +     \"    bytes / second   that   the   unit   test   observed .    This   should   never   be \"  )     +     \"    the   case   since   the   test   underestimates   the   rate   by   looking   at    \"  )     +     \"    end - to - end   time   instead   of   just   block   upload   time .  \"  )  ,     ( uploadRate    >  =    expectedRate )  )  ;", "long   uploadLatency    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,    AzureFileSystemInstrumentation . WASB _ UPLOAD _ LATENCY )  ;", "System . out . println (  (  \" Upload   latency :     \"     +    uploadLatency )  )  ;", "long   expectedLatency    =    uploadDurationMs ;", "Assert . assertTrue (  (  (  \" The   upload   latency    \"     +    uploadLatency )     +     \"    should   be   greater   than   zero   now   that   I ' ve   just   uploaded   a   file .  \"  )  ,     ( uploadLatency    >     0  )  )  ;", "Assert . assertTrue (  (  (  (  (  (  (  \" The   upload   latency    \"     +    uploadLatency )     +     \"    is   more   than   the   expected   range   of   around    \"  )     +    expectedLatency )     +     \"    milliseconds   that   the   unit   test   observed .    This   should   never   be \"  )     +     \"    the   case   since   the   test   overestimates   the   latency   by   looking   at    \"  )     +     \"    end - to - end   time   instead   of   just   block   upload   time .  \"  )  ,     ( uploadLatency    <  =    expectedLatency )  )  ;", "start    =    new   Date (  )  ;", "InputStream   inputStream    =    fs . open ( filePath )  ;", "int   count    =     0  ;", "while    (  ( inputStream . read (  )  )     >  =     0  )     {", "count +  +  ;", "}", "inputStream . close (  )  ;", "long   downloadDurationMs    =     ( new   Date (  )  . getTime (  )  )     -     ( start . getTime (  )  )  ;", "Assert . assertEquals ( FILE _ SIZE ,    count )  ;", "logOpResponseCount (  \" Reading   a    1 K   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     1  ,     1  0  )  ;", "getBandwidthGaugeUpdater (  )  . triggerUpdate ( false )  ;", "long   totalBytesRead    =    AzureMetricsTestUtil . getCurrentTotalBytesRead ( getInstrumentation (  )  )  ;", "Assert . assertEquals ( FILE _ SIZE ,    totalBytesRead )  ;", "long   bytesRead    =    AzureMetricsTestUtil . getCurrentBytesRead ( getInstrumentation (  )  )  ;", "Assert . assertTrue (  (  (  (  (  \" The   bytes   read   in   the   last   second    \"     +    bytesRead )     +     \"    is   pretty   far   from   the   expected   range   of   around    \"  )     +    FILE _ SIZE )     +     \"    bytes   plus   a   little   overhead .  \"  )  ,     (  ( bytesRead    >     ( FILE _ SIZE    /     2  )  )     &  &     ( bytesRead    <     ( FILE _ SIZE    *     2  )  )  )  )  ;", "long   downloadRate    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,    AzureFileSystemInstrumentation . WASB _ DOWNLOAD _ RATE )  ;", "System . out . println (  (  (  \" Download   rate :     \"     +    downloadRate )     +     \"    bytes / second .  \"  )  )  ;", "expectedRate    =     ( FILE _ SIZE    *     1  0  0  0 L )     /    downloadDurationMs ;", "Assert . assertTrue (  (  (  (  (  (  (  \" The   download   rate    \"     +    downloadRate )     +     \"    is   below   the   expected   range   of   around    \"  )     +    expectedRate )     +     \"    bytes / second   that   the   unit   test   observed .    This   should   never   be \"  )     +     \"    the   case   since   the   test   underestimates   the   rate   by   looking   at    \"  )     +     \"    end - to - end   time   instead   of   just   block   download   time .  \"  )  ,     ( downloadRate    >  =    expectedRate )  )  ;", "long   downloadLatency    =    AzureMetricsTestUtil . getLongGaugeValue ( getInstrumentation (  )  ,    AzureFileSystemInstrumentation . WASB _ DOWNLOAD _ LATENCY )  ;", "System . out . println (  (  \" Download   latency :     \"     +    downloadLatency )  )  ;", "expectedLatency    =    downloadDurationMs ;", "Assert . assertTrue (  (  (  \" The   download   latency    \"     +    downloadLatency )     +     \"    should   be   greater   than   zero   now   that   I ' ve   just   downloaded   a   file .  \"  )  ,     ( downloadLatency    >     0  )  )  ;", "Assert . assertTrue (  (  (  (  (  (  (  \" The   download   latency    \"     +    downloadLatency )     +     \"    is   more   than   the   expected   range   of   around    \"  )     +    expectedLatency )     +     \"    milliseconds   that   the   unit   test   observed .    This   should   never   be \"  )     +     \"    the   case   since   the   test   overestimates   the   latency   by   looking   at    \"  )     +     \"    end - to - end   time   instead   of   just   block   download   time .  \"  )  ,     ( downloadLatency    <  =    expectedLatency )  )  ;", "assertNoErrors (  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsOnFileCreateRead"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   base    =    getBaseWebResponses (  )  ;", "Path   filePath    =    new   Path (  \"  / metricsTest _ delete \"  )  ;", "Assert . assertFalse ( fs . exists ( filePath )  )  ;", "logOpResponseCount (  \" Checking   file   existence   for   non - existent   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     1  ,     3  )  ;", "Assert . assertTrue ( fs . createNewFile ( filePath )  )  ;", "base    =    getCurrentWebResponses (  )  ;", "Assert . assertTrue ( fs . exists ( filePath )  )  ;", "logOpResponseCount (  \" Checking   file   existence   for   existent   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     1  ,     2  )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ FILES _ DELETED )  )  ;", "Assert . assertTrue ( fs . delete ( filePath ,    false )  )  ;", "logOpResponseCount (  \" Deleting   a   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     1  ,     4  )  ;", "Assert . assertEquals (  1  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ FILES _ DELETED )  )  ;", "assertNoErrors (  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsOnFileExistsDelete"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   base    =    getBaseWebResponses (  )  ;", "Path   originalPath    =    new   Path (  \"  / metricsTest _ RenameStart \"  )  ;", "Path   destinationPath    =    new   Path (  \"  / metricsTest _ RenameFinal \"  )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ FILES _ CREATED )  )  ;", "Assert . assertTrue ( fs . createNewFile ( originalPath )  )  ;", "logOpResponseCount (  \" Creating   an   empty   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     2  ,     2  0  )  ;", "Assert . assertEquals (  1  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ FILES _ CREATED )  )  ;", "Assert . assertTrue ( fs . rename ( originalPath ,    destinationPath )  )  ;", "logOpResponseCount (  \" Renaming   a   file \"  ,    base )  ;", "base    =    assertWebResponsesInRange ( base ,     2  ,     1  5  )  ;", "assertNoErrors (  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsOnFileRename"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "long   base    =    getBaseWebResponses (  )  ;", "Assert . assertTrue ( fs . mkdirs ( new   Path (  \" a \"  )  )  )  ;", "base    =    assertWebResponsesInRange ( base ,     1  ,     1  2  )  ;", "Assert . assertEquals (  1  ,    AzureMetricsTestUtil . getLongCounterValue ( getInstrumentation (  )  ,     . WASB _ DIRECTORIES _ CREATED )  )  ;", "Assert . assertEquals (  1  ,    fs . listStatus ( new   Path (  \"  /  \"  )  )  . length )  ;", "base    =    assertWebResponsesEquals ( base ,     1  )  ;", "assertNoErrors (  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsOnMkdirList"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestAzureFileSystemInstrumentation"}, {"methodBody": ["METHOD_START", "{", "int   c    =     0  ;", "Map < Thread ,    StackTraceElement [  ]  >    stacksStart    =    Thread . getAllStackTraces (  )  ;", "for    ( Thread   t    :    stacksStart . keySet (  )  )     {", "if    ( t . getName (  )  . equals (  . THREAD _ NAME )  )     {", "c +  +  ;", "}", "}", "return   c ;", "}", "METHOD_END"], "methodName": ["getWasbThreadCount"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestBandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "System . gc (  )  ;", "System . runFinalization (  )  ;", "int   nThreadsStart    =    getWasbThreadCount (  )  ;", "Assert . assertTrue (  \" Existing   WASB   threads   have   not   been   cleared \"  ,     ( nThreadsStart    =  =     0  )  )  ;", "final   int   nFilesystemsToSpawn    =     1  0  ;", "AzureBlobStorageTestAccount   testAccount    =    null ;", "for    ( int   i    =     0  ;    i    <    nFilesystemsToSpawn ;    i +  +  )     {", "testAccount    =    AzureBlobStorageTestAccount . createMock (  )  ;", "testAccount . getFileSystem (  )  ;", "}", "int   nThreadsAfterSpawn    =    getWasbThreadCount (  )  ;", "Assume . assumeTrue (  \" Background   threads   should   have   spawned .  \"  ,     ( nThreadsAfterSpawn    =  =     1  0  )  )  ;", "testAccount    =    null ;", "System . gc (  )  ;", "System . runFinalization (  )  ;", "int   nThreadsAfterCleanup    =    getWasbThreadCount (  )  ;", "Assert . assertTrue (  \" Finalizers   should   have   reduced   the   thread   count .        \"  ,     ( nThreadsAfterCleanup    =  =     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testFinalizerThreadShutdown"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestBandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "final   AzureFileSystemInstrumentation   instrumentation    =    new   AzureFileSystemInstrumentation ( new   Configuration (  )  )  ;", "final      updater    =    new    ( instrumentation ,     1  0  0  0  ,    true )  ;", "Thread [  ]    threads    =    new   Thread [  1  0  ]  ;", "for    ( int   i    =     0  ;    i    <     ( threads . length )  ;    i +  +  )     {", "threads [ i ]     =    new   Thread ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "updater . blockDownloaded ( new   Date (  )  ,    new   Date (  )  ,     1  0  )  ;", "updater . blockDownloaded ( new   Date (  0  )  ,    new   Date (  0  )  ,     1  0  )  ;", "}", "}  )  ;", "}", "for    ( Thread   t    :    threads )     {", "t . start (  )  ;", "}", "for    ( Thread   t    :    threads )     {", "t . join (  )  ;", "}", "updater . triggerUpdate ( false )  ;", "Assert . assertEquals (  (  1  0     *     ( threads . length )  )  ,    AzureMetricsTestUtil . getCurrentBytesRead ( instrumentation )  )  ;", "updater . close (  )  ;", "}", "METHOD_END"], "methodName": ["testMultiThreaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestBandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "AzureFileSystemInstrumentation   instrumentation    =    new   AzureFileSystemInstrumentation ( new   Configuration (  )  )  ;", "updater    =    new    ( instrumentation ,     1  0  0  0  ,    true )  ;", "updater . triggerUpdate ( true )  ;", "Assert . assertEquals (  0  ,    AzureMetricsTestUtil . getCurrentBytesWritten ( instrumentation )  )  ;", "updater . blockUploaded ( new   Date (  )  ,    new   Date (  )  ,     1  5  0  )  ;", "updater . triggerUpdate ( true )  ;", "Assert . assertEquals (  1  5  0  ,    AzureMetricsTestUtil . getCurrentBytesWritten ( instrumentation )  )  ;", "updater . blockUploaded ( new   Date (  (  ( new   Date (  )  . getTime (  )  )     -     1  0  0  0  0  )  )  ,    new   Date (  )  ,     2  0  0  )  ;", "updater . triggerUpdate ( true )  ;", "long   currentBytes    =    AzureMetricsTestUtil . getCurrentBytesWritten ( instrumentation )  ;", "Assert . assertTrue (  (  (  \" We   expect   around    (  2  0  0  /  1  0     =     2  0  )    bytes   written   as   the   gauge   value .  \"     +     \" Got    \"  )     +    currentBytes )  ,     (  ( currentBytes    >     1  8  )     &  &     ( currentBytes    <     2  2  )  )  )  ;", "updater . close (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleThreaded"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestBandwidthGaugeUpdater"}, {"methodBody": ["METHOD_START", "{", "return   testAccount . getLatestMetricValue ( TestNativeAzureFileSystemMetricsSystem . WASB _ FILES _ CREATED ,     0  )  . intValue (  )  ;", "}", "METHOD_END"], "methodName": ["getFilesCreated"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestNativeAzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "AzureBlobStorageTestAccount   a 1  ;", "AzureBlobStorageTestAccount   a 2  ;", "AzureBlobStorageTestAccount   a 3  ;", "a 1     =    AzureBlobStorageTestAccount . createMock (  )  ;", "assertEquals (  0  ,     . getFilesCreated ( a 1  )  )  ;", "a 2     =    AzureBlobStorageTestAccount . createMock (  )  ;", "assertEquals (  0  ,     . getFilesCreated ( a 2  )  )  ;", "a 1  . getFileSystem (  )  . create ( new   Path (  \"  / foo \"  )  )  . close (  )  ;", "a 1  . getFileSystem (  )  . create ( new   Path (  \"  / bar \"  )  )  . close (  )  ;", "a 2  . getFileSystem (  )  . create ( new   Path (  \"  / baz \"  )  )  . close (  )  ;", "assertEquals (  0  ,     . getFilesCreated ( a 1  )  )  ;", "assertEquals (  0  ,     . getFilesCreated ( a 2  )  )  ;", "a 1  . closeFileSystem (  )  ;", "a 2  . closeFileSystem (  )  ;", "assertEquals (  2  ,     . getFilesCreated ( a 1  )  )  ;", "assertEquals (  1  ,     . getFilesCreated ( a 2  )  )  ;", "a 3     =    AzureBlobStorageTestAccount . createMock (  )  ;", "assertEquals (  0  ,     . getFilesCreated ( a 3  )  )  ;", "a 3  . closeFileSystem (  )  ;", "assertEquals (  0  ,     . getFilesCreated ( a 3  )  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsAcrossFileSystems"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestNativeAzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "String   name 1     =    NativeAzureFileSystem . newMetricsSourceName (  )  ;", "String   name 2     =    NativeAzureFileSystem . newMetricsSourceName (  )  ;", "assertTrue ( name 1  . startsWith (  \"  \"  )  )  ;", "assertTrue ( name 2  . startsWith (  \"  \"  )  )  ;", "assertTrue (  (  !  ( name 1  . equals ( name 2  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testMetricsSourceNames"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestNativeAzureFileSystemMetricsSystem"}, {"methodBody": ["METHOD_START", "{", "RollingWindowAverage   average    =    new   RollingWindowAverage (  1  0  0  )  ;", "assertEquals (  0  ,    average . getCurrentAverage (  )  )  ;", "average . addPoint (  5  )  ;", "assertEquals (  5  ,    average . getCurrentAverage (  )  )  ;", "Thread . sleep (  5  0  )  ;", "average . addPoint (  1  5  )  ;", "assertEquals (  1  0  ,    average . getCurrentAverage (  )  )  ;", "Thread . sleep (  6  0  )  ;", "assertEquals (  1  5  ,    average . getCurrentAverage (  )  )  ;", "Thread . sleep (  5  0  )  ;", "assertEquals (  0  ,    average . getCurrentAverage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBasicFunctionality"], "fileName": "org.apache.hadoop.fs.azure.metrics.TestRollingWindowAverage"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . assertDeleted ( fs ,    path ,    recursive )  ;", "}", "METHOD_END"], "methodName": ["assertDeleted"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . assertPathExists ( fs ,    message ,    path )  ;", "}", "METHOD_END"], "methodName": ["assertExists"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . assertIsFile ( fs ,    filename )  ;", "}", "METHOD_END"], "methodName": ["assertIsFile"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  (  ( text    +     \"    wrong   read   result    \"  )     +    result )  ,     (  -  1  )  ,    result )  ;", "}", "METHOD_END"], "methodName": ["assertMinusOne"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "assertTrue ( message ,     ( actual    !  =    expected )  )  ;", "}", "METHOD_END"], "methodName": ["assertNotEqual"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "OutputStream   nativeStream    =    out . getWrappedStream (  )  ;", "int   written    =    getPartitionsWritten ( out )  ;", "if    ( written    !  =    expected )     {", "Assert . fail (  (  (  (  (  (  (  (  ( action    +     \"  :     \"  )     +     ( TestPartitionedUploads . WRONG _ PARTITION _ COUNT )  )     +     \"     +    expected :     \"  )     +    expected )     +     \"    actual :     \"  )     +    written )     +     \"     -  -     \"  )     +    nativeStream )  )  ;", "}", "}", "METHOD_END"], "methodName": ["assertPartitionsWritten"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . assertPathDoesNotExist ( fs ,    message ,    path )  ;", "}", "METHOD_END"], "methodName": ["assertPathDoesNotExist"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeTrue ( renameSupported (  )  )  ;", "}", "METHOD_END"], "methodName": ["assumeRenameSupported"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "if    (  ( SwiftFileSystemBaseTest . lastFs )     !  =    null )     {", "List < DurationStats >    statistics    =    SwiftFileSystemBaseTest . lastFs . getOperationStatistics (  )  ;", "for    ( DurationStats   stat    :    statistics )     {", "SwiftFileSystemBaseTest . LOG . info ( stat . toString (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["classTearDown"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   new   Configuration (  )  ;", "}", "METHOD_END"], "methodName": ["createConfiguration"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   out    =    fs . create ( path )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createEmptyFile"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "createFile ( path ,    data )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   out    =    fs . create ( path )  ;", "out . write ( sourceData ,     0  ,    sourceData . length )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftNativeFileSystem   swiftNativeFileSystem    =    new   SwiftNativeFileSystem (  )  ;", "return   swiftNativeFileSystem ;", "}", "METHOD_END"], "methodName": ["createSwiftFS"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . noteAction ( description )  ;", "}", "METHOD_END"], "methodName": ["describe"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return    1  0  2  4  ;", "}", "METHOD_END"], "methodName": ["getBlockSize"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   SwiftTestUtils . getServiceURI ( createConfiguration (  )  )  ;", "}", "METHOD_END"], "methodName": ["getFilesystemURI"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   fs ;", "}", "METHOD_END"], "methodName": ["getFs"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   SwiftNativeFileSystem . getPartitionsWritten ( out )  ;", "}", "METHOD_END"], "methodName": ["getPartitionsWritten"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "String   lsDst    =    ls ( dst )  ;", "Path   parent    =    dst . getParent (  )  ;", "String   lsParent    =     ( parent    !  =    null )     ?    ls ( parent )     :     \"  \"  ;", "return    (  (  (  (  (  (  \"       result   of    \"     +    src )     +     \"     =  >     \"  )     +    dst )     +     \"     -     \"  )     +    lsDst )     +     \"     \\ n \"  )     +    lsParent ;", "}", "METHOD_END"], "methodName": ["getRenameOutcome"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   fs . getStore (  )  ;", "}", "METHOD_END"], "methodName": ["getStore"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   SwiftTestUtils . ls ( fs ,    path )  ;", "}", "METHOD_END"], "methodName": ["ls"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  \" Failed   to   mkdir \"     +    path )  ,    fs . mkdirs ( path )  )  ;", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( pathString )  . makeQualified ( fs )  ;", "}", "METHOD_END"], "methodName": ["path"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "if    ( renameMustSucceed )     {", "renameToSuccess ( src ,    dst ,    srcExists ,    dstExists )  ;", "} else    {", "renameToFailure ( src ,    dst )  ;", "}", "}", "METHOD_END"], "methodName": ["rename"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "return   true ;", "}", "METHOD_END"], "methodName": ["renameSupported"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "try    {", "getStore (  )  . rename ( src ,    dst )  ;", "fail (  (  (  (  (  \" Expected   failure   renaming    \"     +    src )     +     \"    to    \"  )     +    dst )     +     \"  -    but   got   success \"  )  )  ;", "}    catch    ( SwiftOperationFailedException   e )     {", ". LOG . debug (  (  \" Rename   failed    ( expected )  :  \"     +    e )  )  ;", "}    catch    ( FileNotFoundException   e )     {", ". LOG . debug (  (  \" Rename   failed    ( expected )  :  \"     +    e )  )  ;", "}", "}", "METHOD_END"], "methodName": ["renameToFailure"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "getStore (  )  . rename ( src ,    dst )  ;", "String   outcome    =    getRenameOutcome ( src ,    dst )  ;", "assertEquals (  (  (  (  \" Source    \"     +    src )     +     \" exists :     \"  )     +    outcome )  ,    srcExists ,    exists ( src )  )  ;", "assertEquals (  (  (  (  \" Destination    \"     +    dstExists )     +     \"    exists \"  )     +    outcome )  ,    dstExists ,    exists ( dst )  )  ;", "}", "METHOD_END"], "methodName": ["renameToSuccess"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . noteAction (  \" setup \"  )  ;", "final   URI   uri    =    getFilesystemURI (  )  ;", "conf    =    createConfiguration (  )  ;", "fs    =    createSwiftFS (  )  ;", "try    {", "fs . initialize ( uri ,    conf )  ;", "}    catch    ( IOException   e )     {", "fs    =    null ;", "throw   e ;", "}", ". lastFs    =    fs ;", "SwiftTestUtils . noteAction (  \" setup   complete \"  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . cleanupInTeardown ( fs ,     \"  / test \"  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.fs.swift.SwiftFileSystemBaseTest"}, {"methodBody": ["METHOD_START", "{", "URL   url    =    this . getClass (  )  . getClassLoader (  )  . getResource ( resource )  ;", "if    ( url    !  =    null )     {", "printf (  \" resource    % s   is   at    % s \"  ,    resource ,    url )  ;", "} else    {", "printf (  \" resource    % s   is   not   on   the   classpath \"  ,    resource )  ;", "}", "}", "METHOD_END"], "methodName": ["locateResource"], "fileName": "org.apache.hadoop.fs.swift.TestLogResources"}, {"methodBody": ["METHOD_START", "{", "String   msg    =    String . format ( format ,    args )  ;", "System . out . printf (  ( msg    +     \"  \\ n \"  )  )  ;", ". LOG . info ( msg )  ;", "}", "METHOD_END"], "methodName": ["printf"], "fileName": "org.apache.hadoop.fs.swift.TestLogResources"}, {"methodBody": ["METHOD_START", "{", "locateResource (  \" commons - logging . properties \"  )  ;", "}", "METHOD_END"], "methodName": ["testCommonsLoggingProps"], "fileName": "org.apache.hadoop.fs.swift.TestLogResources"}, {"methodBody": ["METHOD_START", "{", "locateResource (  \" log 4 j . properties \"  )  ;", "}", "METHOD_END"], "methodName": ["testWhichLog4JPropsFile"], "fileName": "org.apache.hadoop.fs.swift.TestLogResources"}, {"methodBody": ["METHOD_START", "{", "locateResource (  \" log 4 j . XML \"  )  ;", "}", "METHOD_END"], "methodName": ["testWhichLog4JXMLFile"], "fileName": "org.apache.hadoop.fs.swift.TestLogResources"}, {"methodBody": ["METHOD_START", "{", "IOUtils . closeStream ( instream )  ;", "instream    =    null ;", "}", "METHOD_END"], "methodName": ["cleanFile"], "fileName": "org.apache.hadoop.fs.swift.TestReadPastBuffer"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( readFile )  ;", "while    (  ( instream . read (  )  )     !  =     (  -  1  )  )  ;", "assertMinusOne (  (  \" reading   after   the    ( large )    file   was   read :     \"     +     ( instream )  )  ,    instream . read (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReadPastBufferSize"], "fileName": "org.apache.hadoop.fs.swift.TestReadPastBuffer"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( readFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "instream . seek (  (  (  . SEEK _ FILE _ LEN )     -     2  )  )  ;", "assertTrue (  \" Premature   EOF \"  ,     (  ( instream . read (  )  )     !  =     (  -  1  )  )  )  ;", "assertTrue (  \" Premature   EOF \"  ,     (  ( instream . read (  )  )     !  =     (  -  1  )  )  )  ;", "assertMinusOne (  \" read   past   end   of   file \"  ,    instream . read (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSeekAndReadPastEndOfFile"], "fileName": "org.apache.hadoop.fs.swift.TestReadPastBuffer"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( readFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "instream . seek (  (  (  . SEEK _ FILE _ LEN )     -     1  )  )  ;", "byte [  ]    buffer    =    new   byte [  1  ]  ;", "int   result    =    instream . read ( buffer ,     0  ,     1  )  ;", "result    =    instream . read ( buffer ,     0  ,     1  )  ;", "assertMinusOne (  \" read   past   end   of   file \"  ,    result )  ;", "result    =    instream . read ( buffer ,     0  ,     1  )  ;", "assertMinusOne (  \" read   past   end   of   file \"  ,    result )  ;", "result    =    instream . read ( buffer ,     0  ,     0  )  ;", "assertEquals (  \" EOF   checks   coming   before   read   range   check \"  ,     0  ,    result )  ;", "}", "METHOD_END"], "methodName": ["testSeekBulkReadPastEndOfFile"], "fileName": "org.apache.hadoop.fs.swift.TestReadPastBuffer"}, {"methodBody": ["METHOD_START", "{", "IOUtils . closeStream ( instream )  ;", "instream    =    null ;", "}", "METHOD_END"], "methodName": ["cleanFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "return   offset    &     2  5  5  ;", "}", "METHOD_END"], "methodName": ["expectedByte"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( zeroByteFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "byte [  ]    buffer    =    new   byte [  1  ]  ;", "int   result    =    instream . read ( buffer ,     0  ,     1  )  ;", "assertMinusOne (  \" block   read   zero   byte   file \"  ,    result )  ;", "}", "METHOD_END"], "methodName": ["testBlockReadZeroByteFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( smallSeekFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "try    {", "instream . seek (  (  -  1  )  )  ;", "long   p    =    instream . getPos (  )  ;", ". LOG . warn (  (  \" Seek   to    -  1    returned   a   position   of    \"     +    p )  )  ;", "int   result    =    instream . read (  )  ;", "fail (  (  (  (  \" expected   an   exception ,    got   data    \"     +    result )     +     \"    at   a   position   of    \"  )     +    p )  )  ;", "}    catch    ( IOException   e )     {", "}", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNegativeSeek"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "Path   testSeekFile    =    new   Path ( testPath ,     \" bigseekfile . txt \"  )  ;", "byte [  ]    block    =    SwiftTestUtils . dataset (  6  5  5  3  6  ,     0  ,     2  5  5  )  ;", "createFile ( testSeekFile ,    block )  ;", "instream    =    fs . open ( testSeekFile )  ;", "instream . seek (  3  9  9  9  9  )  ;", "assertTrue (  (  (  -  1  )     !  =     ( instream . read (  )  )  )  )  ;", "assertEquals (  4  0  0  0  0  ,    instream . getPos (  )  )  ;", "byte [  ]    readBuffer    =    new   byte [  2  5  6  ]  ;", "instream . read (  1  2  8  ,    readBuffer ,     0  ,    readBuffer . length )  ;", "assertEquals (  4  0  0  0  0  ,    instream . getPos (  )  )  ;", "assertEquals (  \"  @  4  0  0  0  0  \"  ,    block [  4  0  0  0  0  ]  ,     (  ( byte )     ( instream . read (  )  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     2  5  6  ;    i +  +  )     {", "assertEquals (  (  \"  @  \"     +    i )  ,    block [  ( i    +     1  2  8  )  ]  ,    readBuffer [ i ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPositionedBulkReadDoesntChangePosition"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( smallSeekFile )  ;", "try    {", "instream . seek (  . SMALL _ SEEK _ FILE _ LEN )  ;", "assertMinusOne (  \" read   after   seeking   past   EOF \"  ,    instream . read (  )  )  ;", "}    catch    ( EOFException   expected )     {", "}", "instream . seek (  1  )  ;", "assertTrue (  \" Premature   EOF \"  ,     (  ( instream . read (  )  )     !  =     (  -  1  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSeekAndPastEndOfFileThenReseekAndRead"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( smallSeekFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "instream . seek (  (  (  . SMALL _ SEEK _ FILE _ LEN )     -     2  )  )  ;", "assertTrue (  \" Premature   EOF \"  ,     (  ( instream . read (  )  )     !  =     (  -  1  )  )  )  ;", "assertTrue (  \" Premature   EOF \"  ,     (  ( instream . read (  )  )     !  =     (  -  1  )  )  )  ;", "assertMinusOne (  \" read   past   end   of   file \"  ,    instream . read (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSeekAndReadPastEndOfFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "Path   testSeekFile    =    new   Path ( testPath ,     \" bigseekfile . txt \"  )  ;", "byte [  ]    block    =    SwiftTestUtils . dataset (  6  5  5  3  6  ,     0  ,     2  5  5  )  ;", "createFile ( testSeekFile ,    block )  ;", "instream    =    fs . open ( testSeekFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "instream . seek (  0  )  ;", "int   result    =    instream . read (  )  ;", "assertEquals (  0  ,    result )  ;", "assertEquals (  1  ,    instream . read (  )  )  ;", "assertEquals (  2  ,    instream . read (  )  )  ;", "instream . seek (  3  2  7  6  8  )  ;", "assertEquals (  \"  @  3  2  7  6  8  \"  ,    block [  3  2  7  6  8  ]  ,     (  ( byte )     ( instream . read (  )  )  )  )  ;", "instream . seek (  4  0  0  0  0  )  ;", "assertEquals (  \"  @  4  0  0  0  0  \"  ,    block [  4  0  0  0  0  ]  ,     (  ( byte )     ( instream . read (  )  )  )  )  ;", "instream . seek (  8  1  9  1  )  ;", "assertEquals (  \"  @  8  1  9  1  \"  ,    block [  8  1  9  1  ]  ,     (  ( byte )     ( instream . read (  )  )  )  )  ;", "instream . seek (  0  )  ;", "assertEquals (  \"  @  0  \"  ,     0  ,     (  ( byte )     ( instream . read (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSeekBigFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( smallSeekFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "instream . seek (  0  )  ;", "int   result    =    instream . read (  )  ;", "assertEquals (  0  ,    result )  ;", "assertEquals (  1  ,    instream . read (  )  )  ;", "assertEquals (  2  ,    instream . getPos (  )  )  ;", "assertEquals (  2  ,    instream . read (  )  )  ;", "assertEquals (  3  ,    instream . getPos (  )  )  ;", "instream . seek (  1  2  8  )  ;", "assertEquals (  1  2  8  ,    instream . getPos (  )  )  ;", "assertEquals (  1  2  8  ,    instream . read (  )  )  ;", "instream . seek (  6  3  )  ;", "assertEquals (  6  3  ,    instream . read (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSeekFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( smallSeekFile )  ;", "instream . close (  )  ;", "try    {", "instream . seek (  0  )  ;", "}    catch    ( SwiftConnectionClosedException   e )     {", "}", "try    {", "instream . read (  )  ;", "}    catch    ( IOException   e )     {", "}", "try    {", "byte [  ]    buffer    =    new   byte [  1  ]  ;", "int   result    =    instream . read ( buffer ,     0  ,     1  )  ;", "}    catch    ( IOException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testSeekReadClosedFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "instream    =    fs . open ( zeroByteFile )  ;", "assertEquals (  0  ,    instream . getPos (  )  )  ;", "int   result    =    instream . read (  )  ;", "assertMinusOne (  \" initial   byte   read \"  ,    result )  ;", "byte [  ]    buffer    =    new   byte [  1  ]  ;", "instream . seek (  0  )  ;", "result    =    instream . read (  )  ;", "assertMinusOne (  \" post - seek   byte   read \"  ,    result )  ;", "result    =    instream . read ( buffer ,     0  ,     1  )  ;", "assertMinusOne (  \" post - seek   buffer   read \"  ,    result )  ;", "}", "METHOD_END"], "methodName": ["testSeekZeroByteFile"], "fileName": "org.apache.hadoop.fs.swift.TestSeek"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    new   Configuration (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ AUTH _ URL ,     \" http :  /  / localhost :  8  0  8  0  \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ TENANT ,     \" tenant \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ USERNAME ,     \" username \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ PASSWORD ,     \" password \"  )  ;", "return   configuration ;", "}", "METHOD_END"], "methodName": ["createCoreConfig"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "URI   uri    =    new   URI (  \" swift :  /  / container . openstack /  \"  )  ;", "return   SwiftRestClient . getInstance ( uri ,    configuration )  ;", "}", "METHOD_END"], "methodName": ["mkInstance"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "configuration . set (  (  (  ( SwiftProtocolConstants . SWIFT _ SERVICE _ PREFIX )     +     ( TestSwiftConfig . SERVICE )  )     +    field )  ,    value )  ;", "}", "METHOD_END"], "methodName": ["set"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ CONNECTION _ TIMEOUT ,     \" three \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testBadConnectTimeout"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ RETRY _ COUNT ,     \" three \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testBadRetryCount"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    new   Configuration (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ AUTH _ URL ,     \" http :  /  / localhost :  8  0  8  0  \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ TENANT ,     \" tenant \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ USERNAME ,     \" username \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testEmptyPassword"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    new   Configuration (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ AUTH _ URL ,     \" http :  /  / localhost :  8  0  8  0  \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ USERNAME ,     \" username \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ PASSWORD ,     \" password \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testEmptyTenant"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    new   Configuration (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ TENANT ,     \" tenant \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ USERNAME ,     \" username \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ PASSWORD ,     \" password \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testEmptyUrl"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    new   Configuration (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ AUTH _ URL ,     \" http :  /  / localhost :  8  0  8  0  \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ TENANT ,     \" tenant \"  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ PASSWORD ,     \" password \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testEmptyUsername"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ RETRY _ COUNT ,     \"  3  \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testGoodRetryCount"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ LOCATION _ AWARE ,     \" false \"  )  ;", "SwiftRestClient   restClient    =    mkInstance ( configuration )  ;", "assertFalse ( restClient . isLocationAware (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocationAwareFalsePropagates"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "set ( configuration ,    SwiftProtocolConstants . DOT _ LOCATION _ AWARE ,     \" true \"  )  ;", "SwiftRestClient   restClient    =    mkInstance ( configuration )  ;", "assertTrue ( restClient . isLocationAware (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocationAwareTruePropagates"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ BLOCKSIZE ,     \"  -  1  \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testNegativeBlocksize"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ PARTITION _ SIZE ,     \"  -  1  \"  )  ;", "SwiftRestClient   restClient    =    mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testNegativePartsize"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "int   size    =     1  2  7  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ BLOCKSIZE ,    Integer . toString ( size )  )  ;", "SwiftRestClient   restClient    =    mkInstance ( configuration )  ;", "assertEquals ( size ,    restClient . getBlocksizeKB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPositiveBlocksize"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "int   size    =     1  2  7  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ PARTITION _ SIZE ,    Integer . toString ( size )  )  ;", "SwiftRestClient   restClient    =    mkInstance ( configuration )  ;", "assertEquals ( size ,    restClient . getPartSizeKB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPositivePartsize"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "String   proxy    =     \" web - proxy \"  ;", "int   port    =     8  0  8  8  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ PROXY _ HOST _ PROPERTY ,    proxy )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ PROXY _ PORT _ PROPERTY ,    Integer . toString ( port )  )  ;", "SwiftRestClient   restClient    =    mkInstance ( configuration )  ;", "assertEquals ( proxy ,    restClient . getProxyHost (  )  )  ;", "assertEquals ( port ,    restClient . getProxyPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["testProxyData"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    createCoreConfig (  )  ;", "configuration . set ( SwiftProtocolConstants . SWIFT _ BLOCKSIZE ,     \"  0  \"  )  ;", "mkInstance ( configuration )  ;", "}", "METHOD_END"], "methodName": ["testZeroBlocksize"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftConfig"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  !  ( fs . delete ( path ,    false )  )  )     {", ". LOG . warn (  (  \" Failed   to   delete    \"     +    path )  )  ;", "}", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" deleting    \"     +    path )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  !  ( fs . delete ( path ,    true )  )  )     {", ". LOG . warn (  (  \" Failed   to   delete    \"     +    path )  )  ;", "}", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" deleting    \"     +    path )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteR"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / CreateDirWithFileParent \"  )  ;", "Path   child    =    new   Path ( path ,     \" subdir / child \"  )  ;", "fs . mkdirs ( path . getParent (  )  )  ;", "try    {", "SwiftTestUtils . writeTextFile ( fs ,    path ,     \" parent \"  ,    true )  ;", "try    {", "fs . mkdirs ( child )  ;", "}    catch    ( ParentNotDirectoryException   expected )     {", ". LOG . debug (  \" Expected   Exception \"  ,    expected )  ;", "}", "}    finally    {", "fs . delete ( path ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testCreateDirWithFileParent"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   base    =    new   Path (  \"  / test / CreateMultilevelDir \"  )  ;", "Path   path    =    new   Path ( base ,     \"  1  /  2  /  3  \"  )  ;", "fs . mkdirs ( path )  ;", "assertExists (  \" deep   multilevel   dir   not   created \"  ,    path )  ;", "fs . delete ( base ,    true )  ;", "assertPathDoesNotExist (  \" Multilevel   delete   failed \"  ,    path )  ;", "assertPathDoesNotExist (  \" Multilevel   delete   failed \"  ,    base )  ;", "}", "METHOD_END"], "methodName": ["testCreateMultilevelDir"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / DeleteNonexistentFile \"  )  ;", "assertFalse (  \" delete   returned   true \"  ,    fs . delete ( path ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteNonexistentFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / DirStatus \"  )  ;", "try    {", "fs . mkdirs ( path )  ;", "TestUtils . assertIsDirectory ( fs ,    path )  ;", "}    finally    {", "delete ( fs ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["testDirStatus"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / dirStaysADir \"  )  ;", "Path   child    =    new   Path ( path ,     \" child \"  )  ;", "try    {", "fs . mkdirs ( path )  ;", "TestUtils . assertIsDirectory ( fs ,    path )  ;", "TestUtils . writeTextFile ( fs ,    child ,     \" child   file \"  ,    true )  ;", "TestUtils . assertIsDirectory ( fs ,    path )  ;", "delete ( fs ,    child )  ;", "}    finally    {", "deleteR ( fs ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["testDirStaysADir"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / FileStatus \"  )  ;", "try    {", "String   text    =     \" Testing   File   Status    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,    text ,    false )  ;", "SwiftTestUtils . assertIsFile ( fs ,    path )  ;", "}    finally    {", "delete ( fs ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["testFileStatus"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   buffer    =    new   StringBuilder (  1  2  0  0  )  ;", "buffer . append (  \"  /  \"  )  ;", "for    ( int   i    =     0  ;    i    <     (  1  2  0  0     /     4  )  ;    i +  +  )     {", "buffer . append ( String . format (  \"  %  0  4 x \"  ,    i )  )  ;", "}", "String   pathString    =    buffer . toString (  )  ;", "Path   path    =    new   Path ( pathString )  ;", "try    {", "TestUtils . writeTextFile ( fs ,    path ,    pathString ,    true )  ;", "fs . delete ( path ,    false )  ;", "fail (  (  \" Managed   to   create   an   object   with   a   name   of   length    \"     +     ( pathString . length (  )  )  )  )  ;", "}    catch    ( BadRequestException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testLongObjectNamesForbidden"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   path    =    new   Path (  \"  / test /  / file \"  )  ;", "FileStatus [  ]    statuses    =    fs . listStatus ( path )  ;", "fail (  (  (  (  \" Should   throw   FileNotFoundException   on    \"     +    path )     +     \"    but   got   list   of   length    \"  )     +     ( statuses . length )  )  )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "}", "}", "METHOD_END"], "methodName": ["testLsNonExistentFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  /  \"  )  ;", "tatus [  ]    statuses    =    fs . listStatus ( path )  ;", "}", "METHOD_END"], "methodName": ["testLsRoot"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / MkDir \"  )  ;", "fs . mkdirs ( path )  ;", "fs . delete ( path ,    true )  ;", "}", "METHOD_END"], "methodName": ["testMkDir"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / Overwrite \"  )  ;", "try    {", "String   text    =     \" Testing   a   put   to   a   file    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,    text ,    false )  ;", "SwiftTestUtils . assertFileHasLength ( fs ,    path ,    text . length (  )  )  ;", "String   text 2     =     \" Overwriting   a   file    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,    text 2  ,    true )  ;", "SwiftTestUtils . assertFileHasLength ( fs ,    path ,    text 2  . length (  )  )  ;", "String   result    =    SwiftTestUtils . readBytesToString ( fs ,    path ,    text 2  . length (  )  )  ;", "assertEquals ( text 2  ,    result )  ;", "}    finally    {", "delete ( fs ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["testOverwrite"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / testOverwriteDirectory \"  )  ;", "try    {", "fs . mkdirs ( path . getParent (  )  )  ;", "String   text    =     \" Testing   a   put   to   a   file    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,    text ,    false )  ;", "SwiftTestUtils . assertFileHasLength ( fs ,    path ,    text . length (  )  )  ;", "}    finally    {", "delete ( fs ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["testOverwriteDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / PutDeleteFileInSubdir / testPutDeleteFileInSubdir \"  )  ;", "String   text    =     \" Testing   a   put   and   get   to   a   file   in   a   subdir    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,    text ,    false )  ;", "assertDeleted ( path ,    false )  ;", "assertDeleted ( new   Path (  \"  / test / PutDeleteFileInSubdir \"  )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testPutDeleteFileInSubdir"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / PutFile \"  )  ;", "Exception   caught    =    null ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,     \" Testing   a   put   to   a   file \"  ,    false )  ;", "assertDeleted ( path ,    false )  ;", "}", "METHOD_END"], "methodName": ["testPutFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    new   Path (  \"  / test / PutGetFile \"  )  ;", "try    {", "String   text    =     \" Testing   a   put   and   get   to   a   file    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    path ,    text ,    false )  ;", "String   result    =    SwiftTestUtils . readBytesToString ( fs ,    path ,    text . length (  )  )  ;", "assertEquals ( text ,    result )  ;", "}    finally    {", "delete ( fs ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["testPutGetFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "Path   childpath    =    new   Path (  \"  / test / testRecursiveDelete \"  )  ;", "String   text    =     \" Testing   a   put   and   get   to   a   file   in   a   subdir    \"     +     (  . currentTimeMillis (  )  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    childpath ,    text ,    false )  ;", "assertDeleted ( new   Path (  \"  / test \"  )  ,    true )  ;", "assertFalse (  (  \" child   entry   still   present    \"     +    childpath )  ,    fs . exists ( childpath )  )  ;", "}", "METHOD_END"], "methodName": ["testRecursiveDelete"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBasicOps"}, {"methodBody": ["METHOD_START", "{", "assertNotNull ( locations )  ;", "if    (  ( locations . length )     !  =     0  )     {", "fail (  (  \" non   empty   locations [  ]    with   first   entry   of    \"     +     ( locations [  0  ]  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["assertEmptyBlockLocations"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "SwiftFileSystemBaseTest . LOG . info ( location )  ;", "String [  ]    hosts    =    location . getHosts (  )  ;", "String [  ]    names    =    location . getNames (  )  ;", "assertNotEqual (  (  \" No   hosts   supplied   for    \"     +    location )  ,     0  ,    hosts . length )  ;", "assertEquals (  (  \" Unequal   names   and   hosts   in    \"     +    location )  ,    hosts . length ,    names . length )  ;", "assertEquals ( SwiftProtocolConstants . BLOCK _ LOCATION ,    location . getNames (  )  [  0  ]  )  ;", "assertEquals ( SwiftProtocolConstants . TOPOLOGY _ PATH ,    location . getTopologyPaths (  )  [  0  ]  )  ;", "}", "METHOD_END"], "methodName": ["assertLocationValid"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "Path   path    =    path (  \"  / test / locatedFile \"  )  ;", "createFile ( path )  ;", "return   fs . getatus ( path )  ;", "}", "METHOD_END"], "methodName": ["createFileAndGetStatus"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" verify   that   locating   a   directory   is   an   error \"  )  ;", "createFile ( path (  \"  / test / filename \"  )  )  ;", "FileStatus   status    =    fs . getFileStatus ( path (  \"  / test \"  )  )  ;", "aseTest . LOG . info (  (  (  (  \" Filesystem   is    \"     +     ( fs )  )     +     \"  ;    target   is    \"  )     +    status )  )  ;", "SwiftTestUtils . assertIsDirectory ( status )  ;", "BlockLocation [  ]    locations ;", "locations    =    getFs (  )  . getFileBlockLocations ( status ,     0  ,     1  )  ;", "assertEmptyBlockLocations ( locations )  ;", "}", "METHOD_END"], "methodName": ["testLocateDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" verify   that   a   negative   length   is   illegal \"  )  ;", "try    {", "[  ]    locations    =    getFs (  )  . getFiles ( createFileAndGetStatus (  )  ,     0  ,     (  -  1  )  )  ;", "fail (  (  (  \" Expected   an   exception ,    got    \"     +     ( locations . length )  )     +     \"    locations \"  )  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testLocateNegativeLen"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" verify   that   a   negative   offset   is   illegal \"  )  ;", "try    {", "[  ]    locations    =    getFs (  )  . getFiles ( createFileAndGetStatus (  )  ,     (  -  1  )  ,     1  )  ;", "fail (  (  (  \" Expected   an   exception ,    got    \"     +     ( locations . length )  )     +     \"    locations \"  )  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testLocateNegativeSeek"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" verify   that   a   null   filestatus   maps   to   a   null   location   array \"  )  ;", "[  ]    locations    =    getFs (  )  . getFiles (  (  ( FileStatus )     ( null )  )  ,     0  ,     1  )  ;", "assertNull ( locations )  ;", "}", "METHOD_END"], "methodName": ["testLocateNullStatus"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  (  \" overshooting   the   length   is   legal ,    as   long   as   the \"     +     \"    origin   location   is   valid \"  )  )  ;", "[  ]    locations    =    getFs (  )  . getFiles ( createFileAndGetStatus (  )  ,     0  ,     (  ( data . length )     +     1  0  0  )  )  ;", "assertNotNull ( locations )  ;", "assertTrue (  (  ( locations . length )     >     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocateOutOfRangeLen"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" Seeking   out   of   the   file   length   returns   an   empty   array \"  )  ;", "[  ]    locations    =    getFs (  )  . getFiles ( createFileAndGetStatus (  )  ,     (  ( data . length )     +     1  0  0  )  ,     1  )  ;", "assertEmptys ( locations )  ;", "}", "METHOD_END"], "methodName": ["testLocateOutOfRangeSrc"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" verify   that   locating   the   root   directory   is   an   error \"  )  ;", "FileStatus   status    =    fs . getFileStatus ( path (  \"  /  \"  )  )  ;", "SwiftTestUtils . assertIsDirectory ( status )  ;", "[  ]    locations ;", "locations    =    getFs (  )  . getFiles ( status ,     0  ,     1  )  ;", "assertEmptys ( locations )  ;", "}", "METHOD_END"], "methodName": ["testLocateRootDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "describe (  \" verify   that   a   file   returns    1  +    blocks \"  )  ;", "FileStatus   fileStatus    =    createFileAndGetStatus (  )  ;", "[  ]    locations    =    getFs (  )  . getFiles ( fileStatus ,     0  ,     1  )  ;", "assertNotEqual (  (  \" No   block   locations   supplied   for    \"     +    fileStatus )  ,     0  ,    locations . length )  ;", "for    (    location    :    locations )     {", "assertLocationValid ( location )  ;", "}", "}", "METHOD_END"], "methodName": ["testLocateSingleFileBlocks"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlockLocation"}, {"methodBody": ["METHOD_START", "{", "Path   smallfile    =    new   Path (  \"  / test / smallfile \"  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    smallfile ,     \" b \"  ,    true )  ;", "createFile ( smallfile )  ;", "FileStatus   status    =    getFs (  )  . getFileStatus ( smallfile )  ;", "assertTrue (  (  \" Zero   b   in    \"     +    status )  ,     (  ( status . getBlockSize (  )  )     !  =     0 L )  )  ;", "assertTrue (  (  \" Zero   replication   in    \"     +    status )  ,     (  ( status . getReplication (  )  )     !  =     0 L )  )  ;", "}", "METHOD_END"], "methodName": ["testBlocksizeNonZeroForFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlocksize"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" Zero   default   blocksize \"  ,     (  0 L    !  =     ( getFs (  )  . getDefaultBlockSize (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultBlocksizeNonZero"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlocksize"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" Zero   default   blocksize \"  ,     (  0 L    !  =     ( getFs (  )  . getDefaultBlockSize ( new   Path (  \"  / test \"  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultBlocksizeOtherPathNonZero"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlocksize"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" Zero   default   blocksize \"  ,     (  0 L    !  =     ( getFs (  )  . getDefaultBlockSize ( new   Path (  \"  /  \"  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultBlocksizeRootPathNonZero"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemBlocksize"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . skip (  \" Skipping   unreliable   test \"  )  ;", "final   String   message    =     \" message \"  ;", "final   Path   fileToRead    =    new   Path (  (  (  . TEST _ RACE _ CONDITION _ ON _ DELETE _ DIR )     +     \"  / files / many - files / file \"  )  )  ;", "final   ExecutorService   executorService    =    Executors . newFixedThreadPool (  2  )  ;", "fs . create ( new   Path (  (  (  . TEST _ RACE _ CONDITION _ ON _ DELETE _ DIR )     +     \"  / file / test / file 1  \"  )  )  )  ;", "fs . create ( new   Path (  (  (  . TEST _ RACE _ CONDITION _ ON _ DELETE _ DIR )     +     \"  / documents / doc 1  \"  )  )  )  ;", "fs . create ( new   Path (  (  (  . TEST _ RACE _ CONDITION _ ON _ DELETE _ DIR )     +     \"  / pictures / picture \"  )  )  )  ;", "executorService . execute ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "assertDeleted ( new   Path (  . TEST _ RACE _ CONDITION _ ON _ DELETE _ DIR )  ,    true )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" deletion   thread :  \"     +    e )  ,    e )  ;", "thread 1 Ex    =    e ;", "throw   new   RuntimeException ( e )  ;", "}", "}", "}  )  ;", "executorService . execute ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "final   FSDataOutputStream   outputStream    =    fs . create ( fileToRead )  ;", "outputStream . write ( message . getBytes (  )  )  ;", "outputStream . close (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" writer   thread :  \"     +    e )  ,    e )  ;", "thread 2 Ex    =    e ;", "throw   new   RuntimeException ( e )  ;", "}", "}", "}  )  ;", "executorService . awaitTermination (  1  ,    TimeUnit . MINUTES )  ;", "if    (  ( thread 1 Ex )     !  =    null )     {", "throw   thread 1 Ex ;", "}", "if    (  ( thread 2 Ex )     !  =    null )     {", "throw   thread 2 Ex ;", "}", "try    {", "fs . open ( fileToRead )  ;", ". LOG . info (  \" concurrency   test   failed   to   trigger   a   failure \"  )  ;", "}    catch    ( FileNotFoundException   expected )     {", "}", "}", "METHOD_END"], "methodName": ["testRaceConditionOnDirDeleteTest"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemConcurrency"}, {"methodBody": ["METHOD_START", "{", "SwiftNativeFileSystem   swiftNativeFileSystem    =    new   SwiftNativeFileSystem (  )  ;", "return   swiftNativeFileSystem ;", "}", "METHOD_END"], "methodName": ["createSwiftFS"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemContract"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["filesystemIsCaseSensitive"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemContract"}, {"methodBody": ["METHOD_START", "{", "return   SwiftTestUtils . getServiceURI ( new   Configuration (  )  )  ;", "}", "METHOD_END"], "methodName": ["getFilesystemURI"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemContract"}, {"methodBody": ["METHOD_START", "{", "final   Path   file    =    new   Path (  \"  / test / testDeleteEmptyFile \"  )  ;", "createEmptyFile ( file )  ;", "SwiftTestUtils . noteAction (  \" about   to   delete \"  )  ;", "assertDeleted ( file ,    true )  ;", "}", "METHOD_END"], "methodName": ["testDeleteEmptyFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDelete"}, {"methodBody": ["METHOD_START", "{", "final   Path   file    =    new   Path (  \"  / test / testDeleteEmptyFileTwice \"  )  ;", "createEmptyFile ( file )  ;", "assertDeleted ( file ,    true )  ;", "SwiftTestUtils . noteAction (  \" multiple   creates ,    and   deletes \"  )  ;", "assertFalse (  \" Delete   returned   true \"  ,    fs . delete ( file ,    false )  )  ;", "createEmptyFile ( file )  ;", "assertDeleted ( file ,    true )  ;", "assertFalse (  \" Delete   returned   true \"  ,    fs . delete ( file ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteEmptyFileTwice"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDelete"}, {"methodBody": ["METHOD_START", "{", "final   Path   file    =    new   Path (  \"  / test / testDeleteNonEmptyFile \"  )  ;", "createFile ( file )  ;", "assertDeleted ( file ,    true )  ;", "}", "METHOD_END"], "methodName": ["testDeleteNonEmptyFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDelete"}, {"methodBody": ["METHOD_START", "{", "final   Path   file    =    new   Path (  \"  / test / testDeleteNonEmptyFileTwice \"  )  ;", "createFile ( file )  ;", "assertDeleted ( file ,    true )  ;", "assertFalse (  \" Delete   returned   true \"  ,    fs . delete ( file ,    false )  )  ;", "createFile ( file )  ;", "assertDeleted ( file ,    true )  ;", "assertFalse (  \" Delete   returned   true \"  ,    fs . delete ( file ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteNonEmptyFileTwice"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDelete"}, {"methodBody": ["METHOD_START", "{", "final   Path   file    =    new   Path (  \"  / test /  \"  )  ;", "fs . d ( file ,    true )  ;", "assertPathDoesNotExist (  \" Test   dir   found \"  ,    file )  ;", "}", "METHOD_END"], "methodName": ["testDeleteTestDir"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDelete"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    path (  \"  /  \"  )  ;", "Path   tes    =    path (  \"  / test \"  )  ;", "createFile ( tes )  ;", "assertTrue (  \" rm (  /  )    returned   false \"  ,    fs . delete ( root ,    true )  )  ;", "assertExists (  \" Root   dir   is   missing \"  ,    root )  ;", "assertPathDoesNotExist (  \" test   file   not   deleted \"  ,    tes )  ;", "}", "METHOD_END"], "methodName": ["testRmRootDirRecursiveIsForbidden"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDelete"}, {"methodBody": ["METHOD_START", "{", "assertNotNull ( statuses )  ;", "return   TestUtils . dumpStats ( pathname ,    statuses )  ;", "}", "METHOD_END"], "methodName": ["statusToString"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDirectories"}, {"methodBody": ["METHOD_START", "{", "Path   test    =    path (  \"  / test / testDirectoriesLowerDownHaveMatchingFileStatus \"  )  ;", "fs . delete ( test ,    true )  ;", "mkdirs ( test )  ;", "assertExists (  \" created   test   sub   directory \"  ,    test )  ;", "FileStatus [  ]    statuses    =    fs . listStatus ( test )  ;", "String   statusString    =    statusToString ( test . toString (  )  ,    statuses )  ;", "assertEquals (  (  \" Wrong   number   of   elements   in   file   status    \"     +    statusString )  ,     0  ,    statuses . length )  ;", "}", "METHOD_END"], "methodName": ["testDirectoriesLowerDownHaveMatchingFileStatus"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDirectories"}, {"methodBody": ["METHOD_START", "{", "Path   test    =    path (  \"  / test \"  )  ;", "fs . delete ( test ,    true )  ;", "mkdirs ( test )  ;", "assertExists (  \" created   test   directory \"  ,    test )  ;", "FileStatus [  ]    statuses    =    fs . listStatus ( test )  ;", "String   statusString    =    statusToString ( test . toString (  )  ,    statuses )  ;", "assertEquals (  (  \" Wrong   number   of   elements   in   file   status    \"     +    statusString )  ,     0  ,    statuses . length )  ;", "Path   src    =    path (  \"  / test / file \"  )  ;", "SwiftTestUtils . touch ( fs ,    src )  ;", "statuses    =    fs . listStatus ( test )  ;", "statusString    =    statusToString ( test . toString (  )  ,    statuses )  ;", "assertEquals (  (  \" Wrong   number   of   elements   in   file   status    \"     +    statusString )  ,     1  ,    statuses . length )  ;", "tatus   stat    =     (  ( tatus )     ( statuses [  0  ]  )  )  ;", "assertTrue (  (  \" isDir (  )  :    Not   a   directory :     \"     +    stat )  ,    stat . isDir (  )  )  ;", "extraStatusAssertions ( stat )  ;", "}", "METHOD_END"], "methodName": ["testDirectoriesOffRootHaveMatchingFileStatus"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDirectories"}, {"methodBody": ["METHOD_START", "{", "Path   src    =    path (  \"  / test / testMultiByteFilesAreFiles \"  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    src ,     \" testMultiByteFilesAreFiles \"  ,    false )  ;", "assertIsFile ( src )  ;", "FileStatus   status    =    fs . getatus ( src )  ;", "assertFalse ( status . isDir (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultiByteFilesAreFiles"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDirectories"}, {"methodBody": ["METHOD_START", "{", "Path   missing    =    path (  \"  / test / testNoStatusForMissingDirectories \"  )  ;", "assertPathDoesNotExist (  \" leftover ?  \"  ,    missing )  ;", "try    {", "FileStatus [  ]    statuses    =    fs . listStatus ( missing )  ;", "fail (  (  \" Expected   a   FileNotFoundException ,    got   the   status    \"     +    statuses )  )  ;", "}    catch    ( FileNotFoundException   expected )     {", "}", "}", "METHOD_END"], "methodName": ["testNoStatusForMissingDirectories"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDirectories"}, {"methodBody": ["METHOD_START", "{", "Path   src    =    path (  \"  / test / testZeroByteFilesAreFiles \"  )  ;", "SwiftTestUtils . touch ( fs ,    src )  ;", "SwiftTestUtils . assertIsy ( fs ,    src )  ;", "}", "METHOD_END"], "methodName": ["testZeroByteFilesAreDirectories"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemDirectories"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "TestUtils . getServiceURI ( conf )  ;", "}", "METHOD_END"], "methodName": ["testConfDefinesFilesystem"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "URI   fsURI    =    TestUtils . getServiceURI ( conf )  ;", "RestClientBindings . bind ( fsURI ,    conf )  ;", "}", "METHOD_END"], "methodName": ["testConfIsValid"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "final   Path   f    =    new   Path (  \"  / test / testCreateFile \"  )  ;", "final   FSDataOutputStream   fsDataOutputStream    =    fs . create ( f )  ;", "fsDataOutputStream . close (  )  ;", "assertExists (  \" created   file \"  ,    f )  ;", "}", "METHOD_END"], "methodName": ["testCreateFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "assertNotNull ( fs . getUri (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFilesystemHasURI"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "String   mixedCaseFilename    =     \"  / test / UPPER . TXT \"  ;", "Path   upper    =    path ( mixedCaseFilename )  ;", "Path   lower    =    path ( mixedCaseFilename . toLowerCase ( Locale . ENGLISH )  )  ;", "assertFalse (  (  \" File   exists \"     +    upper )  ,    fs . exists ( upper )  )  ;", "assertFalse (  (  \" File   exists \"     +    lower )  ,    fs . exists ( lower )  )  ;", "FSDataOutputStream   out    =    fs . create ( upper )  ;", "out . writeUTF (  \" UPPER \"  )  ;", "out . close (  )  ;", "FileStatus   upperStatus    =    fs . getatus ( upper )  ;", "assertExists (  (  \" Original   upper   case   file \"     +    upper )  ,    upper )  ;", "assertPathDoesNotExist (  \" lower   case   file \"  ,    lower )  ;", "out    =    fs . create ( lower )  ;", "out . writeUTF (  \" l \"  )  ;", "out . close (  )  ;", "assertExists (  \" lower   case   file \"  ,    lower )  ;", "assertExists (  (  \" Original   upper   case   file    \"     +    upper )  ,    upper )  ;", "FileStatus   newStatus    =    fs . getatus ( upper )  ;", "assertEquals (  (  (  (  \" Expected   status :  \"     +    upperStatus )     +     \"    actual   status    \"  )     +    newStatus )  ,    upperStatus . getLen (  )  ,    newStatus . getLen (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFilesystemIsCaseSensitive"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "String   scheme    =    fs . getScheme (  )  ;", "assertEquals ( SwiftNative . SWIFT ,    scheme )  ;", "}", "METHOD_END"], "methodName": ["testGetSchemeImplemented"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "final   Path   p    =    new   Path (  \"  / test / testOpenNonExistingFile \"  )  ;", "try    {", "final   FSDataInputStream   in    =    fs . open ( p )  ;", "in . close (  )  ;", "fail (  \" didn ' t   expect   to   get   here \"  )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "BaseTest . LOG . debug (  (  \" Expected :     \"     +    fnfe )  ,    fnfe )  ;", "}", "}", "METHOD_END"], "methodName": ["testOpenNonExistingFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "final   Path   f    =    new   Path (  \"  / test / test \"  )  ;", "final   FSDataOutputStream   fsDataOutputStream    =    fs . create ( f )  ;", "final   String   message    =     \" Test   string \"  ;", "fsDataOutputStream . write ( message . getBytes (  )  )  ;", "fsDataOutputStream . close (  )  ;", "assertExists (  \" created   file \"  ,    f )  ;", "FSDataInputStream   open    =    null ;", "try    {", "open    =    fs . open ( f )  ;", "final   byte [  ]    bytes    =    new   byte [  5  1  2  ]  ;", "final   int   read    =    open . read ( bytes )  ;", "final   byte [  ]    buffer    =    new   byte [ read ]  ;", ". arraycopy ( bytes ,     0  ,    buffer ,     0  ,    read )  ;", "assertEquals ( message ,    new   String ( buffer )  )  ;", "}    finally    {", "fs . delete ( f ,    false )  ;", "IOUtils . closeStream ( open )  ;", "}", "}", "METHOD_END"], "methodName": ["testWriteReadFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemExtendedContract"}, {"methodBody": ["METHOD_START", "{", "testDirs    =    new   Path [  ]  {    path (  \"  / test / hadoop / a \"  )  ,    path (  \"  / test / hadoop / b \"  )  ,    path (  \"  / test / hadoop / c /  1  \"  )     }  ;", "assertPathDoesNotExist (  \" test   directory   setup \"  ,    testDirs [  0  ]  )  ;", "for    ( Path   path    :    testDirs )     {", "mkdirs ( path )  ;", "}", "}", "METHOD_END"], "methodName": ["createTestSubdirs"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "describe (  \" Empty   the   root   dir   and   verify   that   an   LS    /    returns    {  }  \"  )  ;", "TestUtils . cleanup (  \" testListEmptyRoot \"  ,    fs ,     \"  / test \"  )  ;", "TestUtils . cleanup (  \" testListEmptyRoot \"  ,    fs ,     \"  / user \"  )  ;", "FileStatus [  ]    fileStatuses    =    fs . listStatus ( path (  \"  /  \"  )  )  ;", "assertEquals (  (  \" Non - empty   root \"     +     ( TestUtils . dumpStats (  \"  /  \"  ,    fileStatuses )  )  )  ,     0  ,    fileStatuses . length )  ;", "}", "METHOD_END"], "methodName": ["testListEmptyRoot"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "createTestSubdirs (  )  ;", "FileStatus [  ]    paths    =    fs . listStatus ( path (  \"  / test \"  )  )  ;", "assertEquals ( SwiftTestUtils . dumpStats (  \"  / test \"  ,    paths )  ,     1  ,    paths . length )  ;", "assertEquals ( path (  \"  / test /  \"  )  ,    paths [  0  ]  . getPath (  )  )  ;", "}", "METHOD_END"], "methodName": ["testListLevelTest"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "createTestSubdirs (  )  ;", "FileStatus [  ]    paths ;", "paths    =    fs . listStatus ( path (  \"  / test /  \"  )  )  ;", "String   stats    =    SwiftTestUtils . dumpStats (  \"  / test /  \"  ,    paths )  ;", "assertEquals (  (  \" Paths . length   wrong   in    \"     +    stats )  ,     3  ,    paths . length )  ;", "assertEquals (  (  \" Path   element [  0  ]    wrong :     \"     +    stats )  ,    path (  \"  / test /  / a \"  )  ,    paths [  0  ]  . getPath (  )  )  ;", "assertEquals (  (  \" Path   element [  1  ]    wrong :     \"     +    stats )  ,    path (  \"  / test /  / b \"  )  ,    paths [  1  ]  . getPath (  )  )  ;", "assertEquals (  (  \" Path   element [  2  ]    wrong :     \"     +    stats )  ,    path (  \"  / test /  / c \"  )  ,    paths [  2  ]  . getPath (  )  )  ;", "}", "METHOD_END"], "methodName": ["testListLevelTestHadoop"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "Path   test    =    path (  \"  / test \"  )  ;", "TestUtils . touch ( fs ,    test )  ;", "FileStatus [  ]    fileStatuses    =    fs . listStatus ( path (  \"  /  \"  )  )  ;", "String   stats    =    TestUtils . dumpStats (  \"  /  \"  ,    fileStatuses )  ;", "assertEquals (  (  \" Wrong    # of   root   children \"     +    stats )  ,     1  ,    fileStatuses . length )  ;", "FileStatus   status    =    fileStatuses [  0  ]  ;", "assertEquals (  (  \" Wrong   path   value \"     +    stats )  ,    test ,    status . getPath (  )  )  ;", "}", "METHOD_END"], "methodName": ["testListNonEmptyRoot"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "createTestSubdirs (  )  ;", "FileStatus [  ]    paths ;", "paths    =    fs . listStatus ( path (  \"  / test /  / a \"  )  )  ;", "assertEquals ( SwiftTestUtils . dumpStats (  \"  / test /  / a \"  ,    paths )  ,     0  ,    paths . length )  ;", "}", "METHOD_END"], "methodName": ["testListStatusEmptyDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "describe (  (  \" Create   a   single   file   under    / test ;  \"     +     \"    assert   that   listStatus (  / test )    finds   it \"  )  )  ;", "Path   file    =    path (  \"  / test / filename \"  )  ;", "createFile ( file )  ;", "tatus [  ]    pathStats    =    fs . listStatus ( file )  ;", "assertEquals ( SwiftTestUtils . dumpStats (  \"  / test /  \"  ,    pathStats )  ,     1  ,    pathStats . length )  ;", "tatus   lsStat    =    pathStats [  0  ]  ;", "assertEquals (  (  \" Wrong   file   len   in   listing   of    \"     +    lsStat )  ,    data . length ,    lsStat . getLen (  )  )  ;", "}", "METHOD_END"], "methodName": ["testListStatusFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "Path   dir    =    path (  \"  /  \"  )  ;", "Path   child    =    path (  \"  / test \"  )  ;", "TestUtils . touch ( fs ,    child )  ;", "FileStatus [  ]    stats    =    fs . listStatus ( dir ,    new   AcceptAllFilter (  )  )  ;", "boolean   found    =    false ;", "StringBuilder   builder    =    new   StringBuilder (  )  ;", "for    ( FileStatus   stat    :    stats )     {", "builder . append ( stat . toString (  )  )  . append (  '  \\ n '  )  ;", "if    ( stat . getPath (  )  . equals ( child )  )     {", "found    =    true ;", "}", "}", "assertTrue (  (  (  (  (  (  \" Path    \"     +    child )     +     \"    not   found   in   directory    \"  )     +    dir )     +     \"  :  \"  )     +    builder )  ,    found )  ;", "}", "METHOD_END"], "methodName": ["testListStatusFiltered"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "Path   dir    =    path (  \"  /  \"  )  ;", "Path   child    =    path (  \"  / test \"  )  ;", "TestUtils . touch ( fs ,    child )  ;", "TestUtils . assertListStatusFinds ( fs ,    dir ,    child )  ;", "}", "METHOD_END"], "methodName": ["testListStatusRootDir"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemLsOperations"}, {"methodBody": ["METHOD_START", "{", "return   fs . getConf (  )  . getInt (  \" io . file . buffer . size \"  ,     4  0  9  6  )  ;", "}", "METHOD_END"], "methodName": ["getBufferSize"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "int   partitions    =     (  ( int )     ( uploaded    /    partSizeBytes )  )  ;", "int   remainder    =     (  ( int )     ( uploaded    %    partSizeBytes )  )  ;", "if    ( closed )     {", "return   partitions    +     ( remainder    >     0     ?     1     :     0  )  ;", "} else    {", "return   partitions ;", "}", "}", "METHOD_END"], "methodName": ["getExpectedPartitionsWritten"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   listing    =    new   StringBuilder (  )  ;", "for    ( tatus   stat    :    parentDirListing )     {", "listing . append ( stat )  . append (  \"  \\ n \"  )  ;", "}", "return   listing ;", "}", "METHOD_END"], "methodName": ["lsToString"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "FileStatus   listedFileStat    =    null ;", "for    ( FileStatus   stat    :    parentDirListing )     {", "if    ( stat . getPath (  )  . equals ( childPath )  )     {", "listedFileStat    =    stat ;", "}", "}", "return   listedFileStat ;", "}", "METHOD_END"], "methodName": ["resolveChild"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    new   Path (  \"  / test / testDeletePartitionedFile \"  )  ;", "SwiftTestUtils . writeDataset ( fs ,    path ,    data ,    data . length ,     1  0  2  4  ,    false )  ;", "assertExists (  \" Exists \"  ,    path )  ;", "Path   part _  0  0  0  1     =    new   Path ( path ,    SwiftUtils . partitionFilenameFromNumber (  1  )  )  ;", "Path   part _  0  0  0  2     =    new   Path ( path ,    SwiftUtils . partitionFilenameFromNumber (  2  )  )  ;", "String   ls    =    SwiftTestUtils . ls ( fs ,    path )  ;", "assertExists (  (  \" Partition    0  0  0  1    Exists   in    \"     +    ls )  ,    part _  0  0  0  1  )  ;", "assertExists (  (  \" Partition    0  0  0  2    Exists   in    \"     +    ls )  ,    part _  0  0  0  1  )  ;", "fs . delete ( path ,    false )  ;", "assertPathDoesNotExist (  \" deleted   file   still   there \"  ,    path )  ;", "ls    =    SwiftTestUtils . ls ( fs ,    path )  ;", "assertPathDoesNotExist (  (  \" partition    0  0  0  1    file   still   under    \"     +    ls )  ,    part _  0  0  0  1  )  ;", "assertPathDoesNotExist (  (  \" partition    0  0  0  2    file   still   under    \"     +    ls )  ,    part _  0  0  0  2  )  ;", "}", "METHOD_END"], "methodName": ["testDeletePartitionedFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    new   Path (  \"  / test / testDeleteSmallPartitionedFile \"  )  ;", "final   int   len 1     =     1  0  2  4  ;", "final   byte [  ]    src 1     =    SwiftTestUtils . dataset ( len 1  ,     ' A '  ,     ' Z '  )  ;", "SwiftTestUtils . writeDataset ( fs ,    path ,    src 1  ,    len 1  ,     1  0  2  4  ,    false )  ;", "assertExists (  \" Exists \"  ,    path )  ;", "Path   part _  0  0  0  1     =    new   Path ( path ,    SwiftUtils . partitionFilenameFromNumber (  1  )  )  ;", "Path   part _  0  0  0  2     =    new   Path ( path ,    SwiftUtils . partitionFilenameFromNumber (  2  )  )  ;", "String   ls    =    SwiftTestUtils . ls ( fs ,    path )  ;", "assertExists (  (  \" Partition    0  0  0  1    Exists   in    \"     +    ls )  ,    part _  0  0  0  1  )  ;", "assertPathDoesNotExist (  (  \" partition    0  0  0  2    found   under    \"     +    ls )  ,    part _  0  0  0  2  )  ;", "assertExists (  (  \" Partition    0  0  0  2    Exists   in    \"     +    ls )  ,    part _  0  0  0  1  )  ;", "fs . delete ( path ,    false )  ;", "assertPathDoesNotExist (  \" deleted   file   still   there \"  ,    path )  ;", "ls    =    SwiftTestUtils . ls ( fs ,    path )  ;", "assertPathDoesNotExist (  (  \" partition    0  0  0  1    file   still   under    \"     +    ls )  ,    part _  0  0  0  1  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteSmallPartitionedFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    new   Path (  \"  / test / testFilePartUpload \"  )  ;", "int   len    =     8  1  9  2  ;", "final   byte [  ]    src    =    SwiftTestUtils . dataset ( len ,     3  2  ,     1  4  4  )  ;", "FSDataOutputStream   out    =    fs . create ( path ,    false ,    getBufferSize (  )  ,     (  ( short )     (  1  )  )  ,     . BLOCK _ SIZE )  ;", "try    {", "int   totalPartitionsToWrite    =    len    /     (  . PART _ SIZE _ BYTES )  ;", "assertPartitionsWritten (  \" Startup \"  ,    out ,     0  )  ;", "int   firstWriteLen    =     2  0  4  8  ;", "out . write ( src ,     0  ,    firstWriteLen )  ;", "long   expected    =    getExpectedPartitionsWritten ( firstWriteLen ,     . PART _ SIZE _ BYTES ,    false )  ;", "SwiftUtils . debug ( SwiftFileSystemBaseTest . LOG ,     \" First   write :    predict    % d   partitions   written \"  ,    expected )  ;", "assertPartitionsWritten (  \" First   write   completed \"  ,    out ,    expected )  ;", "int   remainder    =    len    -    firstWriteLen ;", "SwiftUtils . debug ( SwiftFileSystemBaseTest . LOG ,     \" remainder :    writing :     % d   bytes \"  ,    remainder )  ;", "out . write ( src ,    firstWriteLen ,    remainder )  ;", "expected    =    getExpectedPartitionsWritten ( len ,     . PART _ SIZE _ BYTES ,    false )  ;", "assertPartitionsWritten (  \" Remaining   data \"  ,    out ,    expected )  ;", "out . close (  )  ;", "expected    =    getExpectedPartitionsWritten ( len ,     . PART _ SIZE _ BYTES ,    true )  ;", "assertPartitionsWritten (  \" Stream   closed \"  ,    out ,    expected )  ;", "Header [  ]    headers    =    fs . getStore (  )  . getObjectHeaders ( path ,    true )  ;", "for    ( Header   header    :    headers )     {", "SwiftFileSystemBaseTest . LOG . info ( header . toString (  )  )  ;", "}", "byte [  ]    dest    =    SwiftTestUtils . readDataset ( fs ,    path ,    len )  ;", "SwiftFileSystemBaseTest . LOG . info (  (  (  (  \" Read   dataset   from    \"     +    path )     +     \"  :    data   length    =  \"  )     +    len )  )  ;", "SwiftTestUtils . compareByteArrays ( src ,    dest ,    len )  ;", "FileStatus   status ;", "final   Path   qualifiedPath    =    path . makeQualified ( fs )  ;", "status    =    fs . getFileStatus ( qualifiedPath )  ;", "BlockLocation [  ]    locations    =    fs . getFileBlockLocations ( status ,     0  ,    len )  ;", "assertNotNull (  \" Null   getFileBlockLocations (  )  \"  ,    locations )  ;", "assertTrue (  \" empty   array   returned   for   getFileBlockLocations (  )  \"  ,     (  ( locations . length )     >     0  )  )  ;", "try    {", "validatePathLen ( path ,    len )  ;", "}    catch    ( AssertionError   e )     {", "throw   new   AssumptionViolatedException ( e ,    null )  ;", "}", "}    finally    {", "IOUtils . closeStream ( out )  ;", "}", "}", "METHOD_END"], "methodName": ["testFilePartUpload"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    new   Path (  \"  / test / testFilePartUploadLengthCheck \"  )  ;", "int   len    =     8  1  9  2  ;", "final   byte [  ]    src    =    SwiftTestUtils . dataset ( len ,     3  2  ,     1  4  4  )  ;", "FSDataOutputStream   out    =    fs . create ( path ,    false ,    getBufferSize (  )  ,     (  ( short )     (  1  )  )  ,     . BLOCK _ SIZE )  ;", "try    {", "int   totalPartitionsToWrite    =    len    /     (  . PART _ SIZE _ BYTES )  ;", "assertPartitionsWritten (  \" Startup \"  ,    out ,     0  )  ;", "int   firstWriteLen    =     2  0  4  8  ;", "out . write ( src ,     0  ,    firstWriteLen )  ;", "long   expected    =    getExpectedPartitionsWritten ( firstWriteLen ,     . PART _ SIZE _ BYTES ,    false )  ;", "SwiftUtils . debug ( SwiftFileSystemBaseTest . LOG ,     \" First   write :    predict    % d   partitions   written \"  ,    expected )  ;", "assertPartitionsWritten (  \" First   write   completed \"  ,    out ,    expected )  ;", "int   remainder    =    len    -    firstWriteLen ;", "SwiftUtils . debug ( SwiftFileSystemBaseTest . LOG ,     \" remainder :    writing :     % d   bytes \"  ,    remainder )  ;", "out . write ( src ,    firstWriteLen ,    remainder )  ;", "expected    =    getExpectedPartitionsWritten ( len ,     . PART _ SIZE _ BYTES ,    false )  ;", "assertPartitionsWritten (  \" Remaining   data \"  ,    out ,    expected )  ;", "out . close (  )  ;", "expected    =    getExpectedPartitionsWritten ( len ,     . PART _ SIZE _ BYTES ,    true )  ;", "assertPartitionsWritten (  \" Stream   closed \"  ,    out ,    expected )  ;", "Header [  ]    headers    =    fs . getStore (  )  . getObjectHeaders ( path ,    true )  ;", "for    ( Header   header    :    headers )     {", "SwiftFileSystemBaseTest . LOG . info ( header . toString (  )  )  ;", "}", "byte [  ]    dest    =    SwiftTestUtils . readDataset ( fs ,    path ,    len )  ;", "SwiftFileSystemBaseTest . LOG . info (  (  (  (  \" Read   dataset   from    \"     +    path )     +     \"  :    data   length    =  \"  )     +    len )  )  ;", "SwiftTestUtils . compareByteArrays ( src ,    dest ,    len )  ;", "FileStatus   status    =    fs . getFileStatus ( path )  ;", "BlockLocation [  ]    locations    =    fs . getFileBlockLocations ( status ,     0  ,    len )  ;", "assertNotNull (  \" Null   getFileBlockLocations (  )  \"  ,    locations )  ;", "assertTrue (  \" empty   array   returned   for   getFileBlockLocations (  )  \"  ,     (  ( locations . length )     >     0  )  )  ;", "}    finally    {", "IOUtils . closeStream ( out )  ;", "}", "}", "METHOD_END"], "methodName": ["testFilePartUploadNoLengthCheck"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    new   Path (  \"  / test / testManyPartitionedFile \"  )  ;", "int   len    =     (  . PART _ SIZE _ BYTES )     *     1  5  ;", "final   byte [  ]    src    =    SwiftTestUtils . dataset ( len ,     3  2  ,     1  4  4  )  ;", "FSDataOutputStream   out    =    fs . create ( path ,    false ,    getBufferSize (  )  ,     (  ( short )     (  1  )  )  ,     . BLOCK _ SIZE )  ;", "out . write ( src ,     0  ,    src . length )  ;", "int   expected    =    getExpectedPartitionsWritten ( len ,     . PART _ SIZE _ BYTES ,    true )  ;", "out . close (  )  ;", "assertPartitionsWritten (  \" write   completed \"  ,    out ,    expected )  ;", "assertEquals (  \" too   few   bytes   written \"  ,    len ,    SwiftNativeFileSystem . getBytesWritten ( out )  )  ;", "assertEquals (  \" too   few   bytes   uploaded \"  ,    len ,    SwiftNativeFileSystem . getBytesUploaded ( out )  )  ;", "byte [  ]    dest    =    SwiftTestUtils . readDataset ( fs ,    path ,    len )  ;", "SwiftTestUtils . compareByteArrays ( src ,    dest ,    len )  ;", "FileStatus [  ]    stats    =    fs . listStatus ( path )  ;", "assertEquals (  (  \" wrong   entry   count   in    \"     +     ( SwiftTestUtils . dumpStats ( path . toString (  )  ,    stats )  )  )  ,    expected ,    stats . length )  ;", "}", "METHOD_END"], "methodName": ["testManyPartitionedFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   path    =    new   Path (  \"  / test / testOverwritePartitionedFile \"  )  ;", "final   int   len 1     =     8  1  9  2  ;", "final   byte [  ]    src 1     =    SwiftTestUtils . dataset ( len 1  ,     ' A '  ,     ' Z '  )  ;", "FSDataOutputStream   out    =    fs . create ( path ,    false ,    getBufferSize (  )  ,     (  ( short )     (  1  )  )  ,     1  0  2  4  )  ;", "out . write ( src 1  ,     0  ,    len 1  )  ;", "out . close (  )  ;", "long   expected    =    getExpectedPartitionsWritten ( len 1  ,     . PART _ SIZE _ BYTES ,    false )  ;", "assertPartitionsWritten (  \" initial   upload \"  ,    out ,    expected )  ;", "assertExists (  \" Exists \"  ,    path )  ;", "FileStatus   status    =    fs . getFileStatus ( path )  ;", "assertEquals (  \" Length \"  ,    len 1  ,    status . getLen (  )  )  ;", "final   int   len 2     =     4  0  9  5  ;", "final   byte [  ]    src 2     =    SwiftTestUtils . dataset ( len 2  ,     ' a '  ,     ' z '  )  ;", "out    =    fs . create ( path ,    true ,    getBufferSize (  )  ,     (  ( short )     (  1  )  )  ,     1  0  2  4  )  ;", "out . write ( src 2  ,     0  ,    len 2  )  ;", "out . close (  )  ;", "status    =    fs . getFileStatus ( path )  ;", "assertEquals (  \" Length \"  ,    len 2  ,    status . getLen (  )  )  ;", "byte [  ]    dest    =    SwiftTestUtils . readDataset ( fs ,    path ,    len 2  )  ;", "SwiftTestUtils . compareByteArrays ( src 2  ,    dest ,    len 2  )  ;", "}", "METHOD_END"], "methodName": ["testOverwritePartitionedFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  1  ,    fs . getStore (  )  . getPartsizeKB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPartionPropertyPropagatesToStore"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  1  ,    getConf (  )  . getInt ( SwiftProtocolConstants . SWIFT _ PARTITION _ SIZE ,     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testPartitionPropertyPropagatesToConf"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "Path   src    =    new   Path (  \"  / test / testRenamePartitionedFileSrc \"  )  ;", "int   len    =    data . length ;", "SwiftTestUtils . writeDataset ( fs ,    src ,    data ,    len ,     1  0  2  4  ,    false )  ;", "assertExists (  \" Exists \"  ,    src )  ;", "String   partOneName    =    SwiftUtils . partitionFilenameFromNumber (  1  )  ;", "Path   srcPart    =    new   Path ( src ,    partOneName )  ;", "Path   dest    =    new   Path (  \"  / test / testRenamePartitionedFileDest \"  )  ;", "Path   destPart    =    new   Path ( src ,    partOneName )  ;", "assertExists (  \" Partition   Exists \"  ,    srcPart )  ;", "fs . rename ( src ,    dest )  ;", "SwiftTestUtils . assertPathExists ( fs ,     \" dest   file   missing \"  ,    dest )  ;", "FileStatus   status    =    fs . getFileStatus ( dest )  ;", "assertEquals (  \" Length   of   renamed   file   is   wrong \"  ,    len ,    status . getLen (  )  )  ;", "byte [  ]    destData    =    SwiftTestUtils . readDataset ( fs ,    dest ,    len )  ;", "SwiftTestUtils . compareByteArrays ( data ,    destData ,    len )  ;", "String   srcLs    =    SwiftTestUtils . ls ( fs ,    src )  ;", "String   destLs    =    SwiftTestUtils . ls ( fs ,    dest )  ;", "assertPathDoesNotExist (  (  \" deleted   file   still   found   in    \"     +    srcLs )  ,    src )  ;", "assertPathDoesNotExist (  (  \" partition   file   still   found   in    \"     +    srcLs )  ,    srcPart )  ;", "}", "METHOD_END"], "methodName": ["testRenamePartitionedFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   Path   qualifiedPath    =    path . makeQualified ( fs )  ;", "FileStatus [  ]    parentDirListing    =    fs . listStatus ( qualifiedPath . getParent (  )  )  ;", "StringBuilder   listing    =    lsToString ( parentDirListing )  ;", "String   parentDirLS    =    listing . toString (  )  ;", "FileStatus   status    =    fs . getatus ( qualifiedPath )  ;", "assertEquals (  (  (  (  (  (  \" Length   of   written   file    \"     +    qualifiedPath )     +     \"    from   status   check    \"  )     +    status )     +     \"    in   dir    \"  )     +    listing )  ,    len ,    status . getLen (  )  )  ;", "String   fileInfo    =     ( qualifiedPath    +     \"        \"  )     +    status ;", "assertFalse (  (  \" File   claims   to   be   a   directory    \"     +    fileInfo )  ,    status . isDir (  )  )  ;", "FileStatus   listedFileStat    =    resolveChild ( parentDirListing ,    qualifiedPath )  ;", "assertNotNull (  (  (  (  \" Did   not   find    \"     +    path )     +     \"    in    \"  )     +    parentDirLS )  ,    listedFileStat )  ;", "assertEquals (  (  (  (  \" Wrong   len   for    \"     +    path )     +     \"    in   listing    \"  )     +    parentDirLS )  ,    len ,    listedFileStat . getLen (  )  )  ;", "listedFileStat . toString (  )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["validatePathLen"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemPartitionedUploads"}, {"methodBody": ["METHOD_START", "{", "final   String   message    =     \" message \"  ;", "final   Path   filePath    =    new   Path (  \"  / test / file . txt \"  )  ;", "TestUtils . writeTextFile ( fs ,    filePath ,    message ,    false )  ;", "try    {", "TestUtils . readBytesToString ( fs ,    filePath ,     2  0  )  ;", "fail (  \" expected   an   exception \"  )  ;", "}    catch    ( EOFException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testOverRead"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRead"}, {"methodBody": ["METHOD_START", "{", "final   String   message    =     \"  {  \"     +     (  (  \"     ' json '  :     {     ' i '  :  4  3  ,     ' b '  : true }  ,  \"     +     \"     ' s '  :  ' string '  \"  )     +     \"  }  \"  )  ;", "final   Path   filePath    =    new   Path (  \"  / test / file . json \"  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    filePath ,    message ,    false )  ;", "String   readJson    =    SwiftTestUtils . readBytesToString ( fs ,    filePath ,    message . length (  )  )  ;", "assertEquals ( message ,    readJson )  ;", "FileStatus   status    =    fs . getatus ( filePath )  ;", "BlockLocation [  ]    locations    =    fs . getFileBlockLocations ( status ,     0  ,     1  0  )  ;", "}", "METHOD_END"], "methodName": ["testRWJson"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRead"}, {"methodBody": ["METHOD_START", "{", "final   String   message    =     \"  < x >  \"     +     (  (  \"     < json   i =  '  4  3  '     ' b '  = true /  >  \"     +     \"    string \"  )     +     \"  <  / x >  \"  )  ;", "final   Path   filePath    =    new   Path (  \"  / test / file . xml \"  )  ;", "TestUtils . writeTextFile ( fs ,    filePath ,    message ,    false )  ;", "String   read    =    TestUtils . readBytesToString ( fs ,    filePath ,    message . length (  )  )  ;", "assertEquals ( message ,    read )  ;", "}", "METHOD_END"], "methodName": ["testRWXML"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRead"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( renameSupported (  )  )  )     {", "return ;", "}", "Path   testdir    =    path (  \" test / dir \"  )  ;", "fs . mkdirs ( testdir )  ;", "Path   parent    =    testdir . getParent (  )  ;", "try    {", "fs . rename ( testdir ,    parent )  ;", "}    catch    ( SwiftOperationFailedException   e )     {", "}", "assertExists (  \" Source   directory   has   been   deleted    \"  ,    testdir )  ;", "}", "METHOD_END"], "methodName": ["testMoveDirUnderParent"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( renameSupported (  )  )  )", "return ;", "Path   filepath    =    path (  \" test / file \"  )  ;", "createFile ( filepath )  ;", "rename ( filepath ,    filepath ,    true ,    true ,    true )  ;", "assertIsFile ( filepath )  ;", "}", "METHOD_END"], "methodName": ["testMoveFileUnderParent"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "Path   parentdir    =    path (  \"  / test / parentdir \"  )  ;", "fs . mkdirs ( parentdir )  ;", "Path   childFile    =    new   Path ( parentdir ,     \" childfile \"  )  ;", "createFile ( childFile )  ;", "Path   childdir    =    new   Path ( parentdir ,     \" childdir \"  )  ;", "rename ( parentdir ,    childdir ,    false ,    true ,    false )  ;", "fs . mkdirs ( childdir )  ;", "Path   childchilddir    =    new   Path ( childdir ,     \" childdir \"  )  ;", "rename ( parentdir ,    childchilddir ,    false ,    true ,    false )  ;", "}", "METHOD_END"], "methodName": ["testRenameChildDirForbidden"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "Path   parentdir    =    path (  \"  / test / parentdir \"  )  ;", "fs . mkdirs ( parentdir )  ;", "Path   child    =    new   Path ( parentdir ,     \" child \"  )  ;", "createFile ( child )  ;", "rename ( parentdir ,    parentdir ,    false ,    true ,    true )  ;", "assertIsFile ( child )  ;", "}", "METHOD_END"], "methodName": ["testRenameDirToSelf"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "final   Path   old    =    new   Path (  \"  / test / data / logs \"  )  ;", "final   Path   newPath    =    new   Path (  \"  / test / var / logs \"  )  ;", "fs . mkdirs ( old )  ;", "fs . mkdirs ( newPath . getParent (  )  )  ;", "assertTrue ( fs . exists ( old )  )  ;", "rename ( old ,    newPath ,    true ,    false ,    true )  ;", "}", "METHOD_END"], "methodName": ["testRenameDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "Path   src    =    path (  \"  / test / olddir / dir \"  )  ;", "fs . mkdirs ( src )  ;", "createFile ( path (  \"  / test / olddir / dir / file 1  \"  )  )  ;", "createFile ( path (  \"  / test / olddir / dir / subdir / file 2  \"  )  )  ;", "Path   dst    =    path (  \"  / test / new / newdir \"  )  ;", "fs . mkdirs ( dst )  ;", "rename ( src ,    dst ,    true ,    false ,    true )  ;", "assertExists (  \" new   dir \"  ,    path (  \"  / test / new / newdir / dir \"  )  )  ;", "assertExists (  \" Renamed   nested   file 1  \"  ,    path (  \"  / test / new / newdir / dir / file 1  \"  )  )  ;", "assertPathDoesNotExist (  \" Nested   file 1    should   have   been   deleted \"  ,    path (  \"  / test / olddir / dir / file 1  \"  )  )  ;", "assertExists (  \" Renamed   nested   subdir \"  ,    path (  \"  / test / new / newdir / dir / subdir /  \"  )  )  ;", "assertExists (  \" file   under   subdir \"  ,    path (  \"  / test / new / newdir / dir / subdir / file 2  \"  )  )  ;", "assertPathDoesNotExist (  \" Nested    / test / hadoop / dir / subdir / file 2    still   exists \"  ,    path (  \"  / test / olddir / dir / subdir / file 2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRenameDirectoryIntoExistingDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "final   Path   old    =    new   Path (  \"  / test / alice / file \"  )  ;", "final   Path   newPath    =    new   Path (  \"  / test / bob / file \"  )  ;", "fs . mkdirs ( newPath . getParent (  )  )  ;", "final   FSDataOutputStream   fsDataOutputStream    =    fs . create ( old )  ;", "final   byte [  ]    message    =     \" Some   data \"  . getBytes (  )  ;", "fsDataOutputStream . write ( message )  ;", "fsDataOutputStream . close (  )  ;", "assertTrue ( fs . exists ( old )  )  ;", "rename ( old ,    newPath ,    true ,    false ,    true )  ;", "final   FSDataInputStream   bobStream    =    fs . open ( newPath )  ;", "final   byte [  ]    bytes    =    new   byte [  5  1  2  ]  ;", "final   int   read    =    bobStream . read ( bytes )  ;", "bobStream . close (  )  ;", "final   byte [  ]    buffer    =    new   byte [ read ]  ;", "System . arraycopy ( bytes ,     0  ,    buffer ,     0  ,    read )  ;", "assertEquals ( new   String ( message )  ,    new   String ( buffer )  )  ;", "}", "METHOD_END"], "methodName": ["testRenameFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "final   Path   filePath    =    new   Path (  \"  / test / home / user / documents / file . txt \"  )  ;", "final   Path   newFilePath    =    new   Path (  \"  / test / home / user / files / file . txt \"  )  ;", "mkdirs ( newFilePath . getParent (  )  )  ;", "int   len    =     1  0  2  4  ;", "byte [  ]    dataset    =    SwiftTestUtils . dataset ( len ,     ' A '  ,     2  6  )  ;", "SwiftTestUtils . writeDataset ( fs ,    filePath ,    dataset ,    len ,    len ,    false )  ;", "rename ( filePath ,    newFilePath ,    true ,    false ,    true )  ;", "byte [  ]    dest    =    SwiftTestUtils . readDataset ( fs ,    newFilePath ,    len )  ;", "SwiftTestUtils . compareByteArrays ( dataset ,    dest ,    len )  ;", "String   reread    =    SwiftTestUtils . readBytesToString ( fs ,    newFilePath ,     2  0  )  ;", "}", "METHOD_END"], "methodName": ["testRenameFileAndVerifyContents"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "Path   src    =    path (  \"  / test / olddir / file \"  )  ;", "createFile ( src )  ;", "Path   dst    =    path (  \"  / test / new / newdir \"  )  ;", "fs . mkdirs ( dst )  ;", "rename ( src ,    dst ,    true ,    false ,    true )  ;", "Path   newFile    =    path (  \"  / test / new / newdir / file \"  )  ;", "if    (  !  ( fs . exists ( newFile )  )  )     {", "String   ls    =    ls ( dst )  ;", "BaseTest . LOG . info ( ls ( path (  \"  / test / new \"  )  )  )  ;", "BaseTest . LOG . info ( ls ( path (  \"  / test / hadoop \"  )  )  )  ;", "fail (  (  (  (  \" did   not   find    \"     +    newFile )     +     \"     -    directory :     \"  )     +    ls )  )  ;", "}", "assertTrue (  \" Destination   changed \"  ,    fs . exists ( path (  \"  / test / new / newdir / file \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRenameFileIntoExistingDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( renameSupported (  )  )  )", "return ;", "Path   filepath    =    path (  \" test / file \"  )  ;", "createFile ( filepath )  ;", "rename ( filepath ,    filepath ,    true ,    true ,    true )  ;", "assertIsFile ( filepath )  ;", "}", "METHOD_END"], "methodName": ["testRenameFileToSelf"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "Path   path    =    path (  \"  / test / RenameMissingFile \"  )  ;", "Path   path 2     =    path (  \"  / test / RenameMissingFileDest \"  )  ;", "mkdirs ( path (  \" test \"  )  )  ;", "rename ( path ,    path 2  ,    false ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testRenameMissingFile"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "rename ( path (  \"  /  \"  )  ,    path (  \"  / test / newRootDir \"  )  ,    false ,    true ,    false )  ;", "}", "METHOD_END"], "methodName": ["testRenameRootDirForbidden"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "final   Path   old    =    new   Path (  \"  / test / usr / data \"  )  ;", "fs . mkdirs ( old )  ;", "rename ( old ,    old ,    false ,    true ,    true )  ;", "}", "METHOD_END"], "methodName": ["testRenameTheSameDirectory"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "assumeRenameSupported (  )  ;", "describe (  (  \" verify   that   overwriting   a   file   with   new   data   doesn ' t   impact \"     +     \"    the   existing   content \"  )  )  ;", "final   Path   filePath    =    new   Path (  \"  / test / home / user / documents / file . txt \"  )  ;", "final   Path   newFilePath    =    new   Path (  \"  / test / home / user / files / file . txt \"  )  ;", "mkdirs ( newFilePath . getParent (  )  )  ;", "int   len    =     1  0  2  4  ;", "byte [  ]    dataset    =    SwiftTestUtils . dataset ( len ,     ' A '  ,     2  6  )  ;", "byte [  ]    dataset 2     =    SwiftTestUtils . dataset ( len ,     ' a '  ,     2  6  )  ;", "SwiftTestUtils . writeDataset ( fs ,    filePath ,    dataset ,    len ,    len ,    false )  ;", "rename ( filePath ,    newFilePath ,    true ,    false ,    true )  ;", "SwiftTestUtils . writeAndRead ( fs ,    filePath ,    dataset 2  ,    len ,    len ,    false ,    true )  ;", "byte [  ]    dest    =    SwiftTestUtils . readDataset ( fs ,    newFilePath ,    len )  ;", "SwiftTestUtils . compareByteArrays ( dataset ,    dest ,    len )  ;", "String   reread    =    SwiftTestUtils . readBytesToString ( fs ,    newFilePath ,     2  0  )  ;", "}", "METHOD_END"], "methodName": ["testRenamedConsistence"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftFileSystemRename"}, {"methodBody": ["METHOD_START", "{", "Assert . assertFalse (  (  (  ( p 1  . toString (  )  )     +     \"    is   a   parent   of    \"  )     +    p 2  )  ,    p 1  . isEqualToOrParentOf ( p 2  )  )  ;", "}", "METHOD_END"], "methodName": ["assertNotParentOf"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "Assert . assertTrue (  (  (  ( p 1  . toString (  )  )     +     \"    is   not   a   parent   of    \"  )     +    p 2  )  ,    p 1  . isEqualToOrParentOf ( p 2  )  )  ;", "}", "METHOD_END"], "methodName": ["assertParentOf"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   parent    =    new   SwiftObjectPath (  \" container \"  ,     \"  / parent \"  )  ;", "SwiftObjectPath   parent 2     =    new   SwiftObjectPath (  \" container \"  ,     \"  / parent 2  \"  )  ;", "SwiftObjectPath   child    =    new   SwiftObjectPath (  \" container \"  ,     \"  / parent / child \"  )  ;", "SwiftObjectPath   sibling    =    new   SwiftObjectPath (  \" container \"  ,     \"  / parent / sibling \"  )  ;", "SwiftObjectPath   grandchild    =    new   SwiftObjectPath (  \" container \"  ,     \"  / parent / child / grandchild \"  )  ;", "assertParentOf ( parent ,    child )  ;", "assertParentOf ( parent ,    grandchild )  ;", "assertParentOf ( child ,    grandchild )  ;", "assertParentOf ( parent ,    parent )  ;", "assertNotParentOf ( child ,    parent )  ;", "assertParentOf ( child ,    child )  ;", "assertNotParentOf ( parent ,    parent 2  )  ;", "assertNotParentOf ( grandchild ,    parent )  ;", "}", "METHOD_END"], "methodName": ["testChildOfProbe"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   root    =    new   SwiftObjectPath (  \" container \"  ,     \"  /  \"  )  ;", "SwiftObjectPath   child    =    new   SwiftObjectPath (  \" container \"  ,     \" child \"  )  ;", "SwiftObjectPath   grandchild    =    new   SwiftObjectPath (  \" container \"  ,     \"  / child / grandchild \"  )  ;", "assertParentOf ( root ,    child )  ;", "assertParentOf ( root ,    grandchild )  ;", "assertParentOf ( child ,    grandchild )  ;", "assertParentOf ( root ,    root )  ;", "assertNotParentOf ( child ,    root )  ;", "assertParentOf ( child ,    child )  ;", "assertNotParentOf ( grandchild ,    root )  ;", "}", "METHOD_END"], "methodName": ["testChildOfRoot"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "String   initialpath    =     \"  / dir / file 1  \"  ;", "Path   ipath    =    new   Path ( initialpath )  ;", "SwiftObjectPath   objectPath    =    SwiftObjectPath . fromPath ( new   URI ( initialpath )  ,    ipath )  ;", "URI   endpoint    =    new   URI (  . ENDPOINT )  ;", "URI   uri    =    SwiftRestClient . pathToURI ( objectPath ,    endpoint )  ;", ". LOG . info (  (  \" Inital   Hadoop   Path    =  \"     +    initialpath )  )  ;", ". LOG . info (  (  \" Merged   URI =  \"     +    uri )  )  ;", "}", "METHOD_END"], "methodName": ["testConvertToPath"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "final   String   hostPart    =     \" swift :  /  / container . service 1  \"  ;", "final   String   pathPart    =     \"  / home / user / files / file 1  \"  ;", "final   String   uriString    =    hostPart    +    pathPart ;", "final      expected    =    new    ( uriString ,    pathPart )  ;", "final      actual    =    new    ( uriString ,    uriString )  ;", "Assert . assertEquals ( expected ,    actual )  ;", "}", "METHOD_END"], "methodName": ["testHandleUrlAsPath"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "final   String   pathString    =     \" swift :  /  / container . service 1  / v 2  / AUTH _  0  0  3  4  5 h 3  4 l 9  3  4  5  9 y 4  / home / tom / documents / finance . docx \"  ;", "final   URI   uri    =    new   URI ( pathString )  ;", "final   Path   path    =    new   Path ( pathString )  ;", "final      expected    =     . fromPath ( uri ,    path )  ;", "final      actual    =    new    ( RestClientBindings . extractContainerName ( uri )  ,     \"  / home / tom / documents / finance . docx \"  )  ;", "Assert . assertEquals ( expected ,    actual )  ;", "}", "METHOD_END"], "methodName": ["testParseAuthenticatedUrl"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "final   String   pathString    =     \"  / home / user / files / file 1  \"  ;", "final   Path   path    =    new   Path ( pathString )  ;", "final   URI   uri    =    new   URI (  \" http :  /  / container . localhost \"  )  ;", "final      expected    =     . fromPath ( uri ,    path )  ;", "final      actual    =    new    ( RestClientBindings . extractContainerName ( uri )  ,    pathString )  ;", "Assert . assertEquals ( expected ,    actual )  ;", "}", "METHOD_END"], "methodName": ["testParsePath"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "final   String   pathString    =     \" swift :  /  / container . service 1  / home / user / files / file 1  \"  ;", "final   URI   uri    =    new   URI ( pathString )  ;", "final   Path   path    =    new   Path ( pathString )  ;", "final      expected    =     . fromPath ( uri ,    path )  ;", "final      actual    =    new    ( RestClientBindings . extractContainerName ( uri )  ,     \"  / home / user / files / file 1  \"  )  ;", "Assert . assertEquals ( expected ,    actual )  ;", "}", "METHOD_END"], "methodName": ["testParseUrlPath"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   object    =    new   SwiftObjectPath (  \" container \"  ,     \"  \"  )  ;", "Assert . assertTrue ( SwiftUtils . isRootDir ( object )  )  ;", "}", "METHOD_END"], "methodName": ["testRootDirProbeEmptyPath"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   object    =    new   SwiftObjectPath (  \" container \"  ,     \"  /  \"  )  ;", "Assert . assertTrue ( SwiftUtils . isRootDir ( object )  )  ;", "}", "METHOD_END"], "methodName": ["testRootDirProbeRootPath"], "fileName": "org.apache.hadoop.fs.swift.TestSwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "return   apiKeyCredentials ;", "}", "METHOD_END"], "methodName": ["getApiKeyCredentials"], "fileName": "org.apache.hadoop.fs.swift.auth.ApiKeyAuthenticationRequest"}, {"methodBody": ["METHOD_START", "{", "this . apiKeyCredentials    =    apiKeyCredentials ;", "}", "METHOD_END"], "methodName": ["setApiKeyCredentials"], "fileName": "org.apache.hadoop.fs.swift.auth.ApiKeyAuthenticationRequest"}, {"methodBody": ["METHOD_START", "{", "return   apikey ;", "}", "METHOD_END"], "methodName": ["getApiKey"], "fileName": "org.apache.hadoop.fs.swift.auth.ApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "return   username ;", "}", "METHOD_END"], "methodName": ["getUsername"], "fileName": "org.apache.hadoop.fs.swift.auth.ApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "this . apikey    =    apikey ;", "}", "METHOD_END"], "methodName": ["setApiKey"], "fileName": "org.apache.hadoop.fs.swift.auth.ApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "this . username    =    username ;", "}", "METHOD_END"], "methodName": ["setUsername"], "fileName": "org.apache.hadoop.fs.swift.auth.ApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "return   tenantName ;", "}", "METHOD_END"], "methodName": ["getTenantName"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationRequest"}, {"methodBody": ["METHOD_START", "{", "this . tenantName    =    tenantName ;", "}", "METHOD_END"], "methodName": ["setTenantName"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationRequest"}, {"methodBody": ["METHOD_START", "{", "return   auth ;", "}", "METHOD_END"], "methodName": ["getAuth"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationRequestWrapper"}, {"methodBody": ["METHOD_START", "{", "this . auth    =    auth ;", "}", "METHOD_END"], "methodName": ["setAuth"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationRequestWrapper"}, {"methodBody": ["METHOD_START", "{", "return   metadata ;", "}", "METHOD_END"], "methodName": ["getMetadata"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "return   serviceCatalog ;", "}", "METHOD_END"], "methodName": ["getServiceCatalog"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "return   token ;", "}", "METHOD_END"], "methodName": ["getToken"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "this . metadata    =    metadata ;", "}", "METHOD_END"], "methodName": ["setMetadata"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "this . serviceCatalog    =    serviceCatalog ;", "}", "METHOD_END"], "methodName": ["setServiceCatalog"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "this . token    =    token ;", "}", "METHOD_END"], "methodName": ["setToken"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "this . user    =    user ;", "}", "METHOD_END"], "methodName": ["setUser"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationResponse"}, {"methodBody": ["METHOD_START", "{", "return   access ;", "}", "METHOD_END"], "methodName": ["getAccess"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationWrapper"}, {"methodBody": ["METHOD_START", "{", "this . access    =    access ;", "}", "METHOD_END"], "methodName": ["setAccess"], "fileName": "org.apache.hadoop.fs.swift.auth.AuthenticationWrapper"}, {"methodBody": ["METHOD_START", "{", "return   apiAccessKeyCredentials ;", "}", "METHOD_END"], "methodName": ["getApiAccessKeyCredentials"], "fileName": "org.apache.hadoop.fs.swift.auth.KeyStoneAuthRequest"}, {"methodBody": ["METHOD_START", "{", "this . apiAccessKeyCredentials    =    apiAccessKeyCredentials ;", "}", "METHOD_END"], "methodName": ["setApiAccessKeyCredentials"], "fileName": "org.apache.hadoop.fs.swift.auth.KeyStoneAuthRequest"}, {"methodBody": ["METHOD_START", "{", "return   accessKey ;", "}", "METHOD_END"], "methodName": ["getAccessKey"], "fileName": "org.apache.hadoop.fs.swift.auth.KeystoneApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "return   secretKey ;", "}", "METHOD_END"], "methodName": ["getSecretKey"], "fileName": "org.apache.hadoop.fs.swift.auth.KeystoneApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "this . accessKey    =    accessKey ;", "}", "METHOD_END"], "methodName": ["setAccessKey"], "fileName": "org.apache.hadoop.fs.swift.auth.KeystoneApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "this . secretKey    =    secretKey ;", "}", "METHOD_END"], "methodName": ["setSecretKey"], "fileName": "org.apache.hadoop.fs.swift.auth.KeystoneApiKeyCredentials"}, {"methodBody": ["METHOD_START", "{", "return   passwordCredentials ;", "}", "METHOD_END"], "methodName": ["getPasswordCredentials"], "fileName": "org.apache.hadoop.fs.swift.auth.PasswordAuthenticationRequest"}, {"methodBody": ["METHOD_START", "{", "this . passwordCredentials    =    passwordCredentials ;", "}", "METHOD_END"], "methodName": ["setPasswordCredentials"], "fileName": "org.apache.hadoop.fs.swift.auth.PasswordAuthenticationRequest"}, {"methodBody": ["METHOD_START", "{", "return   password ;", "}", "METHOD_END"], "methodName": ["getPassword"], "fileName": "org.apache.hadoop.fs.swift.auth.PasswordCredentials"}, {"methodBody": ["METHOD_START", "{", "return   username ;", "}", "METHOD_END"], "methodName": ["getUsername"], "fileName": "org.apache.hadoop.fs.swift.auth.PasswordCredentials"}, {"methodBody": ["METHOD_START", "{", "this . password    =    password ;", "}", "METHOD_END"], "methodName": ["setPassword"], "fileName": "org.apache.hadoop.fs.swift.auth.PasswordCredentials"}, {"methodBody": ["METHOD_START", "{", "this . username    =    username ;", "}", "METHOD_END"], "methodName": ["setUsername"], "fileName": "org.apache.hadoop.fs.swift.auth.PasswordCredentials"}, {"methodBody": ["METHOD_START", "{", "return   description ;", "}", "METHOD_END"], "methodName": ["getDescription"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "return   serviceId ;", "}", "METHOD_END"], "methodName": ["getServiceId"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "return   tenantId ;", "}", "METHOD_END"], "methodName": ["getTenantId"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "this . description    =    description ;", "}", "METHOD_END"], "methodName": ["setDescription"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "this . id    =    id ;", "}", "METHOD_END"], "methodName": ["setId"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "this . serviceId    =    serviceId ;", "}", "METHOD_END"], "methodName": ["setServiceId"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "this . tenantId    =    tenantId ;", "}", "METHOD_END"], "methodName": ["setTenantId"], "fileName": "org.apache.hadoop.fs.swift.auth.Roles"}, {"methodBody": ["METHOD_START", "{", "return   expires ;", "}", "METHOD_END"], "methodName": ["getExpires"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.AccessToken"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.AccessToken"}, {"methodBody": ["METHOD_START", "{", "return   tenant ;", "}", "METHOD_END"], "methodName": ["getTenant"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.AccessToken"}, {"methodBody": ["METHOD_START", "{", "this . expires    =    expires ;", "}", "METHOD_END"], "methodName": ["setExpires"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.AccessToken"}, {"methodBody": ["METHOD_START", "{", "this . id    =    id ;", "}", "METHOD_END"], "methodName": ["setId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.AccessToken"}, {"methodBody": ["METHOD_START", "{", "this . tenant    =    tenant ;", "}", "METHOD_END"], "methodName": ["setTenant"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.AccessToken"}, {"methodBody": ["METHOD_START", "{", "return   endpoints ;", "}", "METHOD_END"], "methodName": ["getEndpoints"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "return   endpoints _ links ;", "}", "METHOD_END"], "methodName": ["getEndpoints_links"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "this . endpoints    =    endpoints ;", "}", "METHOD_END"], "methodName": ["setEndpoints"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "this . endpoints _ links    =    endpoints _ links ;", "}", "METHOD_END"], "methodName": ["setEndpoints_links"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "this . type    =    type ;", "}", "METHOD_END"], "methodName": ["setType"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Catalog"}, {"methodBody": ["METHOD_START", "{", "return   adminURL ;", "}", "METHOD_END"], "methodName": ["getAdminURL"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   internalURL ;", "}", "METHOD_END"], "methodName": ["getInternalURL"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   publicURL ;", "}", "METHOD_END"], "methodName": ["getPublicURL"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   publicURL 2  ;", "}", "METHOD_END"], "methodName": ["getPublicURL2"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   region ;", "}", "METHOD_END"], "methodName": ["getRegion"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   tenantId ;", "}", "METHOD_END"], "methodName": ["getTenantId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   versionId ;", "}", "METHOD_END"], "methodName": ["getVersionId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   versionInfo ;", "}", "METHOD_END"], "methodName": ["getVersionInfo"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   versionList ;", "}", "METHOD_END"], "methodName": ["getVersionList"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . adminURL    =    adminURL ;", "}", "METHOD_END"], "methodName": ["setAdminURL"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . id    =    id ;", "}", "METHOD_END"], "methodName": ["setId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . internalURL    =    internalURL ;", "}", "METHOD_END"], "methodName": ["setInternalURL"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . publicURL    =    publicURL ;", "}", "METHOD_END"], "methodName": ["setPublicURL"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . publicURL 2     =    publicURL 2  ;", "}", "METHOD_END"], "methodName": ["setPublicURL2"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . region    =    region ;", "}", "METHOD_END"], "methodName": ["setRegion"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . tenantId    =    tenantId ;", "}", "METHOD_END"], "methodName": ["setTenantId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . versionId    =    versionId ;", "}", "METHOD_END"], "methodName": ["setVersionId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . versionInfo    =    versionInfo ;", "}", "METHOD_END"], "methodName": ["setVersionInfo"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "this . versionList    =    versionList ;", "}", "METHOD_END"], "methodName": ["setVersionList"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Endpoint"}, {"methodBody": ["METHOD_START", "{", "return   description ;", "}", "METHOD_END"], "methodName": ["getDescription"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "return   enabled ;", "}", "METHOD_END"], "methodName": ["isEnabled"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "this . description    =    description ;", "}", "METHOD_END"], "methodName": ["setDescription"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "this . enabled    =    enabled ;", "}", "METHOD_END"], "methodName": ["setEnabled"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "this . id    =    id ;", "}", "METHOD_END"], "methodName": ["setId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.Tenant"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "return   roles ;", "}", "METHOD_END"], "methodName": ["getRoles"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "return   roles _ links ;", "}", "METHOD_END"], "methodName": ["getRoles_links"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "return   username ;", "}", "METHOD_END"], "methodName": ["getUsername"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "this . id    =    id ;", "}", "METHOD_END"], "methodName": ["setId"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "this . roles    =    roles ;", "}", "METHOD_END"], "methodName": ["setRoles"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "this . roles _ links    =    roles _ links ;", "}", "METHOD_END"], "methodName": ["setRoles_links"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "this . username    =    username ;", "}", "METHOD_END"], "methodName": ["setUsername"], "fileName": "org.apache.hadoop.fs.swift.auth.entities.User"}, {"methodBody": ["METHOD_START", "{", "return    \" Invalid   Response \"  ;", "}", "METHOD_END"], "methodName": ["exceptionTitle"], "fileName": "org.apache.hadoop.fs.swift.exceptions.SwiftInvalidResponseException"}, {"methodBody": ["METHOD_START", "{", "return   body ;", "}", "METHOD_END"], "methodName": ["getBody"], "fileName": "org.apache.hadoop.fs.swift.exceptions.SwiftInvalidResponseException"}, {"methodBody": ["METHOD_START", "{", "return   operation ;", "}", "METHOD_END"], "methodName": ["getOperation"], "fileName": "org.apache.hadoop.fs.swift.exceptions.SwiftInvalidResponseException"}, {"methodBody": ["METHOD_START", "{", "return   statusCode ;", "}", "METHOD_END"], "methodName": ["getStatusCode"], "fileName": "org.apache.hadoop.fs.swift.exceptions.SwiftInvalidResponseException"}, {"methodBody": ["METHOD_START", "{", "return   uri ;", "}", "METHOD_END"], "methodName": ["getUri"], "fileName": "org.apache.hadoop.fs.swift.exceptions.SwiftInvalidResponseException"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  \" isDirectory (  )  :    Not   a   directory :     \"     +    stat )  ,    stat . isDirectory (  )  )  ;", "assertFalse (  (  \" isFile (  )  :    declares   itself   a   file :     \"     +    stat )  ,    stat . isFile (  )  )  ;", "assertFalse (  (  \" isFile (  )  :    declares   itself   a   file :     \"     +    stat )  ,    stat . isSymlink (  )  )  ;", "}", "METHOD_END"], "methodName": ["extraStatusAssertions"], "fileName": "org.apache.hadoop.fs.swift.hdfs2.TestSwiftFileSystemDirectoriesHdfs2"}, {"methodBody": ["METHOD_START", "{", "RemoteIterator < LocatedFileStatus >    iterator    =    fs . listFiles ( dir ,    recursive )  ;", "boolean   found    =    false ;", "int   entries    =     0  ;", "StringBuilder   builder    =    new   StringBuilder (  )  ;", "while    ( iterator . hasNext (  )  )     {", "LocatedFileStatus   next    =    iterator . next (  )  ;", "entries +  +  ;", "builder . append ( next . toString (  )  )  . append (  '  \\ n '  )  ;", "if    ( next . getPath (  )  . equals ( subdir )  )     {", "found    =    true ;", "}", "}", "assertTrue (  (  (  (  (  (  (  (  (  \" Path    \"     +    subdir )     +     \"    not   found   in   directory    \"  )     +    dir )     +     \"     :     \"  )     +     \"    entries =  \"  )     +    entries )     +     \"    content \"  )     +     ( builder . toString (  )  )  )  ,    found )  ;", "}", "METHOD_END"], "methodName": ["assertListFilesFinds"], "fileName": "org.apache.hadoop.fs.swift.hdfs2.TestV2LsOperations"}, {"methodBody": ["METHOD_START", "{", "testDirs    =    new   Path [  ]  {    path (  \"  / test / hadoop / a \"  )  ,    path (  \"  / test / hadoop / b \"  )  ,    path (  \"  / test / hadoop / c /  1  \"  )     }  ;", "assertPathDoesNotExist (  \" test   directory   setup \"  ,    testDirs [  0  ]  )  ;", "for    ( Path   path    :    testDirs )     {", "mkdirs ( path )  ;", "}", "}", "METHOD_END"], "methodName": ["createTestSubdirs"], "fileName": "org.apache.hadoop.fs.swift.hdfs2.TestV2LsOperations"}, {"methodBody": ["METHOD_START", "{", "createTestSubdirs (  )  ;", "Path   dir    =    path (  \"  / test / recursive \"  )  ;", "Path   child    =    new   Path ( dir ,     \" hadoop / a / a . txt \"  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    child ,     \" text \"  ,    false )  ;", ". assertListFilesFinds ( fs ,    dir ,    child ,    true )  ;", "}", "METHOD_END"], "methodName": ["testListFilesRecursive"], "fileName": "org.apache.hadoop.fs.swift.hdfs2.TestV2LsOperations"}, {"methodBody": ["METHOD_START", "{", "Path   dir    =    path (  \"  /  \"  )  ;", "Path   child    =    new   Path ( dir ,     \" test \"  )  ;", "fs . delete ( child ,    true )  ;", "SwiftTestUtils . writeTextFile ( fs ,    child ,     \" text \"  ,    false )  ;", ". assertListFilesFinds ( fs ,    dir ,    child ,    false )  ;", "}", "METHOD_END"], "methodName": ["testListFilesRootDir"], "fileName": "org.apache.hadoop.fs.swift.hdfs2.TestV2LsOperations"}, {"methodBody": ["METHOD_START", "{", "createTestSubdirs (  )  ;", "Path   dir    =    path (  \"  / test / subdir \"  )  ;", "Path   child    =    new   Path ( dir ,     \" text . txt \"  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    child ,     \" text \"  ,    false )  ;", ". assertListFilesFinds ( fs ,    dir ,    child ,    false )  ;", "}", "METHOD_END"], "methodName": ["testListFilesSubDir"], "fileName": "org.apache.hadoop.fs.swift.hdfs2.TestV2LsOperations"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ExceptionDiags . FOR _ MORE _ DETAILS _ SEE )     +     ( ExceptionDiags . HADOOP _ WIKI )  )     +    entry ;", "}", "METHOD_END"], "methodName": ["see"], "fileName": "org.apache.hadoop.fs.swift.http.ExceptionDiags"}, {"methodBody": ["METHOD_START", "{", "String   action    =     ( operation    +     \"     \"  )     +    dest ;", "String   xref    =    null ;", "if    ( exception   instanceof   ConnectException )     {", "xref    =     \" ConnectionRefused \"  ;", "} else", "if    ( exception   instanceof   UnknownHostException )     {", "xref    =     \" UnknownHost \"  ;", "} else", "if    ( exception   instanceof   SocketTimeoutException )     {", "xref    =     \" SocketTimeout \"  ;", "} else", "if    ( exception   instanceof   NoRouteToHostException )     {", "xref    =     \" NoRouteToHost \"  ;", "}", "String   msg    =     ( action    +     \"    failed   on   exception :     \"  )     +    exception ;", "if    ( xref    !  =    null )     {", "msg    =     ( msg    +     \"  ;  \"  )     +     (  . see ( xref )  )  ;", "}", "return    . wrapWithMessage ( exception ,    msg )  ;", "}", "METHOD_END"], "methodName": ["wrapException"], "fileName": "org.apache.hadoop.fs.swift.http.ExceptionDiags"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    clazz    =    exception . getClass (  )  ;", "try    {", "Constructor <  ?    extends   Throwable >    ctor    =    clazz . getConstructor ( String . class )  ;", "Throwable   t    =    ctor . newInstance ( msg )  ;", "return    (  ( T )     ( t . initCause ( exception )  )  )  ;", "}    catch    ( Throwable   e )     {", ". LOG . warn (  (  (  \" Unable   to   wrap   exception   of   type    \"     +    clazz )     +     \"  :    it   has   no    ( String )    constructor \"  )  ,    e )  ;", "return   exception ;", "}", "}", "METHOD_END"], "methodName": ["wrapWithMessage"], "fileName": "org.apache.hadoop.fs.swift.http.ExceptionDiags"}, {"methodBody": ["METHOD_START", "{", "return   contentLength ;", "}", "METHOD_END"], "methodName": ["getContentLength"], "fileName": "org.apache.hadoop.fs.swift.http.HttpBodyContent"}, {"methodBody": ["METHOD_START", "{", "return   inputStream ;", "}", "METHOD_END"], "methodName": ["getInputStream"], "fileName": "org.apache.hadoop.fs.swift.http.HttpBodyContent"}, {"methodBody": ["METHOD_START", "{", "if    (  ( released )     |  |     (  ( inStream )     =  =    null )  )     {", "throw   new   SwiftConnectionClosedException ( reasonClosed )  ;", "}", "}", "METHOD_END"], "methodName": ["assumeNotReleased"], "fileName": "org.apache.hadoop.fs.swift.http.HttpInputStreamWithRelease"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( released )  )     {", "reasonClosed    =    reason ;", "try    {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Releasing   connection   to    \"     +     ( uri )  )     +     \"  :        \"  )     +    reason )  ,    ex )  ;", "}", "if    (  ( method )     !  =    null )     {", "if    (  !  ( dataConsumed )  )     {", "method . abort (  )  ;", "}", "method . releaseConnection (  )  ;", "}", "if    (  ( inStream )     !  =    null )     {", "inStream . close (  )  ;", "}", "return   true ;", "}    finally    {", "released    =    true ;", "dataConsumed    =    true ;", "}", "} else    {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["release"], "fileName": "org.apache.hadoop.fs.swift.http.HttpInputStreamWithRelease"}, {"methodBody": ["METHOD_START", "{", "try    {", "release ( operation ,    ex )  ;", "}    catch    ( IOException   ioe )     {", ". LOG . debug (  (  (  (  \" Exception   during   release :     \"     +    operation )     +     \"     -     \"  )     +    ioe )  ,    ioe )  ;", "if    ( ex    =  =    null )     {", "ex    =    ioe ;", "}", "}", "return   ex ;", "}", "METHOD_END"], "methodName": ["releaseAndRethrow"], "fileName": "org.apache.hadoop.fs.swift.http.HttpInputStreamWithRelease"}, {"methodBody": ["METHOD_START", "{", "String   host    =    fsURI . getHost (  )  ;", "if    (  ( host    =  =    null )     |  |     ( host . isEmpty (  )  )  )     {", "throw    . invalidName ( host )  ;", "}", "String   container    =     . extractContainerName ( host )  ;", "String   service    =     . extractServiceName ( host )  ;", "String   prefix    =     . buildSwiftInstancePrefix ( service )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Filesystem    \"     +    fsURI )     +     \"    is   using   configuration   keys    \"  )     +    prefix )  )  ;", "}", "Properties   props    =    new   Properties (  )  ;", "props . setProperty ( SwiftProtocolConstants . SWIFT _ SERVICE _ PROPERTY ,    service )  ;", "props . setProperty ( SwiftProtocolConstants . SWIFT _ CONTAINER _ PROPERTY ,    container )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ AUTH _ URL )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ AUTH _ PROPERTY ,    true )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ USERNAME )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ USERNAME _ PROPERTY ,    true )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ APIKEY )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ APIKEY _ PROPERTY ,    false )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ PASSWORD )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ PASSWORD _ PROPERTY ,     ( props . contains ( SwiftProtocolConstants . SWIFT _ APIKEY _ PROPERTY )     ?    true    :    false )  )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ TENANT )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ TENANT _ PROPERTY ,    false )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ REGION )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ REGION _ PROPERTY ,    false )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ HTTP _ PORT )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ HTTP _ PORT _ PROPERTY ,    false )  ;", ". copy ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ HTTPS _ PORT )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ HTTPS _ PORT _ PROPERTY ,    false )  ;", ". copyBool ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ PUBLIC )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ PUBLIC _ PROPERTY ,    false )  ;", ". copyBool ( conf ,     ( prefix    +     ( SwiftProtocolConstants . DOT _ LOCATION _ AWARE )  )  ,    props ,    SwiftProtocolConstants . SWIFT _ LOCATION _ AWARE _ PROPERTY ,    false )  ;", "return   props ;", "}", "METHOD_END"], "methodName": ["bind"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "return    ( SwiftProtocolConstants . SWIFT _ SERVICE _ PREFIX )     +    service ;", "}", "METHOD_END"], "methodName": ["buildSwiftInstancePrefix"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "String   val    =    conf . get ( confKey )  ;", "if    ( val    !  =    null )     {", "val    =    val . trim (  )  ;", "}", "if    ( required    &  &     ( val    =  =    null )  )     {", "throw   new   SwiftConfigurationException (  (  \" Missing   mandatory   configuration   option :     \"     +    confKey )  )  ;", "}", ". set ( props ,    propsKey ,    val )  ;", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "boolean   b    =    conf . getBoolean ( confKey ,    defVal )  ;", "propetProperty ( propsKey ,    Boolean . toString ( b )  )  ;", "}", "METHOD_END"], "methodName": ["copyBool"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "int   i    =    hostname . indexOf (  \"  .  \"  )  ;", "if    ( i    <  =     0  )     {", "throw    . invalidName ( hostname )  ;", "}", "return   hostname . substring (  0  ,    i )  ;", "}", "METHOD_END"], "methodName": ["extractContainerName"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "return   RestClientBindings . extractContainerName ( uri . getHost (  )  )  ;", "}", "METHOD_END"], "methodName": ["extractContainerName"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "int   i    =    hostname . indexOf (  \"  .  \"  )  ;", "if    ( i    <  =     0  )     {", "throw    . invalidName ( hostname )  ;", "}", "String   service    =    hostname . substring (  ( i    +     1  )  )  ;", "if    (  ( service . isEmpty (  )  )     |  |     ( service . contains (  \"  .  \"  )  )  )     {", "throw    . invalidName ( hostname )  ;", "}", "return   service ;", "}", "METHOD_END"], "methodName": ["extractServiceName"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "return   RestClientBindings . extractServiceName ( uri . getHost (  )  )  ;", "}", "METHOD_END"], "methodName": ["extractServiceName"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "return   new   SwiftConfigurationException ( String . format ( RestClientBindings . E _ INVALID _ NAME ,    hostname )  )  ;", "}", "METHOD_END"], "methodName": ["invalidName"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "if    ( optVal    !  =    null )     {", "propetProperty ( key ,    optVal )  ;", "}", "}", "METHOD_END"], "methodName": ["set"], "fileName": "org.apache.hadoop.fs.swift.http.RestClientBindings"}, {"methodBody": ["METHOD_START", "{", "if    (  ( getEndpointURI (  )  )     =  =    null )     {", "authicate (  )  ;", "}", "}", "METHOD_END"], "methodName": ["authIfNeeded"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "final   AuthenticationRequest   authenticationRequest ;", "if    ( useKeystoneAuthentication )     {", "authenticationRequest    =    keystoneAuthRequest ;", "} else    {", "authenticationRequest    =    authRequest ;", "}", ". LOG . debug (  \" started   authentication \"  )  ;", "return   perform (  \" authentication \"  ,    authUri ,    new    . AuthenticationPost ( authenticationRequest )  )  ;", "}", "METHOD_END"], "methodName": ["authenticate"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "IOException   fault ;", "String   errorMessage    =    String . format (  (  \" Method    % s   on    % s   failed ,    status   code :     % d ,  \"     +     \"    status   line :     % s \"  )  ,    method . getName (  )  ,    uri ,    statusCode ,    method . getStatusLine (  )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug ( errorMessage )  ;", "}", "switch    ( statusCode )     {", "case   SC _ NOT _ FOUND    :", "fault    =    new   FileNotFoundException (  (  (  (  \" Operation    \"     +     ( method . getName (  )  )  )     +     \"    on    \"  )     +    uri )  )  ;", "break ;", "case   SC _ BAD _ REQUEST    :", "fault    =    new   SwiftBadRequestException (  (  \" Bad   request   against    \"     +    uri )  ,    method . getName (  )  ,    uri ,    method )  ;", "break ;", "case   SC _ REQUESTED _ RANGE _ NOT _ SATISFIABLE    :", "StringBuilder   errorText    =    new   StringBuilder ( method . getStatusText (  )  )  ;", "Header   requestContentLen    =    method . getRequestHeader ( SwiftProtocolConstants . HEADER _ CONTENT _ LENGTH )  ;", "if    ( requestContentLen    !  =    null )     {", "errorText . append (  \"    requested    \"  )  . append ( requestContentLen . getValue (  )  )  ;", "}", "Header   availableContentRange    =    method . getResponseHeader ( SwiftProtocolConstants . HEADER _ CONTENT _ RANGE )  ;", "if    ( requestContentLen    !  =    null )     {", "errorText . append (  \"    available    \"  )  . append ( availableContentRange . getValue (  )  )  ;", "}", "fault    =    new   EOFException ( errorText . toString (  )  )  ;", "break ;", "case   SC _ UNAUTHORIZED    :", "fault    =    new   SwiftAuthenticationFailedException (  (  \" Operation   not   authorized -    current   access   token    =  \"     +     ( getToken (  )  )  )  ,    method . getName (  )  ,    uri ,    method )  ;", "break ;", "case   SwiftProtocolConstants . SC _ TOO _ MANY _ REQUESTS _  4  2  9     :", "case   SwiftProtocolConstants . SC _ THROTTLED _  4  9  8     :", "fault    =    new   SwiftThrottledRequestException (  \" Client   is   being   throttled :    too   many   requests \"  ,    method . getName (  )  ,    uri ,    method )  ;", "break ;", "default    :", "fault    =    new   SwiftInvalidResponseException ( errorMessage ,    method . getName (  )  ,    uri ,    method )  ;", "}", "return   fault ;", "}", "METHOD_END"], "methodName": ["buildException"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   SwiftRestClient . checkNotNull ( reference ,     \" Null   Reference \"  )  ;", "}", "METHOD_END"], "methodName": ["checkNotNull"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    ( reference    =  =    null )     {", "throw   new   InternalStateException ( message )  ;", "}", "return   reference ;", "}", "METHOD_END"], "methodName": ["checkNotNull"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" copyObject \"  )  ;", "return   perform (  \" copy \"  ,    pathToURI ( src )  ,    new    . CopyMethodProcessor < Boolean >  (  )     {", "@ Override", "public   Boolean   extractResult ( CopyMethod   method )    throws   IOException    {", "return    ( method . getStatusCode (  )  )     !  =     ( SC _ NOT _ FOUND )  ;", "}", "@ Override", "protected   void   setup ( CopyMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    headers )  ;", "method . addRequestHeader ( SwiftProtocolConstants . HEADER _ DESTINATION ,    dst . toUriPath (  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["copyObject"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   objectPath    =    new   SwiftObjectPath ( containerName ,     \"  \"  )  ;", "try    {", "headRequest (  \" createContainer \"  ,    objectPath ,     . NEWEST )  ;", "}    catch    ( FileNotFoundException   ex )     {", "int   status    =     0  ;", "try    {", "status    =    putRequest ( objectPath )  ;", "}    catch    ( FileNotFoundException   e )     {", "status    =    SC _ NOT _ FOUND ;", "}", "if    ( status    =  =     ( SC _ BAD _ REQUEST )  )     {", "throw   new   SwiftBadRequestException (  \" Bad   request    - authentication   failure   or   bad   container   name ?  \"  ,    status ,     \" PUT \"  ,    null )  ;", "}", "if    (  !  ( isStatusCodeExpected ( status ,    SC _ OK ,    SC _ CREATED ,    SC _ ACCEPTED ,    SC _ NO _ CONTENT )  )  )     {", "throw   new   SwiftInvalidResponseException (  (  (  (  (  (  \" Couldn ' t   create   container    \"     +    containerName )     +     \"    for   storing   data   in   Swift .  \"  )     +     \"    Try   to   create   container    \"  )     +    containerName )     +     \"    manually    \"  )  ,    status ,     \" PUT \"  ,    null )  ;", "} else    {", "throw   ex ;", "}", "}", "}", "METHOD_END"], "methodName": ["createContainer"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "createContainer ( container )  ;", "}", "METHOD_END"], "methodName": ["createDefaultContainer"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" delete \"  )  ;", "return   perform (  \"  \"  ,    pathToURI ( path )  ,    new    . DeleteMethodProcessor < Boolean >  (  )     {", "@ Override", "public   Boolean   extractResult ( DeleteMethod   method )    throws   IOException    {", "return    ( method . getStatusCode (  )  )     =  =     ( SC _ NO _ CONTENT )  ;", "}", "@ Override", "protected   void   setup ( DeleteMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   perform (  \"  \"  ,    uri ,    new   SwiftRestClient . GetMethodProcessor < HttpBodyContent >  (  )     {", "@ Override", "public   HttpBodyContent   extractResult ( GetMethod   method )    throws   IOException    {", "return   new   HttpBodyContent ( new   HttpInputStreamWithRelease ( uri ,    method )  ,    method . getResponseContentLength (  )  )  ;", "}", "@ Override", "protected   void   setup ( GetMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["doGet"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    ( url . matches (  \"  .  *  \\  \\ s +  .  *  \"  )  )     {", "try    {", "url    =    URLEncoder . encode ( url ,     \" UTF -  8  \"  )  ;", "url    =    url . replace (  \"  +  \"  ,     \"  %  2  0  \"  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "throw   new   Exception (  \" failed   to   encode   URI \"  ,    e )  ;", "}", "}", "return   url ;", "}", "METHOD_END"], "methodName": ["encodeUrl"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "final   HttpClient   client    =    new   HttpClient (  )  ;", "if    (  ( proxyHost )     !  =    null )     {", "client . getParams (  )  . setParameter ( DEFAULT _ PROXY ,    new   HttpHost ( proxyHost ,    proxyPort )  )  ;", "}", "int   statusCode    =    execWithDebugOutput ( method ,    client )  ;", "if    (  (  (  ( statusCode    =  =     ( HttpStatus . SC _ UNAUTHORIZED )  )     |  |     ( statusCode    =  =     ( HttpStatus . SC _ BAD _ REQUEST )  )  )     &  &     ( method   instanceof    . AuthPostMethod )  )     &  &     (  !  ( useKeystoneAuthentication )  )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  \" Operation   failed   with   status    \"     +     ( method . getStatusCode (  )  )  )     +     \"    attempting   keystone   auth \"  )  )  ;", "}", "useKeystoneAuthentication    =    true ;", "final    . AuthPostMethod   authentication    =     (  (  . AuthPostMethod )     ( method )  )  ;", "authentication . setRequestEntity ( getAuthenticationRequst ( keystoneAuthRequest )  )  ;", "statusCode    =    execWithDebugOutput ( method ,    client )  ;", "}", "if    ( statusCode    =  =     ( HttpStatus . SC _ UNAUTHORIZED )  )     {", "if    ( method   instanceof    . AuthPostMethod )     {", "throw   new   SwiftAuthenticationFailedException ( authRequest . toString (  )  ,     \" auth \"  ,    authUri ,    method )  ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  \" Reauthenticating \"  )  ;", "}", "authenticate (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  \" Retrying   original   request \"  )  ;", "}", "statusCode    =    execWithDebugOutput ( method ,    client )  ;", "}", "return   statusCode ;", "}", "METHOD_END"], "methodName": ["exec"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftRestClient . LOG . isDebugEnabled (  )  )     {", "StringBuilder   builder    =    new   StringBuilder (  (  (  (  ( method . getName (  )  )     +     \"     \"  )     +     ( method . getURI (  )  )  )     +     \"  \\ n \"  )  )  ;", "for    ( Header   header    :    method . getRequestHeaders (  )  )     {", "builder . append ( header . toString (  )  )  ;", "}", "SwiftRestClient . LOG . debug ( builder )  ;", "}", "int   statusCode    =    client . executeMethod ( method )  ;", "if    ( SwiftRestClient . LOG . isDebugEnabled (  )  )     {", "SwiftRestClient . LOG . debug (  (  \" Status   code    =     \"     +    statusCode )  )  ;", "}", "return   statusCode ;", "}", "METHOD_END"], "methodName": ["execWithDebugOutput"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "URI   uri ;", "preRemoteCommand (  \" findObjects \"  )  ;", "try    {", "uri    =    new   URI ( location )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   SwiftException (  (  \" Bad   URI :     \"     +    location )  ,    e )  ;", "}", "return   perform (  \" findObjects \"  ,    uri ,    new    . GetMethodProcessor < byte [  ]  >  (  )     {", "@ Override", "public   byte [  ]    extractResult ( GetMethod   method )    throws   IOException    {", "if    (  ( method . getStatusCode (  )  )     =  =     ( SC _ NOT _ FOUND )  )     {", "throw   new   FileNotFoundException (  (  \" Not   found    \"     +     ( method . getURI (  )  )  )  )  ;", "}", "return   method . getResponseBody (  )  ;", "}", "@ Override", "protected   int [  ]    getAllowedStatusCodes (  )     {", "return   new   int [  ]  {    SC _ OK ,    SC _ NOT _ FOUND    }  ;", "}", "@ Override", "protected   void   setup ( GetMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["findObjects"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" findObjectsByPrefix \"  )  ;", "URI   uri ;", "String   dataLocationURI    =    getEndpointURI (  )  . toString (  )  ;", "try    {", "String   object    =    path . getObject (  )  ;", "if    ( object . startsWith (  \"  /  \"  )  )     {", "object    =    object . substring (  1  )  ;", "}", "object    =     . encodeUrl ( object )  ;", "dataLocationURI    =    dataLocationURI . concat (  \"  /  \"  )  . concat ( path . getContainer (  )  )  . concat (  \"  /  ? prefix =  \"  )  . concat ( object )  ;", "uri    =    new   URI ( dataLocationURI )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   SwiftException (  (  \" Bad   URI :     \"     +    dataLocationURI )  ,    e )  ;", "}", "return   perform (  \" findObjectsByPrefix \"  ,    uri ,    new    . GetMethodProcessor < byte [  ]  >  (  )     {", "@ Override", "public   byte [  ]    extractResult ( GetMethod   method )    throws   IOException    {", "if    (  ( method . getStatusCode (  )  )     =  =     ( SC _ NOT _ FOUND )  )     {", "throw   new   FileNotFoundException (  (  \" Not   found    \"     +     ( method . getURI (  )  )  )  )  ;", "}", "return   method . getResponseBody (  )  ;", "}", "@ Override", "protected   int [  ]    getAllowedStatusCodes (  )     {", "return   new   int [  ]  {    SC _ OK ,    SC _ NOT _ FOUND    }  ;", "}", "@ Override", "protected   void   setup ( GetMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["findObjectsByPrefix"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "final   String   data    =    JSONUtil . toJSON ( new   AuthenticationRequestWrapper ( authenticationRequest )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Authenticating   with    \"     +    authenticationRequest )  )  ;", "}", "return    . toJsonEntity ( data )  ;", "}", "METHOD_END"], "methodName": ["getAuthenticationRequst"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   blocksizeKB ;", "}", "METHOD_END"], "methodName": ["getBlocksizeKB"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   bufferSizeKB ;", "}", "METHOD_END"], "methodName": ["getBufferSizeKB"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   connectTimeout ;", "}", "METHOD_END"], "methodName": ["getConnectTimeout"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" getContentLength \"  )  ;", "return   perform (  \" getContentLength \"  ,    uri ,    new    . HeadMethodProcessor < Long >  (  )     {", "@ Override", "public   Long   extractResult ( HeadMethod   method )    throws   IOException    {", "return   method . getResponseContentLength (  )  ;", "}", "@ Override", "protected   void   setup ( HeadMethod   method )    throws   IOException    {", "super . setup ( method )  ;", "method . addRequestHeader (  . NEWEST )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getContentLength"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   getContentLength ( pathToURI ( path )  )  ;", "}", "METHOD_END"], "methodName": ["getContentLength"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    ( offset    <     0  )     {", "throw   new   SwiftException (  (  (  (  (  (  (  (  (  \" Invalid   offset :     \"     +    offset )     +     \"    in   getDataAsInputStream (    path =  \"  )     +    path )     +     \"  ,    offset =  \"  )     +    offset )     +     \"  ,    length    =  \"  )     +    length )     +     \"  )  \"  )  )  ;", "}", "if    ( length    <  =     0  )     {", "throw   new   SwiftException (  (  (  (  (  (  (  (  (  \" Invalid   length :     \"     +    length )     +     \"    in   getDataAsInputStream (    path =  \"  )     +    path )     +     \"  ,    offset =  \"  )     +    offset )     +     \"  ,    length    =  \"  )     +    length )     +     \"  )  \"  )  )  ;", "}", "final   String   range    =    String . format ( SwiftProtocolConstants . SWIFT _ RANGE _ HEADER _ FORMAT _ PATTERN ,    offset ,     (  ( offset    +    length )     -     1  )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" getData :  \"     +    range )  )  ;", "}", "return   getData ( path ,    new   Header ( SwiftProtocolConstants . HEADER _ RANGE ,    range )  ,     . NEWEST )  ;", "}", "METHOD_END"], "methodName": ["getData"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" getData \"  )  ;", "return   doGet ( pathToURI ( path )  ,    requHeaders )  ;", "}", "METHOD_END"], "methodName": ["getData"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   endpointURI ;", "}", "METHOD_END"], "methodName": ["getEndpointURI"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   new   SwiftRestClient ( filesystemURI ,    config )  ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isLocationAware (  )  )  )     {", "return   null ;", "}", "preRemoteCommand (  \" getObjectLocation \"  )  ;", "try    {", "return   perform (  \" getObjectLocation \"  ,    pathToObjectLocation ( path )  ,    new    . GetMethodProcessor < byte [  ]  >  (  )     {", "@ Override", "protected   int [  ]    getAllowedStatusCodes (  )     {", "return   new   int [  ]  {    SC _ OK ,    SC _ FORBIDDEN ,    SC _ NO _ CONTENT    }  ;", "}", "@ Override", "public   byte [  ]    extractResult ( GetMethod   method )    throws   IOException    {", "if    (  (  (  (  ( method . getStatusCode (  )  )     =  =     ( SC _ NOT _ FOUND )  )     |  |     (  ( method . getStatusCode (  )  )     =  =     ( SC _ FORBIDDEN )  )  )     |  |     (  ( method . getStatusCode (  )  )     =  =     ( SC _ NO _ CONTENT )  )  )     |  |     (  ( method . getResponseBodyAsStream (  )  )     =  =    null )  )     {", "return   null ;", "}", "final   InputStream   responseBodyAsStream    =    method . getResponseBodyAsStream (  )  ;", "final   byte [  ]    locationData    =    new   byte [  1  0  2  4  ]  ;", "return    ( responseBodyAsStream . read ( locationData )  )     >     0     ?    locationData    :    null ;", "}", "@ Override", "protected   void   setup ( GetMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  (  (  \" Failed   to   get   the   location   of    \"     +    path )     +     \"  :     \"  )     +    e )  ,    e )  ;", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["getObjectLocation"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   objectLocationURI ;", "}", "METHOD_END"], "methodName": ["getObjectLocationURI"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   durationStats . getDurationStatistics (  )  ;", "}", "METHOD_END"], "methodName": ["getOperationStatistics"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "String   val    =    props . getProperty ( key )  ;", "if    ( val    =  =    null )     {", "throw   new   ConfigurationException (  (  \" Undefined   property :     \"     +    key )  )  ;", "}", "return   val ;", "}", "METHOD_END"], "methodName": ["getOption"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   partSizeKB ;", "}", "METHOD_END"], "methodName": ["getPartSizeKB"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   proxyHost ;", "}", "METHOD_END"], "methodName": ["getProxyHost"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   proxyPort ;", "}", "METHOD_END"], "methodName": ["getProxyPort"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   region ;", "}", "METHOD_END"], "methodName": ["getRegion"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   retryCount ;", "}", "METHOD_END"], "methodName": ["getRetryCount"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   tenant ;", "}", "METHOD_END"], "methodName": ["getTenant"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   throttleDelay ;", "}", "METHOD_END"], "methodName": ["getThrottleDelay"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   token ;", "}", "METHOD_END"], "methodName": ["getToken"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   username ;", "}", "METHOD_END"], "methodName": ["getUsername"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  (  \" headRequest :     \"     +    reason )  )  ;", "return   perform ( reason ,    pathToURI ( path )  ,    new    . HeadMethodProcessor < Header [  ]  >  (  )     {", "@ Override", "public   Header [  ]    extractResult ( HeadMethod   method )    throws   IOException    {", "if    (  ( method . getStatusCode (  )  )     =  =     ( SC _ NOT _ FOUND )  )     {", "throw   new   FileNotFoundException (  (  \" Not   Found    \"     +     ( method . getURI (  )  )  )  )  ;", "}", "return   method . getResponseHeaders (  )  ;", "}", "@ Override", "protected   void   setup ( HeadMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["headRequest"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   locationAware ;", "}", "METHOD_END"], "methodName": ["isLocationAware"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "for    ( int   code    :    expected )     {", "if    ( status    =  =    code )     {", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isStatusCodeExpected"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   usePublicURL ;", "}", "METHOD_END"], "methodName": ["isUsePublicURL"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" listDeepObjectsInDirectory \"  )  ;", "String   endpoint    =    getEndpointURI (  )  . toString (  )  ;", "StringBuilder   dataLocationURI    =    new   StringBuilder (  )  ;", "dataLocationURIpend ( endpoint )  ;", "String   object    =    path . getObject (  )  ;", "if    ( object . startsWith (  \"  /  \"  )  )     {", "object    =    object . substring (  1  )  ;", "}", "if    (  !  ( object . endsWith (  \"  /  \"  )  )  )     {", "object    =    object . concat (  \"  /  \"  )  ;", "}", "if    ( object . equals (  \"  /  \"  )  )     {", "object    =     \"  \"  ;", "}", "dataLocationURI    =    dataLocationURIpend (  \"  /  \"  ) pend ( path . getContainer (  )  ) pend (  \"  /  ? prefix =  \"  ) pend ( object ) pend (  \"  & format = json \"  )  ;", "if    ( listDeep    =  =    false )     {", "dataLocationURIpend (  \"  & delimiter =  /  \"  )  ;", "}", "return   findObjects ( dataLocationURI . toString (  )  ,    requestHeaders )  ;", "}", "METHOD_END"], "methodName": ["listDeepObjectsInDirectory"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "URI   uri ;", "String   dataLocationURI    =    objectLocationURI . toString (  )  ;", "try    {", "if    ( path . toString (  )  . startsWith (  \"  /  \"  )  )     {", "dataLocationURI    =    dataLocationURI . concat ( path . toUriPath (  )  )  ;", "} else    {", "dataLocationURI    =    dataLocationURI . concat (  \"  /  \"  )  . concat ( path . toUriPath (  )  )  ;", "}", "uri    =    new   URI ( dataLocationURI )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   Exception ( e )  ;", "}", "return   uri ;", "}", "METHOD_END"], "methodName": ["pathToObjectLocation"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   SwiftRestClient . pathToURI ( path ,    getEndpointURI (  )  )  ;", "}", "METHOD_END"], "methodName": ["pathToURI"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "SwiftRestClient . checkNotNull ( endpointURI ,     \" Null   Endpoint    - client   is   not   authenticated \"  )  ;", "String   dataLocationURI    =    endpointURI . toString (  )  ;", "try    {", "dataLocationURI    =    SwiftUtils . joinPaths ( dataLocationURI ,    SwiftRestClient . encodeUrl ( path . toUriPath (  )  )  )  ;", "return   new   URI ( dataLocationURI )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   SwiftException (  (  \" Failed   to   create   URI   from    \"     +    dataLocationURI )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["pathToURI"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "SwiftRestClient . checkNotNull ( uri )  ;", "SwiftRestClient . checkNotNull ( processor )  ;", "final   M   method    =    processor . createMethod ( uri . toString (  )  )  ;", "HttpMethodParams   methodParams    =    method . getParams (  )  ;", "methodParams . setParameter ( RETRY _ HANDLER ,    new   DefaultHttpMethodRetryHandler ( retryCount ,    false )  )  ;", "methodParams . setIntParameter ( CONNECTION _ TIMEOUT ,    connectTimeout )  ;", "methodParams . setSoTimeout ( socketTimeout )  ;", "method . addRequestHeader ( SwiftProtocolConstants . HEADER _ USER _ AGENT ,    SwiftProtocolConstants . SWIFT _ USER _ AGENT )  ;", "Duration   duration    =    new   Duration (  )  ;", "boolean   success    =    false ;", "try    {", "int   statusCode    =     0  ;", "try    {", "statusCode    =    exec ( method )  ;", "}    catch    ( IOException   e )     {", "throw   ExceptionDiags . wrapException ( uri . toString (  )  ,    method . getName (  )  ,    e )  ;", "}", "int [  ]    allowedStatusCodes    =    processor . getAllowedStatusCodes (  )  ;", "boolean   validResponse    =    isStatusCodeExpected ( statusCode ,    allowedStatusCodes )  ;", "if    (  ! validResponse )     {", "IOException   ioe    =    buildException ( uri ,    method ,    statusCode )  ;", "throw   ioe ;", "}", "R   r    =    processor . extractResult ( method )  ;", "success    =    true ;", "return   r ;", "}    catch    ( IOException   e )     {", "method . releaseConnection (  )  ;", "throw   e ;", "}    finally    {", "duration . finished (  )  ;", "durationStats . add (  (  (  ( method . getName (  )  )     +     \"     \"  )     +    reason )  ,    duration ,    success )  ;", "}", "}", "METHOD_END"], "methodName": ["perform"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   perform (  \"  \"  ,    uri ,    processor )  ;", "}", "METHOD_END"], "methodName": ["perform"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftRestClient . LOG . isTraceEnabled (  )  )     {", "SwiftRestClient . LOG . trace (  (  \" Executing    \"     +    operation )  )  ;", "}", "authIfNeeded (  )  ;", "}", "METHOD_END"], "methodName": ["preRemoteCommand"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" putRequest \"  )  ;", "return   perform ( pathToURI ( path )  ,    new    . PutMethodProcessor < Integer >  (  )     {", "@ Override", "public   Integer   extractResult ( PutMethod   method )    throws   IOException    {", "return   method . getStatusCode (  )  ;", "}", "@ Override", "protected   void   setup ( PutMethod   method )    throws   SwiftInternalStateException    {", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["putRequest"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftRestClient . LOG . isDebugEnabled (  )  )     {", "SwiftRestClient . LOG . debug ( String . format (  \" setAuth :    endpoint =  % s ;    objectURI =  % s ;    token =  % s \"  ,    endpoint ,    objectLocation ,    authToken )  )  ;", "}", "synchronized ( this )     {", "endpointURI    =    endpoint ;", "objectLocationURI    =    objectLocation ;", "token    =    authToken ;", "}", "}", "METHOD_END"], "methodName": ["setAuthDetails"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "SwiftRestClient . checkNotNull ( accessToken ,     \" Not   authenticated \"  )  ;", "method . addRequestHeader ( SwiftProtocolConstants . HEADER _ AUTH _ KEY ,    accessToken . getId (  )  )  ;", "}", "METHOD_END"], "methodName": ["setAuthToken"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "for    ( Header   header    :    requestHeaders )     {", "method . addRequestHeader ( header )  ;", "}", "setAuthToken ( method ,    getToken (  )  )  ;", "}", "METHOD_END"], "methodName": ["setHeaders"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "StringRequestEntity   entity ;", "try    {", "entity    =    new   StringRequestEntity ( data ,     \" application / json \"  ,     \" UTF -  8  \"  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "throw   new   Exception (  \" Could   not   encode   data   as   UTF -  8  \"  ,    e )  ;", "}", "return   entity ;", "}", "METHOD_END"], "methodName": ["toJsonEntity"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "preRemoteCommand (  \" upload \"  )  ;", "try    {", "perform (  \" upload \"  ,    pathToURI ( path )  ,    new    . PutMethodProcessor < byte [  ]  >  (  )     {", "@ Override", "public   byte [  ]    extractResult ( PutMethod   method )    throws   IOException    {", "return   method . getResponseBody (  )  ;", "}", "@ Override", "protected   void   setup ( PutMethod   method )    throws   SwiftInternalStateException    {", "method . setRequestEntity ( new   InputStreamRequestEntity ( data ,    length )  )  ;", "setHeaders ( method ,    requestHeaders )  ;", "}", "}  )  ;", "}    finally    {", "data . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["upload"], "fileName": "org.apache.hadoop.fs.swift.http.SwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "try    {", "Properties   binding    =     . bind ( fsURI ,    config )  ;", "StringBuilder   details    =    new   StringBuilder (  )  ;", "for    ( Object   key    :    binding . keySet (  )  )     {", "details . append ( key . toString (  )  )  . append (  \"     \"  )  ;", "}", "fail (  (  (  \" Expected   a   failure ,    got   the   binding    [     \"     +    details )     +     \"  ]  \"  )  )  ;", "}    catch    ( SwiftConfigurationException   expected )     {", "}", "}", "METHOD_END"], "methodName": ["expectBindingFailure"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "try    {", "String   container    =     . extractContainerName ( hostname )  ;", "fail (  (  (  (  \" Expected   an   error    - got   a   container   of    '  \"     +    container )     +     \"  '    from    \"  )     +    hostname )  )  ;", "}    catch    ( SwiftConfigurationException   expected )     {", "}", "}", "METHOD_END"], "methodName": ["expectExtractContainerFail"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "try    {", "String   service    =     . extractServiceName ( hostname )  ;", "fail (  (  (  (  \" Expected   an   error    - got   a   service   of    '  \"     +    service )     +     \"  '    from    \"  )     +    hostname )  )  ;", "}    catch    ( SwiftConfigurationException   expected )     {", "}", "}", "METHOD_END"], "methodName": ["expectExtractServiceFail"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "String   instance    =    RestClientBindings . buildSwiftInstancePrefix ( host )  ;", "String   confkey    =    instance    +    key ;", "conf . set ( confkey ,    val )  ;", "}", "METHOD_END"], "methodName": ["setInstanceVal"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "filesysURI    =    new   URI ( TestRestClientBindings . FS _ URI )  ;", "conf    =    new   Configuration ( true )  ;", "setInstanceVal ( conf ,    TestRestClientBindings . SERVICE ,    SwiftProtocolConstants . DOT _ AUTH _ URL ,    TestRestClientBindings . AUTH _ URL )  ;", "setInstanceVal ( conf ,    TestRestClientBindings . SERVICE ,    SwiftProtocolConstants . DOT _ USERNAME ,    TestRestClientBindings . USER )  ;", "setInstanceVal ( conf ,    TestRestClientBindings . SERVICE ,    SwiftProtocolConstants . DOT _ PASSWORD ,    TestRestClientBindings . PASS )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "Properties   props    =    RestClientBindings . bind ( filesysURI ,    conf )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ CONTAINER _ PROPERTY ,     . CONTAINER )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ SERVICE _ PROPERTY ,     . SERVICE )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ AUTH _ PROPERTY ,     . AUTH _ URL )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ AUTH _ PROPERTY ,     . AUTH _ URL )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ USERNAME _ PROPERTY ,     . USER )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ PASSWORD _ PROPERTY ,     . PASS )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ TENANT _ PROPERTY ,    null )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ REGION _ PROPERTY ,    null )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ HTTP _ PORT _ PROPERTY ,    null )  ;", "SwiftTestUtils . assertPropertyEquals ( props ,    SwiftProtocolConstants . SWIFT _ HTTPS _ PORT _ PROPERTY ,    null )  ;", "}", "METHOD_END"], "methodName": ["testBindAgainstConf"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "Configuration   badConf    =    new   Configuration (  )  ;", "expecFailure ( filesysURI ,    badConf )  ;", "}", "METHOD_END"], "methodName": ["testBindAgainstConfMissingInstance"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "TestRestClientBindings . expectExtractContainerFail (  \"  .  \"  )  ;", "TestRestClientBindings . expectExtractServiceFail (  \"  .  \"  )  ;", "}", "METHOD_END"], "methodName": ["testDot"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "RestClientBindings . bind ( new   URI (  \" swift :  /  / hadoop . apache . org /  \"  )  ,    conf )  ;", "}", "METHOD_END"], "methodName": ["testDottedServiceURL"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "TestRestClientBindings . expectExtractContainerFail (  \"  \"  )  ;", "TestRestClientBindings . expectExtractServiceFail (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyHostname"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "TestRestClientBindings . expectExtractServiceFail (  \"  . leading \"  )  ;", "}", "METHOD_END"], "methodName": ["testLeadingDot"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "RestClientBindings . bind ( new   URI (  \" swift :  /  /  /  \"  )  ,    conf )  ;", "}", "METHOD_END"], "methodName": ["testMissingServiceURL"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "String   built    =    RestClientBindings . buildSwiftInstancePrefix ( TestRestClientBindings . SERVICE )  ;", "assertEquals (  (  \" fs . swift . service .  \"     +     ( TestRestClientBindings . SERVICE )  )  ,    built )  ;", "}", "METHOD_END"], "methodName": ["testPrefixBuilder"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "TestRestClientBindings . expectExtractContainerFail (  \" simple \"  )  ;", "TestRestClientBindings . expectExtractServiceFail (  \" simple \"  )  ;", "}", "METHOD_END"], "methodName": ["testSimple"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "TestRestClientBindings . expectExtractServiceFail (  \" simple .  \"  )  ;", "}", "METHOD_END"], "methodName": ["testTrailingDot"], "fileName": "org.apache.hadoop.fs.swift.http.TestRestClientBindings"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeTrue ( runTests )  ;", "}", "METHOD_END"], "methodName": ["assumeEnabled"], "fileName": "org.apache.hadoop.fs.swift.http.TestSwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   SwiftRestClient . getInstance ( serviceURI ,    conf )  ;", "}", "METHOD_END"], "methodName": ["createClient"], "fileName": "org.apache.hadoop.fs.swift.http.TestSwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   Configuration (  )  ;", "runTests    =    TestUtils . hasServiceURI ( conf )  ;", "if    ( runTests )     {", "serviceURI    =    TestUtils . getServiceURI ( conf )  ;", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.fs.swift.http.TestSwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "assumeEnabled (  )  ;", "client    =    createClient (  )  ;", "client . authenticate (  )  ;", "}", "METHOD_END"], "methodName": ["testAuthenticate"], "fileName": "org.apache.hadoop.fs.swift.http.TestSwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "assumeEnabled (  )  ;", "client    =    createClient (  )  ;", "}", "METHOD_END"], "methodName": ["testCreate"], "fileName": "org.apache.hadoop.fs.swift.http.TestSwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "assumeEnabled (  )  ;", "SwiftRestClient   client    =    createClient (  )  ;", "client . authenticate (  )  ;", "Path   path    =    new   Path (  \" restTestPutAndDelete \"  )  ;", "SwiftObjectPath   sobject    =    SwiftObjectPath . fromPath ( serviceURI ,    path )  ;", "byte [  ]    stuff    =    new   byte [  1  ]  ;", "stuff [  0  ]     =     ' a '  ;", "client . upload ( sobject ,    new   ByteArrayInputStream ( stuff )  ,    stuff . length )  ;", "Duration   head    =    new   Duration (  )  ;", "Header [  ]    responseHeaders    =    client . headRequest (  \" expect   success \"  ,    sobject ,    SwiftRestClient . NEWEST )  ;", "head . finished (  )  ;", ". LOG . info (  (  \" head   request   duration    \"     +    head )  )  ;", "for    ( Header   header    :    responseHeaders )     {", ". LOG . info ( header . toString (  )  )  ;", "}", "client . delete ( sobject )  ;", "try    {", "Header [  ]    headers    =    client . headRequest (  \" expect   fail \"  ,    sobject ,    SwiftRestClient . NEWEST )  ;", "Assert . fail (  (  \" Expected   deleted   file ,    but   object   is   still   present :     \"     +    sobject )  )  ;", "}    catch    ( FileNotFoundException   e )     {", "}", "for    ( DurationStats   stats    :    client . getOperationStatistics (  )  )     {", ". LOG . info ( stats )  ;", "}", "}", "METHOD_END"], "methodName": ["testPutAndDelete"], "fileName": "org.apache.hadoop.fs.swift.http.TestSwiftRestClient"}, {"methodBody": ["METHOD_START", "{", "return   getConf (  )  . getLong ( SwiftScaleTestBase . KEY _ OPERATION _ COUNT ,    SwiftScaleTestBase . DEFAULT _ OPERATION _ COUNT )  ;", "}", "METHOD_END"], "methodName": ["getOperationCount"], "fileName": "org.apache.hadoop.fs.swift.scale.SwiftScaleTestBase"}, {"methodBody": ["METHOD_START", "{", "Path   dir    =    new   Path (  \"  / test / manysmallfiles \"  )  ;", "Duration   rm 1     =    new   Duration (  )  ;", "fs . delete ( dir ,    true )  ;", "rm 1  . finished (  )  ;", "fs . mkdirs ( dir )  ;", "Duration   ls 1     =    new   Duration (  )  ;", "fs . listStatus ( dir )  ;", "ls 1  . finished (  )  ;", "long   count    =    getOperationCount (  )  ;", "SwiftTestUtils . noteAction (  (  (  \" Beginning   Write   of    \"     +    count )     +     \"    files    \"  )  )  ;", "DurationStats   writeStats    =    new   DurationStats (  \" write \"  )  ;", "DurationStats   readStats    =    new   DurationStats (  \" read \"  )  ;", "String   format    =     \"  %  0  8 d \"  ;", "for    ( long   l    =     0  ;    l    <    count ;    l +  +  )     {", "String   name    =    String . format ( format ,    l )  ;", "Path   p    =    new   Path ( dir ,     (  \" part -  \"     +    name )  )  ;", "Duration   d    =    new   Duration (  )  ;", "SwiftTestUtils . writeTextFile ( fs ,    p ,    name ,    false )  ;", "d . finished (  )  ;", "writeStats . add ( d )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "SwiftTestUtils . noteAction (  \" Beginning   ls \"  )  ;", "Duration   ls 2     =    new   Duration (  )  ;", "FileStatus [  ]    status 2     =     (  ( FileStatus [  ]  )     ( fs . listStatus ( dir )  )  )  ;", "ls 2  . finished (  )  ;", "assertEquals (  \" Not   enough   entries   in   the   directory \"  ,    count ,    status 2  . length )  ;", "SwiftTestUtils . noteAction (  \" Beginning   read \"  )  ;", "for    ( long   l    =     0  ;    l    <    count ;    l +  +  )     {", "String   name    =    String . format ( format ,    l )  ;", "Path   p    =    new   Path ( dir ,     (  \" part -  \"     +    name )  )  ;", "Duration   d    =    new   Duration (  )  ;", "String   result    =    SwiftTestUtils . readBytesToString ( fs ,    p ,    name . length (  )  )  ;", "assertEquals ( name ,    result )  ;", "d . finished (  )  ;", "readStats . add ( d )  ;", "}", "SwiftTestUtils . noteAction (  \" Beginning   delete \"  )  ;", "Duration   rm 2     =    new   Duration (  )  ;", "fs . delete ( dir ,    true )  ;", "rm 2  . finished (  )  ;", ". LOG . info ( String . format (  \"  ' filesystem '  ,  '  % s '  \"  ,    fs . getUri (  )  )  )  ;", ". LOG . info ( writeStats . toString (  )  )  ;", ". LOG . info ( readStats . toString (  )  )  ;", ". LOG . info ( String . format (  \"  ' rm 1  '  ,  % d ,  ' ls 1  '  ,  % d \"  ,    rm 1  . value (  )  ,    ls 1  . value (  )  )  )  ;", ". LOG . info ( String . format (  \"  ' rm 2  '  ,  % d ,  ' ls 2  '  ,  % d \"  ,    rm 2  . value (  )  ,    ls 2  . value (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testScaledWriteThenRead"], "fileName": "org.apache.hadoop.fs.swift.scale.TestWriteManySmallFiles"}, {"methodBody": ["METHOD_START", "{", "return   isDir (  )  ;", "}", "METHOD_END"], "methodName": ["isDirectory"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftFileStatus"}, {"methodBody": ["METHOD_START", "{", "long   partSizeKB    =    getStore (  )  . getPartsizeKB (  )  ;", "return   new   OutputStream ( getConf (  )  ,    getStore (  )  ,    path . toUri (  )  . toString (  )  ,    partSizeKB )  ;", "}", "METHOD_END"], "methodName": ["createSwiftOutputStream"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftNativeFileSystem . LOG . isDebugEnabled (  )  )     {", "SwiftNativeFileSystem . LOG . debug (  (  (  \" Making   dir    '  \"     +    absolutePath )     +     \"  '    in   Swift \"  )  )  ;", "}", "store . createDirectory ( absolutePath )  ;", "}", "METHOD_END"], "methodName": ["forceMkdir"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "SwiftNativeOutputStream   snos    =    SwiftNativeFileSystem . getSwiftNativeOutputStream ( outputStream )  ;", "return   snos . getBytesUploaded (  )  ;", "}", "METHOD_END"], "methodName": ["getBytesUploaded"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "SwiftNativeOutputStream   snos    =    SwiftNativeFileSystem . getSwiftNativeOutputStream ( outputStream )  ;", "return   snos . getBytesWritten (  )  ;", "}", "METHOD_END"], "methodName": ["getBytesWritten"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   store . getOperationStatistics (  )  ;", "}", "METHOD_END"], "methodName": ["getOperationStatistics"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "SwiftNativeOutputStream   snos    =    SwiftNativeFileSystem . getSwiftNativeOutputStream ( outputStream )  ;", "return   snos . getFilePartSize (  )  ;", "}", "METHOD_END"], "methodName": ["getPartitionSize"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "SwiftNativeOutputStream   snos    =    SwiftNativeFileSystem . getSwiftNativeOutputStream ( outputStream )  ;", "return   snos . getPartitionsWritten (  )  ;", "}", "METHOD_END"], "methodName": ["getPartitionsWritten"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   store ;", "}", "METHOD_END"], "methodName": ["getStore"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "OutputStream   wrappedStream    =    outputStream . getWrappedStream (  )  ;", "return    (  ( OutputStream )     ( wrappedStream )  )  ;", "}", "METHOD_END"], "methodName": ["getSwiftNativeOutputStream"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "return    !  ( isRoot ( absolutePath )  )  ;", "}", "METHOD_END"], "methodName": ["isNotRoot"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "return    ( absolutePath . getParent (  )  )     =  =    null ;", "}", "METHOD_END"], "methodName": ["isRoot"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "return   store . listSubPaths ( makeAbsolute ( path )  ,    true ,    newest )  ;", "}", "METHOD_END"], "methodName": ["listRawFileStatus"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( path . isAbsolute (  )  )     {", "return   path ;", "}", "return   new   Path ( workingDir ,    path )  ;", "}", "METHOD_END"], "methodName": ["makeAbsolute"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "Path   directory    =    makeAbsolute ( path )  ;", "boolean   shouldCreate    =    shouldCreate ( directory )  ;", "if    ( shouldCreate )     {", "forceMkdir ( directory )  ;", "}", "return   shouldCreate ;", "}", "METHOD_END"], "methodName": ["mkdir"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "if    ( readBlockSize    <  =     0  )     {", "throw   new   SwiftConfigurationException (  \" Bad   remote   buffer   size \"  )  ;", "}", "Path   absolutePath    =    makeAbsolute ( path )  ;", "return   new   FSDataInputStream ( new   StrictBufferedFSInputStream ( new   InputStream ( store ,    statistics ,    absolutePath ,    readBlockSize )  ,    bufferSize )  )  ;", "}", "METHOD_END"], "methodName": ["open"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "FileStatus   fileStatus ;", "boolean   shouldCreate ;", "if    ( isRoot ( directory )  )     {", "return   false ;", "}", "try    {", "fileStatus    =    getFileStatus ( directory )  ;", "if    (  !  ( SwiftUtils . isDirectory ( fileStatus )  )  )     {", "throw   new   ParentNotDirectoryException ( String . format (  \"  % s :    can ' t   mkdir   since   it   exists   and   is   not   a   directory :     % s \"  ,    directory ,    fileStatus )  )  ;", "} else    {", "if    ( SwiftNativeFileSystem . LOG . isDebugEnabled (  )  )     {", "SwiftNativeFileSystem . LOG . debug (  (  (  \" skipping   mkdir (  \"     +    directory )     +     \"  )    as   it   exists   already \"  )  )  ;", "}", "shouldCreate    =    false ;", "}", "}    catch    ( FileNotFoundException   e )     {", "shouldCreate    =    true ;", "}", "return   shouldCreate ;", "}", "METHOD_END"], "methodName": ["shouldCreate"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   srcObject    =    toObjectPath ( srcKey )  ;", "SwiftObjectPath   destObject    =    toObjectPath ( dstKey )  ;", "swiftRestClient . copyObject ( srcObject ,    destObject )  ;", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    ( srcObject . isEqualToOrParentOf ( destObject )  )     {", "throw   new   Exception (  (  (  (  \" Can ' t   copy    \"     +    srcObject )     +     \"    onto    \"  )     +    destObject )  )  ;", "}", "boolean   copySucceeded    =    swiftRestClient . copyObject ( srcObject ,    destObject )  ;", "if    (  ! copySucceeded )     {", "throw   new   Exception (  (  (  (  (  \" Copy   of    \"     +    srcObject )     +     \"    to    \"  )     +    destObject )     +     \" failed \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["copyObject"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "copyObject ( srcObject ,    destObject )  ;", "RestClient . delete ( srcObject )  ;", "}", "METHOD_END"], "methodName": ["copyThenDeleteObject"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "innerCreateDirectory ( toDirPath ( path )  )  ;", "}", "METHOD_END"], "methodName": ["createDirectory"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   pathString    =    toObjectPath ( path )  . toString (  )  ;", "if    (  !  ( pathString . endsWith (  \"  /  \"  )  )  )     {", "pathString    =    pathString . concat (  \"  /  \"  )  ;", "}", "if    ( pathString . startsWith (  \"  /  \"  )  )     {", "pathString    =    pathString . substring (  1  )  ;", "}", "RestClient . upload ( toObjectPath ( path )  ,    new   ByteArrayInputStream ( new   byte [  0  ]  )  ,     0  ,    new   Header ( SwiftProtocolConstants . X _ OBJECT _ MANIFEST ,    pathString )  )  ;", "}", "METHOD_END"], "methodName": ["createManifestForPartUpload"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "Path   swiftPath    =    getCorrectSwiftPath ( absolutePath )  ;", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Deleting   path    '  % s '    recursive =  % b \"  ,    absolutePath ,    recursive )  ;", "boolean   askForNewest    =    true ;", "SwiftFileStatus   fileStatus    =    getObjectMetadata ( swiftPath ,    askForNewest )  ;", "FileStatus [  ]    statuses    =    listSubPaths ( absolutePath ,    true ,    askForNewest )  ;", "if    ( statuses    =  =    null )     {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Path    '  % s '    has   no   status    - it   has    ' gone   away '  \"  ,    absolutePath ,    recursive )  ;", "return   false ;", "}", "int   filecount    =    statuses . length ;", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Path    '  % s '     % d   status   entries '  \"  ,    absolutePath ,    filecount )  ;", "if    ( filecount    =  =     0  )     {", "rmdir ( absolutePath )  ;", "return   true ;", "}", "if    ( SwiftNativeFileSystemStore . LOG . isDebugEnabled (  )  )     {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \"  % s \"  ,    SwiftUtils . fileStatsToString ( statuses ,     \"  \\ n \"  )  )  ;", "}", "if    (  ( filecount    =  =     1  )     &  &     ( swiftPath . equals ( statuses [  0  ]  . getPath (  )  )  )  )     {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Deleting   simple   file    % s \"  ,    absolutePath )  ;", "deleteObject ( absolutePath )  ;", "return   true ;", "}", "if    (  !  ( fileStatus . isDir (  )  )  )     {", "SwiftNativeFileSystemStore . LOG . debug (  \" Multiple   child   entries   but   entry   has   data :    assume   partitioned \"  )  ;", "} else", "if    (  ! recursive )     {", "throw   new   exceptions . SwiftOperationFailedException (  (  (  (  \" Directory    \"     +    fileStatus )     +     \"    is   not   empty :     \"  )     +     ( SwiftUtils . fileStatsToString ( statuses ,     \"  ;     \"  )  )  )  )  ;", "}", "for    ( FileStatus   entryStatus    :    statuses )     {", "Path   entryPath    =    entryStatus . getPath (  )  ;", "try    {", "boolean   deleted    =    deleteObject ( entryPath )  ;", "if    (  ! deleted )     {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Failed   to   delete   entry    '  % s '  ;    continuing \"  ,    entryPath )  ;", "}", "}    catch    ( FileNotFoundException   e )     {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Path    '  % s '    is   no   longer   present ;    continuing \"  ,    entryPath )  ;", "}", "throttle (  )  ;", "}", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Deleting   base   entry    % s \"  ,    absolutePath )  ;", "deleteObject ( absolutePath )  ;", "return   true ;", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   swiftObjectPath    =    toObjectPath ( path )  ;", "if    (  !  ( SwiftUtils . isRootDir ( swiftObjectPath )  )  )     {", "return   swiftRestClient . delete ( swiftObjectPath )  ;", "} else    {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  \" Not   deleting   root   directory   entry \"  )  ;", "}", "return   true ;", "}", "}", "METHOD_END"], "methodName": ["deleteObject"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "final   Matcher   matcher    =    SwiftNativeFileSystemStore . URI _ PATTERN . matcher ( json )  ;", "final   List < URI >    result    =    new   ArrayList < URI >  (  )  ;", "while    ( matcher . find (  )  )     {", "final   String   s    =    matcher . group (  )  ;", "final   String   uri    =    s . substring (  1  ,     (  ( s . length (  )  )     -     1  )  )  ;", "try    {", "URI   createdUri    =    URI . create ( uri )  ;", "result . add ( createdUri )  ;", "}    catch    ( IllegalArgumentException   e )     {", "throw   new   SwiftOperationFailedException ( String . format (  (  \" could   not   convert    \\  \"  % s \\  \"    into   a   URI .  \"     +     (  \"    source :     % s    \"     +     \"    first   JSON :     % s \"  )  )  ,    uri ,    path ,    json . substring (  0  ,     2  5  6  )  )  )  ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["extractUris"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return    1  0  2  4 L    *     ( swiftRestClient . getBlocksizeKB (  )  )  ;", "}", "METHOD_END"], "methodName": ["getBlocksize"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   swiftRestClient . getBufferSizeKB (  )  ;", "}", "METHOD_END"], "methodName": ["getBufferSizeKB"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   URI   fullUri    =    new   URI ( uri . getScheme (  )  ,    uri . getAuthority (  )  ,    path . toUri (  )  . getPath (  )  ,    null ,    null )  ;", "return   new   Path ( fullUri )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   Exception (  (  (  \" Specified   path    \"     +    path )     +     \"    is   incorrect \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getCorrectSwiftPath"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   URI   fullUri    =    new   URI ( uri . getScheme (  )  ,    uri . getAuthority (  )  ,    path . getObject (  )  ,    null ,    null )  ;", "return   new   Path ( fullUri )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   Exception (  (  (  \" Specified   path    \"     +    path )     +     \"    is   incorrect \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getCorrectSwiftPath"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   swiftRestClient . getData ( toObjectPath ( path )  ,    SwiftRestClient . NEWEST )  ;", "}", "METHOD_END"], "methodName": ["getObject"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   swiftRestClient . getData ( toObjectPath ( path )  ,    byteRangeStart ,    length )  ;", "}", "METHOD_END"], "methodName": ["getObject"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   objectPath    =    toObjectPath ( path )  ;", "return   stat ( objectPath ,    newest )  ;", "}", "METHOD_END"], "methodName": ["getObjectHeaders"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "final   byte [  ]    objectLocation ;", "objectLocation    =    swiftRestClient . getObjectLocation ( toObjectPath ( path )  )  ;", "if    (  ( objectLocation    =  =    null )     |  |     (  ( objectLocation . length )     =  =     0  )  )     {", "return   new   LinkedList < URI >  (  )  ;", "}", "return    . extractUris ( new   String ( objectLocation )  ,    path )  ;", "}", "METHOD_END"], "methodName": ["getObjectLocation"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   getObjectMetadata ( path ,    true )  ;", "}", "METHOD_END"], "methodName": ["getObjectMetadata"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "SwiftObjectPath   objectPath    =    toObjectPath ( path )  ;", "final   Header [  ]    headers    =    stat ( objectPath ,    newest )  ;", "if    (  ( headers . length )     =  =     0  )     {", "throw   new   FileNotFoundException (  (  \" Not   Found    \"     +     ( path . toUri (  )  )  )  )  ;", "}", "boolean   isDir    =    false ;", "long   length    =     0  ;", "long   lastModified    =     0  ;", "for    ( Header   header    :    headers )     {", "String   headerName    =    header . getName (  )  ;", "if    (  ( headerName . equals ( SwiftProtocolConstants . X _ CONTAINER _ OBJECT _ COUNT )  )     |  |     ( headerName . equals ( SwiftProtocolConstants . X _ CONTAINER _ BYTES _ USED )  )  )     {", "length    =     0  ;", "isDir    =    true ;", "}", "if    ( SwiftProtocolConstants . HEADER _ CONTENT _ LENGTH . equals ( headerName )  )     {", "length    =    Long . parseLong ( header . getValue (  )  )  ;", "}", "if    ( SwiftProtocolConstants . HEADER _ LAST _ MODIFIED . equals ( headerName )  )     {", "final   SimpleDateFormat   simpleDateFormat    =    new   SimpleDateFormat (  . PATTERN )  ;", "try    {", "lastModified    =    simpleDateFormat . parse ( header . getValue (  )  )  . getTime (  )  ;", "}    catch    ( ParseException   e )     {", "throw   new   SwiftException (  (  \" Failed   to   parse    \"     +     ( header . toString (  )  )  )  ,    e )  ;", "}", "}", "}", "if    ( lastModified    =  =     0  )     {", "lastModified    =    System . currentTimeMillis (  )  ;", "}", "Path   correctSwiftPath    =    getCorrectSwiftPath ( path )  ;", "return   new   SwiftFileStatus ( length ,    isDir ,     1  ,    getBlocksize (  )  ,    lastModified ,    correctSwiftPath )  ;", "}", "METHOD_END"], "methodName": ["getObjectMetadata"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   swiftRestClient . getOperationStatistics (  )  ;", "}", "METHOD_END"], "methodName": ["getOperationStatistics"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   swiftRestClient . getPartSizeKB (  )  ;", "}", "METHOD_END"], "methodName": ["getPartsizeKB"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   swiftRestClient . getThrottleDelay (  )  ;", "}", "METHOD_END"], "methodName": ["getThrottleDelay"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "this . uri    =    fsURI ;", "thiRestClient    =    SwiftRestClient . getInstance ( fsURI ,    configuration )  ;", "}", "METHOD_END"], "methodName": ["initialize"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "swiftRestClient . putRequest ( swiftObjectPath )  ;", "}", "METHOD_END"], "methodName": ["innerCreateDirectory"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "final   byte [  ]    bytes ;", "final   ArrayList < FileStatus >    files    =    new   ArrayList < FileStatus >  (  )  ;", "final   Path   correctSwiftPath    =    getCorrectSwiftPath ( path )  ;", "try    {", "bytes    =    swiftRestClient . listDeepObjectsInDirectory ( path ,    listDeep )  ;", "}    catch    ( FileNotFoundException   e )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  \"  \"     +     \" File / Directory   not   found    \"  )     +    path )  )  ;", "}", "if    ( SwiftUtils . isRootDir ( path )  )     {", "return   Collections . emptyList (  )  ;", "} else    {", "throw   e ;", "}", "}    catch    ( SwiftInvalidResponseException   e )     {", "if    (  ( e . getStatusCode (  )  )     =  =     ( HttpStatus . SC _ NO _ CONTENT )  )     {", "if    ( SwiftUtils . isRootDir ( path )  )     {", "return   Collections . emptyList (  )  ;", "} else    {", "FileStatus   stat    =    getObjectMetadata ( correctSwiftPath ,    newest )  ;", "if    ( stat . isDirectory (  )  )     {", "return   Collections . emptyList (  )  ;", "} else    {", "files . add ( stat )  ;", "return   files ;", "}", "}", "} else    {", "throw   e ;", "}", "}", "final   CollectionType   collectionType    =    JSONUtil . getJsonMapper (  )  . getTypeFactory (  )  . constructCollectionType ( List . class ,    SwiftObjectFileStatus . class )  ;", "final   List < SwiftObjectFileStatus >    fileStatusList    =    JSONUtil . toObject ( new   String ( bytes )  ,    collectionType )  ;", "if    ( fileStatusList . isEmpty (  )  )     {", "SwiftFileStatus   objectMetadata    =    getObjectMetadata ( correctSwiftPath ,    newest )  ;", "if    ( objectMetadata . isFile (  )  )     {", "files . add ( objectMetadata )  ;", "}", "return   files ;", "}", "for    ( SwiftObjectFileStatus   status    :    fileStatusList )     {", "if    (  ( status . getName (  )  )     !  =    null )     {", "files . add ( new   SwiftFileStatus ( status . getBytes (  )  ,     (  ( status . getBytes (  )  )     =  =     0  )  ,     1  ,    getBlocksize (  )  ,    status . getLast _ modified (  )  . getTime (  )  ,    getCorrectSwiftPath ( new   Path ( status . getName (  )  )  )  )  )  ;", "}", "}", "return   files ;", "}", "METHOD_END"], "methodName": ["listDirectory"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "final   Collection < FileStatus >    fileStatuses ;", "fileStatuses    =    listDirectory ( toDirPath ( path )  ,    recursive ,    newest )  ;", "return   fileStatuses . toArray ( new   FileStatus [ fileStatuses . size (  )  ]  )  ;", "}", "METHOD_END"], "methodName": ["listSubPaths"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftNativeFileSystemStore . LOG . isDebugEnabled (  )  )     {", "SwiftNativeFileSystemStore . LOG . debug (  (  ( message    +     \"  :    listing   of    \"  )     +    objectPath )  )  ;", "for    ( FileStatus   fileStatus    :    statuses )     {", "SwiftNativeFileSystemStore . LOG . debug ( fileStatus . getPath (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["logDirectory"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   objectExists ( toObjectPath ( path )  )  ;", "}", "METHOD_END"], "methodName": ["objectExists"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "Header [  ]    headers    =    RestClient . headRequest (  \" objectExists \"  ,    path ,    SwiftRestClient . NEWEST )  ;", "return    ( headers . length )     !  =     0  ;", "}    catch    ( FileNotFoundException   e )     {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["objectExists"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftNativeFileSystemStore . LOG . isDebugEnabled (  )  )     {", "SwiftNativeFileSystemStore . LOG . debug (  (  (  (  \" mv    \"     +    src )     +     \"     \"  )     +    dst )  )  ;", "}", "boolean   renamingOnToSelf    =    src . equals ( dst )  ;", "SwiftObjectPath   srcObject    =    toObjectPath ( src )  ;", "SwiftObjectPath   destObject    =    toObjectPath ( dst )  ;", "if    ( SwiftUtils . isRootDir ( srcObject )  )     {", "throw   new   SwiftOperationFailedException (  \" cannot   rename   root   dir \"  )  ;", "}", "final   SwiftFileStatus   srcMetadata ;", "srcMetadata    =    getObjectMetadata ( src )  ;", "SwiftFileStatus   dstMetadata ;", "try    {", "dstMetadata    =    getObjectMetadata ( dst )  ;", "}    catch    ( FileNotFoundException   e )     {", "SwiftNativeFileSystemStore . LOG . debug (  \" Destination   does   not   exist \"  )  ;", "dstMetadata    =    null ;", "}", "Path   srcParent    =    src . getParent (  )  ;", "Path   dstParent    =    dst . getParent (  )  ;", "if    (  ( dstParent    !  =    null )     &  &     (  !  ( dstParent . equals ( srcParent )  )  )  )     {", "try    {", "getObjectMetadata ( dstParent )  ;", "}    catch    ( FileNotFoundException   e )     {", "SwiftNativeFileSystemStore . LOG . debug (  (  (  \" destination   parent   directory    \"     +    dstParent )     +     \"    doesn ' t   exist \"  )  )  ;", "throw   e ;", "}", "}", "boolean   destExists    =    dstMetadata    !  =    null ;", "boolean   destIsDir    =    destExists    &  &     ( SwiftUtils . isDirectory ( dstMetadata )  )  ;", "SwiftObjectPath   destPath ;", "List < FileStatus >    childStats    =    listDirectory ( srcObject ,    true ,    true )  ;", "boolean   srcIsFile    =     !  ( srcMetadata . isDir (  )  )  ;", "if    ( srcIsFile )     {", "if    ( destExists )     {", "if    ( destIsDir )     {", "destPath    =    toObjectPath ( new   Path ( dst ,    src . getName (  )  )  )  ;", "} else    {", "if    (  ! renamingOnToSelf )     {", "throw   new   FileAlreadyExistsException (  \" cannot   rename   a   file   over   one   that   already   exists \"  )  ;", "} else    {", "SwiftNativeFileSystemStore . LOG . debug (  \" Renaming   file   onto   self :    no - op    =  >    success \"  )  ;", "return ;", "}", "}", "} else    {", "destPath    =    toObjectPath ( dst )  ;", "}", "int   childCount    =    childStats . size (  )  ;", "if    ( childCount    =  =     0  )     {", "copyThenDeleteObject ( srcObject ,    destPath )  ;", "} else    {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     (  \" Source   file   appears   to   be   partitioned .  \"     +     \"    copying   file   and   deleting   children \"  )  )  ;", "copyObject ( srcObject ,    destPath )  ;", "for    ( FileStatus   stat    :    childStats )     {", "SwiftUtils . debug ( SwiftNativeFileSystemStore . LOG ,     \" Deleting   partitioned   file    % s    \"  ,    stat )  ;", "deleteObject ( stat . getPath (  )  )  ;", "}", "swiftRestClient . delete ( srcObject )  ;", "}", "} else    {", "if    ( destExists    &  &     (  ! destIsDir )  )     {", "throw   new   FileAlreadyExistsException (  \" the   source   is   a   directory ,    but   not   the   destination \"  )  ;", "}", "Path   targetPath ;", "if    ( destExists )     {", "targetPath    =    new   Path ( dst ,    src . getName (  )  )  ;", "} else    {", "targetPath    =    dst ;", "}", "SwiftObjectPath   targetObjectPath    =    toObjectPath ( targetPath )  ;", "if    ( srcObject . isEqualToOrParentOf ( targetObjectPath )  )     {", "throw   new   SwiftOperationFailedException (  \" cannot   move   a   directory   under   itself \"  )  ;", "}", "SwiftNativeFileSystemStore . LOG . info (  (  (  (  \" mv       \"     +    srcObject )     +     \"     \"  )     +    targetPath )  )  ;", "logDirectory (  \" Directory   to   copy    \"  ,    srcObject ,    childStats )  ;", "String   srcURI    =    src . toUri (  )  . toString (  )  ;", "int   prefixStripCount    =     ( srcURI . length (  )  )     +     1  ;", "for    ( FileStatus   fileStatus    :    childStats )     {", "Path   copySourcePath    =    fileStatus . getPath (  )  ;", "String   copySourceURI    =    copySourcePath . toUri (  )  . toString (  )  ;", "String   copyDestSubPath    =    copySourceURI . substring ( prefixStripCount )  ;", "Path   copyDestPath    =    new   Path ( targetPath ,    copyDestSubPath )  ;", "if    ( SwiftNativeFileSystemStore . LOG . isTraceEnabled (  )  )     {", "SwiftNativeFileSystemStore . LOG . trace (  (  (  (  (  (  (  (  \" srcURI =  \"     +    srcURI )     +     \"  ;    copySourceURI =  \"  )     +    copySourceURI )     +     \"  ;    copyDestSubPath =  \"  )     +    copyDestSubPath )     +     \"  ;    copyDestPath =  \"  )     +    copyDestPath )  )  ;", "}", "SwiftObjectPath   copyDestination    =    toObjectPath ( copyDestPath )  ;", "try    {", "copyThenDeleteObject ( toObjectPath ( copySourcePath )  ,    copyDestination )  ;", "}    catch    ( FileNotFoundException   e )     {", "SwiftNativeFileSystemStore . LOG . info (  (  \" Skipping   rename   of    \"     +    copySourcePath )  )  ;", "}", "throttle (  )  ;", "}", "if    (  !  ( SwiftUtils . isRootDir ( srcObject )  )  )     {", "try    {", "copyThenDeleteObject ( srcObject ,    targetObjectPath )  ;", "}    catch    ( FileNotFoundException   e )     {", "SwiftNativeFileSystemStore . LOG . warn (  \" Source   directory   deleted   during   rename \"  ,    e )  ;", "innerCreateDirectory ( destObject )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["rename"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   deleteObject ( path )  ;", "}", "METHOD_END"], "methodName": ["rmdir"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "Header [  ]    headers ;", "if    ( newest )     {", "headers    =    RestClient . headRequest (  \" getObjectMetadata - newest \"  ,    objectPath ,    SwiftRestClient . NEWEST )  ;", "} else    {", "headers    =    RestClient . headRequest (  \" getObjectMetadata \"  ,    objectPath )  ;", "}", "return   headers ;", "}", "METHOD_END"], "methodName": ["stat"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "int   throttleDelay    =    getThrottleDelay (  )  ;", "if    ( throttleDelay    >     0  )     {", "try    {", "Thread . sleep ( throttleDelay )  ;", "}    catch    ( InterruptedException   e )     {", "throw    (  ( InterruptedIOException )     ( new   InterruptedIOException ( e . toString (  )  )  . initCause ( e )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["throttle"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   SwiftObjectPath . fromPath ( uri ,    path ,    false )  ;", "}", "METHOD_END"], "methodName": ["toDirPath"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "return   SwiftObjectPath . fromPath ( uri ,    path )  ;", "}", "METHOD_END"], "methodName": ["toObjectPath"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "swiftRestClient . upload ( toObjectPath ( path )  ,    inputStream ,    length )  ;", "}", "METHOD_END"], "methodName": ["uploadFile"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "String   stringPath    =    path . toUri (  )  . toString (  )  ;", "String   partitionFilename    =    Utils . partitionFilenameFromNumber ( partNumber )  ;", "if    ( stringPath . endsWith (  \"  /  \"  )  )     {", "stringPath    =    stringPath . concat ( partitionFilename )  ;", "} else    {", "stringPath    =    stringPath . concat (  \"  /  \"  )  . concat ( partitionFilename )  ;", "}", "swiftRestClient . upload ( new   ObjectPath ( toDirPath ( path )  . getContainer (  )  ,    stringPath )  ,    inputStream ,    length )  ;", "}", "METHOD_END"], "methodName": ["uploadFilePart"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore"}, {"methodBody": ["METHOD_START", "{", "int   count    =     0  ;", "if    (  ( http )     !  =    null )     {", "int   result ;", "for    ( long   i    =     0  ;    i    <    bytes ;    i +  +  )     {", "result    =    http . read (  )  ;", "if    ( result    <     0  )     {", "throw   new   SwiftException (  \" Received   error   code   while   chomping   input \"  )  ;", "}", "count +  +  ;", "incPos (  1  )  ;", "}", "}", "return   count ;", "}", "METHOD_END"], "methodName": ["chompBytes"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "long   length    =    targetPos    +     ( bufferSize )  ;", "SwiftUtils . debug (  . LOG ,     \" Fetching    % d   bytes   starting   at    % d \"  ,    length ,    targetPos )  ;", "HttpBodyContent   blob    =    nativeStore . getObject ( path ,    targetPos ,    length )  ;", "httpStream    =    blob . getInputStream (  )  ;", "updateStartOfBufferPosition ( targetPos ,    blob . getContentLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["fillBuffer"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "pos    +  =    offset ;", "rangeOffset    +  =    offset ;", "SwiftUtils . trace (  . LOG ,     \" Inc :    pos =  % d   bufferOffset =  % d \"  ,    pos ,    rangeOffset )  ;", "}", "METHOD_END"], "methodName": ["incPos"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  ( httpStream )     !  =    null )     {", "reasonClosed    =    reason ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Closing   HTTP   input   stream    :     \"     +    reason )  )  ;", "}", "httpStream . close (  )  ;", "}", "}    finally    {", "httpStream    =    null ;", "}", "}", "METHOD_END"], "methodName": ["innerClose"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "innerClose (  \" reopening   buffer   to   trigger   refresh \"  )  ;", "boolean   success    =    false ;", "try    {", "fillBuffer ( pos )  ;", "success    =    true ;", "}    catch    ( EOFException   eof )     {", "this . sonClosed    =     \" End   of   file \"  ;", "}", "return   success ;", "}", "METHOD_END"], "methodName": ["reopenBuffer"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "pos    =    seekPos ;", "rangeOffset    =     0  ;", "this . contentLength    =    contentLength ;", "SwiftUtils . trace (  . LOG ,     \" Move :    pos =  % d ;    bufferOffset =  % d ;    contentLength =  % d \"  ,    pos ,    rangeOffset ,    contentLength )  ;", "}", "METHOD_END"], "methodName": ["updateStartOfBufferPosition"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "if    (  ( httpStream )     =  =    null )     {", "throw   new   SwiftConnectionClosedException ( reasonClosed )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyOpen"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeInputStream"}, {"methodBody": ["METHOD_START", "{", "if    ( file    !  =    null )     {", "SwiftUtils . debug (  . LOG ,     \" deleting    % s \"  ,    file )  ;", "if    (  !  ( file . delete (  )  )  )     {", ". LOG . warn (  (  \" Could   not   delete    \"     +    file )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "return   bytesUploaded ;", "}", "METHOD_END"], "methodName": ["getBytesUploaded"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "return   bytesWritten ;", "}", "METHOD_END"], "methodName": ["getBytesWritten"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "return   filePartSize ;", "}", "METHOD_END"], "methodName": ["getFilePartSize"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "return    ( partNumber )     -     1  ;", "}", "METHOD_END"], "methodName": ["getPartitionsWritten"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "File   dir    =    new   File ( conf . get (  \" hadoop . tmp . dir \"  )  )  ;", "if    (  (  !  ( dir . mkdirs (  )  )  )     &  &     (  !  ( dir . exists (  )  )  )  )     {", "throw   new   SwiftException (  (  \" Cannot   create   Swift   buffer   directory :     \"     +    dir )  )  ;", "}", "File   result    =    File . createTempFile (  \" output -  \"  ,     \"  . tmp \"  ,    dir )  ;", "result . deleteOnExit (  )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["newBackupFile"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "if    (  ( backupStream )     !  =    null )     {", "backupStream . close (  )  ;", "}", "if    (  ( closingUpload    &  &     ( partUpload )  )     &  &     (  ( backupFile . length (  )  )     =  =     0  )  )     {", "SwiftUtils . debug (  . LOG ,     \" skipping   upload   of    0    byte   final   partition \"  )  ;", "delete ( backupFile )  ;", "} else    {", "partUpload    =    true ;", "boolean   uploadSuccess    =    false ;", "int   attempt    =     0  ;", "while    (  ! uploadSuccess )     {", "try    {", "+  + attempt ;", "bytesUploaded    +  =    uploadFilePartAttempt ( attempt )  ;", "uploadSuccess    =    true ;", "}    catch    ( IOException   e )     {", ". LOG . info (  (  \" Upload   failed    \"     +    e )  ,    e )  ;", "if    ( attempt    >     (  . ATTEMPT _ LIMIT )  )     {", "throw   e ;", "}", "}", "}", "delete ( backupFile )  ;", "( partNumber )  +  +  ;", "blockOffset    =     0  ;", "if    (  ! closingUpload )     {", "backupFile    =    newBackupFile (  )  ;", "backupStream    =    new   BufferedOutputStream ( new   FileOutputStream ( backupFile )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["partUpload"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "long   uploadLen    =    backupFile . length (  )  ;", "SwiftUtils . debug (  . LOG ,     (  \" Closing   write   of   file    % s ;  \"     +     \"    localfile =  % s   of   length    % d    -    attempt    % d \"  )  ,    key ,    backupFile ,    uploadLen ,    attempt )  ;", "nativeStore . uploadFile ( keypath ,    new   FileInputStream ( backupFile )  ,    uploadLen )  ;", "return   uploadLen ;", "}", "METHOD_END"], "methodName": ["uploadFileAttempt"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "long   uploadLen    =    backupFile . length (  )  ;", "SwiftUtils . debug (  . LOG ,     (  \" Uploading   part    % d   of   file    % s ;  \"     +     \"    localfile =  % s   of   length    % d       -    attempt    % d \"  )  ,    partNumber ,    key ,    backupFile ,    uploadLen ,    attempt )  ;", "nativeStore . uploadFilePart ( new   Path ( key )  ,    partNumber ,    new   FileInputStream ( backupFile )  ,    uploadLen )  ;", "return   uploadLen ;", "}", "METHOD_END"], "methodName": ["uploadFilePartAttempt"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "boolean   uploadSuccess    =    false ;", "int   attempt    =     0  ;", "while    (  ! uploadSuccess )     {", "try    {", "+  + attempt ;", "bytesUploaded    +  =    uploadFileAttempt ( keypath ,    attempt )  ;", "uploadSuccess    =    true ;", "}    catch    ( IOException   e )     {", ". LOG . info (  (  \" Upload   failed    \"     +    e )  ,    e )  ;", "if    ( attempt    >     (  . ATTEMPT _ LIMIT )  )     {", "throw   e ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["uploadOnClose"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "if    ( closed )     {", "throw   new   ConnectionClosedException (  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyOpen"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "assert   len    >  =     0     :     \" remainder   to   write   is   negative \"  ;", "SwiftUtils . debug (  . LOG ,     \"    writeToBackupStream ( offset =  % d ,    len =  % d )  \"  ,    offset ,    len )  ;", "if    ( len    =  =     0  )     {", "return ;", "}", "backupStream . write ( buffer ,    offset ,    len )  ;", "blockOffset    +  =    len ;", "bytesWritten    +  =    len ;", "}", "METHOD_END"], "methodName": ["writeToBackupStream"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftNativeOutputStream"}, {"methodBody": ["METHOD_START", "{", "return   bytes ;", "}", "METHOD_END"], "methodName": ["getBytes"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "return   content _ type ;", "}", "METHOD_END"], "methodName": ["getContent_type"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "return   hash ;", "}", "METHOD_END"], "methodName": ["getHash"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "return   last _ modified ;", "}", "METHOD_END"], "methodName": ["getLast_modified"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "return   pathToRootPath ( name )  ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "return   pathToRootPath ( subdir )  ;", "}", "METHOD_END"], "methodName": ["getSubdir"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "if    ( path    =  =    null )     {", "return   null ;", "}", "if    ( path . startsWith (  \"  /  \"  )  )     {", "return   path ;", "}", "return    \"  /  \"  . concat ( path )  ;", "}", "METHOD_END"], "methodName": ["pathToRootPath"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . bytes    =    bytes ;", "}", "METHOD_END"], "methodName": ["setBytes"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . content _ type    =    content _ type ;", "}", "METHOD_END"], "methodName": ["setContent_type"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . hash    =    hash ;", "}", "METHOD_END"], "methodName": ["setHash"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . last _ modified    =    last _ modified ;", "}", "METHOD_END"], "methodName": ["setLast_modified"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . subdir    =    subdir ;", "}", "METHOD_END"], "methodName": ["setSubdir"], "fileName": "org.apache.hadoop.fs.swift.snative.SwiftObjectFileStatus"}, {"methodBody": ["METHOD_START", "{", "finished    =    time (  )  ;", "}", "METHOD_END"], "methodName": ["finished"], "fileName": "org.apache.hadoop.fs.swift.util.Duration"}, {"methodBody": ["METHOD_START", "{", "return   Duration . humanTime ( value (  )  )  ;", "}", "METHOD_END"], "methodName": ["getDurationString"], "fileName": "org.apache.hadoop.fs.swift.util.Duration"}, {"methodBody": ["METHOD_START", "{", "long   seconds    =    time    /     1  0  0  0  ;", "long   minutes    =    seconds    /     6  0  ;", "return   String . format (  \"  % d :  %  0  2 d :  %  0  3 d \"  ,    minutes ,     ( seconds    %     6  0  )  ,     ( time    %     1  0  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["humanTime"], "fileName": "org.apache.hadoop.fs.swift.util.Duration"}, {"methodBody": ["METHOD_START", "{", "return   System . currentTimeMillis (  )  ;", "}", "METHOD_END"], "methodName": ["time"], "fileName": "org.apache.hadoop.fs.swift.util.Duration"}, {"methodBody": ["METHOD_START", "{", "return    ( finished )     -     ( started )  ;", "}", "METHOD_END"], "methodName": ["value"], "fileName": "org.apache.hadoop.fs.swift.util.Duration"}, {"methodBody": ["METHOD_START", "{", "( n )  +  +  ;", "sum    +  =    x ;", "uble   delta    =    x    -     ( mean )  ;", "mean    +  =    delta    /     ( n )  ;", "m 2     +  =    delta    *     ( x    -     ( mean )  )  ;", "if    ( x    <     ( min )  )     {", "min    =    x ;", "}", "if    ( x    >     ( max )  )     {", "max    =    x ;", "}", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "add ( duration . value (  )  )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "return   mean ;", "}", "METHOD_END"], "methodName": ["getArithmeticMean"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "return   n ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "double   variance    =    getVariance (  )  ;", "return   variance    >     0     ?    Math . sqrt ( variance )     :     0  ;", "}", "METHOD_END"], "methodName": ["getDeviation"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "return   sum ;", "}", "METHOD_END"], "methodName": ["getSum"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "return    ( n )     >     0     ?     ( m 2  )     /     (  ( n )     -     1  )     :     0  ;", "}", "METHOD_END"], "methodName": ["getVariance"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "n    =     0  ;", "sum    =     0  ;", "sum    =     0  ;", "min    =     1  0  0  0  0  0  0  0  ;", "max    =     0  ;", "mean    =     0  ;", "m 2     =     0  ;", "}", "METHOD_END"], "methodName": ["reset"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStats"}, {"methodBody": ["METHOD_START", "{", "DurationStats   durationStats ;", "String   key    =    operation ;", "if    (  ! success )     {", "key    +  =     \"  - FAIL \"  ;", "}", "synchronized ( this )     {", "durationStats    =    statsTable . get ( key )  ;", "if    ( durationStats    =  =    null )     {", "durationStats    =    new   DurationStats ( key )  ;", "statsTable . put ( key ,    durationStats )  ;", "}", "}", "synchronized ( durationStats )     {", "durationStats . add ( duration )  ;", "}", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStatsTable"}, {"methodBody": ["METHOD_START", "{", "List < DurationStats >    results    =    new   ArrayList < DurationStats >  ( statsTable . size (  )  )  ;", "for    ( DurationStats   stat    :    statsTable . values (  )  )     {", "results . add ( new   DurationStats ( stat )  )  ;", "}", "return   results ;", "}", "METHOD_END"], "methodName": ["getDurationStatistics"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStatsTable"}, {"methodBody": ["METHOD_START", "{", "for    ( DurationStats   stat    :    statsTable . values (  )  )     {", "stat . reset (  )  ;", "}", "}", "METHOD_END"], "methodName": ["reset"], "fileName": "org.apache.hadoop.fs.swift.util.DurationStatsTable"}, {"methodBody": ["METHOD_START", "{", "return   JSONUtil . jsonMapper ;", "}", "METHOD_END"], "methodName": ["getJsonMapper"], "fileName": "org.apache.hadoop.fs.swift.util.JSONUtil"}, {"methodBody": ["METHOD_START", "{", "Writer   json    =    new   StringWriter (  )  ;", "try    {", ". jsonMapper . writeValue ( json ,    object )  ;", "return   json . toString (  )  ;", "}    catch    ( JsonGenerationException   e )     {", "throw   new   SwiftJsonMarshallingException ( e . toString (  )  ,    e )  ;", "}    catch    ( JsonMappingException   e )     {", "throw   new   SwiftJsonMarshallingException ( e . toString (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["toJSON"], "fileName": "org.apache.hadoop.fs.swift.util.JSONUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "return    . jsonMapper . readValue ( value ,    klazz )  ;", "}    catch    ( JsonGenerationException   e )     {", "throw   new   SwiftJsonMarshallingException (  (  (  ( e . toString (  )  )     +     \"    source :     \"  )     +    value )  ,    e )  ;", "}    catch    ( JsonMappingException   e )     {", "throw   new   SwiftJsonMarshallingException (  (  (  ( e . toString (  )  )     +     \"    source :     \"  )     +    value )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["toObject"], "fileName": "org.apache.hadoop.fs.swift.util.JSONUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "return    (  ( T )     (  . jsonMapper . readValue ( value ,    collectionType )  )  )  ;", "}    catch    ( JsonGenerationException   e )     {", "throw   new   SwiftJsonMarshallingException (  (  (  ( e . toString (  )  )     +     \"    source :     \"  )     +    value )  ,    e )  ;", "}    catch    ( JsonMappingException   e )     {", "throw   new   SwiftJsonMarshallingException (  (  (  ( e . toString (  )  )     +     \"    source :     \"  )     +    value )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["toObject"], "fileName": "org.apache.hadoop.fs.swift.util.JSONUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "return    (  ( T )     (  . jsonMapper . readValue ( value ,    typeReference )  )  )  ;", "}    catch    ( JsonGenerationException   e )     {", "throw   new   SwiftJsonMarshallingException (  \" Error   generating   response \"  ,    e )  ;", "}    catch    ( JsonMappingException   e )     {", "throw   new   SwiftJsonMarshallingException (  \" Error   generating   response \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["toObject"], "fileName": "org.apache.hadoop.fs.swift.util.JSONUtil"}, {"methodBody": ["METHOD_START", "{", "return   SwiftUtils . joinPaths ( container ,    object )  ;", "}", "METHOD_END"], "methodName": ["buildUriPath"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "return   SwiftObjectPath . fromPath ( uri ,    path ,    false )  ;", "}", "METHOD_END"], "methodName": ["fromPath"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "String   url    =    path . toUri (  )  . getPath (  )  . replaceAll ( SwiftObjectPath . PATH _ PART _ PATTERN . pattern (  )  ,     \"  \"  )  ;", "if    ( addTrailingSlash    &  &     (  !  ( url . endsWith (  \"  /  \"  )  )  )  )     {", "url    +  =     \"  /  \"  ;", "}", "String   container    =    uri . getHost (  )  ;", "if    ( container    =  =    null )     {", "container    =     \"  \"  ;", "} else", "if    ( container . contains (  \"  .  \"  )  )     {", "container    =    http . RestClientBindings . extractContainerName ( container )  ;", "}", "return   new   SwiftObjectPath ( container ,    url )  ;", "}", "METHOD_END"], "methodName": ["fromPath"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "return   object ;", "}", "METHOD_END"], "methodName": ["getObject"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "String   origPath    =    toUriPath (  )  ;", "String   path    =    origPath ;", "if    (  !  ( path . endsWith (  \"  /  \"  )  )  )     {", "path    =    path    +     \"  /  \"  ;", "}", "String   childPath    =    possibleChild . toUriPath (  )  ;", "return    ( childPath . equals ( origPath )  )     |  |     ( childPath . startsWith ( path )  )  ;", "}", "METHOD_END"], "methodName": ["isEqualToOrParentOf"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "return   object . equals ( path )  ;", "}", "METHOD_END"], "methodName": ["objectMatches"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "return   uriPath ;", "}", "METHOD_END"], "methodName": ["toUriPath"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftObjectPath"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . assertPathExists ( fs ,     \" about   to   be   deleted   file \"  ,    file )  ;", "boolean   deleted    =    fs . delete ( file ,    recursive )  ;", "String   dir    =    SwiftTestUtils . ls ( fs ,    file . getParent (  )  )  ;", "assertTrue (  (  (  (  \" Delete   failed   on    \"     +    file )     +     \"  :     \"  )     +    dir )  ,    deleted )  ;", "SwiftTestUtils . assertPathDoesNotExist ( fs ,     \" Deleted   file \"  ,    file )  ;", "}", "METHOD_END"], "methodName": ["assertDeleted"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "FileStatus   status    =    fs . getFileStatus ( path )  ;", "assertEquals (  (  (  (  \" Wrong   file   length   of   file    \"     +    path )     +     \"    status :     \"  )     +    status )  ,    expected ,    status . getLen (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertFileHasLength"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  \" Should   be   a   dir    - but   isn ' t :     \"     +    fileStatus )  ,    fileStatus . isDirectory (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertIsDirectory"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "FileStatus   fileStatus    =    fs . getFileStatus ( path )  ;", ". assertIsDirectory ( fileStatus )  ;", "}", "METHOD_END"], "methodName": ["assertIsDirectory"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . assertPathExists ( fileSystem ,     \" Expected   file \"  ,    filename )  ;", "FileStatus   status    =    fileSystem . getFileStatus ( filename )  ;", "String   fileInfo    =     ( filename    +     \"        \"  )     +    status ;", "assertFalse (  (  \" File   claims   to   be   a   directory    \"     +    fileInfo )  ,    status . isDirectory (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertIsFile"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "FileStatus [  ]    stats    =    fs . listStatus ( dir )  ;", "boolean   found    =    false ;", "StringBuilder   builder    =    new   StringBuilder (  )  ;", "for    ( FileStatus   stat    :    stats )     {", "builder . append ( stat . toString (  )  )  . append (  '  \\ n '  )  ;", "if    ( stat . getPath (  )  . equals ( subdir )  )     {", "found    =    true ;", "}", "}", "assertTrue (  (  (  (  (  (  \" Path    \"     +    subdir )     +     \"    not   found   in   directory    \"  )     +    dir )     +     \"  :  \"  )     +    builder )  ,    found )  ;", "}", "METHOD_END"], "methodName": ["assertListStatusFinds"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileStatus   status    =    fileSystem . getFileStatus ( th )  ;", "fail (  (  (  (  ( message    +     \"  :    unexpectedly   found    \"  )     +    th )     +     \"    as       \"  )     +    status )  )  ;", "}    catch    ( FileNotFoundException   expected )     {", "}", "}", "METHOD_END"], "methodName": ["assertPathDoesNotExist"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( fileSystem . exists ( path )  )  )     {", "fail (  (  (  (  ( message    +     \"  :    not   found    \"  )     +    path )     +     \"    in    \"  )     +     ( path . getParent (  )  )  )  )  ;", ". ls ( fileSystem ,    path . getParent (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["assertPathExists"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   val    =    props . getProperty ( key )  ;", "if    ( expected    =  =    null )     {", "assertNull (  (  (  (  \" Non   null   property    \"     +    key )     +     \"     =     \"  )     +    val )  ,    val )  ;", "} else    {", "assertEquals (  (  (  (  \" property    \"     +    key )     +     \"     =     \"  )     +    val )  ,    expected ,    val )  ;", "}", "}", "METHOD_END"], "methodName": ["assertPropertyEquals"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . noteAction ( action )  ;", "try    {", "if    ( fileSystem    !  =    null )     {", "fileSystem . delete ( new   Path ( cleanupPath )  . makeQualified ( fileSystem )  ,    true )  ;", "}", "}    catch    ( Exception   e )     {", "SwiftTestUtils . LOG . error (  (  (  (  (  (  \" Error   deleting   in    \"     +    action )     +     \"     -     \"  )     +    cleanupPath )     +     \"  :     \"  )     +    e )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . cleanup (  \" TEARDOWN \"  ,    fileSystem ,    cleanupPath )  ;", "}", "METHOD_END"], "methodName": ["cleanupInTeardown"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" Number   of   bytes   read    !  =    number   written \"  ,    len ,    dest . length )  ;", "int   errors    =     0  ;", "int   first _ error _ byte    =     -  1  ;", "for    ( int   i    =     0  ;    i    <    len ;    i +  +  )     {", "if    (  ( src [ i ]  )     !  =     ( dest [ i ]  )  )     {", "if    ( errors    =  =     0  )     {", "first _ error _ byte    =    i ;", "}", "errors +  +  ;", "}", "}", "if    ( errors    >     0  )     {", "String   message    =    String . format (  \"     % d   errors   in   file   of   length    % d \"  ,    errors ,    len )  ;", ". LOG . warn ( message )  ;", "final   int   overlap    =     1  0  ;", "for    ( int   i    =    Math . max (  0  ,     ( first _ error _ byte    -    overlap )  )  ;    i    <     ( Math . min (  ( first _ error _ byte    +    overlap )  ,    len )  )  ;    i +  +  )     {", "byte   actual    =    dest [ i ]  ;", "byte   expected    =    src [ i ]  ;", "String   letter    =     . toChar ( actual )  ;", "String   line    =    String . format (  \"  [  %  0  4 d ]     %  2 x    % s \\ n \"  ,    i ,    actual ,    letter )  ;", "if    ( expected    !  =    actual )     {", "line    =    String . format (  \"  [  %  0  4 d ]     %  2 x    % s    - expected    %  2 x    % s \\ n \"  ,    i ,    actual ,    letter ,    expected ,     . toChar ( expected )  )  ;", "}", ". LOG . warn ( line )  ;", "}", "fail ( message )  ;", "}", "}", "METHOD_END"], "methodName": ["compareByteArrays"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    dataset    =    new   byte [ len ]  ;", "f    ( int   i    =     0  ;    i    <    len ;    i +  +  )     {", "dataset [ i ]     =     (  ( byte )     ( base    +     ( i    %    modulo )  )  )  ;", "}", "return   dataset ;", "}", "METHOD_END"], "methodName": ["dataset"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "SwiftTestUtils . LOG . warn (  (  \" Downgrading   test    \"     +    message )  ,    failure )  ;", "AssumptionViolatedException   ave    =    new   AssumptionViolatedException ( failure ,    null )  ;", "throw   ave ;", "}", "METHOD_END"], "methodName": ["downgrade"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   pathname    +     ( SwiftUtils . fileStatsToString ( stats ,     \"  \\ n \"  )  )  ;", "}", "METHOD_END"], "methodName": ["dumpStats"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "return    \"  / user /  \"     +     ( System . getProperty (  \" user . name \"  )  )  ;", "}", "METHOD_END"], "methodName": ["getDefaultWorkingDirectory"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   instance    =    conf . get ( SwiftTestUtils . TEST _ FS _ SWIFT )  ;", "if    ( instance    =  =    null )     {", "throw   new   SwiftConfigurationException (  (  \" Missing   configuration   entry    \"     +     ( SwiftTestUtils . TEST _ FS _ SWIFT )  )  )  ;", "}", "try    {", "return   new   URI ( instance )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   SwiftConfigurationException (  (  \" Bad   URI :     \"     +    instance )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getServiceURI"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   instance    =    conf . get ( SwiftTestUtils . TEST _ FS _ SWIFT )  ;", "return   instance    !  =    null ;", "}", "METHOD_END"], "methodName": ["hasServiceURI"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   SwiftUtils . ls ( fileSystem ,    path )  ;", "}", "METHOD_END"], "methodName": ["ls"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( SwiftTestUtils . LOG . isDebugEnabled (  )  )     {", "SwiftTestUtils . LOG . debug (  (  (  \"  =  =  =  =  =  =  =  =  =  =  =  =  =  =        \"     +    action )     +     \"     =  =  =  =  =  =  =  =  =  =  =  =  =  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["noteAction"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "FSDataInputStream   in    =    fs . open ( path )  ;", "try    {", "byte [  ]    buf    =    new   byte [ length ]  ;", "in . readFully (  0  ,    buf )  ;", "return    . toChar ( buf )  ;", "}    finally    {", "in . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["readBytesToString"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "FSDataInputStream   in    =    fs . open ( path )  ;", "byte [  ]    dest    =    new   byte [ len ]  ;", "try    {", "in . readFully (  0  ,    dest )  ;", "}    finally    {", "in . close (  )  ;", "}", "return   dest ;", "}", "METHOD_END"], "methodName": ["readDataset"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "throw   new   AssumptionViolatedException ( message )  ;", "}", "METHOD_END"], "methodName": ["skip"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "char [  ]    chars    =    s . toCharArray (  )  ;", "int   len    =    chars . length ;", "byte [  ]    buffer    =    new   byte [ len ]  ;", "for    ( int   i    =     0  ;    i    <    len ;    i +  +  )     {", "buffer [ i ]     =     (  ( byte )     (  ( chars [ i ]  )     &     2  5  5  )  )  ;", "}", "return   buffer ;", "}", "METHOD_END"], "methodName": ["toAsciiByteArray"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( b    >  =     3  2  )     {", "return   Character . toString (  (  ( char )     ( b )  )  )  ;", "} else    {", "return   String . format (  \"  %  0  2 x \"  ,    b )  ;", "}", "}", "METHOD_END"], "methodName": ["toChar"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   builder    =    new   StringBuilder ( buffer . length )  ;", "for    ( byte   b    :    buffer )     {", "builder . append (  . toChar ( b )  )  ;", "}", "return   builder . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toChar"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "fs . delete ( path ,    true )  ;", ". writeTextFile ( fs ,    path ,    null ,    false )  ;", "}", "METHOD_END"], "methodName": ["touch"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "throw   new   AssumptionViolatedException ( message )  ;", "}", "METHOD_END"], "methodName": ["unsupported"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "fs . mkdirs ( path . getParent (  )  )  ;", ". writeDataset ( fs ,    path ,    src ,    len ,    blocksize ,    overwrite )  ;", "byte [  ]    dest    =     . readDataset ( fs ,    path ,    len )  ;", ". compareByteArrays ( src ,    dest ,    len )  ;", "if    ( delete )     {", "boolean   deleted    =    fs . delete ( path ,    false )  ;", "assertTrue (  \" Deleted \"  ,    deleted )  ;", ". assertPathDoesNotExist ( fs ,     \" Cleanup   failed \"  ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["writeAndRead"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  (  \" Not   enough   data   in   source   array   to   write    \"     +    len )     +     \"    bytes \"  )  ,     (  ( src . length )     >  =    len )  )  ;", "FSDataOutputStream   out    =    fs . create ( path ,    overwrite ,    fs . getConf (  )  . getInt (  . IO _ FILE _ BUFFER _ SIZE ,     4  0  9  6  )  ,     (  ( short )     (  1  )  )  ,    blocksize )  ;", "out . write ( src ,     0  ,    len )  ;", "out . close (  )  ;", ". assertFileHasLength ( fs ,    path ,    len )  ;", "}", "METHOD_END"], "methodName": ["writeDataset"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   stream    =    fs . create ( path ,    overwrite )  ;", "byte [  ]    bytes    =    new   byte [  0  ]  ;", "if    ( text    !  =    null )     {", "bytes    =     . toAsciiByteArray ( text )  ;", "stream . write ( bytes )  ;", "}", "stream . close (  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["writeTextFile"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftTestUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( log . isDebugEnabled (  )  )     {", "log . debug ( String . format ( text ,    args )  )  ;", "}", "}", "METHOD_END"], "methodName": ["debug"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( log . isDebugEnabled (  )  )     {", "log . debug (  ( text    +    ex )  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["debugEx"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   buf    =    new   StringBuilder (  (  ( stats . length )     *     1  2  8  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( stats . length )  ;    i +  +  )     {", "bufpend ( String . format (  \"  [  %  0  2 d ]     % s \"  ,    i ,    stats [ i ]  )  ) pend ( separator )  ;", "}", "return   buf . toString (  )  ;", "}", "METHOD_END"], "methodName": ["fileStatsToString"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "return    ( fileStatus . isDirectory (  )  )     |  |     ( SwiftUtils . isFilePretendingToBeDirectory ( fileStatus )  )  ;", "}", "METHOD_END"], "methodName": ["isDirectory"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "return    ( fileStatus . getLen (  )  )     =  =     0  ;", "}", "METHOD_END"], "methodName": ["isFilePretendingToBeDirectory"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "return    ( swiftObject . objectMatches (  \"  \"  )  )     |  |     ( swiftObject . objectMatches (  \"  /  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["isRootDir"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   result    =    new   StringBuilder (  (  (  ( path 1  . length (  )  )     +     ( path 2  . length (  )  )  )     +     1  )  )  ;", "resultpend ( path 1  )  ;", "boolean   insertSlash    =    true ;", "if    ( path 1  . endsWith (  \"  /  \"  )  )     {", "insertSlash    =    false ;", "} else", "if    ( path 2  . startsWith (  \"  /  \"  )  )     {", "insertSlash    =    false ;", "}", "if    ( insertSlash )     {", "resultpend (  \"  /  \"  )  ;", "}", "resultpend ( path 2  )  ;", "return   result . toString (  )  ;", "}", "METHOD_END"], "methodName": ["joinPaths"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( path    =  =    null )     {", "return    \"  /  \"  ;", "}", "FileStatus [  ]    stats ;", "String   pathtext    =     \" ls    \"     +    path ;", "try    {", "stats    =    fileSystem . listStatus ( path )  ;", "}    catch    ( FileNotFoundException   e )     {", "return   pathtext    +     \"     - file   not   found \"  ;", "}    catch    ( IOException   e )     {", "return    ( pathtext    +     \"     - failed :     \"  )     +    e ;", "}", "return   pathtext    +     (  . fileStatsToString ( stats ,     \"  \\ n \"  )  )  ;", "}", "METHOD_END"], "methodName": ["ls"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \"  %  0  6 d \"  ,    partNumber )  ;", "}", "METHOD_END"], "methodName": ["partitionFilenameFromNumber"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( log . isTraceEnabled (  )  )     {", "log . trace ( String . format ( text ,    args )  )  ;", "}", "}", "METHOD_END"], "methodName": ["trace"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( buffer    =  =    null )     {", "throw   new   NullPointerException (  (  \" Null   byte   array   in \"     +     (  . READ )  )  )  ;", "}", "if    ( off    <     0  )     {", "throw   new   IndexOutOfBoundsException (  (  (  (  \" Negative   buffer   offset    \"     +    off )     +     \"    in    \"  )     +     (  . READ )  )  )  ;", "}", "if    ( len    <     0  )     {", "throw   new   IndexOutOfBoundsException (  (  (  (  \" Negative   read   length    \"     +    len )     +     \"    in    \"  )     +     (  . READ )  )  )  ;", "}", "if    ( off    >     ( buffer . length )  )     {", "throw   new   IndexOutOfBoundsException (  (  (  (  (  (  \" Buffer   offset   of    \"     +    off )     +     \" beyond   buffer   size   of    \"  )     +     ( buffer . length )  )     +     \"    in    \"  )     +     (  . READ )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateReadArgs"], "fileName": "org.apache.hadoop.fs.swift.util.SwiftUtils"}, {"methodBody": ["METHOD_START", "{", "return   jobTrackerInfo ;", "}", "METHOD_END"], "methodName": ["getJobTrackerInfo"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   maxMapTasks ;", "}", "METHOD_END"], "methodName": ["getMaxMapTasks"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   maxReduceTasks ;", "}", "METHOD_END"], "methodName": ["getMaxReduceTasks"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   namenodeInfo ;", "}", "METHOD_END"], "methodName": ["getNamenodeInfo"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   numActiveTrackers ;", "}", "METHOD_END"], "methodName": ["getNumActiveTrackers"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   numBlacklistedTrackers ;", "}", "METHOD_END"], "methodName": ["getNumBlacklistedTrackers"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "jobTrackerInfo    =    conf . get ( JT _ IPC _ ADDRESS )  ;", "namenodeInfo    =    conf . get ( FS _ DEFAULT _ NAME _ KEY )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mapred.gridmix.ClusterSummarizer"}, {"methodBody": ["METHOD_START", "{", "final   Path   in    =    new   Path (  \" foo \"  )  . makeQualified ( GridmixTestUtils . dfs . getUri (  )  ,    GridmixTestUtils . dfs . getWorkingDirectory (  )  )  ;", "final   Path   out    =    GridmixTestUtils . DEST . makeQualified ( GridmixTestUtils . dfs . getUri (  )  ,    GridmixTestUtils . dfs . getWorkingDirectory (  )  )  ;", "final   Path   root    =    new   Path (  . workspace . getName (  )  )  . makeQualified ( GridmixTestUtils . dfs . getUri (  )  ,    GridmixTestUtils . dfs . getWorkingDirectory (  )  )  ;", "if    (  !  (  . workspace . exists (  )  )  )     {", "Assert . assertTrue (  . workspace . mkdirs (  )  )  ;", "}", "Configuration   conf    =    null ;", "try    {", "ArrayList < String >    argsList    =    new   ArrayList < String >  (  )  ;", "argsList . add (  (  (  \"  - D \"     +     ( FilePool . GRIDMIX _ MIN _ FILE )  )     +     \"  =  0  \"  )  )  ;", "argsList . add (  (  (  (  \"  - D \"     +     ( Gridmix . GRIDMIX _ USR _ RSV )  )     +     \"  =  \"  )     +     ( EchoUserResolver . class . getName (  )  )  )  )  ;", "if    ( jobCreatorName    !  =    null )     {", "argsList . add (  (  (  (  \"  - D \"     +     ( JobCreator . GRIDMIX _ JOB _ TYPE )  )     +     \"  =  \"  )     +    jobCreatorName )  )  ;", "}", "if    (  ! defaultOutputPath )     {", "argsList . add (  (  (  (  \"  - D \"     +     ( Gridmix . GRIDMIX _ OUT _ DIR )  )     +     \"  =  \"  )     +    out )  )  ;", "}", "argsList . add (  \"  - generate \"  )  ;", "argsList . add (  (  ( String . valueOf (  . GENDATA )  )     +     \" m \"  )  )  ;", "argsList . add ( in . toString (  )  )  ;", "argsList . add (  \"  -  \"  )  ;", "String [  ]    argv    =    argsList . toArray ( new   String [ argsList . size (  )  ]  )  ;", ". DebugGridmix   client    =    new    . DebugGridmix (  )  ;", "conf    =    GridmixTestUtils . mrvl . getConfig (  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setEnum ( GridmixJobSubmissionPolicy . JOB _ SUBMISSION _ POLICY ,     . policy )  ;", "conf . setBoolean ( GridmixJob . GRIDMIX _ USE _ QUEUE _ IN _ TRACE ,    true )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getLoginUser (  )  ;", "conf . set ( USER _ NAME ,    ugi . getUserName (  )  )  ;", "GridmixTestUtils . dfs . mkdirs ( root ,    new   FsPermission (  (  ( short )     (  7  7  7  )  )  )  )  ;", "GridmixTestUtils . dfs . setPermission ( root ,    new   FsPermission (  (  ( short )     (  7  7  7  )  )  )  )  ;", "int   res    =    ToolRunner . run ( conf ,    client ,    argv )  ;", "Assert . assertEquals (  \" Client   exited   with   nonzero   status \"  ,     0  ,    res )  ;", "client . checkMonitor (  )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "}    finally    {", "in . getFileSystem ( conf )  . delete ( in ,    true )  ;", "out . getFileSystem ( conf )  . delete ( out ,    true )  ;", "root . getFileSystem ( conf )  . delete ( root ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["doSubmission"], "fileName": "org.apache.hadoop.mapred.gridmix.CommonJobTest"}, {"methodBody": ["METHOD_START", "{", "job . setMapperClass ( CompressionEmulationUtil . RandomTextDataMapper . class )  ;", "job . setNumReduceTasks (  0  )  ;", "job . setMapOutputKeyClass ( Text . class )  ;", "job . setMapOutputValueClass ( Text . class )  ;", "job . setInputFormatClass ( GenerateData . GenDataFormat . class )  ;", "job . setJarByClass ( GenerateData . class )  ;", "FileOutputFormat . setCompressOutput ( job ,    true )  ;", "try    {", "FileInputFormat . addInputPath ( job ,    new   Path (  \" ignored \"  )  )  ;", "}    catch    ( IOException   e )     {", "CompressionEmulationUtil . LOG . error (  \" Error   while   adding   input   path    \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "target . setBoolean ( COMPRESS ,    source . getBoolean ( COMPRESS ,    false )  )  ;", "String   jobOutputCompressionCodec    =    source . get ( COMPRESS _ CODEC )  ;", "if    ( jobOutputCompressionCodec    !  =    null )     {", "target . set ( COMPRESS _ CODEC ,    jobOutputCompressionCodec )  ;", "}", "String   jobOutputCompressionType    =    source . get ( COMPRESS _ TYPE )  ;", "if    ( jobOutputCompressionType    !  =    null )     {", "target . set ( COMPRESS _ TYPE ,    jobOutputCompressionType )  ;", "}", "target . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    source . getBoolean ( MAP _ OUTPUT _ COMPRESS ,    false )  )  ;", "String   mapOutputCompressionCodec    =    source . get ( MAP _ OUTPUT _ COMPRESS _ CODEC )  ;", "if    ( mapOutputCompressionCodec    !  =    null )     {", "target . set ( MAP _ OUTPUT _ COMPRESS _ CODEC ,    mapOutputCompressionCodec )  ;", "}", "Path [  ]    inputs    =    FileInputFormat . getInputPaths ( new   JobConf ( source )  )  ;", "boolean   needsCompressedInput    =    false ;", "CompressionCodecFactory   compressionCodecs    =    new   CompressionCodecFactory ( source )  ;", "for    ( Path   input    :    inputs )     {", "CompressionCodec   codec    =    compressionCodecs . getCodec ( input )  ;", "if    ( codec    !  =    null )     {", "needsCompressedInput    =    true ;", "}", "}", "CompressionEmulationUtil . setInputCompressionEmulationEnabled ( target ,    needsCompressedInput )  ;", "}", "METHOD_END"], "methodName": ["configureCompressionEmulation"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "return   conf . getFloat ( CompressionEmulationUtil . GRIDMIX _ JOB _ OUTPUT _ COMPRESSION _ RATIO ,    CompressionEmulationUtil . DEFAULT _ COMPRESSION _ RATIO )  ;", "}", "METHOD_END"], "methodName": ["getJobOutputCompressionEmulationRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "return   conf . getFloat ( CompressionEmulationUtil . GRIDMIX _ MAP _ INPUT _ COMPRESSION _ RATIO ,    CompressionEmulationUtil . DEFAULT _ COMPRESSION _ RATIO )  ;", "}", "METHOD_END"], "methodName": ["getMapInputCompressionEmulationRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "return   conf . getFloat ( CompressionEmulationUtil . GRIDMIX _ MAP _ OUTPUT _ COMPRESSION _ RATIO ,    CompressionEmulationUtil . DEFAULT _ COMPRESSION _ RATIO )  ;", "}", "METHOD_END"], "methodName": ["getMapOutputCompressionEmulationRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    file . getFileSystem ( conf )  ;", "JobConf   jConf    =    new   JobConf ( conf )  ;", "if    ( FileOutputFormat . getCompressOutput ( jConf )  )     {", "Class <  ?    extends   CompressionCodec >    codecClass    =    FileOutputFormat . getOutputCompressorClass ( jConf ,    GzipCodec . class )  ;", "CompressionCodec   codec    =    ReflectionUtils . newInstance ( codecClass ,    conf )  ;", "file    =    file . suffix ( codec . getDefaultExtension (  )  )  ;", "if    ( CompressionEmulationUtil . isCompressionEmulationEnabled ( conf )  )     {", "FSDataOutputStream   fileOut    =    fs . create ( file ,    false )  ;", "return   new   DataOutputStream ( codec . createOutputStream ( fileOut )  )  ;", "}", "}", "return   fs . create ( file ,    false )  ;", "}", "METHOD_END"], "methodName": ["getPossiblyCompressedOutputStream"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    file . getFileSystem ( conf )  ;", "if    (  (  . isCompressionEmulationEnabled ( conf )  )     &  &     (  . isInputCompressionEmulationEnabled ( conf )  )  )     {", "CompressionCodecFactory   compressionCodecs    =    new   CompressionCodecFactory ( conf )  ;", "CompressionCodec   codec    =    compressionCodecs . getCodec ( file )  ;", "if    ( codec    !  =    null )     {", "Decompressor   decompressor    =    CodecPool . getDecompressor ( codec )  ;", "if    ( decompressor    !  =    null )     {", "CompressionInputStream   in    =    codec . createInputStream ( fs . open ( file )  ,    decompressor )  ;", "return    (  ( InputStream )     ( in )  )  ;", "}", "}", "}", "FSDataInputStream   in    =    fs . open ( file )  ;", "in . seek ( offset )  ;", "return    (  ( InputStream )     ( in )  )  ;", "}", "METHOD_END"], "methodName": ["getPossiblyDecompressedInputStream"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "int   wordSize    =    CompressionEmulationUtil . COMPRESSION _ LOOKUP _ TABLE . getWordSizeForRatio ( ratio )  ;", "RandomTextDataGenerator   rtg    =    new   RandomTextDataGenerator ( RandomTextDataGenerator . DEFAULT _ LIST _ SIZE ,    seed ,    wordSize )  ;", "return   rtg ;", "}", "METHOD_END"], "methodName": ["getRandomTextDataGenerator"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "long   uncompressedInputBytes    =    possiblyCompressedInputBytes ;", "if    (  . isInputCompressionEmulationEnabled ( conf )  )     {", "float   inputCompressionRatio    =     . getMapInputCompressionEmulationRatio ( conf )  ;", "uncompressedInputBytes    /  =    inputCompressionRatio ;", "}", "return   uncompressedInputBytes ;", "}", "METHOD_END"], "methodName": ["getUncompressedInputBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "return   conf . getBoolean ( CompressionEmulationUtil . COMPRESSION _ EMULATION _ ENABLE ,    true )  ;", "}", "METHOD_END"], "methodName": ["isCompressionEmulationEnabled"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "return   conf . getBoolean ( CompressionEmulationUtil . INPUT _ DECOMPRESSION _ EMULATION _ ENABLE ,    false )  ;", "}", "METHOD_END"], "methodName": ["isInputCompressionEmulationEnabled"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    inputDir . getFileSystem ( conf )  ;", "CompressionCodecFactory   compressionCodecs    =    new   CompressionCodecFactory ( conf )  ;", "long   compressedDataSize    =     0  ;", "int   numCompressedFiles    =     0  ;", "FileStatus [  ]    outFileStatuses    =    fs . listStatus ( inputDir ,    new   Utils . OutputFileUtils . OutputFilesFilter (  )  )  ;", "for    ( FileStatus   status    :    outFileStatuses )     {", "if    ( compressionCodecs    !  =    null )     {", "CompressionCodec   codec    =    compressionCodecs . getCodec ( status . getPath (  )  )  ;", "if    ( codec    !  =    null )     {", "+  + numCompressedFiles ;", "compressedDataSize    +  =    status . getLen (  )  ;", "}", "}", "}", ". LOG . info (  \" Gridmix   is   configured   to   use   compressed   input   data .  \"  )  ;", ". LOG . info (  (  \" Total   size   of   compressed   input   data    :     \"     +     ( StringUtils . humanReadableInt ( compressedDataSize )  )  )  )  ;", ". LOG . info (  (  \" Total   number   of   compressed   input   data   files    :     \"     +    numCompressedFiles )  )  ;", "if    ( numCompressedFiles    =  =     0  )     {", "throw   new   RuntimeException (  (  (  (  (  (  (  (  (  (  \" No   compressed   file   found   in   the   input \"     +     \"    directory    :     \"  )     +     ( inputDir . toString (  )  )  )     +     \"  .    To   enable   compression \"  )     +     \"    emulation ,    run   Gridmix   either   with    \"  )     +     \"    an   input   directory   containing   compressed   input   file ( s )    or \"  )     +     \"    use   the    - generate   option   to    ( re ) generate   it .    If   compression \"  )     +     \"    emulation   is   not   desired ,    disable   it   by   setting    '  \"  )     +     (  . COMPRESSION _ EMULATION _ ENABLE )  )     +     \"  '    to    ' false '  .  \"  )  )  ;", "}", "if    ( uncompressedDataSize    >     0  )     {", "double   ratio    =     (  ( double )     ( compressedDataSize )  )     /    uncompressedDataSize ;", ". LOG . info (  (  \" Input   Data   Compression   Ratio    :     \"     +    ratio )  )  ;", "}", "return   new   GenerateData . DataStatistics ( compressedDataSize ,    numCompressedFiles ,    true )  ;", "}", "METHOD_END"], "methodName": ["publishCompressedDataStatistics"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( CompressionEmulationUtil . COMPRESSION _ EMULATION _ ENABLE ,    val )  ;", "}", "METHOD_END"], "methodName": ["setCompressionEmulationEnabled"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( CompressionEmulationUtil . INPUT _ DECOMPRESSION _ EMULATION _ ENABLE ,    val )  ;", "}", "METHOD_END"], "methodName": ["setInputCompressionEmulationEnabled"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "conf . setFloat ( CompressionEmulationUtil . GRIDMIX _ JOB _ OUTPUT _ COMPRESSION _ RATIO ,    ratio )  ;", "}", "METHOD_END"], "methodName": ["setJobOutputCompressionEmulationRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "conf . setFloat ( CompressionEmulationUtil . GRIDMIX _ MAP _ INPUT _ COMPRESSION _ RATIO ,    ratio )  ;", "}", "METHOD_END"], "methodName": ["setMapInputCompressionEmulationRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "conf . setFloat ( CompressionEmulationUtil . GRIDMIX _ MAP _ OUTPUT _ COMPRESSION _ RATIO ,    ratio )  ;", "}", "METHOD_END"], "methodName": ["setMapOutputCompressionEmulationRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "boolean   compress    =    CompressionEmulationUtil . isCompressionEmulationEnabled ( conf )  ;", "if    ( compress )     {", "float   ratio    =    CompressionEmulationUtil . getMapInputCompressionEmulationRatio ( conf )  ;", "CompressionEmulationUtil . LOG . info (  (  (  \" GridMix   is   configured   to   generate   compressed   input   data   with    \"     +     \"    a   compression   ratio   of    \"  )     +    ratio )  )  ;", "int   wordSize    =    CompressionEmulationUtil . COMPRESSION _ LOOKUP _ TABLE . getWordSizeForRatio ( ratio )  ;", "RandomTextDataGenerator . setRandomTextDataGeneratorWordSize ( conf ,    wordSize )  ;", "RandomTextDataGenerator . setRandomTextDataGeneratorListSize ( conf ,    RandomTextDataGenerator . DEFAULT _ LIST _ SIZE )  ;", "}", "}", "METHOD_END"], "methodName": ["setupDataGeneratorConfig"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "int   significant    =     (  ( int )     ( Math . round (  ( ratio    *     1  0  0  )  )  )  )  ;", "return    (  ( float )     ( significant )  )     /     1  0  0  ;", "}", "METHOD_END"], "methodName": ["standardizeCompressionRatio"], "fileName": "org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil"}, {"methodBody": ["METHOD_START", "{", "GridmixJobSubmissionPolicy   policy    =    GridmixJobSubmissionPolicy . getPolicy ( conf ,    GridmixJobSubmissionPolicy . STRESS )  ;", "if    ( policy    =  =     ( GridmixJobSubmissionPolicy . REPLAY )  )     {", "return   new    . DebugReplayJobFactory ( submitter ,    scratch ,    numJobs ,    conf ,    startFlag ,    resolver )  ;", "} else", "if    ( policy    =  =     ( GridmixJobSubmissionPolicy . STRESS )  )     {", "return   new    . DebugStressJobFactory ( submitter ,    scratch ,    numJobs ,    conf ,    startFlag ,    resolver )  ;", "} else", "if    ( policy    =  =     ( GridmixJobSubmissionPolicy . SERIAL )  )     {", "return   new    . DebugSerialJobFactory ( submitter ,    scratch ,    numJobs ,    conf ,    startFlag ,    resolver )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getFactory"], "fileName": "org.apache.hadoop.mapred.gridmix.DebugJobFactory"}, {"methodBody": ["METHOD_START", "{", "assert    (  0  .  0     <  =    mindist )     &  &     ( mindist    <  =     1  .  0  )  ;", "final   double   min    =    mindist    /    size ;", "final   double   rem    =     1  .  0     -     ( min    *    size )  ;", "final   double [  ]    tmp    =    new   double [ size ]  ;", "for    ( int   i    =     0  ;    i    <     (  ( tmp . length )     -     1  )  ;     +  + i )     {", "tmp [ i ]     =     ( r . nextDouble (  )  )     *    rem ;", "}", "tmp [  (  ( tmp . length )     -     1  )  ]     =    rem ;", "Arrays . sort ( tmp )  ;", "final   double [  ]    ret    =    new   double [ size ]  ;", "ret [  0  ]     =     ( tmp [  0  ]  )     +    min ;", "for    ( int   i    =     1  ;    i    <    size ;     +  + i )     {", "ret [ i ]     =     (  ( tmp [ i ]  )     -     ( tmp [  ( i    -     1  )  ]  )  )     +    min ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getDistr"], "fileName": "org.apache.hadoop.mapred.gridmix.DebugJobProducer"}, {"methodBody": ["METHOD_START", "{", "JobStory   jobStory ;", "while    (  ( jobStory    =    jsp . getNextJob (  )  )     !  =    null )     {", "if    (  (  ( jobStory . getOutcome (  )  )     =  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )     &  &     (  ( jobStory . getSubmissionTime (  )  )     >  =     0  )  )     {", "updateHDFSDistFilesList ( jobStory )  ;", "}", "}", "jsp . close (  )  ;", "return   writeDistFilesList (  )  ;", "}", "METHOD_END"], "methodName": ["buildDistCacheFilesList"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "if    ( shouldEmulateDistCacheLoad (  )  )     {", "String [  ]    files    =    jobConf . getStrings ( CACHE _ FILES )  ;", "if    ( files    !  =    null )     {", "List < String >    cacheFiles    =    new   ArrayList < String >  (  )  ;", "List < String >    localCacheFiles    =    new   ArrayList < String >  (  )  ;", "String [  ]    visibilities    =    jobConf . getStrings ( CACHE _ FILE _ VISIBILITIES )  ;", "String [  ]    timeStamps    =    jobConf . getStrings ( CACHE _ FILE _ TIMESTAMPS )  ;", "String [  ]    fileSizes    =    jobConf . getStrings ( CACHE _ FILES _ SIZES )  ;", "String   user    =    jobConf . getUser (  )  ;", "for    ( int   i    =     0  ;    i    <     ( files . length )  ;    i +  +  )     {", "boolean   visibility    =     ( visibilities    =  =    null )     ?    true    :    Boolean . valueOf ( visibilities [ i ]  )  ;", "if    (  . isLocalDistCacheFile ( files [ i ]  ,    user ,    visibility )  )     {", "String   fileId    =    MD 5 Hash . digest (  (  ( files [ i ]  )     +     ( timeStamps [ i ]  )  )  )  . toString (  )  ;", "long   fileSize    =    Long . valueOf ( fileSizes [ i ]  )  ;", "Path   mappedLocalFilePath    =    PseudoLocalFs . generateFilePath ( fileId ,    fileSize )  . makeQualified ( pseudoLocalFs . getUri (  )  ,    pseudoLocalFs . getWorkingDirectory (  )  )  ;", "pseudoLocalFs . create ( mappedLocalFilePath )  ;", "localCacheFiles . add ( mappedLocalFilePath . toUri (  )  . toString (  )  )  ;", "} else    {", "String   mappedPath    =    mapDistCacheFilePath ( files [ i ]  ,    timeStamps [ i ]  ,    visibility ,    user )  ;", "cacheFiles . add ( mappedPath )  ;", "}", "}", "if    (  ( cacheFiles . size (  )  )     >     0  )     {", "conf . setStrings ( CACHE _ FILES ,    cacheFiles . toArray ( new   String [ cacheFiles . size (  )  ]  )  )  ;", "}", "if    (  ( localCacheFiles . size (  )  )     >     0  )     {", "conf . setStrings (  \" tmpfiles \"  ,    localCacheFiles . toArray ( new   String [ localCacheFiles . size (  )  ]  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["configureDistCacheFiles"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    FileSystem . get ( conf )  ;", "FileSystem . mkdirs ( fs ,    distPath ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["createDistCacheDirectory"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "return   distCachePath ;", "}", "METHOD_END"], "methodName": ["getDistributedCacheDir"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "emulateDistributedCache    =     ( jobCreator . canEmulateDistCacheLoad (  )  )     &  &     ( conf . getBoolean ( DistributedCacheEmulator . GRIDMIX _ EMULATE _ DISTRIBUTEDCACHE ,    true )  )  ;", "generateDistCacheData    =    generate ;", "if    (  ( generateDistCacheData )     |  |     ( emulateDistributedCache )  )     {", "if    (  \"  -  \"  . equals ( traceIn )  )     {", "DistributedCacheEmulator . LOG . warn (  (  \" Gridmix   will   not   emulate   Distributed   Cache   load   because    \"     +     \" the   input   trace   source   is   a   stream   instead   of   file .  \"  )  )  ;", "emulateDistributedCache    =    generateDistCacheData    =    false ;", "} else", "if    ( FileSystem . getLocal ( conf )  . getUri (  )  . getScheme (  )  . equals ( distCachePath . toUri (  )  . getScheme (  )  )  )     {", "DistributedCacheEmulator . LOG . warn (  (  \" Gridmix   will   not   emulate   Distributed   Cache   load   because    \"     +     \"  < iopath >    provided   is   on   local   file   system .  \"  )  )  ;", "emulateDistributedCache    =    generateDistCacheData    =    false ;", "} else    {", "FileSystem   fs    =    FileSystem . get ( conf )  ;", "Path   cur    =    distCachePath . getParent (  )  ;", "while    ( cur    !  =    null )     {", "if    (  ( cur . toString (  )  . length (  )  )     >     0  )     {", "FsPermission   perm    =    fs . getFileStatus ( cur )  . getPermission (  )  ;", "if    (  !  ( perm . getOtherAction (  )  . and ( FsAction . EXECUTE )  . equals ( FsAction . EXECUTE )  )  )     {", "DistributedCacheEmulator . LOG . warn (  (  (  (  (  \" Gridmix   will   not   emulate   Distributed   Cache   load    \"     +     (  \" because   the   ascendant   directory    ( of   distributed   cache    \"     +     \" directory )     \"  )  )     +    cur )     +     \"    doesn ' t   have   execute   permission    \"  )     +     \" for   others .  \"  )  )  ;", "emulateDistributedCache    =    generateDistCacheData    =    false ;", "break ;", "}", "}", "cur    =    cur . getParent (  )  ;", "}", "}", "}", "try    {", "pseudoLocalFs    =    FileSystem . get ( new   URI (  \" pseudo :  /  /  /  \"  )  ,    conf )  ;", "}    catch    ( URISyntaxException   e )     {", "DistributedCacheEmulator . LOG . warn (  (  \" Gridmix   will   not   emulate   Distributed   Cache   load   because    \"     +     \" creation   of   pseudo   local   file   system   failed .  \"  )  )  ;", "e . printStackTrace (  )  ;", "emulateDistributedCache    =    generateDistCacheData    =    false ;", "return ;", "}", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "return    (  ! visibility )     &  &     ( filePath . contains (  ( user    +     \"  /  . staging \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["isLocalDistCacheFile"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "String   id    =    file    +    timeStamp ;", "if    (  ! isPublic )     {", "id    =    id . concat ( user )  ;", "}", "return   new   fs . Path ( distCachePath ,    MD 5 Hash . digest ( id )  . toString (  )  )  . toUri (  )  . getPath (  )  ;", "}", "METHOD_END"], "methodName": ["mapDistCacheFilePath"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "createDistCacheDirectory (  )  ;", "return   buildDistCacheFilesList ( jsp )  ;", "}", "METHOD_END"], "methodName": ["setupGenerateDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "return   emulateDistributedCache ;", "}", "METHOD_END"], "methodName": ["shouldEmulateDistCacheLoad"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "return   generateDistCacheData ;", "}", "METHOD_END"], "methodName": ["shouldGenerateDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "JobConf   jobConf    =    jobdesc . getJobConf (  )  ;", "String [  ]    files    =    jobConf . getStrings ( CACHE _ FILES )  ;", "if    ( files    !  =    null )     {", "String [  ]    fileSizes    =    jobConf . getStrings ( CACHE _ FILES _ SIZES )  ;", "String [  ]    visibilities    =    jobConf . getStrings ( CACHE _ FILE _ VISIBILITIES )  ;", "String [  ]    timeStamps    =    jobConf . getStrings ( CACHE _ FILE _ TIMESTAMPS )  ;", "FileSystem   fs    =    FileSystem . get ( conf )  ;", "String   user    =    jobConf . getUser (  )  ;", "for    ( int   i    =     0  ;    i    <     ( files . length )  ;    i +  +  )     {", "boolean   visibility    =     ( visibilities    =  =    null )     ?    true    :    Boolean . valueOf ( visibilities [ i ]  )  ;", "if    (  . isLocalDistCacheFile ( files [ i ]  ,    user ,    visibility )  )     {", "continue ;", "}", "String   mappedPath    =    mapDistCacheFilePath ( files [ i ]  ,    timeStamps [ i ]  ,    visibility ,    user )  ;", "if    (  ( distCacheFiles . containsKey ( mappedPath )  )     |  |     ( fs . exists ( new   Path ( mappedPath )  )  )  )     {", "continue ;", "}", "distCacheFiles . put ( mappedPath ,    Long . valueOf ( fileSizes [ i ]  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updateHDFSDistCacheFilesList"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "List   dcFiles    =    new   ArrayList ( distCacheFiles . entrySet (  )  )  ;", "Collections . sort ( dcFiles ,    new   Comparator (  )     {", "public   int   compare ( Object   dc 1  ,    Object   dc 2  )     {", "return    (  ( Comparable )     (  (  ( Map . Entry )     ( dc 2  )  )  . getValue (  )  )  )  . compareTo (  (  ( Map . Entry )     ( dc 1  )  )  . getValue (  )  )  ;", "}", "}  )  ;", "FileSystem   fs    =    FileSystem . get ( conf )  ;", "Path   distCacheFilesList    =    new   Path ( distCachePath ,     \"  _ distCacheFiles . txt \"  )  ;", "conf . set ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ FILE _ LIST ,    distCacheFilesList . toString (  )  )  ;", "SequenceFile . Writer   src _ writer    =    SequenceFile . createWriter ( fs ,    conf ,    distCacheFilesList ,    LongWritable . class ,    BytesWritable . class ,    NONE )  ;", "int   fileCount    =    dcFiles . size (  )  ;", "long   byteCount    =     0  ;", "long   bytesSync    =     0  ;", "for    ( Iterator   it    =    dcFiles . iterator (  )  ;    it . hasNext (  )  ;  )     {", "Map . Entry   entry    =     (  ( Map . Entry )     ( it . next (  )  )  )  ;", "LongWritable   fileSize    =    new   LongWritable ( Long . valueOf ( entry . getValue (  )  . toString (  )  )  )  ;", "BytesWritable   filePath    =    new   BytesWritable ( entry . getKey (  )  . toString (  )  . getBytes (  )  )  ;", "byteCount    +  =    fileSize . get (  )  ;", "bytesSync    +  =    fileSize . get (  )  ;", "if    ( bytesSync    >     (  . AVG _ BYTES _ PER _ MAP )  )     {", "src _ writer . sync (  )  ;", "bytesSync    =    fileSize . get (  )  ;", "}", "src _ writer . append ( fileSize ,    filePath )  ;", "}", "if    ( src _ writer    !  =    null )     {", "src _ writer . close (  )  ;", "}", "fs . deleteOnExit ( distCacheFilesList )  ;", "conf . setInt ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ FILE _ COUNT ,    fileCount )  ;", "conf . setLong ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ BYTE _ COUNT ,    byteCount )  ;", ". LOG . info (  (  (  (  (  \" Number   of   HDFS   based   distributed   cache   files   to   be   generated   is    \"     +    fileCount )     +     \"  .    Total   size   of   HDFS   based   distributed   cache   files    \"  )     +     \" to   be   generated   is    \"  )     +    byteCount )  )  ;", "if    (  (  !  ( shouldGenerateDistCacheData (  )  )  )     &  &     ( fileCount    >     0  )  )     {", ". LOG . error (  (  (  (  (  (  (  (  (  (  (  \" Missing    \"     +    fileCount )     +     \"    distributed   cache   files   under   the    \"  )     +     \"    directory \\ n \"  )     +     ( distCachePath )  )     +     \"  \\ nthat   are   needed   for   gridmix \"  )     +     \"    to   emulate   distributed   cache   load .    Either   use    - generate \\ noption \"  )     +     \"    to   generate   distributed   cache   data   along   with   input   data   OR    \"  )     +     \" disable \\ ndistributed   cache   emulation   by   configuring    \\  '  \"  )     +     (  . GRIDMIX _ EMULATE _ DISTRIBUTEDCACHE )  )     +     \"  '    to   false .  \"  )  )  ;", "return   Gridmix . MISSING _ DIST _ CACHE _ FILES _ ERROR ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["writeDistCacheFilesList"], "fileName": "org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator"}, {"methodBody": ["METHOD_START", "{", "return   ugi ;", "}", "METHOD_END"], "methodName": ["getTargetUgi"], "fileName": "org.apache.hadoop.mapred.gridmix.EchoUserResolver"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["needsTargetUsersList"], "fileName": "org.apache.hadoop.mapred.gridmix.EchoUserResolver"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["setTargetUsers"], "fileName": "org.apache.hadoop.mapred.gridmix.EchoUserResolver"}, {"methodBody": ["METHOD_START", "{", "numJobsInInputTrace    =    factory . numJobsInTrace ;", "endTime    =    System . currentTimeMillis (  )  ;", "if    (  \"  -  \"  . equals ( inputPath )  )     {", "inputTraceLocation    =    Summarizer . NA ;", "inputTraceSignature    =    Summarizer . NA ;", "} else    {", "Path   inputTracePath    =    new   Path ( inputPath )  ;", "FileSystem   fs    =    inputTracePath . getFileSystem ( conf )  ;", "inputTraceLocation    =    fs . makeQualified ( inputTracePath )  . toString (  )  ;", "inputTraceSignature    =     . getTraceSignature ( inputPath )  ;", "}", "jobSubmissionPolicy    =    Gridmix . getJobSubmissionPolicy ( conf )  . name (  )  ;", "resolver    =    userResolver . getClass (  )  . getName (  )  ;", "if    ( dataSize    >     0  )     {", "expectedDataSize    =    StringUtils . humanReadableInt ( dataSize )  ;", "} else    {", "expectedDataSize    =    Summarizer . NA ;", "}", "dataStats    =    stats ;", "totalRuntime    =     ( System . currentTimeMillis (  )  )     -     ( getStartTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["finalize"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   commandLineArgs ;", "}", "METHOD_END"], "methodName": ["getCommandLineArgsString"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   endTime ;", "}", "METHOD_END"], "methodName": ["getEndTime"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   expectedDataSize ;", "}", "METHOD_END"], "methodName": ["getExpectedDataSize"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return    ( simulationStartTime )     -     ( startTime )  ;", "}", "METHOD_END"], "methodName": ["getInitTime"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   ExecutionSummarizer . stringifyDataStatistics ( dataStats )  ;", "}", "METHOD_END"], "methodName": ["getInputDataStatistics"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   inputTraceLocation ;", "}", "METHOD_END"], "methodName": ["getInputTraceLocation"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   inputTraceSignature ;", "}", "METHOD_END"], "methodName": ["getInputTraceSignature"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   jobSubmissionPolicy ;", "}", "METHOD_END"], "methodName": ["getJobSubmissionPolicy"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalFailedJobs ;", "}", "METHOD_END"], "methodName": ["getNumFailedJobs"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   numJobsInInputTrace ;", "}", "METHOD_END"], "methodName": ["getNumJobsInTrace"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalLostJobs ;", "}", "METHOD_END"], "methodName": ["getNumLostJobs"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalMapTasksLaunched ;", "}", "METHOD_END"], "methodName": ["getNumMapTasksLaunched"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalReduceTasksLaunched ;", "}", "METHOD_END"], "methodName": ["getNumReduceTasksLaunched"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return    (  ( totalSuccessfulJobs )     +     ( totalFailedJobs )  )     +     ( totalLostJobs )  ;", "}", "METHOD_END"], "methodName": ["getNumSubmittedJobs"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalSuccessfulJobs ;", "}", "METHOD_END"], "methodName": ["getNumSuccessfulJobs"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalRuntime ;", "}", "METHOD_END"], "methodName": ["getRuntime"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   simulationStartTime ;", "}", "METHOD_END"], "methodName": ["getSimulationStartTime"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   totalSimulationTime ;", "}", "METHOD_END"], "methodName": ["getSimulationTime"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   startTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "Path   inputPath    =    new   Path ( input )  ;", "FileSystem   fs    =    inputPath . getFileSystem ( new   Configura (  )  )  ;", "FileStatus   status    =    fs . getFileStatus ( inputPath )  ;", "Path   qPath    =    fs . makeQualified ( status . getPath (  )  )  ;", "String   traceID    =     (  (  ( status . getModificaTime (  )  )     +     ( qPath . toString (  )  )  )     +     ( status . getOwner (  )  )  )     +     ( status . getLen (  )  )  ;", "return   MD 5 Hash . digest ( traceID )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getTraceSignature"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "return   resolver ;", "}", "METHOD_END"], "methodName": ["getUserResolver"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "processJobState ( stats )  ;", "processJobTasks ( stats )  ;", "}", "METHOD_END"], "methodName": ["process"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "Job   job    =    stats . getJob (  )  ;", "try    {", "if    ( job . isSuccessful (  )  )     {", "+  +  ( totalSuccessfulJobs )  ;", "} else    {", "+  +  ( totalFailedJobs )  ;", "}", "}    catch    ( Excep   e )     {", "+  +  ( totalLostJobs )  ;", "}", "}", "METHOD_END"], "methodName": ["processJobState"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "totalMapTasksLaunched    +  =    stats . getNoOfMaps (  )  ;", "totalReduceTasksLaunched    +  =    stats . getNoOfReds (  )  ;", "}", "METHOD_END"], "methodName": ["processJobTasks"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "simulationStartTime    =    System . currentTimeMillis (  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "if    ( stats    !  =    null )     {", "StringBuffer   buffer    =    new   StringBuffer (  )  ;", "String   compressionStatus    =     ( stats . isDataCompressed (  )  )     ?     \" Compressed \"     :     \" Uncompressed \"  ;", "buffer . append ( compressionStatus )  . append (  \"    input   data   size :     \"  )  ;", "buffer . append ( StringUtils . humanReadableInt ( stats . getDataSize (  )  )  )  ;", "buffer . append (  \"  ,     \"  )  ;", "buffer . append (  \" Number   of   files :     \"  )  . append ( stats . getNumFiles (  )  )  ;", "return   buffer . toString (  )  ;", "} else    {", "return    . NA ;", "}", "}", "METHOD_END"], "methodName": ["stringifyDataStatistics"], "fileName": "org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"}, {"methodBody": ["METHOD_START", "{", "updateLock . readLock (  )  . lock (  )  ;", "try    {", "return   root . selects ( minSize ,    files )  ;", "}    finally    {", "updateLock . readLock (  )  . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getInputFiles"], "fileName": "org.apache.hadoop.mapred.gridmix.FilePool"}, {"methodBody": ["METHOD_START", "{", "return   fs . getFileBlockLocations ( stat ,    start ,    len )  ;", "}", "METHOD_END"], "methodName": ["locationsFor"], "fileName": "org.apache.hadoop.mapred.gridmix.FilePool"}, {"methodBody": ["METHOD_START", "{", "updateLock . writeLock (  )  . lock (  )  ;", "try    {", "root    =    new    . InnerDesc ( fs ,    fs . getFileStatus ( path )  ,    new    . MinFileFilter ( conf . getLong (  . GRIDMIX _ MIN _ FILE ,     (  (  1  2  8     *     1  0  2  4  )     *     1  0  2  4  )  )  ,    conf . getLong (  . GRIDMIX _ MAX _ TOTAL ,     (  1  0  0 L    *     (  1 L    <  <     4  0  )  )  )  )  )  ;", "if    (  0     =  =     ( root . getSize (  )  )  )     {", "throw   new   IOException (  (  \" Found   no   satisfactory   file   in    \"     +     ( path )  )  )  ;", "}", "}    finally    {", "updateLock . writeLock (  )  . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["refresh"], "fileName": "org.apache.hadoop.mapred.gridmix.FilePool"}, {"methodBody": ["METHOD_START", "{", "if    (  0     =  =     ( paths . length )  )     {", "return ;", "}", "if    (  ( input )     !  =    null )     {", "input . close (  )  ;", "}", "idx    =     (  ( idx )     +     1  )     %     ( paths . length )  ;", "curlen    =    lengths [ idx ]  ;", "final   Path   f    =    paths [ idx ]  ;", "input    =    CompressionEmulationUtil . getPossiblyDecompressedInputStream ( f ,    conf ,    startoffset [ idx ]  )  ;", "}", "METHOD_END"], "methodName": ["nextSource"], "fileName": "org.apache.hadoop.mapred.gridmix.FileQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( CompressionEmulationUtil . isCompressionEmulationEnabled ( conf )  )     {", "return   CompressionEmulationUtil . publishCompressedDataStatistics ( inputDir ,    conf ,    genBytes )  ;", "} else    {", "return    . publishPlainDataStatistics ( conf ,    inputDir )  ;", "}", "}", "METHOD_END"], "methodName": ["publishDataStatistics"], "fileName": "org.apache.hadoop.mapred.gridmix.GenerateData"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    inputDir . getFileSystem ( conf )  ;", "long   dataSize    =     0  ;", "long   fileCount    =     0  ;", "RemoteIterator < LocatedFileStatus >    iter    =    fs . listFiles ( inputDir ,    true )  ;", "PathFilter   filter    =    new   Utils . OutputFileUtils . OutputFilesFilter (  )  ;", "while    ( iter . hasNext (  )  )     {", "LocatedFileStatus   lStatus    =    iter . next (  )  ;", "if    ( filter . accept ( lStatus . getPath (  )  )  )     {", "dataSize    +  =    lStatus . getLen (  )  ;", "+  + fileCount ;", "}", "}", "GridmixJob . LOG . info (  (  \" Total   size   of   input   data    :     \"     +     ( StringUtils . humanReadableInt ( dataSize )  )  )  )  ;", "GridmixJob . LOG . info (  (  \" Total   number   of   input   data   files    :     \"     +    fileCount )  )  ;", "return   new    . DataStatistics ( dataSize ,    fileCount ,    false )  ;", "}", "METHOD_END"], "methodName": ["publishPlainDataStatistics"], "fileName": "org.apache.hadoop.mapred.gridmix.GenerateData"}, {"methodBody": ["METHOD_START", "{", "return   GridmixJobSubmissionPolicy . getPolicy ( conf ,    GridmixJobSubmissionPolicy . STRESS )  . createJobFactory ( submitter ,    createJobStoryProducer ( traceIn ,    conf )  ,    scratchDir ,    conf ,    startFlag ,    resolver )  ;", "}", "METHOD_END"], "methodName": ["createJobFactory"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "int   delay    =    conf . getInt ( Gridmix . GRIDMIX _ JOBMONITOR _ SLEEPTIME _ MILLIS ,    Gridmix . GRIDMIX _ JOBMONITOR _ SLEEPTIME _ MILLIS _ DEFAULT )  ;", "int   numThreads    =    conf . getInt ( Gridmix . GRIDMIX _ JOBMONITOR _ THREADS ,    Gridmix . GRIDMIX _ JOBMONITOR _ THREADS _ DEFAULT )  ;", "return   new   JobMonitor ( delay ,    TimeUnit . MILLISECONDS ,    stats ,    numThreads )  ;", "}", "METHOD_END"], "methodName": ["createJobMonitor"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "if    (  \"  -  \"  . equals ( traceIn )  )     {", "return   new   ZombieJobProducer ( System . in ,    null )  ;", "}", "return   new   ZombieJobProducer ( new   Path ( traceIn )  ,    null ,    conf )  ;", "}", "METHOD_END"], "methodName": ["createJobStoryProducer"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   new   JobSubmitter ( monitor ,    threads ,    queueDepth ,    pool ,    statistics )  ;", "}", "METHOD_END"], "methodName": ["createJobSubmitter"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   Gridmix . userResolver ;", "}", "METHOD_END"], "methodName": ["getCurrentUserResolver"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "String   sep    =     \"  \"  ;", "for    ( Enum <  ?  >    v    :    e )     {", "sbpend ( sep )  ;", "sbpend ( v . name (  )  )  ;", "sep    =     \"  |  \"  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getEnumValues"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( ioPath ,     \" input \"  )  ;", "}", "METHOD_END"], "methodName": ["getGridmixInputDataPath"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   GridmixJobSubmissionPolicy . getPolicy ( conf ,    GridmixJobSubmissionPolicy . STRESS )  ;", "}", "METHOD_END"], "methodName": ["getJobSubmissionPolicy"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   getEnumValues ( JobCreator . values (  )  )  ;", "}", "METHOD_END"], "methodName": ["getJobTypes"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   getEnumValues ( GridmixJobSubmissionPolicy . values (  )  )  ;", "}", "METHOD_END"], "methodName": ["getSubmissionPolicies"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "return   summarizer ;", "}", "METHOD_END"], "methodName": ["getSummarizer"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "submitter . add ( job )  ;", "try    {", "while    (  !  ( job . isSubmitted (  )  )  )     {", "try    {", "Thread . sleep (  1  0  0  )  ;", "}    catch    ( InterruptedException   ie )     {", "}", "}", "job . getJob (  )  . waitForCompletion ( false )  ;", "}    catch    ( ClassNotFoundException   e )     {", "throw   new   IOException (  \" Internal   error \"  ,    e )  ;", "}", "if    (  !  ( job . getJob (  )  . isSuccessful (  )  )  )     {", "throw   new   IOException (  (  ( job . getJob (  )  . getJobName (  )  )     +     \"    job   failed !  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["launchGridmixJob"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "int   res    =     -  1  ;", "try    {", "res    =    ToolRunner . run ( new   Configuration (  )  ,    new    ( argv )  ,    argv )  ;", "}    finally    {", "ExitUtil . terminate ( res )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "ToolRunner . printGenericCommandUsage ( out )  ;", "out . println (  \" Usage :    gridmix    [  - generate    < MiB >  ]     [  - users   URI ]     [  - Dname = value    .  .  .  ]     < iopath >     < trace >  \"  )  ;", "out . println (  \"       e . g .    gridmix    - generate    1  0  0 m   foo    -  \"  )  ;", "out . println (  \" Options :  \"  )  ;", "out . println (  (  \"           - generate    < MiB >     :    Generate   input   data   of   size   MiB   under    \"     +     (  \"  < iopath >  / input /    and   generate \\ n \\ t \\ t               distributed   cache   data   under    \"     +     \"  < iopath >  / distributedCache /  .  \"  )  )  )  ;", "out . println (  \"           - users    < usersResourceURI >     :    URI   that   contains   the   users   list .  \"  )  ;", "out . println (  \" Configuration   parameters :  \"  )  ;", "out . println (  \"          General   parameters :  \"  )  ;", "out . printf (  \"                       %  -  4  8 s    :    Output   directory \\ n \"  ,    GRIDMIX _ OUT _ DIR )  ;", "out . printf (  \"                       %  -  4  8 s    :    Submitting   threads \\ n \"  ,    GRIDMIX _ SUB _ THR )  ;", "out . printf (  \"                       %  -  4  8 s    :    Queued   job   desc \\ n \"  ,    GRIDMIX _ QUE _ DEP )  ;", "out . printf (  \"                       %  -  4  8 s    :    User   resolution   class \\ n \"  ,    GRIDMIX _ USR _ RSV )  ;", "out . printf (  \"                       %  -  4  8 s    :    Job   types    (  % s )  \\ n \"  ,    JobCreator . GRIDMIX _ JOB _ TYPE ,    getJobTypes (  )  )  ;", "out . println (  \"          Parameters   related   to   job   submission :  \"  )  ;", "out . printf (  \"                       %  -  4  8 s    :    Default   queue \\ n \"  ,    GridmixJob . GRIDMIX _ DEFAULT _ QUEUE )  ;", "out . printf (  \"                       %  -  4  8 s    :    Enable / disable   using   queues   in   trace \\ n \"  ,    GridmixJob . GRIDMIX _ USE _ QUEUE _ IN _ TRACE )  ;", "out . printf (  \"                       %  -  4  8 s    :    Job   submission   policy    (  % s )  \\ n \"  ,    GridmixJobSubmissionPolicy . JOB _ SUBMISSION _ POLICY ,    getSubmissionPolicies (  )  )  ;", "out . println (  \"          Parameters   specific   for   LOADJOB :  \"  )  ;", "out . printf (  \"                       %  -  4  8 s    :    Key   fraction   of   rec \\ n \"  ,    AvgRecordFactory . GRIDMIX _ KEY _ FRC )  ;", "out . println (  \"          Parameters   specific   for   SLEEPJOB :  \"  )  ;", "out . printf (  \"                       %  -  4  8 s    :    Whether   to   ignore   reduce   tasks \\ n \"  ,    SleepJob . SLEEPJOB _ MAPTASK _ ONLY )  ;", "out . printf (  \"                       %  -  4  8 s    :    Number   of   fake   locations   for   map   tasks \\ n \"  ,    JobCreator . SLEEPJOB _ RANDOM _ LOCATIONS )  ;", "out . printf (  \"                       %  -  4  8 s    :    Maximum   map   task   runtime   in   mili - sec \\ n \"  ,    SleepJob . GRIDMIX _ SLEEP _ MAX _ MAP _ TIME )  ;", "out . printf (  \"                       %  -  4  8 s    :    Maximum   reduce   task   runtime   in   mili - sec    ( merge + reduce )  \\ n \"  ,    SleepJob . GRIDMIX _ SLEEP _ MAX _ REDUCE _ TIME )  ;", "out . println (  \"          Parameters   specific   for   STRESS   submission   throttling   policy :  \"  )  ;", "out . printf (  \"                       %  -  4  8 s    :    jobs   vs   task - tracker   ratio \\ n \"  ,    StressJobFactory . CONF _ MAX _ JOB _ TRACKER _ RATIO )  ;", "out . printf (  \"                       %  -  4  8 s    :    maps   vs   map - slot   ratio \\ n \"  ,    StressJobFactory . CONF _ OVERLOAD _ MAPTASK _ MAPSLOT _ RATIO )  ;", "out . printf (  \"                       %  -  4  8 s    :    reduces   vs   reduce - slot   ratio \\ n \"  ,    StressJobFactory . CONF _ OVERLOAD _ REDUCETASK _ REDUCESLOT _ RATIO )  ;", "out . printf (  \"                       %  -  4  8 s    :    map - slot   share   per   job \\ n \"  ,    StressJobFactory . CONF _ MAX _ MAPSLOT _ SHARE _ PER _ JOB )  ;", "out . printf (  \"                       %  -  4  8 s    :    reduce - slot   share   per   job \\ n \"  ,    StressJobFactory . CONF _ MAX _ REDUCESLOT _ SHARE _ PER _ JOB )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "int   val    =     -  1  ;", "final   Configuration   conf    =    getConf (  )  ;", "UserGroupInfmation . setConfiguration ( conf )  ;", "UserGroupInfmation   ugi    =    UserGroupInfmation . getLoginUser (  )  ;", "val    =    ugi . doAs ( new   PrivilegedExceptionAction < Integer >  (  )     {", "public   Integer   run (  )    throws   Exception    {", "return   runJob ( conf ,    argv )  ;", "}", "}  )  ;", "if    ( val    =  =     0  )     {", "System . out . print (  \"  \\ n \\ n \"  )  ;", "System . out . println ( summarizer . toString (  )  )  ;", "}", "return   val ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "if    (  ( argv . length )     <     2  )     {", "Gridmix . LOG . error (  \" Too   few   arguments   to   Gridmix .  \\ n \"  )  ;", "printUsage ( System . err )  ;", "return   Gridmix . ARGS _ ERROR ;", "}", "long   genbytes    =     -  1 L ;", "String   traceIn    =    null ;", "Path   ioPath    =    null ;", "URI   userRsrc    =    null ;", "try    {", "Gridmix . userResolver    =    ReflectionUtils . newInstance ( conf . getClass ( GRIDMIX _ USR _ RSV ,    SubmitterUserResolver . class ,    UserResolver . class )  ,    conf )  ;", "for    ( int   i    =     0  ;    i    <     (  ( argv . length )     -     2  )  ;     +  + i )     {", "if    (  \"  - generate \"  . equals ( argv [ i ]  )  )     {", "genbytes    =    TraditionalBinaryPrefix . string 2 long ( argv [  (  +  + i )  ]  )  ;", "if    ( genbytes    <  =     0  )     {", "Gridmix . LOG . error (  (  \" size   of   input   data   to   be   generated   specified   using    \"     +     \"  - generate   option   should   be   nonnegative .  \\ n \"  )  )  ;", "return   Gridmix . ARGS _ ERROR ;", "}", "} else", "if    (  \"  - users \"  . equals ( argv [ i ]  )  )     {", "userRsrc    =    new   URI ( argv [  (  +  + i )  ]  )  ;", "} else    {", "Gridmix . LOG . error (  (  (  \" Unknown   option    \"     +     ( argv [ i ]  )  )     +     \"    specified .  \\ n \"  )  )  ;", "printUsage ( System . err )  ;", "return   Gridmix . ARGS _ ERROR ;", "}", "}", "if    ( Gridmix . userResolver . needsTargetUsersList (  )  )     {", "if    ( userRsrc    !  =    null )     {", "if    (  !  ( Gridmix . userResolver . setTargetUsers ( userRsrc ,    conf )  )  )     {", "Gridmix . LOG . warn (  (  (  \" Ignoring   the   user   resource    '  \"     +    userRsrc )     +     \"  '  .  \"  )  )  ;", "}", "} else    {", "Gridmix . LOG . error (  (  ( Gridmix . userResolver . getClass (  )  )     +     \"    needs   target   user   list .    Use    - users   option .  \\ n \"  )  )  ;", "printUsage ( System . err )  ;", "return   Gridmix . ARGS _ ERROR ;", "}", "} else", "if    ( userRsrc    !  =    null )     {", "Gridmix . LOG . warn (  (  (  \" Ignoring   the   user   resource    '  \"     +    userRsrc )     +     \"  '  .  \"  )  )  ;", "}", "ioPath    =    new   Path ( argv [  (  ( argv . length )     -     2  )  ]  )  ;", "traceIn    =    argv [  (  ( argv . length )     -     1  )  ]  ;", "}    catch    ( Exception   e )     {", "Gridmix . LOG . error (  (  ( e . toString (  )  )     +     \"  \\ n \"  )  )  ;", "if    ( Gridmix . LOG . isDebugEnabled (  )  )     {", "e . printStackTrace (  )  ;", "}", "printUsage ( System . err )  ;", "return   Gridmix . ARGS _ ERROR ;", "}", "final   FileSystem   inputFs    =    ioPath . getFileSystem ( conf )  ;", "ioPath    =    ioPath . makeQualified ( inputFs )  ;", "boolean   succeeded    =    false ;", "try    {", "succeeded    =    FileSystem . mkdirs ( inputFs ,    ioPath ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "}    catch    ( IOException   e )     {", "}    finally    {", "if    (  ! succeeded )     {", "Gridmix . LOG . error (  (  (  \" Failed   creation   of    < ioPath >    directory    \"     +    ioPath )     +     \"  \\ n \"  )  )  ;", "return   Gridmix . STARTUP _ FAILED _ ERROR ;", "}", "}", "return   start ( conf ,    traceIn ,    ioPath ,    genbytes ,    Gridmix . userResolver )  ;", "}", "METHOD_END"], "methodName": ["runJob"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "distCacheEmulator . init ( traceIn ,    factory . jobCreator ,    generate )  ;", "int   exitCode    =     0  ;", "if    (  ( distCacheEmulator . shouldGenerateDistCacheData (  )  )     |  |     ( distCacheEmulator . shouldEmulateDistCacheLoad (  )  )  )     {", "JobStoryProducer   jsp    =    createJobStoryProducer ( traceIn ,    conf )  ;", "exitCode    =    distCacheEmulator . setupGenerateDistCacheData ( jsp )  ;", "if    ( exitCode    =  =     0  )     {", "writeDistCacheData ( conf )  ;", "}", "}", "return   exitCode ;", "}", "METHOD_END"], "methodName": ["setupDistCacheEmulation"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "final   FileSystem   scratchFs    =    scratchDir . getFileSystem ( conf )  ;", "FileSystem . mkdirs ( scratchFs ,    scratchDir ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "return   setupDistCEmulation ( conf ,    traceIn ,    ioPath ,    generate )  ;", "}", "METHOD_END"], "methodName": ["setupEmulation"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "GenerateData . DataStatistics   stats    =    null ;", "InputStream   trace    =    null ;", "int   exitCode    =     0  ;", "try    {", "Path   scratchDir    =    new   Path ( ioPath ,    conf . get ( GRIDMIX _ OUT _ DIR ,     \" gridmix \"  )  )  ;", "Runtime . getRuntime (  )  . addShutdownHook ( sdh )  ;", "CountDownLatch   startFlag    =    new   CountDownLatch (  1  )  ;", "try    {", "startThreads ( conf ,    traceIn ,    ioPath ,    scratchDir ,    startFlag ,    userResolver )  ;", "Path   inputDir    =    Gridmix . getGridmixInputDataPath ( ioPath )  ;", "exitCode    =    writeInputData ( genbytes ,    inputDir )  ;", "if    ( exitCode    !  =     0  )     {", "return   exitCode ;", "}", "stats    =    GenerateData . publishDataStatistics ( inputDir ,    genbytes ,    conf )  ;", "submitter . refreshFilePool (  )  ;", "boolean   shouldGenerate    =    genbytes    >     0  ;", "exitCode    =    setupEmulation ( conf ,    traceIn ,    scratchDir ,    ioPath ,    shouldGenerate )  ;", "if    ( exitCode    !  =     0  )     {", "return   exitCode ;", "}", "summarizer . start ( conf )  ;", "factory . start (  )  ;", "statistics . start (  )  ;", "}    catch    ( Throwable   e )     {", "Gridmix . LOG . error (  (  (  \" Startup   failed .     \"     +     ( e . toString (  )  )  )     +     \"  \\ n \"  )  )  ;", "if    ( Gridmix . LOG . isDebugEnabled (  )  )     {", "e . printStackTrace (  )  ;", "}", "if    (  ( factory )     !  =    null )", "factory . abort (  )  ;", "exitCode    =    Gridmix . STARTUP _ FAILED _ ERROR ;", "}    finally    {", "startFlag . countDown (  )  ;", "}", "if    (  ( factory )     !  =    null )     {", "factory . join ( Long . MAX _ VALUE )  ;", "final   Throwable   badTraceException    =    factory . error (  )  ;", "if    ( null    !  =    badTraceException )     {", "Gridmix . LOG . error (  \" Error   in   trace \"  ,    badTraceException )  ;", "throw   new   IOException (  \" Error   in   trace \"  ,    badTraceException )  ;", "}", "submitter . shutdown (  )  ;", "submitter . join ( Long . MAX _ VALUE )  ;", "monitor . shutdown (  )  ;", "monitor . join ( Long . MAX _ VALUE )  ;", "statistics . shutdown (  )  ;", "statistics . join ( Long . MAX _ VALUE )  ;", "}", "}    finally    {", "if    (  ( factory )     !  =    null )     {", "summarizer . finalize ( factory ,    traceIn ,    genbytes ,    userResolver ,    stats ,    conf )  ;", "}", "IOUtils . cleanup ( Gridmix . LOG ,    trace )  ;", "}", "return   exitCode ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   inputDir    =    Gridmix . getGridmixInputDataPath ( ioPath )  ;", "GridmixJobSubmissionPolicy   policy    =    Gridmix . getJobSubmissionPolicy ( conf )  ;", "Gridmix . LOG . info (  (  \"    Submission   policy   is    \"     +     ( policy . name (  )  )  )  )  ;", "statistics    =    new   Statistics ( conf ,    policy . getPollingInterval (  )  ,    startFlag )  ;", "monitor    =    createJobMonitor ( statistics ,    conf )  ;", "int   noOfSubmitterThreads    =     ( policy    =  =     ( GridmixJobSubmissionPolicy . SERIAL )  )     ?     1     :     ( Runtime . getRuntime (  )  . availableProcessors (  )  )     +     1  ;", "int   numThreads    =    conf . getInt ( GRIDMIX _ SUB _ THR ,    noOfSubmitterThreads )  ;", "int   queueDep    =    conf . getInt ( GRIDMIX _ QUE _ DEP ,     5  )  ;", "submitter    =    createJobSubmitter ( monitor ,    numThreads ,    queueDep ,    new   FilePool ( conf ,    inputDir )  ,    userResolver ,    statistics )  ;", "distCacheEmulator    =    new   DistributedCacheEmulator ( conf ,    ioPath )  ;", "factory    =    createJobFactory ( submitter ,    traceIn ,    scratchDir ,    conf ,    startFlag ,    userResolver )  ;", "factory . jobCreator . setDistCacheEmulator ( distCacheEmulator )  ;", "if    ( policy    =  =     ( GridmixJobSubmissionPolicy . SERIAL )  )     {", "statistics . addJobStatsListeners ( factory )  ;", "} else    {", "statistics . addClusterStatsObservers ( factory )  ;", "}", "statistics . addJobStatsListeners ( summarizer . getExecutionSummarizer (  )  )  ;", "statistics . addClusterStatsObservers ( summarizer . getClusterSummarizer (  )  )  ;", "monitor . start (  )  ;", "submitter . start (  )  ;", "}    catch    ( Exception   e )     {", "Gridmix . LOG . error (  \"    Exception   at   start    \"  ,    e )  ;", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["startThreads"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "int   fileCount    =    conf . getInt ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ FILE _ COUNT ,     (  -  1  )  )  ;", "if    ( fileCount    >     0  )     {", "final   Job   genDistCacheData    =    new   GenerateDistCacheData ( conf )  ;", ". LOG . info (  (  \" Generating   distributed   cache   data   of   size    \"     +     ( conf . getLong ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ BYTE _ COUNT ,     (  -  1  )  )  )  )  )  ;", "launchJob ( genDistCacheData )  ;", "}", "}", "METHOD_END"], "methodName": ["writeDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "if    ( genbytes    >     0  )     {", "final   Configuration   conf    =    getConf (  )  ;", "if    ( inputDir . getFileSystem ( conf )  . exists ( inputDir )  )     {", "GLOG . error (  (  (  \" Ginput   data   directory    \"     +    inputDir )     +     \"    already   exists   when    - generate   option   is   used .  \\ n \"  )  )  ;", "return   GSTARTUP _ FAILED _ ERROR ;", "}", "CompressionEmulationUtil . setupDataGeneratorConfig ( conf )  ;", "final   GenerateData   genData    =    new   GenerateData ( conf ,    inputDir ,    genbytes )  ;", "GLOG . info (  (  (  \" Generating    \"     +     ( StringUtils . humanReadableInt ( genbytes )  )  )     +     \"    of   test   data .  .  .  \"  )  )  ;", "launchGob ( genData )  ;", "FsShell   shell    =    new   FsShell ( conf )  ;", "try    {", "GLOG . info (  (  \" Changing   the   permissions   for   inputPath    \"     +     ( inputDir . toString (  )  )  )  )  ;", "shell . run ( new   String [  ]  {     \"  - chmod \"  ,     \"  - R \"  ,     \"  7  7  7  \"  ,    inputDir . toString (  )     }  )  ;", "}    catch    ( Exception   e )     {", "GLOG . error (  \" Couldnt   change   the   file   permissions    \"  ,    e )  ;", "throw   new   IOException ( e )  ;", "}", "GLOG . info (  \" Input   data   generation   successful .  \"  )  ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["writeInputData"], "fileName": "org.apache.hadoop.mapred.gridmix.Gridmix"}, {"methodBody": ["METHOD_START", "{", "if    (  ( conf . get ( limitKey )  )     !  =    null )     {", "long   limit    =    conf . getLong ( limitKey ,    DISABLED _ MEMORY _ LIMIT )  ;", "if    ( limit    >  =     0  )     {", "if    ( convertLimitToMB )     {", "limit    /  =     1  0  2  4     *     1  0  2  4  ;", "}", "long   scaledConfigValue    =    conf . getLong ( jobKey ,    DISABLED _ MEMORY _ LIMIT )  ;", "if    ( scaledConfigValue    >    limit )     {", "throw   new   RuntimeException (  (  (  (  (  (  (  (  (  (  (  (  \" Simulated   job ' s   configuration \"     +     \"    parameter    '  \"  )     +    jobKey )     +     \"  '    got   scaled   to   a   value    '  \"  )     +    scaledConfigValue )     +     \"  '    which   exceeds   the   upper   limit   of    '  \"  )     +    limit )     +     \"  '    defined   for   the   simulated   cluster   by   the   key    '  \"  )     +    limitKey )     +     \"  '  .    To   disable   High - Ram   feature   emulation ,    set    '  \"  )     +     (  . GRIDMIX _ HIGHRAM _ EMULATION _ ENABLE )  )     +     \"  '    to    ' false '  .  \"  )  )  ;", "}", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["checkMemoryUpperLimits"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "GridmixJob . descCache . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clearAll"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "GridmixJob . scaleConfigParameter ( sourceConf ,    destConf ,    MAPMEMORY _ MB ,    MAP _ MEMORY _ MB ,    DEFAULT _ MAP _ MEMORY _ MB )  ;", "GridmixJob . validateTaskMemoryLimits ( destConf ,    MAP _ MEMORY _ MB ,    JT _ MAX _ MAPMEMORY _ MB )  ;", "GridmixJob . scaleConfigParameter ( sourceConf ,    destConf ,    REDUCEMEMORY _ MB ,    REDUCE _ MEMORY _ MB ,    DEFAULT _ REDUCE _ MEMORY _ MB )  ;", "GridmixJob . validateTaskMemoryLimits ( destConf ,    REDUCE _ MEMORY _ MB ,    JT _ MAX _ REDUCEMEMORY _ MB )  ;", "}", "METHOD_END"], "methodName": ["configureHighRamProperties"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "String   srcHeapOpts    =    srcConf . get ( key )  ;", "if    ( srcHeapOpts    !  =    null )     {", "List < String >    srcMaxOptsList    =    new   ArrayList < String >  (  )  ;", "MapReducePropertiesParser . extractMaxHeapOpts ( srcHeapOpts ,    srcMaxOptsList ,    new   ArrayList < String >  (  )  )  ;", "if    (  ( srcMaxOptsList . size (  )  )     >     0  )     {", "List < String >    destOtherOptsList    =    new   ArrayList < String >  (  )  ;", "String   destHeapOpts    =    destConf . get ( key )  ;", "if    ( destHeapOpts    !  =    null )     {", "MapReducePropertiesParser . extractMaxHeapOpts ( destHeapOpts ,    new   ArrayList < String >  (  )  ,    destOtherOptsList )  ;", "}", "StringBuilder   newHeapOpts    =    new   StringBuilder (  )  ;", "for    ( String   otherOpt    :    destOtherOptsList )     {", "newHeapOpts . append ( otherOpt )  . append (  \"     \"  )  ;", "}", "for    ( String   opts    :    srcMaxOptsList )     {", "newHeapOpts . append ( opts )  . append (  \"     \"  )  ;", "}", "destConf . set ( key ,    newHeapOpts . toString (  )  . trim (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["configureTaskJVMMaxHeapOptions"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "GridmixJob . configureTaskJVMMaxHeapOptions ( originalJobConf ,    simulatedJobConf ,    MAPRED _ TASK _ JAVA _ OPTS )  ;", "GridmixJob . configureTaskJVMMaxHeapOptions ( originalJobConf ,    simulatedJobConf ,    MAP _ JAVA _ OPTS )  ;", "GridmixJob . configureTaskJVMMaxHeapOptions ( originalJobConf ,    simulatedJobConf ,    REDUCE _ JAVA _ OPTS )  ;", "}", "METHOD_END"], "methodName": ["configureTaskJVMOptions"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   unit . convert (  (  ( submissionTimeNanos )     -     ( System . nanoTime (  )  )  )  ,    TimeUnit . NANOSECONDS )  ;", "}", "METHOD_END"], "methodName": ["getDelay"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   job ;", "}", "METHOD_END"], "methodName": ["getJob"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   jobdesc ;", "}", "METHOD_END"], "methodName": ["getJobDesc"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   job . getConfiguration (  )  . getInt ( GridmixJob . GRIDMIX _ JOB _ SEQ ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["getJobSeqId"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   ugi ;", "}", "METHOD_END"], "methodName": ["getUgi"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   seq ;", "}", "METHOD_END"], "methodName": ["id"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   submitted ;", "}", "METHOD_END"], "methodName": ["isSubmitted"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   GridmixJob . descCache . remove ( seq )  ;", "}", "METHOD_END"], "methodName": ["pullDescription"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   GridmixJob . pullDescription ( GridmixJob . getJobSeqId ( jobCtxt )  )  ;", "}", "METHOD_END"], "methodName": ["pullDescription"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "if    ( null    !  =     ( GridmixJob . descCache . putIfAbsent ( seq ,    splits )  )  )     {", "throw   new   IllegalArgumentException (  (  \" Description   exists   for   id    \"     +    seq )  )  ;", "}", "}", "METHOD_END"], "methodName": ["pushDescription"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "long   simulatedClusterDefaultValue    =    destConf . getLong ( clusterValueKey ,    defaultValue )  ;", "long   originalClusterDefaultValue    =    sourceConf . getLong ( clusterValueKey ,    defaultValue )  ;", "long   originalJobValue    =    sourceConf . getLong ( jobValueKey ,    defaultValue )  ;", "double   scaleFactor    =     (  ( double )     ( originalJobValue )  )     /    originalClusterDefaultValue ;", "long   simulatedJobValue    =     (  ( long )     ( scaleFactor    *    simulatedClusterDefaultValue )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" For   the   job   configuration   parameter    '  \"     +    jobValueKey )     +     \"  '    and   the   cluster   configuration   parameter    '  \"  )     +    clusterValueKey )     +     \"  '  ,    the   original   job ' s   configuration   value \"  )     +     \"    is   scaled   from    '  \"  )     +    originalJobValue )     +     \"  '    to    '  \"  )     +    simulatedJobValue )     +     \"  '    using   the   default    ( unit )    value   of    \"  )     +     \"  '  \"  )     +    originalClusterDefaultValue )     +     \"  '    for   the   original    \"  )     +     \"    cluster   and    '  \"  )     +    simulatedClusterDefaultValue )     +     \"  '    for   the \"  )     +     \"    simulated   cluster .  \"  )  )  ;", "}", "destConf . setLong ( jobValueKey ,    simulatedJobValue )  ;", "}", "METHOD_END"], "methodName": ["scaleConfigParameter"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "if    ( queue    !  =    null )     {", "jobetConfuration (  )  . set ( QUEUE _ NAME ,    queue )  ;", "}", "}", "METHOD_END"], "methodName": ["setJobQueue"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "submitted    =    true ;", "}", "METHOD_END"], "methodName": ["setSubmitted"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "return   job . getJobName (  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( GridmixJob . checkMemoryUpperLimits ( jobKey ,    UPPER _ LIMIT _ ON _ TASK _ VMEM _ PROPERTY ,    conf ,    true )  )  )     {", "GridmixJob . checkMemoryUpperLimits ( jobKey ,    clusterMaxKey ,    conf ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["validateTaskMemoryLimits"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJob"}, {"methodBody": ["METHOD_START", "{", "String   policy    =    conf . get ( GridmixJobSubmissionPolicy . JOB _ SUBMISSION _ POLICY ,    defaultPolicy . name (  )  )  ;", "return   GridmixJobSubmissionPolicy . valueOf ( policy . toUpperCase (  )  )  ;", "}", "METHOD_END"], "methodName": ["getPolicy"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy"}, {"methodBody": ["METHOD_START", "{", "return   pollingInterval ;", "}", "METHOD_END"], "methodName": ["getPollingInterval"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixJobSubmissionPolicy"}, {"methodBody": ["METHOD_START", "{", "return    (  ( super . fixedBytes (  )  )     +     (  ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )     ?    spec . getSize (  )     :     0  )  )     +     ( GridmixKey . META _ BYTES )  ;", "}", "METHOD_END"], "methodName": ["fixedBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "return   partition ;", "}", "METHOD_END"], "methodName": ["getPartition"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "return   spec . rec _ in ;", "}", "METHOD_END"], "methodName": ["getReduceInputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "return   spec . bytes _ out ;", "}", "METHOD_END"], "methodName": ["getReduceOutputBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "return   spec . rec _ out ;", "}", "METHOD_END"], "methodName": ["getReduceOutputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "return   spec . metrics ;", "}", "METHOD_END"], "methodName": ["getReduceResourceUsageMetrics"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "this . partition    =    partition ;", "}", "METHOD_END"], "methodName": ["setPartition"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "final   int   origSize    =    getSize (  )  ;", "spec . rec _ in    =    rec _ in ;", "setSize ( origSize )  ;", "}", "METHOD_END"], "methodName": ["setReduceInputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "final   int   origSize    =    getSize (  )  ;", "spec . bytes _ out    =    b _ out ;", "setSize ( origSize )  ;", "}", "METHOD_END"], "methodName": ["setReduceOutputBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "final   int   origSize    =    getSize (  )  ;", "spec . rec _ out    =    rec _ out ;", "setSize ( origSize )  ;", "}", "METHOD_END"], "methodName": ["setReduceOutputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "spec . setResourceUsageSpecification ( metrics )  ;", "}", "METHOD_END"], "methodName": ["setReduceResourceUsageMetrics"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "assert    ( GridmixKey . REDUCE _ SPEC )     =  =     ( getType (  )  )  ;", "final   int   origSize    =    getSize (  )  ;", "this . spec . set ( spec )  ;", "setSize ( origSize )  ;", "}", "METHOD_END"], "methodName": ["setSpec"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "final   int   origSize    =    getSize (  )  ;", "switch    ( type )     {", "case    . REDUCE _ SPEC    :", "case    . DATA    :", "this . type    =    type ;", "break ;", "default    :", "throw   new   IOException (  (  \" Invalid   type :     \"     +    type )  )  ;", "}", "setSize ( origSize )  ;", "}", "METHOD_END"], "methodName": ["setType"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixKey"}, {"methodBody": ["METHOD_START", "{", "final   int   iSize    =    Math . max (  0  ,     (  ( getSize (  )  )     -     ( fixedBytes (  )  )  )  )  ;", "final   int   seedLen    =     ( Math . min ( iSize ,    jSize )  )     +     (  . FIXED _ BYTES )  ;", "jSeed    =     . maskSeed ( jSeed ,    seedLen )  ;", "long   iSeed    =     . maskSeed ( seed ,    seedLen )  ;", "final   int   cmplen    =    Math . min ( iSize ,    jSize )  ;", "for    ( int   i    =     0  ;    i    <    cmplen ;    i    +  =    Byte . SIZE )     {", "final   int   k    =    cmplen    -    i ;", "for    ( long   j    =     ( Long . SIZE )     -     ( Byte . SIZE )  ;    j    >  =     (  ( Math . max (  0  ,     (  (  ( Long . SIZE )     /     ( Byte . SIZE )  )     -    k )  )  )     *     ( Byte . SIZE )  )  ;    j    -  =    Byte . SIZE )     {", "final   int   xi    =     (  ( int )     (  ( iSeed    >  >  >    j )     &     2  5  5 L )  )  ;", "final   int   xj    =     (  ( int )     (  ( jSeed    >  >  >    j )     &     2  5  5 L )  )  ;", "if    ( xi    !  =    xj )     {", "return   xi    -    xj ;", "}", "}", "iSeed    =    nextRand ( iSeed )  ;", "jSeed    =    nextRand ( jSeed )  ;", "}", "return   iSize    -    jSize ;", "}", "METHOD_END"], "methodName": ["compareSeed"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "return   GridmixRecord . FIXED _ BYTES ;", "}", "METHOD_END"], "methodName": ["fixedBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "if    ( sz    <  =     ( GridmixRecord . FIXED _ BYTES )  )     {", "sd    =     0 L ;", "} else", "if    ( sz    <     (  (  ( Long . SIZE )     /     ( Byte . SIZE )  )     +     ( GridmixRecord . FIXED _ BYTES )  )  )     {", "final   int   tmp    =    sz    -     ( GridmixRecord . FIXED _ BYTES )  ;", "final   long   mask    =     (  1 L    <  <     (  ( Byte . SIZE )     *    tmp )  )     -     1  ;", "sd    &  =    mask    <  <     (  ( Byte . SIZE )     *     (  (  ( Long . SIZE )     /     ( Byte . SIZE )  )     -    tmp )  )  ;", "}", "return   sd ;", "}", "METHOD_END"], "methodName": ["maskSeed"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "x    ^  =    x    <  <     1  3  ;", "x    ^  =    x    >  >  >     7  ;", "turn   x    ^  =    x    <  <     1  7  ;", "}", "METHOD_END"], "methodName": ["nextRand"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "this . compressible    =    compressible ;", "this . compressionRatio    =    ratio ;", "if    ( compressible )     {", "rtg    =    CompressionEmulationUtil . getRandomTextDataGenerator ( ratio ,    RandomTextDataGenerator . DEFAULT _ SEED )  ;", "}", "}", "METHOD_END"], "methodName": ["setCompressibility"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "this . seed    =    seed ;", "}", "METHOD_END"], "methodName": ["setSeed"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "setSizeInternal ( size )  ;", "}", "METHOD_END"], "methodName": ["setSize"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "this . size    =    Math . max (  1  ,    size )  ;", "try    {", "seed    =     . maskSeed ( seed ,    this . size )  ;", "dob . reset (  )  ;", "dob . writeLong ( seed )  ;", "}    catch    ( IOException   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["setSizeInternal"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "long   tmp    =    seed ;", "out . writeLong ( tmp )  ;", "int   i    =    size    -     (  ( Long . SIZE )     /     ( Byte . SIZE )  )  ;", "while    ( i    >     (  (  ( Long . SIZE )     /     ( Byte . SIZE )  )     -     1  )  )     {", "tmp    =    nextRand ( tmp )  ;", "out . writeLong ( tmp )  ;", "i    -  =     ( Long . SIZE )     /     ( Byte . SIZE )  ;", "}", "for    ( tmp    =    nextRand ( tmp )  ;    i    >     0  ;     -  - i )     {", "out . writeByte (  (  ( int )     ( tmp    &     2  5  5  )  )  )  ;", "tmp    >  >  >  =    Byte . SIZE ;", "}", "}", "METHOD_END"], "methodName": ["writeRandom"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "long   tmp    =    seed ;", "out . writeLong ( tmp )  ;", "int   i    =    size    -     (  ( Long . SIZE )     /     ( Byte . SIZE )  )  ;", "String   randomW    =    rtg . getRandomW (  )  ;", "byte [  ]    bytes    =    randomW . getBytes (  \" UTF -  8  \"  )  ;", "long   randomWSize    =    bytes . length ;", "while    ( i    >  =    randomWSize )     {", "out . write ( bytes )  ;", "i    -  =    randomWSize ;", "randomW    =    rtg . getRandomW (  )  ;", "bytes    =    randomW . getBytes (  \" UTF -  8  \"  )  ;", "randomWSize    =    bytes . length ;", "}", "if    ( i    >     0  )     {", "out . write ( bytes ,     0  ,    i )  ;", "}", "}", "METHOD_END"], "methodName": ["writeRandomText"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixRecord"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "return   inputRecords ;", "}", "METHOD_END"], "methodName": ["getInputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "return   maps ;", "}", "METHOD_END"], "methodName": ["getMapCount"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "if    (  0     =  =     ( reduces )  )     {", "return   new   long [  ]  {    outputBytes    }  ;", "}", "final   long [  ]    ret    =    new   long [ reduces ]  ;", "for    ( int   i    =     0  ;    i    <     ( reduces )  ;     +  + i )     {", "ret [ i ]     =    Math . round (  (  ( outputBytes )     *     ( reduceBytes [ i ]  )  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getOutputBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "if    (  0     =  =     ( reduces )  )     {", "return   new   long [  ]  {    outputRecords    }  ;", "}", "final   long [  ]    ret    =    new   long [ reduces ]  ;", "for    ( int   i    =     0  ;    i    <     ( reduces )  ;     +  + i )     {", "ret [ i ]     =    Math . round (  (  ( outputRecords )     *     ( reduceRecords [ i ]  )  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getOutputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "return   reduceOutputBytes [ i ]  ;", "}", "METHOD_END"], "methodName": ["getReduceBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "return   reduceOutputRecords [ i ]  ;", "}", "METHOD_END"], "methodName": ["getReduceRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixSplit"}, {"methodBody": ["METHOD_START", "{", "fs . setOwner ( homeDirectory ,    user ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["changePermission"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixTestUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileSystem   fs    =     . dfsCluster . getFileSystem (  )  ;", "String   path    =     \"  / user /  \"     +    user ;", "Path   homeDirectory    =    new   Path ( path )  ;", "if    (  !  ( fs . exists ( homeDirectory )  )  )     {", ". LOG . info (  (  \" Creating   Home   directory    :     \"     +    homeDirectory )  )  ;", "fs . mkdirs ( homeDirectory )  ;", ". changePermission ( user ,    homeDirectory ,    fs )  ;", "}", ". changePermission ( user ,    homeDirectory ,    fs )  ;", "Path   stagingArea    =    new   Path ( conf . get (  \" mapreduce . jobtracker . staging . root . dir \"  ,     \"  / tmp / hadoop / mapred / staging \"  )  )  ;", ". LOG . info (  (  \" Creating   Staging   root   directory    :     \"     +    stagingArea )  )  ;", "fs . mkdirs ( stagingArea )  ;", "fs . setPermission ( stagingArea ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "}    catch    ( IOException   ioe )     {", "ioe . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createHomeAndStagingDirectory"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixTestUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  \" mapred . queue . names \"  ,     \" default \"  )  ;", "conf . set (  \" yarn . scheduler . capacity . root . queues \"  ,     \" default \"  )  ;", "conf . set (  \" yarn . scheduler . capacity . root . default . capacity \"  ,     \"  1  0  0  .  0  \"  )  ;", "conf . setBoolean ( GridmixTestUtils . GRIDMIX _ USE _ QUEUE _ IN _ TRACE ,    false )  ;", "conf . set ( GridmixTestUtils . GRIDMIX _ DEFAULT _ QUEUE ,     \" default \"  )  ;", "GridmixTestUtils . dfsCluster    =    new   hdfs . MiniDFSCluster . Builder ( conf )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "GridmixTestUtils . dfs    =    GridmixTestUtils . dfsCluster . getFileSystem (  )  ;", "conf . set ( JT _ RETIREJOBS ,     \" false \"  )  ;", "GridmixTestUtils . mrvl    =    MiniMRClientClusterFactory . create ( caller ,     2  ,    conf )  ;", "conf    =    GridmixTestUtils . mrvl . getConfig (  )  ;", "String [  ]    files    =    conf . getStrings ( CACHE _ FILES )  ;", "if    ( files    !  =    null )     {", "String [  ]    timestamps    =    new   String [ files . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( files . length )  ;    i +  +  )     {", "timestamps [ i ]     =    Long . toString ( System . currentTimeMillis (  )  )  ;", "}", "conf . setStrings ( CACHE _ FILE _ TIMESTAMPS ,    timestamps )  ;", "}", "}", "METHOD_END"], "methodName": ["initCluster"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixTestUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( GridmixTestUtils . mrvl )     !  =    null )     {", "GridmixTestUtils . mrvl . stop (  )  ;", "}", "if    (  ( GridmixTestUtils . dfsCluster )     !  =    null )     {", "GridmixTestUtils . dfsCluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["shutdownCluster"], "fileName": "org.apache.hadoop.mapred.gridmix.GridmixTestUtils"}, {"methodBody": ["METHOD_START", "{", "final   ArrayList < Path >    paths    =    new   ArrayList < Path >  (  )  ;", "final   ArrayList < Long >    start    =    new   ArrayList < Long >  (  )  ;", "final   ArrayList < Long >    length    =    new   ArrayList < Long >  (  )  ;", "final   HashMap < String ,    Double >    sb    =    new   HashMap < String ,    Double >  (  )  ;", "do    {", "paths . add ( current . getPath (  )  )  ;", "start . add ( currentStart )  ;", "final   long   fromFile    =    Math . min ( bytes ,     (  ( current . getLen (  )  )     -     ( currentStart )  )  )  ;", "length . add ( fromFile )  ;", "for    ( BlockLocation   loc    :    inputDir . locationsFor ( current ,    currentStart ,    fromFile )  )     {", "final   double   tedium    =     ( loc . getLength (  )  )     /     (  1  .  0     *    bytes )  ;", "for    ( String   l    :    loc . getHosts (  )  )     {", "Double   j    =    sb . get ( l )  ;", "if    ( null    =  =    j )     {", "sb . put ( l ,    tedium )  ;", "} else    {", "sb . put ( l ,     (  ( j . doubleValue (  )  )     +    tedium )  )  ;", "}", "}", "}", "currentStart    +  =    fromFile ;", "bytes    -  =    fromFile ;", "CompressionCodecFactory   compressionCodecs    =    new   CompressionCodecFactory ( conf )  ;", "CompressionCodec   codec    =    compressionCodecs . getCodec ( current . getPath (  )  )  ;", "if    (  (  (  ( current . getLen (  )  )     -     ( currentStart )  )     =  =     0  )     |  |     ( codec    !  =    null )  )     {", "current    =    files . get (  (  (  +  +  ( idx )  )     %     ( files . size (  )  )  )  )  ;", "currentStart    =     0  ;", "}", "}    while    ( bytes    >     0     )  ;", "final   ArrayList < Map . Entry < String ,    Double >  >    sort    =    new   ArrayList < Map . Entry < String ,    Double >  >  ( sb . entrySet (  )  )  ;", "Collections . sort ( sort ,    InputStriper . hostRank )  ;", "final   String [  ]    hosts    =    new   String [ Math . min ( nLocs ,    sort . size (  )  )  ]  ;", "for    ( int   i    =     0  ;     ( i    <    nLocs )     &  &     ( i    <     ( sort . size (  )  )  )  ;     +  + i )     {", "hosts [ i ]     =    sort . get ( i )  . getKey (  )  ;", "}", "return   new   uce . lib . input . CombineFileSplit ( paths . toArray ( new   Path [  0  ]  )  ,    toLongArray ( start )  ,    toLongArray ( length )  ,    hosts )  ;", "}", "METHOD_END"], "methodName": ["splitFor"], "fileName": "org.apache.hadoop.mapred.gridmix.InputStriper"}, {"methodBody": ["METHOD_START", "{", "final   long [  ]    ret    =    new   long [ sigh . size (  )  ]  ;", "for    ( int   i    =     0  ;    i    <     ( ret . length )  ;     +  + i )     {", "ret [ i ]     =    sigh . get ( i )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["toLongArray"], "fileName": "org.apache.hadoop.mapred.gridmix.InputStriper"}, {"methodBody": ["METHOD_START", "{", "return   conf . getEnum ( JobCreator . GRIDMIX _ JOB _ TYPE ,    defaultPolicy )  ;", "}", "METHOD_END"], "methodName": ["getPolicy"], "fileName": "org.apache.hadoop.mapred.gridmix.JobCreator"}, {"methodBody": ["METHOD_START", "{", "this . dce    =    e ;", "}", "METHOD_END"], "methodName": ["setDistCacheEmulator"], "fileName": "org.apache.hadoop.mapred.gridmix.JobCreator"}, {"methodBody": ["METHOD_START", "{", "rThread . interrupt (  )  ;", "}", "METHOD_END"], "methodName": ["abort"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  (  ( getClass (  )  . getName (  )  )     +     \"    is   at   the   start   of   the   pipeline   and   accepts   no   events \"  )  )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "return   error ;", "}", "METHOD_END"], "methodName": ["error"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "JobStory   job    =    getNextJobFromTrace (  )  ;", "while    (  ( job    !  =    null )     &  &     (  (  (  ( job . getOutcome (  )  )     !  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )     |  |     (  ( job . getSubmissionTime (  )  )     <     0  )  )     |  |     (  ( job . getNumberMaps (  )  )     =  =     0  )  )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", "List < String >    reason    =    new   ArrayList < String >  (  )  ;", "if    (  ( job . getOutcome (  )  )     !  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )     {", "reason . add (  (  (  \" STATE    (  \"     +     ( job . getOutcome (  )  . name (  )  )  )     +     \"  )  \"  )  )  ;", "}", "if    (  ( job . getSubmissionTime (  )  )     <     0  )     {", "reason . add (  (  (  \" SUBMISSION - TIME    (  \"     +     ( job . getSubmissionTime (  )  )  )     +     \"  )  \"  )  )  ;", "}", "if    (  ( job . getNumberMaps (  )  )     =  =     0  )     {", "reason . add (  \" ZERO - MAPS - JOB \"  )  ;", "}", "if    (  ( reason . size (  )  )     =  =     0  )     {", "reason . add (  \" N / A \"  )  ;", "}", ". LOG . debug (  (  (  (  (  \" Ignoring   job    \"     +     ( job . getJobID (  )  )  )     +     \"    from   the   input   trace .  \"  )     +     \"    Reason :     \"  )     +     ( StringUtils . join ( reason ,     \"  ,  \"  )  )  )  )  ;", "}", "job    =    getNextJobFromTrace (  )  ;", "}", "return   null    =  =    job    ?    null    :    new    . FilterJobStory ( job )     {", "@ Override", "public   TaskInfo   getTaskInfo ( TaskType   taskType ,    int   taskNumber )     {", "TaskInfo   info    =    this . job . getTaskInfo ( taskType ,    taskNumber )  ;", "if    ( info    !  =    null )     {", "info    =    new    . MinTaskInfo ( info )  ;", "} else    {", "info    =    new    . MinTaskInfo ( new   TaskInfo (  0  ,     0  ,     0  ,     0  ,     0  )  )  ;", "}", "return   info ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getNextJobFiltered"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "JobStory   story    =    jobProducer . getNextJob (  )  ;", "if    ( story    !  =    null )     {", "+  +  ( numJobsInTrace )  ;", "}", "return   story ;", "}", "METHOD_END"], "methodName": ["getNextJobFromTrace"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "rThread . join ( millis )  ;", "}", "METHOD_END"], "methodName": ["join"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "rThread . interrupt (  )  ;", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "rThread . start (  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mapred.gridmix.JobFactory"}, {"methodBody": ["METHOD_START", "{", "synchronized ( mJobs )     {", "graceful    =    false ;", "shutdown    =    true ;", "}", "executor . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["abort"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "runningJobs . put ( job )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "synchronized ( s )     {", "return   new   ArrayList < StatisticsStats >  ( s )  ;", "}", "}", "METHOD_END"], "methodName": ["getRemainingJobs"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "executor . awaitTermination ( millis ,    TimeUnit . MILLISECONDS )  ;", "}", "METHOD_END"], "methodName": ["join"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "JobMonitor . LOG . info (  (  (  (  (  ( job . getJobName (  )  )     +     \"     (  \"  )     +     ( job . getJobID (  )  )  )     +     \"  )  \"  )     +     \"    failure \"  )  )  ;", "}", "METHOD_END"], "methodName": ["onFailure"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "JobMonitor . LOG . info (  (  (  (  (  ( job . getJobName (  )  )     +     \"     (  \"  )     +     ( job . getJobID (  )  )  )     +     \"  )  \"  )     +     \"    success \"  )  )  ;", "}", "METHOD_END"], "methodName": ["onSuccess"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "synchronized ( mJobs )     {", "graceful    =    true ;", "shutdown    =    true ;", "}", "executor . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( numPollingThreads )  ;     +  + i )     {", "executor . execute ( new    . MonitorThread ( i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "String   jobID    =    job . getJob (  )  . getConfiguration (  )  . get ( Gridmix . ORIGINAL _ JOB _ ID )  ;", ". LOG . info (  (  \" Job   submission   failed   notification   for   job    \"     +    jobID )  )  ;", "synchronized ( statistics )     {", "this . statistics . add ( job )  ;", "}", "}", "METHOD_END"], "methodName": ["submissionFailed"], "fileName": "org.apache.hadoop.mapred.gridmix.JobMonitor"}, {"methodBody": ["METHOD_START", "{", "shutdown    =    true ;", "sd . shutdownNow (  )  ;", "}", "METHOD_END"], "methodName": ["abort"], "fileName": "org.apache.hadoop.mapred.gridmix.JobSubmitter"}, {"methodBody": ["METHOD_START", "{", "final   boolean   addToQueue    =     !  ( shutdown )  ;", "if    ( addToQueue )     {", "final    . SubmitTask   task    =    new    . SubmitTask ( job )  ;", ". LOG . info (  (  \" Total   number   of   queued   jobs :     \"     +     (  ( queueDepth )     -     ( sem . availablePermits (  )  )  )  )  )  ;", "sem . acquire (  )  ;", "try    {", "sched . execute ( task )  ;", "}    catch    ( RejectedExecutionException   e )     {", "sem . release (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.mapred.gridmix.JobSubmitter"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( shutdown )  )     {", "throw   new   IllegalStateException (  \" Cannot   wait   for   active   s   thread \"  )  ;", "}", "sched . awaitTermination ( millis ,    TimeUnit . MILLISECONDS )  ;", "}", "METHOD_END"], "methodName": ["join"], "fileName": "org.apache.hadoop.mapred.gridmix.JobSubmitter"}, {"methodBody": ["METHOD_START", "{", "inputDir . refresh (  )  ;", "}", "METHOD_END"], "methodName": ["refreshFilePool"], "fileName": "org.apache.hadoop.mapred.gridmix.JobSubmitter"}, {"methodBody": ["METHOD_START", "{", "shutdown    =    true ;", "sd . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.mapred.gridmix.JobSubmitter"}, {"methodBody": ["METHOD_START", "{", "ugi . doAs ( new   PrivilegedExceptionAction < Job >  (  )     {", "public   Job   run (  )    throws   IOException ,    ClassNotFoundException ,    InterruptedException    {", "job . setMapperClass (  . LoadMapper . class )  ;", "job . setReducerClass (  . LoadReducer . class )  ;", "job . setNumReduceTasks ( jobdesc . getNumberReduces (  )  )  ;", "job . setMapOutputKeyClass ( GridmixKey . class )  ;", "job . setMapOutputValueClass ( GridmixRecord . class )  ;", "job . setSortComparatorClass (  . LoadSortComparator . class )  ;", "job . setGroupingComparatorClass ( GridmixJob . SpecGroupingComparator . class )  ;", "job . setInputFormatClass (  . LoadInputFormat . class )  ;", "job . setOutputFormatClass ( GridmixJob . RawBytesOutputFormat . class )  ;", "job . setPartitionerClass ( GridmixJob . DraftPartitioner . class )  ;", "job . setJarByClass (  . class )  ;", "job . getConfiguration (  )  . setBoolean ( USED _ GENERIC _ PARSER ,    true )  ;", "FileOutputFormat . setOutputPath ( job ,    outdir )  ;", "job . submit (  )  ;", "return   job ;", "}", "}  )  ;", "return   job ;", "}", "METHOD_END"], "methodName": ["call"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadJob"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   inputRecords ;", "}", "METHOD_END"], "methodName": ["getInputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   maps ;", "}", "METHOD_END"], "methodName": ["getMapCount"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   mapMetrics ;", "}", "METHOD_END"], "methodName": ["getMapResourceUsageMetrics"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "if    (  0     =  =     ( reduces )  )     {", "return   new   long [  ]  {    outputBytes    }  ;", "}", "final   long [  ]    ret    =    new   long [ reduces ]  ;", "for    ( int   i    =     0  ;    i    <     ( reduces )  ;     +  + i )     {", "ret [ i ]     =    Math . round (  (  ( outputBytes )     *     ( reduceBytes [ i ]  )  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getOutputBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "if    (  0     =  =     ( reduces )  )     {", "return   new   long [  ]  {    outputRecords    }  ;", "}", "final   long [  ]    ret    =    new   long [ reduces ]  ;", "for    ( int   i    =     0  ;    i    <     ( reduces )  ;     +  + i )     {", "ret [ i ]     =    Math . round (  (  ( outputRecords )     *     ( reduceRecords [ i ]  )  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getOutputRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   reduceOutputBytes [ i ]  ;", "}", "METHOD_END"], "methodName": ["getReduceBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   reduceOutputRecords [ i ]  ;", "}", "METHOD_END"], "methodName": ["getReduceRecords"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   reduceMetrics [ i ]  ;", "}", "METHOD_END"], "methodName": ["getReduceResourceUsageMetrics"], "fileName": "org.apache.hadoop.mapred.gridmix.LoadSplit"}, {"methodBody": ["METHOD_START", "{", "return   new   Path (  (  ( fileId    +     \"  .  \"  )     +    fileSize )  )  ;", "}", "METHOD_END"], "methodName": ["generateFilePath"], "fileName": "org.apache.hadoop.mapred.gridmix.PseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "path    =    path . makeQualified ( this )  ;", "boolean   valid    =    true ;", "long   fileSize    =     0  ;", "if    (  !  ( path . toUri (  )  . getScheme (  )  . equals ( getUri (  )  . getScheme (  )  )  )  )     {", "valid    =    false ;", "} else    {", "String [  ]    parts    =    path . toUri (  )  . getPath (  )  . split (  \"  \\  \\  .  \"  )  ;", "try    {", "fileSize    =    Long . valueOf ( parts [  (  ( parts . length )     -     1  )  ]  )  ;", "valid    =    fileSize    >  =     0  ;", "}    catch    ( NumberFormatException   e )     {", "valid    =    false ;", "}", "}", "if    (  ! valid )     {", "throw   new   FileNotFoundException (  (  (  \" File    \"     +    path )     +     \"    does   not   exist   in   p   local   file   system \"  )  )  ;", "}", "return   fileSize ;", "}", "METHOD_END"], "methodName": ["validateFileNameFormat"], "fileName": "org.apache.hadoop.mapred.gridmix.PseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "if    ( m    >  =    n )     {", "int [  ]    ret    =    new   int [ n ]  ;", "for    ( int   i    =     0  ;    i    <    n ;     +  + i )     {", "ret [ i ]     =    i ;", "}", "return   ret ;", "}", ". Selector   selector    =    new    . Selector ( n ,     (  (  ( float )     ( m )  )     /    n )  ,    rand )  ;", "int [  ]    selected    =    new   int [ m ]  ;", "for    ( int   i    =     0  ;    i    <    m ;     +  + i )     {", "selected [ i ]     =    selector . next (  )  ;", "}", "return   selected ;", "}", "METHOD_END"], "methodName": ["select"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomAlgorithms"}, {"methodBody": ["METHOD_START", "{", "return   conf . getInt ( RandomTextDataGenerator . GRIDMIX _ DATAGEN _ RANDOMTEXT _ LISTSIZE ,    RandomTextDataGenerator . DEFAULT _ LIST _ SIZE )  ;", "}", "METHOD_END"], "methodName": ["getRandomTextDataGeneratorListSize"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "return   conf . getInt ( RandomTextDataGenerator . GRIDMIX _ DATAGEN _ RANDOMTEXT _ WORDSIZE ,    RandomTextDataGenerator . DEFAULT _ WORD _ SIZE )  ;", "}", "METHOD_END"], "methodName": ["getRandomTextDataGeneratorWordSize"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "int   index    =    random . nextInt ( words . length )  ;", "return   words [ index ]  ;", "}", "METHOD_END"], "methodName": ["getRandomWord"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( words )  ;", "}", "METHOD_END"], "methodName": ["getRandomWords"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "if    ( RandomTextDataGenerator . LOG . isDebugEnabled (  )  )     {", "RandomTextDataGenerator . LOG . debug (  (  (  (  \" Random   text   data   generator   is   configured   to   use   a   dictionary    \"     +     \"    with    \"  )     +    listSize )     +     \"    words \"  )  )  ;", "}", "conf . setInt ( RandomTextDataGenerator . GRIDMIX _ DATAGEN _ RANDOMTEXT _ LISTSIZE ,    listSize )  ;", "}", "METHOD_END"], "methodName": ["setRandomTextDataGeneratorListSize"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "if    ( RandomTextDataGenerator . LOG . isDebugEnabled (  )  )     {", "RandomTextDataGenerator . LOG . debug (  (  (  \" Random   text   data   generator   is   configured   to   use   a   dictionary    \"     +     \"    with   words   of   length    \"  )     +    wordSize )  )  ;", "}", "conf . setInt ( RandomTextDataGenerator . GRIDMIX _ DATAGEN _ RANDOMTEXT _ WORDSIZE ,    wordSize )  ;", "}", "METHOD_END"], "methodName": ["setRandomTextDataGeneratorWordSize"], "fileName": "org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "return    (  (  \" Empty   user   list   is   not   allowed   for   RoundRobinUserResolver .    Provided \"     +     \"    user   resource   URI    '  \"  )     +    userloc )     +     \"  '    resulted   in   an   empty   user   list .  \"  ;", "}", "METHOD_END"], "methodName": ["buildEmptyUsersErrorMsg"], "fileName": "org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver"}, {"methodBody": ["METHOD_START", "{", "return   true ;", "}", "METHOD_END"], "methodName": ["needsTargetUsersList"], "fileName": "org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver"}, {"methodBody": ["METHOD_START", "{", "if    ( null    =  =    userUri )     {", "return   Collections . emptyList (  )  ;", "}", "final   Path   userloc    =    new   Path ( userUri . toString (  )  )  ;", "final   Text   rawUgi    =    new   Text (  )  ;", "final   FileSystem   fs    =    userloc . getFileSystem ( conf )  ;", "final   ArrayList < UserGroupInformation >    ugiList    =    new   ArrayList < UserGroupInformation >  (  )  ;", "LineReader   in    =    null ;", "try    {", "in    =    new   LineReader ( fs . open ( userloc )  )  ;", "while    (  ( in . readLine ( rawUgi )  )     >     0  )     {", "if    ( rawUgi . toString (  )  . trim (  )  . equals (  \"  \"  )  )     {", "continue ;", "}", "int   e    =    rawUgi . find (  \"  ,  \"  )  ;", "if    ( e    =  =     0  )     {", "throw   new   IOException (  (  \" Missing   username :     \"     +    rawUgi )  )  ;", "}", "if    ( e    =  =     (  -  1  )  )     {", "e    =    rawUgi . getLength (  )  ;", "}", "final   String   username    =    Text . decode ( rawUgi . getBytes (  )  ,     0  ,    e )  . trim (  )  ;", "UserGroupInformation   ugi    =    null ;", "try    {", "ugi    =    UserGroupInformation . createProxyUser ( username ,    UserGroupInformation . getLoginUser (  )  )  ;", "}    catch    ( IOException   ioe )     {", ". LOG . error (  \" Error   while   creating   a   proxy   user    \"  ,    ioe )  ;", "}", "if    ( ugi    !  =    null )     {", "ugiList . add ( ugi )  ;", "}", "}", "}    finally    {", "if    ( in    !  =    null )     {", "in . close (  )  ;", "}", "}", "return   ugiList ;", "}", "METHOD_END"], "methodName": ["parseUserList"], "fileName": "org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver"}, {"methodBody": ["METHOD_START", "{", "jobCreator . setDistCacheEmulator ( e )  ;", "}", "METHOD_END"], "methodName": ["setDistCacheEmulator"], "fileName": "org.apache.hadoop.mapred.gridmix.SerialJobFactory"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptInfo   ret ;", "for    ( int   i    =     0  ;    true ;     +  + i )     {", "ret    =    jobdesc . getTaskAttemptInfo ( type ,    task ,    i )  ;", "if    (  ( ret . getRunState (  )  )     =  =     ( State . SUCCEEDED )  )     {", "break ;", "}", "}", "if    (  ( ret . getRunState (  )  )     !  =     ( State . SUCCEEDED )  )     {", ". LOG . warn (  (  (  (  \" No   sucessful   attempts   tasktype    \"     +    type )     +     \"    task    \"  )     +    task )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getSuccessfulAttemptInfo"], "fileName": "org.apache.hadoop.mapred.gridmix.SleepJob"}, {"methodBody": ["METHOD_START", "{", "clusterStatlisteners . add ( listener )  ;", "}", "METHOD_END"], "methodName": ["addClusterStatsObservers"], "fileName": "org.apache.hadoop.mapred.gridmix.Statistics"}, {"methodBody": ["METHOD_START", "{", "int   seq    =    GridmixJob . getJobSeqId ( stats . getJob (  )  )  ;", "if    ( seq    <     0  )     {", ". LOG . info (  (  (  (  \" Not   tracking   job    \"     +     ( stats . getJob (  )  . getJobName (  )  )  )     +     \"    as   seq   id   is   less   than   zero :     \"  )     +    seq )  )  ;", "return ;", "}", ". submittedJobsMap . put ( seq ,    stats )  ;", ". numMapsSubmitted    +  =    stats . getNoOfMaps (  )  ;", ". numReducesSubmitted    +  =    stats . getNoOfReds (  )  ;", "}", "METHOD_END"], "methodName": ["addJobStats"], "fileName": "org.apache.hadoop.mapred.gridmix.Statistics"}, {"methodBody": ["METHOD_START", "{", "this . jobStatListeners . add ( listener )  ;", "}", "METHOD_END"], "methodName": ["addJobStatsListeners"], "fileName": "org.apache.hadoop.mapred.gridmix.Statistics"}, {"methodBody": ["METHOD_START", "{", "int   seq    =    GridmixJob . getJobSeqId ( job )  ;", "if    (  ( seq    >  =     0  )     &  &     ( jobdesc    =  =    null )  )     {", "throw   new   IllegalArgumentException (  (  \" JobStory   not   available   for   job    \"     +     ( job . getJobID (  )  )  )  )  ;", "}", "int   maps    =     -  1  ;", "int   reds    =     -  1  ;", "if    ( jobdesc    !  =    null )     {", "maps    =    jobdesc . getNumberMaps (  )  ;", "reds    =    jobdesc . getNumberReduces (  )  ;", "}", "return   new    . JobStats ( maps ,    reds ,    job )  ;", "}", "METHOD_END"], "methodName": ["generateJobStats"], "fileName": "org.apache.hadoop.mapred.gridmix.Statistics"}, {"methodBody": ["METHOD_START", "{", "float   maxEffIncompleteMapTasks    =    Math . max (  1  .  0 F ,     ( mapSlotCapacity    *     ( maxMapSlotSharePerJob )  )  )  ;", "float   mapProgressAdjusted    =    Math . max ( Math . min ( mapProgress ,     1  .  0 F )  ,     0  .  0 F )  ;", "return   Math . min ( maxEffIncompleteMapTasks ,     ( numMaps    *     (  1  .  0 F    -    mapProgressAdjusted )  )  )  ;", "}", "METHOD_END"], "methodName": ["calcEffectiveIncompleteMapTasks"], "fileName": "org.apache.hadoop.mapred.gridmix.StressJobFactory"}, {"methodBody": ["METHOD_START", "{", "float   maxEffIncompleteReduceTasks    =    Math . max (  1  .  0 F ,     ( reduceSlotCapacity    *     ( maxReduceSlotSharePerJob )  )  )  ;", "float   reduceProgressAdjusted    =    Math . max ( Math . min ( reduceProgress ,     1  .  0 F )  ,     0  .  0 F )  ;", "return   Math . min ( maxEffIncompleteReduceTasks ,     ( numReduces    *     (  1  .  0 F    -    reduceProgressAdjusted )  )  )  ;", "}", "METHOD_END"], "methodName": ["calcEffectiveIncompleteReduceTasks"], "fileName": "org.apache.hadoop.mapred.gridmix.StressJobFactory"}, {"methodBody": ["METHOD_START", "{", "return   new   StressJobFactory . StressReaderThread (  \" StressJobFactory \"  )  ;", "}", "METHOD_END"], "methodName": ["createReaderThread"], "fileName": "org.apache.hadoop.mapred.gridmix.StressJobFactory"}, {"methodBody": ["METHOD_START", "{", "return   this . ugi ;", "}", "METHOD_END"], "methodName": ["getTargetUgi"], "fileName": "org.apache.hadoop.mapred.gridmix.SubmitterUserResolver"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["needsTargetUsersList"], "fileName": "org.apache.hadoop.mapred.gridmix.SubmitterUserResolver"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["setTargetUsers"], "fileName": "org.apache.hadoop.mapred.gridmix.SubmitterUserResolver"}, {"methodBody": ["METHOD_START", "{", "executionSummarizer . finalize ( factory ,    path ,    size ,    resolver ,    stats ,    conf )  ;", "}", "METHOD_END"], "methodName": ["finalize"], "fileName": "org.apache.hadoop.mapred.gridmix.Summarizer"}, {"methodBody": ["METHOD_START", "{", "return   clusterSummarizer ;", "}", "METHOD_END"], "methodName": ["getClusterSummarizer"], "fileName": "org.apache.hadoop.mapred.gridmix.Summarizer"}, {"methodBody": ["METHOD_START", "{", "return   executionSummarizer ;", "}", "METHOD_END"], "methodName": ["getExecutionSummarizer"], "fileName": "org.apache.hadoop.mapred.gridmix.Summarizer"}, {"methodBody": ["METHOD_START", "{", "executionSummarizer . start ( conf )  ;", "clusterSummarizer . start ( conf )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.mapred.gridmix.Summarizer"}, {"methodBody": ["METHOD_START", "{", "JobClient   client    =    new   JobClient ( conf )  ;", "conf . setInt ( NUM _ MAPS ,     1  )  ;", "Job   job    =    new   Job ( conf )  ;", "CompressionEmulationUtil . configure ( job )  ;", "job . setInputFormatClass (  . CustomInputFormat . class )  ;", "FileOutputFormat . setOutputPath ( job ,    tempDir )  ;", "job . submit (  )  ;", "int   ret    =     ( job . waitForCompletion ( true )  )     ?     0     :     1  ;", "assertEquals (  \" Job   Failed \"  ,     0  ,    ret )  ;", "}", "METHOD_END"], "methodName": ["runDataGenJob"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "JobConf   conf    =    new   JobConf (  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "CompressionEmulationUtil . setInputCompressionEmulationEnabled ( conf ,    true )  ;", "FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "int   dataSize    =     (  1  0  2  4     *     1  0  2  4  )     *     1  0  ;", "float   ratio    =     0  .  3  5  7 F ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   tempDir    =    new   Path ( rootTempDir ,     \" TestPossiblyCompressibleGridmixRecord \"  )  ;", "lfs . delete ( tempDir ,    true )  ;", "GridmixRecord   record    =    new   GridmixRecord ( dataSize ,     0  )  ;", "record . setCompressibility ( true ,    ratio )  ;", "conf . setClass ( COMPRESS _ CODEC ,    GzipCodec . class ,    CompressionCodec . class )  ;", "FileOutputFormat . setCompressOutput ( conf ,    true )  ;", "Path   recordFile    =    new   Path ( tempDir ,     \" record \"  )  ;", "OutputStream   outStream    =    CompressionEmulationUtil . getPossiblyCompressedOutputStream ( recordFile ,    conf )  ;", "DataOutputStream   out    =    new   DataOutputStream ( outStream )  ;", "record . write ( out )  ;", "out . close (  )  ;", "outStream . close (  )  ;", "Path   actualRecordFile    =    recordFile . suffix (  \"  . gz \"  )  ;", "InputStream   in    =    CompressionEmulationUtil . getPossiblyDecompressedInputStream ( actualRecordFile ,    conf ,     0  )  ;", "long   compressedFileSize    =    lfs . listStatus ( actualRecordFile )  [  0  ]  . getLen (  )  ;", "GridmixRecord   recordRead    =    new   GridmixRecord (  )  ;", "recordRead . readFields ( new   DataInputStream ( in )  )  ;", "assertEquals (  \" Record   size   mismatch   in   a   compressible   GridmixRecord \"  ,    dataSize ,    recordRead . getSize (  )  )  ;", "assertTrue (  \" Failed   to   generate   a   compressible   GridmixRecord \"  ,     (  ( recordRead . getSize (  )  )     >    compressedFileSize )  )  ;", "float   seenRatio    =     (  ( float )     ( compressedFileSize )  )     /    dataSize ;", "assertEquals ( CompressionEmulationUtil . standardizeCompressionRatio ( ratio )  ,    CompressionEmulationUtil . standardizeCompressionRatio ( seenRatio )  ,     1  .  0  )  ;", "}", "METHOD_END"], "methodName": ["testCompressibleGridmixRecord"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "long   dataSize    =     (  1  0     *     1  0  2  4  )     *     1  0  2  4  ;", "Configuration   conf    =    new   Configuration (  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "CompressionEmulationUtil . setInputCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setLong ( GenerateData . GRIDMIX _ GEN _ BYTES ,    dataSize )  ;", "conf . set (  \" mapreduce . job . hdfs - servers \"  ,     \"  \"  )  ;", "float   expectedRatio    =    CompressionEmulationUtil . DEFAULT _ COMPRESSION _ RATIO ;", "if    ( ratio    >     0  )     {", "CompressionEmulationUtil . setMapInputCompressionEmulationRatio ( conf ,    ratio )  ;", "expectedRatio    =    CompressionEmulationUtil . standardizeCompressionRatio ( ratio )  ;", "}", "CompressionEmulationUtil . setupDataGeneratorConfig ( conf )  ;", "FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   tempDir    =    new   Path ( rootTempDir ,     \" TestCustomRandomCompressedTextDataGenr \"  )  ;", "lfs . delete ( tempDir ,    true )  ;", ". runDataGenJob ( conf ,    tempDir )  ;", "FileStatus [  ]    files    =    lfs . listStatus ( tempDir ,    new   Utils . OutputFileUtils . OutputFilesFilter (  )  )  ;", "long   size    =     0  ;", "for    ( FileStatus   status    :    files )     {", "size    +  =    status . getLen (  )  ;", "}", "float   compressionRatio    =     (  ( float )     ( size )  )     /    dataSize ;", "float   stdRatio    =    CompressionEmulationUtil . standardizeCompressionRatio ( compressionRatio )  ;", "assertEquals ( expectedRatio ,    stdRatio ,     0  .  0  )  ;", "}", "METHOD_END"], "methodName": ["testCompressionRatioConfigure"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  0  .  5  5 F ,    CompressionEmulationUtil . standardizeCompressionRatio (  0  .  5  5 F )  ,     0  .  0  )  ;", "assertEquals (  0  .  6  5 F ,    CompressionEmulationUtil . standardizeCompressionRatio (  0  .  6  5  2 F )  ,     0  .  0  )  ;", "assertEquals (  0  .  7  8 F ,    CompressionEmulationUtil . standardizeCompressionRatio (  0  .  7  7  7 F )  ,     0  .  0  )  ;", "assertEquals (  0  .  8  6 F ,    CompressionEmulationUtil . standardizeCompressionRatio (  0  .  8  5  5 F )  ,     0  .  0  )  ;", "}", "METHOD_END"], "methodName": ["testCompressionRatioStandardization"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "testCompressionRatioConfigure (  0  .  0 F )  ;", "testCompressionRatioConfigure (  0  .  2 F )  ;", "testCompressionRatioConfigure (  0  .  4 F )  ;", "testCompressionRatioConfigure (  0  .  6  5 F )  ;", "testCompressionRatioConfigure (  0  .  6  8  2 F )  ;", "testCompressionRatioConfigure (  0  .  5  6  7 F )  ;", "boolean   failed    =    false ;", "try    {", "testCompressionRatioConfigure (  0  .  0  1 F )  ;", "}    catch    ( RuntimeException   re )     {", "failed    =    true ;", "}", "assertTrue (  \" Compression   ratio   min   value    (  0  .  0  7  )    check   failed !  \"  ,    failed )  ;", "failed    =    false ;", "try    {", "testCompressionRatioConfigure (  0  .  7 F )  ;", "}    catch    ( RuntimeException   re )     {", "failed    =    true ;", "}", "assertTrue (  \" Compression   ratio   max   value    (  0  .  6  8  )    check   failed !  \"  ,    failed )  ;", "}", "METHOD_END"], "methodName": ["testCompressionRatios"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "long   possiblyCompressedInputBytes    =     1  0  0  0  0  0  ;", "float   compressionRatio    =     0  .  4  5 F ;", "Configuration   conf    =    new   Configuration (  )  ;", ". setMapInputCompressionEmulationRatio ( conf ,    compressionRatio )  ;", "long   result    =     . getUncompressedInputBytes ( possiblyCompressedInputBytes ,    conf )  ;", "assertEquals ( possiblyCompressedInputBytes ,    result )  ;", ". setInputCompressionEmulationEnabled ( conf ,    true )  ;", "result    =     . getUncompressedInputBytes ( possiblyCompressedInputBytes ,    conf )  ;", "assertEquals (  (  ( long )     ( possiblyCompressedInputBytes    /    compressionRatio )  )  ,    result )  ;", "}", "METHOD_END"], "methodName": ["testComputeUncompressedInputBytes"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "JobConf   source    =    new   JobConf (  )  ;", "JobConf   target    =    new   JobConf (  )  ;", "source . setBoolean ( COMPRESS ,    false )  ;", "source . set ( COMPRESS _ CODEC ,     \" MyDefaultCodec \"  )  ;", "source . set ( COMPRESS _ TYPE ,     \" MyDefaultType \"  )  ;", "source . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    false )  ;", "source . set ( MAP _ OUTPUT _ COMPRESS _ CODEC ,     \" MyDefaultCodec 2  \"  )  ;", "CompressionEmulationUtil . configureCompressionEmulation ( source ,    target )  ;", "assertFalse ( target . getBoolean ( COMPRESS ,    true )  )  ;", "assertEquals (  \" MyDefaultCodec \"  ,    target . get ( COMPRESS _ CODEC )  )  ;", "assertEquals (  \" MyDefaultType \"  ,    target . get ( COMPRESS _ TYPE )  )  ;", "assertFalse ( target . getBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  )  ;", "assertEquals (  \" MyDefaultCodec 2  \"  ,    target . get ( MAP _ OUTPUT _ COMPRESS _ CODEC )  )  ;", "assertFalse ( CompressionEmulationUtil . isInputCompressionEmulationEnabled ( target )  )  ;", "source . setBoolean ( COMPRESS ,    true )  ;", "source . set ( COMPRESS _ CODEC ,     \" MyCodec \"  )  ;", "source . set ( COMPRESS _ TYPE ,     \" MyType \"  )  ;", "source . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  ;", "source . set ( MAP _ OUTPUT _ COMPRESS _ CODEC ,     \" MyCodec 2  \"  )  ;", "FileInputFormat . setInputPaths ( source ,     \" file . gz \"  )  ;", "target    =    new   JobConf (  )  ;", "CompressionEmulationUtil . configureCompressionEmulation ( source ,    target )  ;", "assertTrue ( target . getBoolean ( COMPRESS ,    false )  )  ;", "assertEquals (  \" MyCodec \"  ,    target . get ( COMPRESS _ CODEC )  )  ;", "assertEquals (  \" MyType \"  ,    target . get ( COMPRESS _ TYPE )  )  ;", "assertTrue ( target . getBoolean ( MAP _ OUTPUT _ COMPRESS ,    false )  )  ;", "assertEquals (  \" MyCodec 2  \"  ,    target . get ( MAP _ OUTPUT _ COMPRESS _ CODEC )  )  ;", "assertTrue ( CompressionEmulationUtil . isInputCompressionEmulationEnabled ( target )  )  ;", "}", "METHOD_END"], "methodName": ["testExtractCompressionConfigs"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "JobConf   conf    =    new   JobConf (  )  ;", "FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "String   inputLine    =     \" Hi   Hello !  \"  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "CompressionEmulationUtil . setInputCompressionEmulationEnabled ( conf ,    true )  ;", "FileOutputFormat . setCompressOutput ( conf ,    true )  ;", "FileOutputFormat . setOutputCompressorClass ( conf ,    GzipCodec . class )  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   tempDir    =    new   Path ( rootTempDir ,     \" TestFileQueueDecompression \"  )  ;", "lfs . delete ( tempDir ,    true )  ;", "Path   compressedFile    =    new   Path ( tempDir ,     \" test \"  )  ;", "OutputStream   out    =    CompressionEmulationUtil . getPossiblyCompressedOutputStream ( compressedFile ,    conf )  ;", "BufferedWriter   writer    =    new   BufferedWriter ( new   OutputStreamWriter ( out )  )  ;", "writer . write ( inputLine )  ;", "writer . close (  )  ;", "compressedFile    =    compressedFile . suffix (  \"  . gz \"  )  ;", "long   fileSize    =    lfs . listStatus ( compressedFile )  [  0  ]  . getLen (  )  ;", "CombineFileSplit   split    =    new   CombineFileSplit ( new   Path [  ]  {    compressedFile    }  ,    new   long [  ]  {    fileSize    }  )  ;", "FileQueue   queue    =    new   FileQueue ( split ,    conf )  ;", "byte [  ]    bytes    =    new   byte [ inputLine . getBytes (  )  . length ]  ;", "queue . read ( bytes )  ;", "queue . close (  )  ;", "String   readLine    =    new   String ( bytes )  ;", "assertEquals (  \" Compression / Decompression   error \"  ,    inputLine ,    readLine )  ;", "}", "METHOD_END"], "methodName": ["testFileQueueDecompression"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "float   ratio    =     0  .  5  6  7 F ;", ". setMapInputCompressionEmulationRatio ( conf ,    ratio )  ;", "assertEquals ( ratio ,     . getMapInputCompressionEmulationRatio ( conf )  ,     0  .  0  )  ;", "}", "METHOD_END"], "methodName": ["testInputCompressionRatioConfiguration"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "float   ratio    =     0  .  5  6  7 F ;", ". setMapOutputCompressionEmulationRatio ( conf ,    ratio )  ;", "assertEquals ( ratio ,     . getMapOutputCompressionEmulationRatio ( conf )  ,     0  .  0  )  ;", "}", "METHOD_END"], "methodName": ["testIntermediateCompressionRatioConfiguration"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "assertTrue (  . isCompressionEmulationEnabled ( conf )  )  ;", ". setCompressionEmulationEnabled ( conf ,    false )  ;", "assertFalse (  . isCompressionEmulationEnabled ( conf )  )  ;", ". setCompressionEmulationEnabled ( conf ,    true )  ;", "assertTrue (  . isCompressionEmulationEnabled ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["testIsCompressionEmulationEnabled"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "float   ratio    =     0  .  5  6  7 F ;", ". setJobOutputCompressionEmulationRatio ( conf ,    ratio )  ;", "assertEquals ( ratio ,     . getJobOutputCompressionEmulationRatio ( conf )  ,     0  .  0  )  ;", "}", "METHOD_END"], "methodName": ["testOutputCompressionRatioConfiguration"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "JobConf   conf    =    new   JobConf (  )  ;", "FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "String   inputLine    =     \" Hi   Hello !  \"  ;", ". setCompressionEmulationEnabled ( conf ,    true )  ;", ". setInputCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( COMPRESS ,    true )  ;", "conf . setClass ( COMPRESS _ CODEC ,    GzipCodec . class ,    CompressionCodec . class )  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   tempDir    =    new   Path ( rootTempDir ,     \" TestPossiblyCompressedDecompressedStreams \"  )  ;", "lfs . delete ( tempDir ,    true )  ;", "Path   compressedFile    =    new   Path ( tempDir ,     \" test \"  )  ;", "OutputStream   out    =     . getPossiblyCompressedOutputStream ( compressedFile ,    conf )  ;", "BufferedWriter   writer    =    new   BufferedWriter ( new   OutputStreamWriter ( out )  )  ;", "writer . write ( inputLine )  ;", "writer . close (  )  ;", "compressedFile    =    compressedFile . suffix (  \"  . gz \"  )  ;", "InputStream   in    =     . getPossiblyDecompressedInputStream ( compressedFile ,    conf ,     0  )  ;", "BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( in )  )  ;", "String   readLine    =    reader . readLine (  )  ;", "assertEquals (  \" Compression / Decompression   error \"  ,    inputLine ,    readLine )  ;", "reader . close (  )  ;", "}", "METHOD_END"], "methodName": ["testPossiblyCompressedDecompressedStreams"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "int   wordSize    =     1  0  ;", "int   listSize    =     2  0  ;", "long   dataSize    =     (  1  0     *     1  0  2  4  )     *     1  0  2  4  ;", "Configuration   conf    =    new   Configuration (  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "CompressionEmulationUtil . setInputCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setInt ( RandomTextDataGenerator . GRIDMIX _ DATAGEN _ RANDOMTEXT _ LISTSIZE ,    listSize )  ;", "conf . setInt ( RandomTextDataGenerator . GRIDMIX _ DATAGEN _ RANDOMTEXT _ WORDSIZE ,    wordSize )  ;", "conf . setLong ( GenerateData . GRIDMIX _ GEN _ BYTES ,    dataSize )  ;", "conf . set (  \" mapreduce . job . hdfs - servers \"  ,     \"  \"  )  ;", "FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   tempDir    =    new   Path ( rootTempDir ,     \" TestRandomCompressedTextDataGenr \"  )  ;", "lfs . delete ( tempDir ,    true )  ;", ". runDataGenJob ( conf ,    tempDir )  ;", "FileStatus [  ]    files    =    lfs . listStatus ( tempDir ,    new   Utils . OutputFileUtils . OutputFilesFilter (  )  )  ;", "long   size    =     0  ;", "long   maxLineSize    =     0  ;", "for    ( FileStatus   status    :    files )     {", "InputStream   in    =    CompressionEmulationUtil . getPossiblyDecompressedInputStream ( status . getPath (  )  ,    conf ,     0  )  ;", "BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( in )  )  ;", "String   line    =    reader . readLine (  )  ;", "if    ( line    !  =    null )     {", "long   lineSize    =    line . getBytes (  )  . length ;", "if    ( lineSize    >    maxLineSize )     {", "maxLineSize    =    lineSize ;", "}", "while    ( line    !  =    null )     {", "for    ( String   word    :    line . split (  \"  \\  \\ s \"  )  )     {", "size    +  =    word . getBytes (  )  . length ;", "}", "line    =    reader . readLine (  )  ;", "}", "}", "reader . close (  )  ;", "}", "assertTrue (  ( size    >  =    dataSize )  )  ;", "assertTrue (  ( size    <  =     ( dataSize    +    maxLineSize )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRandomCompressedTextDataGenerator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestCompressionEmulationUtils"}, {"methodBody": ["METHOD_START", "{", "String   user    =    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ;", "conf . set (  \" user . name \"  ,    user )  ;", "String [  ]    dFiles    =    new   String [  ]  {     \" hdfs :  /  /  / tmp / file 1  . txt \"  ,     (  \"  / tmp /  \"     +    user )     +     \"  /  . staging / job _  1  / file 2  . txt \"  ,     \" hdfs :  /  /  / user / user 1  / file 3  . txt \"  ,     \"  / home / user 2  / file 4  . txt \"  ,     \" subdir 1  / file 5  . txt \"  ,     \" subdir 2  / file 6  . gz \"     }  ;", "String [  ]    fileSizes    =    new   String [  ]  {     \"  4  0  0  \"  ,     \"  2  5  0  0  \"  ,     \"  7  0  0  \"  ,     \"  1  2  0  0  \"  ,     \"  1  5  0  0  \"  ,     \"  5  0  0  \"     }  ;", "String [  ]    visibilities    =    new   String [  ]  {     \" true \"  ,     \" false \"  ,     \" false \"  ,     \" true \"  ,     \" true \"  ,     \" false \"     }  ;", "String [  ]    timeStamps    =    new   String [  ]  {     \"  1  2  3  4  \"  ,     \"  2  3  4  5  \"  ,     \"  3  4  5  6  7  \"  ,     \"  5  4  3  4  \"  ,     \"  1  2  5  \"  ,     \"  1  3  4  \"     }  ;", "conf . setStrings ( CACHE _ FILES ,    dFiles )  ;", "conf . setStrings ( CACHE _ FILES _ SIZES ,    fileSizes )  ;", "conf . setStrings ( CACHE _ FILE _ VISIBILITIES ,    visibilities )  ;", "conf . setStrings ( CACHE _ FILE _ TIMESTAMPS ,    timeStamps )  ;", "long [  ]    sortedFileSizes    =    new   long [  ]  {     1  5  0  0  ,     1  2  0  0  ,     7  0  0  ,     5  0  0  ,     4  0  0     }  ;", "return   sortedFileSizes ;", "}", "METHOD_END"], "methodName": ["configureDummyDistCacheFiles"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "DistributedCacheEmulator   dce    =    new   DistributedCacheEmulator ( conf ,    ioPath )  ;", "JobCreator   jobCreator    =    JobCreator . getPolicy ( conf ,    JobCreator . LOADJOB )  ;", "jobCreator . seor ( dce )  ;", "dce . init (  \" dummytrace \"  ,    jobCreator ,    generate )  ;", "return   dce ;", "}", "METHOD_END"], "methodName": ["createDistributedCacheEmulator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "Path   distCacheDir    =    dce . getDistributedCacheDir (  )  ;", "assertEquals (  (  \" Wrong   permissions   for   distributed   cache   dir    \"     +    distCacheDir )  ,    fs . getFileStatus ( distCacheDir )  . getPermission (  )  . getOtherAction (  )  . and ( EXECUTE )  ,    EXECUTE )  ;", "LongWritable   key    =    new   LongWritable (  )  ;", "BytesWritable   val    =    new   BytesWritable (  )  ;", "for    ( int   i    =     0  ;    i    <     ( sortedFileSizes . length )  ;    i +  +  )     {", "assertTrue (  (  \" Number   of   files   written   to   the   sequence   file   by    \"     +     \" setupGenerateData   is   less   than   the   expected .  \"  )  ,    reader . nextKeyValue (  )  )  ;", "key    =    reader . getCurrentKey (  )  ;", "val    =    reader . getCurrentValue (  )  ;", "long   fileSize    =    key . get (  )  ;", "String   file    =    new   String ( val . getBytes (  )  ,     0  ,    val . getLength (  )  )  ;", "assertEquals (  \" Dist   cache   file   size   is   wrong .  \"  ,    sortedFileSizes [ i ]  ,    fileSize )  ;", "Path   parent    =    new   Path ( file )  . getParent (  )  . makeQualified ( fs . getUri (  )  ,    fs . getWorkingDirectory (  )  )  ;", "assertTrue (  \" Public   dist   cache   file   path   is   wrong .  \"  ,    distCacheDir . equals ( parent )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doValidateSetupGenDC"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . initCluster ( TestDistCacheEmulation . class )  ;", "File   target    =    new   File (  (  (  \" target \"     +     ( File . separator )  )     +     ( TestDistCacheEmulation . class . getName (  )  )  )  )  ;", "if    (  !  ( target . exists (  )  )  )     {", "assertTrue ( target . mkdirs (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "jobConf . setStrings ( CACHE _ FILES ,     \"  \"  )  ;", "jobConf . setStrings ( CACHE _ FILES _ SIZES ,     \"  \"  )  ;", "jobConf . setStrings ( CACHE _ FILE _ TIMESTAMPS ,     \"  \"  )  ;", "jobConf . setStrings ( CACHE _ FILE _ VISIBILITIES ,     \"  \"  )  ;", "jobConf . setStrings (  \" cache . files \"  ,     \"  \"  )  ;", "jobConf . setStrings (  \" cache . files . filesizes \"  ,     \"  \"  )  ;", "jobConf . setStrings (  \" cache . files . visibilities \"  ,     \"  \"  )  ;", "jobConf . setStrings (  \" cache . files . timestamps \"  ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["resetDistCacheConfigProperties"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "long [  ]    fileSizes    =    configureDummyDistCacheFiles ( conf )  ;", "System . arraycopy ( fileSizes ,     0  ,    sortedFileSizes ,     0  ,    fileSizes . length )  ;", "final   int   numJobs    =     3  ;", "DebugJobProducer   jobProducer    =    new   DebugJobProducer ( numJobs ,    conf )  ;", "Configuration   jobConf    =    GridmixTestUtils . mrvl . getConfig (  )  ;", "Path   ioPath    =    new   Path (  \" testSetupGenerateDistCacheData \"  )  . makeQualified ( GridmixTestUtils . dfs . getUri (  )  ,    GridmixTestUtils . dfs . getWorkingDirectory (  )  )  ;", "FileSystem   fs    =    FileSystem . get ( jobConf )  ;", "if    ( fs . exists ( ioPath )  )     {", "fs . delete ( ioPath ,    true )  ;", "}", "FileSystem . mkdirs ( fs ,    ioPath ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "dce    =    createDistributedor ( jobConf ,    ioPath ,    generate )  ;", "int   exitCode    =    dce . setupGenerateDistCacheData ( jobProducer )  ;", "int   expectedExitCode    =     ( generate )     ?     0     :    Gridmix . MISSING _ DIST _ CACHE _ FILES _ ERROR ;", "assertEquals (  \" setupGenerateDistCacheData   failed .  \"  ,    expectedExitCode ,    exitCode )  ;", "resetDistCacheConfigProperties ( jobConf )  ;", "return   jobConf ;", "}", "METHOD_END"], "methodName": ["runSetupGenerateDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . shutdownCluster (  )  ;", "}", "METHOD_END"], "methodName": ["shutDown"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "Configuration   jobConf    =    GridmixTestUtils . mrvl . getConfig (  )  ;", "Path   ioPath    =    new   Path (  \" tConfigurability \"  )  . makeQualified ( GridmixTestUtils . dfs . getUri (  )  ,    GridmixTestUtils . dfs . getWorkingDirectory (  )  )  ;", "FileSystem   fs    =    FileSystem . get ( jobConf )  ;", "FileSystem . mkdirs ( fs ,    ioPath ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "dce    =    createDistributedCacheEmulator ( jobConf ,    ioPath ,    false )  ;", "assertTrue (  (  (  \" Default   configuration   of    \"     +     ( DistributedCacheEmulator . GRIDMIX _ EMULATE _ DISTRIBUTEDCACHE )  )     +     \"    is   wrong .  \"  )  ,    dce . shouldEmulateDistCacheLoad (  )  )  ;", "jobConf . setBoolean ( DistributedCacheEmulator . GRIDMIX _ EMULATE _ DISTRIBUTEDCACHE ,    false )  ;", "dce    =    createDistributedCacheEmulator ( jobConf ,    ioPath ,    false )  ;", "assertFalse (  (  (  \" Disabling   of   emulation   of   distributed   cache   load   by   setting    \"     +     ( DistributedCacheEmulator . GRIDMIX _ EMULATE _ DISTRIBUTEDCACHE )  )     +     \"    to   false   is   not   working .  \"  )  ,    dce . shouldEmulateDistCacheLoad (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDistCacheEmulationConfigurability"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "configureDummyDistCacheFiles ( conf )  ;", "File   ws    =    new   File (  (  (  \" target \"     +     ( File . separator )  )     +     ( this . getClass (  )  . getName (  )  )  )  )  ;", "Path   ioPath    =    new   Path ( ws . getAbsolutePath (  )  )  ;", "Distributedor   dce    =    new   Distributedor ( conf ,    ioPath )  ;", "JobConf   jobConf    =    new   JobConf ( conf )  ;", "jobConf . setUser ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  ;", "File   fin    =    new   File (  (  (  (  (  (  (  (  (  \" src \"     +     ( File . separator )  )     +     \" test \"  )     +     ( File . separator )  )     +     \" resources \"  )     +     ( File . separator )  )     +     \" data \"  )     +     ( File . separator )  )     +     \" wordcount . json \"  )  )  ;", "dce . init ( fin . getAbsolutePath (  )  ,    JobCreator . LOADJOB ,    true )  ;", "dce . configureDistCacheFiles ( conf ,    jobConf )  ;", "String [  ]    caches    =    conf . getStrings ( CACHE _ FILES )  ;", "String [  ]    tmpfiles    =    conf . getStrings (  \" tmpfiles \"  )  ;", "assertEquals (  6  ,     (  ( caches    =  =    null    ?     0     :    caches . length )     +     ( tmpfiles    =  =    null    ?     0     :    tmpfiles . length )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDistCacheEmulator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "long [  ]    sortedFileSizes    =    new   long [  5  ]  ;", "Configuration   jobConf    =    runSetupGenerateData ( true ,    sortedFileSizes )  ;", "GridmixJob   gridmixJob    =    new   GenerateData ( jobConf )  ;", "Job   job    =    gridmixJob . call (  )  ;", "assertEquals (  \" Number   of   reduce   tasks   in   GenerateData   is   not    0  .  \"  ,     0  ,    job . getNumReduceTasks (  )  )  ;", "assertTrue (  \" GenerateData   job   failed .  \"  ,    job . waitForCompletion ( false )  )  ;", "validateData ( jobConf ,    sortedFileSizes )  ;", "}", "METHOD_END"], "methodName": ["testGenerateDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "long [  ]    sortedFileSizes    =    new   long [  5  ]  ;", "Configuration   jobConf    =    runSetupGenerateData ( true ,    sortedFileSizes )  ;", "validateSetupGenDC ( jobConf ,    sortedFileSizes )  ;", "runSetupGenerateData ( false ,    sortedFileSizes )  ;", "}", "METHOD_END"], "methodName": ["testSetupGenerateDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "Path   distCachePath    =    dce . getDistributedCacheDir (  )  ;", "String   filesListFile    =    jobConf . get ( GenerateData . GRIDMIX _ DISTCACHE _ FILE _ LIST )  ;", "FileSystem   fs    =    FileSystem . get ( jobConf )  ;", "Path   listFile    =    new   Path ( filesListFile )  ;", "assertTrue (  \" Path   of   Distributed   Cache   files   list   file   is   wrong .  \"  ,    distCachePath . equals ( listFile . getParent (  )  . makeQualified ( fs . getUri (  )  ,    fs . getWorkingDirectory (  )  )  )  )  ;", "assertTrue (  (  \" Failed   to   delete   distributed   Cache   files   list   file    \"     +    listFile )  ,    fs . delete ( listFile ,    true )  )  ;", "List < Long >    fileSizes    =    new   ArrayList < Long >  (  )  ;", "for    ( long   size    :    sortedFileSizes )     {", "fileSizes . add ( size )  ;", "}", "validateFiles ( fileSizes ,    distCachePath )  ;", "}", "METHOD_END"], "methodName": ["validateDistCacheData"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "FileStatus [  ]    statuses    =    GridmixTestUtils . dfs . listStatus ( distCacheDir )  ;", "int   numFiles    =    filesSizesExpected . size (  )  ;", "assertEquals (  \" Number   of   files   under   distributed   cache   dir   is   wrong .  \"  ,    numFiles ,    statuses . length )  ;", "for    ( int   i    =     0  ;    i    <    numFiles ;    i +  +  )     {", "FileStatus   stat    =    statuses [ i ]  ;", "assertTrue (  (  (  \" File   size   of   distributed   cache   file    \"     +     ( stat . getPath (  )  . toUri (  )  . getPath (  )  )  )     +     \"    is   wrong .  \"  )  ,    filesSizesExpected . remove ( stat . getLen (  )  )  )  ;", "FsPermission   perm    =    stat . getPermission (  )  ;", "assertEquals (  (  \" Wrong   permissions   for   distributed   cache   file    \"     +     ( stat . getPath (  )  . toUri (  )  . getPath (  )  )  )  ,    new   FsPermission ( GenerateData . GRIDMIX _ DISTCACHE _ FILE _ PERM )  ,    perm )  ;", "}", "}", "METHOD_END"], "methodName": ["validateDistCacheFiles"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "long   sumOfFileSizes    =     0  ;", "for    ( int   i    =     0  ;    i    <     ( sortedFileSizes . length )  ;    i +  +  )     {", "sumOfFileSizes    +  =    sortedFileSizes [ i ]  ;", "}", "FileSystem   fs    =    FileSystem . get ( jobConf )  ;", "assertEquals (  \" Number   of   distributed   cache   files   to   be   generated   is   wrong .  \"  ,    sortedFileSizes . length ,    jobConf . getInt ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ FILE _ COUNT ,     (  -  1  )  )  )  ;", "assertEquals (  \" Total   size   of   dist   cache   files   to   be   generated   is   wrong .  \"  ,    sumOfFileSizes ,    jobConf . getLong ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ BYTE _ COUNT ,     (  -  1  )  )  )  ;", "Path   filesListFile    =    new   Path ( jobConf . get ( GenerateDistCacheData . GRIDMIX _ DISTCACHE _ FILE _ LIST )  )  ;", "FileStatus   stat    =    fs . getFileStatus ( filesListFile )  ;", "assertEquals (  (  \" Wrong   permissions   of   dist   Cache   files   list   file    \"     +    filesListFile )  ,    new   FsPermission (  (  ( short )     (  4  2  0  )  )  )  ,    stat . getPermission (  )  )  ;", "InputSplit   split    =    new   uce . lib . input . FileSplit ( filesListFile ,     0  ,    stat . getLen (  )  ,     (  ( String [  ]  )     ( null )  )  )  ;", "TaskAttemptContext   taskContext    =    MapReduceTestUtil . createDummyMapTaskAttemptContext ( jobConf )  ;", "RecordReader < LongWritable ,    BytesWritable >    reader    =    new   GenerateDistCacheData . GenDCDataFormat (  )  . createRecordReader ( split ,    taskContext )  ;", "MapContext < LongWritable ,    BytesWritable ,    NullWritable ,    BytesWritable >    mapContext    =    new   uce . task . MapContextImpl < LongWritable ,    BytesWritable ,    NullWritable ,    BytesWritable >  ( jobConf ,    taskContext . getTaskAttemptID (  )  ,    reader ,    null ,    null ,    MapReduceTestUtil . createDummyReporter (  )  ,    split )  ;", "reader . initialize ( split ,    mapContext )  ;", "doValidateSetupGenDC ( reader ,    fs ,    sortedFileSizes )  ;", "}", "METHOD_END"], "methodName": ["validateSetupGenDC"], "fileName": "org.apache.hadoop.mapred.gridmix.TestDistCacheEmulation"}, {"methodBody": ["METHOD_START", "{", "long   splitBytes    =     0 L ;", "HashSet < Path >    uniq    =    new   HashSet < Path >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( split . getNumPaths (  )  )  ;     +  + i )     {", "splitBytes    +  =    split . getLength ( i )  ;", "assertTrue (  (  ( split . getLength ( i )  )     <  =     ( fs . geStatus ( split . getPath ( i )  )  . getLen (  )  )  )  )  ;", "assertFalse ( uniq . contains ( split . getPath ( i )  )  )  ;", "uniq . add ( split . getPath ( i )  )  ;", "}", "assertEquals ( bytes ,    splitBytes )  ;", "}", "METHOD_END"], "methodName": ["checkSplitEq"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   fs    =    FileSystem . getLocal ( conf )  . getRaw (  )  ;", "fs . delete (  . base ,    true )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   fs    =    FileSystem . getLocal ( conf )  . getRaw (  )  ;", "return   new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  ,     \" t \"  )  . makeQualified ( fs )  ;", "}    catch    ( IOException   e )     {", "fail (  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getBaseDir"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   fs    =    FileSystem . getLocal ( conf )  . getRaw (  )  ;", "fs . delete (  . base ,    true )  ;", "final   Random   r    =    new   Random (  )  ;", "final   long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", ". LOG . info (  (  \" seed :     \"     +    seed )  )  ;", "fs . mkdirs (  . base )  ;", "for    ( int   i    =     0  ;    i    <     (  . NFILES )  ;     +  + i )     {", "Path   file    =     . base ;", "for    ( double   d    =     0  .  6  ;    d    >     0  .  0  ;    d    *  =     0  .  8  )     {", "if    (  ( r . nextDouble (  )  )     <    d )     {", "file    =    new   Path (  . base ,    Integer . toString ( r . nextInt (  3  )  )  )  ;", "continue ;", "}", "break ;", "}", "OutputStream   out    =    null ;", "try    {", "out    =    fs . create ( new   Path ( file ,     (  \"  \"     +     (  ( char )     (  ' A '     +    i )  )  )  )  )  ;", "final   byte [  ]    b    =    new   byte [  1  0  2  4  ]  ;", "Arrays . fill ( b ,     (  ( byte )     (  ' A '     +    i )  )  )  ;", "for    ( int   len    =     (  ( i    %     1  3  )     +     1  )     *     1  0  2  4  ;    len    >     0  ;    len    -  =     1  0  2  4  )     {", "out . write ( b )  ;", "}", "}    finally    {", "if    ( out    !  =    null )     {", "out . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   Configuration   conf    =    new   Configuration (  )  ;", "conf . setLong ( FilePool . GRIDMIX _ MIN _ FILE ,     (  3     *     1  0  2  4  )  )  ;", "final   FilePool   pool    =    new   FilePool ( conf ,     . base )  ;", "pool . refresh (  )  ;", "final   ArrayList < FileStatus >    files    =    new   ArrayList < FileStatus >  (  )  ;", "final   int   expectedPoolSize    =     (  (  (  (  . NFILES )     /     2  )     *     (  (  (  . NFILES )     /     2  )     +     1  )  )     -     6  )     *     1  0  2  4  ;", "assertEquals ( expectedPoolSize ,    pool . getInputFiles ( Long . MAX _ VALUE ,    files )  )  ;", "assertEquals (  (  (  . NFILES )     -     4  )  ,    files . size (  )  )  ;", "files . clear (  )  ;", "assertEquals ( expectedPoolSize ,    pool . getInputFiles ( expectedPoolSize ,    files )  )  ;", "files . clear (  )  ;", "final   long   rand    =    r . nextInt ( expectedPoolSize )  ;", "assertTrue (  (  \" Missed :     \"     +    rand )  ,     (  (  (  (  . NFILES )     /     2  )     *     1  0  2  4  )     >     ( rand    -     ( pool . getInputFiles ( rand ,    files )  )  )  )  )  ;", "conf . setLong ( FilePool . GRIDMIX _ MIN _ FILE ,     0  )  ;", "pool . refresh (  )  ;", "files . clear (  )  ;", "assertEquals (  (  (  (  (  . NFILES )     /     2  )     *     (  (  (  . NFILES )     /     2  )     +     1  )  )     *     1  0  2  4  )  ,    pool . getInputFiles ( Long . MAX _ VALUE ,    files )  )  ;", "}", "METHOD_END"], "methodName": ["testPool"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   fs    =    FileSystem . getLocal ( conf )  . getRaw (  )  ;", "conf . setLong ( FilePool . GRIDMIX _ MIN _ FILE ,     (  3     *     1  0  2  4  )  )  ;", "final   FilePool   pool    =    new   FilePool ( conf ,     . base )     {", "@ Override", "public   BlockLocation [  ]    locationsFor ( FileStatus   stat ,    long   start ,    long   len )    throws   IOException    {", "return   new   BlockLocation [  ]  {    new   BlockLocation (  )     }  ;", "}", "}  ;", "pool . refresh (  )  ;", "final   int   expectedPoolSize    =     (  (  (  (  . NFILES )     /     2  )     *     (  (  (  . NFILES )     /     2  )     +     1  )  )     -     6  )     *     1  0  2  4  ;", "final   InputStriper   striper    =    new   InputStriper ( pool ,    expectedPoolSize )  ;", "int   last    =     0  ;", "for    ( int   i    =     0  ;    i    <    expectedPoolSize ;    last    =    Math . min (  ( expectedPoolSize    -    i )  ,    r . nextInt ( expectedPoolSize )  )  )     {", "checkSplitEq ( fs ,    striper . splitFor ( pool ,    last ,     0  )  ,    last )  ;", "i    +  =    last ;", "}", "final   InputStriper   striper 2     =    new   InputStriper ( pool ,    expectedPoolSize )  ;", "checkSplitEq ( fs ,    striper 2  . splitFor ( pool ,    expectedPoolSize ,     0  )  ,    expectedPoolSize )  ;", "}", "METHOD_END"], "methodName": ["testStriper"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   Configuration   conf    =    new   Configuration (  )  ;", "conf . setLong ( FilePool . GRIDMIX _ MIN _ FILE ,     (  1  4     *     1  0  2  4  )  )  ;", "final   FilePool   pool    =    new   FilePool ( conf ,     . base )  ;", "pool . refresh (  )  ;", "}    catch    ( IOException   e )     {", "return ;", "}", "fail (  )  ;", "}", "METHOD_END"], "methodName": ["testUnsuitable"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFilePool"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   fs    =    FileSystem . getLocal ( conf )  . getRaw (  )  ;", "final   Path   p    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  ,     \" t \"  )  . makeQualified ( fs )  ;", "fs . delete ( p ,    true )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFileQueue"}, {"methodBody": ["METHOD_START", "{", "final   byte [  ]    b    =    new   byte [ TestFileQueue . BLOCK ]  ;", "final   ByteArrayOutputStream   out    =    new   ByteArrayOutputStream (  )  ;", "for    ( int   i    =     0  ;    i    <     ( TestFileQueue . NFILES )  ;     +  + i )     {", "Arrays . fill ( b ,     (  ( byte )     (  ' A '     +    i )  )  )  ;", "out . write ( b ,     0  ,     (  ( int )     ( TestFileQueue . len [ i ]  )  )  )  ;", "}", "return   out ;", "}", "METHOD_END"], "methodName": ["fillVerif"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFileQueue"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   fs    =    FileSystem . getLocal ( conf )  . getRaw (  )  ;", "final   Path   p    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  ,     \" testFileQueue \"  )  . makeQualified ( fs )  ;", "fs . delete ( p ,    true )  ;", "final   byte [  ]    b    =    new   byte [  . BLOCK ]  ;", "for    ( int   i    =     0  ;    i    <     (  . NFILES )  ;     +  + i )     {", "Arrays . fill ( b ,     (  ( byte )     (  ' A '     +    i )  )  )  ;", ". paths [ i ]     =    new   Path ( p ,     (  \"  \"     +     (  ( char )     (  ' A '     +    i )  )  )  )  ;", "OutputStream   f    =    null ;", "try    {", "f    =    fs . create (  . paths [ i ]  )  ;", "f . write ( b )  ;", "}    finally    {", "if    ( f    !  =    null )     {", "f . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFileQueue"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileQueue   q    =    new   FileQueue ( new   uce . lib . input . CombineFileSplit ( new   Path [  0  ]  ,    new   long [  0  ]  ,    new   long [  0  ]  ,    new   String [  0  ]  )  ,    conf )  ;", "}", "METHOD_END"], "methodName": ["testEmpty"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFileQueue"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "Arrays . fill ( TestFileQueue . loc ,     \"  \"  )  ;", "Arrays . fill ( TestFileQueue . start ,     0 L )  ;", "Arrays . fill ( TestFileQueue . len ,    TestFileQueue . BLOCK )  ;", "final   ByteArrayOutputStream   out    =    TestFileQueue . fillVerif (  )  ;", "final   FileQueue   q    =    new   FileQueue ( new   uce . lib . input . CombineFileSplit ( TestFileQueue . paths ,    TestFileQueue . start ,    TestFileQueue . len ,    TestFileQueue . loc )  ,    conf )  ;", "final   byte [  ]    verif    =    out . toByteArray (  )  ;", "final   byte [  ]    check    =    new   byte [  (  2     *     ( TestFileQueue . NFILES )  )     *     ( TestFileQueue . BLOCK )  ]  ;", "q . read ( check ,     0  ,     (  ( TestFileQueue . NFILES )     *     ( TestFileQueue . BLOCK )  )  )  ;", "assertArrayEquals ( verif ,    Arrays . copyOf ( check ,     (  ( TestFileQueue . NFILES )     *     ( TestFileQueue . BLOCK )  )  )  )  ;", "final   byte [  ]    verif 2     =    new   byte [  (  2     *     ( TestFileQueue . NFILES )  )     *     ( TestFileQueue . BLOCK )  ]  ;", "System . arraycopy ( verif ,     0  ,    verif 2  ,     0  ,    verif . length )  ;", "System . arraycopy ( verif ,     0  ,    verif 2  ,    verif . length ,    verif . length )  ;", "q . read ( check ,     0  ,     (  (  2     *     ( TestFileQueue . NFILES )  )     *     ( TestFileQueue . BLOCK )  )  )  ;", "assertArrayEquals ( verif 2  ,    check )  ;", "}", "METHOD_END"], "methodName": ["testRepeat"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFileQueue"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "Arrays . fill ( TestFileQueue . loc ,     \"  \"  )  ;", "Arrays . fill ( TestFileQueue . start ,     0 L )  ;", "Arrays . fill ( TestFileQueue . len ,    TestFileQueue . BLOCK )  ;", "final   int   B 2     =     ( TestFileQueue . BLOCK )     /     2  ;", "for    ( int   i    =     0  ;    i    <     ( TestFileQueue . NFILES )  ;    i    +  =     2  )     {", "TestFileQueue . start [ i ]     +  =    B 2  ;", "TestFileQueue . len [ i ]     -  =    B 2  ;", "}", "final   FileQueue   q    =    new   FileQueue ( new   uce . lib . input . CombineFileSplit ( TestFileQueue . paths ,    TestFileQueue . start ,    TestFileQueue . len ,    TestFileQueue . loc )  ,    conf )  ;", "final   ByteArrayOutputStream   out    =    TestFileQueue . fillVerif (  )  ;", "final   byte [  ]    verif    =    out . toByteArray (  )  ;", "final   byte [  ]    check    =    new   byte [  (  (  ( TestFileQueue . NFILES )     /     2  )     *     ( TestFileQueue . BLOCK )  )     +     (  (  ( TestFileQueue . NFILES )     /     2  )     *    B 2  )  ]  ;", "q . read ( check ,     0  ,    verif . length )  ;", "assertArrayEquals ( verif ,    Arrays . copyOf ( check ,    verif . length )  )  ;", "q . read ( check ,     0  ,    verif . length )  ;", "assertArrayEquals ( verif ,    Arrays . copyOf ( check ,    verif . length )  )  ;", "}", "METHOD_END"], "methodName": ["testUneven"], "fileName": "org.apache.hadoop.mapred.gridmix.TestFileQueue"}, {"methodBody": ["METHOD_START", "{", "Path [  ]    files    =    new   Path [  ]  {    new   Path (  \" one \"  )  ,    new   Path (  \" two \"  )     }  ;", "long [  ]    start    =    new   long [  ]  {     1  ,     2     }  ;", "long [  ]    lengths    =    new   long [  ]  {     1  0  0  ,     2  0  0     }  ;", "String [  ]    locations    =    new   String [  ]  {     \" locOne \"  ,     \" loctwo \"     }  ;", "CombineFileSplit   cfSplit    =    new   CombineFileSplit ( files ,    start ,    lengths ,    locations )  ;", "ResourceUsageMetrics   metrics    =    new   ResourceUsageMetrics (  )  ;", "metrics . setCumulativeCpuUsage (  2  0  0  )  ;", "ResourceUsageMetrics [  ]    rMetrics    =    new   ResourceUsageMetrics [  ]  {    metrics    }  ;", "double [  ]    uceBytes    =    new   double [  ]  {     8  .  1  ,     8  .  2     }  ;", "double [  ]    uceRecords    =    new   double [  ]  {     9  .  1  ,     9  .  2     }  ;", "long [  ]    uceOutputBytes    =    new   long [  ]  {     1  0  1 L ,     1  0  2 L    }  ;", "long [  ]    uceOutputRecords    =    new   long [  ]  {     1  1  1 L ,     1  1  2 L    }  ;", "return   new   LoadSplit ( cfSplit ,     2  ,     1  ,     4 L ,     5 L ,     6 L ,     7 L ,    uceBytes ,    uceRecords ,    uceOutputBytes ,    uceOutputRecords ,    metrics ,    rMetrics )  ;", "}", "METHOD_END"], "methodName": ["getLoadSplit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "String [  ]    locations    =    new   String [  ]  {     \" locOne \"  ,     \" loctwo \"     }  ;", "long [  ]    uceDurations    =    new   long [  ]  {     1  0  1 L ,     1  0  2 L    }  ;", "return   new   SleepJob . SleepSplit (  0  ,     2  0  0  0 L ,    uceDurations ,     2  ,    locations )  ;", "}", "METHOD_END"], "methodName": ["getSleepSplit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Path   outRoot    =    new   Path (  \" target \"  )  ;", "JobStory   jobDesc    =    mock ( JobStory . class )  ;", "when ( jobDesc . getName (  )  )  . thenReturn (  \" JobName \"  )  ;", "when ( jobDesc . getJobConf (  )  )  . thenReturn ( new   JobConf ( conf )  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getCurrentUser (  )  ;", "GridmixJob   j 1     =    new   LoadJob ( conf ,     1  0  0  0 L ,    jobDesc ,    outRoot ,    ugi ,     0  )  ;", "GridmixJob   j 2     =    new   LoadJob ( conf ,     1  0  0  0 L ,    jobDesc ,    outRoot ,    ugi ,     0  )  ;", "GridmixJob   j 3     =    new   LoadJob ( conf ,     1  0  0  0 L ,    jobDesc ,    outRoot ,    ugi ,     1  )  ;", "GridmixJob   j 4     =    new   LoadJob ( conf ,     1  0  0  0 L ,    jobDesc ,    outRoot ,    ugi ,     1  )  ;", "assertTrue ( j 1  . equals ( j 2  )  )  ;", "assertEquals (  0  ,    j 1  . compareTo ( j 2  )  )  ;", "assertFalse ( j 1  . equals ( j 3  )  )  ;", "assertEquals (  (  -  1  )  ,    j 1  . compareTo ( j 3  )  )  ;", "assertEquals (  (  -  1  )  ,    j 1  . compareTo ( j 4  )  )  ;", "}", "METHOD_END"], "methodName": ["testCompareGridmixJob"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "GridmixJob . SpecGroupingComparator   test    =    new   GridmixJob . SpecGroupingComparator (  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   dos    =    new   DataOutputStream ( data )  ;", "WritableUtils . writeVInt ( dos ,     2  )  ;", "WritableUtils . writeVInt ( dos ,     1  )  ;", "WritableUtils . writeVInt ( dos ,     0  )  ;", "WritableUtils . writeVInt ( dos ,     7  )  ;", "WritableUtils . writeVInt ( dos ,     4  )  ;", "byte [  ]    b 1     =    data . toByteArray (  )  ;", "byte [  ]    b 2     =    data . toByteArray (  )  ;", "assertEquals (  0  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     0  ,     1  )  )  ;", "b 2  [  2  ]     =     1  ;", "assertEquals (  (  -  1  )  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     0  ,     1  )  )  ;", "b 2  [  2  ]     =     1  ;", "assertEquals (  (  -  1  )  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     0  ,     1  )  )  ;", "assertEquals (  0  ,    test . compare ( new   GridmixKey ( GridmixKey . DATA ,     1  0  0  ,     2  )  ,    new   GridmixKey ( GridmixKey . DATA ,     1  0  0  ,     2  )  )  )  ;", "assertEquals (  (  -  1  )  ,    test . compare ( new   GridmixKey ( GridmixKey . REDUCE _ SPEC ,     1  0  0  ,     2  )  ,    new   GridmixKey ( GridmixKey . DATA ,     1  0  0  ,     2  )  )  )  ;", "assertEquals (  1  ,    test . compare ( new   GridmixKey ( GridmixKey . DATA ,     1  0  0  ,     2  )  ,    new   GridmixKey ( GridmixKey . REDUCE _ SPEC ,     1  0  0  ,     2  )  )  )  ;", "assertEquals (  2  ,    test . compare ( new   GridmixKey ( GridmixKey . DATA ,     1  0  2  ,     2  )  ,    new   GridmixKey ( GridmixKey . DATA ,     1  0  0  ,     2  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testGridmixJobSpecGroupingComparator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "Path [  ]    files    =    new   Path [  ]  {    new   Path (  \" one \"  )  ,    new   Path (  \" two \"  )     }  ;", "long [  ]    start    =    new   long [  ]  {     1  ,     2     }  ;", "long [  ]    lengths    =    new   long [  ]  {     1  0  0  ,     2  0  0     }  ;", "String [  ]    locations    =    new   String [  ]  {     \" locOne \"  ,     \" loctwo \"     }  ;", "CombineFileSplit   cfSplit    =    new   CombineFileSplit ( files ,    start ,    lengths ,    locations )  ;", "ResourceUsageMetrics   metrics    =    new   ResourceUsageMetrics (  )  ;", "metrics . setCumulativeCpuUsage (  2  0  0  )  ;", "double [  ]    reduceBytes    =    new   double [  ]  {     8  .  1  ,     8  .  2     }  ;", "double [  ]    reduceRecords    =    new   double [  ]  {     9  .  1  ,     9  .  2     }  ;", "long [  ]    reduceOutputBytes    =    new   long [  ]  {     1  0  1 L ,     1  0  2 L    }  ;", "long [  ]    reduceOutputRecords    =    new   long [  ]  {     1  1  1 L ,     1  1  2 L    }  ;", "GSplit   test    =    new   GSplit ( cfSplit ,     2  ,     3  ,     4 L ,     5 L ,     6 L ,     7 L ,    reduceBytes ,    reduceRecords ,    reduceOutputBytes ,    reduceOutputRecords )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   out    =    new   DataOutputStream ( data )  ;", "test . write ( out )  ;", "GSplit   copy    =    new   GSplit (  )  ;", "copy . readFields ( new   DataInputStream ( new   ByteArrayInputStream ( data . toByteArray (  )  )  )  )  ;", "assertEquals ( test . getId (  )  ,    copy . getId (  )  )  ;", "assertEquals ( test . getMapCount (  )  ,    copy . getMapCount (  )  )  ;", "assertEquals ( test . getInputRecords (  )  ,    copy . getInputRecords (  )  )  ;", "assertEquals ( test . getOutputBytes (  )  [  0  ]  ,    copy . getOutputBytes (  )  [  0  ]  )  ;", "assertEquals ( test . getOutputRecords (  )  [  0  ]  ,    copy . getOutputRecords (  )  [  0  ]  )  ;", "assertEquals ( test . getReduceBytes (  0  )  ,    copy . getReduceBytes (  0  )  )  ;", "assertEquals ( test . getReduceRecords (  0  )  ,    copy . getReduceRecords (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testGridmixSplit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "LoadJob . LoadRecordReader   test    =    new   LoadJob . LoadRecordReader (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "FileSystem   fs 1     =    mock ( FileSystem . class )  ;", "when ( fs 1  . open (  (  ( Path )     ( anyObject (  )  )  )  )  )  . thenReturn ( new   TestGridMixClasses . FakeFSDataInputStream ( new   TestGridMixClasses . FakeInputStream (  )  )  )  ;", "Path   p 1     =    mock ( Path . class )  ;", "when ( p 1  . getFileSystem (  (  ( JobConf )     ( anyObject (  )  )  )  )  )  . thenReturn ( fs 1  )  ;", "FileSystem   fs 2     =    mock ( FileSystem . class )  ;", "when ( fs 2  . open (  (  ( Path )     ( anyObject (  )  )  )  )  )  . thenReturn ( new   TestGridMixClasses . FakeFSDataInputStream ( new   TestGridMixClasses . FakeInputStream (  )  )  )  ;", "Path   p 2     =    mock ( Path . class )  ;", "when ( p 2  . getFileSystem (  (  ( JobConf )     ( anyObject (  )  )  )  )  )  . thenReturn ( fs 2  )  ;", "Path [  ]    paths    =    new   Path [  ]  {    p 1  ,    p 2     }  ;", "long [  ]    start    =    new   long [  ]  {     0  ,     0     }  ;", "long [  ]    lengths    =    new   long [  ]  {     1  0  0  0  ,     1  0  0  0     }  ;", "String [  ]    locations    =    new   String [  ]  {     \" temp 1  \"  ,     \" temp 2  \"     }  ;", "CombineFileSplit   cfsplit    =    new   CombineFileSplit ( paths ,    start ,    lengths ,    locations )  ;", "double [  ]    reduceBytes    =    new   double [  ]  {     1  0  0  ,     1  0  0     }  ;", "double [  ]    reduceRecords    =    new   double [  ]  {     2  ,     2     }  ;", "long [  ]    reduceOutputBytes    =    new   long [  ]  {     5  0  0  ,     5  0  0     }  ;", "long [  ]    reduceOutputRecords    =    new   long [  ]  {     2  ,     2     }  ;", "ResourceUsageMetrics   metrics    =    new   ResourceUsageMetrics (  )  ;", "ResourceUsageMetrics [  ]    rMetrics    =    new   ResourceUsageMetrics [  ]  {    new   ResourceUsageMetrics (  )  ,    new   ResourceUsageMetrics (  )     }  ;", "LoadSplit   input    =    new   LoadSplit ( cfsplit ,     2  ,     3  ,     1  5  0  0 L ,     2 L ,     3  0  0  0 L ,     2 L ,    reduceBytes ,    reduceRecords ,    reduceOutputBytes ,    reduceOutputRecords ,    metrics ,    rMetrics )  ;", "TaskAttemptID   taskId    =    new   TaskAttemptID (  )  ;", "TaskAttemptContext   ctx    =    new   uce . task . TaskAttemptContextImpl ( conf ,    taskId )  ;", "test . initialize ( input ,    ctx )  ;", "GridmixRecord   gr    =    test . getCurrentValue (  )  ;", "int   counter    =     0  ;", "while    ( test . nextKeyValue (  )  )     {", "gr    =    test . getCurrentValue (  )  ;", "if    ( counter    =  =     0  )     {", "assertEquals (  0  .  5  ,    test . getProgress (  )  ,     0  .  0  0  1  )  ;", "} else", "if    ( counter    =  =     1  )     {", "assertEquals (  1  .  0  ,    test . getProgress (  )  ,     0  .  0  0  1  )  ;", "}", "assertEquals (  1  0  0  0  ,    gr . getSize (  )  )  ;", "counter +  +  ;", "}", "assertEquals (  1  0  0  0  ,    gr . getSize (  )  )  ;", "assertEquals (  2  ,    counter )  ;", "test . close (  )  ;", "}", "METHOD_END"], "methodName": ["testLoadJobLoadRecordReader"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "LoadJob . LoadReducer   test    =    new   LoadJob . LoadReducer (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( NUM _ REDUCES ,     2  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( COMPRESS ,    true )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  ;", "TaskAttemptID   taskid    =    new   TaskAttemptID (  )  ;", "RawKeyValueIterator   input    =    new   TestGridMixClasses . FakeRawKeyValueIterator (  )  ;", "Counter   counter    =    new   GenericCounter (  )  ;", "Counter   inputValueCounter    =    new   GenericCounter (  )  ;", "TestGridMixClasses . LoadRecordWriter   output    =    new   TestGridMixClasses . LoadRecordWriter (  )  ;", "OutputCommitter   committer    =    new   CustomOutputCommitter (  )  ;", "StatusReporter   reporter    =    new   DummyReporter (  )  ;", "RawComparator < GridmixKey >    comparator    =    new   TestGridMixClasses . FakeRawComparator (  )  ;", "ReduceContext < GridmixKey ,    GridmixRecord ,    NullWritable ,    GridmixRecord >    reduceContext    =    new   uce . task . ReduceContextImpl < GridmixKey ,    GridmixRecord ,    NullWritable ,    GridmixRecord >  ( conf ,    taskid ,    input ,    counter ,    inputValueCounter ,    output ,    committer ,    reporter ,    comparator ,    GridmixKey . class ,    GridmixRecord . class )  ;", "reduceContext . nextKeyValue (  )  ;", "Context   context    =    new   uce . lib . reduce . WrappedReducer < GridmixKey ,    GridmixRecord ,    NullWritable ,    GridmixRecord >  (  )  . getReducerContext ( reduceContext )  ;", "test . run ( context )  ;", "assertEquals (  9  ,    counter . getValue (  )  )  ;", "assertEquals (  1  0  ,    inputValueCounter . getValue (  )  )  ;", "assertEquals (  1  ,    output . getData (  )  . size (  )  )  ;", "GridmixRecord   record    =    output . getData (  )  . values (  )  . iterator (  )  . next (  )  ;", "assertEquals (  1  5  9  3  ,    record . getSize (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadJobLoadReducer"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "LoadJob . LoadSortComparator   test    =    new   LoadJob . LoadSortComparator (  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   dos    =    new   DataOutputStream ( data )  ;", "WritableUtils . writeVInt ( dos ,     2  )  ;", "WritableUtils . writeVInt ( dos ,     1  )  ;", "WritableUtils . writeVInt ( dos ,     4  )  ;", "WritableUtils . writeVInt ( dos ,     7  )  ;", "WritableUtils . writeVInt ( dos ,     4  )  ;", "byte [  ]    b 1     =    data . toByteArray (  )  ;", "byte [  ]    b 2     =    data . toByteArray (  )  ;", "rtEquals (  0  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     0  ,     1  )  )  ;", "b 2  [  2  ]     =     5  ;", "rtEquals (  (  -  1  )  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     0  ,     1  )  )  ;", "b 2  [  2  ]     =     2  ;", "rtEquals (  2  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     0  ,     1  )  )  ;", "b 2  [  2  ]     =     4  ;", "rtEquals (  1  ,    test . compare ( b 1  ,     0  ,     1  ,    b 2  ,     1  ,     1  )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadJobLoadSortComparator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( NUM _ REDUCES ,     2  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  ;", "TaskAttemptID   taskId    =    new   TaskAttemptID (  )  ;", "RecordReader < NullWritable ,    GridmixRecord >    reader    =    new   TestGridMixClasses . FakeRecordReader (  )  ;", "TestGridMixClasses . LoadRecordGkGrWriter   writer    =    new   TestGridMixClasses . LoadRecordGkGrWriter (  )  ;", "OutputCommitter   committer    =    new   CustomOutputCommitter (  )  ;", "StatusReporter   reporter    =    new   TaskAttemptContextImpl . DummyReporter (  )  ;", "LoadSplit   split    =    getLoadSplit (  )  ;", "MapContext < NullWritable ,    GridmixRecord ,    GridmixKey ,    GridmixRecord >    mapContext    =    new   uce . task . MapContextImpl < NullWritable ,    GridmixRecord ,    GridmixKey ,    GridmixRecord >  ( conf ,    taskId ,    reader ,    writer ,    committer ,    reporter ,    split )  ;", "Context   ctx    =    new   uce . lib . map . WrappedMapper < NullWritable ,    GridmixRecord ,    GridmixKey ,    GridmixRecord >  (  )  . getMapContext ( mapContext )  ;", "reader . initialize ( split ,    ctx )  ;", "ctx . getConfiguration (  )  . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( ctx . getConfiguration (  )  ,    true )  ;", "LoadJob . LoadMapper   mapper    =    new   LoadJob . LoadMapper (  )  ;", "mapper . run ( ctx )  ;", "Map < GridmixKey ,    GridmixRecord >    data    =    writer . getData (  )  ;", "assertEquals (  2  ,    data . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadMapper"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "LoadSplit   test    =    getLoadSplit (  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   out    =    new   DataOutputStream ( data )  ;", "test . write ( out )  ;", "LoadSplit   copy    =    new   LoadSplit (  )  ;", "copy . readFields ( new   DataInputStream ( new   ByteArrayInputStream ( data . toByteArray (  )  )  )  )  ;", "rtEquals ( test . getId (  )  ,    copy . getId (  )  )  ;", "rtEquals ( test . getMapCount (  )  ,    copy . getMapCount (  )  )  ;", "rtEquals ( test . getInputRecords (  )  ,    copy . getInputRecords (  )  )  ;", "rtEquals ( test . getOutputBytes (  )  [  0  ]  ,    copy . getOutputBytes (  )  [  0  ]  )  ;", "rtEquals ( test . getOutputRecords (  )  [  0  ]  ,    copy . getOutputRecords (  )  [  0  ]  )  ;", "rtEquals ( test . getReduceBytes (  0  )  ,    copy . getReduceBytes (  0  )  )  ;", "rtEquals ( test . getReduceRecords (  0  )  ,    copy . getReduceRecords (  0  )  )  ;", "rtEquals ( test . getMapResourceUsageMetrics (  )  . getCumulativeCpuUsage (  )  ,    copy . getMapResourceUsageMetrics (  )  . getCumulativeCpuUsage (  )  )  ;", "rtEquals ( test . getReduceResourceUsageMetrics (  0  )  . getCumulativeCpuUsage (  )  ,    copy . getReduceResourceUsageMetrics (  0  )  . getCumulativeCpuUsage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadSplit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   rf    =    new   TestGridMixClasses . FakeRecordFactory (  )  ;", "TestGridMixClasses . FakeInputStream   input    =    new   TestGridMixClasses . FakeInputStream (  )  ;", "ReadRecordFactory   test    =    new   ReadRecordFactory ( rf ,    input ,    new   Configuration (  )  )  ;", "GridmixKey   key    =    new   GridmixKey ( GridmixKey . DATA ,     1  0  0  ,     2  )  ;", "GridmixRecord   val    =    new   GridmixRecord (  2  0  0  ,     2  )  ;", "while    ( test . next ( key ,    val )  )     {", "}", "assertEquals (  3  0  0  0  ,    input . getCounter (  )  )  ;", "assertEquals (  (  -  1  )  ,    rf . getProgress (  )  ,     0  .  0  1  )  ;", "test . close (  )  ;", "}", "METHOD_END"], "methodName": ["testReadRecordFactory"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "File   fin    =    new   File (  (  (  (  (  (  (  (  (  \" src \"     +     ( File . separator )  )     +     \" test \"  )     +     ( File . separator )  )     +     \" resources \"  )     +     ( File . separator )  )     +     \" data \"  )     +     ( File . separator )  )     +     \" wordcount 2  . json \"  )  )  ;", "JobStoryProducer   jobProducer    =    new   ZombieJobProducer ( new   Path ( fin . getAbsolutePath (  )  )  ,    null ,    conf )  ;", "CountDownLatch   startFlag    =    new   CountDownLatch (  1  )  ;", "UserResolver   resolver    =    new   SubmitterUserResolver (  )  ;", ". FakeJobSubmitter   submitter    =    new    . FakeJobSubmitter (  )  ;", "File   ws    =    new   File (  (  (  \" target \"     +     ( File . separator )  )     +     ( this . getClass (  )  . getName (  )  )  )  )  ;", "if    (  !  ( ws . exists (  )  )  )     {", "Assert . assertTrue ( ws . mkdirs (  )  )  ;", "}", "SerialJobFactory   jobFactory    =    new   SerialJobFactory ( submitter ,    jobProducer ,    new   Path ( ws . getAbsolutePath (  )  )  ,    conf ,    startFlag ,    resolver )  ;", "Path   ioPath    =    new   Path ( ws . getAbsolutePath (  )  )  ;", "jobFactory . setDistCacheEmulator ( new   DistributedCacheEmulator ( conf ,    ioPath )  )  ;", "Thread   test    =    jobFactory . createReaderThread (  )  ;", "test . start (  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "assertEquals (  0  ,    submitter . getJobs (  )  . size (  )  )  ;", "startFlag . countDown (  )  ;", "while    ( test . isAlive (  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "jobFactory . update ( null )  ;", "}", "assertEquals (  2  ,    submitter . getJobs (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSerialReaderThread"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "SleepJob . SleepMapper   test    =    new   SleepJob . SleepMapper (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( NUM _ REDUCES ,     2  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  ;", "TaskAttemptID   taskId    =    new   TaskAttemptID (  )  ;", "TestGridMixClasses . FakeRecordLLReader   reader    =    new   TestGridMixClasses . FakeRecordLLReader (  )  ;", "TestGridMixClasses . LoadRecordGkNullWriter   writer    =    new   TestGridMixClasses . LoadRecordGkNullWriter (  )  ;", "OutputCommitter   committer    =    new   CustomOutputCommitter (  )  ;", "StatusReporter   reporter    =    new   TaskAttemptContextImpl . DummyReporter (  )  ;", "SleepJob . SleepSplit   split    =    getSleepSplit (  )  ;", "MapContext < LongWritable ,    LongWritable ,    GridmixKey ,    NullWritable >    mapcontext    =    new   uce . task . MapContextImpl < LongWritable ,    LongWritable ,    GridmixKey ,    NullWritable >  ( conf ,    taskId ,    reader ,    writer ,    committer ,    reporter ,    split )  ;", "Context   context    =    new   uce . lib . map . WrappedMapper < LongWritable ,    LongWritable ,    GridmixKey ,    NullWritable >  (  )  . getMapContext ( mapcontext )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "TestGridMixClasses . LOG . info (  (  \" start :  \"     +    start )  )  ;", "LongWritable   key    =    new   LongWritable (  ( start    +     2  0  0  0  )  )  ;", "LongWritable   value    =    new   LongWritable (  ( start    +     2  0  0  0  )  )  ;", "test . map ( key ,    value ,    context )  ;", "TestGridMixClasses . LOG . info (  (  \" finish :  \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "assertTrue (  (  ( System . currentTimeMillis (  )  )     >  =     ( start    +     2  0  0  0  )  )  )  ;", "test . cleanup ( context )  ;", "assertEquals (  1  ,    writer . getData (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSleepMapper"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( NUM _ REDUCES ,     2  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( COMPRESS ,    true )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "conf . setBoolean ( MAP _ OUTPUT _ COMPRESS ,    true )  ;", "TaskAttemptID   taskId    =    new   TaskAttemptID (  )  ;", "RawKeyValueIterator   input    =    new   TestGridMixClasses . FakeRawKeyValueReducerIterator (  )  ;", "Counter   counter    =    new   GenericCounter (  )  ;", "Counter   inputValueCounter    =    new   GenericCounter (  )  ;", "RecordWriter < NullWritable ,    NullWritable >    output    =    new   TestGridMixClasses . LoadRecordReduceWriter (  )  ;", "OutputCommitter   committer    =    new   CustomOutputCommitter (  )  ;", "StatusReporter   reporter    =    new   DummyReporter (  )  ;", "RawComparator < GridmixKey >    comparator    =    new   TestGridMixClasses . FakeRawComparator (  )  ;", "ReduceContext < GridmixKey ,    NullWritable ,    NullWritable ,    NullWritable >    reducecontext    =    new   uce . task . ReduceContextImpl < GridmixKey ,    NullWritable ,    NullWritable ,    NullWritable >  ( conf ,    taskId ,    input ,    counter ,    inputValueCounter ,    output ,    committer ,    reporter ,    comparator ,    GridmixKey . class ,    NullWritable . class )  ;", "Context   context    =    new   uce . lib . reduce . WrappedReducer < GridmixKey ,    NullWritable ,    NullWritable ,    NullWritable >  (  )  . getReducerContext ( reducecontext )  ;", "SleepJob . SleepReducer   test    =    new   SleepJob . SleepReducer (  )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "test . setup ( context )  ;", "long   sleeper    =    context . getCurrentKey (  )  . getReduceOutputBytes (  )  ;", "assertEquals (  (  (  \" Sleeping .  .  .     \"     +    sleeper )     +     \"    ms   left \"  )  ,    context . getStatus (  )  )  ;", "assertTrue (  (  ( System . currentTimeMillis (  )  )     >  =     ( start    +    sleeper )  )  )  ;", "test . cleanup ( context )  ;", "assertEquals (  (  \" Slept   for    \"     +    sleeper )  ,    context . getStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSleepReducer"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridMixClasses"}, {"methodBody": ["METHOD_START", "{", "TestResourceUsageEmulators . FakeProgressive   fakeProgress    =    new   TestResourceUsageEmulators . FakeProgressive (  )  ;", "fakeCore . resetFake (  )  ;", "heapPlugin . initialize ( conf ,    metrics ,    monitor ,    fakeProgress )  ;", "int   numLoops    =     0  ;", "while    (  ( fakeProgress . getProgress (  )  )     <     1  )     {", "+  + numLoops ;", "float   progress    =    numLoops    /     1  0  0  .  0 F ;", "fakeProgress . setProgress ( progress )  ;", "heapPlugin . emulate (  )  ;", "}", "assertEquals (  \" Cumulative   heap   usage   emulator   plugin   failed    ( total   usage )  !  \"  ,    expectedTotalHeapUsageInMB ,    fakeCore . getHeapUsageInMB (  )  ,     1 L )  ;", "assertEquals (  \" Cumulative   heap   usage   emulator   plugin   failed    ( num   calls )  !  \"  ,    expectedTotalNumCalls ,    fakeCore . getNumCalls (  )  ,     0 L )  ;", "}", "METHOD_END"], "methodName": ["testEmulationAccuracy"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "fakeProgress . setProgress ( progress )  ;", "heapPlugin . emulate (  )  ;", "assertEquals (  (  (  \"    interval   test   for   heap   usage   failed    \"     +    info )     +     \"  !  \"  )  ,    expectedTotalHeapUsageInMB ,    fakeCore . getHeapUsageInMB (  )  ,     0 L )  ;", "assertEquals (  (  (  \"    interval   test   for   heap   usage   failed    \"     +    info )     +     \"  !  \"  )  ,    expectedTotalNumCalls ,    fakeCore . getNumCalls (  )  ,     0 L )  ;", "}", "METHOD_END"], "methodName": ["testEmulationBoundary"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "TestGridmixMemoryEmulation . FakeHeapUsageEmulatorCore   heapEmulator    =    new   TestGridmixMemoryEmulation . FakeHeapUsageEmulatorCore (  )  ;", "long   testSizeInMB    =     1  0  ;", "long   previousHeap    =    heapEmulator . getHeapUsageInMB (  )  ;", "heapEmulator . load ( testSizeInMB )  ;", "long   currentHeap    =    heapEmulator . getHeapUsageInMB (  )  ;", "assertEquals (  \" Default   heap   emulator   failed   to   load    1  0 mb \"  ,     ( previousHeap    +    testSizeInMB )  ,    currentHeap )  ;", "heapEmulator . resetFake (  )  ;", "assertEquals (  \" Default   heap   emulator   failed   to   reset \"  ,     0  ,    heapEmulator . getHeapUsageInMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHeapUsageEmulator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "testJavaHeapOptions ( null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )  ;", "testJavaHeapOptions (  \"  - Xms 1  0 m \"  ,     \"  - Xms 2  0 m \"  ,     \"  - Xms 3  0 m \"  ,    null ,    null ,    null ,    null ,    null ,    null )  ;", "testJavaHeapOptions ( null ,    null ,    null ,     \"  - Xms 1  0 m \"  ,     \"  - Xms 2  0 m \"  ,     \"  - Xms 3  0 m \"  ,     \"  - Xms 1  0 m \"  ,     \"  - Xms 2  0 m \"  ,     \"  - Xms 3  0 m \"  )  ;", "testJavaHeapOptions (  \"  \"  ,     \"  \"  ,     \"  \"  ,    null ,    null ,    null ,    null ,    null ,    null )  ;", "testJavaHeapOptions ( null ,    null ,    null ,     \"  \"  ,     \"  \"  ,     \"  \"  ,     \"  \"  ,     \"  \"  ,     \"  \"  )  ;", "testJavaHeapOptions (  \"  \"  ,     \"  \"  ,     \"  \"  ,     \"  - Xmx 1  0 m    - Xms 1 m \"  ,     \"  - Xmx 5  0 m    - Xms 2 m \"  ,     \"  - Xms 2 m    - Xmx 1  0  0 m \"  ,     \"  - Xmx 1  0 m    - Xms 1 m \"  ,     \"  - Xmx 5  0 m    - Xms 2 m \"  ,     \"  - Xms 2 m    - Xmx 1  0  0 m \"  )  ;", "testJavaHeapOptions (  \"  - Xmx 1  0 m \"  ,     \"  - Xmx 2  0 m \"  ,     \"  - Xmx 3  0 m \"  ,    null ,    null ,    null ,     \"  - Xmx 1  0 m \"  ,     \"  - Xmx 2  0 m \"  ,     \"  - Xmx 3  0 m \"  )  ;", "testJavaHeapOptions (  \"  - Xms 5 m    - Xmx 2  0  0 m \"  ,     \"  - Xms 1  5 m    - Xmx 3  0  0 m \"  ,     \"  - Xms 2  5 m    - Xmx 5  0 m \"  ,     \"  - XXabc \"  ,     \"  - XXxyz \"  ,     \"  - XXdef \"  ,     \"  - XXabc    - Xmx 2  0  0 m \"  ,     \"  - XXxyz    - Xmx 3  0  0 m \"  ,     \"  - XXdef    - Xmx 5  0 m \"  )  ;", "testJavaHeapOptions (  \"  - Xms 5 m    - Xmx 2  0  0 m \"  ,     \"  - Xms 1  5 m    - Xmx 3  0  0 m \"  ,     \"  - Xms 2  5 m    - Xmx 5  0 m \"  ,     \"  - XXabc    - Xmx 5  0  0 m \"  ,     \"  - XXxyz    - Xmx 6  0  0 m \"  ,     \"  - XXdef    - Xmx 7  0  0 m \"  ,     \"  - XXabc    - Xmx 2  0  0 m \"  ,     \"  - XXxyz    - Xmx 3  0  0 m \"  ,     \"  - XXdef    - Xmx 5  0 m \"  )  ;", "testJavaHeapOptions (  \"  - Xmx 1  0 m \"  ,     \"  - Xmx 2  0 m \"  ,     \"  - Xmx 5  0 m \"  ,     \"  - Xms 2 m \"  ,     \"  - Xms 3 m \"  ,     \"  - Xms 5 m \"  ,     \"  - Xms 2 m    - Xmx 1  0 m \"  ,     \"  - Xms 3 m    - Xmx 2  0 m \"  ,     \"  - Xms 5 m    - Xmx 5  0 m \"  )  ;", "testJavaHeapOptions (  \"  - Xmx 1  0 m \"  ,     \"  - Xmx 2  0 m \"  ,     \"  - Xmx 5  0 m \"  ,     \"  - Xmx 2 m \"  ,     \"  - Xmx 3 m \"  ,     \"  - Xmx 5 m \"  ,     \"  - Xmx 1  0 m \"  ,     \"  - Xmx 2  0 m \"  ,     \"  - Xmx 5  0 m \"  )  ;", "}", "METHOD_END"], "methodName": ["testJavaHeapOptions"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "Configuration   simulatedConf    =    new   Configuration (  )  ;", "simulatedConf . unset ( MAP _ JAVA _ OPTS )  ;", "simulatedConf . unset ( REDUCE _ JAVA _ OPTS )  ;", "simulatedConf . unset ( MAPRED _ TASK _ JAVA _ OPTS )  ;", "if    ( defaultMapOptions    !  =    null )     {", "simulatedConf . set ( MAP _ JAVA _ OPTS ,    defaultMapOptions )  ;", "}", "if    ( defaultReduceOptions    !  =    null )     {", "simulatedConf . set ( REDUCE _ JAVA _ OPTS ,    defaultReduceOptions )  ;", "}", "if    ( defaultTaskOptions    !  =    null )     {", "simulatedConf . set ( MAPRED _ TASK _ JAVA _ OPTS ,    defaultTaskOptions )  ;", "}", "Configuration   originalConf    =    new   Configuration (  )  ;", "originalConf . unset ( MAP _ JAVA _ OPTS )  ;", "originalConf . unset ( REDUCE _ JAVA _ OPTS )  ;", "originalConf . unset ( MAPRED _ TASK _ JAVA _ OPTS )  ;", "if    ( mapOptions    !  =    null )     {", "originalConf . set ( MAP _ JAVA _ OPTS ,    mapOptions )  ;", "}", "if    ( reduceOptions    !  =    null )     {", "originalConf . set ( REDUCE _ JAVA _ OPTS ,    reduceOptions )  ;", "}", "if    ( taskOptions    !  =    null )     {", "originalConf . set ( MAPRED _ TASK _ JAVA _ OPTS ,    taskOptions )  ;", "}", "Job . configureTaskJVMOptions ( originalConf ,    simulatedConf )  ;", "assertEquals (  \" Map   heap   options   mismatch !  \"  ,    expectedMapOptions ,    simulatedConf . get ( MAP _ JAVA _ OPTS )  )  ;", "assertEquals (  \" Reduce   heap   options   mismatch !  \"  ,    expectedReduceOptions ,    simulatedConf . get ( REDUCE _ JAVA _ OPTS )  )  ;", "assertEquals (  \" Task   heap   options   mismatch !  \"  ,    expectedTaskOptions ,    simulatedConf . get ( MAPRED _ TASK _ JAVA _ OPTS )  )  ;", "}", "METHOD_END"], "methodName": ["testJavaHeapOptions"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "Configuration   gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setBoolean ( GridmixJob . GRIDMIX _ TASK _ JVM _ OPTIONS _ ENABLE ,    false )  ;", "gridmixConf . set ( MAP _ JAVA _ OPTS ,     \"  - Xmx 1 m \"  )  ;", "gridmixConf . set ( REDUCE _ JAVA _ OPTS ,     \"  - Xmx 2 m \"  )  ;", "gridmixConf . set ( MAPRED _ TASK _ JAVA _ OPTS ,     \"  - Xmx 3 m \"  )  ;", "final   JobConf   originalConf    =    new   JobConf (  )  ;", "originalConf . set ( MAP _ JAVA _ OPTS ,     \"  - Xmx 1  0 m \"  )  ;", "originalConf . set ( REDUCE _ JAVA _ OPTS ,     \"  - Xmx 2  0 m \"  )  ;", "originalConf . set ( MAPRED _ TASK _ JAVA _ OPTS ,     \"  - Xmx 3  0 m \"  )  ;", "DebugJobProducer . MockJob   story    =    new   DebugJobProducer . MockJob ( originalConf )     {", "public   JobConf   getJobConf (  )     {", "return   originalConf ;", "}", "}  ;", "GridmixJob   job    =    new   TestHighRamJob . DummyGridmixJob ( gridmixConf ,    story )  ;", "Job   simulatedJob    =    job . getJob (  )  ;", "Configuration   simulatedConf    =    simulatedJob . getConfiguration (  )  ;", "assertEquals (  \" Map   heap   options   works   when   disabled !  \"  ,     \"  - Xmx 1 m \"  ,    simulatedConf . get ( MAP _ JAVA _ OPTS )  )  ;", "assertEquals (  \" Reduce   heap   options   works   when   disabled !  \"  ,     \"  - Xmx 2 m \"  ,    simulatedConf . get ( REDUCE _ JAVA _ OPTS )  )  ;", "assertEquals (  \" Task   heap   options   works   when   disabled !  \"  ,     \"  - Xmx 3 m \"  ,    simulatedConf . get ( MAPRED _ TASK _ JAVA _ OPTS )  )  ;", "}", "METHOD_END"], "methodName": ["testJavaHeapOptionsDisabled"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "ResourceCalculatorPlugin   monitor    =    new   DummyResourceCalculatorPlugin (  )  ;", "long   maxHeapUsage    =     1  0  2  4     *     ( TotalHeapUsageEmulatorPlugin . ONE _ MB )  ;", "conf . setLong ( DummyResourceCalculatorPlugin . MAXPMEM _ TESTING _ PROPERTY ,    maxHeapUsage )  ;", "monitor . setConf ( conf )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . MIN _ HEAP _ FREE _ RATIO ,     0  .  0 F )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ LOAD _ RATIO ,     1  .  0 F )  ;", "long   targetHeapUsageInMB    =     2  0  0  ;", "TestResourceUsageEmulators . FakeProgressive   fakeProgress    =    new   TestResourceUsageEmulators . FakeProgressive (  )  ;", ". FakeHeapUsageEmulatorCore   fakeCore    =    new    . FakeHeapUsageEmulatorCore (  )  ;", ". FakeHeapUsageEmulatorPlugin   heapPlugin    =    new    . FakeHeapUsageEmulatorPlugin ( fakeCore )  ;", "ResourceUsageMetrics   invalidUsage    =    TestResourceUsageEmulators . createMetrics (  0  )  ;", "heapPlugin . initialize ( conf ,    invalidUsage ,    null ,    null )  ;", "int   numCallsPre    =    fakeCore . getNumCalls (  )  ;", "long   heapUsagePre    =    fakeCore . getHeapUsageInMB (  )  ;", "heapPlugin . emulate (  )  ;", "int   numCallsPost    =    fakeCore . getNumCalls (  )  ;", "long   heapUsagePost    =    fakeCore . getHeapUsageInMB (  )  ;", "assertEquals (  \" Disabled   heap   usage   emulation   plugin   works !  \"  ,    numCallsPre ,    numCallsPost )  ;", "assertEquals (  \" Disabled   heap   usage   emulation   plugin   works !  \"  ,    heapUsagePre ,    heapUsagePost )  ;", "float   progress    =    heapPlugin . getProgress (  )  ;", "assertEquals (  (  \" Invalid   progress   of   disabled   cumulative   heap   usage   emulation    \"     +     \" plugin !  \"  )  ,     1  .  0 F ,    progress ,     0  .  0 F )  ;", "Boolean   failed    =    null ;", "invalidUsage    =    TestResourceUsageEmulators . createMetrics (  ( maxHeapUsage    +     ( TotalHeapUsageEmulatorPlugin . ONE _ MB )  )  )  ;", "try    {", "heapPlugin . initialize ( conf ,    invalidUsage ,    monitor ,    null )  ;", "failed    =    false ;", "}    catch    ( Exception   e )     {", "failed    =    true ;", "}", "assertNotNull (  \" Fail   case   failure !  \"  ,    failed )  ;", "assertTrue (  \" Expected   failure !  \"  ,    failed )  ;", "ResourceUsageMetrics   metrics    =    TestResourceUsageEmulators . createMetrics (  ( targetHeapUsageInMB    *     ( TotalHeapUsageEmulatorPlugin . ONE _ MB )  )  )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    heapPlugin ,     2  0  0  ,     1  0  )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ EMULATION _ PROGRESS _ INTERVAL ,     0  .  2 F )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    heapPlugin ,     2  0  0  ,     5  )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ LOAD _ RATIO ,     1  .  0 F )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . MIN _ HEAP _ FREE _ RATIO ,     0  .  5 F )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    heapPlugin ,     1  2  0  ,     2  )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ LOAD _ RATIO ,     0  .  5 F )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . MIN _ HEAP _ FREE _ RATIO ,     0  .  0 F )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    heapPlugin ,     2  0  0  ,     1  0  )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . MIN _ HEAP _ FREE _ RATIO ,     0  .  2  5 F )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ LOAD _ RATIO ,     0  .  5 F )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    heapPlugin ,     1  6  2  ,     6  )  ;", "fakeProgress    =    new   TestResourceUsageEmulators . FakeProgressive (  )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . MIN _ HEAP _ FREE _ RATIO ,     0  .  0 F )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ LOAD _ RATIO ,     1  .  0 F )  ;", "conf . setFloat ( TotalHeapUsageEmulatorPlugin . HEAP _ EMULATION _ PROGRESS _ INTERVAL ,     0  .  2  5 F )  ;", "heapPlugin . initialize ( conf ,    metrics ,    monitor ,    fakeProgress )  ;", "fakeCore . resetFake (  )  ;", "long   initHeapUsage    =    fakeCore . getHeapUsageInMB (  )  ;", "long   initNumCallsUsage    =    fakeCore . getNumCalls (  )  ;", ". testEmulationBoundary (  0  .  0 F ,    fakeCore ,    fakeProgress ,    heapPlugin ,    initHeapUsage ,    initNumCallsUsage ,     \"  [ no - op ,     0    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  2  4 F ,    fakeCore ,    fakeProgress ,    heapPlugin ,    initHeapUsage ,    initNumCallsUsage ,     \"  [ no - op ,     2  4  %    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  2  5 F ,    fakeCore ,    fakeProgress ,    heapPlugin ,     ( targetHeapUsageInMB    /     4  )  ,     1  ,     \"  [ op ,     2  5  %    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  8 F ,    fakeCore ,    fakeProgress ,    heapPlugin ,     (  ( targetHeapUsageInMB    *     4  )     /     5  )  ,     2  ,     \"  [ op ,     8  0  %    progress ]  \"  )  ;", ". testEmulationBoundary (  1  .  0 F ,    fakeCore ,    fakeProgress ,    heapPlugin ,    targetHeapUsageInMB ,     3  ,     \"  [ op ,     1  0  0  %    progress ]  \"  )  ;", "}", "METHOD_END"], "methodName": ["testTotalHeapUsageEmulatorPlugin"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixMemoryEmulation"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   s    =    r . nextLong (  )  ;", "r . setSeed ( s )  ;", ". LOG . info (  (  \" sort :     \"     +    s )  )  ;", "final   DataOutputBuffer   out 1     =    new   DataOutputBuffer (  )  ;", "final   DataOutputBuffer   out 2     =    new   DataOutputBuffer (  )  ;", "for    ( int   i    =    min ;    i    <    max ;     +  + i )     {", "final   long   seed 1     =    r . nextLong (  )  ;", ". setSerialize ( x ,    seed 1  ,    i ,    out 1  )  ;", "assertEquals (  0  ,    x . compareSeed ( seed 1  ,    Math . max (  0  ,     ( i    -     ( x . fixedBytes (  )  )  )  )  )  )  ;", "final   long   seed 2     =    r . nextLong (  )  ;", ". setSerialize ( y ,    seed 2  ,    i ,    out 2  )  ;", "assertEquals (  0  ,    y . compareSeed ( seed 2  ,    Math . max (  0  ,     ( i    -     ( x . fixedBytes (  )  )  )  )  )  )  ;", "final   int   chk    =    WritableComparator . compareBytes ( out 1  . getData (  )  ,     0  ,    out 1  . getLength (  )  ,    out 2  . getData (  )  ,     0  ,    out 2  . getLength (  )  )  ;", "assertEquals ( Integer . signum ( chk )  ,    Integer . signum ( x . compareTo ( y )  )  )  ;", "assertEquals ( Integer . signum ( chk )  ,    Integer . signum ( cmp . compare ( out 1  . getData (  )  ,     0  ,    out 1  . getLength (  )  ,    out 2  . getData (  )  ,     0  ,    out 2  . getLength (  )  )  )  )  ;", "final   int   s 1     =    out 1  . getLength (  )  ;", "x . write ( out 1  )  ;", "assertEquals (  0  ,    cmp . compare ( out 1  . getData (  )  ,     0  ,    s 1  ,    out 1  . getData (  )  ,    s 1  ,     (  ( out 1  . getLength (  )  )     -    s 1  )  )  )  ;", "final   int   s 2     =    out 2  . getLength (  )  ;", "y . write ( out 2  )  ;", "assertEquals (  0  ,    cmp . compare ( out 2  . getData (  )  ,     0  ,    s 2  ,    out 2  . getData (  )  ,    s 2  ,     (  ( out 2  . getLength (  )  )     -    s 2  )  )  )  ;", "assertEquals ( Integer . signum ( chk )  ,    Integer . signum ( cmp . compare ( out 1  . getData (  )  ,     0  ,    s 1  ,    out 2  . getData (  )  ,    s 2  ,     (  ( out 2  . getLength (  )  )     -    s 2  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["binSortTest"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   s    =    r . nextLong (  )  ;", "r . setSeed ( s )  ;", ". LOG . info (  (  \" spec :     \"     +    s )  )  ;", "final   DataInputBuffer   in    =    new   DataInputBuffer (  )  ;", "final   DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "a . setType ( GridmixKey . REDUCE _ SPEC )  ;", "b . setType ( GridmixKey . REDUCE _ SPEC )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;     +  + i )     {", "final   int   in _ rec    =    r . nextInt ( Integer . MAX _ VALUE )  ;", "a . setReduceInputRecords ( in _ rec )  ;", "final   int   out _ rec    =    r . nextInt ( Integer . MAX _ VALUE )  ;", "a . setReduceOutputRecords ( out _ rec )  ;", "final   int   out _ bytes    =    r . nextInt ( Integer . MAX _ VALUE )  ;", "a . setReduceOutputBytes ( out _ bytes )  ;", "final   int   min    =     (  (  ( WritableUtils . getVIntSize ( in _ rec )  )     +     ( WritableUtils . getVIntSize ( out _ rec )  )  )     +     ( WritableUtils . getVIntSize ( out _ bytes )  )  )     +     ( WritableUtils . getVIntSize (  0  )  )  ;", "assertEquals (  ( min    +     2  )  ,    a . fixedBytes (  )  )  ;", "final   int   size    =     (  ( r . nextInt (  1  0  2  4  )  )     +     ( a . fixedBytes (  )  )  )     +     1  ;", ". setSerialize ( a ,    r . nextLong (  )  ,    size ,    out )  ;", "assertEquals ( size ,    out . getLength (  )  )  ;", "assertTrue ( a . equals ( a )  )  ;", "assertEquals (  0  ,    a . compareTo ( a )  )  ;", "in . reset ( out . getData (  )  ,     0  ,    out . getLength (  )  )  ;", "b . readFields ( in )  ;", "assertEquals ( size ,    b . getSize (  )  )  ;", "assertEquals ( in _ rec ,    b . getReduceInputRecords (  )  )  ;", "assertEquals ( out _ rec ,    b . getReduceOutputRecords (  )  )  ;", "assertEquals ( out _ bytes ,    b . getReduceOutputBytes (  )  )  ;", "assertTrue ( a . equals ( b )  )  ;", "assertEquals (  0  ,    a . compareTo ( b )  )  ;", "assertEquals ( a . hashCode (  )  ,    b . hashCode (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkSpec"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   s    =    r . nextLong (  )  ;", "r . setSeed ( s )  ;", ". LOG . info (  (  \" eqSeed :     \"     +    s )  )  ;", "assertEquals ( x . fixedBytes (  )  ,    y . fixedBytes (  )  )  ;", "final   int   min    =     ( x . fixedBytes (  )  )     +     1  ;", "final   DataOutputBuffer   out 1     =    new   DataOutputBuffer (  )  ;", "final   DataOutputBuffer   out 2     =    new   DataOutputBuffer (  )  ;", "for    ( int   i    =    min ;    i    <    max ;     +  + i )     {", "final   long   seed    =    r . nextLong (  )  ;", ". setSerialize ( x ,    seed ,    i ,    out 1  )  ;", ". setSerialize ( y ,    seed ,    i ,    out 2  )  ;", "assertEquals ( x ,    y )  ;", "assertEquals ( x . hashCode (  )  ,    y . hashCode (  )  )  ;", "assertEquals ( out 1  . getLength (  )  ,    out 2  . getLength (  )  )  ;", "assertEquals (  \" Bad   test \"  ,    out 1  . getData (  )  . length ,    out 2  . getData (  )  . length )  ;", "assertArrayEquals ( out 1  . getData (  )  ,    out 2  . getData (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["eqSeedTest"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", ". LOG . info (  (  \" length :     \"     +    seed )  )  ;", "final   DataInputBuffer   in    =    new   DataInputBuffer (  )  ;", "final   DataOutputBuffer   out 1     =    new   DataOutputBuffer (  )  ;", "final   DataOutputBuffer   out 2     =    new   DataOutputBuffer (  )  ;", "for    ( int   i    =    min ;    i    <    max ;     +  + i )     {", ". setSerialize ( x ,    r . nextLong (  )  ,    i ,    out 1  )  ;", "assertEquals ( i ,    out 1  . getLength (  )  )  ;", "x . write ( out 2  )  ;", "in . reset ( out 1  . getData (  )  ,     0  ,    out 1  . getLength (  )  )  ;", "y . readFields ( in )  ;", "assertEquals ( i ,    x . getSize (  )  )  ;", "assertEquals ( i ,    y . getSize (  )  )  ;", "}", "in . reset ( out 2  . getData (  )  ,     0  ,    out 2  . getLength (  )  )  ;", "for    ( int   i    =    min ;    i    <    max ;     +  + i )     {", "y . readFields ( in )  ;", "assertEquals ( i ,    y . getSize (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["lengthTest"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "boolean   fail    =    false ;", "final      test    =    new    (  )  ;", "try    {", "test . testKeySpec (  )  ;", "}    catch    ( Exception   e )     {", "fail    =    true ;", "e . printStackTrace (  )  ;", "}", "try    {", "test . testKeyData (  )  ;", "}    catch    ( Exception   e )     {", "fail    =    true ;", "e . printStackTrace (  )  ;", "}", "try    {", "test . testBaseRecord (  )  ;", "}    catch    ( Exception   e )     {", "fail    =    true ;", "e . printStackTrace (  )  ;", "}", "System . exit (  ( fail    ?     -  1     :     0  )  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "a . wte ( out )  ;", "System . out . pntln (  (  \" A    \"     +     ( Arrays . toStng ( Arrays . copyOf ( out . getData (  )  ,    out . getLength (  )  )  )  )  )  )  ;", "out . reset (  )  ;", "b . wte ( out )  ;", "System . out . pntln (  (  \" B    \"     +     ( Arrays . toStng ( Arrays . copyOf ( out . getData (  )  ,    out . getLength (  )  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["printDebug"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", ". LOG . info (  (  \" randReplay :     \"     +    seed )  )  ;", "final   DataOutputBuffer   out 1     =    new   DataOutputBuffer (  )  ;", "for    ( int   i    =    min ;    i    <    max ;     +  + i )     {", "final   int   s    =    out 1  . getLength (  )  ;", "x . setSeed ( r . nextLong (  )  )  ;", "x . setSize ( i )  ;", "x . write ( out 1  )  ;", "assertEquals ( i ,     (  ( out 1  . getLength (  )  )     -    s )  )  ;", "}", "final   DataInputBuffer   in    =    new   DataInputBuffer (  )  ;", "in . reset ( out 1  . getData (  )  ,     0  ,    out 1  . getLength (  )  )  ;", "final   DataOutputBuffer   out 2     =    new   DataOutputBuffer (  )  ;", "for    ( int   i    =    min ;    i    <    max ;     +  + i )     {", "final   int   s    =    in . getPosition (  )  ;", "y . readFields ( in )  ;", "assertEquals ( i ,     (  ( in . getPosition (  )  )     -    s )  )  ;", "y . write ( out 2  )  ;", "}", "assertEquals ( out 1  . getLength (  )  ,    out 2  . getLength (  )  )  ;", "assertEquals (  \" Bad   test \"  ,    out 1  . getData (  )  . length ,    out 2  . getData (  )  . length )  ;", "assertArrayEquals ( out 1  . getData (  )  ,    out 2  . getData (  )  )  ;", "}", "METHOD_END"], "methodName": ["randomReplayTest"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "x . setSeed ( seed )  ;", "x . setSize ( size )  ;", "out . reset (  )  ;", "x . write ( out )  ;", "}", "METHOD_END"], "methodName": ["setSerialize"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   int   min    =     1  ;", "final   int   max    =     3  0  0  ;", "final   GridmixRecord   a    =    new   GridmixRecord (  )  ;", "final   GridmixRecord   b    =    new   GridmixRecord (  )  ;", ". lengthTest ( a ,    b ,    min ,    max )  ;", ". randomReplayTest ( a ,    b ,    min ,    max )  ;", ". binSortTest ( a ,    b ,    min ,    max ,    new   GridmixRecord . Comparator (  )  )  ;", ". eqSeedTest ( a ,    b ,     3  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testBaseRecord"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   int   min    =     2  ;", "final   int   max    =     3  0  0  ;", "final   GridmixKey   a    =    new   GridmixKey ( GridmixKey . DATA ,     1  ,     0 L )  ;", "final   GridmixKey   b    =    new   GridmixKey ( GridmixKey . DATA ,     1  ,     0 L )  ;", ". lengthTest ( a ,    b ,    min ,    max )  ;", ". randomReplayTest ( a ,    b ,    min ,    max )  ;", ". binSortTest ( a ,    b ,    min ,    max ,    new   GridmixKey . Comparator (  )  )  ;", ". eqSeedTest ( a ,    b ,     3  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testKeyData"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "final   int   min    =     6  ;", "final   int   max    =     3  0  0  ;", "final   GridmixKey   a    =    new   GridmixKey ( GridmixKey . REDUCE _ SPEC ,     1  ,     0 L )  ;", "final   GridmixKey   b    =    new   GridmixKey ( GridmixKey . REDUCE _ SPEC ,     1  ,     0 L )  ;", ". lengthTest ( a ,    b ,    min ,    max )  ;", ". randomReplayTest ( a ,    b ,    min ,    max )  ;", ". binSortTest ( a ,    b ,    min ,    max ,    new   GridmixKey . Comparator (  )  )  ;", ". eqSeedTest ( a ,    b ,    max )  ;", ". checkSpec ( a ,    b )  ;", "}", "METHOD_END"], "methodName": ["testKeySpec"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixRecord"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buff    =    new   byte [  4  0  9  6  ]  ;", "GZIPInputStream   gis    =    new   GZIPInputStream ( fs . en ( in )  )  ;", "FSDataOutputStream   fsdOs    =    fs . create ( out )  ;", "int   numRead ;", "while    (  ( numRead    =    gis . read ( buff ,     0  ,    buff . length )  )     !  =     (  -  1  )  )     {", "fsdOs . write ( buff ,     0  ,    numRead )  ;", "}", "gis . close (  )  ;", "fsdOs . close (  )  ;", "}", "METHOD_END"], "methodName": ["expandGzippedTrace"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . initCluster ( TestGridmixSubmission . class )  ;", "System . setProperty (  \" src . test . data \"  ,    TestGridmixSubmission . inSpace . getAbsolutePath (  )  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . shutdownCluster (  )  ;", "}", "METHOD_END"], "methodName": ["shutDown"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "SecurityManager   securityManager    =    System . getSecurityManager (  )  ;", "final   ByteArrayOutputStream   bytes    =    new   ByteArrayOutputStream (  )  ;", "final   PrintStream   out    =    new   PrintStream ( bytes )  ;", "final   PrintStream   oldOut    =    System . out ;", "System . setErr ( out )  ;", "ExitUtil . disableSystemExit (  )  ;", "try    {", "String [  ]    argv    =    new   String [  0  ]  ;", "CommonJobTest . Debug . main ( argv )  ;", "}    catch    ( ExitUtil   e )     {", "assertEquals (  \" ExitException \"  ,    e . getMessage (  )  )  ;", "ExitUtil . resetFirstExitException (  )  ;", "}    finally    {", "System . setErr ( oldOut )  ;", "System . setSecurityManager ( securityManager )  ;", "}", "String   print    =    bytes . toString (  )  ;", "assertTrue ( print . contains (  \" Usage :    gridmix    [  - generate    < MiB >  ]     [  - users   URI ]     [  - Dname = value    .  .  .  ]     < iopath >     < trace >  \"  )  )  ;", "assertTrue ( print . contains (  \" e . g .    gridmix    - generate    1  0  0 m   foo    -  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testMain"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "CommonJobTest . policy    =    GridmixJobSubmissionPolicy . REPLAY ;", "CommonJobTest . LOG . info (  (  \"    Replay   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( null ,    false )  ;", "CommonJobTest . LOG . info (  (  \"    Replay   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testReplaySubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "CommonJobTest . policy    =    GridmixJobSubmissionPolicy . STRESS ;", "CommonJobTest . LOG . info (  (  \"    Stress   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( null ,    false )  ;", "CommonJobTest . LOG . info (  (  \"    Stress   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testStressSubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "Path   rootInputDir    =    new   Path ( System . getProperty (  \" src . test . data \"  )  )  ;", "rootInputDir    =    rootInputDir . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,    System . getProperty (  \" tmpdir \"  )  )  ,     \" testTraceReader \"  )  ;", "rootTempDir    =    rootTempDir . makeQualified ( lfs . getUri (  )  ,    lfs . getWorkingDirectory (  )  )  ;", "Path   inputFile    =    new   Path ( rootInputDir ,     \" wordcount . json . gz \"  )  ;", "Path   tempFile    =    new   Path ( rootTempDir ,     \"  3  - wc . json \"  )  ;", "InputStream   origStdIn    =    System . in ;", "InputStream   tmpIs    =    null ;", "try    {", "CommonJobTest . DebugGridmix   dgm    =    new   CommonJobTest . DebugGridmix (  )  ;", "JobStoryProducer   jsp    =    dgm . createJobStoryProducer ( inputFile . toString (  )  ,    conf )  ;", "CommonJobTest . LOG . info (  \" Verifying   JobStory   from   compressed   trace .  .  .  \"  )  ;", "verifyWordCountJobStory ( jsp . getNextJob (  )  )  ;", "expandGzippedTrace ( lfs ,    inputFile ,    tempFile )  ;", "jsp    =    dgm . createJobStoryProducer ( tempFile . toString (  )  ,    conf )  ;", "CommonJobTest . LOG . info (  \" Verifying   JobStory   from   uncompressed   trace .  .  .  \"  )  ;", "verifyWordCountJobStory ( jsp . getNextJob (  )  )  ;", "tmpIs    =    lfs . open ( tempFile )  ;", "System . setIn ( tmpIs )  ;", "CommonJobTest . LOG . info (  \" Verifying   JobStory   from   trace   in   standard   input .  .  .  \"  )  ;", "jsp    =    dgm . createJobStoryProducer (  \"  -  \"  ,    conf )  ;", "verifyWordCountJobStory ( jsp . getNextJob (  )  )  ;", "}    finally    {", "System . setIn ( origStdIn )  ;", "if    ( tmpIs    !  =    null )     {", "tmpIs . close (  )  ;", "}", "lfs . delete ( rootTempDir ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testTraceReader"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "assertNotNull (  \" Null   JobStory \"  ,    js )  ;", "String   expectedJobStory    =     \" WordCount : johndoe : default :  1  2  8  5  3  2  2  6  4  5  1  4  8  :  3  :  1  \"  ;", "String   actualJobStory    =     (  (  (  (  (  (  (  (  (  ( js . getName (  )  )     +     \"  :  \"  )     +     ( js . getUser (  )  )  )     +     \"  :  \"  )     +     ( js . getQueueName (  )  )  )     +     \"  :  \"  )     +     ( js . getTime (  )  )  )     +     \"  :  \"  )     +     ( js . getNumberMaps (  )  )  )     +     \"  :  \"  )     +     ( js . getNumberReduces (  )  )  ;", "assertEquals (  \" Unexpected   JobStory \"  ,    expectedJobStory ,    actualJobStory )  ;", "}", "METHOD_END"], "methodName": ["verifyWordCountJobStory"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSubmission"}, {"methodBody": ["METHOD_START", "{", "Job   fakeJob    =    new   Job (  )     {", "@ Override", "public   int   getNumReduceTasks (  )     {", "return   numReds ;", "}", "@ Override", "public   boolean   isSuccessful (  )    throws   IOException    {", "if    ( lost )     {", "throw   new   IOException (  \"    failure !  \"  )  ;", "}", "return   isSuccessful ;", "}", "}  ;", "return   new   Statistics . JobStats ( numMaps ,    numReds ,    fakeJob )  ;", "}", "METHOD_END"], "methodName": ["generateFakeJobStats"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSummary"}, {"methodBody": ["METHOD_START", "{", "ClusterSummarizer   cs    =    new   ClusterSummarizer (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "String   jt    =     \" test - jt :  1  2  3  4  \"  ;", "String   nn    =     \" test - nn :  5  6  7  8  \"  ;", "conf . set ( JT _ IPC _ ADDRESS ,    jt )  ;", "conf . set ( FS _ DEFAULT _ NAME _ KEY ,    nn )  ;", "cs . start ( conf )  ;", "assertEquals (  \" JT   name   mismatch \"  ,    jt ,    cs . getJobTrackerInfo (  )  )  ;", "assertEquals (  \" NN   name   mismatch \"  ,    nn ,    cs . getNamenodeInfo (  )  )  ;", "Statistics . ClusterStats   cStats    =    Statistics . ClusterStats . getClusterStats (  )  ;", "conf . set ( JT _ IPC _ ADDRESS ,     \" local \"  )  ;", "conf . set ( FS _ DEFAULT _ NAME _ KEY ,     \" local \"  )  ;", "JobClient   jc    =    new   JobClient ( conf )  ;", "cStats . setClusterMetric ( jc . getClusterStatus (  )  )  ;", "cs . update ( cStats )  ;", "assertEquals (  \" Cluster   summary   test   failed !  \"  ,     1  ,    cs . getMaxMapTasks (  )  )  ;", "assertEquals (  \" Cluster   summary   test   failed !  \"  ,     1  ,    cs . getMaxReduceTasks (  )  )  ;", "assertEquals (  \" Cluster   summary   test   failed !  \"  ,     1  ,    cs . getNumActiveTrackers (  )  )  ;", "assertEquals (  \" Cluster   summary   test   failed !  \"  ,     0  ,    cs . getNumBlacklistedTrackers (  )  )  ;", "}", "METHOD_END"], "methodName": ["testClusterSummarizer"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSummary"}, {"methodBody": ["METHOD_START", "{", "GenerateData . DataStatistics   stats    =    new   GenerateData . DataStatistics (  1  0  ,     2  ,    true )  ;", "assertEquals (  \" Data   size   mismatch \"  ,     1  0  ,    stats . getDataSize (  )  )  ;", "assertEquals (  \" Num   files   mismatch \"  ,     2  ,    stats . getNumFiles (  )  )  ;", "assertTrue (  \" Compression   configuration   mismatch \"  ,    stats . isDataCompressed (  )  )  ;", "stats    =    new   GenerateData . DataStatistics (  1  0  0  ,     5  ,    false )  ;", "assertEquals (  \" Data   size   mismatch \"  ,     1  0  0  ,    stats . getDataSize (  )  )  ;", "assertEquals (  \" Num   files   mismatch \"  ,     5  ,    stats . getNumFiles (  )  )  ;", "assertFalse (  \" Compression   configuration   mismatch \"  ,    stats . isDataCompressed (  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ;", "Path   testDir    =    new   Path ( rootTempDir ,     \" testDataStatistics \"  )  ;", "FileSystem   fs    =    testDir . getFileSystem ( conf )  ;", "fs . delete ( testDir ,    true )  ;", "Path   testInputDir    =    new   Path ( testDir ,     \" test \"  )  ;", "fs . mkdirs ( testInputDir )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "Boolean   failed    =    null ;", "try    {", "GenerateData . publishDataStatistics ( testInputDir ,     1  0  2  4 L ,    conf )  ;", "failed    =    false ;", "}    catch    ( RuntimeException   e )     {", "failed    =    true ;", "}", "assertNotNull (  \" Expected   failure !  \"  ,    failed )  ;", "assertTrue (  \" Compression   data   publishing   error \"  ,    failed )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    false )  ;", "stats    =    GenerateData . publishDataStatistics ( testInputDir ,     1  0  2  4 L ,    conf )  ;", "assertEquals (  \" Data   size   mismatch \"  ,     0  ,    stats . getDataSize (  )  )  ;", "assertEquals (  \" Num   files   mismatch \"  ,     0  ,    stats . getNumFiles (  )  )  ;", "assertFalse (  \" Compression   configuration   mismatch \"  ,    stats . isDataCompressed (  )  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    false )  ;", "Path   inputDataFile    =    new   Path ( testInputDir ,     \" test \"  )  ;", "long   size    =    UtilsForTests . createTmpFileDFS ( fs ,    inputDataFile ,    createImmutable (  (  ( short )     (  7  7  7  )  )  )  ,     \" hi   hello   bye \"  )  . size (  )  ;", "stats    =    GenerateData . publishDataStatistics ( testInputDir ,     (  -  1  )  ,    conf )  ;", "assertEquals (  \" Data   size   mismatch \"  ,    size ,    stats . getDataSize (  )  )  ;", "assertEquals (  \" Num   files   mismatch \"  ,     1  ,    stats . getNumFiles (  )  )  ;", "assertFalse (  \" Compression   configuration   mismatch \"  ,    stats . isDataCompressed (  )  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "failed    =    null ;", "try    {", "GenerateData . publishDataStatistics ( testInputDir ,     1  2  3  4 L ,    conf )  ;", "failed    =    false ;", "}    catch    ( RuntimeException   e )     {", "failed    =    true ;", "}", "assertNotNull (  \" Expected   failure !  \"  ,    failed )  ;", "assertTrue (  \" Compression   data   publishing   error \"  ,    failed )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    false )  ;", "fs . delete ( inputDataFile ,    false )  ;", "inputDataFile    =    new   Path ( testInputDir ,     \" test . gz \"  )  ;", "size    =    UtilsForTests . createTmpFileDFS ( fs ,    inputDataFile ,    createImmutable (  (  ( short )     (  7  7  7  )  )  )  ,     \" hi   hello \"  )  . size (  )  ;", "stats    =    GenerateData . publishDataStatistics ( testInputDir ,     1  2  3  4 L ,    conf )  ;", "assertEquals (  \" Data   size   mismatch \"  ,    size ,    stats . getDataSize (  )  )  ;", "assertEquals (  \" Num   files   mismatch \"  ,     1  ,    stats . getNumFiles (  )  )  ;", "assertFalse (  \" Compression   configuration   mismatch \"  ,    stats . isDataCompressed (  )  )  ;", "CompressionEmulationUtil . setCompressionEmulationEnabled ( conf ,    true )  ;", "stats    =    GenerateData . publishDataStatistics ( testInputDir ,     1  2  3  4 L ,    conf )  ;", "assertEquals (  \" Data   size   mismatch \"  ,    size ,    stats . getDataSize (  )  )  ;", "assertEquals (  \" Num   files   mismatch \"  ,     1  ,    stats . getNumFiles (  )  )  ;", "assertTrue (  \" Compression   configuration   mismatch \"  ,    stats . isDataCompressed (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDataStatistics"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSummary"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "ExecutionSummarizer   es    =    new   ExecutionSummarizer (  )  ;", "assertEquals (  \" ExecutionSummarizer   init   failed \"  ,    Summarizer . NA ,    es . getCommandLineArgsString (  )  )  ;", "long   startTime    =    System . currentTimeMillis (  )  ;", "String [  ]    initArgs    =    new   String [  ]  {     \"  - Xmx 2  0 m \"  ,     \"  - Dtest . args =  ' test '  \"     }  ;", "es    =    new   ExecutionSummarizer ( initArgs )  ;", "assertEquals (  \" ExecutionSummarizer   init   failed \"  ,     \"  - Xmx 2  0 m    - Dtest . args =  ' test '  \"  ,    es . getCommandLineArgsString (  )  )  ;", "assertTrue (  \" Start   time   mismatch \"  ,     (  ( es . getStartTime (  )  )     >  =    startTime )  )  ;", "assertTrue (  \" Start   time   mismatch \"  ,     (  ( es . getStartTime (  )  )     <  =     ( System . currentTimeMillis (  )  )  )  )  ;", "es . update ( null )  ;", "assertEquals (  \" ExecutionSummarizer   init   failed \"  ,     0  ,    es . getSimulationStartTime (  )  )  ;", ". testExecutionSummarizer (  0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,    es )  ;", "long   simStartTime    =    System . currentTimeMillis (  )  ;", "es . start ( null )  ;", "assertTrue (  \" Simulation   start   time   mismatch \"  ,     (  ( es . getSimulationStartTime (  )  )     >  =    simStartTime )  )  ;", "assertTrue (  \" Simulation   start   time   mismatch \"  ,     (  ( es . getSimulationStartTime (  )  )     <  =     ( System . currentTimeMillis (  )  )  )  )  ;", "Statistics . JobStats   stats    =     . generateFakeJobStats (  1  ,     1  0  ,    true ,    false )  ;", "es . update ( stats )  ;", ". testExecutionSummarizer (  1  ,     1  0  ,     0  ,     1  ,     1  ,     0  ,     0  ,    es )  ;", "stats    =     . generateFakeJobStats (  5  ,     1  ,    false ,    false )  ;", "es . update ( stats )  ;", ". testExecutionSummarizer (  6  ,     1  1  ,     0  ,     2  ,     1  ,     1  ,     0  ,    es )  ;", "stats    =     . generateFakeJobStats (  1  ,     1  ,    true ,    true )  ;", "es . update ( stats )  ;", ". testExecutionSummarizer (  7  ,     1  2  ,     0  ,     3  ,     1  ,     1  ,     1  ,    es )  ;", "stats    =     . generateFakeJobStats (  2  ,     2  ,    false ,    true )  ;", "es . update ( stats )  ;", ". testExecutionSummarizer (  9  ,     1  4  ,     0  ,     4  ,     1  ,     1  ,     2  ,    es )  ;", "JobFactory   factory    =    new    . FakeJobFactory ( conf )  ;", "factory . numJobsInTrace    =     3  ;", "Path   rootTempDir    =    new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ;", "Path   testDir    =    new   Path ( rootTempDir ,     \" testGridmixSummary \"  )  ;", "Path   testTraceFile    =    new   Path ( testDir ,     \" test - trace . json \"  )  ;", "FileSystem   fs    =    FileSystem . getLocal ( conf )  ;", "fs . create ( testTraceFile )  . close (  )  ;", "UserResolver   resolver    =    new   RoundRobinUserResolver (  )  ;", "GenerateData . DataStatistics   dataStats    =    new   GenerateData . DataStatistics (  1  0  0  ,     2  ,    true )  ;", "String   policy    =    GridmixJobSubmissionPolicy . REPLAY . name (  )  ;", "conf . set ( GridmixJobSubmissionPolicy . JOB _ SUBMISSION _ POLICY ,    policy )  ;", "es . finalize ( factory ,    testTraceFile . toString (  )  ,     1  0  2  4 L ,    resolver ,    dataStats ,    conf )  ;", "assertEquals (  \" Mismtach   in   num   jobs   in   trace \"  ,     3  ,    es . getNumJobsInTrace (  )  )  ;", "String   tid    =    ExecutionSummarizer . getTraceSignature ( testTraceFile . toString (  )  )  ;", "assertEquals (  \" Mismatch   in   trace   signature \"  ,    tid ,    es . getInputTraceSignature (  )  )  ;", "Path   qPath    =    fs . makeQualified ( testTraceFile )  ;", "assertEquals (  \" Mismatch   in   trace   filename \"  ,    qPath . toString (  )  ,    es . getInputTraceLocation (  )  )  ;", "assertEquals (  \" Mismatch   in   expected   data   size \"  ,     \"  1    K \"  ,    es . getExpectedDataSize (  )  )  ;", "assertEquals (  \" Mismatch   in   input   data   statistics \"  ,    ExecutionSummarizer . stringifyDataStatistics ( dataStats )  ,    es . getInputDataStatistics (  )  )  ;", "assertEquals (  \" Mismatch   in   user   resolver \"  ,    resolver . getClass (  )  . getName (  )  ,    es . getUserResolver (  )  )  ;", "assertEquals (  \" Mismatch   in   policy \"  ,    policy ,    es . getJobSubmissionPolicy (  )  )  ;", "es . finalize ( factory ,    testTraceFile . toString (  )  ,     (  (  (  1  0  2  4     *     1  0  2  4  )     *     1  0  2  4  )     *     1  0 L )  ,    resolver ,    dataStats ,    conf )  ;", "assertEquals (  \" Mismatch   in   expected   data   size \"  ,     \"  1  0    G \"  ,    es . getExpectedDataSize (  )  )  ;", "fs . delete ( testTraceFile ,    false )  ;", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   ie )     {", "}", "fs . create ( testTraceFile )  . close (  )  ;", "es . finalize ( factory ,    testTraceFile . toString (  )  ,     0 L ,    resolver ,    dataStats ,    conf )  ;", "assertEquals (  \" Mismatch   in   trace   data   size \"  ,    Summarizer . NA ,    es . getExpectedDataSize (  )  )  ;", "assertFalse (  \" Mismatch   in   trace   signature \"  ,    tid . equals ( es . getInputTraceSignature (  )  )  )  ;", "tid    =    ExecutionSummarizer . getTraceSignature ( testTraceFile . toString (  )  )  ;", "assertEquals (  \" Mismatch   in   trace   signature \"  ,    tid ,    es . getInputTraceSignature (  )  )  ;", "testTraceFile    =    new   Path ( testDir ,     \" test - trace 2  . json \"  )  ;", "fs . create ( testTraceFile )  . close (  )  ;", "es . finalize ( factory ,    testTraceFile . toString (  )  ,     0 L ,    resolver ,    dataStats ,    conf )  ;", "assertFalse (  \" Mismatch   in   trace   signature \"  ,    tid . equals ( es . getInputTraceSignature (  )  )  )  ;", "tid    =    ExecutionSummarizer . getTraceSignature ( testTraceFile . toString (  )  )  ;", "assertEquals (  \" Mismatch   in   trace   signature \"  ,    tid ,    es . getInputTraceSignature (  )  )  ;", "es . finalize ( factory ,     \"  -  \"  ,     0 L ,    resolver ,    dataStats ,    conf )  ;", "assertEquals (  \" Mismatch   in   trace   signature \"  ,    Summarizer . NA ,    es . getInputTraceSignature (  )  )  ;", "assertEquals (  \" Mismatch   in   trace   file   location \"  ,    Summarizer . NA ,    es . getInputTraceLocation (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExecutionSummarizer"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSummary"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - maps ]  \"  ,    numMaps ,    es . getNumMapTasksLaunched (  )  )  ;", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - reducers ]  \"  ,    numReds ,    es . getNumReduceTasksLaunched (  )  )  ;", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - jobs - in - trace ]  \"  ,    totalJobsInTrace ,    es . getNumJobsInTrace (  )  )  ;", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - submitted   jobs ]  \"  ,    totalJobSubmitted ,    es . getNumSubmittedJobs (  )  )  ;", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - successful - jobs ]  \"  ,    numSuccessfulJob ,    es . getNumSuccessfulJobs (  )  )  ;", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - failed   jobs ]  \"  ,    numFailedJobs ,    es . getNumFailedJobs (  )  )  ;", "assertEquals (  \" ExecutionSummarizer   test   failed    [ num - lost   jobs ]  \"  ,    numLostJobs ,    es . getNumLostJobs (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExecutionSummarizer"], "fileName": "org.apache.hadoop.mapred.gridmix.TestGridmixSummary"}, {"methodBody": ["METHOD_START", "{", "Configuration   simulatedJobConf    =    new   Configuration ( gConf )  ;", "simulatedJobConf . setLong ( MAPMEMORY _ MB ,    simulatedClusterMapMB )  ;", "simulatedJobConf . setLong ( REDUCEMEMORY _ MB ,    simulatedClusterReduceMB )  ;", "Configuration   sourceConf    =    new   Configuration (  )  ;", "sourceConf . setLong ( MAP _ MEMORY _ MB ,    jobMapMB )  ;", "sourceConf . setLong ( MAPMEMORY _ MB ,    clusterMapMB )  ;", "sourceConf . setLong ( REDUCE _ MEMORY _ MB ,    jobReduceMB )  ;", "sourceConf . setLong ( REDUCEMEMORY _ MB ,    clusterReduceMB )  ;", "DebugJobProducer . MockJob   story    =    new   DebugJobProducer . MockJob ( sourceConf )  ;", "GridmixJob   job    =    new    . DummyGridmixJob ( simulatedJobConf ,    story )  ;", "Job   simulatedJob    =    job . getJob (  )  ;", "Configuration   simulatedConf    =    simulatedJob . getConfiguration (  )  ;", "assertEquals ( expectedMapMB ,    simulatedConf . getLong ( MAP _ MEMORY _ MB ,    DEFAULT _ MAP _ MEMORY _ MB )  )  ;", "assertEquals ( expectedReduceMB ,    simulatedConf . getLong ( REDUCE _ MEMORY _ MB ,    DEFAULT _ MAP _ MEMORY _ MB )  )  ;", "}", "METHOD_END"], "methodName": ["testHighRamConfig"], "fileName": "org.apache.hadoop.mapred.gridmix.TestHighRamJob"}, {"methodBody": ["METHOD_START", "{", "Configuration   gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setBoolean ( GridmixJob . GRIDMIX _ HIGHRAM _ EMULATION _ ENABLE ,    false )  ;", ". testHighRamConfig (  1  0  ,     2  0  ,     5  ,     1  0  ,    DEFAULT _ MAP _ MEMORY _ MB ,    DEFAULT _ REDUCE _ MEMORY _ MB ,    DEFAULT _ MAP _ MEMORY _ MB ,    DEFAULT _ REDUCE _ MEMORY _ MB ,    gridmixConf )  ;", "gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setLong ( UPPER _ LIMIT _ ON _ TASK _ VMEM _ PROPERTY ,     (  (  2  0     *     1  0  2  4  )     *     1  0  2  4  )  )  ;", ". testHighRamConfig (  1  0  ,     2  0  ,     5  ,     1  0  ,     5  ,     1  0  ,     1  0  ,     2  0  ,    gridmixConf )  ;", "gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setLong ( JT _ MAX _ MAPMEMORY _ MB ,     1  0  0  )  ;", "gridmixConf . setLong ( JT _ MAX _ REDUCEMEMORY _ MB ,     3  0  0  )  ;", ". testHighRamConfig (  1  0  ,     4  5  ,     5  ,     1  5  ,     5  0  ,     1  0  0  ,     1  0  0  ,     3  0  0  ,    gridmixConf )  ;", "gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setLong ( UPPER _ LIMIT _ ON _ TASK _ VMEM _ PROPERTY ,     (  (  7  0     *     1  0  2  4  )     *     1  0  2  4  )  )  ;", "Boolean   failed    =    null ;", "try    {", ". testHighRamConfig (  1  0  ,     4  5  ,     5  ,     1  5  ,     5  0  ,     1  0  0  ,     1  0  0  ,     3  0  0  ,    gridmixConf )  ;", "failed    =    false ;", "}    catch    ( Exception   e )     {", "failed    =    true ;", "}", "assertNotNull ( failed )  ;", "assertTrue (  (  \" Exception   expected   for   exceeding   map   memory   limit    \"     +     \"  ( deprecation )  !  \"  )  ,    failed )  ;", "gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setLong ( UPPER _ LIMIT _ ON _ TASK _ VMEM _ PROPERTY ,     (  (  1  5  0     *     1  0  2  4  )     *     1  0  2  4  )  )  ;", "failed    =    null ;", "try    {", ". testHighRamConfig (  1  0  ,     4  5  ,     5  ,     1  5  ,     5  0  ,     1  0  0  ,     1  0  0  ,     3  0  0  ,    gridmixConf )  ;", "failed    =    false ;", "}    catch    ( Exception   e )     {", "failed    =    true ;", "}", "assertNotNull ( failed )  ;", "assertTrue (  (  \" Exception   expected   for   exceeding   reduce   memory   limit    \"     +     \"  ( deprecation )  !  \"  )  ,    failed )  ;", "gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setLong ( JT _ MAX _ MAPMEMORY _ MB ,     7  0  )  ;", "failed    =    null ;", "try    {", ". testHighRamConfig (  1  0  ,     4  5  ,     5  ,     1  5  ,     5  0  ,     1  0  0  ,     1  0  0  ,     3  0  0  ,    gridmixConf )  ;", "failed    =    false ;", "}    catch    ( Exception   e )     {", "failed    =    true ;", "}", "assertNotNull ( failed )  ;", "assertTrue (  \" Exception   expected   for   exceeding   map   memory   limit !  \"  ,    failed )  ;", "gridmixConf    =    new   Configuration (  )  ;", "gridmixConf . setLong ( JT _ MAX _ REDUCEMEMORY _ MB ,     2  0  0  )  ;", "failed    =    null ;", "try    {", ". testHighRamConfig (  1  0  ,     4  5  ,     5  ,     1  5  ,     5  0  ,     1  0  0  ,     1  0  0  ,     3  0  0  ,    gridmixConf )  ;", "failed    =    false ;", "}    catch    ( Exception   e )     {", "failed    =    true ;", "}", "assertNotNull ( failed )  ;", "assertTrue (  \" Exception   expected   for   exceeding   reduce   memory   limit !  \"  ,    failed )  ;", "}", "METHOD_END"], "methodName": ["testHighRamFeatureEmulation"], "fileName": "org.apache.hadoop.mapred.gridmix.TestHighRamJob"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . initCluster ( TestLoadJob . class )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.mapred.gridmix.TestLoadJob"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . shutdownCluster (  )  ;", "}", "METHOD_END"], "methodName": ["shutDown"], "fileName": "org.apache.hadoop.mapred.gridmix.TestLoadJob"}, {"methodBody": ["METHOD_START", "{", "CommonJobTest . policy    =    GridmixJobSubmissionPolicy . REPLAY ;", ". LOG . info (  (  \"    Replay   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( JobCreator . LOADJOB . name (  )  ,    false )  ;", ". LOG . info (  (  \"    Replay   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testReplaySubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestLoadJob"}, {"methodBody": ["METHOD_START", "{", "CommonJobTest . policy    =    GridmixJobSubmissionPolicy . SERIAL ;", ". LOG . info (  (  \" Serial   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( JobCreator . LOADJOB . name (  )  ,    false )  ;", ". LOG . info (  (  \" Serial   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSerialSubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestLoadJob"}, {"methodBody": ["METHOD_START", "{", "PseudoLocalFs   pfs    =    new   PseudoLocalFs (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass (  \" fs . pseudo . impl \"  ,    PseudoLocalFs . class ,    FileSystem . class )  ;", "Path   path    =    new   Path (  \" pseudo :  /  /  / myPsedoFile .  1  2  3  4  \"  )  ;", "FileSystem   testFs    =    path . getFileSystem ( conf )  ;", "assertEquals (  \" Failed   to   obtain   a   pseudo   local   file   system   object   from   path \"  ,    pfs . getUri (  )  . getScheme (  )  ,    testFs . getUri (  )  . getScheme (  )  )  ;", "path    =    new   Path (  \" file :  /  /  / myPsedoFile .  1  2  3  4  5  \"  )  ;", "validateGetFileStatus ( pfs ,    path ,    false )  ;", "validateCreate ( pfs ,    path ,    false )  ;", "validateOpen ( pfs ,    path ,    false )  ;", "validateExists ( pfs ,    path ,    false )  ;", "path    =    new   Path (  \" pseudo :  /  /  / myPsedoFile \"  )  ;", "validateGetFileStatus ( pfs ,    path ,    false )  ;", "validateCreate ( pfs ,    path ,    false )  ;", "validateOpen ( pfs ,    path ,    false )  ;", "validateExists ( pfs ,    path ,    false )  ;", "path    =    new   Path (  \" pseudo :  /  /  / myPsedoFile . txt \"  )  ;", "validateGetFileStatus ( pfs ,    path ,    false )  ;", "validateCreate ( pfs ,    path ,    false )  ;", "validateOpen ( pfs ,    path ,    false )  ;", "validateExists ( pfs ,    path ,    false )  ;", "long   fileSize    =     2  3  1  4  5  6  ;", "path    =    PseudoLocalFs . generateFilePath (  \" my . Psedo . File \"  ,    fileSize )  ;", "assertEquals (  \" generateFilePath (  )    failed .  \"  ,    fileSize ,    pfs . validateFileNameFormat ( path )  )  ;", "validateGetFileStatus ( pfs ,    path ,    true )  ;", "validateCreate ( pfs ,    path ,    true )  ;", "validateOpen ( pfs ,    path ,    true )  ;", "validateExists ( pfs ,    path ,    true )  ;", "path    =    new   Path (  \" myPsedoFile .  1  2  3  7  \"  )  ;", "path    =    path . makeQualified ( pfs )  ;", "validateGetFileStatus ( pfs ,    path ,    true )  ;", "validateCreate ( pfs ,    path ,    true )  ;", "validateOpen ( pfs ,    path ,    true )  ;", "validateExists ( pfs ,    path ,    true )  ;", "}", "METHOD_END"], "methodName": ["testPseudoLocalFsFileNames"], "fileName": "org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "long   fileSize    =     1  0  0  0  0  ;", "Path   path    =     . generateFilePath (  \" myPsedoFile \"  ,    fileSize )  ;", "pfs    =    new    (  )  ;", "pfs . create ( path )  ;", "InputStream   in    =    pfs . open ( path ,     0  )  ;", "long   totalSize    =     0  ;", "while    (  ( in . read (  )  )     >  =     0  )     {", "+  + totalSize ;", "}", "in . close (  )  ;", "assertEquals (  \" File   size   mismatch   with   read (  )  .  \"  ,    fileSize ,    totalSize )  ;", "in    =    pfs . open ( path ,     0  )  ;", "totalSize    =     0  ;", "byte [  ]    b    =    new   byte [  1  0  2  4  ]  ;", "int   bytesRead    =    in . read ( b )  ;", "while    ( bytesRead    >  =     0  )     {", "totalSize    +  =    bytesRead ;", "bytesRead    =    in . read ( b )  ;", "}", "assertEquals (  \" File   size   mismatch   with   read ( byte [  ]  )  .  \"  ,    fileSize ,    totalSize )  ;", "}", "METHOD_END"], "methodName": ["testPseudoLocalFsFileSize"], "fileName": "org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "boolean   expectedExceptionSeen    =    false ;", "try    {", "pfs . create ( path )  ;", "}    catch    ( IOException   e )     {", "expectedExceptionSeen    =    true ;", "}", "if    ( shouldSucceed )     {", "assertFalse (  (  \" create (  )    has   thrown   Exception   for   valid   file   name    \"     +    path )  ,    expectedExceptionSeen )  ;", "} else    {", "assertTrue (  (  \" create (  )    did   not   throw   Exception   for   invalid   file   name    \"     +    path )  ,    expectedExceptionSeen )  ;", "}", "}", "METHOD_END"], "methodName": ["validateCreate"], "fileName": "org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "boolean   ret    =    pfs . exists ( path )  ;", "if    ( shouldSucceed )     {", "assertTrue (  (  \" exists (  )    returned   false   for   valid   file   name    \"     +    path )  ,    ret )  ;", "} else    {", "assertFalse (  (  \" exists (  )    returned   true   for   invalid   file   name    \"     +    path )  ,    ret )  ;", "}", "}", "METHOD_END"], "methodName": ["validateExists"], "fileName": "org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "boolean   expectedExceptionSeen    =    false ;", "FileStatus   stat    =    null ;", "try    {", "stat    =    pfs . getFileStatus ( path )  ;", "}    catch    ( FileNotFoundException   e )     {", "expectedExceptionSeen    =    true ;", "}", "if    ( shouldSucceed )     {", "assertFalse (  (  \" getFileStatus (  )    has   thrown   Exception   for   valid   file   name    \"     +    path )  ,    expectedExceptionSeen )  ;", "assertNotNull (  \" Missing   file   status   for   a   valid   file .  \"  ,    stat )  ;", "String [  ]    parts    =    path . toUri (  )  . getPath (  )  . split (  \"  \\  \\  .  \"  )  ;", "long   expectedFileSize    =    Long . valueOf ( parts [  (  ( parts . length )     -     1  )  ]  )  ;", "assertEquals (  \" Invalid   file   size .  \"  ,    expectedFileSize ,    stat . getLen (  )  )  ;", "} else    {", "assertTrue (  (  (  \" getFileStatus (  )    did   not   throw   Exception   for   invalid   file    \"     +     \"    name    \"  )     +    path )  ,    expectedExceptionSeen )  ;", "}", "}", "METHOD_END"], "methodName": ["validateGetFileStatus"], "fileName": "org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "boolean   expectedExceptionSeen    =    false ;", "try    {", "pfs . open ( path )  ;", "}    catch    ( IOException   e )     {", "expectedExceptionSeen    =    true ;", "}", "if    ( shouldSucceed )     {", "assertFalse (  (  \" open (  )    has   thrown   Exception   for   valid   file   name    \"     +    path )  ,    expectedExceptionSeen )  ;", "} else    {", "assertTrue (  (  \" open (  )    did   not   throw   Exception   for   invalid   file   name    \"     +    path )  ,    expectedExceptionSeen )  ;", "}", "}", "METHOD_END"], "methodName": ["validateOpen"], "fileName": "org.apache.hadoop.mapred.gridmix.TestPseudoLocalFs"}, {"methodBody": ["METHOD_START", "{", "List < Integer >    ret    =    new   ArrayList < Integer >  ( from . length )  ;", "for    ( int   v    :    from )     {", "ret . add ( v )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["convertIntArray"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm"}, {"methodBody": ["METHOD_START", "{", "for    ( int [  ]    param    :    TestRandomAlgorithm . parameters )     {", "testRandomSelect ( param [  0  ]  ,    param [  1  ]  ,    param [  2  ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRandomSelect"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm"}, {"methodBody": ["METHOD_START", "{", "Random   random    =    new   Random (  )  ;", "Map < List < Integer >  ,    Integer >    results    =    new   HashMap < List < Integer >  ,    Integer >  ( niter )  ;", "for    ( int   i    =     0  ;    i    <    niter ;     +  + i )     {", "int [  ]    result    =    s . select ( m ,    n ,    random )  ;", "Arrays . sort ( result )  ;", "List < Integer >    resultAsList    =    convertIntArray ( result )  ;", "Integer   count    =    results . get ( resultAsList )  ;", "if    ( count    =  =    null )     {", "results . put ( resultAsList ,     1  )  ;", "} else    {", "results . put ( resultAsList ,     (  +  + count )  )  ;", "}", "}", "verifyResults ( results ,    m ,    n )  ;", "}", "METHOD_END"], "methodName": ["testRandomSelect"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm"}, {"methodBody": ["METHOD_START", "{", "for    ( int [  ]    param    :    TestRandomAlgorithm . parameters )     {", "testRandomSelectSelector ( param [  0  ]  ,    param [  1  ]  ,    param [  2  ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRandomSelectSelector"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm"}, {"methodBody": ["METHOD_START", "{", "RandomAlgorithms . Selector   selector    =    new   RandomAlgorithms . Selector ( n ,     (  (  ( double )     ( m )  )     /    n )  ,    new   Random (  )  )  ;", "Map < List < Integer >  ,    Integer >    results    =    new   HashMap < List < Integer >  ,    Integer >  ( niter )  ;", "for    ( int   i    =     0  ;    i    <    niter ;     +  + i    ,    selector . reset (  )  )     {", "int [  ]    result    =    new   int [ m ]  ;", "for    ( int   j    =     0  ;    j    <    m ;     +  + j )     {", "int   v    =    selector . next (  )  ;", "if    ( v    <     0  )", "break ;", "result [ j ]     =    v ;", "}", "Arrays . sort ( result )  ;", "List < Integer >    resultAsList    =    convertIntArray ( result )  ;", "Integer   count    =    results . get ( resultAsList )  ;", "if    ( count    =  =    null )     {", "results . put ( resultAsList ,     1  )  ;", "} else    {", "results . put ( resultAsList ,     (  +  + count )  )  ;", "}", "}", "verifyResults ( results ,    m ,    n )  ;", "}", "METHOD_END"], "methodName": ["testRandomSelectSelector"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm"}, {"methodBody": ["METHOD_START", "{", "if    ( n    >  =     1  0  )     {", "assertTrue (  (  ( results . size (  )  )     >  =     ( Mathin ,     2  )  )  )  )  ;", "}", "for    ( List < Integer >    result    :    results . keySet (  )  )     {", "assertEquals ,    result . size (  )  )  ;", "Set < Integer >    seen    =    new   HashSet < Integer >  (  )  ;", "for    ( int   v    :    result )     {", "Syst . out . printf (  \"  % d    \"  ,    v )  ;", "assertTrue (  (  ( v    >  =     0  )     &  &     ( v    <    n )  )  )  ;", "assertTrue ( seen . add ( v )  )  ;", "}", "Syst . out . printf (  \"     =  =  >     % d \\ n \"  ,    results . get ( result )  )  ;", "}", "Syst . out . println (  \"  =  =  =  =  \"  )  ;", "}", "METHOD_END"], "methodName": ["verifyResults"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomAlgorithm"}, {"methodBody": ["METHOD_START", "{", "RandomTextDataGenerator   rtdg    =    new   RandomTextDataGenerator (  1  0  ,     0 L ,     5  )  ;", "List < String >    words    =    rtdg . getRandomWords (  )  ;", "assertEquals (  \" List   size   mismatch \"  ,     1  0  ,    words . size (  )  )  ;", "Set < String >    wordsSet    =    new   HashSet < String >  ( words )  ;", "assertEquals (  \" List   size   mismatch   due   to   duplicates \"  ,     1  0  ,    wordsSet . size (  )  )  ;", "for    ( String   word    :    wordsSet )     {", "assertEquals (  \" Word   size   mismatch \"  ,     5  ,    word . length (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRandomTextDataGenerator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "RandomTextDataGenerator   rtdg 1     =    new   RandomTextDataGenerator (  1  0  ,     0 L ,     5  )  ;", "List < String >    words 1     =    rtdg 1  . getRandomWords (  )  ;", "RandomTextDataGenerator   rtdg 2     =    new   RandomTextDataGenerator (  1  0  ,     0 L ,     5  )  ;", "List < String >    words 2     =    rtdg 2  . getRandomWords (  )  ;", "assertTrue (  \" List   mismatch \"  ,    words 1  . equals ( words 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testRandomTextDataGeneratorRepeatability"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "RandomTextDataGenerator   rtdg 1     =    new   RandomTextDataGenerator (  1  0  ,     1 L ,     5  )  ;", "Set < String >    words 1     =    new   HashSet ( rtdg 1  . getRandomWords (  )  )  ;", "RandomTextDataGenerator   rtdg 2     =    new   RandomTextDataGenerator (  1  0  ,     0 L ,     5  )  ;", "Set < String >    words 2     =    new   HashSet ( rtdg 2  . getRandomWords (  )  )  ;", "assertFalse (  \" List   size   mismatch   across   lists \"  ,    words 1  . equals ( words 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testRandomTextDataGeneratorUniqueness"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRandomTextDataGenerator"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   avgsize    =     ( r . nextInt (  (  1     <  <     1  0  )  )  )     +     1  ;", "final   long   targetRecs    =    r . nextInt (  (  1     <  <     1  4  )  )  ;", ". testFactory (  ( targetRecs    *    avgsize )  ,    targetRecs )  ;", "}", "METHOD_END"], "methodName": ["testAvg"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRecordFactory"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   GridmixKey   key    =    new   GridmixKey (  )  ;", "final   GridmixRecord   val    =    new   GridmixRecord (  )  ;", ". LOG . info (  (  (  (  \" Target   bytes / records :     \"     +    targetBytes )     +     \"  /  \"  )     +    targetRecs )  )  ;", "final   RecordFactory   f    =    new   AvgRecordFactory ( targetBytes ,    targetRecs ,    conf )  ;", "targetRecs    =     (  ( targetRecs    <  =     0  )     &  &     ( targetBytes    >  =     0  )  )     ?    Math . max (  1  ,     ( targetBytes    /     ( conf . getInt ( AvgRecordFactory . GRIDMIX _ MISSING _ REC _ SIZE ,     (  6  4     *     1  0  2  4  )  )  )  )  )     :    targetRecs ;", "long   records    =     0 L ;", "final   DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "while    ( f . next ( key ,    val )  )     {", "+  + records ;", "key . write ( out )  ;", "val . write ( out )  ;", "}", "assertEquals ( targetRecs ,    records )  ;", "assertEquals ( targetBytes ,    out . getLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFactory"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRecordFactory"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   targetBytes    =     ( r . nextInt (  (  1     <  <     2  0  )  )  )     +     (  3     *     (  1     <  <     1  4  )  )  ;", "final   long   targetRecs    =    r . nextInt (  (  1     <  <     1  4  )  )  ;", ". testFactory ( targetBytes ,    targetRecs )  ;", "}", "METHOD_END"], "methodName": ["testRandom"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRecordFactory"}, {"methodBody": ["METHOD_START", "{", "final   Random   r    =    new   Random (  )  ;", "final   long   targetBytes    =    r . nextInt (  (  1     <  <     2  0  )  )  ;", ". testFactory ( targetBytes ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testZero"], "fileName": "org.apache.hadoop.mapred.gridmix.TestRecordFactory"}, {"methodBody": ["METHOD_START", "{", "ResourceUsageMetrics   metrics    =    new   ResourceUsageMetrics (  )  ;", "metrics . setCumulativeCpuUsage ( target )  ;", "metrics . setVirtualMemoryUsage ( target )  ;", "metrics . setPhysicalMemoryUsage ( target )  ;", "metrics . setHeapUsage ( target )  ;", "return   metrics ;", "}", "METHOD_END"], "methodName": ["createMetrics"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "long   target    =     1  0  0  0  0  0 L ;", "int   unitUsage    =     5  0  ;", ". FakeCpuUsageEmulatorCore   fakeCpuEmulator    =    new    . FakeCpuUsageEmulatorCore (  )  ;", "fakeCpuEmulator . setUnitUsage ( unitUsage )  ;", ". FakeResourceUsageMonitor   fakeMonitor    =    new    . FakeResourceUsageMonitor ( fakeCpuEmulator )  ;", "fakeCpuEmulator . calibrate ( fakeMonitor ,    target )  ;", "assertEquals (  \" Fake   calibration   failed \"  ,     1  0  0  ,    fakeMonitor . getCumulativeCpuTime (  )  )  ;", "assertEquals (  \" Fake   calibration   failed \"  ,     1  0  0  ,    fakeCpuEmulator . getCpuUsage (  )  )  ;", "assertEquals (  \" Fake   calibration   failed \"  ,     2  ,    fakeCpuEmulator . getNumCalls (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCpuUsageEmulator"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "long   targetCpuUsage    =     1  0  0  0 L ;", "int   unitCpuUsage    =     5  0  ;", ". FakeProgressive   fakeProgress    =    new    . FakeProgressive (  )  ;", ". FakeCpuUsageEmulatorCore   fakeCore    =    new    . FakeCpuUsageEmulatorCore (  )  ;", "fakeCore . setUnitUsage ( unitCpuUsage )  ;", "CumulativeCpuUsageEmulatorPlugin   cpuPlugin    =    new   CumulativeCpuUsageEmulatorPlugin ( fakeCore )  ;", "ResourceUsageMetrics   invalidUsage    =     . createMetrics (  0  )  ;", "cpuPlugin . initialize ( conf ,    invalidUsage ,    null ,    null )  ;", "int   numCallsPre    =    fakeCore . getNumCalls (  )  ;", "long   cpuUsagePre    =    fakeCore . getCpuUsage (  )  ;", "cpuPlugin . emulate (  )  ;", "int   numCallsPost    =    fakeCore . getNumCalls (  )  ;", "long   cpuUsagePost    =    fakeCore . getCpuUsage (  )  ;", "assertEquals (  \" Disabled   cumulative   CPU   usage   emulation   plugin   works !  \"  ,    numCallsPre ,    numCallsPost )  ;", "assertEquals (  \" Disabled   cumulative   CPU   usage   emulation   plugin   works !  \"  ,    cpuUsagePre ,    cpuUsagePost )  ;", "float   progress    =    cpuPlugin . getProgress (  )  ;", "assertEquals (  (  \" Invalid   progress   of   disabled   cumulative   CPU   usage   emulation    \"     +     \" plugin !  \"  )  ,     1  .  0 F ,    progress ,     0  .  0 F )  ;", "ResourceUsageMetrics   metrics    =     . createMetrics ( targetCpuUsage )  ;", "ResourceCalculatorPlugin   monitor    =    new    . FakeResourceUsageMonitor ( fakeCore )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    cpuPlugin ,    targetCpuUsage ,     ( targetCpuUsage    /    unitCpuUsage )  )  ;", "conf . setFloat ( CumulativeCpuUsageEmulatorPlugin . CPU _ EMULATION _ PROGRESS _ INTERVAL ,     0  .  2 F )  ;", ". testEmulationAccuracy ( conf ,    fakeCore ,    monitor ,    metrics ,    cpuPlugin ,    targetCpuUsage ,     ( targetCpuUsage    /    unitCpuUsage )  )  ;", "fakeProgress    =    new    . FakeProgressive (  )  ;", "fakeCore . reset (  )  ;", "fakeCore . setUnitUsage (  1  )  ;", "conf . setFloat ( CumulativeCpuUsageEmulatorPlugin . CPU _ EMULATION _ PROGRESS _ INTERVAL ,     0  .  2  5 F )  ;", "cpuPlugin . initialize ( conf ,    metrics ,    monitor ,    fakeProgress )  ;", "long   initCpuUsage    =    monitor . getCumulativeCpuTime (  )  ;", "long   initNumCalls    =    fakeCore . getNumCalls (  )  ;", ". testEmulationBoundary (  0  .  0 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    initCpuUsage ,    initNumCalls ,     \"  [ no - op ,     0    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  2  4 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    initCpuUsage ,    initNumCalls ,     \"  [ no - op ,     2  4  %    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  2  5 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    initCpuUsage ,    initNumCalls ,     \"  [ op ,     2  5  %    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  8 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,     4  1  0  ,     4  1  0  ,     \"  [ op ,     8  0  %    progress ]  \"  )  ;", ". testEmulationBoundary (  1  .  0 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    targetCpuUsage ,    targetCpuUsage ,     \"  [ op ,     1  0  0  %    progress ]  \"  )  ;", "fakeProgress    =    new    . FakeProgressive (  )  ;", "fakeCore . reset (  )  ;", "fakeCore . setUnitUsage ( unitCpuUsage )  ;", "conf . setFloat ( CumulativeCpuUsageEmulatorPlugin . CPU _ EMULATION _ PROGRESS _ INTERVAL ,     0  .  4 F )  ;", "cpuPlugin . initialize ( conf ,    metrics ,    monitor ,    fakeProgress )  ;", "initCpuUsage    =    monitor . getCumulativeCpuTime (  )  ;", "initNumCalls    =    fakeCore . getNumCalls (  )  ;", ". testEmulationBoundary (  0  .  0 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    initCpuUsage ,    initNumCalls ,     \"  [ no - op ,     0    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  3  9 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    initCpuUsage ,    initNumCalls ,     \"  [ no - op ,     3  9  %    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  4 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    initCpuUsage ,    initNumCalls ,     \"  [ op ,     4  0  %    progress ]  \"  )  ;", ". testEmulationBoundary (  0  .  9 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,     7  0  0  ,     (  7  0  0     /    unitCpuUsage )  ,     \"  [ op ,     9  0  %    progress ]  \"  )  ;", ". testEmulationBoundary (  1  .  0 F ,    fakeCore ,    fakeProgress ,    cpuPlugin ,    targetCpuUsage ,     ( targetCpuUsage    /    unitCpuUsage )  ,     \"  [ op ,     1  0  0  %    progress ]  \"  )  ;", "}", "METHOD_END"], "methodName": ["testCumulativeCpuUsageEmulatorPlugin"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "TestResourceUsageEmulators . FakeProgressive   fakeProgress    =    new   TestResourceUsageEmulators . FakeProgressive (  )  ;", "fakeCore . reset (  )  ;", "cpuPlugin . initialize ( conf ,    metrics ,    monitor ,    fakeProgress )  ;", "int   numLoops    =     0  ;", "while    (  ( fakeProgress . getProgress (  )  )     <     1  )     {", "+  + numLoops ;", "float   progress    =     (  ( float )     ( numLoops )  )     /     1  0  0  ;", "fakeProgress . setProgress ( progress )  ;", "cpuPlugin . emulate (  )  ;", "}", "assertEquals (  \" Cumulative   cpu   usage   emulator   plugin   failed    ( num   calls )  !  \"  ,    expectedTotalNumCalls ,    fakeCore . getNumCalls (  )  ,     0 L )  ;", "assertEquals (  \" Cumulative   cpu   usage   emulator   plugin   failed    ( total   usage )  !  \"  ,    expectedTotalCpuUsage ,    fakeCore . getCpuUsage (  )  ,     0 L )  ;", "}", "METHOD_END"], "methodName": ["testEmulationAccuracy"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "fakeProgress . setProgress ( progress )  ;", "cpuPlugin . emulate (  )  ;", "assertEquals (  (  (  \" ion   interval   test   for   cpu   usage   failed    \"     +    info )     +     \"  !  \"  )  ,    expectedTotalCpuUsage ,    fakeCore . getCpuUsage (  )  ,     0 L )  ;", "assertEquals (  (  (  \" ion   interval   test   for   num   calls   failed    \"     +    info )     +     \"  !  \"  )  ,    expectedTotalNumCalls ,    fakeCore . getNumCalls (  )  ,     0 L )  ;", "}", "METHOD_END"], "methodName": ["testEmulationBoundary"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "ResourceUsageMatcher   matcher    =    new   ResourceUsageMatcher (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( ResourceUsageMatcher . RESOURCE _ USAGE _ EMULATION _ PLUGINS , sPlugin . class ,    ResourceUsageEmulatorPlugin . class )  ;", "long   currentTime    =    System . currentTimeMillis (  )  ;", "matcher . configure ( conf ,    null ,    null ,    null )  ;", "matcher . matchResourceUsage (  )  ;", "String   id    = sPlugin . DEFAULT _ IDENTIFIER ;", "long   result    = sPlugin . testInitialization ( id ,    conf )  ;", "assertTrue (  (  \" Resource   usage   matcher   failed   to   initialize   the   configured \"     +     \"    plugin \"  )  ,     ( result    >    currentTime )  )  ;", "result    = sPlugin . testEmulation ( id ,    conf )  ;", "assertTrue (  (  \" Resource   usage   matcher   failed   to   load   and   emulate   the \"     +     \"    configured   plugin \"  )  ,     ( result    >    currentTime )  )  ;", "conf . setStrings ( ResourceUsageMatcher . RESOURCE _ USAGE _ EMULATION _ PLUGINS ,     (  ( s . TestCpu . class . getName (  )  )     +     \"  ,  \"  )     +    s . TestOthers . class . getName (  )  )  )  )  ;", "matcher . configure ( conf ,    null ,    null ,    null )  ;", "long   time 1     = sPlugin . testInitializations . TestCpu . ID ,    conf )  ;", "long   time 2     = sPlugin . testInitializations . TestOthers . ID ,    conf )  ;", "assertTrue (  (  \" Resource   usage   matcher   failed   to   initialize   the   configured \"     +     \"    plugins   in   order \"  )  ,     ( time 1     <    time 2  )  )  ;", "matcher . matchResourceUsage (  )  ;", "time 1     = sPlugin . testInitializations . TestCpu . ID ,    conf )  ;", "time 2     = sPlugin . testInitializations . TestOthers . ID ,    conf )  ;", "assertTrue (  \" Resource   usage   matcher   failed   to   load   the   configured   plugins \"  ,     ( time 1     <    time 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceUsageMatcher"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", ". FakeProgressive   progress    =    new    . FakeProgressive (  )  ;", "conf . setClass ( TT _ RESOURCE _ CALCULATOR _ PLUGIN ,    DummyResourceCalculatorPlugin . class ,    ResourceCalculatorPlugin . class )  ;", "conf . setClass ( ResourceUsageMatcher . RESOURCE _ USAGE _ EMULATION _ PLUGINS ,     . TestResourceUsageEmulatorPlugin . class ,    ResourceUsageEmulatorPlugin . class )  ;", "long   currentTime    =    System . currentTimeMillis (  )  ;", "TaskAttemptID   id    =    new   TaskAttemptID (  \" test \"  ,     1  ,    TaskType . MAP ,     1  ,     1  )  ;", "StatusReporter   reporter    =    new    . DummyReporter ( progress )  ;", "TaskInputOutputContext   context    =    new   MapContextImpl ( conf ,    id ,    null ,    null ,    null ,    reporter ,    null )  ;", ". FakeResourceUsageMatcherRunner   matcher    =    new    . FakeResourceUsageMatcherRunner ( context ,    null )  ;", "String   identifier    =     . TestResourceUsageEmulatorPlugin . DEFAULT _ IDENTIFIER ;", "long   initTime    =     . TestResourceUsageEmulatorPlugin . testInitialization ( identifier ,    conf )  ;", "assertTrue (  (  \" ResourceUsageMatcherRunner   failed   to   initialize   the \"     +     \"    configured   plugin \"  )  ,     ( initTime    >    currentTime )  )  ;", "assertEquals (  \" Progress   mismatch   in   ResourceUsageMatcherRunner \"  ,     0  ,    progress . getProgress (  )  ,     0  .  0  )  ;", "progress . setProgress (  0  .  0  1 F )  ;", "currentTime    =    System . currentTimeMillis (  )  ;", "matcher . test (  )  ;", "long   emulateTime    =     . TestResourceUsageEmulatorPlugin . testEmulation ( identifier ,    conf )  ;", "assertTrue (  (  \" ProgressBasedResourceUsageMatcher   failed   to   load   and   emulate \"     +     \"    the   configured   plugin \"  )  ,     ( emulateTime    >    currentTime )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceUsageMatcherRunner"], "fileName": "org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . initCluster ( TestSleepJob . class )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "GridmixTestUtils . shutdownCluster (  )  ;", "}", "METHOD_END"], "methodName": ["shutDown"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    GridmixTestUtils . mrvl . getConfig (  )  ;", "DebugJobProducer   jobProducer    =    new   DebugJobProducer (  5  ,    configuration )  ;", "configuration . setBoolean (  . SLEEPJOB _ MAPTASK _ ONLY ,    true )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getLoginUser (  )  ;", "JobStory   story ;", "int   seq    =     1  ;", "while    (  ( story    =    jobProducer . getNextJob (  )  )     !  =    null )     {", "GridmixJob   gridmixJob    =    JobCreator . SLEEPJOB . createGridmixJob ( configuration ,     0  ,    story ,    new   Path (  \" ignored \"  )  ,    ugi ,     ( seq +  +  )  )  ;", "gridmixJob . buildSplits ( null )  ;", "Job   job    =    gridmixJob . call (  )  ;", "assertEquals (  0  ,    job . getNumReduceTasks (  )  )  ;", "}", "jobProducer . close (  )  ;", "assertEquals (  6  ,    seq )  ;", "}", "METHOD_END"], "methodName": ["testMapTasksOnlySleepJobs"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi    =    UserGroupInformation . getLoginUser (  )  ;", "tRandomLocation (  1  ,     1  0  ,    ugi )  ;", "tRandomLocation (  2  ,     1  0  ,    ugi )  ;", "}", "METHOD_END"], "methodName": ["testRandomLocation"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    new   Configuration (  )  ;", "DebugJobProducer   jobProducer    =    new   DebugJobProducer ( njobs ,    configuration )  ;", "Configuration   jconf    =    GridmixTestUtils . mrvl . getConfig (  )  ;", "jconf . setInt ( JobCreator . SLEEPJOB _ RANDOM _ LOCATIONS ,    locations )  ;", "JobStory   story ;", "int   seq    =     1  ;", "while    (  ( story    =    jobProducer . getNextJob (  )  )     !  =    null )     {", "GridmixJob   gridmixJob    =    JobCreator . SLEEPJOB . createGridmixJob ( jconf ,     0  ,    story ,    new   Path (  \" ignored \"  )  ,    ugi ,     ( seq +  +  )  )  ;", "gridmixJob . buildSplits ( null )  ;", "List < InputSplit >    splits    =    new    . SleepInputFormat (  )  . getSplits ( gridmixJob . getJob (  )  )  ;", "for    ( InputSplit   split    :    splits )     {", "assertEquals ( locations ,    split . getLocations (  )  . length )  ;", "}", "}", "jobProducer . close (  )  ;", "}", "METHOD_END"], "methodName": ["testRandomLocation"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "TestSleepJob . policy    =    GridmixJobSubmissionPolicy . REPLAY ;", "TestSleepJob . LOG . info (  (  \"    Replay   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( JobCreator . SLEEPJOB . name (  )  ,    false )  ;", "TestSleepJob . LOG . info (  (  \"    Replay   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testReplaySubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "TestSleepJob . policy    =    GridmixJobSubmissionPolicy . SERIAL ;", "TestSleepJob . LOG . info (  (  \" Serial   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( JobCreator . SLEEPJOB . name (  )  ,    false )  ;", "TestSleepJob . LOG . info (  (  \" Serial   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSerialSubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "TestSleepJob . policy    =    GridmixJobSubmissionPolicy . STRESS ;", "TestSleepJob . LOG . info (  (  \"    Replay   started   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "doSubmission ( JobCreator . SLEEPJOB . name (  )  ,    false )  ;", "TestSleepJob . LOG . info (  (  \"    Replay   ended   at    \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testStressSubmit"], "fileName": "org.apache.hadoop.mapred.gridmix.TestSleepJob"}, {"methodBody": ["METHOD_START", "{", "TestUserResolve . conf    =    new   Configuration (  )  ;", "TestUserResolve . fs    =    FileSystem . getLocal ( TestUserResolve . conf )  ;", "TestUserResolve . rootDir    =    new   Path ( new   Path ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . makeQualified ( TestUserResolve . fs )  ,     \" gridmixUserResolve \"  )  ;", "}", "METHOD_END"], "methodName": ["createRootDir"], "fileName": "org.apache.hadoop.mapred.gridmix.TestUserResolve"}, {"methodBody": ["METHOD_START", "{", "final   UserResolver   rslv    =    new   RoundRobinUserResolver (  )  ;", "Path   usersFilePath    =    new   Path (  . rootDir ,     \" users \"  )  ;", "URI   userRsrc    =    new   URI ( usersFilePath . toString (  )  )  ;", ". fs . delete ( usersFilePath ,    false )  ;", "String   expectedErrorMsg    =     (  \" File    \"     +    userRsrc )     +     \"    does   not   exist \"  ;", "validateBadUsersFile ( rslv ,    userRsrc ,    expectedErrorMsg )  ;", ". writeUserList ( usersFilePath ,     \"  \"  )  ;", "expectedErrorMsg    =    RoundRobinUserResolver . buildEmptyUsersErrorMsg ( userRsrc )  ;", "validateBadUsersFile ( rslv ,    userRsrc ,    expectedErrorMsg )  ;", ". writeUserList ( usersFilePath ,     \" user 0  , groupA , groupB , groupC \\ nuser 1  , groupA , groupC \\ n \"  )  ;", "validateValidUsersFile ( rslv ,    userRsrc )  ;", ". writeUserList ( usersFilePath ,     \" user 0  , groupA , groupB \\ nuser 1  ,  \"  )  ;", "validateValidUsersFile ( rslv ,    userRsrc )  ;", ". writeUserList ( usersFilePath ,     \" user 0  \\ nuser 1  \"  )  ;", "validateValidUsersFile ( rslv ,    userRsrc )  ;", "}", "METHOD_END"], "methodName": ["testRoundRobinResolver"], "fileName": "org.apache.hadoop.mapred.gridmix.TestUserResolve"}, {"methodBody": ["METHOD_START", "{", "final   UserResolver   rslv    =    new   SubmitterUserResolver (  )  ;", "assertFalse ( rslv . needsTargetUsersList (  )  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getCurrentUser (  )  ;", "assertEquals ( ugi ,    rslv . getTargetUgi (  (  ( UserGroupInformation )     ( null )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSubmitterResolver"], "fileName": "org.apache.hadoop.mapred.gridmix.TestUserResolve"}, {"methodBody": ["METHOD_START", "{", "boolean   fail    =    false ;", "try    {", "rslv . setTargetUsers ( userRsrc ,     . conf )  ;", "}    catch    ( IOException   e )     {", "assertTrue (  \" Exception   message   from   RoundRobinUserResolver   is   wrong \"  ,    e . getMessage (  )  . equals ( expectedErrorMsg )  )  ;", "fail    =    true ;", "}", "assertTrue (  \" User   list   required   for   RoundRobinUserResolver \"  ,    fail )  ;", "}", "METHOD_END"], "methodName": ["validateBadUsersFile"], "fileName": "org.apache.hadoop.mapred.gridmix.TestUserResolve"}, {"methodBody": ["METHOD_START", "{", "assertTrue ( rslv . setTargetUsers ( userRsrc ,    TestUserResolve . conf )  )  ;", "UserGroupInformation   ugi 1     =    UserGroupInformation . createRemoteUser (  \" hfre 0  \"  )  ;", "assertEquals (  \" user 0  \"  ,    rslv . getTargetUgi ( ugi 1  )  . getUserName (  )  )  ;", "assertEquals (  \" user 1  \"  ,    rslv . getTargetUgi ( UserGroupInformation . createRemoteUser (  \" hfre 1  \"  )  )  . getUserName (  )  )  ;", "assertEquals (  \" user 0  \"  ,    rslv . getTargetUgi ( UserGroupInformation . createRemoteUser (  \" hfre 2  \"  )  )  . getUserName (  )  )  ;", "assertEquals (  \" user 0  \"  ,    rslv . getTargetUgi ( ugi 1  )  . getUserName (  )  )  ;", "assertEquals (  \" user 1  \"  ,    rslv . getTargetUgi ( UserGroupInformation . createRemoteUser (  \" hfre 3  \"  )  )  . getUserName (  )  )  ;", "assertEquals (  \" user 0  \"  ,    rslv . getTargetUgi ( UserGroupInformation . createRemoteUser (  \" hfre 0  \"  )  )  . getUserName (  )  )  ;", "assertEquals (  \" user 0  \"  ,    rslv . getTargetUgi ( UserGroupInformation . createRemoteUser (  \" hfre 5  \"  )  )  . getUserName (  )  )  ;", "assertEquals (  \" user 0  \"  ,    rslv . getTargetUgi ( UserGroupInformation . createRemoteUser (  \" hfre 0  \"  )  )  . getUserName (  )  )  ;", "}", "METHOD_END"], "methodName": ["validateValidUsersFile"], "fileName": "org.apache.hadoop.mapred.gridmix.TestUserResolve"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   out    =    null ;", "try    {", "out    =     . fs . create ( usersFilePath ,    true )  ;", "out . writeBytes ( usersFileContent )  ;", "}    finally    {", "if    ( out    !  =    null )     {", "out . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["writeUserList"], "fileName": "org.apache.hadoop.mapred.gridmix.TestUserResolve"}, {"methodBody": ["METHOD_START", "{", "return   monitor . getCumulativeCpuTime (  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentCPUUsage"], "fileName": "org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin"}, {"methodBody": ["METHOD_START", "{", "return    (  ( progress    *    progress )     *    progress )     *    progress ;", "}", "METHOD_END"], "methodName": ["getWeightForProgressInterval"], "fileName": "org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin"}, {"methodBody": ["METHOD_START", "{", "Class [  ]    plugins    =    conf . getClasses ( ResourceUsageMatcher . RESOURCE _ USAGE _ EMULATION _ PLUGINS )  ;", "if    ( plugins    =  =    null )     {", "System . out . println (  \" No   resource   usage   emulator   plugins   configured .  \"  )  ;", "} else    {", "for    ( Class   clazz    :    plugins )     {", "if    ( clazz    !  =    null )     {", "if    ( ResourceUsageEmulatorPlugin . class . isAssignableFrom ( clazz )  )     {", "ResourceUsageEmulatorPlugin   plugin    =     (  ( ResourceUsageEmulatorPlugin )     ( ReflectionUtils . newInstance ( clazz ,    conf )  )  )  ;", "emulationPlugins . add ( plugin )  ;", "} else    {", "throw   new   RuntimeException (  (  (  (  (  (  \" Misconfigured   resource   usage   plugins .     \"     +     \" Class    \"  )     +     ( clazz . getClass (  )  . getName (  )  )  )     +     \"    is   not   a   resource    \"  )     +     \" usage   plugin   as   it   does   not   extend    \"  )     +     ( ResourceUsageEmulatorPlugin . class . getName (  )  )  )  )  ;", "}", "}", "}", "}", "for    ( ResourceUsageEmulatorPlugin   emulator    :    emulationPlugins )     {", "emulator . initialize ( conf ,    metrics ,    monitor ,    progress )  ;", "}", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher"}, {"methodBody": ["METHOD_START", "{", "for    ( ResourceUsageEmulatorPlugin   emulator    :    emulationPlugins )     {", "emulator . emulate (  )  ;", "}", "}", "METHOD_END"], "methodName": ["matchResourceUsage"], "fileName": "org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher"}, {"methodBody": ["METHOD_START", "{", "return    ( Runtime . getRuntime (  )  . maxMemory (  )  )     /     ( TotalHeapUsageEmulatorPlugin . ONE _ MB )  ;", "}", "METHOD_END"], "methodName": ["getMaxHeapUsageInMB"], "fileName": "org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin"}, {"methodBody": ["METHOD_START", "{", "return    ( Runtime . getRuntime (  )  . totalMemory (  )  )     /     ( TotalHeapUsageEmulatorPlugin . ONE _ MB )  ;", "}", "METHOD_END"], "methodName": ["getTotalHeapUsageInMB"], "fileName": "org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin"}, {"methodBody": ["METHOD_START", "{", "int   res    =    ToolRunner . run ( new   GetGroups ( new   Configuration (  )  )  ,    argv )  ;", "System . exit ( res )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.mapred.tools.GetGroups"}, {"methodBody": ["METHOD_START", "{", "cluster    =    new   MiniMRCluster (  0  ,     \" file :  /  /  /  \"  ,     1  )  ;", "conf    =    cluster . createJobConf (  )  ;", "}", "METHOD_END"], "methodName": ["setUpJobTracker"], "fileName": "org.apache.hadoop.mapred.tools.TestGetGroups"}, {"methodBody": ["METHOD_START", "{", "cluster . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDownJobTracker"], "fileName": "org.apache.hadoop.mapred.tools.TestGetGroups"}, {"methodBody": ["METHOD_START", "{", "BinaryRecordInput   bin    =     (  ( BinaryRecordInput )     ( BinaryRecordInput . bIn . get (  )  )  )  ;", "bin . setDataInput ( inp )  ;", "return   bin ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.record.BinaryRecordInput"}, {"methodBody": ["METHOD_START", "{", "this . in    =    inp ;", "}", "METHOD_END"], "methodName": ["setDataInput"], "fileName": "org.apache.hadoop.record.BinaryRecordInput"}, {"methodBody": ["METHOD_START", "{", "BinaryRecordOutput   bout    =     (  ( BinaryRecordOutput )     ( BinaryRecordOutput . bOut . get (  )  )  )  ;", "bout . setDataOutput ( out )  ;", "return   bout ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.record.BinaryRecordOutput"}, {"methodBody": ["METHOD_START", "{", "this . out    =    out ;", "}", "METHOD_END"], "methodName": ["setDataOutput"], "fileName": "org.apache.hadoop.record.BinaryRecordOutput"}, {"methodBody": ["METHOD_START", "{", "append ( bytes ,     0  ,    bytes . length )  ;", "}", "METHOD_END"], "methodName": ["append"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "setCapacity (  (  ( count )     +    length )  )  ;", "System . arraycopy ( bytes ,    offset ,    this . get (  )  ,    count ,    length )  ;", "count    =     ( count )     +    length ;", "}", "METHOD_END"], "methodName": ["append"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . bytes )     =  =    null )     |  |     (  ( this . bytes . length )     <    length )  )     {", "this . bytes    =    new   byte [ length ]  ;", "}", "Systemrycopy ( bytes ,    offset ,    this . bytes ,     0  ,    length )  ;", "this . count    =    length ;", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( bytes )     =  =    null )     {", "bytes    =    new   byte [  0  ]  ;", "}", "turn   bytes ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "return   this . get (  )  . length ;", "}", "METHOD_END"], "methodName": ["getCapacity"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "return   count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "setCapacity (  0  )  ;", "}", "METHOD_END"], "methodName": ["reset"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "this . count    =     ( bytes    =  =    null )     ?     0     :    bytes . length ;", "this . bytes    =    bytes ;", "}", "METHOD_END"], "methodName": ["set"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "if    ( newCapacity    <     0  )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   capacity   argument    \"     +    newCapacity )  )  ;", "}", "if    ( newCapacity    =  =     0  )     {", "this . bytes    =    null ;", "this . count    =     0  ;", "return ;", "}", "if    ( newCapacity    !  =     ( getCapacity (  )  )  )     {", "byte [  ]    data    =    new   byte [ newCapacity ]  ;", "if    ( newCapacity    <     ( count )  )     {", "count    =    newCapacity ;", "}", "if    (  ( count )     !  =     0  )     {", "System . arraycopy ( this . get (  )  ,     0  ,    data ,     0  ,    count )  ;", "}", "bytes    =    data ;", "}", "}", "METHOD_END"], "methodName": ["setCapacity"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "return   new   String ( this . get (  )  ,     0  ,    this . getCount (  )  ,    charsetName )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "setCapacity ( count )  ;", "}", "METHOD_END"], "methodName": ["truncate"], "fileName": "org.apache.hadoop.record.Buffer"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isFirst )  )     {", "stam . print (  \"  ,  \"  )  ;", "}", "isFirst    =    false ;", "}", "METHOD_END"], "methodName": ["printCommaUnlessFirst"], "fileName": "org.apache.hadoop.record.CsvRecordOutput"}, {"methodBody": ["METHOD_START", "{", "if    ( stream . checkError (  )  )     {", "throw   new   IOException (  (  \" Error   serializing    \"     +    tag )  )  ;", "}", "}", "METHOD_END"], "methodName": ["throwExceptionOnError"], "fileName": "org.apache.hadoop.record.CsvRecordOutput"}, {"methodBody": ["METHOD_START", "{", "this . deserialize ( rin ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["deserialize"], "fileName": "org.apache.hadoop.record.Record"}, {"methodBody": ["METHOD_START", "{", "this . serialize ( rout ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.record.Record"}, {"methodBody": ["METHOD_START", "{", "WritableComparator . define ( c ,    comparator )  ;", "}", "METHOD_END"], "methodName": ["define"], "fileName": "org.apache.hadoop.record.RecordComparator"}, {"methodBody": ["METHOD_START", "{", "if    (  ( b    &     ( Utils . B 1  1  )  )     !  =     ( Utils . B 1  0  )  )     {", "throw   new   IOException (  \" Invalid   UTF -  8    representation .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkB10"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableComparator . compareBytes ( b 1  ,    s 1  ,    l 1  ,    b 2  ,    s 2  ,    l 2  )  ;", "}", "METHOD_END"], "methodName": ["compareBytes"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "final   int   utf 8 Len    =    Utils . readVInt ( din )  ;", "final   byte [  ]    bytes    =    new   byte [ utf 8 Len ]  ;", "din . readFully ( bytes )  ;", "int   len    =     0  ;", "StringBuilder   sb    =    new   StringBuilder ( utf 8 Len )  ;", "while    ( len    <    utf 8 Len )     {", "int   cpt    =     0  ;", "final   int   b 1     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "if    ( b 1     <  =     1  2  7  )     {", "cpt    =    b 1  ;", "} else", "if    (  ( b 1     &     ( Utils . B 1  1  1  1  1  )  )     =  =     ( Utils . B 1  1  1  1  0  )  )     {", "int   b 2     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "Utils . checkB 1  0  ( b 2  )  ;", "int   b 3     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "Utils . checkB 1  0  ( b 3  )  ;", "int   b 4     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "Utils . checkB 1  0  ( b 4  )  ;", "cpt    =    Utils . utf 8 ToCodePoint ( b 1  ,    b 2  ,    b 3  ,    b 4  )  ;", "} else", "if    (  ( b 1     &     ( Utils . B 1  1  1  1  )  )     =  =     ( Utils . B 1  1  1  0  )  )     {", "int   b 2     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "Utils . checkB 1  0  ( b 2  )  ;", "int   b 3     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "Utils . checkB 1  0  ( b 3  )  ;", "cpt    =    Utils . utf 8 ToCodePoint ( b 1  ,    b 2  ,    b 3  )  ;", "} else", "if    (  ( b 1     &     ( Utils . B 1  1  1  )  )     =  =     ( Utils . B 1  1  0  )  )     {", "int   b 2     =     ( bytes [  ( len +  +  )  ]  )     &     2  5  5  ;", "Utils . checkB 1  0  ( b 2  )  ;", "cpt    =    Utils . utf 8 ToCodePoint ( b 1  ,    b 2  )  ;", "} else    {", "throw   new   IOException (  (  (  (  (  (  \" Invalid   UTF -  8    byte    \"     +     ( Integer . toHexString ( b 1  )  )  )     +     \"    at   offset    \"  )     +     ( len    -     1  )  )     +     \"    in   length   of    \"  )     +    utf 8 Len )  )  ;", "}", "if    (  !  ( Utils . isValidCodePoint ( cpt )  )  )     {", "throw   new   IOException (  (  (  \" Illegal   Unicode   Codepoint    \"     +     ( Integer . toHexString ( cpt )  )  )     +     \"    in   stream .  \"  )  )  ;", "}", "sb . appendCodePoint ( cpt )  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["fromBinaryString"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( s . charAt (  0  )  )     !  =     '  #  '  )     {", "throw   new   IOException (  \" Error   deserializing   buffer .  \"  )  ;", "}", "if    (  ( s . length (  )  )     =  =     1  )     {", "return   new   Buffer (  )  ;", "}", "int   blen    =     (  ( s . length (  )  )     -     1  )     /     2  ;", "byte [  ]    barr    =    new   byte [ blen ]  ;", "for    ( int   idx    =     0  ;    idx    <    blen ;    idx +  +  )     {", "char   c 1     =    s . charAt (  (  (  2     *    idx )     +     1  )  )  ;", "char   c 2     =    s . charAt (  (  (  2     *    idx )     +     2  )  )  ;", "barr [ idx ]     =     (  ( byte )     ( Integer . parseInt (  (  (  \"  \"     +    c 1  )     +    c 2  )  ,     1  6  )  )  )  ;", "}", "return   new   Buffer ( barr )  ;", "}", "METHOD_END"], "methodName": ["fromCSVBuffer"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( s . charAt (  0  )  )     !  =     '  \\  '  '  )     {", "throw   new   IOException (  \" Error   deserializing   string .  \"  )  ;", "}", "int   len    =    s . length (  )  ;", "StringBuilder   sb    =    new   StringBuilder (  ( len    -     1  )  )  ;", "for    ( int   i    =     1  ;    i    <    len ;    i +  +  )     {", "char   c    =    s . charAt ( i )  ;", "if    ( c    =  =     '  %  '  )     {", "char   ch 1     =    s . charAt (  ( i    +     1  )  )  ;", "char   ch 2     =    s . charAt (  ( i    +     2  )  )  ;", "i    +  =     2  ;", "if    (  ( ch 1     =  =     '  0  '  )     &  &     ( ch 2     =  =     '  0  '  )  )     {", "sbpend (  '  \\ u 0  0  0  0  '  )  ;", "} else", "if    (  ( ch 1     =  =     '  0  '  )     &  &     ( ch 2     =  =     ' A '  )  )     {", "sbpend (  '  \\ n '  )  ;", "} else", "if    (  ( ch 1     =  =     '  0  '  )     &  &     ( ch 2     =  =     ' D '  )  )     {", "sbpend (  '  \\ r '  )  ;", "} else", "if    (  ( ch 1     =  =     '  2  '  )     &  &     ( ch 2     =  =     ' C '  )  )     {", "sbpend (  '  ,  '  )  ;", "} else", "if    (  ( ch 1     =  =     '  7  '  )     &  &     ( ch 2     =  =     ' D '  )  )     {", "sbpend (  '  }  '  )  ;", "} else", "if    (  ( ch 1     =  =     '  2  '  )     &  &     ( ch 2     =  =     '  5  '  )  )     {", "sbpend (  '  %  '  )  ;", "} else    {", "throw   new   IOException (  \" Error   deserializing   string .  \"  )  ;", "}", "} else    {", "sbpend ( c )  ;", "}", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["fromCSVString"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( s . length (  )  )     =  =     0  )     {", "turn   new   Buffer (  )  ;", "}", "int   blen    =     ( s . length (  )  )     /     2  ;", "byte [  ]    barr    =    new   byte [ blen ]  ;", "for    ( int   idx    =     0  ;    idx    <    blen ;    idx +  +  )     {", "char   c 1     =    s . charAt (  (  2     *    idx )  )  ;", "char   c 2     =    s . charAt (  (  (  2     *    idx )     +     1  )  )  ;", "barr [ idx ]     =     (  ( byte )     ( Integer . parseInt (  (  (  \"  \"     +    c 1  )     +    c 2  )  ,     1  6  )  )  )  ;", "}", "turn   new   Buffer ( barr )  ;", "}", "METHOD_END"], "methodName": ["fromXMLBuffer"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    ( int   idx    =     0  ;    idx    <     ( s . length (  )  )  ;  )     {", "char   ch    =    s . charAt (  ( idx +  +  )  )  ;", "if    ( ch    =  =     '  %  '  )     {", "int   ch 1     =     (  . h 2 c ( s . charAt (  ( idx +  +  )  )  )  )     <  <     1  2  ;", "int   ch 2     =     (  . h 2 c ( s . charAt (  ( idx +  +  )  )  )  )     <  <     8  ;", "int   ch 3     =     (  . h 2 c ( s . charAt (  ( idx +  +  )  )  )  )     <  <     4  ;", "int   ch 4     =     . h 2 c ( s . charAt (  ( idx +  +  )  )  )  ;", "char   res    =     (  ( char )     (  (  ( ch 1     |    ch 2  )     |    ch 3  )     |    ch 4  )  )  ;", "sb . append ( res )  ;", "} else    {", "sb . append ( ch )  ;", "}", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["fromXMLString"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableUtils . getVIntSize ( i )  ;", "}", "METHOD_END"], "methodName": ["getVIntSize"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( ch    >  =     '  0  '  )     &  &     ( ch    <  =     '  9  '  )  )     {", "return   ch    -     '  0  '  ;", "} else", "if    (  ( ch    >  =     ' A '  )     &  &     ( ch    <  =     ' F '  )  )     {", "return    ( ch    -     ' A '  )     +     1  0  ;", "} else", "if    (  ( ch    >  =     ' a '  )     &  &     ( ch    <  =     ' f '  )  )     {", "return    ( ch    -     ' a '  )     +     1  0  ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["h2c"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return    !  (  (  ( cpt    >     1  1  1  4  1  1  1  )     |  |     (  ( cpt    >  =     5  5  2  9  6  )     &  &     ( cpt    <  =     5  7  3  4  3  )  )  )     |  |     (  ( cpt    >  =     6  5  5  3  4  )     &  &     ( cpt    <  =     6  5  5  3  5  )  )  )  ;", "}", "METHOD_END"], "methodName": ["isValidCodePoint"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableComparator . readDouble ( bytes ,    start )  ;", "}", "METHOD_END"], "methodName": ["readDouble"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableComparator . readFloat ( bytes ,    start )  ;", "}", "METHOD_END"], "methodName": ["readFloat"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableComparator . readVInt ( bytes ,    start )  ;", "}", "METHOD_END"], "methodName": ["readVInt"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableUtils . readVInt ( in )  ;", "}", "METHOD_END"], "methodName": ["readVInt"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableComparator . readVLong ( bytes ,    start )  ;", "}", "METHOD_END"], "methodName": ["readVLong"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   WritableUtils . readVLong ( in )  ;", "}", "METHOD_END"], "methodName": ["readVLong"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "final   int   strlen    =    str . length (  )  ;", "byte [  ]    bytes    =    new   byte [ strlen    *     4  ]  ;", "int   utf 8 Len    =     0  ;", "int   idx    =     0  ;", "while    ( idx    <    strlen )     {", "final   int   cpt    =    str . codePointAt ( idx )  ;", "idx    +  =     ( Character . isSupplementaryCodePoint ( cpt )  )     ?     2     :     1  ;", "utf 8 Len    +  =     . writeUtf 8  ( cpt ,    bytes ,    utf 8 Len )  ;", "}", ". writeVInt ( out ,    utf 8 Len )  ;", "out . write ( bytes ,     0  ,    utf 8 Len )  ;", "}", "METHOD_END"], "methodName": ["toBinaryString"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  \"  #  \"  )  ;", "sbpend ( buf . toString (  )  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toCSVBuffer"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  (  ( s . length (  )  )     +     1  )  )  ;", "sbpend (  '  \\  '  '  )  ;", "int   len    =    s . length (  )  ;", "for    ( int   i    =     0  ;    i    <    len ;    i +  +  )     {", "char   c    =    s . charAt ( i )  ;", "switch    ( c )     {", "case    '  \\ u 0  0  0  0  '     :", "sbpend (  \"  %  0  0  \"  )  ;", "break ;", "case    '  \\ n '     :", "sbpend (  \"  %  0 A \"  )  ;", "break ;", "case    '  \\ r '     :", "sbpend (  \"  %  0 D \"  )  ;", "break ;", "case    '  ,  '     :", "sbpend (  \"  %  2 C \"  )  ;", "break ;", "case    '  }  '     :", "sbpend (  \"  %  7 D \"  )  ;", "break ;", "case    '  %  '     :", "sbpend (  \"  %  2  5  \"  )  ;", "break ;", "default    :", "sbpend ( c )  ;", "}", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toCSVString"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "return   s . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toXMLBuffer"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    ( int   idx    =     0  ;    idx    <     ( s . length (  )  )  ;    idx +  +  )     {", "char   ch    =    s . charAt ( idx )  ;", "if    ( ch    =  =     '  <  '  )     {", "sb . append (  \"  & lt ;  \"  )  ;", "} else", "if    ( ch    =  =     '  &  '  )     {", "sb . append (  \"  & amp ;  \"  )  ;", "} else", "if    ( ch    =  =     '  %  '  )     {", "sb . append (  \"  %  0  0  2  5  \"  )  ;", "} else", "if    (  (  ( ch    <     3  2  )     |  |     (  ( ch    >     5  5  2  9  5  )     &  &     ( ch    <     5  7  3  4  4  )  )  )     |  |     ( ch    >     6  5  5  3  3  )  )     {", "sb . append (  \"  %  \"  )  ;", "sb . append (  . hexchars [  (  ( ch    &     6  1  4  4  0  )     >  >     1  2  )  ]  )  ;", "sb . append (  . hexchars [  (  ( ch    &     3  8  4  0  )     >  >     8  )  ]  )  ;", "sb . append (  . hexchars [  (  ( ch    &     2  4  0  )     >  >     4  )  ]  )  ;", "sb . append (  . hexchars [  ( ch    &     1  5  )  ]  )  ;", "} else    {", "sb . append ( ch )  ;", "}", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toXMLString"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( cpt    >  =     0  )     &  &     ( cpt    <  =     1  2  7  )  )     {", "turn    1  ;", "}", "if    (  ( cpt    >  =     1  2  8  )     &  &     ( cpt    <  =     2  0  4  7  )  )     {", "turn    2  ;", "}", "if    (  (  ( cpt    >  =     2  0  4  8  )     &  &     ( cpt    <     5  5  2  9  6  )  )     |  |     (  ( cpt    >     5  7  3  4  3  )     &  &     ( cpt    <  =     6  5  5  3  3  )  )  )     {", "turn    3  ;", "}", "if    (  ( cpt    >  =     6  5  5  3  6  )     &  &     ( cpt    <  =     1  1  1  4  1  1  1  )  )     {", "turn    4  ;", "}", "throw   new   IOException (  (  (  \" Illegal   Unicode   Codepoint    \"     +     ( Integer . toHexString ( cpt )  )  )     +     \"    in   string .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["utf8LenForCodePoint"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "int   cpt    =     0  ;", "cpt    =     (  ( b 1     &     (  ~  (  . B 1  1  1  )  )  )     <  <     6  )     |     ( b 2     &     (  ~  (  . B 1  1  )  )  )  ;", "return   cpt ;", "}", "METHOD_END"], "methodName": ["utf8ToCodePoint"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "int   cpt    =     0  ;", "cpt    =     (  (  ( b 1     &     (  ~  (  . B 1  1  1  1  )  )  )     <  <     1  2  )     |     (  ( b 2     &     (  ~  (  . B 1  1  )  )  )     <  <     6  )  )     |     ( b 3     &     (  ~  (  . B 1  1  )  )  )  ;", "return   cpt ;", "}", "METHOD_END"], "methodName": ["utf8ToCodePoint"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "int   cpt    =     0  ;", "cpt    =     (  (  (  ( b 1     &     (  ~  (  . B 1  1  1  1  1  )  )  )     <  <     1  8  )     |     (  ( b 2     &     (  ~  (  . B 1  1  )  )  )     <  <     1  2  )  )     |     (  ( b 3     &     (  ~  (  . B 1  1  )  )  )     <  <     6  )  )     |     ( b 4     &     (  ~  (  . B 1  1  )  )  )  ;", "return   cpt ;", "}", "METHOD_END"], "methodName": ["utf8ToCodePoint"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( cpt    >  =     0  )     &  &     ( cpt    <  =     1  2  7  )  )     {", "bytes [ offset ]     =     (  ( byte )     ( cpt )  )  ;", "return    1  ;", "}", "if    (  ( cpt    >  =     1  2  8  )     &  &     ( cpt    <  =     2  0  4  7  )  )     {", "bytes [  ( offset    +     1  )  ]     =     (  ( byte )     (  (  . B 1  0  )     |     ( cpt    &     6  3  )  )  )  ;", "cpt    =    cpt    >  >     6  ;", "bytes [ offset ]     =     (  ( byte )     (  (  . B 1  1  0  )     |     ( cpt    &     3  1  )  )  )  ;", "return    2  ;", "}", "if    (  (  ( cpt    >  =     2  0  4  8  )     &  &     ( cpt    <     5  5  2  9  6  )  )     |  |     (  ( cpt    >     5  7  3  4  3  )     &  &     ( cpt    <  =     6  5  5  3  3  )  )  )     {", "bytes [  ( offset    +     2  )  ]     =     (  ( byte )     (  (  . B 1  0  )     |     ( cpt    &     6  3  )  )  )  ;", "cpt    =    cpt    >  >     6  ;", "bytes [  ( offset    +     1  )  ]     =     (  ( byte )     (  (  . B 1  0  )     |     ( cpt    &     6  3  )  )  )  ;", "cpt    =    cpt    >  >     6  ;", "bytes [ offset ]     =     (  ( byte )     (  (  . B 1  1  1  0  )     |     ( cpt    &     1  5  )  )  )  ;", "return    3  ;", "}", "if    (  ( cpt    >  =     6  5  5  3  6  )     &  &     ( cpt    <  =     1  1  1  4  1  1  1  )  )     {", "bytes [  ( offset    +     3  )  ]     =     (  ( byte )     (  (  . B 1  0  )     |     ( cpt    &     6  3  )  )  )  ;", "cpt    =    cpt    >  >     6  ;", "bytes [  ( offset    +     2  )  ]     =     (  ( byte )     (  (  . B 1  0  )     |     ( cpt    &     6  3  )  )  )  ;", "cpt    =    cpt    >  >     6  ;", "bytes [  ( offset    +     1  )  ]     =     (  ( byte )     (  (  . B 1  0  )     |     ( cpt    &     6  3  )  )  )  ;", "cpt    =    cpt    >  >     6  ;", "bytes [ offset ]     =     (  ( byte )     (  (  . B 1  1  1  1  0  )     |     ( cpt    &     7  )  )  )  ;", "return    4  ;", "}", "throw   new   IOException (  (  (  \" Illegal   Unicode   Codepoint    \"     +     ( Integer . toHexString ( cpt )  )  )     +     \"    in   string .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["writeUtf8"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "WritableUtils . writeVInt ( stream ,    i )  ;", "}", "METHOD_END"], "methodName": ["writeVInt"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "WritableUtils . writeVLong ( stream ,    i )  ;", "}", "METHOD_END"], "methodName": ["writeVLong"], "fileName": "org.apache.hadoop.record.Utils"}, {"methodBody": ["METHOD_START", "{", "textInputFormat . configure ( job )  ;", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.streaming.AutoInputFormat"}, {"methodBody": ["METHOD_START", "{", "FileSplit   fileSplit    =     (  ( FileSplit )     ( split )  )  ;", "FileSystem   fs    =    FileSystem . get ( fileSplit . getPath (  )  . toUri (  )  ,    job )  ;", "FSDataInputStream   is    =    fs . open ( fileSplit . getPath (  )  )  ;", "byte [  ]    header    =    new   byte [  3  ]  ;", "RecordReader   reader    =    null ;", "try    {", "is . readFully ( header )  ;", "}    catch    ( EOFException   eof )     {", "reader    =    text . getRecordReader ( split ,    job ,    reporter )  ;", "}    finally    {", "is . close (  )  ;", "}", "if    (  (  (  ( header [  0  ]  )     =  =     ' S '  )     &  &     (  ( header [  1  ]  )     =  =     ' E '  )  )     &  &     (  ( header [  2  ]  )     =  =     ' Q '  )  )     {", "reader    =    seqFile . getRecordReader ( split ,    job ,    reporter )  ;", "} else    {", "reader    =    text . getRecordReader ( split ,    job ,    reporter )  ;", "}", "return   reader ;", "}", "METHOD_END"], "methodName": ["getRecordReader"], "fileName": "org.apache.hadoop.streaming.AutoInputFormat"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    new   BufferedReader ( new   InputStreamReader ( System . in )  )  ;", "String   line ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "Thread . sleep (  ( seconds    *     1  0  0  0 L )  )  ;", "System . out . println ( line )  ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.DelayEchoApp"}, {"methodBody": ["METHOD_START", "{", "int   seconds    =     5  ;", "if    (  ( args . length )     >  =     1  )     {", "try    {", "seconds    =    Integer . valueOf ( args [  0  ]  )  ;", "}    catch    ( NumberFormatException   e )     {", "}", "}", "app    =    new    (  )  ;", "app . go ( seconds )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.DelayEchoApp"}, {"methodBody": ["METHOD_START", "{", "JobConf   job    =    new   JobConf ( getConf (  )  )  ;", "DataOutputStream   dout    =    new   DataOutputStream ( System . out )  ;", "AutoInputFormat   autoInputFormat    =    new   AutoInputFormat (  )  ;", "for    ( FileStatus   fileStatus    :    files )     {", "FileSplit   split    =    new   FileSplit ( fileStatus . getPath (  )  ,     0  ,     (  ( fileStatus . getLen (  )  )     *     ( fileStatus . getBlockSize (  )  )  )  ,     (  ( String [  ]  )     ( null )  )  )  ;", "RecordReader   recReader    =    null ;", "try    {", "recReader    =    autoInputFormat . getRecordReader ( split ,    job ,    NULL )  ;", "Object   key    =    recReader . createKey (  )  ;", "Object   value    =    recReader . createValue (  )  ;", "while    ( recReader . next ( key ,    value )  )     {", "if    ( key   instanceof   Writable )     {", "WritableOutput . get ( dout )  . write (  (  ( Writable )     ( key )  )  )  ;", "} else    {", "Output . get ( dout )  . write ( key )  ;", "}", "if    ( value   instanceof   Writable )     {", "WritableOutput . get ( dout )  . write (  (  ( Writable )     ( value )  )  )  ;", "} else    {", "Output . get ( dout )  . write ( value )  ;", "}", "}", "}    finally    {", "if    ( recReader    !  =    null )     {", "recReader . close (  )  ;", "}", "}", "}", "dout . flush (  )  ;", "return    0  ;", "}", "METHOD_END"], "methodName": ["dumpTypedBytes"], "fileName": "org.apache.hadoop.streaming.DumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.streaming.DumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "DumpTypedBytes   dumptb    =    new   DumpTypedBytes (  )  ;", "int   res    =    ToolRunner . run ( dumptb ,    args )  ;", "System . exit ( res )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.DumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  \" Usage :     $ HADOOP _ PREFIX / bin / hadoop   jar   hadoop - streaming . jar \"     +     \"    dumptb    < glob - pattern >  \"  )  )  ;", "System . out . println (  (  \"       Dumps   all   files   that   match   the   given   pattern   to    \"     +     \" standard   output   as   typed   bytes .  \"  )  )  ;", "System . out . println (  \"       The   files   can   be   text   or   sequence   files \"  )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.streaming.DumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     =  =     0  )     {", "System . err . println (  \" Too   few   arguments !  \"  )  ;", "printUsage (  )  ;", "return    1  ;", "}", "Path   pattern    =    new   Path ( args [  0  ]  )  ;", "FileSystem   fs    =    pattern . getFileSystem ( getConf (  )  )  ;", "fs . setVerifyChecksum ( true )  ;", "for    ( Path   p    :    FileUtil . stat 2 Paths ( fs . globStatus ( pattern )  ,    pattern )  )     {", "List < FileStatus >    inputFiles    =    new   ArrayList < FileStatus >  (  )  ;", "FileStatus   status    =    fs . getFileStatus ( p )  ;", "if    ( status . isDirectory (  )  )     {", "FileStatus [  ]    files    =    fs . listStatus ( p )  ;", "Collections . addAll ( inputFiles ,    files )  ;", "} else    {", "inputFiles . add ( status )  ;", "}", "return   d ( inputFiles )  ;", "}", "return    -  1  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.streaming.DumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    conf ;", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.streaming.DumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "String   host    =    getProperty (  \" HOST \"  )  ;", "if    ( host    =  =    null )     {", "try    {", "host    =    InetAddress . getLocalHost (  )  . getHostName (  )  ;", "}    catch    ( IOException   io )     {", "io . printStackTrace (  )  ;", "}", "}", "return   host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.streaming.Environment"}, {"methodBody": ["METHOD_START", "{", "String [  ]    arr    =    new   String [ super . size (  )  ]  ;", "Enumeration < Object >    it    =    super . keys (  )  ;", "int   i    =     -  1  ;", "while    ( it . hasMoreEles (  )  )     {", "String   key    =     (  ( String )     ( it . nextEle (  )  )  )  ;", "String   val    =     (  ( String )     ( get ( key )  )  )  ;", "i +  +  ;", "arr [ i ]     =     ( key    +     \"  =  \"  )     +    val ;", "}", "return   arr ;", "}", "METHOD_END"], "methodName": ["toArray"], "fileName": "org.apache.hadoop.streaming.Environment"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    map    =    new   HashMap < String ,    String >  (  )  ;", "Enumeration < Object >    it    =    super . keys (  )  ;", "while    ( it . hasMoreEles (  )  )     {", "String   key    =     (  ( String )     ( it . nextEle (  )  )  )  ;", "String   val    =     (  ( String )     ( get ( key )  )  )  ;", "map . put ( key ,    val )  ;", "}", "return   map ;", "}", "METHOD_END"], "methodName": ["toMap"], "fileName": "org.apache.hadoop.streaming.Environment"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    new   BufferedReader ( new   InputStreamReader ( System . in )  )  ;", "String   line ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "System . out . println ( line )  ;", "}", "if    ( fail )     {", "throw   new   RuntimeException (  \" Intentionally   failing   task \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.FailApp"}, {"methodBody": ["METHOD_START", "{", "boolean   fail    =    true ;", "if    (  (  ( args . length )     >  =     1  )     &  &     (  \" false \"  . equals ( args [  0  ]  )  )  )     {", "fail    =    false ;", "}", "app    =    new    (  )  ;", "app . go ( fail )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.FailApp"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     <     1  )     {", "System . err . println (  \" No   Arguments   Given !  \"  )  ;", ". printUsage (  )  ;", "System . exit (  1  )  ;", "}", "int   returnStatus    =     0  ;", "String   cmd    =    args [  0  ]  ;", "String [  ]    remainingArgs    =    Arrays . copyOfRange ( args ,     1  ,    args . length )  ;", "if    ( cmd . equalsIgnoreCase (  \" dumptb \"  )  )     {", "DumpTypedBytes   dumptb    =    new   DumpTypedBytes (  )  ;", "returnStatus    =    ToolRunner . run ( dumptb ,    remainingArgs )  ;", "} else", "if    ( cmd . equalsIgnoreCase (  \" loadtb \"  )  )     {", "LoadTypedBytes   loadtb    =    new   LoadTypedBytes (  )  ;", "returnStatus    =    ToolRunner . run ( loadtb ,    remainingArgs )  ;", "} else", "if    ( cmd . equalsIgnoreCase (  \" streamjob \"  )  )     {", "StreamJob   job    =    new   StreamJob (  )  ;", "returnStatus    =    ToolRunner . run ( job ,    remainingArgs )  ;", "} else    {", "StreamJob   job    =    new   StreamJob (  )  ;", "returnStatus    =    ToolRunner . run ( job ,    args )  ;", "}", "if    ( returnStatus    !  =     0  )     {", "System . err . println (  \" Streaming   Command   Failed !  \"  )  ;", "System . exit ( returnStatus )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.HadoopStreaming"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  \" Usage :     $ HADOOP _ PREFIX / bin / hadoop   jar   hadoop - streaming . jar \"     +     \"     [ options ]  \"  )  )  ;", "System . out . println (  \" Options :  \"  )  ;", "System . out . println (  (  \"       dumptb    < glob - pattern >    Dumps   all   files   that   match   the \"     +     \"    given   pattern   to    \"  )  )  ;", "System . out . println (  (  \"                                                                         standard   output   as   typed    \"     +     \" bytes .  \"  )  )  ;", "System . out . println (  (  \"       loadtb    < path >    Reads   typed   bytes   from   standard   input \"     +     \"    and   stores   them   in \"  )  )  ;", "System . out . println (  \"                                                 a   sequence   file   in   the   specified   path \"  )  ;", "System . out . println (  (  \"        [ streamjob ]     < args >    Runs   streaming   job   with   given \"     +     \"    arguments \"  )  )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.streaming.HadoopStreaming"}, {"methodBody": ["METHOD_START", "{", "File [  ]    contents    =    dir . listFiles (  )  ;", "if    ( contents    !  =    null )     {", "for    ( int   i    =     0  ;    i    <     ( contents . length )  ;    i +  +  )     {", "File   f    =    contents [ i ]  ;", "String   fBaseName    =     ( depth    =  =     0  )     ?     \"  \"     :    dir . getName (  )  ;", "if    (  ( jarBaseName . length (  )  )     >     0  )     {", "fBaseName    =     ( jarBaseName    +     \"  /  \"  )     +    fBaseName ;", "}", "if    ( f . isDirectory (  )  )     {", "addDirectory ( dst ,    fBaseName ,    f ,     ( depth    +     1  )  )  ;", "} else    {", "addFileS ( dst ,     ( fBaseName    +     \"  /  \"  )  ,    f )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["addDirectory"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "FileInputStream   in    =    new   FileInputStream ( file )  ;", "try    {", "String   name    =    jarBaseName    +     ( file . getName (  )  )  ;", "addNamedStream ( dst ,    name ,    in )  ;", "}    finally    {", "in . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addFileStream"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "Enumeration   entries    =    src . entries (  )  ;", "JarEntry   entry    =    null ;", "while    ( entries . hasMoreElements (  )  )     {", "entry    =     (  ( JarEntry )     ( entries . nextElement (  )  )  )  ;", "InputS   in    =    src . getInputS ( entry )  ;", "addNamedS ( dst ,    entry . getName (  )  ,    in )  ;", "}", "}", "METHOD_END"], "methodName": ["addJarEntries"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( verbose )     {", "System . err . println (  (  \"  . addNamedStream    \"     +    name )  )  ;", "}", "try    {", "dst . putNextEntry ( new   JarEntry ( name )  )  ;", "int   bytesRead    =     0  ;", "while    (  ( bytesRead    =    in . read ( buffer ,     0  ,     . BUFF _ SIZE )  )     !  =     (  -  1  )  )     {", "dst . write ( buffer ,     0  ,    bytesRead )  ;", "}", "}    catch    ( ZipException   ze )     {", "if    (  ( ze . getMessage (  )  . indexOf (  \" duplicate   entry \"  )  )     >  =     0  )     {", "if    ( verbose )     {", "System . err . println (  (  ( ze    +     \"    Skip   duplicate   entry    \"  )     +    name )  )  ;", "}", "} else    {", "throw   ze ;", "}", "}    finally    {", "in . close (  )  ;", "dst . flush (  )  ;", "dst . closeEntry (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addNamedStream"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "int   leafPos    =    file . lastIndexOf (  '  /  '  )  ;", "if    ( leafPos    =  =     (  ( file . length (  )  )     -     1  )  )", "return    \"  \"  ;", "String   leafName    =    file . subing (  ( leafPos    +     1  )  )  ;", "int   dotPos    =    leafName . lastIndexOf (  '  .  '  )  ;", "if    ( dotPos    =  =     (  -  1  )  )", "return    \"  \"  ;", "String   ext    =    leafName . subing (  ( dotPos    +     1  )  )  ;", "return   ext ;", "}", "METHOD_END"], "methodName": ["fileExtension"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "String   ext    =    fileExtension ( sourceFile )  ;", "if    ( ext . equals (  \" class \"  )  )     {", "return    \" classes /  \"  ;", "} else", "if    (  ( ext . equals (  \" jar \"  )  )     |  |     ( ext . equals (  \" zip \"  )  )  )     {", "return    \" lib /  \"  ;", "} else    {", "return    \"  \"  ;", "}", "}", "METHOD_END"], "methodName": ["getBasePathInJarOut"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     <     2  )     {", "System . err . println (  \" Usage :    JarFiles   merged . jar    [ src . jar    |    dir    |    file    ]  +  \"  )  ;", "} else    {", "jarFiles    =    new    (  )  ;", "List   names    =    new   ArrayList (  )  ;", "List   unjar    =    new   ArrayList (  )  ;", "for    ( int   i    =     1  ;    i    <     ( args . length )  ;    i +  +  )     {", "String   f    =    args [ i ]  ;", "String   ext    =    jarFiles . fileExtension ( f )  ;", "boolean   expandAsJar    =     ( ext . equals (  \" jar \"  )  )     |  |     ( ext . equals (  \" zip \"  )  )  ;", "if    ( expandAsJar )     {", "unjar . add ( f )  ;", "} else    {", "names . add ( f )  ;", "}", "}", "try    {", "jarFiles . merge ( names ,    unjar ,    args [  0  ]  )  ;", "Date   lastMod    =    new   Date ( new   File ( args [  0  ]  )  . lastModified (  )  )  ;", "System . out . println (  (  (  (  \" Merge   done   to    \"     +     ( args [  0  ]  )  )     +     \"     \"  )     +    lastMod )  )  ;", "}    catch    ( Exception   ge )     {", "ge . printStackTrace ( System . err )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "String   source    =    null ;", "JarOutputS   jarOut    =    null ;", "JarFile   jarSource    =    null ;", "jarOut    =    new   JarOutputS ( new   FileOutputS ( dstJar )  )  ;", "boolean   throwing    =    false ;", "try    {", "if    ( srcNames    !  =    null )     {", "Iterator   iter    =    srcNames . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "source    =     (  ( String )     ( iter . next (  )  )  )  ;", "File   fsource    =    new   File ( source )  ;", "String   base    =    getBasePathInJarOut ( source )  ;", "if    (  !  ( fsource . exists (  )  )  )     {", "throwing    =    true ;", "throw   new   FileNotFoundException ( fsource . getAbsolutePath (  )  )  ;", "}", "if    ( fsource . isDirectory (  )  )     {", "addDirectory ( jarOut ,    base ,    fsource ,     0  )  ;", "} else    {", "addFileS ( jarOut ,    base ,    fsource )  ;", "}", "}", "}", "if    ( srcUnjar    !  =    null )     {", "Iterator   iter    =    srcUnjar . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "source    =     (  ( String )     ( iter . next (  )  )  )  ;", "jarSource    =    new   JarFile ( source )  ;", "addJarEntries ( jarOut ,    jarSource )  ;", "jarSource . close (  )  ;", "}", "}", "}    finally    {", "try    {", "jarOut . close (  )  ;", "}    catch    ( ZipException   z )     {", "if    (  ! throwing )     {", "throw   new   IOException ( z . toString (  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["merge"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "this . verbose    =    v ;", "}", "METHOD_END"], "methodName": ["setVerbose"], "fileName": "org.apache.hadoop.streaming.JarBuilder"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.streaming.LoadTypedBytes"}, {"methodBody": ["METHOD_START", "{", "LoadTypedBytes   loadtb    =    new   LoadTypedBytes (  )  ;", "int   res    =    ToolRunner . run ( loadtb ,    args )  ;", "System . exit ( res )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.LoadTypedBytes"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  \" Usage :     $ HADOOP _ PREFIX / bin / hadoop   jar   hadoop - streaming . jar \"     +     \"    loadtb    < path >  \"  )  )  ;", "System . out . println (  (  \"       Reads   typed   bytes   from   standard   input \"     +     \"    and   stores   them   in   a   sequence   file   in \"  )  )  ;", "System . out . println (  \"       the   specified   path \"  )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.streaming.LoadTypedBytes"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     =  =     0  )     {", "System . err . println (  \" Too   few   arguments !  \"  )  ;", "printUsage (  )  ;", "return    1  ;", "}", "Path   path    =    new   Path ( args [  0  ]  )  ;", "FileSystem   fs    =    path . getFileSystem ( getConf (  )  )  ;", "if    ( fs . exists ( path )  )     {", "System . err . println (  \" given   path   exists   already !  \"  )  ;", "return    -  1  ;", "}", "Input   tbinput    =    new   Input ( new   DataInputStream ( System . in )  )  ;", "SequenceFile . Writer   writer    =    SequenceFile . createWriter ( fs ,    conf ,    path ,    Writable . class ,    Writable . class )  ;", "try    {", "Writable   key    =    new   Writable (  )  ;", "Writable   value    =    new   Writable (  )  ;", "byte [  ]    rawKey    =    tbinput . readRaw (  )  ;", "while    ( rawKey    !  =    null )     {", "byte [  ]    rawValue    =    tbinput . readRaw (  )  ;", "key . set ( rawKey ,     0  ,    rawKey . length )  ;", "value . set ( rawValue ,     0  ,    rawValue . length )  ;", "writer . append ( key ,    value )  ;", "rawKey    =    tbinput . readRaw (  )  ;", "}", "}    finally    {", "writer . close (  )  ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.streaming.LoadTypedBytes"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    conf ;", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.streaming.LoadTypedBytes"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     <     1  )     {", "System . err . println (  \" Usage :       NUMRECORDS \"  )  ;", "return ;", "}", "int   numRecords    =    Integer . parseInt ( args [  0  ]  )  ;", "while    (  ( numRecords -  -  )     >     0  )     {", "System . out . println (  \" key \\ tvalue \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.OutputOnlyApp"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  ( pathenv )     =  =    null )     |  |     (  ( pathSep )     =  =    null )  )     |  |     (  ( fileSep )     =  =    null )  )     {", "return   null ;", "}", "int   val    =     -  1  ;", "String   classvalue    =     ( pathenv )     +     ( pathSep )  ;", "while    (  (  ( val    =    classvalue . xOf ( pathSep )  )     >  =     0  )     &  &     (  ( classvalue . length (  )  )     >     0  )  )     {", "String   entry    =    classvalue . substring (  0  ,    val )  . trim (  )  ;", "File   f    =    new   File ( entry )  ;", "if    ( f . isDirectory (  )  )     {", "f    =    new   File (  (  ( entry    +     ( fileSep )  )     +    filename )  )  ;", "}", "if    (  ( f . isFile (  )  )     &  &     ( FileUtil . canRead ( f )  )  )     {", "return   f ;", "}", "classvalue    =    classvalue . substring (  ( val    +     1  )  )  . trim (  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getAbsolutePath"], "fileName": "org.apache.hadoop.streaming.PathFinder"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     <     1  )     {", "System . out . println (  \" Usage :    java       < filename >  \"  )  ;", "System . exit (  1  )  ;", "}", "finder    =    new    (  \" PATH \"  )  ;", "File   file    =    finder . getAbsolutePath ( args [  0  ]  )  ;", "if    ( file    !  =    null )     {", "System . out . println (  (  \" Full   path   name    =     \"     +     ( file . getCanonicalPath (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.PathFinder"}, {"methodBody": ["METHOD_START", "{", "pathenv    =     ( str    +     ( pathSep )  )     +     ( pathenv )  ;", "}", "METHOD_END"], "methodName": ["prependPathComponent"], "fileName": "org.apache.hadoop.streaming.PathFinder"}, {"methodBody": ["METHOD_START", "{", "return    ( getPipeCommand ( job _  )  )     !  =    null ;", "}", "METHOD_END"], "methodName": ["getDoPipe"], "fileName": "org.apache.hadoop.streaming.PipeCombiner"}, {"methodBody": ["METHOD_START", "{", "String   str    =    job . get (  \" stream . combine . streamprocessor \"  )  ;", "try    {", "if    ( str    !  =    null )     {", "return   URLDecoder . decode ( str ,     \" UTF -  8  \"  )  ;", "}", "}    catch    ( UnsupportedEncodingException   e )     {", "System . err . println (  (  \" stream . combine . streamprocessor \"     +     \"    in   jobconf   not   found \"  )  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getPipeCommand"], "fileName": "org.apache.hadoop.streaming.PipeCombiner"}, {"methodBody": ["METHOD_START", "{", "if    ( nameVals    =  =    null )", "return ;", "String [  ]    nv    =    nameVals . split (  \"     \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( nv . length )  ;    i +  +  )     {", "String [  ]    pair    =    nv [ i ]  . split (  \"  =  \"  ,     2  )  ;", "if    (  ( pair . length )     !  =     2  )     {", ". LOG . info (  (  \" Skip   env   entry :  \"     +     ( nv [ i ]  )  )  )  ;", "} else    {", "envPut ( env ,    pair [  0  ]  ,    pair [  1  ]  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addEnvironment"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "Iterator   it    =    conf . iterator (  )  ;", "while    ( itsNext (  )  )     {", "Map . Entry   en    =     (  ( Map . Entry )     ( it . next (  )  )  )  ;", "String   name    =     (  ( String )     ( en . getKey (  )  )  )  ;", "String   value    =    conf . get ( name )  ;", "name    =    safeEnvVarName ( name )  ;", "envPut ( env ,    name ,    value )  ;", "}", "}", "METHOD_END"], "methodName": ["addJobConfToEnvironment"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "try    {", "String   argv    =    getPipeCommand ( job )  ;", "joinDelay _     =    job . getLong (  \" stream . joindelay . milli \"  ,     0  )  ;", "job _     =    job ;", "mapInputWriterClass _     =    job _  . getClass (  \" stream . map . input . writer . class \"  ,    TextInputWriter . class ,    InputWriter . class )  ;", "mapOutputReaderClass _     =    job _  . getClass (  \" stream . map . output . reader . class \"  ,    TextOutputReader . class ,    OutputReader . class )  ;", "reduceInputWriterClass _     =    job _  . getClass (  \" stream . reduce . input . writer . class \"  ,    TextInputWriter . class ,    InputWriter . class )  ;", "reduceOutputReaderClass _     =    job _  . getClass (  \" stream . reduce . output . reader . class \"  ,    TextOutputReader . class ,    OutputReader . class )  ;", "nonZeroExitIsFailure _     =    job _  . getBoolean (  \" stream . non . zero . exit . is . failure \"  ,    true )  ;", "doPipe _     =    getDoPipe (  )  ;", "if    (  !  ( doPipe _  )  )", "return ;", "setStreamJobDetails ( job )  ;", "String [  ]    argvSplit    =     . splitArgs ( argv )  ;", "String   prog    =    argvSplit [  0  ]  ;", "File   currentDir    =    new   File (  \"  .  \"  )  . getAbsoluteFile (  )  ;", "if    ( new   File ( prog )  . isAbsolute (  )  )     {", "} else    {", "FileUtil . chmod ( new   File ( currentDir ,    prog )  . toString (  )  ,     \" a + x \"  )  ;", "}", "if    (  !  ( new   File ( argvSplit [  0  ]  )  . isAbsolute (  )  )  )     {", "PathFinder   finder    =    new   PathFinder (  \" PATH \"  )  ;", "finder . prependPathComponent ( currentDir . toString (  )  )  ;", "File   f    =    finder . getAbsolutePath ( argvSplit [  0  ]  )  ;", "if    ( f    !  =    null )     {", "argvSplit [  0  ]     =    f . getAbsolutePath (  )  ;", "}", "f    =    null ;", "}", ". LOG . info (  (  \"    exec    \"     +     ( Arrays . asList ( argvSplit )  )  )  )  ;", "Environment   childEnv    =     (  ( Environment )     ( StreamUtil . env (  )  . clone (  )  )  )  ;", "addJobConfToEnvironment ( job _  ,    childEnv )  ;", "addEnvironment ( childEnv ,    job _  . get (  \" stream . addenvironment \"  )  )  ;", "envPut ( childEnv ,     \" TMPDIR \"  ,    System . getProperty (  \" tmpdir \"  )  )  ;", "ProcessBuilder   builder    =    new   ProcessBuilder ( argvSplit )  ;", "builder . environment (  )  . putAll ( childEnv . toMap (  )  )  ;", "sim    =    builder . start (  )  ;", "clientOut _     =    new   DataOutputStream ( new   BufferedOutputStream ( sim . getOutputStream (  )  ,     . BUFFER _ SIZE )  )  ;", "clientIn _     =    new   DataInputStream ( new   BufferedInputStream ( sim . getInputStream (  )  ,     . BUFFER _ SIZE )  )  ;", "clientErr _     =    new   DataInputStream ( new   BufferedInputStream ( sim . getErrorStream (  )  )  )  ;", "startTime _     =    System . currentTimeMillis (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" configuration   exception \"  ,    e )  ;", "throw   new   RuntimeException (  \" configuration   exception \"  ,    e )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . error (  \" configuration   exception \"  ,    e )  ;", "throw   new   RuntimeException (  \" configuration   exception \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "InputWriter   inputWriter    =    ReflectionUtils . newInstance ( inputWriterClass ,    job _  )  ;", "inputWriter . initialize ( this )  ;", "return   inputWriter ;", "}", "METHOD_END"], "methodName": ["createInputWriter"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "OutputReader   outputReader    =    ReflectionUtils . newInstance ( outputReaderClass ,    job _  )  ;", "outputReader . initialize ( this )  ;", "return   outputReader ;", "}", "METHOD_END"], "methodName": ["createOutputReader"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "if    ( PipeMapRed . LOG . isDebugEnabled (  )  )     {", "PipeMapRed . LOG . debug (  (  (  (  \" Add      env   entry :  \"     +    name )     +     \"  =  \"  )     +    value )  )  ;", "}", "env . put ( name ,    value )  ;", "}", "METHOD_END"], "methodName": ["envPut"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "return    (  ( var    +     \"  =  \"  )     +     ( StreamUtil . env (  )  . get ( var )  )  )     +     \"  \\ n \"  ;", "}", "METHOD_END"], "methodName": ["envline"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "return   clientIn _  ;", "}", "METHOD_END"], "methodName": ["getClientInput"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "return   clientOut _  ;", "}", "METHOD_END"], "methodName": ["getClientOutput"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "return   job _  ;", "}", "METHOD_END"], "methodName": ["getConfiguration"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "String   s    =     ( numRecInfo (  )  )     +     \"  \\ n \"  ;", "s    +  =     (  \" minRecWrittenToEnableSkip _  =  \"     +     ( minRecWrittenToEnableSkip _  )  )     +     \"     \"  ;", "s    +  =    envline (  \" HOST \"  )  ;", "s    +  =    envline (  \" USER \"  )  ;", "s    +  =    envline (  \" HADOOP _ USER \"  )  ;", "if    (  ( outThread _  )     !  =    null )     {", "s    +  =     (  \" last   tool   output :     |  \"     +     ( outReader _  . getLastOutput (  )  )  )     +     \"  |  \\ n \"  ;", "}", "return   s ;", "}", "METHOD_END"], "methodName": ["getContext"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  !  ( doPipe _  )  )     {", ". LOG . info (  \" mapRedFinished \"  )  ;", "return ;", "}", "if    (  ( clientOut _  )     !  =    null )     {", "try    {", "clientOut _  . flush (  )  ;", "clientOut _  . close (  )  ;", "}    catch    ( IOException   io )     {", ". LOG . warn ( io )  ;", "}", "}", "try    {", "waitOutputThreads (  )  ;", "}    catch    ( IOException   io )     {", ". LOG . warn ( io )  ;", "}", "if    (  ( sim )     !  =    null )", "sim . destroy (  )  ;", ". LOG . info (  \" mapRedFinished \"  )  ;", "}    catch    ( RuntimeException   e )     {", ". LOG . info (  \"    failed !  \"  ,    e )  ;", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["mapRedFinished"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "if    (  ( numRecRead _  )     >  =     ( nextRecReadLog _  )  )     {", "String   info    =    numRecInfo (  )  ;", ". LOG . info ( info )  ;", "if    (  ( nextRecReadLog _  )     <     1  0  0  0  0  0  )     {", "nextRecReadLog _     *  =     1  0  ;", "} else    {", "nextRecReadLog _     +  =     1  0  0  0  0  0  ;", "}", "}", "}", "METHOD_END"], "methodName": ["maybeLogRecord"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =     (  ( System . currentTimeMillis (  )  )     -     ( startTime _  )  )     /     1  0  0  0  ;", "return    (  (  (  (  (  (  (  (  (  (  \" R / W / S =  \"     +     ( numRecRead _  )  )     +     \"  /  \"  )     +     ( numRecWritten _  )  )     +     \"  /  \"  )     +     ( numRecSkipped _  )  )     +     \"    in :  \"  )     +     ( safeDiv ( numRecRead _  ,    elapsed )  )  )     +     \"     [ rec / s ]  \"  )     +     \"    out :  \"  )     +     ( safeDiv ( numRecWritten _  ,    elapsed )  )  )     +     \"     [ rec / s ]  \"  ;", "}", "METHOD_END"], "methodName": ["numRecInfo"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "return   d    =  =     0     ?     \" NA \"     :     (  (  (  (  \"  \"     +     ( n    /    d )  )     +     \"  =  \"  )     +    n )     +     \"  /  \"  )     +    d ;", "}", "METHOD_END"], "methodName": ["safeDiv"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   safe    =    new   StringBuffer (  )  ;", "int   len    =    var . length (  )  ;", "for    ( int   i    =     0  ;    i    <    len ;    i +  +  )     {", "char   c    =    var . charAt ( i )  ;", "char   s ;", "if    (  (  (  ( c    >  =     '  0  '  )     &  &     ( c    <  =     '  9  '  )  )     |  |     (  ( c    >  =     ' A '  )     &  &     ( c    <  =     ' Z '  )  )  )     |  |     (  ( c    >  =     ' a '  )     &  &     ( c    <  =     ' z '  )  )  )     {", "s    =    c ;", "} else    {", "s    =     '  _  '  ;", "}", "safe . append ( s )  ;", "}", "return   safe . toString (  )  ;", "}", "METHOD_END"], "methodName": ["safeEnvVarName"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "String   s    =    job . get (  \" stream . minRecWrittenToEnableSkip _  \"  )  ;", "if    ( s    !  =    null )     {", "minRecWrittenToEnableSkip _     =    Long . parseLong ( s )  ;", ". LOG . info (  (  \" JobConf   set   minRecWrittenToEnableSkip _     =  \"     +     ( minRecWrittenToEnableSkip _  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setStreamJobDetails"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "ArrayList   argList    =    new   ArrayList (  )  ;", "char [  ]    ch    =    args . toCharArray (  )  ;", "int   clen    =    ch . length ;", "int   state    =     . OUTSIDE ;", "int   argstart    =     0  ;", "for    ( int   c    =     0  ;    c    <  =    clen ;    c +  +  )     {", "boolean   last    =    c    =  =    clen ;", "int   lastState    =    state ;", "boolean   endToken    =    false ;", "if    (  ! last )     {", "if    (  ( ch [ c ]  )     =  =     '  \\  '  '  )     {", "if    ( state    =  =     (  . OUTSIDE )  )     {", "state    =     . SINGLEQ ;", "} else", "if    ( state    =  =     (  . SINGLEQ )  )     {", "state    =     . OUTSIDE ;", "}", "endToken    =    state    !  =    lastState ;", "} else", "if    (  ( ch [ c ]  )     =  =     '  \"  '  )     {", "if    ( state    =  =     (  . OUTSIDE )  )     {", "state    =     . DOUBLEQ ;", "} else", "if    ( state    =  =     (  . DOUBLEQ )  )     {", "state    =     . OUTSIDE ;", "}", "endToken    =    state    !  =    lastState ;", "} else", "if    (  ( ch [ c ]  )     =  =     '     '  )     {", "if    ( state    =  =     (  . OUTSIDE )  )     {", "endToken    =    true ;", "}", "}", "}", "if    ( last    |  |    endToken )     {", "if    ( c    =  =    argstart )     {", "} else    {", "String   a ;", "a    =    args . substring ( argstart ,    c )  ;", "argList . add ( a )  ;", "}", "argstart    =    c    +     1  ;", "lastState    =    state ;", "}", "}", "return    (  ( String [  ]  )     ( argList . toArray ( new   String [  0  ]  )  )  )  ;", "}", "METHOD_END"], "methodName": ["splitArgs"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "inWriter _     =    createInputWriter (  )  ;", "outReader _     =    createOutputReader (  )  ;", "outThread _     =    new    . MROutputThread ( outReader _  ,    output ,    reporter )  ;", "outThread _  . start (  )  ;", "errThread _     =    new    . MRErrorThread (  )  ;", "errThread _  . setReporter ( reporter )  ;", "errThread _  . start (  )  ;", "}", "METHOD_END"], "methodName": ["startOutputThreads"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  ( outThread _  )     =  =    null )     {", "OutputCollector   collector    =    new   OutputCollector (  )     {", "public   void   collect ( Object   key ,    Object   value )    throws   IOException    {", "}", "}  ;", "Reporter   reporter    =    Reporter . NULL ;", "startOutputThreads ( collector ,    reporter )  ;", "}", "int   exitVal    =    sim . waitFor (  )  ;", "if    ( exitVal    !  =     0  )     {", "if    ( nonZeroExitIsFailure _  )     {", "throw   new   RuntimeException (  (  \"  . waitOutputThreads (  )  :    subprocess   failed   with   code    \"     +    exitVal )  )  ;", "} else    {", ". LOG . info (  (  (  (  (  \"  . waitOutputThreads (  )  :    subprocess   exited   with    \"     +     \" code    \"  )     +    exitVal )     +     \"    in    \"  )     +     (  . class . getName (  )  )  )  )  ;", "}", "}", "if    (  ( outThread _  )     !  =    null )     {", "outThread _  . join ( joinDelay _  )  ;", "}", "if    (  ( errThread _  )     !  =    null )     {", "errThread _  . join ( joinDelay _  )  ;", "}", "if    (  ( outerrThreadsThrowable )     !  =    null )     {", "throw   new   RuntimeException ( outerrThreadsThrowable )  ;", "}", "}    catch    ( InterruptedException   e )     {", "}", "}", "METHOD_END"], "methodName": ["waitOutputThreads"], "fileName": "org.apache.hadoop.streaming.PipeMapRed"}, {"methodBody": ["METHOD_START", "{", "PipeMapper   pipeMapper    =     (  ( PipeMapper )     ( PipeMapRunner . getMapper (  )  )  )  ;", "pipeMapper . startOutputThreads ( output ,    reporter )  ;", "super . run ( input ,    output ,    reporter )  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.streaming.PipeMapRunner"}, {"methodBody": ["METHOD_START", "{", "mapRedFinished (  )  ;", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.streaming.PipeMapper"}, {"methodBody": ["METHOD_START", "{", "super . configure ( job )  ;", "SkipBadRecords . setAutoIncrProcCount ( job ,    false )  ;", "skipping    =    job . getBoolean ( SKIP _ RECORDS ,    false )  ;", "if    ( mapInputWriterClass _  . getCanonicalName (  )  . equals ( TextInputWriter . class . getCanonicalName (  )  )  )     {", "String   inputFormatClassName    =    job . getClass (  \" mapred . input . format . class \"  ,    TextInputFormat . class )  . getCanonicalName (  )  ;", "ignoreKey    =    job . getBoolean (  \" stream . map . input . ignoreKey \"  ,    inputFormatClassName . equals ( TextInputFormat . class . getCanonicalName (  )  )  )  ;", "}", "try    {", "mapOutputFieldSeparator    =    job . get (  \" stream . map . output . field . separator \"  ,     \"  \\ t \"  )  . getBytes (  \" UTF -  8  \"  )  ;", "mapInputFieldSeparator    =    job . get (  \" stream . map . input . field . separator \"  ,     \"  \\ t \"  )  . getBytes (  \" UTF -  8  \"  )  ;", "numOfMapOutputKeyFields    =    job . getInt (  \" stream . num . map . output . key . fields \"  ,     1  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "throw   new   RuntimeException (  \" The   current   system   does   not   support   UTF -  8    encoding !  \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.streaming.PipeMapper"}, {"methodBody": ["METHOD_START", "{", "return   true ;", "}", "METHOD_END"], "methodName": ["getDoPipe"], "fileName": "org.apache.hadoop.streaming.PipeMapper"}, {"methodBody": ["METHOD_START", "{", "String   str    =    job . get (  \" stream . map . streamprocessor \"  )  ;", "if    ( str    =  =    null )     {", "return   str ;", "}", "try    {", "return   URLDecoder . decode ( str ,     \" UTF -  8  \"  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "System . err . println (  \" stream . map . streamprocessor   in   jobconf   not   found \"  )  ;", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["getPipeCommand"], "fileName": "org.apache.hadoop.streaming.PipeMapper"}, {"methodBody": ["METHOD_START", "{", "if    (  ( outerrThreadsThrowable )     !  =    null )     {", "mapRedFinished (  )  ;", "throw   new   IOException (  \" MROutput / MRErrThread   failed :  \"  ,    outerrThreadsThrowable )  ;", "}", "try    {", "( numRecRead _  )  +  +  ;", "maybeLogRecord (  )  ;", "if    (  ( numExceptions _  )     =  =     0  )     {", "if    (  !  ( this . ignoreKey )  )     {", "inWriter _  . writeKey ( key )  ;", "}", "inWriter _  . writeValue ( value )  ;", "if    ( skipping )     {", "clientOut _  . flush (  )  ;", "}", "} else    {", "( numRecSkipped _  )  +  +  ;", "}", "}    catch    ( IOException   io )     {", "( numExceptions _  )  +  +  ;", "if    (  (  ( numExceptions _  )     >     1  )     |  |     (  ( numRecWritten _  )     <     ( minRecWrittenToEnableSkip _  )  )  )     {", "Red . LOG . info ( getContext (  )  ,    io )  ;", "mapRedFinished (  )  ;", "throw   io ;", "} else    {", "}", "}", "}", "METHOD_END"], "methodName": ["map"], "fileName": "org.apache.hadoop.streaming.PipeMapper"}, {"methodBody": ["METHOD_START", "{", "mapRedFinished (  )  ;", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.streaming.PipeReducer"}, {"methodBody": ["METHOD_START", "{", "super . configure ( job )  ;", "SkipBadRecords . setAutoIncrProcCount ( job ,    false )  ;", "skipping    =    job . getBoolean ( SKIP _ RECORDS ,    false )  ;", "try    {", "reduceOutFieldSeparator    =    job _  . get (  \" stream . reduce . output . field . separator \"  ,     \"  \\ t \"  )  . getBytes (  \" UTF -  8  \"  )  ;", "reduceInputFieldSeparator    =    job _  . get (  \" stream . reduce . input . field . separator \"  ,     \"  \\ t \"  )  . getBytes (  \" UTF -  8  \"  )  ;", "this . numOfReduceOutputKeyFields    =    job _  . getInt (  \" stream . num . reduce . output . key . fields \"  ,     1  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "throw   new   RuntimeException (  \" The   current   system   does   not   support   UTF -  8    encoding !  \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["configure"], "fileName": "org.apache.hadoop.streaming.PipeReducer"}, {"methodBody": ["METHOD_START", "{", "String   argv    =    getPipeCommand ( job _  )  ;", "return    ( argv    !  =    null )     &  &     (  !  ( SJob . REDUCE _ NONE . equals ( argv )  )  )  ;", "}", "METHOD_END"], "methodName": ["getDoPipe"], "fileName": "org.apache.hadoop.streaming.PipeReducer"}, {"methodBody": ["METHOD_START", "{", "String   str    =    job . get (  \" stream . reduce . streamprocessor \"  )  ;", "if    ( str    =  =    null )     {", "return   str ;", "}", "try    {", "return   URLDecoder . decode ( str ,     \" UTF -  8  \"  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "System . err . println (  \" stream . reduce . streamprocessor   in   jobconf   not   found \"  )  ;", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["getPipeCommand"], "fileName": "org.apache.hadoop.streaming.PipeReducer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( doPipe _  )     &  &     (  ( outThread _  )     =  =    null )  )     {", "startOutputThreads ( output ,    reporter )  ;", "}", "try    {", "while    ( values . hasNext (  )  )     {", "Writable   val    =     (  ( Writable )     ( values . next (  )  )  )  ;", "( numRecRead _  )  +  +  ;", "maybeLogRecord (  )  ;", "if    ( doPipe _  )     {", "if    (  ( outerrThreadsThrowable )     !  =    null )     {", "mapRedFinished (  )  ;", "throw   new   IOException (  \" MROutput / MRErrThread   failed :  \"  ,    outerrThreadsThrowable )  ;", "}", "inWriter _  . writeKey ( key )  ;", "inWriter _  . writeValue ( val )  ;", "} else    {", "output . collect ( key ,    val )  ;", "}", "}", "if    (  ( doPipe _  )     &  &     ( skipping )  )     {", "clientOut _  . flush (  )  ;", "}", "}    catch    ( IOException   io )     {", "String   extraInfo    =     \"  \"  ;", "try    {", "int   exitVal    =    sim . exitValue (  )  ;", "if    ( exitVal    =  =     0  )     {", "extraInfo    =     \" subprocess   exited   successfully \\ n \"  ;", "} else    {", "extraInfo    =     (  \" subprocess   exited   with   error   code    \"     +    exitVal )     +     \"  \\ n \"  ;", "}", "}    catch    ( IllegalThreadStateException   e )     {", "extraInfo    =     \" subprocess   still   running \\ n \"  ;", "}", "mapRedFinished (  )  ;", "throw   new   IOException (  (  ( extraInfo    +     ( getContext (  )  )  )     +     ( io . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["reduce"], "fileName": "org.apache.hadoop.streaming.PipeReducer"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    new   BufferedReader ( new   InputStreamReader ( System . in )  )  ;", "String   line ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "for    ( String   part    :    line . split ( find )  )     {", "writeString ( part )  ;", "writeInt (  1  )  ;", "}", "}", "System . out . flush (  )  ;", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.RawBytesMapApp"}, {"methodBody": ["METHOD_START", "{", "RawBytesMapApp   app    =    new   RawBytesMapApp ( args [  0  ]  . replace (  \"  .  \"  ,     \"  \\  \\  .  \"  )  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.RawBytesMapApp"}, {"methodBody": ["METHOD_START", "{", "dos . writeInt (  4  )  ;", "IntWritable   iw    =    new   IntWritable ( i )  ;", "iw . write ( dos )  ;", "}", "METHOD_END"], "methodName": ["writeInt"], "fileName": "org.apache.hadoop.streaming.RawBytesMapApp"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    str . getBytes (  \" UTF -  8  \"  )  ;", "dos . writeInt ( bytes . length )  ;", "dos . write ( bytes )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.streaming.RawBytesMapApp"}, {"methodBody": ["METHOD_START", "{", "String   prevKey    =    null ;", "int   sum    =     0  ;", "String   key    =    readString (  )  ;", "while    ( key    !  =    null )     {", "if    (  ( prevKey    !  =    null )     &  &     (  !  ( key . equals ( prevKey )  )  )  )     {", "System . out . println (  (  ( prevKey    +     \"  \\ t \"  )     +    sum )  )  ;", "sum    =     0  ;", "}", "sum    +  =    readInt (  )  ;", "prevKey    =    key ;", "key    =    readString (  )  ;", "}", "System . out . println (  (  ( prevKey    +     \"  \\ t \"  )     +    sum )  )  ;", "System . out . flush (  )  ;", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.RawBytesReduceApp"}, {"methodBody": ["METHOD_START", "{", "RawBytesReduceApp   app    =    new   RawBytesReduceApp (  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.RawBytesReduceApp"}, {"methodBody": ["METHOD_START", "{", "dis . readInt (  )  ;", "IntWritable   iw    =    new   IntWritable (  )  ;", "iw . readFields ( dis )  ;", "return   iw . get (  )  ;", "}", "METHOD_END"], "methodName": ["readInt"], "fileName": "org.apache.hadoop.streaming.RawBytesReduceApp"}, {"methodBody": ["METHOD_START", "{", "int   length ;", "try    {", "length    =    dis . readInt (  )  ;", "}    catch    ( EOFException   eof )     {", "return   null ;", "}", "byte [  ]    b    =    new   byte [ length ]  ;", "dis . readFully ( b )  ;", "return   new   String ( b ,     \" UTF -  8  \"  )  ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.streaming.RawBytesReduceApp"}, {"methodBody": ["METHOD_START", "{", "StderrApp . go ( preWriteLines ,    sleep ,    postWriteLines ,    false )  ;", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.StderrApp"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    new   BufferedReader ( new   InputStreamReader ( System . in )  )  ;", "String   line ;", "if    ( status )     {", "System . err . println (  \" reporter : status : starting   echo \"  )  ;", "}", "while    ( preWriteLines    >     0  )     {", "-  - preWriteLines ;", "System . err . println (  (  (  (  \" some   stderr   output   before   reading   input ,     \"     +    preWriteLines )     +     \"    lines   remaining ,    sleeping    \"  )     +    sleep )  )  ;", "try    {", "Thread . sleep ( sleep )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "System . out . println ( line )  ;", "}", "while    ( postWriteLines    >     0  )     {", "-  - postWriteLines ;", "System . err . println (  (  \" some   stderr   output   after   reading   input ,    lines   remaining    \"     +    postWriteLines )  )  ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.StderrApp"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     <     3  )     {", "System . err . println (  \" Usage :       PREWRITE   SLEEP   POSTWRITE    [ STATUS ]  \"  )  ;", "return ;", "}", "int   preWriteLines    =    Integer . parseInt ( args [  0  ]  )  ;", "int   sleep    =    Integer . parseInt ( args [  1  ]  )  ;", "int   postWriteLines    =    Integer . parseInt ( args [  2  ]  )  ;", "boolean   status    =     (  ( args . length )     >     3  )     ?    Boolean . parseBoolean ( args [  3  ]  )     :    false ;", ". go ( preWriteLines ,    sleep ,    postWriteLines ,    status )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.StderrApp"}, {"methodBody": ["METHOD_START", "{", "testParentJobConfToEnvVars (  )  ;", "BufferedReader   in    =    new   BufferedReader ( new   InputReader ( System . in )  )  ;", "String   line ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "String [  ]    words    =    line . split (  \"     \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( words . length )  ;    i +  +  )     {", "String   out    =     (  (  \" LongValueSum :  \"     +     ( words [ i ]  . trim (  )  )  )     +     \"  \\ t \"  )     +     \"  1  \"  ;", "System . out . println ( out )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.StreamAggregate"}, {"methodBody": ["METHOD_START", "{", "TrApp   app    =    new   StreamAggregate (  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.StreamAggregate"}, {"methodBody": ["METHOD_START", "{", "in _  . close (  )  ;", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   new   Text (  )  ;", "}", "METHOD_END"], "methodName": ["createKey"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   new   Text (  )  ;", "}", "METHOD_END"], "methodName": ["createValue"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   in _  . getPos (  )  ;", "}", "METHOD_END"], "methodName": ["getPos"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    (  ( end _  )     =  =     ( start _  )  )     {", "return    1  .  0 F ;", "} else    {", "return    (  ( float )     (  ( in _  . getPos (  )  )     -     ( start _  )  )  )     /     (  ( float )     (  ( end _  )     -     ( start _  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getProgress"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "long   pos    =     -  1  ;", "try    {", "pos    =    getPos (  )  ;", "}    catch    ( IOException   io )     {", "}", "String   recStr ;", "if    (  ( record . length (  )  )     >     ( statusMaxChars _  )  )     {", "recStr    =     ( record . subSequence (  0  ,    statusMaxChars _  )  )     +     \"  .  .  .  \"  ;", "} else    {", "recStr    =    record . toString (  )  ;", "}", "String   unqualSplit    =     (  (  (  ( split _  . getPath (  )  . getName (  )  )     +     \"  :  \"  )     +     ( split _  . getStart (  )  )  )     +     \"  +  \"  )     +     ( split _  . getLength (  )  )  ;", "String   status    =     (  (  (  (  (  (  (  (  \" HSTR    \"     +     ( StreamUtil . getHost (  )  )  )     +     \"     \"  )     +     ( numRec _  )  )     +     \"  .    pos =  \"  )     +    pos )     +     \"     \"  )     +    unqualSplit )     +     \"    Processing   record =  \"  )     +    recStr ;", "status    +  =     \"     \"     +     ( splitName _  )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["getStatus"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "( numRec _  )  +  +  ;", "if    (  ( numRec _  )     =  =     ( nextStatusRec _  )  )     {", "String   recordStr    =    new   String ( record ,    start ,    Math . min ( len ,    statusMaxRecordChars _  )  ,     \" UTF -  8  \"  )  ;", "nextStatusRec _     +  =     1  0  0  ;", "String   status    =    getStatus ( recordStr )  ;", ". LOG . info ( status )  ;", "reporter _  . setStatus ( status )  ;", "}", "}", "METHOD_END"], "methodName": ["numRecStats"], "fileName": "org.apache.hadoop.streaming.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "String   c    =    job . get (  \" stream . recordreader . class \"  )  ;", "if    (  ( c    =  =    null )     |  |     (  ( c . indexOf (  \" LineRecordReader \"  )  )     >  =     0  )  )     {", "return   super . getRecordReader ( genericSplit ,    job ,    reporter )  ;", "}", "FileSplit   split    =     (  ( FileSplit )     ( genericSplit )  )  ;", "LOG . info (  (  \" getRecordReader   start .  .  .  .  . split =  \"     +    split )  )  ;", "reporter . setStatus ( split . toString (  )  )  ;", "FileSystem   fs    =    split . getPath (  )  . getFileSystem ( job )  ;", "FSDataInputStream   in    =    fs . open ( split . getPath (  )  )  ;", "Class   readerClass ;", "{", "readerClass    =    StreamUtil . goodClassOrNull ( job ,    c ,    null )  ;", "if    ( readerClass    =  =    null )     {", "throw   new   RuntimeException (  (  \" Class   not   found :     \"     +    c )  )  ;", "}", "}", "Constructor   ctor ;", "try    {", "ctor    =    readerClass . getConstructor ( new   Class [  ]  {    FSDataInputStream . class ,    FileSplit . class ,    Reporter . class ,    JobConf . class ,    class    }  )  ;", "}    catch    ( NoSuchMethodException   nsm )     {", "throw   new   RuntimeException ( nsm )  ;", "}", "RecordReader < Text ,    Text >    reader ;", "try    {", "reader    =     (  ( RecordReader < Text ,    Text >  )     ( ctor . newInstance ( new   Object [  ]  {    in ,    split ,    reporter ,    job ,    fs    }  )  )  )  ;", "}    catch    ( Exception   nsm )     {", "throw   new   RuntimeException ( nsm )  ;", "}", "return   reader ;", "}", "METHOD_END"], "methodName": ["getRecordReader"], "fileName": "org.apache.hadoop.streaming.StreamInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   OptionBuilder . withDescription ( desc )  . create ( name )  ;", "}", "METHOD_END"], "methodName": ["createBoolOption"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "StreamJob   job    =    new   StreamJob (  )  ;", "job . argv _     =    argv ;", "job . init (  )  ;", "job . preProcessArgs (  )  ;", "job . parseArgv (  )  ;", "job . postProcessArgs (  )  ;", "job . setJobConf (  )  ;", "return   job . jobConf _  ;", "}", "METHOD_END"], "methodName": ["createJob"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "return   OptionBuilder . withArgName ( argName )  . hasArgs ( max )  . withDescription ( desc )  . isRequired ( required )  . create ( name )  ;", "}", "METHOD_END"], "methodName": ["createOption"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "printUsage ( detailed )  ;", "fail (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["exitUsage"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "System . err . println ( message )  ;", "System . err . println (  \" Try    - help   for   more   information \"  )  ;", "throw   new   IllegalArgumentException ( message )  ;", "}", "METHOD_END"], "methodName": ["fail"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "return    \" default \"  ;", "}", "METHOD_END"], "methodName": ["getClusterNick"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "String   h    =    env _  . getProperty (  \" HADOOP _ PREFIX \"  )  ;", "if    ( h    =  =    null )     {", "h    =     \" UNDEF \"  ;", "}", "return   h ;", "}", "METHOD_END"], "methodName": ["getHadoopClientHome"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "return   jobConf _  . get ( JT _ IPC _ ADDRESS )  ;", "}", "METHOD_END"], "methodName": ["getJobTrackerHostPort"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "String [  ]    archives    =    StringUtils . getStrings ( lcacheArchives )  ;", "String [  ]    files    =    StringUtils . getStrings ( lcacheFiles )  ;", "fileURIs    =    StringUtils . stringToURI ( files )  ;", "archiveURIs    =    StringUtils . stringToURI ( archives )  ;", "}", "METHOD_END"], "methodName": ["getURIs"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   run ( argv _  )  ;", "}    catch    ( Exception   ex )     {", "throw   new   IOException ( ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "try    {", "env _     =    new   Environment (  )  ;", "}    catch    ( IOException   io )     {", "throw   new   RuntimeException ( io )  ;", "}", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "return   StreamUtil . isLocalJobTracker ( jobConf _  )  ;", "}", "METHOD_END"], "methodName": ["isLocalHadoop"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "msg (  \"  =  =  =  =    JobConf   properties :  \"  )  ;", "TreeMap < String ,    String >    sorted    =    new   TreeMap < String ,    String >  (  )  ;", "for    ( final   Map . Entry < String ,    String >    en    :    jobConf _  )     {", "sorted . put ( en . getKey (  )  ,    en . getValue (  )  )  ;", "}", "for    ( final   Map . Entry < String ,    String >    en    :    sorted . entrySet (  )  )     {", "msg (  (  (  ( en . getKey (  )  )     +     \"  =  \"  )     +     ( en . getValue (  )  )  )  )  ;", "}", "msg (  \"  =  =  =  =  \"  )  ;", "}", "METHOD_END"], "methodName": ["listJobConfProperties"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "if    ( verbose _  )     {", "Syem . out . println (  (  \" STREAM :     \"     +    msg )  )  ;", "}", "}", "METHOD_END"], "methodName": ["msg"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "ArrayList < String >    unjarFiles    =    new   ArrayList < String >  (  )  ;", "String   runtimeClasses    =    config _  . get (  \" stream . shipped . hadoop \"  )  ;", "if    ( runtimeClasses    =  =    null )     {", "runtimeClasses    =    StreamUtil . findInClasspath ( StreamJob . class . getName (  )  )  ;", "}", "if    ( runtimeClasses    =  =    null )     {", "throw   new   IOException (  (  \" runtime   classes   not   found :     \"     +     ( getClass (  )  . getPackage (  )  )  )  )  ;", "} else    {", "msg (  (  \" Found   runtime   classes   in :     \"     +    runtimeClasses )  )  ;", "}", "if    ( isLocalHadoop (  )  )     {", "} else    {", "if    ( new   File ( runtimeClasses )  . isDirectory (  )  )     {", "packageFiles _  . add ( runtimeClasses )  ;", "} else    {", "unjarFiles . add ( runtimeClasses )  ;", "}", "}", "if    (  (  ( packageFiles _  . size (  )  )     +     ( unjarFiles . size (  )  )  )     =  =     0  )     {", "return   null ;", "}", "String   tmp    =    jobConf _  . get (  \" stream . tmpdir \"  )  ;", "File   tmpDir    =     ( tmp    =  =    null )     ?    null    :    new   File ( tmp )  ;", "File   jobJar    =    File . createTempFile (  \" streamjob \"  ,     \"  . jar \"  ,    tmpDir )  ;", "System . out . println (  (  (  (  (  (  (  (  \" packageJobJar :     \"     +     ( packageFiles _  )  )     +     \"     \"  )     +    unjarFiles )     +     \"     \"  )     +    jobJar )     +     \"    tmpDir =  \"  )     +    tmpDir )  )  ;", "if    (  ( debug _  )     =  =     0  )     {", "jobJar . deleteOnExit (  )  ;", "}", "JarBuilder   builder    =    new   JarBuilder (  )  ;", "if    ( verbose _  )     {", "builder . setVerbose ( true )  ;", "}", "String   jobJarName    =    jobJar . getAbsolutePath (  )  ;", "builder . merge ( packageFiles _  ,    unjarFiles ,    jobJarName )  ;", "return   jobJarName ;", "}", "METHOD_END"], "methodName": ["packageJobJar"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "CommandLine   cmdLine    =    null ;", "try    {", "cmdLine    =    parser . parse ( allOptions ,    argv _  )  ;", "}    catch    ( Exception   oe )     {", ". LOG . error ( oe . getMessage (  )  )  ;", "exitUsage (  (  (  ( argv _  . length )     >     0  )     &  &     (  \"  - info \"  . equals ( argv _  [  0  ]  )  )  )  )  ;", "}", "if    ( cmdLine    !  =    null )     {", "@ SuppressWarnings (  \" unchecked \"  )", "List < String >    args    =    cmdLine . getArgList (  )  ;", "if    (  ( args    !  =    null )     &  &     (  ( args . size (  )  )     >     0  )  )     {", "fail (  (  (  (  (  \" Found    \"     +     ( args . size (  )  )  )     +     \"    unexpected   arguments   on   the    \"  )     +     \" command   line    \"  )     +    args )  )  ;", "}", "detailedUsage _     =    cmdLine . hasOption (  \" info \"  )  ;", "if    (  ( cmdLine . hasOption (  \" help \"  )  )     |  |     ( detailedUsage _  )  )     {", "printUsage    =    true ;", "return ;", "}", "verbose _     =    cmdLine . hasOption (  \" verbose \"  )  ;", "background _     =    cmdLine . hasOption (  \" background \"  )  ;", "debug _     =     ( cmdLine . hasOption (  \" debug \"  )  )     ?     ( debug _  )     +     1     :    debug _  ;", "String [  ]    values    =    cmdLine . getOptionValues (  \" input \"  )  ;", "if    (  ( values    !  =    null )     &  &     (  ( values . length )     >     0  )  )     {", "for    ( String   input    :    values )     {", "inputSpecs _  . add ( input )  ;", "}", "}", "output _     =    cmdLine . getOptionValue (  \" output \"  )  ;", "mapCmd _     =    cmdLine . getOptionValue (  \" mapper \"  )  ;", "comCmd _     =    cmdLine . getOptionValue (  \" combiner \"  )  ;", "redCmd _     =    cmdLine . getOptionValue (  \" reducer \"  )  ;", "lazyOutput _     =    cmdLine . hasOption (  \" lazyOutput \"  )  ;", "values    =    cmdLine . getOptionValues (  \" file \"  )  ;", "if    (  ( values    !  =    null )     &  &     (  ( values . length )     >     0  )  )     {", ". LOG . warn (  (  \"  - file   option   is   deprecated ,    please   use   generic   option \"     +     \"     - files   instead .  \"  )  )  ;", "StringBuffer   fileList    =    new   StringBuffer (  )  ;", "for    ( String   file    :    values )     {", "packageFiles _  . add ( file )  ;", "try    {", "Path   path    =    new   Path ( file )  ;", "FileSystem   localFs    =    FileSystem . getLocal ( config _  )  ;", "String   finalPath    =    path . makeQualified ( localFs )  . toString (  )  ;", "if    (  ( fileList . length (  )  )     >     0  )     {", "fileList . append (  '  ,  '  )  ;", "}", "fileList . append ( finalPath )  ;", "}    catch    ( Exception   e )     {", "throw   new   IllegalArgumentException ( e )  ;", "}", "}", "String   tmpFiles    =    config _  . get (  \" tmpfiles \"  ,     \"  \"  )  ;", "if    ( tmpFiles . isEmpty (  )  )     {", "tmpFiles    =    fileList . toString (  )  ;", "} else    {", "tmpFiles    =     ( tmpFiles    +     \"  ,  \"  )     +    fileList ;", "}", "config _  . set (  \" tmpfiles \"  ,    tmpFiles )  ;", "validate ( packageFiles _  )  ;", "}", "String   fsName    =    cmdLine . getOptionValue (  \" dfs \"  )  ;", "if    ( null    !  =    fsName )     {", ". LOG . warn (  \"  - dfs   option   is   deprecated ,    please   use    - fs   instead .  \"  )  ;", "config _  . set (  \" fs . default . name \"  ,    fsName )  ;", "}", "additionalConfSpec _     =    cmdLine . getOptionValue (  \" additionalconfspec \"  )  ;", "inputFormatSpec _     =    cmdLine . getOptionValue (  \" inputformat \"  )  ;", "outputFormatSpec _     =    cmdLine . getOptionValue (  \" outputformat \"  )  ;", "numReduceTasksSpec _     =    cmdLine . getOptionValue (  \" numReduceTasks \"  )  ;", "partitionerSpec _     =    cmdLine . getOptionValue (  \" partitioner \"  )  ;", "inReaderSpec _     =    cmdLine . getOptionValue (  \" inputreader \"  )  ;", "mapDebugSpec _     =    cmdLine . getOptionValue (  \" mapdebug \"  )  ;", "reduceDebugSpec _     =    cmdLine . getOptionValue (  \" reducedebug \"  )  ;", "ioSpec _     =    cmdLine . getOptionValue (  \" io \"  )  ;", "String [  ]    car    =    cmdLine . getOptionValues (  \" cacheArchive \"  )  ;", "if    (  ( null    !  =    car )     &  &     (  ( car . length )     >     0  )  )     {", ". LOG . warn (  \"  - cacheArchive   option   is   deprecated ,    please   use    - archives   instead .  \"  )  ;", "for    ( String   s    :    car )     {", "cacheArchives    =     (  ( cacheArchives )     =  =    null )     ?    s    :     (  ( cacheArchives )     +     \"  ,  \"  )     +    s ;", "}", "}", "String [  ]    caf    =    cmdLine . getOptionValues (  \" cacheFile \"  )  ;", "if    (  ( null    !  =    caf )     &  &     (  ( caf . length )     >     0  )  )     {", ". LOG . warn (  \"  - cacheFile   option   is   deprecated ,    please   use    - files   instead .  \"  )  ;", "for    ( String   s    :    caf )     {", "cacheFiles    =     (  ( cacheFiles )     =  =    null )     ?    s    :     (  ( cacheFiles )     +     \"  ,  \"  )     +    s ;", "}", "}", "String [  ]    jobconf    =    cmdLine . getOptionValues (  \" jobconf \"  )  ;", "if    (  ( null    !  =    jobconf )     &  &     (  ( jobconf . length )     >     0  )  )     {", ". LOG . warn (  \"  - jobconf   option   is   deprecated ,    please   use    - D   instead .  \"  )  ;", "for    ( String   s    :    jobconf )     {", "String [  ]    parts    =    s . split (  \"  =  \"  ,     2  )  ;", "config _  . set ( parts [  0  ]  ,    parts [  1  ]  )  ;", "}", "}", "String [  ]    cmd    =    cmdLine . getOptionValues (  \" cmdenv \"  )  ;", "if    (  ( null    !  =    cmd )     &  &     (  ( cmd . length )     >     0  )  )     {", "for    ( String   s    :    cmd )     {", "if    (  ( addTaskEnvironment _  . length (  )  )     >     0  )     {", "addTaskEnvironment _     +  =     \"     \"  ;", "}", "addTaskEnvironment _     +  =    s ;", "}", "}", "} else    {", "exitUsage (  (  (  ( argv _  . length )     >     0  )     &  &     (  \"  - info \"  . equals ( argv _  [  0  ]  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["parseArgv"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( inputSpecs _  . size (  )  )     =  =     0  )     {", "fail (  \" Required   argument :     - input    < name >  \"  )  ;", "}", "if    (  ( output _  )     =  =    null )     {", "fail (  \" Required   argument :     - output    \"  )  ;", "}", "msg (  (  \" addTaskEnvironment =  \"     +     ( addTaskEnvironment _  )  )  )  ;", "for    ( final   String   kageFile    :    kageFiles _  )     {", "File   f    =    new   File ( kageFile )  ;", "if    ( f . isFile (  )  )     {", "shippedCanonFiles _  . add ( f . getCanonicalPath (  )  )  ;", "}", "}", "msg (  (  \" shippedCanonFiles _  =  \"     +     ( shippedCanonFiles _  )  )  )  ;", "mapCmd _     =    unqualifyIfLocalPath ( mapCmd _  )  ;", "comCmd _     =    unqualifyIfLocalPath ( comCmd _  )  ;", "redCmd _     =    unqualifyIfLocalPath ( redCmd _  )  ;", "}", "METHOD_END"], "methodName": ["postProcessArgs"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "verbose _     =    false ;", "dTaskEnvironment _     =     \" HADOOP _ ROOT _ LOGGER =  \"  ;", "}", "METHOD_END"], "methodName": ["preProcessArgs"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  \" Usage :     $ HADOOP _ PREFIX / bin / hadoop   jar   hadoop - streaming . jar \"     +     \"     [ options ]  \"  )  )  ;", "System . out . println (  \" Options :  \"  )  ;", "System . out . println (  (  \"        - input                               < path >    DFS   input   file ( s )    for   the   Map \"     +     \"    step .  \"  )  )  ;", "System . out . println (  (  \"        - output                            < path >    DFS   output   directory   for   the \"     +     \"    Reduce   step .  \"  )  )  ;", "System . out . println (  (  \"        - mapper                            < cmd | JavaClassName >    Optional .    Command \"     +     \"    to   be   run   as   mapper .  \"  )  )  ;", "System . out . println (  (  \"        - combiner                      < cmd | JavaClassName >    Optional .    Command \"     +     \"    to   be   run   as   combiner .  \"  )  )  ;", "System . out . println (  (  \"        - reducer                         < cmd | JavaClassName >    Optional .    Command \"     +     \"    to   be   run   as   reducer .  \"  )  )  ;", "System . out . println (  (  \"        - file                                  < file >    Optional .    File / dir   to   be    \"     +     (  \" shipped   in   the   Job   jar   file .  \\ n \"     +     \"                                                       Deprecated .    Use   generic   option    \\  \"  - files \\  \"    instead .  \"  )  )  )  ;", "System . out . println (  (  \"        - inputformat             < TextInputFormat ( default )  \"     +     (  \"  | SequenceFileAsTextInputFormat | JavaClassName >  \\ n \"     +     \"                                                       Optional .    The   input   format   class .  \"  )  )  )  ;", "System . out . println (  (  \"        - outputformat          < TextOutputFormat ( default )  \"     +     (  \"  | JavaClassName >  \\ n \"     +     \"                                                       Optional .    The   output   format   class .  \"  )  )  )  ;", "System . out . println (  (  \"        - partitioner             < JavaClassName >       Optional .    The \"     +     \"    partitioner   class .  \"  )  )  ;", "System . out . println (  (  \"        - numReduceTasks    < num >    Optional .    Number   of   reduce    \"     +     \" tasks .  \"  )  )  ;", "System . out . println (  (  \"        - inputreader             < spec >    Optional .    Input   recordreader \"     +     \"    spec .  \"  )  )  ;", "System . out . println (  (  \"        - cmdenv                            < n >  =  < v >    Optional .    Pass   env . var   to \"     +     \"    streaming   commands .  \"  )  )  ;", "System . out . println (  (  \"        - mapdebug                      < cmd >    Optional .     \"     +     \" To   run   this   script   when   a   map   task   fails .  \"  )  )  ;", "System . out . println (  (  \"        - reducedebug             < cmd >    Optional .  \"     +     \"    To   run   this   script   when   a   reduce   task   fails .  \"  )  )  ;", "System . out . println (  (  \"        - io                                        < identifier >    Optional .    Format   to   use \"     +     \"    for   input   to   and   output \"  )  )  ;", "System . out . println (  \"                                                       from   mapper / reducer   commands \"  )  ;", "System . out . println (  \"        - lazyOutput               Optional .    Lazily   create   Output .  \"  )  ;", "System . out . println (  \"        - background               Optional .    Submit   the   job   and   don ' t   wait   till   it   completes .  \"  )  ;", "System . out . println (  \"        - verbose                        Optional .    Print   verbose   output .  \"  )  ;", "System . out . println (  \"        - info                                 Optional .    Print   detailed   usage .  \"  )  ;", "System . out . println (  \"        - help                                 Optional .    Print   help   message .  \"  )  ;", "System . out . println (  )  ;", "GenericOptionsParser . printGenericCommandUsage ( System . out )  ;", "if    (  ! detailed )     {", "System . out . println (  )  ;", "System . out . println (  \" For   more   details   about   these   options :  \"  )  ;", "System . out . println (  (  \" Use    \"     +     \"  $ HADOOP _ PREFIX / bin / hadoop   jar   hadoop - streaming . jar    - info \"  )  )  ;", "return ;", "}", "System . out . println (  )  ;", "System . out . println (  \" Usage   tips :  \"  )  ;", "System . out . println (  (  \" In    - input :    globbing   on    < path >    is   supported   and   can    \"     +     \" have   multiple    - input \"  )  )  ;", "System . out . println (  )  ;", "System . out . println (  (  \" Default   Map   input   format :    a   line   is   a   record   in   UTF -  8     \"     +     \" the   key   part   ends   at   first \"  )  )  ;", "System . out . println (  \"       TAB ,    the   rest   of   the   line   is   the   value \"  )  ;", "System . out . println (  )  ;", "System . out . println (  \" To   pass   a   Custom   input   format :  \"  )  ;", "System . out . println (  \"        - inputformat   package . MyInputFormat \"  )  ;", "System . out . println (  )  ;", "System . out . println (  \" Similarly ,    to   pass   a   custom   output   format :  \"  )  ;", "System . out . println (  \"        - outputformat   package . MyOutputFormat \"  )  ;", "System . out . println (  )  ;", "System . out . println (  (  \" The   files   with   extensions    . class   and    . jar /  . zip ,  \"     +     \"    specified   for   the    - file \"  )  )  ;", "System . out . println (  (  \"       argument [ s ]  ,    end   up   in    \\  \" classes \\  \"    and    \\  \" lib \\  \"     \"     +     \" directories   respectively   inside \"  )  )  ;", "System . out . println (  (  \"       the   working   directory   when   the   mapper   and   reducer   are \"     +     \"    run .    All   other   files \"  )  )  ;", "System . out . println (  (  \"       specified   for   the    - file   argument [ s ]  \"     +     \"    end   up   in   the   working   directory   when   the \"  )  )  ;", "System . out . println (  (  \"       mapper   and   reducer   are   run .    The   location   of   this    \"     +     \" working   directory   is \"  )  )  ;", "System . out . println (  \"       unspecified .  \"  )  ;", "System . out . println (  )  ;", "System . out . println (  (  \" To   set   the   number   of   reduce   tasks    ( num .    of   output    \"     +     \" files )    as ,    say    1  0  :  \"  )  )  ;", "System . out . println (  \"       Use    - numReduceTasks    1  0  \"  )  ;", "System . out . println (  \" To   skip   the   sort / combine / shuffle / sort / reduce   step :  \"  )  ;", "System . out . println (  \"       Use    - numReduceTasks    0  \"  )  ;", "System . out . println (  (  \"       Map   output   then   becomes   a    ' side - effect    \"     +     \" output '    rather   than   a   reduce   input .  \"  )  )  ;", "System . out . println (  (  \"       This   speeds   up   processing .    This   also   feels    \"     +     \" more   like    \\  \" in - place \\  \"    processing \"  )  )  ;", "System . out . println (  (  \"       because   the   input   filename   and   the   map    \"     +     \" input   order   are   preserved .  \"  )  )  ;", "System . out . println (  \"       This   is   equivalent   to    - reducer   NONE \"  )  ;", "System . out . println (  )  ;", "System . out . println (  \" To   speed   up   the   last   maps :  \"  )  ;", "System . out . println (  (  (  \"        - D    \"     +     ( MRJobConfig . MAP _ SPECULATIVE )  )     +     \"  = true \"  )  )  ;", "System . out . println (  \" To   speed   up   the   last   reduces :  \"  )  ;", "System . out . println (  (  (  \"        - D    \"     +     ( MRJobConfig . REDUCE _ SPECULATIVE )  )     +     \"  = true \"  )  )  ;", "System . out . println (  \" To   name   the   job    ( appears   in   the   JobTracker   Web   UI )  :  \"  )  ;", "System . out . println (  (  (  \"        - D    \"     +     ( MRJobConfig . JOB _ NAME )  )     +     \"  =  ' My   Job '  \"  )  )  ;", "System . out . println (  \" To   change   the   local   temp   directory :  \"  )  ;", "System . out . println (  \"        - D   dfs . data . dir =  / tmp / dfs \"  )  ;", "System . out . println (  \"        - D   stream . tmpdir =  / tmp / streaming \"  )  ;", "System . out . println (  \" Additional   local   temp   directories   with    - jt   local :  \"  )  ;", "System . out . println (  (  (  \"        - D    \"     +     ( MRConfig . LOCAL _ DIR )  )     +     \"  =  / tmp / local \"  )  )  ;", "System . out . println (  (  (  \"        - D    \"     +     ( JTConfig . JT _ SYSTEM _ DIR )  )     +     \"  =  / tmp / system \"  )  )  ;", "System . out . println (  (  (  \"        - D    \"     +     ( MRConfig . TEMP _ DIR )  )     +     \"  =  / tmp / temp \"  )  )  ;", "System . out . println (  \" To   treat   tasks   with   non - zero   exit   status   as   SUCCEDED :  \"  )  ;", "System . out . println (  \"        - D   stream . non . zero . exit . is . failure = false \"  )  ;", "System . out . println (  (  \" Use   a   custom   hadoop   streaming   build   along   with   standard \"     +     \"    hadoop   install :  \"  )  )  ;", "System . out . println (  (  \"        $ HADOOP _ PREFIX / bin / hadoop   jar    \"     +     \"  / path / my - hadoop - streaming . jar    [  .  .  .  ]  \\  \\  \"  )  )  ;", "System . out . println (  (  \"              [  .  .  .  ]     - D   stream . shipped . hadoopstreaming =  \"     +     \"  / path / my - hadoop - streaming . jar \"  )  )  ;", "System . out . println (  \" For   more   details   about   jobconf   parameters   see :  \"  )  ;", "System . out . println (  \"       http :  /  / wiki . apache . org / hadoop / JobConfFile \"  )  ;", "System . out . println (  (  \" To   set   an   environement   variable   in   a   streaming    \"     +     \" command :  \"  )  )  ;", "System . out . println (  \"           - cmdenv   EXAMPLE _ DIR =  / home / example / dictionaries /  \"  )  ;", "System . out . println (  )  ;", "System . out . println (  \" Shortcut :  \"  )  ;", "System . out . println (  (  \"          setenv   HSTREAMING    \\  \"  $ HADOOP _ PREFIX / bin / hadoop   jar    \"     +     \" hadoop - streaming . jar \\  \"  \"  )  )  ;", "System . out . println (  )  ;", "System . out . println (  (  \" Example :     $ HSTREAMING    - mapper    \"     +     \"  \\  \"  / usr / local / bin / perl 5    filter . pl \\  \"  \"  )  )  ;", "System . out . println (  (  \"                                   - file    / local / filter . pl    - input    \"     +     \"  \\  \"  / logs /  0  6  0  4  *  /  *  \\  \"     [  .  .  .  ]  \"  )  )  ;", "System . out . println (  (  \"       Ships   a   script ,    invokes   the   non - shipped   perl    \"     +     \" interpreter .    Shipped   files   go   to \"  )  )  ;", "System . out . println (  (  \"       the   working   directory   so   filter . pl   is   found   by   perl .     \"     +     \" Input   files   are   all   the \"  )  )  ;", "System . out . println (  \"       daily   logs   for   days   in   month    2  0  0  6  -  0  4  \"  )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( additionalConfSpec _  )     !  =    null )     {", "StreamJob . LOG . warn (  \"  - additionalconfspec   option   is   deprecated ,    please   use    - conf   instead .  \"  )  ;", "config _  . addResource ( new   Path ( additionalConfSpec _  )  )  ;", "}", "jobConf _     =    new   mapred . JobConf ( config _  ,    StreamJob . class )  ;", "for    ( int   i    =     0  ;    i    <     ( inputSpecs _  . size (  )  )  ;    i +  +  )     {", "FileInputFormat . addInputPaths ( jobConf _  ,     (  ( String )     ( inputSpecs _  . get ( i )  )  )  )  ;", "}", "String   defaultPackage    =    this . getClass (  )  . getPackage (  )  . getName (  )  ;", "Class   c ;", "Class   fmt    =    null ;", "if    (  (  ( inReaderSpec _  )     =  =    null )     &  &     (  ( inputFormatSpec _  )     =  =    null )  )     {", "fmt    =    TextInputFormat . class ;", "} else", "if    (  ( inputFormatSpec _  )     !  =    null )     {", "if    (  (  ( inputFormatSpec _  . equals ( TextInputFormat . class . getName (  )  )  )     |  |     ( inputFormatSpec _  . equals ( TextInputFormat . class . getCanonicalName (  )  )  )  )     |  |     ( inputFormatSpec _  . equals ( TextInputFormat . class . getSimpleName (  )  )  )  )     {", "fmt    =    TextInputFormat . class ;", "} else", "if    (  (  ( inputFormatSpec _  . equals ( mapred . KeyValueTextInputFormat . class . getName (  )  )  )     |  |     ( inputFormatSpec _  . equals ( mapred . KeyValueTextInputFormat . class . getCanonicalName (  )  )  )  )     |  |     ( inputFormatSpec _  . equals ( mapred . KeyValueTextInputFormat . class . getSimpleName (  )  )  )  )     {", "if    (  ( inReaderSpec _  )     =  =    null )     {", "fmt    =    mapred . KeyValueTextInputFormat . class ;", "}", "} else", "if    (  (  ( inputFormatSpec _  . equals ( mapred . SequenceFileInputFormat . class . getName (  )  )  )     |  |     ( inputFormatSpec _  . equals ( mapred . SequenceFileInputFormat . class . getCanonicalName (  )  )  )  )     |  |     ( inputFormatSpec _  . equals ( mapred . SequenceFileInputFormat . class . getSimpleName (  )  )  )  )     {", "if    (  ( inReaderSpec _  )     =  =    null )     {", "fmt    =    mapred . SequenceFileInputFormat . class ;", "}", "} else", "if    (  (  ( inputFormatSpec _  . equals ( mapred . SequenceFileAsTextInputFormat . class . getName (  )  )  )     |  |     ( inputFormatSpec _  . equals ( mapred . SequenceFileAsTextInputFormat . class . getCanonicalName (  )  )  )  )     |  |     ( inputFormatSpec _  . equals ( mapred . SequenceFileAsTextInputFormat . class . getSimpleName (  )  )  )  )     {", "fmt    =    mapred . SequenceFileAsTextInputFormat . class ;", "} else    {", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    inputFormatSpec _  ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "fmt    =    c ;", "} else    {", "fail (  (  \"  - inputformat    :    class   not   found    :     \"     +     ( inputFormatSpec _  )  )  )  ;", "}", "}", "}", "if    ( fmt    =  =    null )     {", "fmt    =    StreamInputFormat . class ;", "}", "jobConf _  . setInputFormat ( fmt )  ;", "if    (  ( ioSpec _  )     !  =    null )     {", "jobConf _  . set (  \" stream . map . input \"  ,    ioSpec _  )  ;", "jobConf _  . set (  \" stream . map . output \"  ,    ioSpec _  )  ;", "jobConf _  . set (  \" stream . reduce . input \"  ,    ioSpec _  )  ;", "jobConf _  . set (  \" stream . reduce . output \"  ,    ioSpec _  )  ;", "}", "Class <  ?    extends   IdentifierResolver >    idResolverClass    =    jobConf _  . getClass (  \" stream . io . identifier . resolver . class \"  ,    IdentifierResolver . class ,    IdentifierResolver . class )  ;", "IdentifierResolver   idResolver    =    ReflectionUtils . newInstance ( idResolverClass ,    jobConf _  )  ;", "idResolver . resolve ( jobConf _  . get (  \" stream . map . input \"  ,    IdentifierResolver . TEXT _ ID )  )  ;", "jobConf _  . setClass (  \" stream . map . input . writer . class \"  ,    idResolver . getInputWriterClass (  )  ,    InputWriter . class )  ;", "idResolver . resolve ( jobConf _  . get (  \" stream . reduce . input \"  ,    IdentifierResolver . TEXT _ ID )  )  ;", "jobConf _  . setClass (  \" stream . reduce . input . writer . class \"  ,    idResolver . getInputWriterClass (  )  ,    InputWriter . class )  ;", "jobConf _  . set (  \" stream . addenvironment \"  ,    addTaskEnvironment _  )  ;", "boolean   isMapperACommand    =    false ;", "if    (  ( mapCmd _  )     !  =    null )     {", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    mapCmd _  ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "jobConf _  . setMapperClass ( c )  ;", "} else    {", "isMapperACommand    =    true ;", "jobConf _  . setMapperClass ( PipeMapper . class )  ;", "jobConf _  . setMapRunnerClass ( PipeMapRunner . class )  ;", "jobConf _  . set (  \" stream . map . streamprocessor \"  ,    URLEncoder . encode ( mapCmd _  ,     \" UTF -  8  \"  )  )  ;", "}", "}", "if    (  ( comCmd _  )     !  =    null )     {", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    comCmd _  ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "jobConf _  . setCombinerClass ( c )  ;", "} else    {", "jobConf _  . setCombinerClass ( PipeCombiner . class )  ;", "jobConf _  . set (  \" stream . combine . streamprocessor \"  ,    URLEncoder . encode ( comCmd _  ,     \" UTF -  8  \"  )  )  ;", "}", "}", "if    (  ( numReduceTasksSpec _  )     !  =    null )     {", "int   numReduceTasks    =    Integer . parseInt ( numReduceTasksSpec _  )  ;", "jobConf _  . setNumReduceTasks ( numReduceTasks )  ;", "}", "boolean   isReducerACommand    =    false ;", "if    (  ( redCmd _  )     !  =    null )     {", "if    ( redCmd _  . equals ( StreamJob . REDUCE _ NONE )  )     {", "jobConf _  . setNumReduceTasks (  0  )  ;", "}", "if    (  ( jobConf _  . getNumReduceTasks (  )  )     !  =     0  )     {", "if    (  ( redCmd _  . compareToIgnoreCase (  \" aggregate \"  )  )     =  =     0  )     {", "jobConf _  . setReducerClass ( ValueAggregatorReducer . class )  ;", "jobConf _  . setCombinerClass ( ValueAggregatorCombiner . class )  ;", "} else    {", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    redCmd _  ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "jobConf _  . setReducerClass ( c )  ;", "} else    {", "isReducerACommand    =    true ;", "jobConf _  . setReducerClass ( PipeReducer . class )  ;", "jobConf _  . set (  \" stream . reduce . streamprocessor \"  ,    URLEncoder . encode ( redCmd _  ,     \" UTF -  8  \"  )  )  ;", "}", "}", "}", "}", "idResolver . resolve ( jobConf _  . get (  \" stream . map . output \"  ,    IdentifierResolver . TEXT _ ID )  )  ;", "jobConf _  . setClass (  \" stream . map . output . reader . class \"  ,    idResolver . getOutputReaderClass (  )  ,    OutputReader . class )  ;", "if    ( isMapperACommand    |  |     (  ( jobConf _  . get (  \" stream . map . output \"  )  )     !  =    null )  )     {", "jobConf _  . setMapOutputKeyClass ( idResolver . getOutputKeyClass (  )  )  ;", "jobConf _  . setMapOutputValueClass ( idResolver . getOutputValueClass (  )  )  ;", "if    (  ( jobConf _  . getNumReduceTasks (  )  )     =  =     0  )     {", "jobConf _  . setOutputKeyClass ( idResolver . getOutputKeyClass (  )  )  ;", "jobConf _  . setOutputValueClass ( idResolver . getOutputValueClass (  )  )  ;", "}", "}", "idResolver . resolve ( jobConf _  . get (  \" stream . reduce . output \"  ,    IdentifierResolver . TEXT _ ID )  )  ;", "jobConf _  . setClass (  \" stream . reduce . output . reader . class \"  ,    idResolver . getOutputReaderClass (  )  ,    OutputReader . class )  ;", "if    ( isReducerACommand    |  |     (  ( jobConf _  . get (  \" stream . reduce . output \"  )  )     !  =    null )  )     {", "jobConf _  . setOutputKeyClass ( idResolver . getOutputKeyClass (  )  )  ;", "jobConf _  . setOutputValueClass ( idResolver . getOutputValueClass (  )  )  ;", "}", "if    (  ( inReaderSpec _  )     !  =    null )     {", "String [  ]    args    =    inReaderSpec _  . split (  \"  ,  \"  )  ;", "String   readerClass    =    args [  0  ]  ;", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    readerClass ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "jobConf _  . set (  \" stream . recordreader . class \"  ,    c . getName (  )  )  ;", "} else    {", "fail (  (  \"  - inputreader :    class   not   found :     \"     +    readerClass )  )  ;", "}", "for    ( int   i    =     1  ;    i    <     ( args . length )  ;    i +  +  )     {", "String [  ]    nv    =    args [ i ]  . split (  \"  =  \"  ,     2  )  ;", "String   k    =     \" stream . recordreader .  \"     +     ( nv [  0  ]  )  ;", "String   v    =     (  ( nv . length )     >     1  )     ?    nv [  1  ]     :     \"  \"  ;", "jobConf _  . set ( k ,    v )  ;", "}", "}", "FileOutputFormat . setOutputPath ( jobConf _  ,    new   Path ( output _  )  )  ;", "fmt    =    null ;", "if    (  ( outputFormatSpec _  )     !  =    null )     {", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    outputFormatSpec _  ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "fmt    =    c ;", "} else    {", "fail (  (  \"  - outputformat    :    class   not   found    :     \"     +     ( outputFormatSpec _  )  )  )  ;", "}", "}", "if    ( fmt    =  =    null )     {", "fmt    =    TextOutputFormat . class ;", "}", "if    ( lazyOutput _  )     {", "LazyOutputFormat . setOutputFormatClass ( jobConf _  ,    fmt )  ;", "} else    {", "jobConf _  . setOutputFormat ( fmt )  ;", "}", "if    (  ( partitionerSpec _  )     !  =    null )     {", "c    =    StreamUtil . goodClassOrNull ( jobConf _  ,    partitionerSpec _  ,    defaultPackage )  ;", "if    ( c    !  =    null )     {", "jobConf _  . setPartitionerClass ( c )  ;", "} else    {", "fail (  (  \"  - partitioner    :    class   not   found    :     \"     +     ( partitionerSpec _  )  )  )  ;", "}", "}", "if    (  ( mapDebugSpec _  )     !  =    null )     {", "jobConf _  . setMapDebugScript ( mapDebugSpec _  )  ;", "}", "if    (  ( reduceDebugSpec _  )     !  =    null )     {", "jobConf _  . setReduceDebugScript ( reduceDebugSpec _  )  ;", "}", "jar _     =    packageJobJar (  )  ;", "if    (  ( jar _  )     !  =    null )     {", "jobConf _  . setJar ( jar _  )  ;", "}", "if    (  (  ( cacheArchives )     !  =    null )     |  |     (  ( cacheFiles )     !  =    null )  )     {", "getURIs ( cacheArchives ,    cacheFiles )  ;", "boolean   b    =    DistributedCache . checkURIs ( fileURIs ,    archiveURIs )  ;", "if    (  ! b )", "fail ( StreamJob . LINK _ URI )  ;", "}", "if    (  ( cacheArchives )     !  =    null )", "DistributedCache . setCacheArchives ( archiveURIs ,    jobConf _  )  ;", "if    (  ( cacheFiles )     !  =    null )", "DistributedCache . setCacheFiles ( fileURIs ,    jobConf _  )  ;", "if    ( verbose _  )     {", "listJobConfProperties (  )  ;", "}", "msg (  (  \" submitting   to   jobconf :     \"     +     ( getJobTrackerHostPort (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setJobConf"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "Option   input    =    createOption (  \" input \"  ,     \" DFS   input   file ( s )    for   the   Map   step \"  ,     \" path \"  ,    Integer . MAX _ VALUE ,    false )  ;", "Option   output    =    createOption (  \" output \"  ,     \" DFS   output   directory   for   the   Reduce   step \"  ,     \" path \"  ,     1  ,    false )  ;", "Option   mapper    =    createOption (  \" mapper \"  ,     \" The      command   to   run \"  ,     \" cmd \"  ,     1  ,    false )  ;", "Option   combiner    =    createOption (  \" combiner \"  ,     \" The      command   to   run \"  ,     \" cmd \"  ,     1  ,    false )  ;", "Option   reducer    =    createOption (  \" reducer \"  ,     \" The      command   to   run \"  ,     \" cmd \"  ,     1  ,    false )  ;", "Option   file    =    createOption (  \" file \"  ,     \" File   to   be   shipped   in   the   Job   jar   file \"  ,     \" file \"  ,    Integer . MAX _ VALUE ,    false )  ;", "Option   dfs    =    createOption (  \" dfs \"  ,     \" Optional .    Override   DFS   configuration \"  ,     \"  < h : p >  | local \"  ,     1  ,    false )  ;", "Option   additionalconfspec    =    createOption (  \" additionalconfspec \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   inputformat    =    createOption (  \" inputformat \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   outputformat    =    createOption (  \" outputformat \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   partitioner    =    createOption (  \" partitioner \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   numReduceTasks    =    createOption (  \" numReduceTasks \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   inputreader    =    createOption (  \" inputreader \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   mapDebug    =    createOption (  \" mapdebug \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   reduceDebug    =    createOption (  \" reducedebug \"  ,     \" Optional \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   jobconf    =    createOption (  \" jobconf \"  ,     \"  ( n = v )    Optional .    Add   or   override   a   JobConf   property .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   cmdenv    =    createOption (  \" cmdenv \"  ,     \"  ( n = v )    Pass   env . var   to      commands .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   cacheFile    =    createOption (  \" cacheFile \"  ,     \" File   name   URI \"  ,     \" fileNameURI \"  ,    Integer . MAX _ VALUE ,    false )  ;", "Option   cacheArchive    =    createOption (  \" cacheArchive \"  ,     \" File   name   URI \"  ,     \" fileNameURI \"  ,    Integer . MAX _ VALUE ,    false )  ;", "Option   io    =    createOption (  \" io \"  ,     \" Optional .  \"  ,     \" spec \"  ,     1  ,    false )  ;", "Option   background    =    createBoolOption (  \" background \"  ,     \" Submit   the   job   and   don ' t   wait   till   it   completes .  \"  )  ;", "Option   verbose    =    createBoolOption (  \" verbose \"  ,     \" print   verbose   output \"  )  ;", "Option   info    =    createBoolOption (  \" info \"  ,     \" print   verbose   output \"  )  ;", "Option   help    =    createBoolOption (  \" help \"  ,     \" print   this   help   message \"  )  ;", "Option   debug    =    createBoolOption (  \" debug \"  ,     \" print   debug   output \"  )  ;", "Option   lazyOutput    =    createBoolOption (  \" lazyOutput \"  ,     \" create   outputs   lazily \"  )  ;", "allOptions    =    new   Options (  )  . addOption ( input )  . addOption ( output )  . addOption ( mapper )  . addOption ( combiner )  . addOption ( reducer )  . addOption ( file )  . addOption ( dfs )  . addOption ( additionalconfspec )  . addOption ( inputformat )  . addOption ( outputformat )  . addOption ( partitioner )  . addOption ( numReduceTasks )  . addOption ( inputreader )  . addOption ( mapDebug )  . addOption ( reduceDebug )  . addOption ( jobconf )  . addOption ( cmdenv )  . addOption ( cacheFile )  . addOption ( cacheArchive )  . addOption ( io )  . addOption ( background )  . addOption ( verbose )  . addOption ( info )  . addOption ( debug )  . addOption ( help )  . addOption ( lazyOutput )  ;", "}", "METHOD_END"], "methodName": ["setupOptions"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( jar _  )     !  =    null )     &  &     ( isLocalHadoop (  )  )  )     {", "File   wd    =    new   File (  \"  .  \"  )  . getAbsoluteFile (  )  ;", "RunJar . unJar ( new   File ( jar _  )  ,    wd )  ;", "}", "jc _     =    new   mapred . JobClient ( jobConf _  )  ;", "running _     =    null ;", "try    {", "running _     =    jc _  . submitJob ( jobConf _  )  ;", "jobId _     =    running _  . getID (  )  ;", "if    ( background _  )     {", "StreamJob . LOG . info (  \" Job   is   running   in   background .  \"  )  ;", "} else", "if    (  !  ( jc _  . monitorAndPrintJob ( jobConf _  ,    running _  )  )  )     {", "StreamJob . LOG . error (  \" Job   not   Successful !  \"  )  ;", "return    1  ;", "}", "StreamJob . LOG . info (  (  \" Output   directory :     \"     +     ( output _  )  )  )  ;", "}    catch    ( FileNotFoundException   fe )     {", "StreamJob . LOG . error (  (  \" Error   launching   job    ,    bad   input   path    :     \"     +     ( fe . getMessage (  )  )  )  )  ;", "return    2  ;", "}    catch    ( InvalidJobConfException   je )     {", "StreamJob . LOG . error (  (  \" Error   launching   job    ,    Invalid   job   conf    :     \"     +     ( je . getMessage (  )  )  )  )  ;", "return    3  ;", "}    catch    ( FileAlreadyExistsException   fae )     {", "StreamJob . LOG . error (  (  \" Error   launching   job    ,    Output   path   already   exists    :     \"     +     ( fae . getMessage (  )  )  )  )  ;", "return    4  ;", "}    catch    ( IOException   ioe )     {", "StreamJob . LOG . error (  (  \" Error   Launching   job    :     \"     +     ( ioe . getMessage (  )  )  )  )  ;", "return    5  ;", "}    catch    ( InterruptedException   ie )     {", "StreamJob . LOG . error (  (  \" Error   monitoring   job    :     \"     +     ( ie . getMessage (  )  )  )  )  ;", "return    6  ;", "}    finally    {", "jc _  . close (  )  ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["submitAndMonitorJob"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "if    ( cmd    =  =    null )     {", "} else    {", "ing   prog    =    cmd ;", "ing   args    =     \"  \"  ;", "int   s    =    cmd . indexOf (  \"     \"  )  ;", "if    ( s    !  =     (  -  1  )  )     {", "prog    =    cmd . substring (  0  ,    s )  ;", "args    =    cmd . substring (  ( s    +     1  )  )  ;", "}", "ing   progCanon ;", "try    {", "progCanon    =    new   File ( prog )  . getCanonicalPath (  )  ;", "}    catch    ( IOException   io )     {", "progCanon    =    prog ;", "}", "boolean   shipped    =    shippedCanonFiles _  . contains ( progCanon )  ;", "msg (  (  (  (  \" shipped :     \"     +    shipped )     +     \"     \"  )     +    progCanon )  )  ;", "if    ( shipped )     {", "prog    =    new   File ( prog )  . getName (  )  ;", "if    (  ( args . length (  )  )     >     0  )     {", "cmd    =     ( prog    +     \"     \"  )     +    args ;", "} else    {", "cmd    =    prog ;", "}", "}", "}", "msg (  (  \" cmd =  \"     +    cmd )  )  ;", "return   cmd ;", "}", "METHOD_END"], "methodName": ["unqualifyIfLocalPath"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "for    ( String   file    :    values )     {", "File   f    =    new   File ( file )  ;", "if    (  !  ( FileUtil . canRead ( f )  )  )     {", "fail (  (  (  \" File :     \"     +     ( f . getAbsolutePath (  )  )  )     +     \"    does   not   exist ,    or   is   not   readable .  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validate"], "fileName": "org.apache.hadoop.streaming.StreamJob"}, {"methodBody": ["METHOD_START", "{", "return   findNthByte ( utf ,     0  ,    utf . length ,     (  ( byte )     (  '  \\ t '  )  )  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["findTab"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =    start ;    i    <     ( start    +    length )  ;    i +  +  )     {", "if    (  ( utf [ i ]  )     =  =     (  ( byte )     (  '  \\ t '  )  )  )     {", "return   i ;", "}", "}", "return    -  1  ;", "}", "METHOD_END"], "methodName": ["findTab"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "out . clear (  )  ;", "return   lineReader . dLine ( out )  ;", "}", "METHOD_END"], "methodName": ["readLine"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "StreamKeyValUtil . splitKeyVal ( utf ,    start ,    length ,    key ,    val ,    splitPos ,     1  )  ;", "}", "METHOD_END"], "methodName": ["splitKeyVal"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "if    (  ( splitPos    <    start )     |  |     ( splitPos    >  =     ( start    +    length )  )  )", "throw   new   IllegalArgumentException (  (  (  (  (  (  (  \" splitPos   must   be   in   the   range    \"     +     \"  [  \"  )     +    start )     +     \"  ,     \"  )     +     ( start    +    length )  )     +     \"  ]  :     \"  )     +    splitPos )  )  ;", "int   keyLen    =    splitPos    -    start ;", "int   valLen    =     (  ( start    +    length )     -    splitPos )     -    separatorLength ;", "key . set ( utf ,    start ,    keyLen )  ;", "val . set ( utf ,     ( splitPos    +    separatorLength )  ,    valLen )  ;", "}", "METHOD_END"], "methodName": ["splitKeyVal"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "StreamKeyValUtil . splitKeyVal ( utf ,     0  ,    utf . length ,    key ,    val ,    splitPos ,     1  )  ;", "}", "METHOD_END"], "methodName": ["splitKeyVal"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "StreamKeyValUtil . splitKeyVal ( utf ,     0  ,    utf . length ,    key ,    val ,    splitPos ,    separatorLength )  ;", "}", "METHOD_END"], "methodName": ["splitKeyVal"], "fileName": "org.apache.hadoop.streaming.StreamKeyValUtil"}, {"methodBody": ["METHOD_START", "{", "if    (  ( StreamUtil . env )     !  =    null )     {", "return   StreamUtil . env ;", "}", "try    {", "StreamUtil . env    =    new   Environment (  )  ;", "}    catch    ( IOException   io )     {", "io . printStackTrace (  )  ;", "}", "return   StreamUtil . env ;", "}", "METHOD_END"], "methodName": ["env"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "return   StreamUtil . findInClasspath ( className ,    StreamUtil . class . getClassLoader (  )  )  ;", "}", "METHOD_END"], "methodName": ["findInClasspath"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "String   relPath    =    className ;", "relPath    =    relPath . replace (  '  .  '  ,     '  /  '  )  ;", "relPath    +  =     \"  . class \"  ;", "URL   classUrl    =    loader . getResource ( relPath )  ;", "String   codePath ;", "if    ( classUrl    !  =    null )     {", "boolean   inJar    =    classUrl . getProtocol (  )  . equals (  \" jar \"  )  ;", "codePath    =    classUrl . toString (  )  ;", "if    ( codePath . startsWith (  \" jar :  \"  )  )     {", "codePath    =    codePath . substring (  \" jar :  \"  . length (  )  )  ;", "}", "if    ( codePath . startsWith (  \" file :  \"  )  )     {", "codePath    =    codePath . substring (  \" file :  \"  . length (  )  )  ;", "}", "if    ( inJar )     {", "int   bang    =    codePath . lastIndexOf (  '  !  '  )  ;", "codePath    =    codePath . substring (  0  ,    bang )  ;", "} else    {", "int   pos    =    codePath . lastIndexOf ( relPath )  ;", "if    ( pos    =  =     (  -  1  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" invalid   codePath :    className =  \"     +    className )     +     \"    codePath =  \"  )     +    codePath )  )  ;", "}", "codePath    =    codePath . substring (  0  ,    pos )  ;", "}", "} else    {", "codePath    =    null ;", "}", "return   codePath ;", "}", "METHOD_END"], "methodName": ["findInClasspath"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "return   StreamUtil . host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "Class   clazz    =    null ;", "y    {", "clazz    =    conf . getClassByName ( className )  ;", "}    catch    ( ClassNotFoundException   cnf )     {", "}", "if    ( clazz    =  =    null )     {", "if    (  (  ( className . indexOf (  '  .  '  )  )     =  =     (  -  1  )  )     &  &     ( defaultPackage    !  =    null )  )     {", "className    =     ( defaultPackage    +     \"  .  \"  )     +    className ;", "y    {", "clazz    =    conf . getClassByName ( className )  ;", "}    catch    ( ClassNotFoundException   cnf )     {", "}", "}", "}", "return   clazz ;", "}", "METHOD_END"], "methodName": ["goodClassOrNull"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "String   framework    =    job . get ( FRAMEWORK _ NAME ,    LOCAL _ FRAMEWORK _ NAME )  ;", "return   framework . equals ( LOCAL _ FRAMEWORK _ NAME )  ;", "}", "METHOD_END"], "methodName": ["isLocalJobTracker"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "return    . qualifyHost ( new   URL ( url )  )  . toString (  )  ;", "}    catch    ( IOException   io )     {", "return   url ;", "}", "}", "METHOD_END"], "methodName": ["qualifyHost"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "InetAddress   a    =    InetAddress . getByName ( url . getHost (  )  )  ;", "Str   qualHost    =    a . getCanonicalHostName (  )  ;", "URL   q    =    new   URL ( url . getProtocol (  )  ,    qualHost ,    url . getPort (  )  ,    url . getFile (  )  )  ;", "return   q ;", "}    catch    ( IOException   io )     {", "return   url ;", "}", "}", "METHOD_END"], "methodName": ["qualifyHost"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   buf    =    new   StringBuffer (  )  ;", "char [  ]    ch    =    plain . toCharArray (  )  ;", "int   csup    =    ch . length ;", "for    ( int   c    =     0  ;    c    <    csup ;    c +  +  )     {", "if    (  (  . regexpSpecials . indexOf ( ch [ c ]  )  )     !  =     (  -  1  )  )     {", "buf . append (  \"  \\  \\  \"  )  ;", "}", "buf . append ( ch [ c ]  )  ;", "}", "return   buf . toString (  )  ;", "}", "METHOD_END"], "methodName": ["regexpEscape"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "int   len    =     (  ( int )     ( f . length (  )  )  )  ;", "byte [  ]    buf    =    new   byte [ len ]  ;", "FileInput   in    =    new   FileInput ( f )  ;", "String   contents    =    null ;", "try    {", "in . read ( buf ,     0  ,    len )  ;", "contents    =    new   String ( buf ,     \" UTF -  8  \"  )  ;", "}    finally    {", "in . close (  )  ;", "}", "return   contents ;", "}", "METHOD_END"], "methodName": ["slurp"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "int   len    =     (  ( int )     ( fs . getFileStatus ( p )  . getLen (  )  )  )  ;", "byte [  ]    buf    =    new   byte [ len ]  ;", "FSDataInput   in    =    fs . open ( p )  ;", "String   contents    =    null ;", "try    {", "in . readFully ( in . getPos (  )  ,    buf )  ;", "contents    =    new   String ( buf ,     \" UTF -  8  \"  )  ;", "}    finally    {", "in . close (  )  ;", "}", "return   contents ;", "}", "METHOD_END"], "methodName": ["slurpHadoop"], "fileName": "org.apache.hadoop.streaming.StreamUtil"}, {"methodBody": ["METHOD_START", "{", "if    (  ( pat . length (  )  )     >     0  )     {", "patpend (  \"  |  \"  )  ;", "}", "patpend (  \"  (  \"  )  ;", "patpend ( esedGroup )  ;", "patpend (  \"  )  \"  )  ;", "}", "METHOD_END"], "methodName": ["addGroup"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "String   val    =    job _  . get ( prop )  ;", "if    ( val    =  =    null )     {", "throw   new   IOException (  (  \" JobConf :    missing   required   property :     \"     +    prop )  )  ;", "}", "return   val ;", "}", "METHOD_END"], "methodName": ["checkJobGet"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    cpat    =    textPat . getBytes (  \" UTF -  8  \"  )  ;", "int   m    =     0  ;", "boolean   match    =    false ;", "int   msup    =    cpat . length ;", "int   LL    =     1  2  0  0  0  0     *     1  0  ;", "bin _  . mark ( LL )  ;", "while    ( true )     {", "int   b    =    bin _  . d (  )  ;", "if    ( b    =  =     (  -  1  )  )", "bk ;", "byte   c    =     (  ( byte )     ( b )  )  ;", "if    ( c    =  =     ( cpat [ m ]  )  )     {", "m +  +  ;", "if    ( m    =  =    msup )     {", "match    =    true ;", "bk ;", "}", "} else    {", "bin _  . mark ( LL )  ;", "if    ( outBufOrNull    !  =    null )     {", "outBufOrNull . write ( cpat ,     0  ,    m )  ;", "outBufOrNull . write ( c )  ;", "}", "pos _     +  =    m    +     1  ;", "m    =     0  ;", "}", "}", "if    (  (  ! includePat )     &  &    match )     {", "bin _  . reset (  )  ;", "} else", "if    ( outBufOrNull    !  =    null )     {", "outBufOrNull . write ( cpat )  ;", "pos _     +  =    msup ;", "}", "return   match ;", "}", "METHOD_END"], "methodName": ["fastReadUntilMatch"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "StreamBaseRecordReader . LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  \" StreamBaseRecordReader . init :     \"     +     \"    start _  =  \"  )     +     ( start _  )  )     +     \"    end _  =  \"  )     +     ( end _  )  )     +     \"    length _  =  \"  )     +     ( length _  )  )     +     \"    start _     >    in _  . getPos (  )     =  \"  )     +     (  ( start _  )     >     ( in _  . getPos (  )  )  )  )     +     \"     \"  )     +     ( start _  )  )     +     \"     >     \"  )     +     ( in _  . getPos (  )  )  )  )  ;", "if    (  ( start _  )     >     ( in _  . getPos (  )  )  )     {", "in _  . seek ( start _  )  ;", "}", "pos _     =    start _  ;", "bin _     =    new   BufferedInputStream ( in _  )  ;", "seekNextRecordBoundary (  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   pat    =    new   StringBuffer (  )  ;", "addGroup ( pat ,    Util . regexpEscape (  \" CDATA [  \"  )  )  ;", "addGroup ( pat ,    Util . regexpEscape (  \"  ]  ]  >  \"  )  )  ;", "addGroup ( pat ,    escapedMark )  ;", "return   Pattern . compile ( pat . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["makePatternCDataOrMark"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "( numNext )  +  +  ;", "if    (  ( pos _  )     >  =     ( end _  )  )     {", "return   false ;", "}", "DataOutputBuffer   buf    =    new   DataOutputBuffer (  )  ;", "if    (  !  ( readUntilMatchBegin (  )  )  )     {", "return   false ;", "}", "if    (  (  ( pos _  )     >  =     ( end _  )  )     |  |     (  !  ( readUntilMatchEnd ( buf )  )  )  )     {", "return   false ;", "}", "byte [  ]    r    =    new   byte [ buf . getLength (  )  ]  ;", "System . arraycopy ( buf . getData (  )  ,     0  ,    r ,     0  ,    r . length )  ;", "numRecStats ( r ,     0  ,    r . length )  ;", "key . set ( r )  ;", "value . set (  \"  \"  )  ;", "return   true ;", "}", "METHOD_END"], "methodName": ["next"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "switch    ( state )     {", "case    . CDATA _ UNK    :", "case    . CDATA _ OUT    :", "switch    ( input )     {", "case    . CDATA _ BEGIN    :", "return    . CDATA _ IN ;", "case    . CDATA _ END    :", "if    ( state    =  =     (  . CDATA _ OUT )  )     {", "}", "return    . CDATA _ OUT ;", "case    . RECORD _ MAYBE    :", "return   state    =  =     (  . CDATA _ UNK )     ?     . CDATA _ UNK    :     . RECORD _ ACCEPT ;", "}", "break ;", "case    . CDATA _ IN    :", "return   input    =  =     (  . CDATA _ END )     ?     . CDATA _ OUT    :     . CDATA _ IN ;", "}", "throw   new   IllegalStateException (  (  (  (  (  (  ( state    +     \"     \"  )     +    input )     +     \"     \"  )     +    bufPos )     +     \"     \"  )     +     ( splitName _  )  )  )  ;", "}", "METHOD_END"], "methodName": ["nextState"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    ( slowMatch _  )     {", "return   slowUntilMatch ( beginPat _  ,    false ,    null )  ;", "} else    {", "return   fastUntilMatch ( beginMark _  ,    false ,    null )  ;", "}", "}", "METHOD_END"], "methodName": ["readUntilMatchBegin"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    ( slowMatch _  )     {", "return   slowUntilMatch ( endPat _  ,    true ,    buf )  ;", "} else    {", "return   fastUntilMatch ( endMark _  ,    true ,    buf )  ;", "}", "}", "METHOD_END"], "methodName": ["readUntilMatchEnd"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "readUntilMatchBegin (  )  ;", "}", "METHOD_END"], "methodName": ["seekNextRecordBoundary"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buf    =    new   byte [ Math . max ( lookAhead _  ,    maxRecSize _  )  ]  ;", "int   read    =     0  ;", "bin _  . mark (  (  ( Math . max ( lookAhead _  ,    maxRecSize _  )  )     +     2  )  )  ;", "read    =    bin _  . read ( buf )  ;", "if    ( read    =  =     (  -  1  )  )", "return   false ;", "String   sbuf    =    new   String ( buf ,     0  ,    read ,     \" UTF -  8  \"  )  ;", "Matcher   match    =    markPattern . matcher ( sbuf )  ;", "firstMatchStart _     =     . NA ;", "firstMatchEnd _     =     . NA ;", "int   bufPos    =     0  ;", "int   state    =     ( synched _  )     ?     . CDATA _ OUT    :     . CDATA _ UNK ;", "int   s    =     0  ;", "while    ( match . find ( bufPos )  )     {", "int   input ;", "if    (  ( match . group (  1  )  )     !  =    null )     {", "input    =     . CDATA _ BEGIN ;", "} else", "if    (  ( match . group (  2  )  )     !  =    null )     {", "input    =     . CDATA _ END ;", "firstMatchStart _     =     . NA ;", "} else    {", "input    =     . RECORD _ MAYBE ;", "}", "if    ( input    =  =     (  . RECORD _ MAYBE )  )     {", "if    (  ( firstMatchStart _  )     =  =     (  . NA )  )     {", "firstMatchStart _     =    match . start (  )  ;", "firstMatchEnd _     =    match . end (  )  ;", "}", "}", "state    =    nextState ( state ,    input ,    match . start (  )  )  ;", "if    ( state    =  =     (  . RECORD _ ACCEPT )  )     {", "break ;", "}", "bufPos    =    match . end (  )  ;", "s +  +  ;", "}", "if    ( state    !  =     (  . CDATA _ UNK )  )     {", "synched _     =    true ;", "}", "boolean   matched    =     (  ( firstMatchStart _  )     !  =     (  . NA )  )     &  &     (  ( state    =  =     (  . RECORD _ ACCEPT )  )     |  |     ( state    =  =     (  . CDATA _ UNK )  )  )  ;", "if    ( matched )     {", "int   endPos    =     ( includePat )     ?    firstMatchEnd _     :    firstMatchStart _  ;", "bin _  . reset (  )  ;", "for    ( long   skiplen    =    endPos ;    skiplen    >     0  ;  )     {", "skiplen    -  =    bin _  . skip ( skiplen )  ;", "}", "pos _     +  =    endPos ;", "if    ( outBufOrNull    !  =    null )     {", "outBufOrNull . writeBytes ( sbuf . substring (  0  ,    endPos )  )  ;", "}", "}", "return   matched ;", "}", "METHOD_END"], "methodName": ["slowReadUntilMatch"], "fileName": "org.apache.hadoop.streaming.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "JobConf   job    =    new   JobConf ( TestAutoInputFormat . conf )  ;", "FileSystem   fs    =    FileSystem . getLocal ( TestAutoInputFormat . conf )  ;", "Path   dir    =    new   Path (  (  ( System . getProperty (  \" test . build . data \"  ,     \"  .  \"  )  )     +     \"  / mapred \"  )  )  ;", "Path   txtFile    =    new   Path ( dir ,     \" auto . txt \"  )  ;", "Path   seqFile    =    new   Path ( dir ,     \" auto . seq \"  )  ;", "fs . delete ( dir ,    true )  ;", "FileInputFormat . setInputPaths ( job ,    dir )  ;", "Writer   txtWriter    =    new   OutputStreamWriter ( fs . create ( txtFile )  )  ;", "try    {", "for    ( int   i    =     0  ;    i    <     ( TestAutoInputFormat . LINES _ COUNT )  ;    i +  +  )     {", "txtWriter . write (  (  \"  \"     +     (  1  0     *    i )  )  )  ;", "txtWriter . write (  \"  \\ n \"  )  ;", "}", "}    finally    {", "txtWriter . close (  )  ;", "}", "SequenceFile . Writer   seqWriter    =    SequenceFile . createWriter ( fs ,    TestAutoInputFormat . conf ,    seqFile ,    IntWritable . class ,    LongWritable . class )  ;", "try    {", "for    ( int   i    =     0  ;    i    <     ( TestAutoInputFormat . RECORDS _ COUNT )  ;    i +  +  )     {", "IntWritable   key    =    new   IntWritable (  (  1  1     *    i )  )  ;", "LongWritable   value    =    new   LongWritable (  (  1  2     *    i )  )  ;", "seqWriter . append ( key ,    value )  ;", "}", "}    finally    {", "seqWriter . close (  )  ;", "}", "AutoInputFormat   format    =    new   AutoInputFormat (  )  ;", "InputSplit [  ]    splits    =    format . getSplits ( job ,    TestAutoInputFormat . SPLITS _ COUNT )  ;", "for    ( InputSplit   split    :    splits )     {", "RecordReader   reader    =    format . getRecordReader ( split ,    job ,    NULL )  ;", "Object   key    =    reader . createKey (  )  ;", "Object   value    =    reader . createValue (  )  ;", "try    {", "while    ( reader . next ( key ,    value )  )     {", "if    ( key   instanceof   LongWritable )     {", "assertEquals (  \" Wrong   value   class .  \"  ,    Text . class ,    value . getClass (  )  )  ;", "assertTrue (  \" Invalid   value \"  ,     (  (  ( Integer . parseInt (  (  ( Text )     ( value )  )  . toString (  )  )  )     %     1  0  )     =  =     0  )  )  ;", "} else    {", "assertEquals (  \" Wrong   key   class .  \"  ,    IntWritable . class ,    key . getClass (  )  )  ;", "assertEquals (  \" Wrong   value   class .  \"  ,    LongWritable . class ,    value . getClass (  )  )  ;", "assertTrue (  \" Invalid   key .  \"  ,     (  (  (  (  ( IntWritable )     ( key )  )  . get (  )  )     %     1  1  )     =  =     0  )  )  ;", "assertTrue (  \" Invalid   value .  \"  ,     (  (  (  (  ( LongWritable )     ( value )  )  . get (  )  )     %     1  2  )     =  =     0  )  )  ;", "}", "}", "}    finally    {", "reader . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFormat"], "fileName": "org.apache.hadoop.streaming.TestAutoInputFormat"}, {"methodBody": ["METHOD_START", "{", "new   TestClassWithNoPackage (  )  . testGoodClassOrNull (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestClassWithNoPackage"}, {"methodBody": ["METHOD_START", "{", "String   NAME    =     \" ClassWithNoPackage \"  ;", "ClassLoader   cl    =     . class . getClassLoader (  )  ;", "String   JAR    =    JarFinder . getJar ( cl . loadClass ( NAME )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClassLoader ( new   URLClassLoader ( new   URL [  ]  {    new   URL (  \" file \"  ,    null ,    JAR )     }  ,    null )  )  ;", "String   defaultPackage    =    this . getClass (  )  . getPackage (  )  . getName (  )  ;", "Class   c    =    StreamUtil . goodClassOrNull ( conf ,    NAME ,    defaultPackage )  ;", "assertNotNull (  (  (  \" Class    \"     +    NAME )     +     \"    not   found !  \"  )  ,    c )  ;", "}", "METHOD_END"], "methodName": ["testGoodClassOrNull"], "fileName": "org.apache.hadoop.streaming.TestClassWithNoPackage"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "MiniDFSCluster   cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "FileSystem   fs    =    cluster . getFileSystem (  )  ;", "PrintStream   psBackup    =    System . out ;", "ByteArrayOutputStream   out    =    new   ByteArrayOutputStream (  )  ;", "PrintStream   psOut    =    new   PrintStream ( out )  ;", "System . setOut ( psOut )  ;", "dumptb    =    new    ( conf )  ;", "try    {", "Path   root    =    new   Path (  \"  / typedbytestest \"  )  ;", "assertTrue ( fs . mkdirs ( root )  )  ;", "assertTrue ( fs . exists ( root )  )  ;", "OutputStreamWriter   writer    =    new   OutputStreamWriter ( fs . create ( new   Path ( root ,     \" test . txt \"  )  )  )  ;", "try    {", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "writer . write (  (  (  \"  \"     +     (  1  0     *    i )  )     +     \"  \\ n \"  )  )  ;", "}", "}    finally    {", "writer . close (  )  ;", "}", "String [  ]    args    =    new   String [  1  ]  ;", "args [  0  ]     =     \"  / typedbytestest \"  ;", "int   ret    =    dumptb . run ( args )  ;", "assertEquals (  \" Return   value    !  =     0  .  \"  ,     0  ,    ret )  ;", "ByteArrayInputStream   in    =    new   ByteArrayInputStream ( out . toByteArray (  )  )  ;", "TypedBytesInput   tbinput    =    new   TypedBytesInput ( new   DataInputStream ( in )  )  ;", "int   counter    =     0  ;", "Object   key    =    tbinput . read (  )  ;", "while    ( key    !  =    null )     {", "assertEquals ( Long . class ,    key . getClass (  )  )  ;", "Object   value    =    tbinput . read (  )  ;", "assertEquals ( String . class ,    value . getClass (  )  )  ;", "assertTrue (  \" Invalid   output .  \"  ,     (  (  ( Integer . parseInt ( value . toString (  )  )  )     %     1  0  )     =  =     0  )  )  ;", "counter +  +  ;", "key    =    tbinput . read (  )  ;", "}", "assertEquals (  \" Wrong   number   of   outputs .  \"  ,     1  0  0  ,    counter )  ;", "}    finally    {", "try    {", "fs . close (  )  ;", "}    catch    ( Exception   e )     {", "}", "System . setOut ( psBackup )  ;", "cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDumping"], "fileName": "org.apache.hadoop.streaming.TestDumpTypedBytes"}, {"methodBody": ["METHOD_START", "{", "GZIPOutputStream   out    =    new   GZIPOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestGzipInput"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "MiniDFSCluster   cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "FileSystem   fs    =    cluster . getFileSystem (  )  ;", "ByteArrayOutputStream   out    =    new   ByteArrayOutputStream (  )  ;", "TypedBytesOutput   tboutput    =    new   TypedBytesOutput ( new   DataOutputStream ( out )  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "tboutput . write ( new   Long ( i )  )  ;", "tboutput . write (  (  \"  \"     +     (  1  0     *    i )  )  )  ;", "}", "InputStream   isBackup    =    System . in ;", "ByteArrayInputStream   in    =    new   ByteArrayInputStream ( out . toByteArray (  )  )  ;", "System . setIn ( in )  ;", "loadtb    =    new    ( conf )  ;", "try    {", "Path   root    =    new   Path (  \"  / typedbytestest \"  )  ;", "assertTrue ( fs . mkdirs ( root )  )  ;", "assertTrue ( fs . exists ( root )  )  ;", "String [  ]    args    =    new   String [  1  ]  ;", "args [  0  ]     =     \"  / typedbytestest / test . seq \"  ;", "int   ret    =    loadtb . run ( args )  ;", "assertEquals (  \" Return   value    !  =     0  .  \"  ,     0  ,    ret )  ;", "Path   file    =    new   Path ( root ,     \" test . seq \"  )  ;", "assertTrue ( fs . exists ( file )  )  ;", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader ( fs ,    file ,    conf )  ;", "int   counter    =     0  ;", "TypedBytesWritable   key    =    new   TypedBytesWritable (  )  ;", "TypedBytesWritable   value    =    new   TypedBytesWritable (  )  ;", "while    ( reader . next ( key ,    value )  )     {", "assertEquals ( Long . class ,    key . getValue (  )  . getClass (  )  )  ;", "assertEquals ( String . class ,    value . getValue (  )  . getClass (  )  )  ;", "assertTrue (  \" Invalid   record .  \"  ,     (  (  ( Integer . parseInt ( value . toString (  )  )  )     %     1  0  )     =  =     0  )  )  ;", "counter +  +  ;", "}", "assertEquals (  \" Wrong   number   of   records .  \"  ,     1  0  0  ,    counter )  ;", "}    finally    {", "try    {", "fs . close (  )  ;", "}    catch    ( Exception   e )     {", "}", "System . setIn ( isBackup )  ;", "cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testLoading"], "fileName": "org.apache.hadoop.streaming.TestLoadTypedBytes"}, {"methodBody": ["METHOD_START", "{", "JobConf   jobConf    =    new   JobConf (  )  ;", "jobConf . set ( JT _ IPC _ ADDRESS ,    LOCAL _ FRAMEWORK _ NAME )  ;", "jobConf . set ( FRAMEWORK _ NAME ,    YARN _ FRAMEWORK _ NAME )  ;", "assertFalse (  \" Expected    ' isLocal '    to   be   false \"  ,    SUtil . isLocalJobTracker ( jobConf )  )  ;", "jobConf . set ( JT _ IPC _ ADDRESS ,    LOCAL _ FRAMEWORK _ NAME )  ;", "jobConf . set ( FRAMEWORK _ NAME ,    CLASSIC _ FRAMEWORK _ NAME )  ;", "assertFalse (  \" Expected    ' isLocal '    to   be   false \"  ,    SUtil . isLocalJobTracker ( jobConf )  )  ;", "jobConf . set ( JT _ IPC _ ADDRESS ,     \" jthost :  9  0  9  0  \"  )  ;", "jobConf . set ( FRAMEWORK _ NAME ,    LOCAL _ FRAMEWORK _ NAME )  ;", "assertTrue (  \" Expected    ' isLocal '    to   be   true \"  ,    SUtil . isLocalJobTracker ( jobConf )  )  ;", "}", "METHOD_END"], "methodName": ["testFramework"], "fileName": "org.apache.hadoop.streaming.TestMRFramework"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   output    =    new   StringBuffer (  2  5  6  )  ;", "Path [  ]    fileList    =    FileUtil . stat 2 Paths ( fileSys . listStatus ( new   Path ( OUTPUT _ DIR )  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( fileList . length )  ;    i +  +  )     {", ". LOG . info (  (  \" Adding   output   from   file :     \"     +     ( fileList [ i ]  )  )  )  ;", "output . append ( StreamUtil . slurpHadoop ( fileList [ i ]  ,    fileSys )  )  ;", "}", "assertOutput ( expectedOutput ,    output . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkOutput"], "fileName": "org.apache.hadoop.streaming.TestMultipleArchiveFiles"}, {"methodBody": ["METHOD_START", "{", "fileSys . delete ( new   Path ( INPUT _ DIR )  ,    true )  ;", "DataOutputStream   dos    =    fileSys . create ( new   Path ( INPUT _ FILE )  )  ;", "String   inputFileString    =     (  (  (  \" symlink 1  \"     +     ( File . separator )  )     +     \" cach 1  \\ nsymlink 2  \"  )     +     ( File . separator )  )     +     \" cach 2  \"  ;", "dos . write ( inputFileString . getBytes (  \" UTF -  8  \"  )  )  ;", "dos . close (  )  ;", "DataOutputStream   out    =    fileSys . create ( new   Path ( CACHE _ ARCHIVE _  1  . toString (  )  )  )  ;", "ZipOutputStream   zos    =    new   ZipOutputStream ( out )  ;", "ZipEntry   ze    =    new   ZipEntry ( CACHE _ FILE _  1  . toString (  )  )  ;", "zos . putNextEntry ( ze )  ;", "zos . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "zos . closeEntry (  )  ;", "zos . close (  )  ;", "out    =    fileSys . create ( new   Path ( CACHE _ ARCHIVE _  2  . toString (  )  )  )  ;", "zos    =    new   ZipOutputStream ( out )  ;", "ze    =    new   ZipEntry ( CACHE _ FILE _  2  . toString (  )  )  ;", "zos . putNextEntry ( ze )  ;", "zos . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "zos . closeEntry (  )  ;", "zos . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestMultipleArchiveFiles"}, {"methodBody": ["METHOD_START", "{", "String   workDir    =     ( fileSys . getWorkingDirectory (  )  . toString (  )  )     +     \"  /  \"  ;", "String   cache 1     =     ( workDir    +     ( CACHE _ ARCHIVE _  1  )  )     +     \"  # symlink 1  \"  ;", "String   cache 2     =     ( workDir    +     ( CACHE _ ARCHIVE _  2  )  )     +     \"  # symlink 2  \"  ;", "for    ( Map . Entry < String ,    String >    entry    :    mr . createJobConf (  )  )     {", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  (  ( entry . getKey (  )  )     +     \"  =  \"  )     +     ( entry . getValue (  )  )  )  )  ;", "}", "args . add (  \"  - jobconf \"  )  ;", "args . add (  \" mapreduce . job . reduces =  1  \"  )  ;", "args . add (  \"  - cacheArchive \"  )  ;", "args . add ( cache 1  )  ;", "args . add (  \"  - cacheArchive \"  )  ;", "args . add ( cache 2  )  ;", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  \" mapred . jar =  \"     +     ( TestSSTREAMING _ JAR )  )  )  ;", "return   super . genArgs (  )  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestMultipleArchiveFiles"}, {"methodBody": ["METHOD_START", "{", "new   TestMultipleCachefiles (  )  . testMultipleCachefiles (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestMultipleCachefiles"}, {"methodBody": ["METHOD_START", "{", "boolean   mayExit    =    false ;", "MiniMRCluster   mr    =    null ;", "MiniDFSCluster   dfs    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "dfs    =    new   MiniDFSCluster . Builder ( conf )  . build (  )  ;", "FileSystem   fileSys    =    dfs . getFileSystem (  )  ;", "String   namenode    =    fileSys . getUri (  )  . toString (  )  ;", "mr    =    new   MiniMRCluster (  1  ,    namenode ,     3  )  ;", "List < String >    args    =    new   ArrayList < String >  (  )  ;", "for    ( Map . Entry < String ,    String >    entry    :    mr . createJobConf (  )  )     {", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  (  ( entry . getKey (  )  )     +     \"  =  \"  )     +     ( entry . getValue (  )  )  )  )  ;", "}", "String [  ]    argv    =    new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE ,     \"  - output \"  ,    OUTPUT _ DIR ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,    reduce ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - jobconf \"  ,     (  (  (  (  (  (  (  ( JobConf . MAPRED _ MAP _ TASK _ JAVA _ OPTS )     +     \"  =  \"  )     +     \"  - Dcontrib . name =  \"  )     +     ( System . getProperty (  \" contrib . name \"  )  )  )     +     \"     \"  )     +     \"  - Dbuild . test =  \"  )     +     ( System . getProperty (  \" build . test \"  )  )  )     +     \"     \"  )     +     ( conf . get ( MAPRED _ MAP _ TASK _ JAVA _ OPTS ,    conf . get ( MAPRED _ TASK _ JAVA _ OPTS ,     \"  \"  )  )  )  ,     \"  - jobconf \"  ,     (  (  (  (  (  (  (  ( JobConf . MAPRED _ REDUCE _ TASK _ JAVA _ OPTS )     +     \"  =  \"  )     +     \"  - Dcontrib . name =  \"  )     +     ( System . getProperty (  \" contrib . name \"  )  )  )     +     \"     \"  )     +     \"  - Dbuild . test =  \"  )     +     ( System . getProperty (  \" build . test \"  )  )  )     +     \"     \"  )     +     ( conf . get ( MAPRED _ REDUCE _ TASK _ JAVA _ OPTS ,    conf . get ( MAPRED _ TASK _ JAVA _ OPTS ,     \"  \"  )  )  )  ,     \"  - cacheFile \"  ,     (  (  ( fileSys . getUri (  )  )     +     ( CACHE _ FILE )  )     +     \"  #  \"  )     +     ( mapString )  ,     \"  - cacheFile \"  ,     (  (  ( fileSys . getUri (  )  )     +     ( CACHE _ FILE _  2  )  )     +     \"  #  \"  )     +     ( mapString 2  )  ,     \"  - jobconf \"  ,     \" mapred . jar =  \"     +     ( TestSSTREAMING _ JAR )     }  ;", "for    ( String   arg    :    argv )     {", "args . add ( arg )  ;", "}", "argv    =    args . toArray ( new   String [ args . size (  )  ]  )  ;", "fileSys . delete ( new   Path ( OUTPUT _ DIR )  ,    true )  ;", "DataOutputStream   file    =    fileSys . create ( new   Path ( INPUT _ FILE )  )  ;", "file . writeBytes (  (  ( mapString )     +     \"  \\ n \"  )  )  ;", "file . writeBytes (  (  ( mapString 2  )     +     \"  \\ n \"  )  )  ;", "file . close (  )  ;", "file    =    fileSys . create ( new   Path ( CACHE _ FILE )  )  ;", "file . writeBytes (  (  ( cacheString )     +     \"  \\ n \"  )  )  ;", "file . close (  )  ;", "file    =    fileSys . create ( new   Path ( CACHE _ FILE _  2  )  )  ;", "file . writeBytes (  (  ( cacheString 2  )     +     \"  \\ n \"  )  )  ;", "file . close (  )  ;", "job    =    new   StreamJob ( argv ,    mayExit )  ;", "job . go (  )  ;", "fileSys    =    dfs . getFileSystem (  )  ;", "String   line    =    null ;", "String   line 2     =    null ;", "Path [  ]    fileList    =    FileUtil . stat 2 Paths ( fileSys . listStatus ( new   Path ( OUTPUT _ DIR )  ,    new   Utils . OutputFileUtils . OutputFilesFilter (  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( fileList . length )  ;    i +  +  )     {", "System . out . println ( fileList [ i ]  . toString (  )  )  ;", "BufferedReader   bread    =    new   BufferedReader ( new   InputStreamReader ( fileSys . open ( fileList [ i ]  )  )  )  ;", "line    =    bread . readLine (  )  ;", "System . out . println ( line )  ;", "line 2     =    bread . readLine (  )  ;", "System . out . println ( line 2  )  ;", "}", "assertEquals (  (  ( cacheString )     +     \"  \\ t \"  )  ,    line )  ;", "assertEquals (  (  ( cacheString 2  )     +     \"  \\ t \"  )  ,    line 2  )  ;", "}    finally    {", "if    ( dfs    !  =    null )     {", "dfs . shutdown (  )  ;", "}", "if    ( mr    !  =    null )     {", "mr . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testMultipleCachefiles"], "fileName": "org.apache.hadoop.streaming.TestMultipleCachefiles"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestRawBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,    reduce ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - jobconf \"  ,     \" stream . map . output = rawbytes \"  ,     \"  - jobconf \"  ,     \" stream . reduce . input = rawbytes \"  ,     \"  - verbose \"     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestRawBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "try    {", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "createInput (  )  ;", "OUTPUT _ DIR . delete (  )  ;", "Job   job    =    new   Job (  )  ;", "job . setConf ( new   Configuration (  )  )  ;", "job . run ( genArgs (  )  )  ;", "File   outFile    =    new   File ( OUTPUT _ DIR ,     \" part -  0  0  0  0  0  \"  )  . getAbsoluteFile (  )  ;", "String   output    =    Util . slurp ( outFile )  ;", "outFile . delete (  )  ;", "System . out . println (  (  \"          map =  \"     +     ( map )  )  )  ;", "System . out . println (  (  \" reduce =  \"     +     ( reduce )  )  )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestRawBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreamAggregate"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,     \" aggregate \"  ,     \"  - jobconf \"  ,     ( MRJobConfig . PRESERVE _ FAILED _ TASK _ FILES )     +     \"  = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamAggregate"}, {"methodBody": ["METHOD_START", "{", "new   TestStreaming (  )  . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestStreamAggregate"}, {"methodBody": ["METHOD_START", "{", "try    {", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "createInput (  )  ;", "boolean   mayExit    =    false ;", "job    =    new   Job ( genArgs (  )  ,    mayExit )  ;", "job . go (  )  ;", "File   outFile    =    new   File ( OUTPUT _ DIR ,     \" part -  0  0  0  0  0  \"  )  . getAbsoluteFile (  )  ;", "String   output    =    Util . slurp ( outFile )  ;", "outFile . delete (  )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreamAggregate"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreamDataProtocol"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,    reduce ,     \"  - partitioner \"  ,    KeyFieldBasedPartitioner . class . getCanonicalName (  )  ,     \"  - jobconf \"  ,     \" stream . map . output . field . separator =  .  \"  ,     \"  - jobconf \"  ,     \" stream . num . map . output . key . fields =  2  \"  ,     \"  - jobconf \"  ,     \" mapreduce . map . output . key . field . separator =  .  \"  ,     \"  - jobconf \"  ,     \" num . key . fields . for . partition =  1  \"  ,     \"  - jobconf \"  ,     \" mapreduce . job . reduces =  2  \"  ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamDataProtocol"}, {"methodBody": ["METHOD_START", "{", "new   TestStreamDataProtocol (  )  . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestStreamDataProtocol"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "try    {", "createInput (  )  ;", "boolean   mayExit    =    false ;", "job    =    new   Job ( genArgs (  )  ,    mayExit )  ;", "job . go (  )  ;", "File   outFile    =    new   File ( OUTPUT _ DIR ,     \" part -  0  0  0  0  0  \"  )  . getAbsoluteFile (  )  ;", "String   output    =    Util . slurp ( outFile )  ;", "outFile . delete (  )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "System . err . println (  (  \"       equals =  \"     +     ( outputExpect . compareTo ( output )  )  )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreamDataProtocol"}, {"methodBody": ["METHOD_START", "{", "JobConf   job ;", "ArrayList < String >    dummyArgs    =    new   ArrayList < String >  (  )  ;", "dummyArgs . add (  \"  - input \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \"  - output \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \"  - mapper \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \"  - reducer \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "ArrayList < String >    args ;", "args    =    new   ArrayList < String >  ( dummyArgs )  ;", "args . add (  \"  - inputformat \"  )  ;", "args . add (  \" mapred . KeyValueTextInputFormat \"  )  ;", "job    =    StreamJob . createJob ( args . toArray ( new   String [  ]  {        }  )  )  ;", "assertEquals ( KeyValueTextInputFormat . class ,    job . getInputFormat (  )  . getClass (  )  )  ;", "args    =    new   ArrayList < String >  ( dummyArgs )  ;", "args . add (  \"  - inputformat \"  )  ;", "args . add (  \" mapred . SequenceFileInputFormat \"  )  ;", "job    =    StreamJob . createJob ( args . toArray ( new   String [  ]  {        }  )  )  ;", "assertEquals ( SequenceFileInputFormat . class ,    job . getInputFormat (  )  . getClass (  )  )  ;", "args    =    new   ArrayList < String >  ( dummyArgs )  ;", "args . add (  \"  - inputformat \"  )  ;", "args . add (  \" mapred . KeyValueTextInputFormat \"  )  ;", "args . add (  \"  - inputreader \"  )  ;", "args . add (  \" StreamXmlRecordReader , begin =  < doc >  , end =  <  / doc >  \"  )  ;", "job    =    StreamJob . createJob ( args . toArray ( new   String [  ]  {        }  )  )  ;", "assertEquals ( StreamInputFormat . class ,    job . getInputFormat (  )  . getClass (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateJob"], "fileName": "org.apache.hadoop.streaming.TestStreamJob"}, {"methodBody": ["METHOD_START", "{", "ArrayList < String >    dummyArgs    =    new   ArrayList < String >  (  )  ;", "dummyArgs . add (  \"  - input \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \"  - output \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \"  - mapper \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", "dummyArgs . add (  \"  - reducer \"  )  ;", "dummyArgs . add (  \" dummy \"  )  ;", ". createJob ( dummyArgs . toArray ( new   String [  ]  {        }  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateJobWithExtraArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamJob"}, {"methodBody": ["METHOD_START", "{", "StreamJob   streamJob    =    new   StreamJob (  )  ;", "assertEquals (  1  ,    streamJob . run ( new   String [  0  ]  )  )  ;", "assertEquals (  0  ,    streamJob . run ( new   String [  ]  {     \"  - help \"     }  )  )  ;", "assertEquals (  0  ,    streamJob . run ( new   String [  ]  {     \"  - info \"     }  )  )  ;", "}", "METHOD_END"], "methodName": ["testOptions"], "fileName": "org.apache.hadoop.streaming.TestStreamJob"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreamReduceNone"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,     \" IdentityReducer \"  ,     \"  - numReduceTasks \"  ,     \"  0  \"  ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" mapreduce . job . maps =  1  \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamReduceNone"}, {"methodBody": ["METHOD_START", "{", "new   TestStreamReduceNone (  )  . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestStreamReduceNone"}, {"methodBody": ["METHOD_START", "{", "String   outFileName    =     \" part -  0  0  0  0  0  \"  ;", "File   outFile    =    null ;", "try    {", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "createInput (  )  ;", "boolean   mayExit    =    false ;", "job    =    new   Job ( genArgs (  )  ,    mayExit )  ;", "job . go (  )  ;", "outFile    =    new   File ( OUTPUT _ DIR ,    outFileName )  . getAbsoluteFile (  )  ;", "String   output    =    Util . slurp ( outFile )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreamReduceNone"}, {"methodBody": ["METHOD_START", "{", "if    ( hasPerl )     {", "blockSize    =     6  0  ;", "isSlowMatch    =     \" false \"  ;", "super . testCommandLine (  )  ;", "} else    {", ". LOG . warn (  \" No   perl ;    skipping   test .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStreamXmlMultiInnerFast"], "fileName": "org.apache.hadoop.streaming.TestStreamXmlMultipleRecords"}, {"methodBody": ["METHOD_START", "{", "if    ( hasPerl )     {", "blockSize    =     6  0  ;", "isSlowMatch    =     \" true \"  ;", "super . testCommandLine (  )  ;", "} else    {", ". LOG . warn (  \" No   perl ;    skipping   test .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStreamXmlMultiInnerSlow"], "fileName": "org.apache.hadoop.streaming.TestStreamXmlMultipleRecords"}, {"methodBody": ["METHOD_START", "{", "if    ( hasPerl )     {", "blockSize    =     8  0  ;", "isSlowMatch    =     \" false \"  ;", "super . testCommandLine (  )  ;", "} else    {", ". LOG . warn (  \" No   perl ;    skipping   test .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStreamXmlMultiOuterFast"], "fileName": "org.apache.hadoop.streaming.TestStreamXmlMultipleRecords"}, {"methodBody": ["METHOD_START", "{", "if    ( hasPerl )     {", "blockSize    =     8  0  ;", "isSlowMatch    =     \" true \"  ;", "super . testCommandLine (  )  ;", "} else    {", ". LOG . warn (  \" No   perl ;    skipping   test .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStreamXmlMultiOuterSlow"], "fileName": "org.apache.hadoop.streaming.TestStreamXmlMultipleRecords"}, {"methodBody": ["METHOD_START", "{", "String [  ]    words    =    expectedOutput . split (  \"  \\ t \\ n \"  )  ;", "Set < String >    expectedWords    =    new   HashSet < String >  ( Arrays . asList ( words )  )  ;", "words    =    output . split (  \"  \\ t \\ n \"  )  ;", "Set < String >    returnedWords    =    new   HashSet < String >  ( Arrays . asList ( words )  )  ;", "assertTrue ( returnedWords . containsAll ( expectedWords )  )  ;", "}", "METHOD_END"], "methodName": ["assertOutput"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "Path   outPath    =    new   Path ( OUTPUT _ DIR . getPath (  )  ,     \" part -  0  0  0  0  0  \"  )  ;", "FileSystem   fs    =    getFileSystem (  )  ;", "String   output    =    Util . slurpHadoop ( outPath ,    fs )  ;", "fs . delete ( outPath ,    true )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( getExpectedOutput (  )  )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertOutput ( getExpectedOutput (  )  ,    output )  ;", "}", "METHOD_END"], "methodName": ["checkOutput"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    getFileSystem (  )  . create ( new   Path ( INPUT _ FILE . getPath (  )  )  )  ;", "out . write ( getInputData (  )  . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "args . add (  \"  - input \"  )  ;", "args . add ( inputFile )  ;", "args . add (  \"  - output \"  )  ;", "args . add ( outDir )  ;", "args . add (  \"  - mapper \"  )  ;", "args . add ( map )  ;", "args . add (  \"  - reducer \"  )  ;", "args . add ( reduce )  ;", "args . add (  \"  - jobconf \"  )  ;", "args . add (  \" mapreduce . task . files . preserve . failedtasks = true \"  )  ;", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  \"  . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  )  )  ;", "String [  ]    str    =    new   String [ args . size (  )  ]  ;", "args . toArray ( str )  ;", "return   str ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "return   new   Configuration (  )  ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "return   outputExpect ;", "}", "METHOD_END"], "methodName": ["getExpectedOutput"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "return   FileSystem . get ( getConf (  )  )  ;", "}", "METHOD_END"], "methodName": ["getFileSystem"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "return   input ;", "}", "METHOD_END"], "methodName": ["getInputData"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "setInputOutput (  )  ;", "createInput (  )  ;", "boolean   mayExit    =    false ;", "job    =    new   Job ( genArgs (  )  ,    mayExit )  ;", "return   job . go (  )  ;", "}", "METHOD_END"], "methodName": ["runStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "inputFile    =    INPUT _ FILE . getPath (  )  ;", "outDir    =    OUTPUT _ DIR . getPath (  )  ;", "}", "METHOD_END"], "methodName": ["setInputOutput"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "TEST _ DIR    =    testDir ;", "OUTPUT _ DIR    =    new   File ( testDir ,     \" out \"  )  ;", "INPUT _ FILE    =    new   File ( testDir ,     \" input . txt \"  )  ;", "}", "METHOD_END"], "methodName": ["setTestDir"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "UtilTest . recursiveDelete ( TEST _ DIR )  ;", "assertTrue (  (  \" Creating    \"     +     ( TEST _ DIR )  )  ,    TEST _ DIR . mkdirs (  )  )  ;", "args . clear (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "UtilTest . recursiveDelete ( TEST _ DIR )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "int   ret    =    runStreamJob (  )  ;", "assertEquals (  0  ,    ret )  ;", "checkOutput (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreaming"}, {"methodBody": ["METHOD_START", "{", "boolean   mayExit    =    false ;", "int   returnStatus    =     0  ;", "StreamJob   job    =    new   StreamJob ( args ,    mayExit )  ;", "returnStatus    =    job . go (  )  ;", "Assert . assertEquals (  \"    Job   expected   to   succeed \"  ,     0  ,    returnStatus )  ;", "job . running _  . killJob (  )  ;", "job . running _  . waitForCompletion (  )  ;", "}", "METHOD_END"], "methodName": ["runStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreamingBackground"}, {"methodBody": ["METHOD_START", "{", "UtilTest . recursiveDelete ( TEST _ DIR )  ;", "Assert . assertTrue ( TEST _ DIR . mkdirs (  )  )  ;", "FileOutpu   out    =    new   FileOutpu ( INPUT _ FILE . getAbsoluteFile (  )  )  ;", "out . write (  \" hello \\ n \"  . getBytes (  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.streaming.TestStreamingBackground"}, {"methodBody": ["METHOD_START", "{", "runStreamJob (  )  ;", "}", "METHOD_END"], "methodName": ["testBackgroundSubmitOk"], "fileName": "org.apache.hadoop.streaming.TestStreamingBackground"}, {"methodBody": ["METHOD_START", "{", "OutputStream   os    =    getFileSystem (  )  . create ( new   Path ( getInputDir (  )  ,     \" text . txt \"  )  )  ;", "Writer   wr    =    new   OutputStreamWriter ( os )  ;", "String   prefix    =    new   String ( new   byte [  2  0     *     1  0  2  4  ]  )  ;", "for    ( int   i    =     1  ;    i    <  =     (  . INPUTSIZE )  ;    i +  +  )     {", "String   str    =     \"  \"     +    i ;", "int   zerosToPrepend    =     3     -     ( str . length (  )  )  ;", "for    ( int   j    =     0  ;    j    <    zerosToPrepend ;    j +  +  )     {", "str    =     \"  0  \"     +    str ;", "}", "wr . write (  (  (  ( prefix    +     \" hey \"  )     +    str )     +     \"  \\ n \"  )  )  ;", "}", "wr . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreamingBadRecords"}, {"methodBody": ["METHOD_START", "{", "Properties   props    =    new   Properties (  )  ;", "props . setProperty ( JT _ RETIREJOBS ,     \" false \"  )  ;", "props . setProperty ( JT _ PERSIST _ JOBSTATUS ,     \" false \"  )  ;", "startCluster ( true ,    props )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.streaming.TestStreamingBadRecords"}, {"methodBody": ["METHOD_START", "{", "TestStreamingBadRecords . LOG . info ( runningJob . getCounters (  )  . toString (  )  )  ;", "assertTrue ( runningJob . isSuccessful (  )  )  ;", "if    ( validateCount )     {", "String   counterGrp    =     \" Task $ Counter \"  ;", "Counters   counters    =    runningJob . getCounters (  )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" MAP _ SKIPPED _ RECORDS \"  )  . getCounter (  )  ,    TestStreamingBadRecords . MAPPER _ BAD _ RECORDS . size (  )  )  ;", "int   mapRecs    =     ( TestStreamingBadRecords . INPUTSIZE )     -     ( TestStreamingBadRecords . MAPPER _ BAD _ RECORDS . size (  )  )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" MAP _ INPUT _ RECORDS \"  )  . getCounter (  )  ,    mapRecs )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" MAP _ OUTPUT _ RECORDS \"  )  . getCounter (  )  ,    mapRecs )  ;", "int   redRecs    =    mapRecs    -     ( TestStreamingBadRecords . REDUCER _ BAD _ RECORDS . size (  )  )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" REDUCE _ SKIPPED _ RECORDS \"  )  . getCounter (  )  ,    TestStreamingBadRecords . REDUCER _ BAD _ RECORDS . size (  )  )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" REDUCE _ SKIPPED _ GROUPS \"  )  . getCounter (  )  ,    TestStreamingBadRecords . REDUCER _ BAD _ RECORDS . size (  )  )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" REDUCE _ INPUT _ GROUPS \"  )  . getCounter (  )  ,    redRecs )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" REDUCE _ INPUT _ RECORDS \"  )  . getCounter (  )  ,    redRecs )  ;", "assertEquals ( counters . findCounter ( counterGrp ,     \" REDUCE _ OUTPUT _ RECORDS \"  )  . getCounter (  )  ,    redRecs )  ;", "}", "List < String >    badRecs    =    new   ArrayList < String >  (  )  ;", "badRecs . addAll ( TestStreamingBadRecords . MAPPER _ BAD _ RECORDS )  ;", "badRecs . addAll ( TestStreamingBadRecords . REDUCER _ BAD _ RECORDS )  ;", "Path [  ]    outputFiles    =    FileUtil . stat 2 Paths ( getFileSystem (  )  . listStatus ( getOutputDir (  )  ,    new   Utils . OutputFileUtils . OutputFilesFilter (  )  )  )  ;", "if    (  ( outputFiles . length )     >     0  )     {", "InputStream   is    =    getFileSystem (  )  . open ( outputFiles [  0  ]  )  ;", "BufferedReader   reader    =    new   BufferedReader ( new   InputStreamReader ( is )  )  ;", "String   line    =    reader . readLine (  )  ;", "int   counter    =     0  ;", "while    ( line    !  =    null )     {", "counter +  +  ;", "StringTokenizer   tokeniz    =    new   StringTokenizer ( line ,     \"  \\ t \"  )  ;", "String   value    =    tokeniz . nextToken (  )  ;", "int   index    =    value . indexOf (  \" hey \"  )  ;", "assertTrue (  ( index    >     (  -  1  )  )  )  ;", "if    ( index    >     (  -  1  )  )     {", "String   heyStr    =    value . substring ( index )  ;", "assertTrue (  (  !  ( badRecs . contains ( heyStr )  )  )  )  ;", "}", "line    =    reader . readLine (  )  ;", "}", "reader . close (  )  ;", "if    ( validateCount )     {", "assertEquals (  (  ( TestStreamingBadRecords . INPUTSIZE )     -     ( badRecs . size (  )  )  )  ,    counter )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateOutput"], "fileName": "org.apache.hadoop.streaming.TestStreamingBadRecords"}, {"methodBody": ["METHOD_START", "{", "args . add (  \"  - combiner \"  )  ;", "args . add ( combine )  ;", "return   super . genArgs (  )  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamingCombiner"}, {"methodBody": ["METHOD_START", "{", "super . testCommandLine (  )  ;", "String   counterGrp    =     \" mapred . Task $ Counter \"  ;", "Counters   counters    =    job . running _  . getCounters (  )  ;", "assertTrue (  (  ( counters . findCounter ( counterGrp ,     \" COMBINE _ INPUT _ RECORDS \"  )  . getValue (  )  )     !  =     0  )  )  ;", "assertTrue (  (  ( counters . findCounter ( counterGrp ,     \" COMBINE _ OUTPUT _ RECORDS \"  )  . getValue (  )  )     !  =     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreamingCombiner"}, {"methodBody": ["METHOD_START", "{", "super . testCommandLine (  )  ;", "validate (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreamingCounters"}, {"methodBody": ["METHOD_START", "{", "Counters   counters    =    job . running _  . getCounters (  )  ;", "assertNotNull (  \" Counters \"  ,    counters )  ;", "Group   group    =    counters . getGroup (  \" UserCounters \"  )  ;", "assertNotNull (  \" Group \"  ,    group )  ;", "Counter   counter    =    group . getCounterForName (  \" InputLines \"  )  ;", "assertNotNull (  \" Counter \"  ,    counter )  ;", "assertEquals (  3  ,    counter . getCounter (  )  )  ;", "}", "METHOD_END"], "methodName": ["validateCounters"], "fileName": "org.apache.hadoop.streaming.TestStreamingCounters"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    failMap    ?    failingTask    :    echoTask ,     \"  - reducer \"  ,    failMap    ?    echoTask    :    failingTask ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" stream . non . zero . exit . is . failure =  \"     +    exitStatusIsFailure ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - jobconf \"  ,     \" mapreduce . task . io . sort . mb =  1  0  \"     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "boolean   mayExit    =    false ;", "int   returnStatus    =     0  ;", "StreamJob   job    =    new   StreamJob ( genArgs ( eIsFailure ,    failMap )  ,    mayExit )  ;", "returnStatus    =    job . go (  )  ;", "if    ( eIsFailure )     {", "assertEquals (  \" Streaming   Job   failure   code   expected \"  ,     1  ,    returnStatus )  ;", "} else    {", "assertEquals (  \" Streaming   Job   expected   to   succeed \"  ,     0  ,    returnStatus )  ;", "}", "}", "METHOD_END"], "methodName": ["runStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "UtilTest . recursiveDelete ( TEST _ DIR )  ;", "assertTrue ( TEST _ DIR . mkdirs (  )  )  ;", "FileOutpu   out    =    new   FileOutpu ( INPUT _ FILE . getAbsoluteFile (  )  )  ;", "out . write (  \" hello \\ n \"  . getBytes (  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "runStreamJob ( true ,    true )  ;", "}", "METHOD_END"], "methodName": ["testMapFailNotOk"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "runStreamJob ( false ,    true )  ;", "}", "METHOD_END"], "methodName": ["testMapFailOk"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "runStreamJob ( true ,    false )  ;", "}", "METHOD_END"], "methodName": ["testReduceFailNotOk"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "runStreamJob ( false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testReduceFailOk"], "fileName": "org.apache.hadoop.streaming.TestStreamingExitStatus"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreamingKeyValue"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    TestStreaming . CAT ,     \"  - jobconf \"  ,     ( MRJobConfig . PRESERVE _ FAILED _ TASK _ FILES )     +     \"  = true \"  ,     \"  - jobconf \"  ,     \" stream . non . zero . exit . is . failure = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - jobconf \"  ,     \" stream . map . input . ignoreKey =  \"     +    ignoreKey    }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamingKeyValue"}, {"methodBody": ["METHOD_START", "{", "new   TestStreamingKeyValue (  )  . testCommandLineWithKey (  )  ;", "new   TestStreamingKeyValue (  )  . testCommandLineWithoutKey (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestStreamingKeyValue"}, {"methodBody": ["METHOD_START", "{", "String   outFileName    =     \" part -  0  0  0  0  0  \"  ;", "File   outFile    =    null ;", "try    {", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "createInput (  )  ;", "boolean   mayExit    =    false ;", "job    =    new   Job ( genArgs ( ignoreKey )  ,    mayExit )  ;", "job . go (  )  ;", "outFile    =    new   File ( OUTPUT _ DIR ,    outFileName )  . getAbsoluteFile (  )  ;", "String   output    =    Util . slurp ( outFile )  ;", "System . err . println (  (  \" outEx 1  =  \"     +    outputExpect )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["runStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreamingKeyValue"}, {"methodBody": ["METHOD_START", "{", "runStreamJob ( outputWithKey ,    false )  ;", "}", "METHOD_END"], "methodName": ["testCommandLineWithKey"], "fileName": "org.apache.hadoop.streaming.TestStreamingKeyValue"}, {"methodBody": ["METHOD_START", "{", "runStreamJob ( outputWithoutKey ,    true )  ;", "}", "METHOD_END"], "methodName": ["testCommandLineWithoutKey"], "fileName": "org.apache.hadoop.streaming.TestStreamingKeyValue"}, {"methodBody": ["METHOD_START", "{", "map    =    TestStreaming . CAT ;", "reduce    =    TestStreaming . CAT ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandMapperAndCommandReducer"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =    TestStreaming . CAT ;", "reduce    =    TestStreaming . CAT ;", "args . add (  \"  - numReduceTasks \"  )  ;", "args . add (  \"  0  \"  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandMapperAndCommandReducerAndZeroReduces"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =    TestStreaming . CAT ;", "reduce    =     . MyReducer . class . getName (  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandMapperAndJavaReducer"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =    TestStreaming . CAT ;", "reduce    =     . MyReducer . class . getName (  )  ;", "args . add (  \"  - numReduceTasks \"  )  ;", "args . add (  \"  0  \"  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandMapperAndJavaReducerAndZeroReduces"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =    TestStreaming . CAT ;", "reduce    =     \" NONE \"  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testCommandMapperWithReduceNone"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "args . add (  \"  - mapper \"  )  ;", "args . add ( map )  ;", "args . add (  \"  - jobconf \"  )  ;", "args . add (  \" mapreduce . task . files . preserve . failedtasks = true \"  )  ;", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  \"  . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  )  )  ;", "args . add (  \"  - inputformat \"  )  ;", "args . add ( TextInputFormat . class . getName (  )  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultToIdentityReducer"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =     \" IdentityMapper \"  ;", "reduce    =    TestStreaming . CAT ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testJavaMapperAndCommandReducer"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =     \" IdentityMapper \"  ;", "reduce    =    TestStreaming . CAT ;", "args . add (  \"  - numReduceTasks \"  )  ;", "args . add (  \"  0  \"  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testJavaMapperAndCommandReducerAndZeroReduces"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =     \" IdentityMapper \"  ;", "reduce    =     \" IdentityReducer \"  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testJavaMapperAndJavaReducer"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =     \" IdentityMapper \"  ;", "reduce    =     \" IdentityReducer \"  ;", "args . add (  \"  - numReduceTasks \"  )  ;", "args . add (  \"  0  \"  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testJavaMapperAndJavaReducerAndZeroReduces"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "map    =     \" IdentityMapper \"  ;", "reduce    =     \" NONE \"  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testJavaMapperWithReduceNone"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputKeyValueTypes"}, {"methodBody": ["METHOD_START", "{", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  \"  . reduce . input \"     +     \"  = keyonlytext \"  )  )  ;", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  \"  . reduce . output \"     +     \"  = keyonlytext \"  )  )  ;", "super . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["testOutputOnlyKeys"], "fileName": "org.apache.hadoop.streaming.TestStreamingOutputOnlyKeys"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestStreamingSeparator"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,    reduce ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - inputformat \"  ,     \" KeyValueTextInputFormat \"  ,     \"  - jobconf \"  ,     \" mapreduce . input . keyvaluelinerecordreader . key . value . separator =  1  \"  ,     \"  - jobconf \"  ,     \" stream . map . input . field . separator =  2  \"  ,     \"  - jobconf \"  ,     \" stream . map . output . field . separator =  3  \"  ,     \"  - jobconf \"  ,     \" stream . reduce . input . field . separator =  3  \"  ,     \"  - jobconf \"  ,     \" stream . reduce . output . field . separator =  4  \"  ,     \"  - jobconf \"  ,     \" mapreduce . output . textoutputformat . separator =  5  \"     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamingSeparator"}, {"methodBody": ["METHOD_START", "{", "new   TestStreamingSeparator (  )  . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestStreamingSeparator"}, {"methodBody": ["METHOD_START", "{", "try    {", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "createInput (  )  ;", "boolean   mayExit    =    false ;", "job    =    new   Job ( genArgs (  )  ,    mayExit )  ;", "job . go (  )  ;", "File   outFile    =    new   File ( OUTPUT _ DIR ,     \" part -  0  0  0  0  0  \"  )  . getAbsoluteFile (  )  ;", "String   output    =    Util . slurp ( outFile )  ;", "outFile . delete (  )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestStreamingSeparator"}, {"methodBody": ["METHOD_START", "{", "if    (  ( expectedOutput )     =  =    null )     {", "expectedOutput    =     \"  \"  ;", "for    ( int   i    =     1  5  0  0  ;    i    >  =     1  ;    i -  -  )     {", "expectedOutput    =    expectedOutput . concat (  (  ( Integer . toStr ( i )  )     +     \"     \"  )  )  ;", "}", "expectedOutput    =    expectedOutput . trim (  )  ;", "}", "}", "METHOD_END"], "methodName": ["buildExpectedJobOutput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "deleteOutDir ( fs )  ;", "try    {", "Path   file    =    new   Path ( INPUT _ FILE )  ;", "if    ( fs . exists ( file )  )     {", "fs . delete ( file ,    false )  ;", "}", "file    =    new   Path ( scriptFile )  ;", "if    ( fs . exists ( file )  )     {", "fs . delete ( file ,    false )  ;", "}", "}    catch    ( Exception   e )     {", "e . prinackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["clean"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "makeInput ( fs ,     ( isEmptyInput    ?     \"  \"     :    input )  )  ;", "DataOutpu   file    =    fs . create ( new   Path ( scriptFileName )  )  ;", "file . writeBytes ( script )  ;", "file . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInputAndScript"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   outDir    =    new   Path ( OUTPUT _ DIR )  ;", "fs . delete ( outDir ,    true )  ;", "}    catch    ( Exception   e )     {", "}", "}", "METHOD_END"], "methodName": ["deleteOutDir"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE ,     \"  - output \"  ,    OUTPUT _ DIR ,     \"  - mapper \"  ,    mapper ,     \"  - reducer \"  ,    reducer ,     \"  - jobconf \"  ,     ( MRJobConfig . NUM _ MAPS )     +     \"  =  1  \"  ,     \"  - jobconf \"  ,     ( MRJobConfig . NUM _ REDUCES )     +     \"  =  1  \"  ,     \"  - jobconf \"  ,     ( MRJobConfig . PRESERVE _ FAILED _ TASK _ FILES )     +     \"  = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( new   Path ( TestStreamingStatus . TEST _ ROOT _ DIR )  . toUri (  )  . getPath (  )  )  ,     \"  - jobconf \"  ,     (  ( JTConfig . JT _ IPC _ ADDRESS )     +     \"  =  \"  )     +    jobtracker ,     \"  - jobconf \"  ,     \" fs . default . name = file :  /  /  /  \"  ,     \"  - jobconf \"  ,     \" mapred . jar =  \"     +     ( TestStreaming . STREAMING _ JAR )  ,     \"  - jobconf \"  ,     \" mapreduce . framework . name = yarn \"     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "Path   inFile    =    new   Path ( INPUT _ FILE )  ;", "DataOutpu   file    =    fs . create ( inFile )  ;", "file . writeBytes ( input )  ;", "file . close (  )  ;", "}", "METHOD_END"], "methodName": ["makeInput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "boolean   mayExit    =    false ;", "Job   job    =    new   Job ( genArgs ( mr . createJobConf (  )  . get ( JT _ IPC _ ADDRESS )  ,    map ,    reduce )  ,    mayExit )  ;", "int   returnValue    =    job . go (  )  ;", "assertEquals (  0  ,    returnValue )  ;", "int   expectedCounterValue    =     0  ;", "if    (  ( type    =  =     ( TaskType . MAP )  )     |  |     (  ! isEmptyInput )  )     {", "validateTaskStatus ( job ,    type )  ;", "validateJobOutput ( job . getConf (  )  )  ;", "expectedCounterValue    =     2  ;", "}", "validateUserCounter ( job ,    expectedCounterValue )  ;", "validateTaskStderr ( job ,    type )  ;", "deleteOutDir ( fs )  ;", "}", "METHOD_END"], "methodName": ["runStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   JobConf (  )  ;", "conf . setBoolean ( JT _ RETIREJOBS ,    false )  ;", "conf . setBoolean ( JT _ PERSIST _ JOBSTATUS ,    false )  ;", "mr    =    new   mapred . MiniMRCluster (  1  ,     \" file :  /  /  /  \"  ,     3  ,    null ,    null ,    conf )  ;", "Path   inFile    =    new   Path ( INPUT _ FILE )  ;", "fs    =    inFile . getFileSystem ( mr . createJobConf (  )  )  ;", "clean ( fs )  ;", "buildExpectedJobOutput (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "if    (  ( fs )     !  =    null )     {", "cln ( fs )  ;", "}", "if    (  ( mr )     !  =    null )     {", "mr . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "testStreamJob ( false )  ;", "testStreamJob ( true )  ;", "}", "METHOD_END"], "methodName": ["testReporting"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "createInputAndScript ( isEmptyInput ,    script )  ;", "map    =    scriptFileName ;", "reduce    =     \"  / bin / cat \"  ;", "runJob ( MAP ,    isEmptyInput )  ;", "deleteOutDir ( fs )  ;", "map    =     \"  / bin / cat \"  ;", "reduce    =    scriptFileName ;", "runJob ( REDUCE ,    isEmptyInput )  ;", "clean ( fs )  ;", "}", "METHOD_END"], "methodName": ["testStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "String   output    =    MapReduceTestUtil . readOutput ( new   Path ( OUTPUT _ DIR )  ,    conf )  . trim (  )  ;", "assertTrue ( output . equals ( expectedOutput )  )  ;", "}", "METHOD_END"], "methodName": ["validateJobOutput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "String   finalPhaseInTask ;", "TaskReport [  ]    reports ;", "if    ( type    =  =     ( TaskType . MAP )  )     {", "reports    =    job . jc _  . getMapTaskReports ( job . jobId _  )  ;", "finalPhaseInTask    =     \" sort \"  ;", "} else    {", "reports    =    job . jc _  . getReduceTaskReports ( job . jobId _  )  ;", "finalPhaseInTask    =     \" reduce \"  ;", "}", "assertEquals (  1  ,    reports . length )  ;", "assertEquals (  (  (  ( expected )     +     \"     >     \"  )     +    finalPhaseInTask )  ,    reports [  0  ]  . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["validateTaskStatus"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptID   attemptId    =    new   TaskAttemptID ( new   TaskID ( job . jobId _  ,    type ,     0  )  ,     0  )  ;", "String   log    =    MapReduceTestUtil . readTaskLog ( STDERR ,    attemptId ,    false )  ;", "assertTrue ( log . equals ( expectedStderr . trim (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["validateTaskStderr"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "Counters   counters    =    job . running _  . getCounters (  )  ;", "assertEquals ( expectedCounterValue ,    counters . findCounter (  \" myOwnCounterGroup \"  ,     \" myOwnCounter \"  )  . getValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["validateUserCounter"], "fileName": "org.apache.hadoop.streaming.TestStreamingStatus"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    input . getAbsolutePath (  )  ,     \"  - output \"  ,    output . getAbsolutePath (  )  ,     \"  - mapper \"  ,    UtilTest . makeJavaCommand ( StderrApp . class ,    new   String [  ]  {    Integer . toString ( preLines )  ,    Integer . toString ( duringLines )  ,    Integer . toString ( postLines )     }  )  ,     \"  - reducer \"  ,    StreamJob . REDUCE _ NONE ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" mapreduce . task . timeout =  5  0  0  0  \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "File   input    =    setupInput ( baseName ,    hasInput )  ;", "File   output    =    setupOutput ( baseName )  ;", "boolean   mayExit    =    false ;", "int   returnStatus    =     0  ;", "Job   job    =    new   Job ( genArgs ( input ,    output ,    preLines ,    duringLines ,    postLines )  ,    mayExit )  ;", "returnStatus    =    job . go (  )  ;", "assertEquals (  \" Job   success \"  ,     0  ,    returnStatus )  ;", "}", "METHOD_END"], "methodName": ["runStreamJob"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "File   input    =    new   File (  ( base    +     \"  - input . txt \"  )  )  ;", "UtilTest . recursiveDelete ( input )  ;", "FileOutpu   in    =    new   FileOutpu ( input . getAbsoluteFile (  )  )  ;", "if    ( hasInput )     {", "in . write (  \" hello \\ n \"  . getBytes (  )  )  ;", "}", "in . close (  )  ;", "return   input ;", "}", "METHOD_END"], "methodName": ["setupInput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "File   output    =    new   File (  ( base    +     \"  - out \"  )  )  ;", "Util . recursiveDelete ( output )  ;", "return   output ;", "}", "METHOD_END"], "methodName": ["setupOutput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "runStreamJob (  \" target / stderr - post \"  ,    false ,     0  ,     0  ,     1  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testStderrAfterOutput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "runStreamJob (  \" target / stderr - progress \"  ,    true ,     1  0  ,     1  0  0  0  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testStderrCountsAsProgress"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "runStreamJob (  \" target / stderr - pre \"  ,    false ,     1  0  0  0  0  ,     0  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testStderrNoInput"], "fileName": "org.apache.hadoop.streaming.TestStreamingStderr"}, {"methodBody": ["METHOD_START", "{", "new   TestStreaming (  )  . testCommandLine (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TestSymLink"}, {"methodBody": ["METHOD_START", "{", "boolean   mayExit    =    false ;", "MiniMRCluster   mr    =    null ;", "MiniDFSCluster   dfs    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "dfs    =    new   MiniDFSCluster . Builder ( conf )  . build (  )  ;", "FileSystem   fileSys    =    dfs . getFileSystem (  )  ;", "String   namenode    =    fileSys . getUri (  )  . toString (  )  ;", "mr    =    new   MiniMRCluster (  1  ,    namenode ,     3  )  ;", "List < String >    args    =    new   ArrayList < String >  (  )  ;", "for    ( Map . Entry < String ,    String >    entry    :    mr . createJobConf (  )  )     {", "args . add (  \"  - jobconf \"  )  ;", "args . add (  (  (  ( entry . getKey (  )  )     +     \"  =  \"  )     +     ( entry . getValue (  )  )  )  )  ;", "}", "String [  ]    argv    =    new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE ,     \"  - output \"  ,    OUTPUT _ DIR ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,    reduce ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - jobconf \"  ,     (  (  (  (  (  (  (  ( JobConf . MAPRED _ MAP _ TASK _ JAVA _ OPTS )     +     \"  =  \"  )     +     \"  - Dcontrib . name =  \"  )     +     ( System . getProperty (  \" contrib . name \"  )  )  )     +     \"     \"  )     +     \"  - Dbuild . test =  \"  )     +     ( System . getProperty (  \" build . test \"  )  )  )     +     \"     \"  )     +     ( conf . get ( MAPRED _ MAP _ TASK _ JAVA _ OPTS ,    conf . get ( MAPRED _ TASK _ JAVA _ OPTS ,     \"  \"  )  )  )  ,     \"  - jobconf \"  ,     (  (  (  (  (  (  (  ( JobConf . MAPRED _ REDUCE _ TASK _ JAVA _ OPTS )     +     \"  =  \"  )     +     \"  - Dcontrib . name =  \"  )     +     ( System . getProperty (  \" contrib . name \"  )  )  )     +     \"     \"  )     +     \"  - Dbuild . test =  \"  )     +     ( System . getProperty (  \" build . test \"  )  )  )     +     \"     \"  )     +     ( conf . get ( MAPRED _ REDUCE _ TASK _ JAVA _ OPTS ,    conf . get ( MAPRED _ TASK _ JAVA _ OPTS ,     \"  \"  )  )  )  ,     \"  - cacheFile \"  ,     (  ( fileSys . getUri (  )  )     +     ( CACHE _ FILE )  )     +     \"  # testlink \"  ,     \"  - jobconf \"  ,     \" mapred . jar =  \"     +     ( TestSSTREAMING _ JAR )     }  ;", "for    ( String   arg    :    argv )     {", "args . add ( arg )  ;", "}", "argv    =    args . toArray ( new   String [ args . size (  )  ]  )  ;", "fileSys . delete ( new   Path ( OUTPUT _ DIR )  ,    true )  ;", "DataOutputStream   file    =    fileSys . create ( new   Path ( INPUT _ FILE )  )  ;", "file . writeBytes ( mapString )  ;", "file . close (  )  ;", "file    =    fileSys . create ( new   Path ( CACHE _ FILE )  )  ;", "file . writeBytes ( cacheString )  ;", "file . close (  )  ;", "job    =    new   StreamJob ( argv ,    mayExit )  ;", "job . go (  )  ;", "fileSys    =    dfs . getFileSystem (  )  ;", "String   line    =    null ;", "Path [  ]    fileList    =    FileUtil . stat 2 Paths ( fileSys . listStatus ( new   Path ( OUTPUT _ DIR )  ,    new   Utils . OutputFileUtils . OutputFilesFilter (  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( fileList . length )  ;    i +  +  )     {", "System . out . println ( fileList [ i ]  . toString (  )  )  ;", "BufferedReader   bread    =    new   BufferedReader ( new   InputStreamReader ( fileSys . open ( fileList [ i ]  )  )  )  ;", "line    =    bread . readLine (  )  ;", "System . out . println ( line )  ;", "}", "assertEquals (  (  ( cacheString )     +     \"  \\ t \"  )  ,    line )  ;", "}    finally    {", "if    ( dfs    !  =    null )     {", "dfs . shutdown (  )  ;", "}", "if    ( mr    !  =    null )     {", "mr . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testSymLink"], "fileName": "org.apache.hadoop.streaming.TestSymLink"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "INPUT _ FILE . delete (  )  ;", "cteInput (  )  ;", "}", "METHOD_END"], "methodName": ["cleanupOutput"], "fileName": "org.apache.hadoop.streaming.TestTypedBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestTypedBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,    reduce ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  ,     \"  - io \"  ,     \" typedbytes \"     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestTypedBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "StreamJob   job    =    new   StreamJob (  )  ;", "job . setConf ( new   Configuration (  )  )  ;", "job . run ( genArgs (  )  )  ;", "File   outFile    =    new   File ( OUTPUT _ DIR ,     \" part -  0  0  0  0  0  \"  )  . getAbsoluteFile (  )  ;", "String   output    =    StreamUtil . slurp ( outFile )  ;", "outFile . delete (  )  ;", "System . out . println (  (  \"          map =  \"     +     ( map )  )  )  ;", "System . out . println (  (  \" reduce =  \"     +     ( reduce )  )  )  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertEquals ( outputExpect ,    output )  ;", "}", "METHOD_END"], "methodName": ["testCommandLine"], "fileName": "org.apache.hadoop.streaming.TestTypedBytesStreaming"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   out    =    new   DataOutputStream ( new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  0  0  ;     +  + i )     {", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "}", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.TestUnconsumedInput"}, {"methodBody": ["METHOD_START", "{", "return   new   String [  ]  {     \"  - input \"  ,    INPUT _ FILE . getAbsolutePath (  )  ,     \"  - output \"  ,    OUTPUT _ DIR . getAbsolutePath (  )  ,     \"  - mapper \"  ,    map ,     \"  - reducer \"  ,     \" IdentityReducer \"  ,     \"  - numReduceTasks \"  ,     \"  0  \"  ,     \"  - jobconf \"  ,     \" mapreduce . task . files . preserve . failedtasks = true \"  ,     \"  - jobconf \"  ,     \" stream . tmpdir =  \"     +     ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )     }  ;", "}", "METHOD_END"], "methodName": ["genArgs"], "fileName": "org.apache.hadoop.streaming.TestUnconsumedInput"}, {"methodBody": ["METHOD_START", "{", "String   outFileName    =     \" part -  0  0  0  0  0  \"  ;", "File   outFile    =    null ;", "try    {", "try    {", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}    catch    ( Exception   e )     {", "}", "createInput (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  \"  . minRecWrittenToEnableSkip _  \"  ,     \"  0  \"  )  ;", "job    =    new   StreamJob (  )  ;", "job . setConf ( conf )  ;", "int   exitCode    =    job . run ( genArgs (  )  )  ;", "assertEquals (  \" Job   failed \"  ,     0  ,    exitCode )  ;", "outFile    =    new   File ( OUTPUT _ DIR ,    outFileName )  . getAbsoluteFile (  )  ;", "String   output    =    StreamUtil . slurp ( outFile )  ;", "assertEquals (  \" Output   was   truncated \"  ,    EXPECTED _ OUTPUT _ SIZE ,    StringUtils . countMatches ( output ,     \"  \\ t \"  )  )  ;", "}    finally    {", "INPUT _ FILE . delete (  )  ;", "FileUtil . fullyDelete ( OUTPUT _ DIR . getAbsoluteFile (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUnconsumedInput"], "fileName": "org.apache.hadoop.streaming.TestUnconsumedInput"}, {"methodBody": ["METHOD_START", "{", "if    ( s . equals (  \"  \\  \\ n \"  )  )     {", "turn    \"  \\ n \"  ;", "} else    {", "turn   s ;", "}", "}", "METHOD_END"], "methodName": ["CUnescape"], "fileName": "org.apache.hadoop.streaming.TrApp"}, {"methodBody": ["METHOD_START", "{", "String   got    =    env . getProperty ( evName )  ;", "if    (  !  ( evVal . equals ( got )  )  )     {", "String   msg    =     (  (  (  (  \" FAIL   evName =  \"     +    evName )     +     \"    got =  \"  )     +    got )     +     \"    expect =  \"  )     +    evVal ;", "throw   new   IOException ( msg )  ;", "}", "}", "METHOD_END"], "methodName": ["expect"], "fileName": "org.apache.hadoop.streaming.TrApp"}, {"methodBody": ["METHOD_START", "{", "String   got    =    env . getProperty ( evName )  ;", "if    ( got    =  =    null )     {", "String   msg    =     (  \" FAIL   evName =  \"     +    evName )     +     \"    is   undefined .    Expect   defined .  \"  ;", "throw   new   IOException ( msg )  ;", "}", "}", "METHOD_END"], "methodName": ["expectDefined"], "fileName": "org.apache.hadoop.streaming.TrApp"}, {"methodBody": ["METHOD_START", "{", "testParentJobConfToEnvVars (  )  ;", "BufferedReader   in    =    new   BufferedReader ( new   InputSReader ( System . in )  )  ;", "String   line ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "String   out    =    line . replace ( find ,    replace )  ;", "System . out . println ( out )  ;", "System . err . println (  \" reporter : counter : UserCounters , InputLines ,  1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.TrApp"}, {"methodBody": ["METHOD_START", "{", "args [  0  ]     =    TrApp . CUnescape ( args [  0  ]  )  ;", "args [  1  ]     =    TrApp . CUnescape ( args [  1  ]  )  ;", "TrApp   app    =    new   TrApp ( args [  0  ]  . charAt (  0  )  ,    args [  1  ]  . charAt (  0  )  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TrApp"}, {"methodBody": ["METHOD_START", "{", "env    =    new   Environment (  )  ;", "expectDefined (  \" mapreduce _ cluster _ local _ dir \"  )  ;", "expect (  \" mapreduce _ map _ output _ key _ class \"  ,     \" io . Text \"  )  ;", "expect (  \" mapreduce _ map _ output _ value _ class \"  ,     \" io . Text \"  )  ;", "expect (  \" mapreduce _ task _ ismap \"  ,     \" true \"  )  ;", "expectDefined (  \" mapreduce _ task _ attempt _ id \"  )  ;", "expectDefined (  \" mapreduce _ map _ input _ file \"  )  ;", "expectDefined (  \" mapreduce _ map _ input _ length \"  )  ;", "expectDefined (  \" mapreduce _ task _ io _ sort _ factor \"  )  ;", "}", "METHOD_END"], "methodName": ["testParentJobConfToEnvVars"], "fileName": "org.apache.hadoop.streaming.TrApp"}, {"methodBody": ["METHOD_START", "{", "if    ( s . equals (  \"  \\  \\ n \"  )  )     {", "turn    \"  \\ n \"  ;", "} else    {", "turn   s ;", "}", "}", "METHOD_END"], "methodName": ["CUnescape"], "fileName": "org.apache.hadoop.streaming.TrAppReduce"}, {"methodBody": ["METHOD_START", "{", "String   got    =    env . getProperty ( evName )  ;", "if    (  !  ( evVal . equals ( got )  )  )     {", "String   msg    =     (  (  (  (  \" FAIL   evName =  \"     +    evName )     +     \"    got =  \"  )     +    got )     +     \"    expect =  \"  )     +    evVal ;", "throw   new   IOException ( msg )  ;", "}", "}", "METHOD_END"], "methodName": ["expect"], "fileName": "org.apache.hadoop.streaming.TrAppReduce"}, {"methodBody": ["METHOD_START", "{", "String   got    =    env . getProperty ( evName )  ;", "if    ( got    =  =    null )     {", "String   msg    =     (  \" FAIL   evName =  \"     +    evName )     +     \"    is   undefined .    Expect   defined .  \"  ;", "throw   new   IOException ( msg )  ;", "}", "}", "METHOD_END"], "methodName": ["expectDefined"], "fileName": "org.apache.hadoop.streaming.TrAppReduce"}, {"methodBody": ["METHOD_START", "{", "testParentJobConfToEnvVars (  )  ;", "BufferedReader   in    =    new   BufferedReader ( new   InputSReader ( System . in )  )  ;", "String   line ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "String   out    =    line . replace ( find ,    replace )  ;", "System . out . println ( out )  ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.TrAppReduce"}, {"methodBody": ["METHOD_START", "{", "args [  0  ]     =    TrAppReduce . CUnescape ( args [  0  ]  )  ;", "args [  1  ]     =    TrAppReduce . CUnescape ( args [  1  ]  )  ;", "TrAppReduce   app    =    new   TrAppReduce ( args [  0  ]  . charAt (  0  )  ,    args [  1  ]  . charAt (  0  )  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TrAppReduce"}, {"methodBody": ["METHOD_START", "{", "env    =    new   Environment (  )  ;", "expect (  \" mapreduce _ jobtracker _ address \"  ,     \" local \"  )  ;", "expectDefined (  \" mapreduce _ cluster _ local _ dir \"  )  ;", "expect (  \" mapred _ output _ format _ class \"  ,     \" mapred . TextOutputFormat \"  )  ;", "expect (  \" mapreduce _ job _ output _ key _ class \"  ,     \" io . Text \"  )  ;", "expect (  \" mapreduce _ job _ output _ value _ class \"  ,     \" io . Text \"  )  ;", "expect (  \" mapreduce _ task _ ismap \"  ,     \" false \"  )  ;", "expectDefined (  \" mapreduce _ task _ attempt _ id \"  )  ;", "expectDefined (  \" mapreduce _ task _ io _ sort _ factor \"  )  ;", "}", "METHOD_END"], "methodName": ["testParentJobConfToEnvVars"], "fileName": "org.apache.hadoop.streaming.TrAppReduce"}, {"methodBody": ["METHOD_START", "{", "TypedBytesInput   tbinput    =    new   TypedBytesInput ( new   DataInputStream ( System . in )  )  ;", "TypedBytesOutput   tboutput    =    new   TypedBytesOutput ( new   DataOutputStream ( System . out )  )  ;", "Object   key    =    tbinput . readRaw (  )  ;", "while    ( key    !  =    null )     {", "Object   value    =    tbinput . read (  )  ;", "for    ( String   part    :    value . toString (  )  . split ( find )  )     {", "tboutput . write ( part )  ;", "tboutput . write (  1  )  ;", "}", "System . err . println (  \" reporter : counter : UserCounters , InputLines ,  1  \"  )  ;", "key    =    tbinput . readRaw (  )  ;", "}", "System . out . flush (  )  ;", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.TypedBytesMapApp"}, {"methodBody": ["METHOD_START", "{", "TypedBytesMapApp   app    =    new   TypedBytesMapApp ( args [  0  ]  . replace (  \"  .  \"  ,     \"  \\  \\  .  \"  )  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TypedBytesMapApp"}, {"methodBody": ["METHOD_START", "{", "TypedBytesInput   tbinput    =    new   TypedBytesInput ( new   DataInputStream ( System . in )  )  ;", "TypedBytesOutput   tboutput    =    new   TypedBytesOutput ( new   DataOutputStream ( System . out )  )  ;", "Object   prevKey    =    null ;", "int   sum    =     0  ;", "Object   key    =    tbinput . read (  )  ;", "while    ( key    !  =    null )     {", "if    (  ( prevKey    !  =    null )     &  &     (  !  ( key . equals ( prevKey )  )  )  )     {", "tboutput . write ( prevKey )  ;", "tboutput . write ( sum )  ;", "sum    =     0  ;", "}", "sum    +  =     (  ( Integer )     ( tbinput . read (  )  )  )  ;", "prevKey    =    key ;", "key    =    tbinput . read (  )  ;", "}", "tboutput . write ( prevKey )  ;", "tboutput . write ( sum )  ;", "System . out . flush (  )  ;", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.TypedBytesReduceApp"}, {"methodBody": ["METHOD_START", "{", "TypedBytesReduceApp   app    =    new   TypedBytesReduceApp (  )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.TypedBytesReduceApp"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    new   BufferedReader ( new   InputStreamReader ( System . in )  )  ;", "String   line ;", "String   prevLine    =    null ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", "if    (  !  ( line . equals ( prevLine )  )  )     {", "System . out . println (  (  ( header )     +    line )  )  ;", "}", "prevLine    =    line ;", "}", "}", "METHOD_END"], "methodName": ["go"], "fileName": "org.apache.hadoop.streaming.UniqApp"}, {"methodBody": ["METHOD_START", "{", "String   h    =     (  ( args . length )     <     1  )     ?     \"  \"     :    args [  0  ]  ;", "app    =    new    ( h )  ;", "app . go (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.streaming.UniqApp"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   buf    =    new   StringBuffer (  )  ;", "Iterator < String >    it    =    args . iterator (  )  ;", "while    ( it . hasNext (  )  )     {", "if    (  ( buf . length (  )  )     >     0  )     {", "buf . append (  \"     \"  )  ;", "}", "buf . append ( it . next (  )  )  ;", "}", "return   buf . toString (  )  ;", "}", "METHOD_END"], "methodName": ["collate"], "fileName": "org.apache.hadoop.streaming.UtilTest"}, {"methodBody": ["METHOD_START", "{", "boolean   hasPerl    =    false ;", "ShellCommandExecutor   shexec    =    new   ShellCommandExecutor ( new   String [  ]  {     \" perl \"  ,     \"  - e \"  ,     \" print    4  2  \"     }  )  ;", "try    {", "shexec . execute (  )  ;", "if    ( shexec . getOutput (  )  . equals (  \"  4  2  \"  )  )     {", "hasPerl    =    true ;", "} else    {", ". LOG . warn (  \" Perl   is   installed ,    but   isn ' t   behaving   as   expected .  \"  )  ;", "}", "}    catch    ( Exception   e )     {", ". LOG . warn (  (  \" Could   not   run   perl :     \"     +    e )  )  ;", "}", "return   hasPerl ;", "}", "METHOD_END"], "methodName": ["hasPerlSupport"], "fileName": "org.apache.hadoop.streaming.UtilTest"}, {"methodBody": ["METHOD_START", "{", "String   OS    =    System . getProperty (  \" os . name \"  )  ;", "return    ( OS . indexOf (  \" Windows \"  )  )     >     (  -  1  )  ;", "}", "METHOD_END"], "methodName": ["isCygwin"], "fileName": "org.apache.hadoop.streaming.UtilTest"}, {"methodBody": ["METHOD_START", "{", "ArrayList < String >    vargs    =    new   ArrayList < String >  (  )  ;", "File   javaHomeBin    =    new   File ( System . getProperty (  \" home \"  )  ,     \" bin \"  )  ;", "File   jvm    =    new   File ( javaHomeBin ,     \" java \"  )  ;", "vargs . add ( jvm . toString (  )  )  ;", "vargs . add (  \"  - classpath \"  )  ;", "vargs . add (  (  (  \"  \\  \"  \"     +     ( System . getProperty (  \" path \"  )  )  )     +     \"  \\  \"  \"  )  )  ;", "vargs . add (  (  \"  - Xmx \"     +     ( Runtime . getRuntime (  )  . maxMemory (  )  )  )  )  ;", "vargs . add ( main . getName (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( argv . length )  ;    i +  +  )     {", "vargs . add ( argv [ i ]  )  ;", "}", "return    . collate ( vargs ,     \"     \"  )  ;", "}", "METHOD_END"], "methodName": ["makeJavaCommand"], "fileName": "org.apache.hadoop.streaming.UtilTest"}, {"methodBody": ["METHOD_START", "{", "file    =    file . getAbsoluteFile (  )  ;", "if    (  !  ( file . exists (  )  )  )", "return ;", "if    ( file . isDirectory (  )  )     {", "for    ( File   child    :    file . listFiles (  )  )     {", ". recursiveDelete ( child )  ;", "}", "}", "if    (  !  ( file . delete (  )  )  )     {", "throw   new   RuntimeException (  (  \" Failed   to   delete    \"     +    file )  )  ;", "}", "}", "METHOD_END"], "methodName": ["recursiveDelete"], "fileName": "org.apache.hadoop.streaming.UtilTest"}, {"methodBody": ["METHOD_START", "{", "boolean   fromAntJunit    =     ( System . getProperty (  \" test . build . data \"  )  )     !  =    null ;", "if    ( fromAntJunit )     {", "new   File ( antTestDir _  )  . mkdirs (  )  ;", "File   outFile    =    new   File ( antTestDir _  ,     (  ( testName _  )     +     \"  . log \"  )  )  ;", "PrintS   out    =    new   PrintS ( new   FileOutputS ( outFile )  )  ;", "System . setOut ( out )  ;", "System . setErr ( out )  ;", "}", "}", "METHOD_END"], "methodName": ["redirectIfAntJunit"], "fileName": "org.apache.hadoop.streaming.UtilTest"}, {"methodBody": ["METHOD_START", "{", "int   count    =     0  ;", "while    ( arg 1  . hasNext (  )  )     {", "count    +  =     1  ;", "arg 1  . next (  )  ;", "}", "arg 2  . collect ( arg 0  ,    new   Text (  (  \"  \"     +    count )  )  )  ;", "}", "METHOD_END"], "methodName": ["reduce"], "fileName": "org.apache.hadoop.streaming.ValueCountReduce"}, {"methodBody": ["METHOD_START", "{", "return   inputWriterClass ;", "}", "METHOD_END"], "methodName": ["getInputWriterClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "return   outputKeyClass ;", "}", "METHOD_END"], "methodName": ["getOutputKeyClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "return   outputReaderClass ;", "}", "METHOD_END"], "methodName": ["getOutputReaderClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "return   outputValueClass ;", "}", "METHOD_END"], "methodName": ["getOutputValueClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "if    ( identifier . equalsIgnoreCase ( IdentifierResolver . RAW _ BYTES _ ID )  )     {", "setInputWriterClass ( RawBytesInputWriter . class )  ;", "setOutputReaderClass ( RawBytesOutputReader . class )  ;", "setOutputKeyClass ( BytesWritable . class )  ;", "setOutputValueClass ( BytesWritable . class )  ;", "} else", "if    ( identifier . equalsIgnoreCase ( IdentifierResolver . TYPED _ BYTES _ ID )  )     {", "setInputWriterClass ( TypedBytesInputWriter . class )  ;", "setOutputReaderClass ( TypedBytesOutputReader . class )  ;", "setOutputKeyClass ( class )  ;", "setOutputValueClass ( class )  ;", "} else", "if    ( identifier . equalsIgnoreCase ( IdentifierResolver . KEY _ ONLY _ TEXT _ ID )  )     {", "setInputWriterClass ( KeyOnlyTextInputWriter . class )  ;", "setOutputReaderClass ( KeyOnlyTextOutputReader . class )  ;", "setOutputKeyClass ( class )  ;", "setOutputValueClass ( class )  ;", "} else    {", "setInputWriterClass ( TextInputWriter . class )  ;", "setOutputReaderClass ( TextOutputReader . class )  ;", "setOutputKeyClass ( class )  ;", "setOutputValueClass ( class )  ;", "}", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "this . inputWriterClass    =    inputWriterClass ;", "}", "METHOD_END"], "methodName": ["setInputWriterClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "this . outputKeyClass    =    outputKeyClass ;", "}", "METHOD_END"], "methodName": ["setOutputKeyClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "this . outputReaderClass    =    outputReaderClass ;", "}", "METHOD_END"], "methodName": ["setOutputReaderClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "this . outputValueClass    =    outputValueClass ;", "}", "METHOD_END"], "methodName": ["setOutputValueClass"], "fileName": "org.apache.hadoop.streaming.io.IdentifierResolver"}, {"methodBody": ["METHOD_START", "{", "if    ( writable   instanceof   BytesWritable )     {", "BytesWritable   bw    =     (  ( BytesWritable )     ( writable )  )  ;", "byte [  ]    bytes    =    bw . getBytes (  )  ;", "int   length    =    bw . getLength (  )  ;", "clientOut . writeInt ( length )  ;", "clientOut . write ( bytes ,     0  ,    length )  ;", "} else    {", "bufferOut . reset (  )  ;", "writable . write ( bufferDataOut )  ;", "byte [  ]    bytes    =    bufferOut . toByteArray (  )  ;", "clientOut . writeInt ( bytes . length )  ;", "clientOut . write ( bytes )  ;", "}", "}", "METHOD_END"], "methodName": ["writeRawBytes"], "fileName": "org.apache.hadoop.streaming.io.RawBytesInputWriter"}, {"methodBody": ["METHOD_START", "{", "bytes    =    new   byte [ length ]  ;", "clientIn . readFully ( bytes )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readBytes"], "fileName": "org.apache.hadoop.streaming.io.RawBytesOutputReader"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   clientIn . dInt (  )  ;", "}    catch    ( EOFException   eof )     {", "return    -  1  ;", "}", "}", "METHOD_END"], "methodName": ["readLength"], "fileName": "org.apache.hadoop.streaming.io.RawBytesOutputReader"}, {"methodBody": ["METHOD_START", "{", "String   text    =     \" key , value \\ nkey 2  , value 2  \\ nnocomma \\ n \"  ;", "PipeMapRed   pipeMapRed    =    new    . MyPipeMapRed ( text )  ;", "KeyOnlyTextOutputReader   outputReader    =    new   KeyOnlyTextOutputReader (  )  ;", "outputReader . initialize ( pipeMapRed )  ;", "outputReader . readKeyValue (  )  ;", "Assert . assertEquals ( new   Text (  \" key , value \"  )  ,    outputReader . getCurrentKey (  )  )  ;", "outputReader . readKeyValue (  )  ;", "Assert . assertEquals ( new   Text (  \" key 2  , value 2  \"  )  ,    outputReader . getCurrentKey (  )  )  ;", "outputReader . readKeyValue (  )  ;", "Assert . assertEquals ( new   Text (  \" nocomma \"  )  ,    outputReader . getCurrentKey (  )  )  ;", "Assert . assertEquals ( false ,    outputReader . readKeyValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testKeyOnlyTextOutputReader"], "fileName": "org.apache.hadoop.streaming.io.TestKeyOnlyTextOutputReader"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bval ;", "int   valSize ;", "if    ( object   instanceof   BytesWritable )     {", "BytesWritable   val    =     (  ( BytesWritable )     ( object )  )  ;", "bval    =    val . getBytes (  )  ;", "valSize    =    val . getLength (  )  ;", "} else", "if    ( object   instanceof   io . Text )     {", "io . Text   val    =     (  ( io . Text )     ( object )  )  ;", "bval    =    val . getBytes (  )  ;", "valSize    =    val . getLength (  )  ;", "} else    {", "String   sval    =    object . toString (  )  ;", "bval    =    sval . getBytes (  \" UTF -  8  \"  )  ;", "valSize    =    bval . length ;", "}", "clientOut . write ( bval ,     0  ,    valSize )  ;", "}", "METHOD_END"], "methodName": ["writeUTF8"], "fileName": "org.apache.hadoop.streaming.io.TextInputWriter"}, {"methodBody": ["METHOD_START", "{", "int   pos    =    UTF 8 ByteArrayUtils . findBytes ( line ,     0  ,    length ,    separator )  ;", "for    ( int   k    =     1  ;     ( k    <     ( numKeyFields )  )     &  &     ( pos    !  =     (  -  1  )  )  ;    k +  +  )     {", "pos    =    UTF 8 ByteArrayUtils . findBytes ( line ,     ( pos    +     ( separator . length )  )  ,    length ,    separator )  ;", "}", "try    {", "if    ( pos    =  =     (  -  1  )  )     {", "key . set ( line ,     0  ,    length )  ;", "val . set (  \"  \"  )  ;", "} else    {", "SKeyValUtil . splitKeyVal ( line ,     0  ,    length ,    key ,    val ,    pos ,    separator . length )  ;", "}", "}    catch    ( CharacterCodingException   e )     {", "throw   new   IOException ( StringUtils . stringifyException ( e )  )  ;", "}", "}", "METHOD_END"], "methodName": ["splitKeyVal"], "fileName": "org.apache.hadoop.streaming.io.TextOutputReader"}, {"methodBody": ["METHOD_START", "{", "if    ( value   instanceof   Writable )     {", "tbwOut . write (  (  ( Writable )     ( value )  )  )  ;", "} else    {", "tbOut . write ( value )  ;", "}", "}", "METHOD_END"], "methodName": ["writeTypedBytes"], "fileName": "org.apache.hadoop.streaming.io.TypedBytesInputWriter"}, {"methodBody": ["METHOD_START", "{", "in _  . close (  )  ;", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   new   Text (  )  ;", "}", "METHOD_END"], "methodName": ["createKey"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   new   Text (  )  ;", "}", "METHOD_END"], "methodName": ["createValue"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   in _  . getPos (  )  ;", "}", "METHOD_END"], "methodName": ["getPos"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    (  ( end _  )     =  =     ( start _  )  )     {", "return    1  .  0 F ;", "} else    {", "return    (  ( float )     (  ( in _  . getPos (  )  )     -     ( start _  )  )  )     /     (  ( float )     (  ( end _  )     -     ( start _  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getProgress"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "long   pos    =     -  1  ;", "try    {", "pos    =    getPos (  )  ;", "}    catch    ( IOException   io )     {", "}", "String   recStr ;", "if    (  ( record . length (  )  )     >     ( statusMaxChars _  )  )     {", "recStr    =     ( record . subSequence (  0  ,    statusMaxChars _  )  )     +     \"  .  .  .  \"  ;", "} else    {", "recStr    =    record . toString (  )  ;", "}", "String   unqualSplit    =     (  (  (  ( split _  . getPath (  )  . getName (  )  )     +     \"  :  \"  )     +     ( split _  . getStart (  )  )  )     +     \"  +  \"  )     +     ( split _  . getLength (  )  )  ;", "String   status    =     (  (  (  (  (  (  (  (  \" HSTR    \"     +     ( StreamUtil . getHost (  )  )  )     +     \"     \"  )     +     ( numRec _  )  )     +     \"  .    pos =  \"  )     +    pos )     +     \"     \"  )     +    unqualSplit )     +     \"    Processing   record =  \"  )     +    recStr ;", "status    +  =     \"     \"     +     ( splitName _  )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["getStatus"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "( numRec _  )  +  +  ;", "if    (  ( numRec _  )     =  =     ( nextStatusRec _  )  )     {", "String   recordStr    =    new   String ( record ,    start ,    Math . min ( len ,    statusMaxRecordChars _  )  ,     \" UTF -  8  \"  )  ;", "nextStatusRec _     +  =     1  0  0  ;", "String   status    =    getStatus ( recordStr )  ;", ". LOG . info ( status )  ;", "context _  . setStatus ( status )  ;", "}", "}", "METHOD_END"], "methodName": ["numRecStats"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamBaseRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    (  ( pat . length (  )  )     >     0  )     {", "patpend (  \"  |  \"  )  ;", "}", "patpend (  \"  (  \"  )  ;", "patpend ( esedGroup )  ;", "patpend (  \"  )  \"  )  ;", "}", "METHOD_END"], "methodName": ["addGroup"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "String   val    =    conf _  . get ( prop )  ;", "if    ( val    =  =    null )     {", "throw   new   IOException (  (  \" JobConf :    missing   required   property :     \"     +    prop )  )  ;", "}", "return   val ;", "}", "METHOD_END"], "methodName": ["checkJobGet"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    cpat    =    textPat . getBytes (  \" UTF -  8  \"  )  ;", "int   m    =     0  ;", "booleantch    =    false ;", "int   msup    =    cpat . length ;", "int   LL    =     1  2  0  0  0  0     *     1  0  ;", "bin _ rk ( LL )  ;", "while    ( true )     {", "int   b    =    bin _  . read (  )  ;", "if    ( b    =  =     (  -  1  )  )", "break ;", "byte   c    =     (  ( byte )     ( b )  )  ;", "if    ( c    =  =     ( cpat [ m ]  )  )     {", "m +  +  ;", "if    ( m    =  =    msup )     {", "tch    =    true ;", "break ;", "}", "} else    {", "bin _ rk ( LL )  ;", "if    ( outBufOrNull    !  =    null )     {", "outBufOrNull . write ( cpat ,     0  ,    m )  ;", "outBufOrNull . write ( c )  ;", "}", "pos _     +  =    m    +     1  ;", "m    =     0  ;", "}", "}", "if    (  (  ! includePat )     &  & tch )     {", "bin _  . reset (  )  ;", "} else", "if    ( outBufOrNull    !  =    null )     {", "outBufOrNull . write ( cpat )  ;", "pos _     +  =    msup ;", "}", "returntch ;", "}", "METHOD_END"], "methodName": ["fastReadUntilMatch"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "StreamBaseRecordReader . LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  \" StreamBaseRecordReader . init :     \"     +     \"    start _  =  \"  )     +     ( start _  )  )     +     \"    end _  =  \"  )     +     ( end _  )  )     +     \"    length _  =  \"  )     +     ( length _  )  )     +     \"    start _     >    in _  . getPos (  )     =  \"  )     +     (  ( start _  )     >     ( in _  . getPos (  )  )  )  )     +     \"     \"  )     +     ( start _  )  )     +     \"     >     \"  )     +     ( in _  . getPos (  )  )  )  )  ;", "if    (  ( start _  )     >     ( in _  . getPos (  )  )  )     {", "in _  . seek ( start _  )  ;", "}", "pos _     =    start _  ;", "bin _     =    new   BufferedInputStream ( in _  )  ;", "seekNextRecordBoundary (  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   pat    =    new   StringBuffer (  )  ;", "addGroup ( pat ,    Util . regexpEscape (  \" CDATA [  \"  )  )  ;", "addGroup ( pat ,    Util . regexpEscape (  \"  ]  ]  >  \"  )  )  ;", "addGroup ( pat ,    escapedMark )  ;", "return   Pattern . compile ( pat . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["makePatternCDataOrMark"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "( numNext )  +  +  ;", "if    (  ( pos _  )     >  =     ( end _  )  )     {", "return   false ;", "}", "DataOutputBuffer   buf    =    new   DataOutputBuffer (  )  ;", "if    (  !  ( readUntilMatchBegin (  )  )  )     {", "return   false ;", "}", "if    (  (  ( pos _  )     >  =     ( end _  )  )     |  |     (  !  ( readUntilMatchEnd ( buf )  )  )  )     {", "return   false ;", "}", "byte [  ]    r    =    new   byte [ buf . getLength (  )  ]  ;", "System . arraycopy ( buf . getData (  )  ,     0  ,    r ,     0  ,    r . length )  ;", "numRecStats ( r ,     0  ,    r . length )  ;", "key . set ( r )  ;", "value . set (  \"  \"  )  ;", "return   true ;", "}", "METHOD_END"], "methodName": ["next"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "switch    ( state )     {", "case    . CDATA _ UNK    :", "case    . CDATA _ OUT    :", "switch    ( input )     {", "case    . CDATA _ BEGIN    :", "return    . CDATA _ IN ;", "case    . CDATA _ END    :", "if    ( state    =  =     (  . CDATA _ OUT )  )     {", "}", "return    . CDATA _ OUT ;", "case    . RECORD _ MAYBE    :", "return   state    =  =     (  . CDATA _ UNK )     ?     . CDATA _ UNK    :     . RECORD _ ACCEPT ;", "}", "break ;", "case    . CDATA _ IN    :", "return   input    =  =     (  . CDATA _ END )     ?     . CDATA _ OUT    :     . CDATA _ IN ;", "}", "throw   new   IllegalStateException (  (  (  (  (  (  ( state    +     \"     \"  )     +    input )     +     \"     \"  )     +    bufPos )     +     \"     \"  )     +     ( splitName _  )  )  )  ;", "}", "METHOD_END"], "methodName": ["nextState"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    ( slowMatch _  )     {", "return   slowUntilMatch ( beginPat _  ,    false ,    null )  ;", "} else    {", "return   fastUntilMatch ( beginMark _  ,    false ,    null )  ;", "}", "}", "METHOD_END"], "methodName": ["readUntilMatchBegin"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "if    ( slowMatch _  )     {", "return   slowUntilMatch ( endPat _  ,    true ,    buf )  ;", "} else    {", "return   fastUntilMatch ( endMark _  ,    true ,    buf )  ;", "}", "}", "METHOD_END"], "methodName": ["readUntilMatchEnd"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "readUntilMatchBegin (  )  ;", "}", "METHOD_END"], "methodName": ["seekNextRecordBoundary"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buf    =    new   byte [ Math . max ( lookAhead _  ,    maxRecSize _  )  ]  ;", "int   read    =     0  ;", "bin _  . mark (  (  ( Math . max ( lookAhead _  ,    maxRecSize _  )  )     +     2  )  )  ;", "read    =    bin _  . read ( buf )  ;", "if    ( read    =  =     (  -  1  )  )", "return   false ;", "String   sbuf    =    new   String ( buf ,     0  ,    read ,     \" UTF -  8  \"  )  ;", "Matcher   match    =    markPattern . matcher ( sbuf )  ;", "firstMatchStart _     =     . NA ;", "firstMatchEnd _     =     . NA ;", "int   bufPos    =     0  ;", "int   state    =     ( synched _  )     ?     . CDATA _ OUT    :     . CDATA _ UNK ;", "int   s    =     0  ;", "while    ( match . find ( bufPos )  )     {", "int   input ;", "if    (  ( match . group (  1  )  )     !  =    null )     {", "input    =     . CDATA _ BEGIN ;", "} else", "if    (  ( match . group (  2  )  )     !  =    null )     {", "input    =     . CDATA _ END ;", "firstMatchStart _     =     . NA ;", "} else    {", "input    =     . RECORD _ MAYBE ;", "}", "if    ( input    =  =     (  . RECORD _ MAYBE )  )     {", "if    (  ( firstMatchStart _  )     =  =     (  . NA )  )     {", "firstMatchStart _     =    match . start (  )  ;", "firstMatchEnd _     =    match . end (  )  ;", "}", "}", "state    =    nextState ( state ,    input ,    match . start (  )  )  ;", "if    ( state    =  =     (  . RECORD _ ACCEPT )  )     {", "break ;", "}", "bufPos    =    match . end (  )  ;", "s +  +  ;", "}", "if    ( state    !  =     (  . CDATA _ UNK )  )     {", "synched _     =    true ;", "}", "boolean   matched    =     (  ( firstMatchStart _  )     !  =     (  . NA )  )     &  &     (  ( state    =  =     (  . RECORD _ ACCEPT )  )     |  |     ( state    =  =     (  . CDATA _ UNK )  )  )  ;", "if    ( matched )     {", "int   endPos    =     ( includePat )     ?    firstMatchEnd _     :    firstMatchStart _  ;", "bin _  . reset (  )  ;", "for    ( long   skiplen    =    endPos ;    skiplen    >     0  ;  )     {", "skiplen    -  =    bin _  . skip ( skiplen )  ;", "}", "pos _     +  =    endPos ;", "if    ( outBufOrNull    !  =    null )     {", "outBufOrNull . writeBytes ( sbuf . substring (  0  ,    endPos )  )  ;", "}", "}", "return   matched ;", "}", "METHOD_END"], "methodName": ["slowReadUntilMatch"], "fileName": "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "String [  ]    words    =    expectedOutput . split (  \"  \\ t \\ n \"  )  ;", "Set < String >    expectedWords    =    new   HashSet < String >  ( Arrays . asList ( words )  )  ;", "words    =    output . split (  \"  \\ t \\ n \"  )  ;", "Set < String >    returnedWords    =    new   HashSet < String >  ( Arrays . asList ( words )  )  ;", "Assert . assertTrue ( returnedWords . containsAll ( expectedWords )  )  ;", "}", "METHOD_END"], "methodName": ["assertOutput"], "fileName": "org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "File   outFile    =    new   File ( OUTPUT _ DIR . toString (  )  )  ;", "Path   outPath    =    new   Path ( outFile . getAbsolutePath (  )  ,     \" part - r -  0  0  0  0  0  \"  )  ;", "String   output    =    slurpH ( outPath ,    fs )  ;", "fs . delete ( outPath ,    true )  ;", "outputExpect    =     (  \"  < PATTERN >  \\ n \"     +     ( outputExpect )  )     +     \"  <  / PATTERN >  \"  ;", "System . err . println (  (  \" outEx 1  =  \"     +     ( outputExpect )  )  )  ;", "System . err . println (  (  \"       out 1  =  \"     +    output )  )  ;", "assertOutput ( outputExpect ,    output )  ;", "}", "METHOD_END"], "methodName": ["checkOutput"], "fileName": "org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "FileOutputStream   out    =    new   FileOutputStream ( INPUT _ FILE . getAbsoluteFile (  )  )  ;", "String   dummyXmlStartTag    =     \"  < PATTERN >  \\ n \"  ;", "String   dummyXmlEndTag    =     \"  <  / PATTERN >  \\ n \"  ;", "out . write ( dummyXmlStartTag . getBytes (  \" UTF -  8  \"  )  )  ;", "out . write ( input . getBytes (  \" UTF -  8  \"  )  )  ;", "out . write ( dummyXmlEndTag . getBytes (  \" UTF -  8  \"  )  )  ;", "out . close (  )  ;", "}", "METHOD_END"], "methodName": ["createInput"], "fileName": "org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "int   len    =     (  ( int )     ( fs . getFileStatus ( p )  . getLen (  )  )  )  ;", "byte [  ]    buf    =    new   byte [ len ]  ;", "FSDataInpu   in    =    fs . open ( p )  ;", "String   contents    =    null ;", "try    {", "in . readFully ( in . getPos (  )  ,    buf )  ;", "contents    =    new   String ( buf ,     \" UTF -  8  \"  )  ;", "}    finally    {", "in . close (  )  ;", "}", "return   contents ;", "}", "METHOD_END"], "methodName": ["slurpHadoop"], "fileName": "org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "fs . delete ( OUTPUT _ DIR ,    true )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "Job   job    =    new   Job (  )  ;", "Configuration   conf    =    job . getConfiguration (  )  ;", "job . setJarByClass ( TestStreamXmlRecordReader . class )  ;", "job . setMapperClass ( Mapper . class )  ;", "conf . set (  \" stream . recordreader . class \"  ,     \" StreamXmlRecordReader \"  )  ;", "conf . set (  \" stream . recordreader . begin \"  ,     \"  < PATTERN >  \"  )  ;", "conf . set (  \" stream . recordreader . end \"  ,     \"  <  / PATTERN >  \"  )  ;", "job . setInputFormatClass ( StreamInputFormat . class )  ;", "job . setMapOutputKeyClass ( Text . class )  ;", "job . setMapOutputValueClass ( Text . class )  ;", "job . setOutputKeyClass ( Text . class )  ;", "job . setOutputValueClass ( Text . class )  ;", "FileInputFormat . addInputPath ( job ,    new   Path (  \" target / input . xml \"  )  )  ;", "OUTPUT _ DIR    =    new   Path (  \" target / output \"  )  ;", "fs    =    FileSystem . get ( conf )  ;", "if    ( fs . exists ( OUTPUT _ DIR )  )     {", "fs . delete ( OUTPUT _ DIR ,    true )  ;", "}", "FileOutputFormat . setOutputPath ( job ,    OUTPUT _ DIR )  ;", "boolean   ret    =    job . waitForCompletion ( true )  ;", "Assert . assertEquals ( true ,    ret )  ;", "checkOutput (  )  ;", "}", "METHOD_END"], "methodName": ["testStreamXmlRecordReader"], "fileName": "org.apache.hadoop.streaming.mapreduce.TestStreamXmlRecordReader"}, {"methodBody": ["METHOD_START", "{", "validatePaths ( options )  ;", "doBuild ( pathToListFile ,    options )  ;", "Configuration   config    =    getConf (  )  ;", "config . set ( DistCpConstants . CONF _ LABEL _ LISTING _ FILE _ PATH ,    pathToListFile . toString (  )  )  ;", "config . setLong ( DistCpConstants . CONF _ LABEL _ TOTAL _ BYTES _ TO _ BE _ COPIED ,    getBytesToCopy (  )  )  ;", "config . setLong ( DistCpConstants . CONF _ LABEL _ TOTAL _ NUMBER _ OF _ RECORDS ,    getNumberOfPaths (  )  )  ;", "validateFinal ( pathToListFile ,    options )  ;", "}", "METHOD_END"], "methodName": ["buildListing"], "fileName": "org.apache.hadoop.tools.CopyListing"}, {"methodBody": ["METHOD_START", "{", "String   copyListingClassName    =    configuration . get ( DistCpConstants . CONF _ LABEL _ COPY _ LISTING _ CLASS ,     \"  \"  )  ;", "Class <  ?    extends    >    copyListingClass ;", "try    {", "if    (  !  ( copyListingClassName . isEmpty (  )  )  )     {", "copyListingClass    =    configuration . getClass ( DistCpConstants . CONF _ LABEL _ COPY _ LISTING _ CLASS ,    Globbed . class ,     . class )  ;", "} else    {", "if    (  ( options . getSourceFileListing (  )  )     =  =    null )     {", "copyListingClass    =    Globbed . class ;", "} else    {", "copyListingClass    =    FileBased . class ;", "}", "}", "copyListingClassName    =    copyListingClass . getName (  )  ;", "Constructor <  ?    extends    >    constructor    =    copyListingClass . getDeclaredConstructor ( Configuration . class ,    Credentials . class )  ;", "return   constructor . newInstance ( configuration ,    credentials )  ;", "}    catch    ( Exception   e )     {", "throw   new   IOException (  (  \" Unable   to   instantiate    \"     +    copyListingClassName )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getCopyListing"], "fileName": "org.apache.hadoop.tools.CopyListing"}, {"methodBody": ["METHOD_START", "{", "return   credentials ;", "}", "METHOD_END"], "methodName": ["getCredentials"], "fileName": "org.apache.hadoop.tools.CopyListing"}, {"methodBody": ["METHOD_START", "{", "this . credentials    =    credentials ;", "}", "METHOD_END"], "methodName": ["setCredentials"], "fileName": "org.apache.hadoop.tools.CopyListing"}, {"methodBody": ["METHOD_START", "{", "Configuration   config    =    getConf (  )  ;", "FileSystem   fs    =    pathToListFile . getFileSystem ( config )  ;", "Path   sortedList    =    DistCpUtils . sortListing ( fs ,    config ,    pathToListFile )  ;", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader ( config ,    Reader . file ( sortedList )  )  ;", "try    {", "Text   lastKey    =    new   Text (  \"  *  \"  )  ;", "FileStatus   lastFileStatus    =    new   FileStatus (  )  ;", "Text   currentKey    =    new   Text (  )  ;", "Set < URI >    aclSupportCheckFsSet    =    Sets . newHashSet (  )  ;", "Set < URI >    xAttrSupportCheckFsSet    =    Sets . newHashSet (  )  ;", "while    ( reader . next ( currentKey )  )     {", "if    ( currentKey . equals ( lastKey )  )     {", "FileStatus   currentFileStatus    =    new   FileStatus (  )  ;", "reader . getCurrentValue ( currentFileStatus )  ;", "throw   new    . DuplicateFileException (  (  (  (  (  \" File    \"     +     ( lastFileStatus . getPath (  )  )  )     +     \"    and    \"  )     +     ( currentFileStatus . getPath (  )  )  )     +     \"    would   cause   duplicates .    Aborting \"  )  )  ;", "}", "reader . getCurrentValue ( lastFileStatus )  ;", "if    ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )     {", "FileSystem   lastFs    =    lastFileStatus . getPath (  )  . getFileSystem ( config )  ;", "URI   lastFsUri    =    lastFs . getUri (  )  ;", "if    (  !  ( aclSupportCheckFsSet . contains ( lastFsUri )  )  )     {", "DistCpUtils . checkFileSystemAclSupport ( lastFs )  ;", "aclSupportCheckFsSet . add ( lastFsUri )  ;", "}", "}", "if    ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )     {", "FileSystem   lastFs    =    lastFileStatus . getPath (  )  . getFileSystem ( config )  ;", "URI   lastFsUri    =    lastFs . getUri (  )  ;", "if    (  !  ( xAttrSupportCheckFsSet . contains ( lastFsUri )  )  )     {", "DistCpUtils . checkFileSystemXAttrSupport ( lastFs )  ;", "xAttrSupportCheckFsSet . add ( lastFsUri )  ;", "}", "}", "lastKey . set ( currentKey )  ;", "}", "}    finally    {", "IOUtils . closeStream ( reader )  ;", "}", "}", "METHOD_END"], "methodName": ["validateFinalListing"], "fileName": "org.apache.hadoop.tools.CopyListing"}, {"methodBody": ["METHOD_START", "{", "return   AclUtil . getAclFromPermAndEntries ( getPermission (  )  ,     (  ( aclEntries )     !  =    null    ?    aclEntries    :    Collections .  < AclEntry > emptyList (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getAclEntries"], "fileName": "org.apache.hadoop.tools.CopyListingFileStatus"}, {"methodBody": ["METHOD_START", "{", "return   xAttrs ;", "}", "METHOD_END"], "methodName": ["getXAttrs"], "fileName": "org.apache.hadoop.tools.CopyListingFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . aclEntries    =    aclEntries ;", "}", "METHOD_END"], "methodName": ["setAclEntries"], "fileName": "org.apache.hadoop.tools.CopyListingFileStatus"}, {"methodBody": ["METHOD_START", "{", "this . xAttrs    =    xAttrs ;", "}", "METHOD_END"], "methodName": ["setXAttrs"], "fileName": "org.apache.hadoop.tools.CopyListingFileStatus"}, {"methodBody": ["METHOD_START", "{", "List < Path >    srcs    =    new   ArrayList < Path >  (  )  ;", "for    (  . FileOperation   op    :    ops )     {", "srcs . add ( op . src )  ;", "}", "DistTool . checkSource ( conf ,    srcs )  ;", "}", "METHOD_END"], "methodName": ["check"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "SequenceFile . Sorter   sorter    =    new   SequenceFile . Sorter ( fs ,    new   Text . Comparator (  )  ,    Text . class ,    DistCh . FileOperation . class ,    conf )  ;", "sorter . sort ( file ,    sorted )  ;", "SequenceFile . Reader   in    =    null ;", "try    {", "in    =    new   SequenceFile . Reader ( fs ,    sorted ,    conf )  ;", "DistCh . FileOperation   curop    =    new   DistCh . FileOperation (  )  ;", "Text   prevsrc    =    null ;", "Text   cursrc    =    new   Text (  )  ;", "for    (  ;    in . next ( cursrc ,    curop )  ;  )     {", "if    (  ( prevsrc    !  =    null )     &  &     ( cursrc . equals ( prevsrc )  )  )     {", "throw   new   DistTool . DuplicationException (  (  (  (  \" Invalid   input ,    there   are   duplicated   files   in   the   sources :     \"     +    prevsrc )     +     \"  ,     \"  )     +    cursrc )  )  ;", "}", "prevsrc    =    cursrc ;", "cursrc    =    new   Text (  )  ;", "curop    =    new   DistCh . FileOperation (  )  ;", "}", "}    finally    {", "in . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkDuplication"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "JobConf   jobconf    =    new   JobConf ( conf ,    DistCh . class )  ;", "jobconf . setJobName ( DistCh . NAME )  ;", "jobconf . setMapSpeculativeExecution ( false )  ;", "jobconf . setInputFormat ( DistCh . ChangeInputFormat . class )  ;", "jobconf . setOutputKeyClass ( Text . class )  ;", "jobconf . setOutputValueClass ( Text . class )  ;", "jobconf . setMapperClass ( DistCh . ChangeFilesMapper . class )  ;", "jobconf . setNumReduceTasks (  0  )  ;", "return   jobconf ;", "}", "METHOD_END"], "methodName": ["createJobConf"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "List < DistCh . FileOperation >    result    =    new   ArrayList < DistCh . FileOperation >  (  )  ;", "for    ( String   line    :    DistTool . readFile ( conf ,    inputfile )  )     {", "result . add ( new   DistCh . FileOperation ( line )  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["fetchList"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "int   numMaps    =     (  ( int )     ( srcCount    /     ( DistCh . OP _ PER _ MAP )  )  )  ;", "numMaps    =    Math . min ( numMaps ,     ( numNodes    *     ( DistCh . MAX _ MAPS _ PER _ NODE )  )  )  ;", "return   Math . max ( numMaps ,     1  )  ;", "}", "METHOD_END"], "methodName": ["getMapCount"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "System . exit ( ToolRunner . run ( new   DistCh ( new   Configuration (  )  )  ,    args )  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "List < DistCh . FileOperation >    ops    =    new   ArrayList < DistCh . FileOperation >  (  )  ;", "Path   logpath    =    null ;", "boolean   isIgnoreFailures    =    false ;", "try    {", "for    ( int   idx    =     0  ;    idx    <     ( args . length )  ;    idx +  +  )     {", "if    (  \"  - f \"  . equals ( args [ idx ]  )  )     {", "if    (  (  +  + idx )     =  =     ( args . length )  )     {", "System . out . println (  \" urilist _ uri   not   specified \"  )  ;", "System . out . println ( DistCh . USAGE )  ;", "return    -  1  ;", "}", "ops . addAll ( DistCh . fetchList ( jobconf ,    new   Path ( args [ idx ]  )  )  )  ;", "} else", "if    ( DistCh . Option . IGNORE _ FAILURES . cmd . equals ( args [ idx ]  )  )     {", "isIgnoreFailures    =    true ;", "} else", "if    (  \"  - log \"  . equals ( args [ idx ]  )  )     {", "if    (  (  +  + idx )     =  =     ( args . length )  )     {", "System . out . println (  \" logdir   not   specified \"  )  ;", "System . out . println ( DistCh . USAGE )  ;", "return    -  1  ;", "}", "logpath    =    new   Path ( args [ idx ]  )  ;", "} else", "if    (  '  -  '     =  =     ( args [ idx ]  . codePointAt (  0  )  )  )     {", "System . out . println (  (  \" Invalid   switch    \"     +     ( args [ idx ]  )  )  )  ;", "System . out . println ( DistCh . USAGE )  ;", "ToolRunner . printGenericCommandUsage ( System . out )  ;", "return    -  1  ;", "} else    {", "ops . add ( new   DistCh . FileOperation ( args [ idx ]  )  )  ;", "}", "}", "if    ( ops . isEmpty (  )  )     {", "throw   new   IllegalStateException (  \" Operation   is   empty \"  )  ;", "}", "DistTool . LOG . info (  (  \" ops =  \"     +    ops )  )  ;", "DistTool . LOG . info (  (  \" isIgnoreFailures =  \"     +    isIgnoreFailures )  )  ;", "jobconf . setBoolean ( DistCh . Option . IGNORE _ FAILURES . propertyname ,    isIgnoreFailures )  ;", "DistCh . check ( jobconf ,    ops )  ;", "try    {", "if    ( setup ( ops ,    logpath )  )     {", "JobClient . runJob ( jobconf )  ;", "}", "}    finally    {", "try    {", "if    ( logpath    =  =    null )     {", "final   Path   logdir    =    FileOutputFormat . getOutputPath ( jobconf )  ;", "if    ( logdir    !  =    null )     {", "logdir . getFileSystem ( jobconf )  . delete ( logdir ,    true )  ;", "}", "}", "}    finally    {", "final   String   jobdir    =    jobconf . get ( DistCh . JOB _ DIR _ LABEL )  ;", "if    ( jobdir    !  =    null )     {", "final   Path   jobpath    =    new   Path ( jobdir )  ;", "jobpath . getFileSystem ( jobconf )  . delete ( jobpath ,    true )  ;", "}", "}", "}", "}    catch    ( DistTool . DuplicationException   e )     {", "DistTool . LOG . error (  \" Input   error :  \"  ,    e )  ;", "return   DistTool . DuplicationException . ERROR _ CODE ;", "}    catch    ( Exception   e )     {", "DistTool . LOG . error (  (  ( DistCh . NAME )     +     \"    failed :     \"  )  ,    e )  ;", "System . out . println ( DistCh . USAGE )  ;", "ToolRunner . printGenericCommandUsage ( System . out )  ;", "return    -  1  ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "final   String   randomId    =    DistTool . getRandomId (  )  ;", "JobClient   jClient    =    new   JobClient ( jobconf )  ;", "Path   stagingArea ;", "try    {", "stagingArea    =    JobSubmissionFiles . getStagingDir ( jClient . getClusterHandle (  )  ,    jobconf )  ;", "}    catch    ( InterruptedException   ie )     {", "throw   new   IOException ( ie )  ;", "}", "Path   jobdir    =    new   Path (  (  (  ( stagingArea    +     (  . NAME )  )     +     \"  _  \"  )     +    randomId )  )  ;", "FsPermission   mapredSysPerms    =    new   FsPermission ( JobSubmissionFiles . JOB _ DIR _ PERMISSION )  ;", "FileSystem . mkdirs ( jClient . getFs (  )  ,    jobdir ,    mapredSysPerms )  ;", "DistTool . LOG . info (  (  (  (  . JOB _ DIR _ LABEL )     +     \"  =  \"  )     +    jobdir )  )  ;", "if    ( log    =  =    null )     {", "log    =    new   Path ( jobdir ,     \"  _ logs \"  )  ;", "}", "FileOutputFormat . setOutputPath ( jobconf ,    log )  ;", "DistTool . LOG . info (  (  \" log =  \"     +    log )  )  ;", "FileSystem   fs    =    jobdir . getFileSystem ( jobconf )  ;", "Path   opList    =    new   Path ( jobdir ,     (  \"  _  \"     +     (  . OP _ LIST _ LABEL )  )  )  ;", "jobconf . set (  . OP _ LIST _ LABEL ,    opList . toString (  )  )  ;", "int   opCount    =     0  ;", "int   synCount    =     0  ;", "SequenceFile . Writer   opWriter    =    null ;", "try    {", "opWriter    =    SequenceFile . createWriter ( fs ,    jobconf ,    opList ,    Text . class ,     . FileOperation . class ,    NONE )  ;", "for    (  . FileOperation   op    :    ops )     {", "FileStatus   srcstat    =    fs . getFileStatus ( op . src )  ;", "if    (  ( srcstat . isDirectory (  )  )     &  &     ( op . isDifferent ( srcstat )  )  )     {", "+  + opCount ;", "opWriter . append ( new   Text ( op . src . toString (  )  )  ,    op )  ;", "}", "Stack < Path >    pathstack    =    new   Stack < Path >  (  )  ;", "for    ( pathstack . push ( op . src )  ;     !  ( pathstack . empty (  )  )  ;  )     {", "for    ( FileStatus   stat    :    fs . listStatus ( pathstack . pop (  )  )  )     {", "if    ( stat . isDirectory (  )  )     {", "pathstack . push ( stat . getPath (  )  )  ;", "}", "if    ( op . isDifferent ( stat )  )     {", "+  + opCount ;", "if    (  (  +  + synCount )     >     (  . SYNC _ FILE _ MAX )  )     {", "opWriter . sync (  )  ;", "synCount    =     0  ;", "}", "Path   f    =    stat . getPath (  )  ;", "opWriter . append ( new   Text ( f . toString (  )  )  ,    new    . FileOperation ( f ,    op )  )  ;", "}", "}", "}", "}", "}    finally    {", "opWriter . close (  )  ;", "}", ". checkDuplication ( fs ,    opList ,    new   Path ( jobdir ,     \"  _ sorted \"  )  ,    jobconf )  ;", "jobconf . setInt (  . OP _ COUNT _ LABEL ,    opCount )  ;", "DistTool . LOG . info (  (  (  (  . OP _ COUNT _ LABEL )     +     \"  =  \"  )     +    opCount )  )  ;", "jobconf . setNumMapTasks (  . getMapCount ( opCount ,    new   JobClient ( jobconf )  . getClusterStatus (  )  . getTaskTrackers (  )  )  )  ;", "return   opCount    !  =     0  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.DistCh"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    job . getConfiguration (  )  ;", "FileSystem   localFS    =    FileSystem . getLocal ( configuration )  ;", "Configuration   sslConf    =    new   Configuration ( false )  ;", "sslConf . addResource ( sslConfigPath )  ;", "Path   localStorePath    =    getLocalStorePath ( sslConf ,    Constants . CONF _ LABEL _ SSL _ TRUST _ STORE _ LOCATION )  ;", "job . addCacheFile ( localStorePath . makeQualified ( localFS . getUri (  )  ,    localFS . getWorkingDirectory (  )  )  . toUri (  )  )  ;", "configuration . set ( Constants . CONF _ LABEL _ SSL _ TRUST _ STORE _ LOCATION ,    localStorePath . getName (  )  )  ;", "localStorePath    =    getLocalStorePath ( sslConf ,    Constants . CONF _ LABEL _ SSL _ KEY _ STORE _ LOCATION )  ;", "job . addCacheFile ( localStorePath . makeQualified ( localFS . getUri (  )  ,    localFS . getWorkingDirectory (  )  )  . toUri (  )  )  ;", "configuration . set ( Constants . CONF _ LABEL _ SSL _ KEY _ STORE _ LOCATION ,    localStorePath . getName (  )  )  ;", "job . addCacheFile ( sslConfigPath . makeQualified ( localFS . getUri (  )  ,    localFS . getWorkingDirectory (  )  )  . toUri (  )  )  ;", "}", "METHOD_END"], "methodName": ["addSSLFilesToDistCache"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  ( metaFolder )     =  =    null )", "return ;", "jobFS . delete ( metaFolder ,    true )  ;", "metaFolder    =    null ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Unable   to   cleanup   meta   folder :     \"     +     ( metaFolder )  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    job . getConfiguration (  )  ;", "Path   targetPath    =    inputOptions . getTargetPath (  )  ;", "FileSystem   targetFS    =    targetPath . getFileSystem ( configuration )  ;", "targetPath    =    targetPath . makeQualified ( targetFS . getUri (  )  ,    targetFS . getWorkingDirectory (  )  )  ;", "if    ( inputOptions . shouldPreserve ( Options . FileAttribute . ACL )  )     {", "Utils . checkFileSystemAclSupport ( targetFS )  ;", "}", "if    ( inputOptions . shouldPreserve ( Options . FileAttribute . XATTR )  )     {", "Utils . checkFileSystemXAttrSupport ( targetFS )  ;", "}", "if    ( inputOptions . shouldAtomicCommit (  )  )     {", "Path   workDir    =    inputOptions . getAtomicWorkPath (  )  ;", "if    ( workDir    =  =    null )     {", "workDir    =    targetPath . getParent (  )  ;", "}", "workDir    =    new   Path ( workDir ,     (  (  (  . WIP _ PREFIX )     +     ( targetPath . getName (  )  )  )     +     (  . rand . nextInt (  )  )  )  )  ;", "FileSystem   workFS    =    workDir . getFileSystem ( configuration )  ;", "if    (  !  ( Utils . compareFs ( targetFS ,    workFS )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Work   path    \"     +    workDir )     +     \"    and   target   path    \"  )     +    targetPath )     +     \"    are   in   different   file   system \"  )  )  ;", "}", "CopyOutputFormat . setWorkingDirectory ( job ,    workDir )  ;", "} else    {", "CopyOutputFormat . setWorkingDirectory ( job ,    targetPath )  ;", "}", "CopyOutputFormat . setCommitDirectory ( job ,    targetPath )  ;", "Path   logPath    =    inputOptions . getLogPath (  )  ;", "if    ( logPath    =  =    null )     {", "logPath    =    new   Path ( metaFolder ,     \"  _ logs \"  )  ;", "} else    {", ". LOG . info (  (  \"    job   log   path :     \"     +    logPath )  )  ;", "}", "CopyOutputFormat . setOutputPath ( job ,    logPath )  ;", "}", "METHOD_END"], "methodName": ["configureOutputFormat"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "Path   fileListingPath    =    getFileListingPath (  )  ;", "CopyListing   copyListing    =    CopyListing . getCopyListing ( job . getConfiguration (  )  ,    job . getCredentials (  )  ,    inputOptions )  ;", "copyListing . buildListing ( fileListingPath ,    inputOptions )  ;", "return   fileListingPath ;", "}", "METHOD_END"], "methodName": ["createInputFileListing"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "String   jobName    =     \" distcp \"  ;", "String   userChosenName    =    getConf (  )  . get ( JOB _ NAME )  ;", "if    ( userChosenName    !  =    null )", "jobName    +  =     \"  :     \"     +    userChosenName ;", "Job   job    =    Job . getInstance ( getConf (  )  )  ;", "job . setJobName ( jobName )  ;", "job . setInputFormatClass ( Utils . getStrategy ( getConf (  )  ,    inputOptions )  )  ;", "job . setJarByClass ( CopyMapper . class )  ;", "configureOutputFormat ( job )  ;", "job . setMapperClass ( CopyMapper . class )  ;", "job . setNumReduceTasks (  0  )  ;", "job . setMapOutputKeyClass ( Text . class )  ;", "job . setMapOutputValueClass ( Text . class )  ;", "job . setOutputFormatClass ( CopyOutputFormat . class )  ;", "job . getConfiguration (  )  . set ( MAP _ SPECULATIVE ,     \" false \"  )  ;", "job . getConfiguration (  )  . set ( NUM _ MAPS ,    String . valueOf ( inputOptions . getMaxMaps (  )  )  )  ;", "if    (  ( inputOptions . getSslConfigurationFile (  )  )     !  =    null )     {", "setupSSLConfig ( job )  ;", "}", "inputOptions . appendToConf ( job . getConfiguration (  )  )  ;", "return   job ;", "}", "METHOD_END"], "methodName": ["createJob"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    getConf (  )  ;", "Path   stagingDir    =    JobSubmissionFiles . getStagingDir ( new   mapreduce . Cluster ( configuration )  ,    configuration )  ;", "Path   metaFolderPath    =    new   Path ( stagingDir ,     (  ( DistCp . PREFIX )     +     ( String . valueOf ( DistCp . rand . nextInt (  )  )  )  )  )  ;", "if    ( DistCp . LOG . isDebugEnabled (  )  )", "DistCp . LOG . debug (  (  \" Meta   folder   location :     \"     +    metaFolderPath )  )  ;", "configuration . set ( DistCpConstants . CONF _ LABEL _ META _ FOLDER ,    metaFolderPath . toString (  )  )  ;", "return   metaFolderPath ;", "}", "METHOD_END"], "methodName": ["createMetaFolderPath"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "assert    ( inputOptions )     !  =    null ;", "assert    ( getConf (  )  )     !  =    null ;", "Job   job    =    null ;", "try    {", "synchronized ( this )     {", "metaFolder    =    createMetaFolderPath (  )  ;", "jobFS    =    metaFolder . getFileSystem ( getConf (  )  )  ;", "job    =    createJob (  )  ;", "}", "createInputFileListing ( job )  ;", "job . submit (  )  ;", "submitted    =    true ;", "}    finally    {", "if    (  !  ( submitted )  )     {", "cleanup (  )  ;", "}", "}", "String   jobID    =    job . getJobID (  )  . toString (  )  ;", "job . getConfiguration (  )  . set ( Constants . CONF _ LABEL _ DISTCP _ JOB _ ID ,    jobID )  ;", ". LOG . info (  (  \"    job - id :     \"     +    jobID )  )  ;", "if    (  ( inputOptions . shouldBlock (  )  )     &  &     (  !  ( job . waitForCompletion ( true )  )  )  )     {", "throw   new   IOException (  (  (  (  \"    failure :    Job    \"     +    jobID )     +     \"    has   failed :     \"  )     +     ( job . getStatus (  )  . getFailureInfo (  )  )  )  )  ;", "}", "return   job ;", "}", "METHOD_END"], "methodName": ["execute"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "Configuration   config    =    new   Configuration (  )  ;", "config . addResource (  . DISTCP _ DEFAULT _ XML )  ;", "return   config ;", "}", "METHOD_END"], "methodName": ["getDefaultConf"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "String   fileListPathStr    =     ( metaFolder )     +     \"  / fileList . seq \"  ;", "Path   path    =    new   Path ( fileListPathStr )  ;", "return   new   Path ( path . toUri (  )  . normalize (  )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["getFileListingPath"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "if    (  ( sslConf . get ( storeKey )  )     !  =    null )     {", "return   new   fs . Path ( sslConf . get ( storeKey )  )  ;", "} else    {", "throw   new   IOException (  (  (  (  \" Store   for    \"     +    storeKey )     +     \"    is   not   set   in    \"  )     +     ( inputOptions . getSslConfigurationFile (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getLocalStorePath"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "return   submitted ;", "}", "METHOD_END"], "methodName": ["isSubmitted"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "int   exitCode ;", "try    {", "distCp    =    new    (  )  ;", ". Cleanup   CLEANUP    =    new    . Cleanup ( distCp )  ;", "ShutdownHookManager . get (  )  . addShutdownHook ( CLEANUP ,     . SHUTDOWN _ HOOK _ PRIORITY )  ;", "exitCode    =    ToolRunner . run (  . getDefaultConf (  )  ,    distCp ,    argv )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Couldn ' t   complete      operation :     \"  ,    e )  ;", "exitCode    =    Constants . UNKNOWN _ ERROR ;", "}", "System . exit ( exitCode )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "if    (  ( argv . length )     <     1  )     {", "OptionsParser . usage (  )  ;", "return   Constants . INVALID _ ARGUMENT ;", "}", "try    {", "inputOptions    =    OptionsParser . parse ( argv )  ;", "setTargetPathExists (  )  ;", ". LOG . info (  (  \" Input   Options :     \"     +     ( inputOptions )  )  )  ;", "}    catch    ( Throwable   e )     {", ". LOG . error (  \" Invalid   arguments :     \"  ,    e )  ;", "System . err . println (  (  \" Invalid   arguments :     \"     +     ( e . getMessage (  )  )  )  )  ;", "OptionsParser . usage (  )  ;", "return   Constants . INVALID _ ARGUMENT ;", "}", "try    {", "execute (  )  ;", "}    catch    ( CopyListing . InvalidInputException   e )     {", ". LOG . error (  \" Invalid   input :     \"  ,    e )  ;", "return   Constants . INVALID _ ARGUMENT ;", "}    catch    ( CopyListing . DuplicateFileException   e )     {", ". LOG . error (  \" Duplicate   files   in   input   path :     \"  ,    e )  ;", "return   Constants . DUPLICATE _ INPUT ;", "}    catch    ( CopyListing . AclsNotSupportedException   e )     {", ". LOG . error (  \" ACLs   not   supported   on   at   least   one   file   system :     \"  ,    e )  ;", "return   Constants . ACLS _ NOT _ SUPPORTED ;", "}    catch    ( CopyListing . XAttrsNotSupportedException   e )     {", ". LOG . error (  \" XAttrs   not   supported   on   at   least   one   file   system :     \"  ,    e )  ;", "return   Constants . XATTRS _ NOT _ SUPPORTED ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "return   Constants . UNKNOWN _ ERROR ;", "}", "return   Constants . SUCCESS ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "Path   target    =    inputOptions . getTargetPath (  )  ;", "FileSystem   targetFS    =    target . getFileSystem ( getConf (  )  )  ;", "boolean   targetExists    =    targetFS . exists ( target )  ;", "inputOptions . setTargetPathExists ( targetExists )  ;", "getConf (  )  . setBoolean ( Constants . CONF _ LABEL _ TARGET _ PATH _ EXISTS ,    targetExists )  ;", "}", "METHOD_END"], "methodName": ["setTargetPathExists"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    job . getConfiguration (  )  ;", "Path   sslConfigPath    =    new   Path ( configuration . getResource ( inputOptions . getSslConfigurationFile (  )  )  . toString (  )  )  ;", "addSSLFilesToDistCache ( job ,    sslConfigPath )  ;", "configuration . set ( Constants . CONF _ LABEL _ SSL _ CONF ,    sslConfigPath . getName (  )  )  ;", "configuration . set ( Constants . CONF _ LABEL _ SSL _ KEYSTORE ,    sslConfigPath . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["setupSSLConfig"], "fileName": "org.apache.hadoop.tools.DistCp"}, {"methodBody": ["METHOD_START", "{", "conf . set ( option . getConfigLabel (  )  ,     \" true \"  )  ;", "}", "METHOD_END"], "methodName": ["addToConf"], "fileName": "org.apache.hadoop.tools.DistCpOptionSwitch"}, {"methodBody": ["METHOD_START", "{", "conf . set ( option . getConfigLabel (  )  ,    value )  ;", "}", "METHOD_END"], "methodName": ["addToConf"], "fileName": "org.apache.hadoop.tools.DistCpOptionSwitch"}, {"methodBody": ["METHOD_START", "{", "return   confLabel ;", "}", "METHOD_END"], "methodName": ["getConfigLabel"], "fileName": "org.apache.hadoop.tools.DistCpOptionSwitch"}, {"methodBody": ["METHOD_START", "{", "return   option ;", "}", "METHOD_END"], "methodName": ["getOption"], "fileName": "org.apache.hadoop.tools.DistCpOptionSwitch"}, {"methodBody": ["METHOD_START", "{", "return   option . getOpt (  )  ;", "}", "METHOD_END"], "methodName": ["getSwitch"], "fileName": "org.apache.hadoop.tools.DistCpOptionSwitch"}, {"methodBody": ["METHOD_START", "{", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . ATOMIC _ COMMIT ,    String . valueOf ( atomicCommit )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . IGNORE _ FAILURES ,    String . valueOf ( ignoreFailures )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . SYNC _ FOLDERS ,    String . valueOf ( syncFolder )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . DELETE _ MISSING ,    String . valueOf ( deleteMissing )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . OVERWRITE ,    String . valueOf ( overwrite )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . APPEND ,    String . valueOf ( append )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . SKIP _ CRC ,    String . valueOf ( skipCRC )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . BANDWIDTH ,    String . valueOf ( mapBandwidth )  )  ;", "DistCpOptionSwitch . addToConf ( conf ,    DistCpOptionSwitch . PRESERVE _ STATUS ,    DistCpUtils . packAttributes ( preserveStatus )  )  ;", "}", "METHOD_END"], "methodName": ["appendToConf"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   atomicWorkPath ;", "}", "METHOD_END"], "methodName": ["getAtomicWorkPath"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   copyStrategy ;", "}", "METHOD_END"], "methodName": ["getCopyStrategy"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   logPath ;", "}", "METHOD_END"], "methodName": ["getLogPath"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   mapBandwidth ;", "}", "METHOD_END"], "methodName": ["getMapBandwidth"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   maxMaps ;", "}", "METHOD_END"], "methodName": ["getMaxMaps"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   sourceFileListing ;", "}", "METHOD_END"], "methodName": ["getSourceFileListing"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   sourcePaths ;", "}", "METHOD_END"], "methodName": ["getSourcePaths"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   sslConfigurationFile ;", "}", "METHOD_END"], "methodName": ["getSslConfigurationFile"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   targetPath ;", "}", "METHOD_END"], "methodName": ["getTargetPath"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   targetPathExists ;", "}", "METHOD_END"], "methodName": ["getTargetPathExists"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "for    ( DistCpOptions . FileAttribute   attribute    :    preserveStatus )     {", "if    ( attribute . equals ( fileAttribute )  )     {", "return ;", "}", "}", "preserveStatus . add ( fileAttribute )  ;", "}", "METHOD_END"], "methodName": ["preserve"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   preserveStatus . iterator (  )  ;", "}", "METHOD_END"], "methodName": ["preserveAttributes"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "preserveRawXattrs    =    true ;", "}", "METHOD_END"], "methodName": ["preserveRawXattrs"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "validate ( DistCpOptionSwitch . APPEND ,    append )  ;", "this . append    =    append ;", "}", "METHOD_END"], "methodName": ["setAppend"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "validate ( DistCpOptionSwitch . ATOMIC _ COMMIT ,    atomicCommit )  ;", "this . atomicCommit    =    atomicCommit ;", "}", "METHOD_END"], "methodName": ["setAtomicCommit"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . atomicWorkPath    =    atomicWorkPath ;", "}", "METHOD_END"], "methodName": ["setAtomicWorkPath"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . blocking    =    blocking ;", "}", "METHOD_END"], "methodName": ["setBlocking"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . copyStrategy    =    copyStrategy ;", "}", "METHOD_END"], "methodName": ["setCopyStrategy"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "validate ( DistCpOptionSwitch . DELETE _ MISSING ,    deleteMissing )  ;", "this . deleteMissing    =    deleteMissing ;", "}", "METHOD_END"], "methodName": ["setDeleteMissing"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . ignoreFailures    =    ignoreFailures ;", "}", "METHOD_END"], "methodName": ["setIgnoreFailures"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . logPath    =    logPath ;", "}", "METHOD_END"], "methodName": ["setLogPath"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "assert   mapBandwidth    >     0     :     (  \" Bandwidth    \"     +    mapBandwidth )     +     \"    is   invalid    ( should   be    >     0  )  \"  ;", "this . mapBandwidth    =    mapBandwidth ;", "}", "METHOD_END"], "methodName": ["setMapBandwidth"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . maxMaps    =    Math . max ( maxMaps ,     1  )  ;", "}", "METHOD_END"], "methodName": ["setMaxMaps"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "validate ( DistCpOptionSwitch . OVERWRITE ,    overwrite )  ;", "this . overwrite    =    overwrite ;", "}", "METHOD_END"], "methodName": ["setOverwrite"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "validate ( DistCpOptionSwitch . SKIP _ CRC ,    skipCRC )  ;", "this . skipCRC    =    skipCRC ;", "}", "METHOD_END"], "methodName": ["setSkipCRC"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "assert    ( sourcePaths    !  =    null )     &  &     (  ( sourcePaths . size (  )  )     !  =     0  )  ;", "this . sourcePaths    =    sourcePaths ;", "}", "METHOD_END"], "methodName": ["setSourcePaths"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "this . sslConfigurationFile    =    sslConfigurationFile ;", "}", "METHOD_END"], "methodName": ["setSslConfigurationFile"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "validate ( DistCpOptionSwitch . SYNC _ FOLDERS ,    syncFolder )  ;", "this . syncFolder    =    syncFolder ;", "}", "METHOD_END"], "methodName": ["setSyncFolder"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   this . targetPathExists    =    targetPathExists ;", "}", "METHOD_END"], "methodName": ["setTargetPathExists"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   append ;", "}", "METHOD_END"], "methodName": ["shouldAppend"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   atomicCommit ;", "}", "METHOD_END"], "methodName": ["shouldAtomicCommit"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   blocking ;", "}", "METHOD_END"], "methodName": ["shouldBlock"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   deleteMissing ;", "}", "METHOD_END"], "methodName": ["shouldDeleteMissing"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   ignoreFailures ;", "}", "METHOD_END"], "methodName": ["shouldIgnoreFailures"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   overwrite ;", "}", "METHOD_END"], "methodName": ["shouldOverwrite"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   preserveStatus . contains ( attribute )  ;", "}", "METHOD_END"], "methodName": ["shouldPreserve"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   preserveRawXattrs ;", "}", "METHOD_END"], "methodName": ["shouldPreserveRawXattrs"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   skipCRC ;", "}", "METHOD_END"], "methodName": ["shouldSkipCRC"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "return   syncFolder ;", "}", "METHOD_END"], "methodName": ["shouldSyncFolder"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "boolean   syncFolder    =     ( option    =  =     ( DistCpOptionSwitch . SYNC _ FOLDERS )  )     ?    value    :    this . syncFolder ;", "boolean   overwrite    =     ( option    =  =     ( DistCpOptionSwitch . OVERWRITE )  )     ?    value    :    this . overwrite ;", "boolean   deleteMissing    =     ( option    =  =     ( DistCpOptionSwitch . DELETE _ MISSING )  )     ?    value    :    this . deleteMissing ;", "boolean   atomicCommit    =     ( option    =  =     ( DistCpOptionSwitch . ATOMIC _ COMMIT )  )     ?    value    :    this . atomicCommit ;", "boolean   skipCRC    =     ( option    =  =     ( DistCpOptionSwitch . SKIP _ CRC )  )     ?    value    :    this . skipCRC ;", "boolean   append    =     ( option    =  =     ( DistCpOptionSwitch . APPEND )  )     ?    value    :    this . append ;", "if    ( syncFolder    &  &    atomicCommit )     {", "throw   new   IllegalArgumentException (  (  \" Atomic   commit   can ' t   be   used   with    \"     +     \" sync   folder   or   overwrite   options \"  )  )  ;", "}", "if    ( deleteMissing    &  &     (  !  ( overwrite    |  |    syncFolder )  )  )     {", "throw   new   IllegalArgumentException (  (  \" Delete   missing   is   applicable    \"     +     \" only   with   update   or   overwrite   options \"  )  )  ;", "}", "if    ( overwrite    &  &    syncFolder )     {", "throw   new   IllegalArgumentException (  (  \" Overwrite   and   update   options   are    \"     +     \" mutually   exclusive \"  )  )  ;", "}", "if    (  (  ! syncFolder )     &  &    skipCRC )     {", "throw   new   IllegalArgumentException (  \" Skip   CRC   is   valid   only   with   update   options \"  )  ;", "}", "if    (  (  ! syncFolder )     &  &    append )     {", "throw   new   IllegalArgumentException (  \" Append   is   valid   only   with   update   options \"  )  ;", "}", "if    ( skipCRC    &  &    append )     {", "throw   new   IllegalArgumentException (  \" Append   is   disallowed   when   skipping   CRC \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["validate"], "fileName": "org.apache.hadoop.tools.DistCpOptions"}, {"methodBody": ["METHOD_START", "{", "if    ( io    !  =    null )     {", "try    {", "io . close (  )  ;", "}    catch    ( IOException   ioe )     {", ". LOG . warn ( StringUtils . stringifyException ( ioe )  )  ;", "return   false ;", "}", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["checkAndClose"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "SequenceFile . Reader   in    =    null ;", "try    {", "SequenceFile . Sorter   sorter    =    new   SequenceFile . Sorter ( fs ,    new   Text . Comparator (  )  ,    Text . class ,    Text . class ,    conf )  ;", "sorter . sort ( file ,    sorted )  ;", "in    =    new   SequenceFile . Reader ( fs ,    sorted ,    conf )  ;", "Text   prevdst    =    null ;", "Text   curdst    =    new   Text (  )  ;", "Text   prevsrc    =    null ;", "Text   cursrc    =    new   Text (  )  ;", "for    (  ;    in . next ( curdst ,    cursrc )  ;  )     {", "if    (  ( prevdst    !  =    null )     &  &     ( curdst . equals ( prevdst )  )  )     {", "throw   new    . DuplicationException (  (  (  (  \" Invalid   input ,    there   are   duplicated   files   in   the   sources :     \"     +    prevsrc )     +     \"  ,     \"  )     +    cursrc )  )  ;", "}", "prevdst    =    curdst ;", "curdst    =    new   Text (  )  ;", "prevsrc    =    cursrc ;", "cursrc    =    new   Text (  )  ;", "}", "}    finally    {", ". checkAndClose ( in )  ;", "}", "}", "METHOD_END"], "methodName": ["checkDuplication"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "List < IOException >    rslt    =    new   ArrayList < IOException >  (  )  ;", "List < Path >    unglobbed    =    new   LinkedList < Path >  (  )  ;", "Path [  ]    ps    =    new   Path [ srcPaths . size (  )  ]  ;", "ps    =    srcPaths . toArray ( ps )  ;", "TokenCobtainTokensForNamenodes ( jobConf . getCredentials (  )  ,    ps ,    jobConf )  ;", "for    ( Path   p    :    srcPaths )     {", "FileSystem   fs    =    p . getFileSystem ( jobConf )  ;", "FileStatus [  ]    inputs    =    fs . globStatus ( p )  ;", "if    (  ( inputs    !  =    null )     &  &     (  ( inputs . length )     >     0  )  )     {", "for    ( FileStatus   onePath    :    inputs )     {", "unglobbed . add ( onePath . getPath (  )  )  ;", "}", "} else    {", "rslt . add ( new   IOException (  (  (  \" Input   source    \"     +    p )     +     \"    does   not   exist .  \"  )  )  )  ;", "}", "}", "if    (  !  ( rslt . isEmpty (  )  )  )     {", "throw   new   InvalidInputException ( rslt )  ;", "}", "srcPaths . clear (  )  ;", "srcPaths . addAll ( unglobbed )  ;", "}", "METHOD_END"], "methodName": ["checkSrcPath"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "final   Path   src    =    new   Path ( srcPath )  ;", "List < Path >    tmp    =    new   ArrayList < Path >  (  )  ;", "if    ( srcAsList )     {", "tmp . addAll (  . fetchFileList ( conf ,    src )  )  ;", "} else    {", "tmp . add ( src )  ;", "}", "EnumSet <  . Options >    flags    =     ( ignoreReadFailures )     ?    EnumSet . of (  . Options . IGNORE _ READ _ FAILURES )     :    EnumSet . noneOf (  . Options . class )  ;", "final   Path   dst    =    new   Path ( destPath )  ;", ". copy ( conf ,    new    . Arguments ( tmp ,    null ,    dst ,    logPath ,    flags ,    null ,    Long . MAX _ VALUE ,    Long . MAX _ VALUE ,    null ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "DistCpV 1  . LOG . info (  (  \" srcPaths =  \"     +     ( args . srcs )  )  )  ;", "if    (  (  !  ( args . dryrun )  )     |  |     ( args . flags . contains ( DistCpV 1  . Options . UPDATE )  )  )     {", "DistCpV 1  . LOG . info (  (  \" destPath =  \"     +     ( args . dst )  )  )  ;", "}", "JobConf   job    =    DistCpV 1  . createJobConf ( conf )  ;", "DistCpV 1  . checkSrcPath ( job ,    args . srcs )  ;", "if    (  ( args . preservedAttributes )     !  =    null )     {", "job . set ( DistCpV 1  . PRESERVE _ STATUS _ LABEL ,    args . preservedAttributes )  ;", "}", "if    (  ( args . mapredSslConf )     !  =    null )     {", "job . set (  \" dfs . https . client . keystore . resource \"  ,    args . mapredSslConf )  ;", "}", "try    {", "if    ( DistCpV 1  . setup ( conf ,    job ,    args )  )     {", "JobClient . runJob ( job )  ;", "}", "if    (  !  ( args . dryrun )  )     {", "DistCpV 1  . finalize ( conf ,    job ,    args . dst ,    args . preservedAttributes )  ;", "}", "}    finally    {", "if    (  !  ( args . dryrun )  )     {", "DistCpV 1  . fullyDelete ( job . get ( DistCpV 1  . TMP _ DIR _ LABEL )  ,    job )  ;", "}", "DistCpV 1  . fullyDelete ( job . get ( DistCpV 1  . JOB _ DIR _ LABEL )  ,    job )  ;", "}", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "JobConf   jobconf    =    new   JobConf ( conf ,    DistCpV 1  . class )  ;", "jobconf . setJobName ( conf . get (  \" mapred . job . name \"  ,    DistCpV 1  . NAME )  )  ;", "jobconf . setMapSpeculativeExecution ( false )  ;", "jobconf . setInputFormat ( DistCpV 1  . CopyInputFormat . class )  ;", "jobconf . setOutputKeyClass ( Text . class )  ;", "jobconf . setOutputValueClass ( Text . class )  ;", "jobconf . setMapperClass ( DistCpV 1  . CopyFilesMapper . class )  ;", "jobconf . setNumReduceTasks (  0  )  ;", "return   jobconf ;", "}", "METHOD_END"], "methodName": ["createJobConf"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "if    ( dstroot . isFile (  )  )     {", "throw   new   IOException (  (  (  (  (  \" dst   must   be   a   directory   when   option    \"     +     (  . Options . DELETE . cmd )  )     +     \"    is   set ,    but   dst    (  =     \"  )     +     ( dstroot . getPath (  )  )  )     +     \"  )    is   not   a   directory .  \"  )  )  ;", "}", "final   Path   dstlsr    =    new   Path ( jobdir ,     \"  _ distcp _ dst _ lsr \"  )  ;", "final   SequenceFile . Writer   writer    =    SequenceFile . createWriter ( jobfs ,    jobconf ,    dstlsr ,    Text . class ,    NullWritable . class ,    NONE )  ;", "try    {", "final   Stack < FileStatus >    lsrstack    =    new   Stack < FileStatus >  (  )  ;", "for    ( lsrstack . push ( dstroot )  ;     !  ( lsrstack . isEmpty (  )  )  ;  )     {", "final   FileStatus   status    =    lsrstack . pop (  )  ;", "if    ( status . isDirectory (  )  )     {", "for    ( FileStatus   child    :    dstfs . listStatus ( status . getPath (  )  )  )     {", "String   relative    =     . makeRelative ( dstroot . getPath (  )  ,    child . getPath (  )  )  ;", "writer . append ( new   Text ( relative )  ,    NullWritable . get (  )  )  ;", "lsrstack . push ( child )  ;", "}", "}", "}", "}    finally    {", ". checkAndClose ( writer )  ;", "}", "final   Path   sortedlsr    =    new   Path ( jobdir ,     \"  _ distcp _ dst _ lsr _ sorted \"  )  ;", "SequenceFile . Sorter   sorter    =    new   SequenceFile . Sorter ( jobfs ,    new   Text . Comparator (  )  ,    Text . class ,    NullWritable . class ,    jobconf )  ;", "sorter . sort ( dstlsr ,    sortedlsr )  ;", "SequenceFile . Reader   lsrin    =    null ;", "SequenceFile . Reader   dstin    =    null ;", "long   deletedPathsCount    =     0  ;", "try    {", "lsrin    =    new   SequenceFile . Reader ( jobfs ,    sortedlsr ,    jobconf )  ;", "dstin    =    new   SequenceFile . Reader ( jobfs ,    dstsorted ,    jobconf )  ;", "final   Text   lsrpath    =    new   Text (  )  ;", "final   Text   dstpath    =    new   Text (  )  ;", "final   Text   dstfrom    =    new   Text (  )  ;", "final   Trash   trash    =    new   Trash ( dstfs ,    conf )  ;", "Path   lastpath    =    null ;", "boolean   hasnext    =    dstin . next ( dstpath ,    dstfrom )  ;", "while    ( lsrin . next ( lsrpath ,    NullWritable . get (  )  )  )     {", "int   dst _ cmp _ lsr    =    dstpath . compareTo ( lsrpath )  ;", "while    ( hasnext    &  &     ( dst _ cmp _ lsr    <     0  )  )     {", "hasnext    =    dstin . next ( dstpath ,    dstfrom )  ;", "dst _ cmp _ lsr    =    dstpath . compareTo ( lsrpath )  ;", "}", "if    ( dst _ cmp _ lsr    =  =     0  )     {", "hasnext    =    dstin . next ( dstpath ,    dstfrom )  ;", "} else    {", "final   Path   rmpath    =    new   Path ( dstroot . getPath (  )  ,    lsrpath . toString (  )  )  ;", "+  + deletedPathsCount ;", "if    (  ( lastpath    =  =    null )     |  |     (  !  (  . isAncestorPath ( lastpath ,    rmpath )  )  )  )     {", "if    (  !  (  ( trash . moveToTrash ( rmpath )  )     |  |     ( dstfs . delete ( rmpath ,    true )  )  )  )     {", "throw   new   IOException (  (  \" Failed   to   delete    \"     +    rmpath )  )  ;", "}", "lastpath    =    rmpath ;", "}", "}", "}", "}    finally    {", ". checkAndClose ( lsrin )  ;", ". checkAndClose ( dstin )  ;", "}", "return   deletedPathsCount ;", "}", "METHOD_END"], "methodName": ["deleteNonexisting"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "FileSystem   destFileSys    =    dst . getFileSystem ( conf )  ;", "FileStatus   status    =    null ;", "try    {", "status    =    destFileSys . getFileStatus ( dst )  ;", "}    catch    ( FileNotFoundException   e )     {", "return   false ;", "}", "if    ( status . isFile (  )  )     {", "throw   new   fs . FileAlreadyExistsException (  (  (  \" Not   a   dir :     \"     +    dst )     +     \"    is   a   file .  \"  )  )  ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["dirExists"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "List < Path >    result    =    new   ArrayList < Path >  (  )  ;", "FileSystem   fs    =    srcList . getFileSystem ( conf )  ;", "BufferedReader   input    =    null ;", "try    {", "input    =    new   BufferedReader ( new   InputStreamReader ( fs . open ( srcList )  )  )  ;", "String   line    =    input . readLine (  )  ;", "while    ( line    !  =    null )     {", "result . add ( new   Path ( line )  )  ;", "line    =    input . readLine (  )  ;", "}", "}    finally    {", ". checkAndClose ( input )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["fetchFileList"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "if    ( presevedAttributes    =  =    null )     {", "return ;", "}", "EnumSet <  . FileAttribute >    preseved    =     . FileAttribute . parse ( presevedAttributes )  ;", "if    (  (  (  !  ( preseved . contains (  . FileAttribute . USER )  )  )     &  &     (  !  ( preseved . contains (  . FileAttribute . GROUP )  )  )  )     &  &     (  !  ( preseved . contains (  . FileAttribute . PERMISSION )  )  )  )     {", "return ;", "}", "FileSystem   dstfs    =    destPath . getFileSystem ( conf )  ;", "Path   dstdirlist    =    new   Path ( jobconf . get (  . DST _ DIR _ LIST _ LABEL )  )  ;", "SequenceFile . Reader   in    =    null ;", "try    {", "in    =    new   SequenceFile . Reader ( dstdirlist . getFileSystem ( jobconf )  ,    dstdirlist ,    jobconf )  ;", "Text   dsttext    =    new   Text (  )  ;", ". FilePair   pair    =    new    . FilePair (  )  ;", "for    (  ;    in . next ( dsttext ,    pair )  ;  )     {", "Path   absdst    =    new   Path ( destPath ,    pair . output )  ;", ". updateDestStatus ( pair . input ,    dstfs . getFileStatus ( absdst )  ,    preseved ,    dstfs )  ;", "}", "}    finally    {", ". checkAndClose ( in )  ;", "}", "}", "METHOD_END"], "methodName": ["finalize"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "if    ( dir    !  =    null )     {", "Path   tmp    =    new   Path ( dir )  ;", "boolean   success    =    tmp . getFileSystem ( conf )  . delete ( tmp ,    true )  ;", "if    (  ! success )     {", ". LOG . warn (  (  \" Could   not   fully   delete    \"     +    tmp )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["fullyDelete"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "return   Integer . toString ( DistCpV 1  . RANDOM . nextInt ( Integer . MAX _ VALUE )  ,     3  6  )  ;", "}", "METHOD_END"], "methodName": ["getRandomId"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "final   String   x    =    xp . toString (  )  ;", "final   String   y    =    yp . toString (  )  ;", "if    (  !  ( y . startsWith ( x )  )  )     {", "return   false ;", "}", "final   int   len    =    x . length (  )  ;", "return    (  ( y . length (  )  )     =  =    len )     |  |     (  ( y . charAt ( len )  )     =  =     ( Path . SEPARATOR _ CHAR )  )  ;", "}", "METHOD_END"], "methodName": ["isAncestorPath"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "JobConf   job    =    new   JobConf ( DistCpV 1  . class )  ;", "DistCpV 1    distcp    =    new   DistCpV 1  ( job )  ;", "int   res    =    ToolRunner . run ( distcp ,    args )  ;", "System . exit ( res )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( absPath . isAbsolute (  )  )  )     {", "throw   new   IllegalArgumentException (  (  \"  ! absPath . isAbsolute (  )  ,    absPath =  \"     +    absPath )  )  ;", "}", "String   p    =    absPathUri (  )  . getPath (  )  ;", "StringTokenizer   pathTokens    =    new   StringTokenizer ( p ,     \"  /  \"  )  ;", "for    ( StringTokenizer   rootTokens    =    new   StringTokenizer ( rootUri (  )  . getPath (  )  ,     \"  /  \"  )  ;    rootTokens . hasMoreTokens (  )  ;  )     {", "if    (  !  ( rootTokens . nextToken (  )  . equals ( pathTokens . nextToken (  )  )  )  )     {", "return   null ;", "}", "}", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    (  ;    pathTokens . hasMoreTokens (  )  ;  )     {", "sb . append ( pathTokens . nextToken (  )  )  ;", "if    ( pathTokens . hasMoreTokens (  )  )     {", "sb . append ( SEPARATOR )  ;", "}", "}", "return    ( sb . length (  )  )     =  =     0     ?     \"  .  \"     :    sbString (  )  ;", "}", "METHOD_END"], "methodName": ["makeRelative"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "try    {", ". copy ( conf ,     . Arguments . valueOf ( args ,    conf )  )  ;", "return    0  ;", "}    catch    ( IllegalArgumentException   e )     {", "System . err . println (  (  (  ( StringUtils . stringifyException ( e )  )     +     \"  \\ n \"  )     +     (  . usage )  )  )  ;", "ToolRunner . printGenericCommandUsage ( System . err )  ;", "return    -  1  ;", "}    catch    (  . DuplicationException   e )     {", "System . err . println ( StringUtils . stringifyException ( e )  )  ;", "return    . DuplicationException . ERROR _ CODE ;", "}    catch    ( RemoteException   e )     {", "final   IOException   unwrapped    =    e . unwrapRemoteException ( FileNotFoundException . class ,    AccessControlException . class ,    QuotaExceededException . class )  ;", "System . err . println ( StringUtils . stringifyException ( unwrapped )  )  ;", "return    -  3  ;", "}    catch    ( Exception   e )     {", "System . err . println (  (  \" With   failures ,    global   counters   are   inaccurate ;     \"     +     \" consider   running   with    - i \"  )  )  ;", "System . err . println (  (  \" Copy   failed :     \"     +     ( StringUtils . stringifyException ( e )  )  )  )  ;", "return    -  9  9  9  ;", "}", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "FileStatus   dststatus ;", "try    {", "dststatus    =    dstfs . getFileStatus ( dstpath )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "return   false ;", "}", "if    (  ( srcstatus . getLen (  )  )     !  =     ( dststatus . getLen (  )  )  )     {", "return   false ;", "}", "if    ( skipCRCCheck )     {", ". LOG . debug (  \" Skipping   the   CRC   check \"  )  ;", "return   true ;", "}", "final   FileChecksum   srccs ;", "try    {", "srccs    =    srcfs . getFileChecksum ( srcstatus . getPath (  )  )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "return   true ;", "}", "try    {", "final   FileChecksum   dstcs    =    dstfs . getFileChecksum ( dststatus . getPath (  )  )  ;", "return    (  ( srccs    =  =    null )     |  |     ( dstcs    =  =    null )  )     |  |     ( srccs . equals ( dstcs )  )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["sameFile"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "if    ( conf   instanceof   JobConf )     {", "this . conf    =     (  ( JobConf )     ( conf )  )  ;", "} else    {", "this . conf    =    new   JobConf ( conf )  ;", "}", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "int   numMaps    =     (  ( int )     ( totalBytes    /     ( job . getLong ( DistCpV 1  . BYTES _ PER _ MAP _ LABEL ,    DistCpV 1  . BYTES _ PER _ MAP )  )  )  )  ;", "numMaps    =    Math . min ( numMaps ,    job . getInt ( DistCpV 1  . MAX _ MAPS _ LABEL ,     (  ( DistCpV 1  . MAX _ MAPS _ PER _ NODE )     *     ( new   mapred . JobClient ( job )  . getClusterStatus (  )  . getTaskTrackers (  )  )  )  )  )  ;", "numMaps    =    Math . max ( numMaps ,     1  )  ;", "job . setNumMapTasks ( numMaps )  ;", "return   numMaps ;", "}", "METHOD_END"], "methodName": ["setMapCount"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "int   numMaxMaps    =    new   JobClient ( jobConf )  . getClusterStatus (  )  . getMaxMapTasks (  )  ;", "short   replication    =     (  ( short )     ( Math . ceil ( Math . sqrt ( Math . min ( numMaxMaps ,    numMaps )  )  )  )  )  ;", "FileSystem   fs    =    srcfilelist . getFileSystem ( conf )  ;", "FileStatus   srcStatus    =    fs . getFileStatus ( srcfilelist )  ;", "if    (  ( srcStatus . getReplication (  )  )     <    replication )     {", "if    (  !  ( fs . setReplication ( srcfilelist ,    replication )  )  )     {", "throw   new   IOException (  (  \" Unable   to   increase   the   replication   of   file    \"     +    srcfilelist )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setReplication"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "jobConf . set ( DistCpV 1  . DST _ DIR _ LABEL ,    args . dst . toUri (  )  . toString (  )  )  ;", "final   boolean   update    =    args . flags . contains ( DistCpV 1  . Options . UPDATE )  ;", "final   boolean   skipCRCCheck    =    args . flags . contains ( DistCpV 1  . Options . SKIPCRC )  ;", "final   boolean   overwrite    =     (  (  ! update )     &  &     ( args . flags . contains ( DistCpV 1  . Options . OVERWRITE )  )  )     &  &     (  !  ( args . dryrun )  )  ;", "jobConf . setBoolean ( DistCpV 1  . Options . UPDATE . propertyname ,    update )  ;", "jobConf . setBoolean ( DistCpV 1  . Options . SKIPCRC . propertyname ,    skipCRCCheck )  ;", "jobConf . setBoolean ( DistCpV 1  . Options . OVERWRITE . propertyname ,    overwrite )  ;", "jobConf . setBoolean ( DistCpV 1  . Options . IGNORE _ READ _ FAILURES . propertyname ,    args . flags . contains ( DistCpV 1  . Options . IGNORE _ READ _ FAILURES )  )  ;", "jobConf . setBoolean ( DistCpV 1  . Options . PRESERVE _ STATUS . propertyname ,    args . flags . contains ( DistCpV 1  . Options . PRESERVE _ STATUS )  )  ;", "final   String   randomId    =    DistCpV 1  . getRandomId (  )  ;", "JobClient   jClient    =    new   JobClient ( jobConf )  ;", "Path   stagingArea ;", "try    {", "stagingArea    =    JobSubmissionFiles . getStagingDir ( jClient . getClusterHandle (  )  ,    conf )  ;", "}    catch    ( InterruptedException   ie )     {", "throw   new   IOException ( ie )  ;", "}", "Path   jobDirectory    =    new   Path (  (  (  ( stagingArea    +     ( DistCpV 1  . NAME )  )     +     \"  _  \"  )     +    randomId )  )  ;", "FsPermission   mapredSysPerms    =    new   FsPermission ( JobSubmissionFiles . JOB _ DIR _ PERMISSION )  ;", "FileSystem . mkdirs ( jClient . getFs (  )  ,    jobDirectory ,    mapredSysPerms )  ;", "jobConf . set ( DistCpV 1  . JOB _ DIR _ LABEL ,    jobDirectory . toString (  )  )  ;", "long   maxBytesPerMap    =    conf . getLong ( DistCpV 1  . BYTES _ PER _ MAP _ LABEL ,    DistCpV 1  . BYTES _ PER _ MAP )  ;", "FileSystem   dstfs    =    args . dst . getFileSystem ( conf )  ;", "TokenCache . obtainTokensForNamenodes ( jobConf . getCredentials (  )  ,    new   Path [  ]  {    args . dst    }  ,    conf )  ;", "boolean   dstExists    =    dstfs . exists ( args . dst )  ;", "boolean   dstIsDir    =    false ;", "if    ( dstExists )     {", "dstIsDir    =    dstfs . getFileStatus ( args . dst )  . isDirectory (  )  ;", "}", "Path   logPath    =    args . log ;", "if    ( logPath    =  =    null )     {", "String   filename    =     \"  _ distcp _ logs _  \"     +    randomId ;", "if    (  (  ! dstExists )     |  |     (  ! dstIsDir )  )     {", "Path   parent    =    args . dst . getParent (  )  ;", "if    ( null    =  =    parent )     {", "parent    =    args . dst ;", "}", "if    (  !  ( dstfs . exists ( parent )  )  )     {", "dstfs . mkdirs ( parent )  ;", "}", "logPath    =    new   Path ( parent ,    filename )  ;", "} else    {", "logPath    =    new   Path ( args . dst ,    filename )  ;", "}", "}", "FileOutputFormat . setOutputPath ( jobConf ,    logPath )  ;", "FileSystem   jobfs    =    jobDirectory . getFileSystem ( jobConf )  ;", "Path   srcfilelist    =    new   Path ( jobDirectory ,     \"  _ distcp _ src _ files \"  )  ;", "jobConf . set ( DistCpV 1  . SRC _ LIST _ LABEL ,    srcfilelist . toString (  )  )  ;", "SequenceFile . Writer   src _ writer    =    SequenceFile . createWriter ( jobfs ,    jobConf ,    srcfilelist ,    LongWritable . class ,    DistCpV 1  . FilePair . class ,    NONE )  ;", "Path   dstfilelist    =    new   Path ( jobDirectory ,     \"  _ distcp _ dst _ files \"  )  ;", "SequenceFile . Writer   dst _ writer    =    SequenceFile . createWriter ( jobfs ,    jobConf ,    dstfilelist ,    Text . class ,    Text . class ,    NONE )  ;", "Path   dstdirlist    =    new   Path ( jobDirectory ,     \"  _ distcp _ dst _ dirs \"  )  ;", "jobConf . set ( DistCpV 1  . DST _ DIR _ LIST _ LABEL ,    dstdirlist . toString (  )  )  ;", "SequenceFile . Writer   dir _ writer    =    SequenceFile . createWriter ( jobfs ,    jobConf ,    dstdirlist ,    Text . class ,    DistCpV 1  . FilePair . class ,    NONE )  ;", "final   boolean   special    =     (  (  (  ( args . srcs . size (  )  )     =  =     1  )     &  &     (  ! dstExists )  )     |  |    update )     |  |    overwrite ;", "int   srcCount    =     0  ;", "int   cnsyncf    =     0  ;", "int   dirsyn    =     0  ;", "long   fileCount    =     0 L ;", "long   dirCount    =     0 L ;", "long   byteCount    =     0 L ;", "long   cbsyncs    =     0 L ;", "long   skipFileCount    =     0 L ;", "long   skipByteCount    =     0 L ;", "Path   basedir    =    null ;", "HashSet < Path >    parentDirsToCopy    =    new   HashSet < Path >  (  )  ;", "if    (  ( args . basedir )     !  =    null )     {", "FileSystem   basefs    =    args . basedir . getFileSystem ( conf )  ;", "basedir    =    args . basedir . makeQualified ( basefs )  ;", "if    (  !  ( basefs . isDirectory ( basedir )  )  )     {", "throw   new   IOException (  (  (  \" Basedir    \"     +    basedir )     +     \"    is   not   a   directory .  \"  )  )  ;", "}", "}", "try    {", "for    ( Iterator < Path >    srcItr    =    args . srcs . iterator (  )  ;    srcItr . hasNext (  )  ;  )     {", "final   Path   src    =    srcItr . next (  )  ;", "FileSystem   srcfs    =    src . getFileSystem ( conf )  ;", "FileStatus   srcfilestat    =    srcfs . getFileStatus ( src )  ;", "Path   root    =     ( special    &  &     ( srcfilestat . isDirectory (  )  )  )     ?    src    :    src . getParent (  )  ;", "if    (  ( dstExists    &  &     (  ! dstIsDir )  )     &  &     (  (  ( args . srcs . size (  )  )     >     1  )     |  |     ( srcfilestat . isDirectory (  )  )  )  )     {", "throw   new   IOException (  (  (  (  (  \" Destination    \"     +     ( args . dst )  )     +     \"    should   be   a   dir \"  )     +     \"    if   multiple   source   paths   are   there   OR   if \"  )     +     \"    the   source   path   is   a   dir \"  )  )  ;", "}", "if    ( basedir    !  =    null )     {", "root    =    basedir ;", "Path   parent    =    src . getParent (  )  . makeQualified ( srcfs )  ;", "while    (  ( parent    !  =    null )     &  &     (  !  ( parent . equals ( basedir )  )  )  )     {", "if    (  !  ( parentDirsToCopy . contains ( parent )  )  )     {", "parentDirsToCopy . add ( parent )  ;", "String   dst    =    DistCpV 1  . makeRelative ( root ,    parent )  ;", "FileStatus   pst    =    srcfs . getFileStatus ( parent )  ;", "src _ writer . append ( new   LongWritable (  0  )  ,    new   DistCpV 1  . FilePair ( pst ,    dst )  )  ;", "dst _ writer . append ( new   Text ( dst )  ,    new   Text ( parent . toString (  )  )  )  ;", "dir _ writer . append ( new   Text ( dst )  ,    new   DistCpV 1  . FilePair ( pst ,    dst )  )  ;", "if    (  (  +  + dirsyn )     >     ( DistCpV 1  . SYNC _ FILE _ MAX )  )     {", "dirsyn    =     0  ;", "dir _ writer . sync (  )  ;", "}", "}", "parent    =    parent . getParent (  )  ;", "}", "if    ( parent    =  =    null )     {", "throw   new   IOException (  (  (  (  \" Basedir    \"     +    basedir )     +     \"    is   not   a   prefix   of   source   path    \"  )     +    src )  )  ;", "}", "}", "if    ( srcfilestat . isDirectory (  )  )     {", "+  + srcCount ;", "final   String   dst    =    DistCpV 1  . makeRelative ( root ,    src )  ;", "if    (  (  ! update )     |  |     (  !  ( DistCpV 1  . dirExists ( conf ,    new   Path ( args . dst ,    dst )  )  )  )  )     {", "+  + dirCount ;", "src _ writer . append ( new   LongWritable (  0  )  ,    new   DistCpV 1  . FilePair ( srcfilestat ,    dst )  )  ;", "}", "dst _ writer . append ( new   Text ( dst )  ,    new   Text ( src . toString (  )  )  )  ;", "}", "Stack < FileStatus >    pathstack    =    new   Stack < FileStatus >  (  )  ;", "for    ( pathstack . push ( srcfilestat )  ;     !  ( pathstack . empty (  )  )  ;  )     {", "FileStatus   cur    =    pathstack . pop (  )  ;", "FileStatus [  ]    children    =    srcfs . listStatus ( cur . getPath (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( children . length )  ;    i +  +  )     {", "boolean   skipPath    =    false ;", "final   FileStatus   child    =    children [ i ]  ;", "final   String   dst    =    DistCpV 1  . makeRelative ( root ,    child . getPath (  )  )  ;", "+  + srcCount ;", "if    ( child . isDirectory (  )  )     {", "pathstack . push ( child )  ;", "if    (  (  ! update )     |  |     (  !  ( DistCpV 1  . dirExists ( conf ,    new   Path ( args . dst ,    dst )  )  )  )  )     {", "+  + dirCount ;", "} else    {", "skipPath    =    true ;", "}", "} else    {", "Path   destPath    =    new   Path ( args . dst ,    dst )  ;", "if    (  ( cur . isFile (  )  )     &  &     (  ( args . srcs . size (  )  )     =  =     1  )  )     {", "Path   dstparent    =    destPath . getParent (  )  ;", "FileSystem   destFileSys    =    destPath . getFileSystem ( jobConf )  ;", "if    (  !  (  ( destFileSys . exists ( dstparent )  )     &  &     ( destFileSys . getFileStatus ( dstparent )  . isDirectory (  )  )  )  )     {", "destPath    =    dstparent ;", "}", "}", "skipPath    =    update    &  &     ( DistCpV 1  . sameFile ( srcfs ,    child ,    dstfs ,    destPath ,    skipCRCCheck )  )  ;", "skipPath    |  =     ( fileCount    =  =     ( args . filelimit )  )     |  |     (  ( byteCount    +     ( child . getLen (  )  )  )     >     ( args . sizelimit )  )  ;", "if    (  ! skipPath )     {", "+  + fileCount ;", "byteCount    +  =    child . getLen (  )  ;", "if    ( DistCpV 1  . LOG . isTraceEnabled (  )  )     {", "DistCpV 1  . LOG . trace (  (  \" adding   file    \"     +     ( child . getPath (  )  )  )  )  ;", "}", "+  + cnsyncf ;", "cbsyncs    +  =    child . getLen (  )  ;", "if    (  ( cnsyncf    >     ( DistCpV 1  . SYNC _ FILE _ MAX )  )     |  |     ( cbsyncs    >    maxBytesPerMap )  )     {", "src _ writer . sync (  )  ;", "dst _ writer . sync (  )  ;", "cnsyncf    =     0  ;", "cbsyncs    =     0 L ;", "}", "} else    {", "+  + skipFileCount ;", "skipByteCount    +  =    child . getLen (  )  ;", "if    ( DistCpV 1  . LOG . isTraceEnabled (  )  )     {", "DistCpV 1  . LOG . trace (  (  \" skipping   file    \"     +     ( child . getPath (  )  )  )  )  ;", "}", "}", "}", "if    (  ! skipPath )     {", "src _ writer . append ( new   LongWritable (  ( child . isDirectory (  )     ?     0     :    child . getLen (  )  )  )  ,    new   DistCpV 1  . FilePair ( child ,    dst )  )  ;", "}", "dst _ writer . append ( new   Text ( dst )  ,    new   Text ( child . getPath (  )  . toString (  )  )  )  ;", "}", "if    ( cur . isDirectory (  )  )     {", "String   dst    =    DistCpV 1  . makeRelative ( root ,    cur . getPath (  )  )  ;", "dir _ writer . append ( new   Text ( dst )  ,    new   DistCpV 1  . FilePair ( cur ,    dst )  )  ;", "if    (  (  +  + dirsyn )     >     ( DistCpV 1  . SYNC _ FILE _ MAX )  )     {", "dirsyn    =     0  ;", "dir _ writer . sync (  )  ;", "}", "}", "}", "}", "}    finally    {", "DistCpV 1  . checkAndClose ( src _ writer )  ;", "DistCpV 1  . checkAndClose ( dst _ writer )  ;", "DistCpV 1  . checkAndClose ( dir _ writer )  ;", "}", "DistCpV 1  . LOG . info (  (  \" sourcePathsCount ( files + directories )  =  \"     +    srcCount )  )  ;", "DistCpV 1  . LOG . info (  (  \" filesToCopyCount =  \"     +    fileCount )  )  ;", "DistCpV 1  . LOG . info (  (  \" bytesToCopyCount =  \"     +     ( StringUtils . humanReadableInt ( byteCount )  )  )  )  ;", "if    ( update )     {", "DistCpV 1  . LOG . info (  (  \" filesToSkipCopyCount =  \"     +    skipFileCount )  )  ;", "DistCpV 1  . LOG . info (  (  \" bytesToSkipCopyCount =  \"     +     ( StringUtils . humanReadableInt ( skipByteCount )  )  )  )  ;", "}", "if    ( args . dryrun )     {", "return   false ;", "}", "int   mapCount    =    DistCpV 1  . setMapCount ( byteCount ,    jobConf )  ;", "DistCpV 1  . setReplication ( conf ,    jobConf ,    srcfilelist ,    mapCount )  ;", "FileStatus   dststatus    =    null ;", "try    {", "dststatus    =    dstfs . getFileStatus ( args . dst )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "DistCpV 1  . LOG . info (  (  ( args . dst )     +     \"    does   not   exist .  \"  )  )  ;", "}", "if    ( dststatus    =  =    null )     {", "if    (  ( srcCount    >     1  )     &  &     (  !  ( dstfs . mkdirs ( args . dst )  )  )  )     {", "throw   new   IOException (  (  \" Failed   to   create \"     +     ( args . dst )  )  )  ;", "}", "}", "final   Path   sorted    =    new   Path ( jobDirectory ,     \"  _ distcp _ sorted \"  )  ;", "DistCpV 1  . checkDuplication ( jobfs ,    dstfilelist ,    sorted ,    conf )  ;", "if    (  ( dststatus    !  =    null )     &  &     ( args . flags . contains ( DistCpV 1  . Options . DELETE )  )  )     {", "long   deletedPathsCount    =    DistCpV 1  . deleteNonexisting ( dstfs ,    dststatus ,    sorted ,    jobfs ,    jobDirectory ,    jobConf ,    conf )  ;", "DistCpV 1  . LOG . info (  (  \" deletedPathsFromDestCount ( files + directories )  =  \"     +    deletedPathsCount )  )  ;", "}", "Path   tmpDir    =    new   Path (  (  ( dstExists    &  &     (  ! dstIsDir )  )     |  |     (  (  ! dstExists )     &  &     ( srcCount    =  =     1  )  )     ?    args . dst . getParent (  )     :    args . dst )  ,     (  \"  _ distcp _ tmp _  \"     +    randomId )  )  ;", "jobConf . set ( DistCpV 1  . TMP _ DIR _ LABEL ,    tmpDir . toUri (  )  . toString (  )  )  ;", "tmpDir . getFileSystem ( conf )  . mkdirs ( tmpDir )  ;", "DistCpV 1  . LOG . info (  (  \" sourcePathsCount =  \"     +    srcCount )  )  ;", "DistCpV 1  . LOG . info (  (  \" filesToCopyCount =  \"     +    fileCount )  )  ;", "DistCpV 1  . LOG . info (  (  \" bytesToCopyCount =  \"     +     ( StringUtils . humanReadableInt ( byteCount )  )  )  )  ;", "jobConf . setInt ( DistCpV 1  . SRC _ COUNT _ LABEL ,    srcCount )  ;", "jobConf . setLong ( DistCpV 1  . TOTAL _ SIZE _ LABEL ,    byteCount )  ;", "return    ( fileCount    +    dirCount )     >     0  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "String   owner    =    null ;", "String   group    =    null ;", "if    (  ( preseved . contains (  . FileAttribute . USER )  )     &  &     (  !  ( src . getOwner (  )  . equals ( dst . getOwner (  )  )  )  )  )     {", "owner    =    src . getOwner (  )  ;", "}", "if    (  ( preseved . contains (  . FileAttribute . GROUP )  )     &  &     (  !  ( src . getGroup (  )  . equals ( dst . getGroup (  )  )  )  )  )     {", "group    =    src . getGroup (  )  ;", "}", "if    (  ( owner    !  =    null )     |  |     ( group    !  =    null )  )     {", "destFileSys . setOwner ( dst . getPath (  )  ,    owner ,    group )  ;", "}", "if    (  ( preseved . contains (  . FileAttribute . PERMISSION )  )     &  &     (  !  ( src . getPermission (  )  . equals ( dst . getPermission (  )  )  )  )  )     {", "destFileSys . setPermission ( dst . getPath (  )  ,    src . getPermission (  )  )  ;", "}", "if    ( preseved . contains (  . FileAttribute . TIMES )  )     {", "destFileSys . setTimes ( dst . getPath (  )  ,    src . getModificationTime (  )  ,    src . getAccessTime (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateDestStatus"], "fileName": "org.apache.hadoop.tools.DistCpV1"}, {"methodBody": ["METHOD_START", "{", "List < IOException >    ioes    =    new   ArrayList < IOException >  (  )  ;", "for    ( Path   p    :    srcs )     {", "try    {", "if    (  !  ( p . getFileSystem ( conf )  . exists ( p )  )  )     {", "ioes . add ( new   FileNotFoundException (  (  (  \" Source    \"     +    p )     +     \"    does   not   exist .  \"  )  )  )  ;", "}", "}    catch    ( IOException   e )     {", "ioes . add ( e )  ;", "}", "}", "if    (  !  ( ioes . isEmpty (  )  )  )     {", "throw   new   InvalidInputException ( ioes )  ;", "}", "}", "METHOD_END"], "methodName": ["checkSource"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "return   jobconf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "return   Integer . toString ( DistTool . RANDOM . nextInt ( Integer . MAX _ VALUE )  ,     3  6  )  ;", "}", "METHOD_END"], "methodName": ["getRandomId"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "List < String >    result    =    new   ArrayList < String >  (  )  ;", "FileSystem   fs    =    inputfile . getFileSystem ( conf )  ;", "BufferedReader   input    =    null ;", "try    {", "input    =    new   BufferedReader ( new   InputStreamReader ( fs . open ( inputfile )  )  )  ;", "for    ( String   line ;     ( line    =    input . readLine (  )  )     !  =    null ;  )     {", "result . add ( line )  ;", "}", "}    finally    {", "input . close (  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["readFile"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "if    ( in . readBoolean (  )  )     {", "return   Text . readString ( in )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "if    (  ( jobconf )     !  =    conf )     {", "jobconf    =     ( conf   inanceof   JobConf )     ?     (  ( JobConf )     ( conf )  )     :    new   JobConf ( conf )  ;", "}", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "boolean   b    =    s    !  =    null ;", "out . writeBoolean ( b )  ;", "if    ( b )     {", "Text . writeString ( out ,    s )  ;", "}", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.tools.DistTool"}, {"methodBody": ["METHOD_START", "{", "List < Path >    result    =    new   ArrayList < Path >  (  )  ;", "FileSystem   fs    =    source . getFileSystem ( getConf (  )  )  ;", "BufferedReader   input    =    null ;", "try    {", "input    =    new   BufferedReader ( new   InputStreamReader ( fs . open ( source )  )  )  ;", "String   line    =    input . readLine (  )  ;", "while    ( line    !  =    null )     {", "result . add ( new   Path ( line )  )  ;", "line    =    input . readLine (  )  ;", "}", "}    finally    {", "IOUtils . closeStream ( input )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["fetchFileList"], "fileName": "org.apache.hadoop.tools.FileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "srcWriter . append ( new   LongWritable ( len )  ,    new   HadoopArchives . HarEntry ( path ,    children )  )  ;", "}", "METHOD_END"], "methodName": ["append"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "HadoopArchives . checkPaths ( conf ,    srcPaths )  ;", "int   numFiles    =     0  ;", "long   totalSize    =     0  ;", "FileSystem   fs    =    parentPath . getFileSystem ( conf )  ;", "this . blockSize    =    conf . getLong ( HadoopArchives . HAR _ BLOCKSIZE _ LABEL ,    blockSize )  ;", "this . partSize    =    conf . getLong ( HadoopArchives . HAR _ PARTSIZE _ LABEL ,    partSize )  ;", "conf . setLong ( HadoopArchives . HAR _ BLOCKSIZE _ LABEL ,    blockSize )  ;", "conf . setLong ( HadoopArchives . HAR _ PARTSIZE _ LABEL ,    partSize )  ;", "conf . set ( HadoopArchives . DST _ HAR _ LABEL ,    archiveName )  ;", "conf . set ( HadoopArchives . SRC _ PARENT _ LABEL ,    parentPath . makeQualified ( fs )  . toString (  )  )  ;", "Path   outputPath    =    new   Path ( dest ,    archiveName )  ;", "FileOutputFormat . setOutputPath ( conf ,    outputPath )  ;", "FileSystem   outFs    =    outputPath . getFileSystem ( conf )  ;", "if    (  ( outFs . exists ( outputPath )  )     |  |     ( outFs . isFile ( dest )  )  )     {", "throw   new   IOException (  (  \" Invalid   Output :     \"     +    outputPath )  )  ;", "}", "conf . set ( HadoopArchives . DST _ DIR _ LABEL ,    outputPath . toString (  )  )  ;", "Path   stagingArea ;", "try    {", "stagingArea    =    JobSubmissionFiles . getStagingDir ( new   mapreduce . Cluster ( conf )  ,    conf )  ;", "}    catch    ( InterruptedException   ie )     {", "throw   new   IOException ( ie )  ;", "}", "Path   jobDirectory    =    new   Path ( stagingArea ,     (  (  ( HadoopArchives . NAME )     +     \"  _  \"  )     +     ( Integer . toString ( new   Random (  )  . nextInt ( Integer . MAX _ VALUE )  ,     3  6  )  )  )  )  ;", "FsPermission   mapredSysPerms    =    new   FsPermission ( JobSubmissionFiles . JOB _ DIR _ PERMISSION )  ;", "FileSystem . mkdirs ( jobDirectory . getFileSystem ( conf )  ,    jobDirectory ,    mapredSysPerms )  ;", "conf . set ( HadoopArchives . JOB _ DIR _ LABEL ,    jobDirectory . toString (  )  )  ;", "FileSystem   jobfs    =    jobDirectory . getFileSystem ( conf )  ;", "Path   srcFiles    =    new   Path ( jobDirectory ,     \"  _ har _ src _ files \"  )  ;", "conf . set ( HadoopArchives . SRC _ LIST _ LABEL ,    srcFiles . toString (  )  )  ;", "SequenceFile . Writer   srcWriter    =    SequenceFile . createWriter ( jobfs ,    conf ,    srcFiles ,    LongWritable . class ,    HadoopArchives . HarEntry . class ,    NONE )  ;", "try    {", "writeTopLevelDirs ( srcWriter ,    srcPaths ,    parentPath )  ;", "srcWriter . sync (  )  ;", "for    ( Path   src    :    srcPaths )     {", "ArrayList < HadoopArchives . FileStatusDir >    allFiles    =    new   ArrayList < HadoopArchives . FileStatusDir >  (  )  ;", "FileStatus   fstatus    =    fs . getFileStatus ( src )  ;", "HadoopArchives . FileStatusDir   fdir    =    new   HadoopArchives . FileStatusDir ( fstatus ,    null )  ;", "recursivels ( fs ,    fdir ,    allFiles )  ;", "for    ( HadoopArchives . FileStatusDir   statDir    :    allFiles )     {", "FileStatus   stat    =    statDir . getFileStatus (  )  ;", "long   len    =     ( stat . isDirectory (  )  )     ?     0     :    stat . getLen (  )  ;", "final   Path   path    =    relPathToRoot ( stat . getPath (  )  ,    parentPath )  ;", "final   String [  ]    children ;", "if    ( stat . isDirectory (  )  )     {", "FileStatus [  ]    list    =    statDir . getChildren (  )  ;", "children    =    new   String [ list . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( list . length )  ;    i +  +  )     {", "children [ i ]     =    list [ i ]  . getPath (  )  . getName (  )  ;", "}", "} else    {", "children    =    null ;", "}", "append ( srcWriter ,    len ,    path . toString (  )  ,    children )  ;", "srcWriter . sync (  )  ;", "numFiles +  +  ;", "totalSize    +  =    len ;", "}", "}", "}    finally    {", "srcWriter . close (  )  ;", "}", "jobfs . setReplication ( srcFiles ,     (  ( short )     (  1  0  )  )  )  ;", "conf . setInt ( HadoopArchives . SRC _ COUNT _ LABEL ,    numFiles )  ;", "conf . setLong ( HadoopArchives . TOTAL _ SIZE _ LABEL ,    totalSize )  ;", "int   numMaps    =     (  ( int )     ( totalSize    /     ( partSize )  )  )  ;", "conf . setNumMapTasks (  ( numMaps    =  =     0     ?     1     :    numMaps )  )  ;", "conf . setNumReduceTasks (  1  )  ;", "conf . setInputFormat ( HadoopArchives . HArchiveInputFormat . class )  ;", "conf . setOutputFormat ( NullOutputFormat . class )  ;", "conf . setMapperClass ( HadoopArchives . HArchivesMapper . class )  ;", "conf . setReducerClass ( HadoopArchives . HArchivesReducer . class )  ;", "conf . setMapOutputKeyClass ( IntWritable . class )  ;", "conf . setMapOutputValueClass ( Text . class )  ;", "FileInputFormat . addInputPath ( conf ,    jobDirectory )  ;", "conf . setSpeculativeExecution ( false )  ;", "JobClient . runJob ( conf )  ;", "try    {", "jobfs . delete ( jobDirectory ,    true )  ;", "}    catch    ( IOException   ie )     {", "HadoopArchives . LOG . info (  (  \" Unable   to   clean   tmp   directory    \"     +    jobDirectory )  )  ;", "}", "}", "METHOD_END"], "methodName": ["archive"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "for    ( Path   p    :    paths )     {", "FileSystem   fs    =    p . getFileSystem ( conf )  ;", "if    (  !  ( fs . exists ( p )  )  )     {", "throw   new   FileNotFoundException (  (  (  \" Source    \"     +    p )     +     \"    does   not   exist .  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["checkPaths"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "Path   tmp    =    new   Path ( name )  ;", "if    (  ( tmp . depth (  )  )     !  =     1  )     {", "return   false ;", "}", "if    ( name . endsWith (  \" r \"  )  )", "return   true ;", "return   false ;", "}", "METHOD_END"], "methodName": ["checkValidName"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "return   this . conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "Path   deepest    =    paths . get (  0  )  ;", "for    ( Path   p    :    paths )     {", "if    (  ( p . depth (  )  )     >     ( deepest . depth (  )  )  )     {", "deepest    =    p ;", "}", "}", "return   deepest ;", "}", "METHOD_END"], "methodName": ["largestDepth"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "JobConf   job    =    new   JobConf ( HadoopArchives . class )  ;", "HadoopArchives   harchives    =    new   HadoopArchives ( job )  ;", "int   ret    =     0  ;", "try    {", "ret    =    ToolRunner . run ( harchives ,    args )  ;", "}    catch    ( Exception   e )     {", "HadoopArchives . LOG . debug (  \" Exception   in   archives       \"  ,    e )  ;", "System . err . println (  (  ( e . getClass (  )  . getSimpleName (  )  )     +     \"    in   archives \"  )  )  ;", "final   String   s    =    e . getLocalizedMessage (  )  ;", "if    ( s    !  =    null )     {", "System . err . println ( s )  ;", "} else    {", "e . printStackTrace ( System . err )  ;", "}", "System . exit (  1  )  ;", "}", "System . exit ( ret )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "if    ( fdir . getFileStatus (  )  . isFile (  )  )     {", "out . add ( fdir )  ;", "return ;", "} else    {", "out . add ( fdir )  ;", "FileStatus [  ]    listStatus    =    fs . listStatus ( fdir . getFileStatus (  )  . getPath (  )  )  ;", "fdir . setChildren ( listStatus )  ;", "for    ( FileStatus   stat    :    listStatus )     {", ". FileStatusDir   fstatDir    =    new    . FileStatusDir ( stat ,    null )  ;", "recursivels ( fs ,    fstatDir ,    out )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["recursivels"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   Path   justRoot    =    new   Path ( Path . SEPARATOR )  ;", "if    (  ( fullPath . depth (  )  )     =  =     ( root . depth (  )  )  )     {", "return   justRoot ;", "} else", "if    (  ( fullPath . depth (  )  )     >     ( root . depth (  )  )  )     {", "Path   retPath    =    new   Path ( fullPath . getName (  )  )  ;", "Path   parent    =    fullPath . getParent (  )  ;", "for    ( int   i    =     0  ;    i    <     (  (  ( fullPath . depth (  )  )     -     ( root . depth (  )  )  )     -     1  )  ;    i +  +  )     {", "retPath    =    new   Path ( parent . getName (  )  ,    retPath )  ;", "parent    =    parent . getParent (  )  ;", "}", "return   new   Path ( justRoot ,    retPath )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["relPathToRoot"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   parentPath    =    null ;", "List < Path >    srcPaths    =    new   ArrayList < Path >  (  )  ;", "Path   destPath    =    null ;", "String   archiveName    =    null ;", "if    (  ( args . length )     <     5  )     {", "System . out . println (  . usage )  ;", "throw   new   IOException (  \" Invalid   usage .  \"  )  ;", "}", "if    (  !  (  \"  - archiveName \"  . equals ( args [  0  ]  )  )  )     {", "System . out . println (  . usage )  ;", "throw   new   IOException (  \" Archive   Name   not   specified .  \"  )  ;", "}", "archiveName    =    args [  1  ]  ;", "if    (  !  ( checkValidName ( archiveName )  )  )     {", "System . out . println (  . usage )  ;", "throw   new   IOException (  (  \" Invalid   name   for   archives .     \"     +    archiveName )  )  ;", "}", "int   i    =     2  ;", "if    (  !  (  \"  - p \"  . equals ( args [ i ]  )  )  )     {", "System . out . println (  . usage )  ;", "throw   new   IOException (  \" Parent   path   not   specified .  \"  )  ;", "}", "parentPath    =    new   Path ( args [  ( i    +     1  )  ]  )  ;", "if    (  !  ( parentPath . isAbsolute (  )  )  )     {", "parentPath    =    parentPath . getFileSystem ( getConf (  )  )  . makeQualified ( parentPath )  ;", "}", "i    +  =     2  ;", "for    (  ;    i    <     ( args . length )  ;    i +  +  )     {", "if    ( i    =  =     (  ( args . length )     -     1  )  )     {", "destPath    =    new   Path ( args [ i ]  )  ;", "if    (  !  ( destPath . isAbsolute (  )  )  )     {", "destPath    =    destPath . getFileSystem ( getConf (  )  )  . makeQualified ( destPath )  ;", "}", "} else    {", "Path   argPath    =    new   Path ( args [ i ]  )  ;", "if    ( argPath . isAbsolute (  )  )     {", "System . out . println (  . usage )  ;", "throw   new   IOException (  (  (  (  \" source   path    \"     +    argPath )     +     \"    is   not   relative      to    \"  )     +    parentPath )  )  ;", "}", "srcPaths . add ( new   Path ( parentPath ,    argPath )  )  ;", "}", "}", "if    (  ( srcPaths . size (  )  )     =  =     0  )     {", "srcPaths . add ( parentPath )  ;", "}", "List < Path >    globPaths    =    new   ArrayList < Path >  (  )  ;", "for    ( Path   p    :    srcPaths )     {", "FileSystem   fs    =    p . getFileSystem ( getConf (  )  )  ;", "FileStatus [  ]    statuses    =    fs . globStatus ( p )  ;", "if    ( statuses    !  =    null )     {", "for    ( FileStatus   status    :    statuses )     {", "globPaths . add ( fs . makeQualified ( status . getPath (  )  )  )  ;", "}", "}", "}", "if    ( globPaths . isEmpty (  )  )     {", "throw   new   IOException (  (  (  \" The   resolved   paths   set   is   empty .  \"     +     \"       Please   check   whether   the   srcPaths   exist ,    where   srcPaths    =     \"  )     +    srcPaths )  )  ;", "}", "archive ( parentPath ,    globPaths ,    archiveName ,    destPath )  ;", "}    catch    ( IOException   ie )     {", "System . err . println ( ie . getLocalizedMessage (  )  )  ;", "return    -  1  ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "if    ( conf   instanceof   JobConf )     {", "this . conf    =     (  ( JobConf )     ( conf )  )  ;", "} else    {", "this . conf    =    new   JobConf ( conf ,     . class )  ;", "}", "String   testJar    =    System . getProperty (  . TEST _ HADOOP _ ARCHIVES _ JAR _ PATH ,    null )  ;", "if    ( testJar    !  =    null )     {", "this . conf . setJar ( testJar )  ;", "}", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "List < Path >    justDirs    =    new   ArrayList < Path >  (  )  ;", "for    ( Path   p    :    paths )     {", "if    (  !  ( p . getFileSystem ( getConf (  )  )  . isFile ( p )  )  )     {", "justDirs . add ( new   Path ( Uri (  )  . getPath (  )  )  )  ;", "} else    {", "justDirs . add ( new   Path ( p . getParent (  )  . toUri (  )  . getPath (  )  )  )  ;", "}", "}", "TreeMap < String ,    HashSet < String >  >    allpaths    =    new   TreeMap < String ,    HashSet < String >  >  (  )  ;", "Path   deepest    =    largestDepth ( paths )  ;", "Path   root    =    new   Path ( Path . SEPARATOR )  ;", "for    ( int   i    =    parentPath . depth (  )  ;    i    <     ( deepest . depth (  )  )  ;    i +  +  )     {", "List < Path >    parents    =    new   ArrayList < Path >  (  )  ;", "for    ( Path   p    :    justDirs )     {", "if    (  ( p . compareTo ( root )  )     =  =     0  )     {", "} else    {", "Path   parent    =    p . getParent (  )  ;", "if    ( null    !  =    parent )     {", "if    ( allpaths . containsKey ( parent . toString (  )  )  )     {", "HashSet < String >    children    =    allpaths . get ( parent . toString (  )  )  ;", "children . add ( p . getName (  )  )  ;", "} else    {", "HashSet < String >    children    =    new   HashSet < String >  (  )  ;", "children . add ( p . getName (  )  )  ;", "allpaths . put ( parent . toString (  )  ,    children )  ;", "}", "parents . add ( parent )  ;", "}", "}", "}", "justDirs    =    parents ;", "}", "Set < Map . Entry < String ,    HashSet < String >  >  >    keyVals    =    allpaths . entrySet (  )  ;", "for    ( Map . Entry < String ,    HashSet < String >  >    entry    :    keyVals )     {", "final   Path   relPath    =    relPathToRoot ( new   Path ( entry . getKey (  )  )  ,    parentPath )  ;", "if    ( relPath    !  =    null )     {", "final   String [  ]    children    =    new   String [ entry . getValue (  )  . size (  )  ]  ;", "int   i    =     0  ;", "for    ( String   child    :    entry . getValue (  )  )     {", "children [  ( i +  +  )  ]     =    child ;", "}", "append ( srcWriter ,     0 L ,    relPath . toString (  )  ,    children )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["writeTopLevelDirs"], "fileName": "org.apache.hadoop.tools.HadoopArchives"}, {"methodBody": ["METHOD_START", "{", "Path   grepInput    =    new   Path ( inputFilesDirectory )  ;", "Path   analysisOutput    =    null ;", "if    ( outputDirectory . equals (  \"  \"  )  )     {", "analysisOutput    =    new   Path ( inputFilesDirectory ,     (  \" logalyzer _  \"     +     ( Integer . toString ( new   Random (  )  . nextInt ( Integer . MAX _ VALUE )  )  )  )  )  ;", "} else    {", "analysisOutput    =    new   Path ( outputDirectory )  ;", "}", "JobConf   grepJob    =    new   JobConf (  . fsConfig )  ;", "grepJob . setJobName (  \" logalyzer - grep - sort \"  )  ;", "FileInputFormat . setInputPaths ( grepJob ,    grepInput )  ;", "grepJob . setInputFormat ( TextInputFormat . class )  ;", "grepJob . setMapperClass (  . LogRegexMapper . class )  ;", "grepJob . set ( PATTERN ,    grepPattern )  ;", "grepJob . set (  . SORT _ COLUMNS ,    sortColumns )  ;", "grepJob . set (  . COLUMN _ SEPARATOR ,    columnSeparator )  ;", "grepJob . setCombinerClass ( LongSumReducer . class )  ;", "grepJob . setReducerClass ( LongSumReducer . class )  ;", "FileOutputFormat . setOutputPath ( grepJob ,    analysisOutput )  ;", "grepJob . setOutputFormat ( TextOutputFormat . class )  ;", "grepJob . setOutputKeyClass ( Text . class )  ;", "grepJob . setOutputValueClass ( LongWritable . class )  ;", "grepJob . setOutputKeyComparatorClass (  . LogComparator . class )  ;", "grepJob . setNumReduceTasks (  1  )  ;", "JobClient . runJob ( grepJob )  ;", "}", "METHOD_END"], "methodName": ["doAnalyze"], "fileName": "org.apache.hadoop.tools.Logalyzer"}, {"methodBody": ["METHOD_START", "{", "String   destURL    =     ( FileSystem . getDefaultUri ( Logalyzer . fsConfig )  )     +    archiveDirectory ;", "DistCpV 1  . copy ( new   mapred . JobConf ( Logalyzer . fsConfig )  ,    logListURI ,    destURL ,    null ,    true ,    false )  ;", "}", "METHOD_END"], "methodName": ["doArchive"], "fileName": "org.apache.hadoop.tools.Logalyzer"}, {"methodBody": ["METHOD_START", "{", "Log   LOG    =    LogFactory . getLog ( Logalyzer . class )  ;", "String   version    =     \" Logalyzer .  0  .  0  .  1  \"  ;", "String   usage    =     \" Usage :    Logalyzer    [  - archive    - logs    < urlsFile >  ]     \"     +     (  (  \"  - archiveDir    < archiveDirectory >     \"     +     \"  - grep    < pattern >     - sort    < column 1  , column 2  ,  .  .  .  >     - separator    < separator >     \"  )     +     \"  - analysis    < outputDirectory >  \"  )  ;", "System . out . println ( version )  ;", "if    (  ( args . length )     =  =     0  )     {", "System . err . println ( usage )  ;", "System . exit (  (  -  1  )  )  ;", "}", "boolean   archive    =    false ;", "boolean   grep    =    false ;", "boolean   sort    =    false ;", "String   archiveDir    =     \"  \"  ;", "String   logListURI    =     \"  \"  ;", "String   grepPattern    =     \"  .  *  \"  ;", "String   sortColumns    =     \"  \"  ;", "String   columnSeparator    =     \"     \"  ;", "String   outputDirectory    =     \"  \"  ;", "for    ( int   i    =     0  ;    i    <     ( args . length )  ;    i +  +  )     {", "if    ( args [ i ]  . equals (  \"  - archive \"  )  )     {", "archive    =    true ;", "} else", "if    ( args [ i ]  . equals (  \"  - archiveDir \"  )  )     {", "archiveDir    =    args [  (  +  + i )  ]  ;", "} else", "if    ( args [ i ]  . equals (  \"  - grep \"  )  )     {", "grep    =    true ;", "grepPattern    =    args [  (  +  + i )  ]  ;", "} else", "if    ( args [ i ]  . equals (  \"  - logs \"  )  )     {", "logListURI    =    args [  (  +  + i )  ]  ;", "} else", "if    ( args [ i ]  . equals (  \"  - sort \"  )  )     {", "sort    =    true ;", "sortColumns    =    args [  (  +  + i )  ]  ;", "} else", "if    ( args [ i ]  . equals (  \"  - separator \"  )  )     {", "columnSeparator    =    args [  (  +  + i )  ]  ;", "} else", "if    ( args [ i ]  . equals (  \"  - analysis \"  )  )     {", "outputDirectory    =    args [  (  +  + i )  ]  ;", "}", "}", "LOG . info (  (  \" analysisDir    =     \"     +    outputDirectory )  )  ;", "LOG . info (  (  \" archiveDir    =     \"     +    archiveDir )  )  ;", "LOG . info (  (  \" logListURI    =     \"     +    logListURI )  )  ;", "LOG . info (  (  \" grepPattern    =     \"     +    grepPattern )  )  ;", "LOG . info (  (  \" sortColumns    =     \"     +    sortColumns )  )  ;", "LOG . info (  (  \" separator    =     \"     +    columnSeparator )  )  ;", "try    {", "Logalyzer   logalyzer    =    new   Logalyzer (  )  ;", "if    ( archive )     {", "logalyzer . doArchive ( logListURI ,    archiveDir )  ;", "}", "if    ( grep    |  |    sort )     {", "logalyzer . doAnalyze ( archiveDir ,    outputDirectory ,    grepPattern ,    sortColumns ,    columnSeparator )  ;", "}", "}    catch    ( IOException   ioe )     {", "ioe . printStackTrace (  )  ;", "System . exit (  (  -  1  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.Logalyzer"}, {"methodBody": ["METHOD_START", "{", "String   optionValue    =    command . getOptionValue ( swtch )  ;", "if    ( optionValue    =  =    null )     {", "return   null ;", "} else    {", "return   optionValue . trim (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getVal"], "fileName": "org.apache.hadoop.tools.OptionsParser"}, {"methodBody": ["METHOD_START", "{", "CommandLineParser   parser    =    new   OptionsParser . CustomParser (  )  ;", "CommandLine   command ;", "try    {", "command    =    parser . parse ( OptionsParser . cliOptions ,    args ,    true )  ;", "}    catch    ( ParseException   e )     {", "throw   new   IllegalArgumentException (  (  \" Unable   to   parse   arguments .     \"     +     ( Arrays . toString ( args )  )  )  ,    e )  ;", "}", "DistCpOptions   option ;", "Path   targetPath ;", "List < Path >    sourcePaths    =    new   ArrayList < Path >  (  )  ;", "String [  ]    leftOverArgs    =    command . getArgs (  )  ;", "if    (  ( leftOverArgs    =  =    null )     |  |     (  ( leftOverArgs . length )     <     1  )  )     {", "throw   new   IllegalArgumentException (  \" Target   path   not   specified \"  )  ;", "}", "targetPath    =    new   Path ( leftOverArgs [  (  ( leftOverArgs . length )     -     1  )  ]  . trim (  )  )  ;", "for    ( int   index    =     0  ;    index    <     (  ( leftOverArgs . length )     -     1  )  ;    index +  +  )     {", "sourcePaths . add ( new   Path ( leftOverArgs [ index ]  . trim (  )  )  )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . SOURCE _ FILE _ LISTING . getSwitch (  )  )  )     {", "if    (  !  ( sourcePaths . isEmpty (  )  )  )     {", "throw   new   IllegalArgumentException (  \" Both   source   file   listing   and   source   paths   present \"  )  ;", "}", "option    =    new   DistCpOptions ( new   Path ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . SOURCE _ FILE _ LISTING . getSwitch (  )  )  )  ,    targetPath )  ;", "} else    {", "if    ( sourcePaths . isEmpty (  )  )     {", "throw   new   IllegalArgumentException (  \" Neither   source   file   listing   nor   source   paths   present \"  )  ;", "}", "option    =    new   DistCpOptions ( sourcePaths ,    targetPath )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . IGNORE _ FAILURES . getSwitch (  )  )  )     {", "option . setIgnoreFailures ( true )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . ATOMIC _ COMMIT . getSwitch (  )  )  )     {", "option . setAtomicCommit ( true )  ;", "}", "if    (  ( command . hasOption ( DistCpOptionSwitch . WORK _ PATH . getSwitch (  )  )  )     &  &     ( option . shouldAtomicCommit (  )  )  )     {", "String   workPath    =    OptionsParser . getVal ( command ,    DistCpOptionSwitch . WORK _ PATH . getSwitch (  )  )  ;", "if    (  ( workPath    !  =    null )     &  &     (  !  ( workPath . isEmpty (  )  )  )  )     {", "option . setAtomicWorkPath ( new   Path ( workPath )  )  ;", "}", "} else", "if    ( command . hasOption ( DistCpOptionSwitch . WORK _ PATH . getSwitch (  )  )  )     {", "throw   new   IllegalArgumentException (  \"  - tmp   work - path   can   only   be   specified   along   with    - atomic \"  )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . LOG _ PATH . getSwitch (  )  )  )     {", "option . setLogPath ( new   Path ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . LOG _ PATH . getSwitch (  )  )  )  )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . SYNC _ FOLDERS . getSwitch (  )  )  )     {", "option . setSyncFolder ( true )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . OVERWRITE . getSwitch (  )  )  )     {", "option . setOverwrite ( true )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . APPEND . getSwitch (  )  )  )     {", "option . setAppend ( true )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . DELETE _ MISSING . getSwitch (  )  )  )     {", "option . setDeleteMissing ( true )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . SKIP _ CRC . getSwitch (  )  )  )     {", "option . setSkipCRC ( true )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . BLOCKING . getSwitch (  )  )  )     {", "option . setBlocking ( false )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . BANDWIDTH . getSwitch (  )  )  )     {", "try    {", "Integer   mapBandwidth    =    Integer . parseInt ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . BANDWIDTH . getSwitch (  )  )  . trim (  )  )  ;", "if    (  ( mapBandwidth . intValue (  )  )     <  =     0  )     {", "throw   new   IllegalArgumentException (  (  \" Bandwidth   specified   is   not   positive :     \"     +    mapBandwidth )  )  ;", "}", "option . setMapBandwidth ( mapBandwidth )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   IllegalArgumentException (  (  \" Bandwidth   specified   is   invalid :     \"     +     ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . BANDWIDTH . getSwitch (  )  )  )  )  ,    e )  ;", "}", "}", "if    ( command . hasOption ( DistCpOptionSwitch . SSL _ CONF . getSwitch (  )  )  )     {", "option . setSslConfigurationFile ( command . getOptionValue ( DistCpOptionSwitch . SSL _ CONF . getSwitch (  )  )  )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . MAX _ MAPS . getSwitch (  )  )  )     {", "try    {", "Integer   maps    =    Integer . parseInt ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . MAX _ MAPS . getSwitch (  )  )  . trim (  )  )  ;", "option . setMaxMaps ( maps )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   IllegalArgumentException (  (  \" Number   of   maps   is   invalid :     \"     +     ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . MAX _ MAPS . getSwitch (  )  )  )  )  ,    e )  ;", "}", "}", "if    ( command . hasOption ( DistCpOptionSwitch . COPY _ STRATEGY . getSwitch (  )  )  )     {", "option . setCopyStrategy ( OptionsParser . getVal ( command ,    DistCpOptionSwitch . COPY _ STRATEGY . getSwitch (  )  )  )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . PRESERVE _ STATUS . getSwitch (  )  )  )     {", "String   attributes    =    OptionsParser . getVal ( command ,    DistCpOptionSwitch . PRESERVE _ STATUS . getSwitch (  )  )  ;", "if    (  ( attributes    =  =    null )     |  |     ( attributes . isEmpty (  )  )  )     {", "for    ( DistCpOptions . FileAttribute   attribute    :    DistCpOptions . FileAttribute . values (  )  )     {", "option . preserve ( attribute )  ;", "}", "} else    {", "for    ( int   index    =     0  ;    index    <     ( attributes . length (  )  )  ;    index +  +  )     {", "option . preserve ( DistCpOptions . FileAttribute . getAttribute ( attributes . charAt ( index )  )  )  ;", "}", "}", "}", "if    ( command . hasOption ( DistCpOptionSwitch . FILE _ LIMIT . getSwitch (  )  )  )     {", "String   fileLimitString    =    OptionsParser . getVal ( command ,    DistCpOptionSwitch . FILE _ LIMIT . getSwitch (  )  . trim (  )  )  ;", "try    {", "Integer . parseInt ( fileLimitString )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   IllegalArgumentException (  (  \" File - limit   is   invalid :     \"     +    fileLimitString )  ,    e )  ;", "}", "OptionsParser . LOG . warn (  (  (  ( DistCpOptionSwitch . FILE _ LIMIT . getSwitch (  )  )     +     \"    is   a   deprecated \"  )     +     \"    option .    Ignoring .  \"  )  )  ;", "}", "if    ( command . hasOption ( DistCpOptionSwitch . SIZE _ LIMIT . getSwitch (  )  )  )     {", "String   sizeLimitString    =    OptionsParser . getVal ( command ,    DistCpOptionSwitch . SIZE _ LIMIT . getSwitch (  )  . trim (  )  )  ;", "try    {", "Long . parseLong ( sizeLimitString )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   IllegalArgumentException (  (  \" Size - limit   is   invalid :     \"     +    sizeLimitString )  ,    e )  ;", "}", "OptionsParser . LOG . warn (  (  (  ( DistCpOptionSwitch . SIZE _ LIMIT . getSwitch (  )  )     +     \"    is   a   deprecated \"  )     +     \"    option .    Ignoring .  \"  )  )  ;", "}", "return   option ;", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.apache.hadoop.tools.OptionsParser"}, {"methodBody": ["METHOD_START", "{", "HelpFormatter   formatter    =    new   HelpFormatter (  )  ;", "formatter . printHelp (  \" distcp   OPTIONS    [ source _ path .  .  .  ]     < target _ path >  \\ n \\ nOPTIONS \"  ,     . cliOptions )  ;", "}", "METHOD_END"], "methodName": ["usage"], "fileName": "org.apache.hadoop.tools.OptionsParser"}, {"methodBody": ["METHOD_START", "{", "Path   target    =    options . getTargetPath (  )  ;", "FileSystem   targetFS    =    target . getFileSystem ( getConf (  )  )  ;", "final   bean   targetPathExists    =    options . getTargetPathExists (  )  ;", "bean   solitaryFile    =     (  ( options . getSourcePaths (  )  . size (  )  )     =  =     1  )     &  &     (  !  ( sourceStatus . isDirectory (  )  )  )  ;", "if    ( solitaryFile )     {", "if    (  ( targetFS . isFile ( target )  )     |  |     (  ! targetPathExists )  )     {", "return   sourceStatus . getPath (  )  ;", "} else    {", "return   sourceStatus . getPath (  )  . getParent (  )  ;", "}", "} else    {", "bean   specialHandling    =     (  (  (  ( options . getSourcePaths (  )  . size (  )  )     =  =     1  )     &  &     (  ! targetPathExists )  )     |  |     ( options . shouldSyncFolder (  )  )  )     |  |     ( options . shouldOverwrite (  )  )  ;", "return   specialHandling    &  &     ( sourceStatus . isDirectory (  )  )     ?    sourceStatus . getPath (  )     :    sourceStatus . getPath (  )  . getParent (  )  ;", "}", "}", "METHOD_END"], "methodName": ["computeSourceRootPath"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "for    ( Path   path    :    options . getSourcePaths (  )  )     {", "FileSystem   sourceFS    =    path . getFileSystem ( getConf (  )  )  ;", "final   boolean   preserveAcls    =    options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  ;", "final   boolean   preserveXAttrs    =    options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  ;", "final   boolean   preserveRawXAttrs    =    options . shouldPreserveRawXattrs (  )  ;", "path    =    makeQualified ( path )  ;", "FileStatus   rootStatus    =    sourceFS . getFileStatus ( path )  ;", "Path   sourcePathRoot    =    computeSourceRootPath ( rootStatus ,    options )  ;", "FileStatus [  ]    sourceFiles    =    sourceFS . listStatus ( path )  ;", "boolean   explore    =     ( sourceFiles    !  =    null )     &  &     (  ( sourceFiles . length )     >     0  )  ;", "if    (  (  ! explore )     |  |     ( rootStatus . isDirectory (  )  )  )     {", "CopyListingFileStatus   rootCopyListingStatus    =    DistCpUtils . toCopyListingFileStatus ( sourceFS ,    rootStatus ,    preserveAcls ,    preserveXAttrs ,    preserveRawXAttrs )  ;", "writeToFileListingRoot ( fileListWriter ,    rootCopyListingStatus ,    sourcePathRoot ,    options )  ;", "}", "if    ( explore )     {", "for    ( FileStatus   sourceStatus    :    sourceFiles )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  \" Recording   source - path :     \"     +     ( sourceStatus . getPath (  )  )  )     +     \"    for   copy .  \"  )  )  ;", "}", "CopyListingFileStatus   sourceCopyListingStatus    =    DistCpUtils . toCopyListingFileStatus ( sourceFS ,    sourceStatus ,     ( preserveAcls    &  &     ( sourceStatus . isDirectory (  )  )  )  ,     ( preserveXAttrs    &  &     ( sourceStatus . isDirectory (  )  )  )  ,     ( preserveRawXAttrs    &  &     ( sourceStatus . isDirectory (  )  )  )  )  ;", "writeToFileListing ( fileListWriter ,    sourceCopyListingStatus ,    sourcePathRoot ,    options )  ;", "if    (  . isDirectoryAndNotEmpty ( sourceFS ,    sourceStatus )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Traversing   non - empty   source   dir :     \"     +     ( sourceStatus . getPath (  )  )  )  )  ;", "}", "traverseNonEmptyDirectory ( fileListWriter ,    sourceStatus ,    sourcePathRoot ,    options )  ;", "}", "}", "}", "}", "fileListWriter . close (  )  ;", "fileListWriter    =    null ;", "}    finally    {", "IOUtils . cleanup (  . LOG ,    fileListWriter )  ;", "}", "}", "METHOD_END"], "methodName": ["doBuildListing"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "return   fileSystem . listStatus ( parent . getPath (  )  )  ;", "}", "METHOD_END"], "methodName": ["getChildren"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    pathToListFile . getFileSystem ( getConf (  )  )  ;", "if    ( fs . exists ( pathToListFile )  )     {", "fs . delete ( pathToListFile ,    false )  ;", "}", "return   SequenceFile . createWriter ( getConf (  )  ,    Writer . file ( pathToListFile )  ,    Writer . keyClass ( Text . class )  ,    Writer . valueClass ( FileStatus . class )  ,    Writer . compression ( NONE )  )  ;", "}", "METHOD_END"], "methodName": ["getWriter"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "return    ( fileStatus . isDirectory (  )  )     &  &     (  ( SimpleCopyListing . getChildren ( fileSystem ,    fileStatus )  . length )     >     0  )  ;", "}", "METHOD_END"], "methodName": ["isDirectoryAndNotEmpty"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "final   FileSystem   fs    =    path . getFileSystem ( getConf (  )  )  ;", "return   path . makeQualified ( fs . getUri (  )  ,    fs . getWorkDirectory (  )  )  ;", "}", "METHOD_END"], "methodName": ["makeQualified"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "return   true ;", "}", "METHOD_END"], "methodName": ["shouldCopy"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   sourceFS    =    sourcePathRoot . getFileSystem ( getConf (  )  )  ;", "final   boolean   preserveAcls    =    options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  ;", "final   boolean   preserveXAttrs    =    options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  ;", "final   boolean   preserveRawXattrs    =    options . shouldPreserveRawXattrs (  )  ;", "Stack < FileStatus >    pathStack    =    new   Stack < FileStatus >  (  )  ;", "pathStack . push ( sourceStatus )  ;", "while    (  !  ( pathStack . isEmpty (  )  )  )     {", "for    ( FileStatus   child    :     . getChildren ( sourceFS ,    pathStack . pop (  )  )  )     {", "if    (  . LOG . isDebugEnabled (  )  )", ". LOG . debug (  (  (  \" Recording   source - path :     \"     +     ( sourceStatus . getPath (  )  )  )     +     \"    for   copy .  \"  )  )  ;", "CopyListingFileStatus   childCopyListingStatus    =    DistCpUtils . toCopyListingFileStatus ( sourceFS ,    child ,     ( preserveAcls    &  &     ( child . isDirectory (  )  )  )  ,     ( preserveXAttrs    &  &     ( child . isDirectory (  )  )  )  ,     ( preserveRawXattrs    &  &     ( child . isDirectory (  )  )  )  )  ;", "writeToFileListing ( fileListWriter ,    childCopyListingStatus ,    sourcePathRoot ,    options )  ;", "if    (  . isDirectoryAndNotEmpty ( sourceFS ,    child )  )     {", "if    (  . LOG . isDebugEnabled (  )  )", ". LOG . debug (  (  \" Traversing   non - empty   source   dir :     \"     +     ( sourceStatus . getPath (  )  )  )  )  ;", "pathStack . push ( child )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["traverseNonEmptyDirectory"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "if    ( SimpleCopyListing . LOG . isDebugEnabled (  )  )     {", "SimpleCopyListing . LOG . debug (  (  (  (  \" REL   PATH :     \"     +     ( DistCpUtils . getRelativePath ( sourcePathRoot ,    fileStatus . getPath (  )  )  )  )     +     \"  ,    FULL   PATH :     \"  )     +     ( fileStatus . getPath (  )  )  )  )  ;", "}", "FileStatus   status    =    fileStatus ;", "if    (  !  ( shouldCopy ( fileStatus . getPath (  )  ,    options )  )  )     {", "return ;", "}", "fileListWriter . append ( new   io . Text ( DistCpUtils . getRelativePath ( sourcePathRoot ,    fileStatus . getPath (  )  )  )  ,    status )  ;", "fileListWriter . sync (  )  ;", "if    (  !  ( fileStatus . isDirectory (  )  )  )     {", "totalBytesToCopy    +  =    fileStatus . getLen (  )  ;", "}", "( totalPaths )  +  +  ;", "}", "METHOD_END"], "methodName": ["writeToFileListing"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "boolean   syncOrOverwrite    =     ( options . shouldSyncFolder (  )  )     |  |     ( options . shouldOverwrite (  )  )  ;", "if    (  (  ( fileStatus . getPath (  )  . equals ( sourcePathRoot )  )     &  &     ( fileStatus . isDirectory (  )  )  )     &  &    syncOrOverwrite )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Skip    \"     +     ( fileStatus . getPath (  )  )  )  )  ;", "}", "return ;", "}", "writeToFileListing ( fileListWriter ,    fileStatus ,    sourcePathRoot ,    options )  ;", "}", "METHOD_END"], "methodName": ["writeToFileListingRoot"], "fileName": "org.apache.hadoop.tools.SimpleCopyListing"}, {"methodBody": ["METHOD_START", "{", "return   mapperContext ;", "}", "METHOD_END"], "methodName": ["getContext"], "fileName": "org.apache.hadoop.tools.StubContext"}, {"methodBody": ["METHOD_START", "{", "return   reader ;", "}", "METHOD_END"], "methodName": ["getReader"], "fileName": "org.apache.hadoop.tools.StubContext"}, {"methodBody": ["METHOD_START", "{", "return   reporter ;", "}", "METHOD_END"], "methodName": ["getReporter"], "fileName": "org.apache.hadoop.tools.StubContext"}, {"methodBody": ["METHOD_START", "{", "return   new   TaskAttemptID (  \"  \"  ,     0  ,    TaskType . MAP ,    taskId ,     0  )  ;", "}", "METHOD_END"], "methodName": ["getTaskAttemptID"], "fileName": "org.apache.hadoop.tools.StubContext"}, {"methodBody": ["METHOD_START", "{", "return   writer ;", "}", "METHOD_END"], "methodName": ["getWriter"], "fileName": "org.apache.hadoop.tools.StubContext"}, {"methodBody": ["METHOD_START", "{", "return   TestCopyFiles . checkFiles ( fs ,    topdir ,    files ,    false )  ;", "}", "METHOD_END"], "methodName": ["checkFiles"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    new   Path ( topdir )  ;", "for    ( int   idx    =     0  ;    idx    <     ( f . length )  ;    idx +  +  )     {", "Path   fPath    =    new   Path ( root ,    f [ idx ]  . getName (  )  )  ;", "try    {", "fs . getFileStatus ( fPath )  ;", "FSDataInputStream   in    =    fs . open ( fPath )  ;", "byte [  ]    toRead    =    new   byte [ f [ idx ]  . getSize (  )  ]  ;", "byte [  ]    toCompare    =    new   byte [ f [ idx ]  . getSize (  )  ]  ;", "Random   rb    =    new   Random ( f [ idx ]  . getSeed (  )  )  ;", "rb . nextBytes ( toCompare )  ;", "assertEquals (  \" Cannnot   read   file .  \"  ,    toRead . length ,    in . read ( toRead )  )  ;", "in . close (  )  ;", "for    ( int   i    =     0  ;    i    <     ( toRead . length )  ;    i +  +  )     {", "if    (  ( toRead [ i ]  )     !  =     ( toCompare [ i ]  )  )     {", "return   false ;", "}", "}", "toRead    =    null ;", "toCompare    =    null ;", "}    catch    ( FileNotFoundException   fnfe )     {", "if    (  ! existingOnly )     {", "throw   fnfe ;", "}", "}", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["checkFiles"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    new   Path ( topdir )  ;", "for    ( int   idx    =     0  ;    idx    <    nupdate ;     +  + idx )     {", "final   FileStatus   stat    =    fs . getFileStatus ( new   Path ( root ,    upd [ idx ]  . getName (  )  )  )  ;", "if    (  ( stat . getModificationTime (  )  )     <  =     ( old [ idx ]  . getModificationTime (  )  )  )     {", "return   false ;", "}", "}", "for    ( int   idx    =    nupdate ;    idx    <     (  . NFILES )  ;     +  + idx )     {", "final   FileStatus   stat    =    fs . getFileStatus ( new   Path ( root ,    upd [ idx ]  . getName (  )  )  )  ;", "if    (  ( stat . getModificationTime (  )  )     !  =     ( old [ idx ]  . getModificationTime (  )  )  )     {", "return   false ;", "}", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["checkUpdate"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   out    =    fs . create ( f )  ;", "try    {", "byte [  ]    b    =    new   byte [  1  0  2  4     +     (  . RAN . nextInt (  1  0  2  4  )  )  ]  ;", ". RAN . nextBytes ( b )  ;", "out . write ( b )  ;", "}    finally    {", "if    ( out    !  =    null )", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "return   TestCopyFiles . createFile ( root ,    fs ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "TestCopyFiles . MyFile   f    =     ( levels    <     0  )     ?    new   TestCopyFiles . MyFile (  )     :    new   TestCopyFiles . MyFile ( levels )  ;", "Path   p    =    new   Path ( root ,    f . getName (  )  )  ;", "FSDataOutputStream   out    =    fs . create ( p )  ;", "byte [  ]    toWrite    =    new   byte [ f . getSize (  )  ]  ;", "new   Random ( f . getSeed (  )  )  . nextBytes ( toWrite )  ;", "out . write ( toWrite )  ;", "out . close (  )  ;", "LOG . info (  (  (  (  \" created :     \"     +    p )     +     \"  ,    size =  \"  )     +     ( f . getSize (  )  )  )  )  ;", "return   f ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "return   TestCopyFiles . createFiles ( FileSystem . get ( fsname ,    new   Configuration (  )  )  ,    topdir )  ;", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    new   Path ( topdir )  ;", ". MyFile [  ]    files    =    new    . MyFile [  . NFILES ]  ;", "for    ( int   i    =     0  ;    i    <     (  . NFILES )  ;    i +  +  )     {", "files [ i ]     =     . createFile ( root ,    fs )  ;", "}", "return   files ;", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "final   Path   home    =    new   Path (  (  \"  / ur /  \"     +     ( ugi . getUrName (  )  )  )  )  ;", "fmkdirhome )  ;", "ftOwner ( home ,    ugi . getUrName (  )  ,    ugi . getGroupName )  [  0  ]  )  ;", "ftPermiion ( home ,    new   Fermiion (  (  ( ort )     (  4  4  8  )  )  )  )  ;", "return   home ;", "}", "METHOD_END"], "methodName": ["createHomeDirectory"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "String   username    =    name    +     ( TestCopyFiles . now )  ;", "String   group    =     ( issuper )     ?     \" supergroup \"     :    username ;", "return   UserGroupInformation . createUserForTesting ( username ,    new   String [  ]  {    group    }  )  ;", "}", "METHOD_END"], "methodName": ["createUGI"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "fs . delete ( new   Path ( topdir )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["deldir"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "ByteArrayOutputStream   baout    =    new   ByteArrayOutputStream (  )  ;", "PrintStream   out    =    new   PrintStream ( baout ,    true )  ;", "PrintStream   old    =    System . out ;", "System . setOut ( out )  ;", "shell . run ( args )  ;", "out . close (  )  ;", "System . setOut ( old )  ;", "return   baoutString (  )  ;", "}", "METHOD_END"], "methodName": ["execCmd"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "return   TestCopyFiles . getFileStatus ( fs ,    topdir ,    files ,    false )  ;", "}", "METHOD_END"], "methodName": ["getFileStatus"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    new   Path ( topdir )  ;", "List < FileStatus >    statuses    =    new   ArrayList < FileStatus >  (  )  ;", "for    ( int   idx    =     0  ;    idx    <     (  . NFILES )  ;     +  + idx )     {", "try    {", "statuses . add ( fs . getFileStatus ( new   Path ( root ,    files [ idx ]  . getName (  )  )  )  )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "if    (  ! existingOnly )     {", "throw   fnfe ;", "}", "}", "}", "return   statuses . toArray ( new   FileStatus [ statuses . size (  )  ]  )  ;", "}", "METHOD_END"], "methodName": ["getFileStatus"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "final   int   prefixlen    =    prefix . length (  )  ;", "final   StringTokenizer   t    =    new   StringTokenizer ( lines ,     \"  \\ n \"  )  ;", "final   StringBuffer   results    =    new   StringBuffer (  )  ;", "for    (  ;    tsMoreTokens (  )  ;  )     {", "String   s    =    t . nextToken (  )  ;", "results . append (  (  ( s . substring (  (  ( s . indexOf ( prefix )  )     +    prefixlen )  )  )     +     \"  \\ n \"  )  )  ;", "}", "return   results . toString (  )  ;", "}", "METHOD_END"], "methodName": ["removePrefix"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "String   namenode    =    null ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "namenode    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles ( URI . create ( namenode )  ,     \"  / basedir / middle / srcdat \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - basedir \"  ,     \"  / basedir \"  ,    namenode    +     \"  / basedir / middle / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( hdfs ,     \"  / destdat / middle / srcdat \"  ,    files )  )  ;", ". deldir ( hdfs ,     \"  / destdat \"  )  ;", ". deldir ( hdfs ,     \"  / basedir \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testBasedir"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "final   String   namenode    =    hdfs . getUri (  )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles ( URI . create ( namenode )  ,     \"  / srcdat \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - p \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( hdfs ,     \"  / destdat \"  ,    files )  )  ;", "FileSystem   fs    =    FileSystem . get ( URI . create (  ( namenode    +     \"  / logs \"  )  )  ,    conf )  ;", "assertTrue (  \" Log   directory   does   not   exist .  \"  ,    fs . exists ( new   Path (  ( namenode    +     \"  / logs \"  )  )  )  )  ;", "FileStatus [  ]    dchkpoint    =     . getFileStatus ( hdfs ,     \"  / destdat \"  ,    files )  ;", "final   int   nupdate    =     (  . NFILES )     >  >     2  ;", ". updateFiles ( cluster . getFileSystem (  )  ,     \"  / srcdat \"  ,    files ,    nupdate )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - prbugp \"  ,     \"  - update \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( hdfs ,     \"  / destdat \"  ,    files )  )  ;", "assertTrue (  \" Update   failed   to   replicate   all   changes   in   src \"  ,     . checkUpdate ( hdfs ,    dchkpoint ,     \"  / destdat \"  ,    files ,    nupdate )  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - prbugp \"  ,     \"  - overwrite \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( hdfs ,     \"  / destdat \"  ,    files )  )  ;", "assertTrue (  \"  - overwrite   didn ' t .  \"  ,     . checkUpdate ( hdfs ,    dchkpoint ,     \"  / destdat \"  ,    files ,     . NFILES )  )  ;", ". deldir ( hdfs ,     \"  / destdat \"  )  ;", ". deldir ( hdfs ,     \"  / srcdat \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCopyDfsToDfsUpdateOverwrite"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "final   String   namenode    =    hdfs . getUri (  )  . toString (  )  ;", "FileSystem   fs    =    FileSystem . get ( URI . create ( namenode )  ,    new   Configuration (  )  )  ;", "final   String   testfilename    =     \" test \"  ;", "final   String   srcData    =     \" act   act   act \"  ;", "final   String   destData    =     \" cat   cat   cat \"  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "Path   srcPath    =    new   Path (  \"  / srcdat \"  ,    testfilename )  ;", "Path   destPath    =    new   Path (  \"  / destdat \"  ,    testfilename )  ;", "FSDataOutputStream   out    =    fs . create ( srcPath ,    true )  ;", "out . writeUTF ( srcData )  ;", "out . close (  )  ;", "out    =    fs . create ( destPath ,    true )  ;", "out . writeUTF ( destData )  ;", "out . close (  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - p \"  ,     \"  - update \"  ,     \"  - skipcrccheck \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "FSDataInputStream   in    =    hdfs . open ( destPath )  ;", "String   s    =    in . readUTF (  )  ;", "System . out . println (  (  \" Dest   had :     \"     +    s )  )  ;", "assertTrue (  \" Dest   got   over   written   even   with   skip   crc \"  ,    s . equalsIgnoreCase ( destData )  )  ;", "in . close (  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - p \"  ,     \"  - update \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "in    =    hdfs . open ( destPath )  ;", "s    =    in . readUTF (  )  ;", "System . out . println (  (  \" Dest   had :     \"     +    s )  )  ;", "assertTrue (  \" Dest   did   not   get   overwritten   without   skip   crc \"  ,    s . equalsIgnoreCase ( srcData )  )  ;", "in . close (  )  ;", ". deldir ( hdfs ,     \"  / destdat \"  )  ;", ". deldir ( hdfs ,     \"  / srcdat \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCopyDfsToDfsUpdateWithSkipCRC"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "final   FileSystem   localfs    =    FileSystem . get ( TestCopyFiles . LOCAL _ FS ,    new   Configuration (  )  )  ;", "try    {", "TestCopyFiles . MyFile [  ]    files    =    TestCopyFiles . createFiles ( localfs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / src 2  / srcdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,    TestCopyFiles . checkFiles ( localfs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / src 2  / srcdat \"  )  ,    files )  )  ;", "assertEquals ( DistCpV 1  . DuplicationException . ERROR _ CODE ,    ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / src 2  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / destdat \"     }  )  )  ;", "}    finally    {", "TestCopyFiles . deldir ( localfs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  )  ;", "TestCopyFiles . deldir ( localfs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "TestCopyFiles . deldir ( localfs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / src 2  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopyDuplication"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "String   namenode    =    null ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "namenode    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles ( URI . create ( namenode )  ,     \"  / srcdat \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( hdfs ,     \"  / destdat \"  ,    files )  )  ;", "FileSystem   fs    =    FileSystem . get ( URI . create (  ( namenode    +     \"  / logs \"  )  )  ,    conf )  ;", "assertTrue (  \" Log   directory   does   not   exist .  \"  ,    fs . exists ( new   Path (  ( namenode    +     \"  / logs \"  )  )  )  )  ;", ". deldir ( hdfs ,     \"  / destdat \"  )  ;", ". deldir ( hdfs ,     \"  / srcdat \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCopyFromDfsToDfs"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   localfs    =    FileSystem . get (  . LOCAL _ FS ,    conf )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "final   String   namenode    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles ( URI . create ( namenode )  ,     \"  / srcdat \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - log \"  ,     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     (  . TEST _ ROOT _ DIR )  )     +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( localfs ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  ,    files )  )  ;", "assertTrue (  \" Log   directory   does   not   exist .  \"  ,    hdfs . exists ( new   Path (  \"  / logs \"  )  )  )  ;", ". deldir ( localfs ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", ". deldir ( hdfs ,     \"  / srcdat \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCopyFromDfsToLocal"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "final   String   namenode    =    hdfs . getUri (  )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles (  . LOCAL _ FS ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - log \"  ,    namenode    +     \"  / logs \"  ,     (  \" file :  /  /  /  \"     +     (  . TEST _ ROOT _ DIR )  )     +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( cluster . getFileSystem (  )  ,     \"  / destdat \"  ,    files )  )  ;", "assertTrue (  \" Log   directory   does   not   exist .  \"  ,    hdfs . exists ( new   Path (  ( namenode    +     \"  / logs \"  )  )  )  )  ;", ". deldir ( hdfs ,     \"  / destdat \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", ". deldir ( FileSystem . get (  . LOCAL _ FS ,    conf )  ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCopyFromLocalToDfs"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "FileSystem   localfs    =    FileSystem . get (  . LOCAL _ FS ,    conf )  ;", ". MyFile [  ]    files    =     . createFiles (  . LOCAL _ FS ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     (  \" file :  /  /  /  \"     +     (  . TEST _ ROOT _ DIR )  )     +     \"  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     (  . TEST _ ROOT _ DIR )  )     +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( localfs ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  ,    files )  )  ;", ". deldir ( localfs ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  )  ;", ". deldir ( localfs ,     (  (  . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testCopyFromLocalToLocal"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    FileSystem . get ( TestCopyFiles . LOCAL _ FS ,    new   Configuration (  )  )  ;", "Path   root    =    new   Path (  (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "try    {", "TestCopyFiles . MyFile [  ]    files    =    new   TestCopyFiles . MyFile [  ]  {    TestCopyFiles . createFile ( root ,    fs )     }  ;", "ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,    TestCopyFiles . checkFiles ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  ,    files )  )  ;", "String   fname    =    files [  0  ]  . getName (  )  ;", "Path   p    =    new   Path ( root ,    fname )  ;", "LOG . info (  (  (  (  \" fname =  \"     +    fname )     +     \"  ,    exists ?     \"  )     +     ( fs . exists ( p )  )  )  )  ;", "ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     (  (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat /  \"  )     +    fname ,     (  (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / dest 2  /  \"  )     +    fname    }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,    TestCopyFiles . checkFiles ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / dest 2  \"  )  ,    files )  )  ;", "String [  ]    args    =    new   String [  ]  {     \"  - update \"  ,     (  (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat /  \"  )     +    fname ,     (  (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / dest 2  /  \"  )     +    fname    }  ;", "Configuration   conf    =    new   Configuration (  )  ;", "JobConf   job    =    new   JobConf ( conf ,    DistCpV 1  . class )  ;", "DistCpV 1  . Arguments   distcpArgs    =    DistCpV 1  . Arguments . valueOf ( args ,    conf )  ;", "assertFalse (  (  \" Single   file   update   failed   to   skip   copying   even   though   the    \"     +     \" file   exists   at   destination .  \"  )  ,    DistCpV 1  . setup ( conf ,    job ,    distcpArgs )  )  ;", "TestCopyFiles . deldir ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / dest 2  \"  )  )  ;", "fs . mkdirs ( new   Path (  (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / dest 2  \"  )  )  )  ;", "TestCopyFiles . MyFile [  ]    files 2     =    new   TestCopyFiles . MyFile [  ]  {    TestCopyFiles . createFile ( root ,    fs ,     0  )     }  ;", "String   sname    =    files 2  [  0  ]  . getName (  )  ;", "ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     \"  - update \"  ,     (  (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat /  \"  )     +    sname ,     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / dest 2  /  \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,    TestCopyFiles . checkFiles ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / dest 2  \"  )  ,    files 2  )  )  ;", "TestCopyFiles . updateFiles ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  ,    files 2  ,     1  )  ;", "ToolRunner . run ( new   DistCpV 1  ( new   Configuration (  )  )  ,    new   String [  ]  {     \"  - update \"  ,     (  (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / srcdat /  \"  )     +    sname ,     (  \" file :  /  /  /  \"     +     ( TestCopyFiles . TEST _ ROOT _ DIR )  )     +     \"  / dest 2  /  \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,    TestCopyFiles . checkFiles ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / dest 2  \"  )  ,    files 2  )  )  ;", "}    finally    {", "TestCopyFiles . deldir ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / destdat \"  )  )  ;", "TestCopyFiles . deldir ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / dest 2  \"  )  )  ;", "TestCopyFiles . deldir ( fs ,     (  ( TestCopyFiles . TEST _ ROOT _ DIR )     +     \"  / srcdat \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopySingleFile"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt (  \" fs . trash . interval \"  ,     6  0  )  ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   URI   nnURI    =    FileSystem . getDefaultUri ( conf )  ;", "final   String   nnUri    =    nnURI . toString (  )  ;", "final   FileSystem   fs    =    FileSystem . get ( URI . create ( nnUri )  ,    conf )  ;", "final   DistCpV 1    distcp    =    new   DistCpV 1  ( conf )  ;", "final   FsShell   shell    =    new   FsShell ( conf )  ;", "final   String   srcrootdir    =     \"  / src _ root \"  ;", "final   String   dstrootdir    =     \"  / dst _ root \"  ;", "{", ". createFiles ( nnURI ,    srcrootdir )  ;", "String   srcresults    =     . execCmd ( shell ,     \"  - lsr \"  ,    srcrootdir )  ;", "srcresults    =     . removePrefix ( srcresults ,    srcrootdir )  ;", "System . out . println (  (  \" srcresults =  \"     +    srcresults )  )  ;", ". createFiles ( nnURI ,    dstrootdir )  ;", "System . out . println (  (  \" dstrootdir =  \"     +    dstrootdir )  )  ;", "shell . run ( new   String [  ]  {     \"  - lsr \"  ,    dstrootdir    }  )  ;", "ToolRunner . run ( distcp ,    new   String [  ]  {     \"  - delete \"  ,     \"  - update \"  ,     \"  - log \"  ,     \"  / log \"  ,    nnUri    +    srcrootdir ,    nnUri    +    dstrootdir    }  )  ;", "String   dstresults    =     . execCmd ( shell ,     \"  - lsr \"  ,    dstrootdir )  ;", "dstresults    =     . removePrefix ( dstresults ,    dstrootdir )  ;", "System . out . println (  (  \" first   dstresults =  \"     +    dstresults )  )  ;", "assertEquals ( srcresults ,    dstresults )  ;", ". create ( fs ,    new   Path ( dstrootdir ,     \" foo \"  )  )  ;", ". create ( fs ,    new   Path ( dstrootdir ,     \" foobar \"  )  )  ;", "ToolRunner . run ( distcp ,    new   String [  ]  {     \"  - delete \"  ,     \"  - update \"  ,     \"  - log \"  ,     \"  / log 2  \"  ,    nnUri    +    srcrootdir ,    nnUri    +    dstrootdir    }  )  ;", "dstresults    =     . execCmd ( shell ,     \"  - lsr \"  ,    dstrootdir )  ;", "dstresults    =     . removePrefix ( dstresults ,    dstrootdir )  ;", "System . out . println (  (  \" second   dstresults =  \"     +    dstresults )  )  ;", "assertEquals ( srcresults ,    dstresults )  ;", "assertTrue ( fs . exists ( new   Path ( fs . getHomeDirectory (  )  ,     (  (  \"  . Trash / Current \"     +    dstrootdir )     +     \"  / foo \"  )  )  )  )  ;", "assertTrue ( fs . exists ( new   Path ( fs . getHomeDirectory (  )  ,     (  (  \"  . Trash / Current \"     +    dstrootdir )     +     \"  / foobar \"  )  )  )  )  ;", ". deldir ( fs ,    dstrootdir )  ;", ". deldir ( fs ,    srcrootdir )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testDelete"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   localfs    =    FileSystem . get (  . LOCAL _ FS ,    conf )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "final   String   namenode    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles ( URI . create ( namenode )  ,     \"  / srcdat \"  )  ;", "String   destdir    =     (  . TEST _ ROOT _ DIR )     +     \"  / destdat \"  ;", ". MyFile [  ]    localFiles    =     . createFiles ( localfs ,    destdir )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - delete \"  ,     \"  - update \"  ,     \"  - log \"  ,     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,     (  \" file :  /  /  /  \"     +     (  . TEST _ ROOT _ DIR )  )     +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( localfs ,    destdir ,    files )  )  ;", "assertTrue (  \" Log   directory   does   not   exist .  \"  ,    hdfs . exists ( new   Path (  \"  / logs \"  )  )  )  ;", ". deldir ( localfs ,    destdir )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", ". deldir ( hdfs ,     \"  / srcdat \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testDeleteLocal"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "String   namenode    =    null ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "namenode    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", "FileSystem   fs    =    FileSystem . get ( URI . create ( namenode )  ,    new   Configuration (  )  )  ;", "fs . mkdirs ( new   Path (  \"  / empty \"  )  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / empty \"  ,    namenode    +     \"  / dest \"     }  )  ;", "fs    =    FileSystem . get ( URI . create (  ( namenode    +     \"  / destdat \"  )  )  ,    conf )  ;", "assertTrue (  \" Destination   directory   does   not   exist .  \"  ,    fs . exists ( new   Path (  ( namenode    +     \"  / dest \"  )  )  )  )  ;", ". deldir ( hdfs ,     \"  / dest \"  )  ;", ". deldir ( hdfs ,     \"  / empty \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testEmptyDir"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "String   namenode    =    null ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   FileSystem   hdfs    =    cluster . getFileSystem (  )  ;", "namenode    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "if    ( namenode . startsWith (  \" hdfs :  /  /  \"  )  )     {", ". MyFile [  ]    files    =     . createFiles ( URI . create ( namenode )  ,     \"  / srcdat \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat /  *  \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( hdfs ,     \"  / destdat \"  ,    files )  )  ;", "FileSystem   fs    =    FileSystem . get ( URI . create (  ( namenode    +     \"  / logs \"  )  )  ,    conf )  ;", "assertTrue (  \" Log   directory   does   not   exist .  \"  ,    fs . exists ( new   Path (  ( namenode    +     \"  / logs \"  )  )  )  )  ;", ". deldir ( hdfs ,     \"  / destdat \"  )  ;", ". deldir ( hdfs ,     \"  / srcdat \"  )  ;", ". deldir ( hdfs ,     \"  / logs \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testGlobbing"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "final   String   nnUri    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "final   FileSystem   fs    =    FileSystem . get ( URI . create ( nnUri )  ,    conf )  ;", "final   DistCpV 1    distcp    =    new   DistCpV 1  ( conf )  ;", "final   FsShell   shell    =    new   FsShell ( conf )  ;", "final   String   srcrootdir    =     \"  / src _ root \"  ;", "final   Path   srcrootpath    =    new   Path ( srcrootdir )  ;", "final   String   dstrootdir    =     \"  / dst _ root \"  ;", "final   Path   dstrootpath    =    new   Path ( dstrootdir )  ;", "{", ". MyFile [  ]    files    =     . createFiles ( URI . create ( nnUri )  ,    srcrootdir )  ;", "int   filelimit    =     ( files . length )     /     2  ;", "System . out . println (  (  \" filelimit =  \"     +    filelimit )  )  ;", "ToolRunner . run ( distcp ,    new   String [  ]  {     \"  - filelimit \"  ,     \"  \"     +    filelimit ,    nnUri    +    srcrootdir ,    nnUri    +    dstrootdir    }  )  ;", "String   results    =     . execCmd ( shell ,     \"  - lsr \"  ,    dstrootdir )  ;", "results    =     . removePrefix ( results ,    dstrootdir )  ;", "System . out . println (  (  \" results =  \"     +    results )  )  ;", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,    dstrootdir ,    files ,    true )  ;", "assertEquals ( filelimit ,    dststat . length )  ;", ". deldir ( fs ,    dstrootdir )  ;", ". deldir ( fs ,    srcrootdir )  ;", "}", "{", ". createFiles ( URI . create ( nnUri )  ,    srcrootdir )  ;", "long   sizelimit    =     ( fs . getContentSummary ( srcrootpath )  . getLength (  )  )     /     2  ;", "System . out . println (  (  \" sizelimit =  \"     +    sizelimit )  )  ;", "ToolRunner . run ( distcp ,    new   String [  ]  {     \"  - sizelimit \"  ,     \"  \"     +    sizelimit ,    nnUri    +    srcrootdir ,    nnUri    +    dstrootdir    }  )  ;", "ContentSummary   summary    =    fs . getContentSummary ( dstrootpath )  ;", "System . out . println (  (  \" summary =  \"     +    summary )  )  ;", "assertTrue (  (  ( summary . getLength (  )  )     <  =    sizelimit )  )  ;", ". deldir ( fs ,    dstrootdir )  ;", ". deldir ( fs ,    srcrootdir )  ;", "}", "{", "final    . MyFile [  ]    srcs    =     . createFiles ( URI . create ( nnUri )  ,    srcrootdir )  ;", "final   long   totalsize    =    fs . getContentSummary ( srcrootpath )  . getLength (  )  ;", "System . out . println (  (  \" src . length =  \"     +     ( srcs . length )  )  )  ;", "System . out . println (  (  \" totalsize    =  \"     +    totalsize )  )  ;", "fs . mkdirs ( dstrootpath )  ;", "final   int   parts    =     (  . RAN . nextInt (  (  (  (  . NFILES )     /     3  )     -     1  )  )  )     +     2  ;", "final   int   filelimit    =     ( srcs . length )     /    parts ;", "final   long   sizelimit    =    totalsize    /    parts ;", "System . out . println (  (  \" filelimit =  \"     +    filelimit )  )  ;", "System . out . println (  (  \" sizelimit =  \"     +    sizelimit )  )  ;", "System . out . println (  (  \" parts             =  \"     +    parts )  )  ;", "final   String [  ]    args    =    new   String [  ]  {     \"  - filelimit \"  ,     \"  \"     +    filelimit ,     \"  - sizelimit \"  ,     \"  \"     +    sizelimit ,     \"  - update \"  ,    nnUri    +    srcrootdir ,    nnUri    +    dstrootdir    }  ;", "int   dstfilecount    =     0  ;", "long   dstsize    =     0  ;", "for    ( int   i    =     0  ;    i    <  =    parts ;    i +  +  )     {", "ToolRunner . run ( distcp ,    args )  ;", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,    dstrootdir ,    srcs ,    true )  ;", "System . out . println (  (  ( i    +     \"  )    dststat . length =  \"  )     +     ( dststat . length )  )  )  ;", "assertTrue (  (  (  ( dststat . length )     -    dstfilecount )     <  =    filelimit )  )  ;", "ContentSummary   summary    =    fs . getContentSummary ( dstrootpath )  ;", "System . out . println (  (  ( i    +     \"  )    summary . getLength (  )  =  \"  )     +     ( summary . getLength (  )  )  )  )  ;", "assertTrue (  (  (  ( summary . getLength (  )  )     -    dstsize )     <  =    sizelimit )  )  ;", "assertTrue (  . checkFiles ( fs ,    dstrootdir ,    srcs ,    true )  )  ;", "dstfilecount    =    dststat . length ;", "dstsize    =    summary . getLength (  )  ;", "}", ". deldir ( fs ,    dstrootdir )  ;", ". deldir ( fs ,    srcrootdir )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLimits"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "String   namenode    =    null ;", "MiniDFSCluster   dfs    =    null ;", "MiniDFSCluster   mr    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "dfs    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  3  )  . format ( true )  . build (  )  ;", "FileSystem   fs    =    dfs . getFileSystem (  )  ;", "final   FsShell   shell    =    new   FsShell ( conf )  ;", "namenode    =    fs . getUri (  )  . toString (  )  ;", "TestCopyFiles . MyFile [  ]    files    =    TestCopyFiles . createFiles ( fs . getUri (  )  ,     \"  / srcdat \"  )  ;", "long   totsize    =     0  ;", "for    ( TestCopyFiles . MyFile   f    :    files )     {", "totsize    +  =    f . getSize (  )  ;", "}", "Configuration   job    =    new   mapred . JobConf ( conf )  ;", "job . setLong (  \" distcp . bytes . per . map \"  ,     ( totsize    /     3  )  )  ;", "ToolRunner . run ( new   DistCpV 1  ( job )  ,    new   String [  ]  {     \"  - m \"  ,     \"  1  0  0  \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,    TestCopyFiles . checkFiles ( fs ,     \"  / destdat \"  ,    files )  )  ;", "String   logdir    =    namenode    +     \"  / logs \"  ;", "System . out . println ( TestCopyFiles . execCmd ( shell ,     \"  - lsr \"  ,    logdir )  )  ;", "FileStatus [  ]    logs    =    fs . listStatus ( new   Path ( logdir )  )  ;", "assertTrue (  (  ( logs . length )     =  =     2  )  )  ;", "TestCopyFiles . deldir ( fs ,     \"  / destdat \"  )  ;", "TestCopyFiles . deldir ( fs ,     \"  / logs \"  )  ;", "ToolRunner . run ( new   DistCpV 1  ( job )  ,    new   String [  ]  {     \"  - m \"  ,     \"  1  \"  ,     \"  - log \"  ,    namenode    +     \"  / logs \"  ,    namenode    +     \"  / srcdat \"  ,    namenode    +     \"  / destdat \"     }  )  ;", "System . out . println ( TestCopyFiles . execCmd ( shell ,     \"  - lsr \"  ,    logdir )  )  ;", "logs    =    fs . globStatus ( new   Path (  ( namenode    +     \"  / logs / part *  \"  )  )  )  ;", "assertTrue (  (  \" Unexpected   map   count ,    logs . length =  \"     +     ( logs . length )  )  ,     (  ( logs . length )     =  =     1  )  )  ;", "}    finally    {", "if    ( dfs    !  =    null )     {", "dfs . shutdown (  )  ;", "}", "if    ( mr    !  =    null )     {", "mr . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testMapCount"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "String   nnUri    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "FileSystem   fs    =    FileSystem . get ( URI . create ( nnUri )  ,    conf )  ;", "{", ". MyFile [  ]    files    =     . createFiles ( URI . create ( nnUri )  ,     \"  / srcdat \"  )  ;", "FileStatus [  ]    srcstat    =     . getFileStatus ( fs ,     \"  / srcdat \"  ,    files )  ;", "for    ( int   i    =     0  ;    i    <     ( srcstat . length )  ;    i +  +  )     {", "fs . setOwner ( srcstat [ i ]  . getPath (  )  ,     (  \" u \"     +    i )  ,    null )  ;", "}", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - pu \"  ,    nnUri    +     \"  / srcdat \"  ,    nnUri    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( fs ,     \"  / destdat \"  ,    files )  )  ;", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,     \"  / destdat \"  ,    files )  ;", "for    ( int   i    =     0  ;    i    <     ( dststat . length )  ;    i +  +  )     {", "assertEquals (  (  \" i =  \"     +    i )  ,     (  \" u \"     +    i )  ,    dststat [ i ]  . getOwner (  )  )  ;", "}", ". deldir ( fs ,     \"  / destdat \"  )  ;", ". deldir ( fs ,     \"  / srcdat \"  )  ;", "}", "{", ". MyFile [  ]    files    =     . createFiles ( URI . create ( nnUri )  ,     \"  / srcdat \"  )  ;", "FileStatus [  ]    srcstat    =     . getFileStatus ( fs ,     \"  / srcdat \"  ,    files )  ;", "for    ( int   i    =     0  ;    i    <     ( srcstat . length )  ;    i +  +  )     {", "fs . setOwner ( srcstat [ i ]  . getPath (  )  ,    null ,     (  \" g \"     +    i )  )  ;", "}", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - pg \"  ,    nnUri    +     \"  / srcdat \"  ,    nnUri    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( fs ,     \"  / destdat \"  ,    files )  )  ;", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,     \"  / destdat \"  ,    files )  ;", "for    ( int   i    =     0  ;    i    <     ( dststat . length )  ;    i +  +  )     {", "assertEquals (  (  \" i =  \"     +    i )  ,     (  \" g \"     +    i )  ,    dststat [ i ]  . getGroup (  )  )  ;", "}", ". deldir ( fs ,     \"  / destdat \"  )  ;", ". deldir ( fs ,     \"  / srcdat \"  )  ;", "}", "{", ". MyFile [  ]    files    =     . createFiles ( URI . create ( nnUri )  ,     \"  / srcdat \"  )  ;", "FileStatus [  ]    srcstat    =     . getFileStatus ( fs ,     \"  / srcdat \"  ,    files )  ;", "FsPermission [  ]    permissions    =    new   FsPermission [ srcstat . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( srcstat . length )  ;    i +  +  )     {", "permissions [ i ]     =    new   FsPermission (  (  ( short )     ( i    &     4  3  8  )  )  )  ;", "fs . setPermission ( srcstat [ i ]  . getPath (  )  ,    permissions [ i ]  )  ;", "}", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - pp \"  ,    nnUri    +     \"  / srcdat \"  ,    nnUri    +     \"  / destdat \"     }  )  ;", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( fs ,     \"  / destdat \"  ,    files )  )  ;", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,     \"  / destdat \"  ,    files )  ;", "for    ( int   i    =     0  ;    i    <     ( dststat . length )  ;    i +  +  )     {", "assertEquals (  (  \" i =  \"     +    i )  ,    permissions [ i ]  ,    dststat [ i ]  . getPermission (  )  )  ;", "}", ". deldir ( fs ,     \"  / destdat \"  )  ;", ". deldir ( fs ,     \"  / srcdat \"  )  ;", "}", "{", ". MyFile [  ]    files    =     . createFiles ( URI . create ( nnUri )  ,     \"  / srcdat \"  )  ;", "fs . mkdirs ( new   Path (  \"  / srcdat / tmpf 1  \"  )  )  ;", "fs . mkdirs ( new   Path (  \"  / srcdat / tmpf 2  \"  )  )  ;", "FileStatus [  ]    srcstat    =     . getFileStatus ( fs ,     \"  / srcdat \"  ,    files )  ;", "FsPermission [  ]    permissions    =    new   FsPermission [ srcstat . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( srcstat . length )  ;    i +  +  )     {", "fs . setTimes ( srcstat [ i ]  . getPath (  )  ,     4  0  ,     5  0  )  ;", "}", "ToolRunner . run ( new   DistCpV 1  ( conf )  ,    new   String [  ]  {     \"  - pt \"  ,    nnUri    +     \"  / srcdat \"  ,    nnUri    +     \"  / destdat \"     }  )  ;", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,     \"  / destdat \"  ,    files )  ;", "for    ( int   i    =     0  ;    i    <     ( dststat . length )  ;    i +  +  )     {", "assertEquals (  (  \" Modif .    Time   i =  \"     +    i )  ,     4  0  ,    dststat [ i ]  . getModificationTime (  )  )  ;", "assertEquals (  (  (  (  (  \" Access   Time   i =  \"     +    i )     +     ( srcstat [ i ]  . getPath (  )  )  )     +     \"  -  \"  )     +     ( dststat [ i ]  . getPath (  )  )  )  ,     5  0  ,    dststat [ i ]  . getAccessTime (  )  )  ;", "}", "assertTrue (  \" Source   and   destination   directories   do   not   match .  \"  ,     . checkFiles ( fs ,     \"  / destdat \"  ,    files )  )  ;", ". deldir ( fs ,     \"  / destdat \"  )  ;", ". deldir ( fs ,     \"  / srcdat \"  )  ;", "}", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testPreserveOption"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "assert   nupdate    <  =     ( TestCopyFiles . NFILES )  ;", "Path   root    =    new   Path ( topdir )  ;", "for    ( int   idx    =     0  ;    idx    <    nupdate ;     +  + idx )     {", "Path   fPath    =    new   Path ( root ,    files [ idx ]  . getName (  )  )  ;", "assertTrue (  (  ( fPath . toString (  )  )     +     \"    does   not   exist \"  )  ,    fs . exists ( fPath )  )  ;", "FSDataOutputStream   out    =    fs . create ( fPath )  ;", "files [ idx ]  . reset (  )  ;", "byte [  ]    toWrite    =    new   byte [ files [ idx ]  . getSize (  )  ]  ;", "Random   rb    =    new   Random ( files [ idx ]  . getSeed (  )  )  ;", "rb . nextBytes ( toWrite )  ;", "out . write ( toWrite )  ;", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateFiles"], "fileName": "org.apache.hadoop.tools.TestCopyFiles"}, {"methodBody": ["METHOD_START", "{", "TestCopyListing . cluster    =    new   Builder ( TestCopyListing . config )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestCopyListing . cluster )     !  =    null )     {", "TestCopyListing . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["destroy"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    null ;", "try    {", "fs    =    FileSystem . get ( getConf (  )  )  ;", "List < Path >    srcPaths    =    new   ArrayList < Path >  (  )  ;", "Path   p 1     =    new   Path (  \"  / tmp / in /  1  \"  )  ;", "Path   p 2     =    new   Path (  \"  / tmp / in /  2  \"  )  ;", "Path   p 3     =    new   Path (  \"  / tmp / in 2  /  2  \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / out /  1  \"  )  ;", "srcPaths . add ( p 1  . getParent (  )  )  ;", "srcPaths . add ( p 3  . getParent (  )  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in /  1  \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in /  2  \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in 2  /  2  \"  )  ;", "fs . mkdirs ( target )  ;", "OutputStream   out    =    fs . create ( p 1  )  ;", "out . write (  \" ABC \"  . getBytes (  )  )  ;", "out . close (  )  ;", "out    =    fs . create ( p 2  )  ;", "out . write (  \" DEF \"  . getBytes (  )  )  ;", "out . close (  )  ;", "out    =    fs . create ( p 3  )  ;", "out . write (  \" GHIJ \"  . getBytes (  )  )  ;", "out . close (  )  ;", "Path   listingFile    =    new   Path (  \"  / tmp / file \"  )  ;", "DistCpOptions   options    =    new   DistCpOptions ( srcPaths ,    target )  ;", "options . setSyncFolder ( true )  ;", "CopyListing   listing    =    new   SimpleCopyListing ( getConf (  )  ,     . CREDENTIALS )  ;", "try    {", "listing . buildListing ( listingFile ,    options )  ;", "Assert . fail (  \" Duplicates   not   detected \"  )  ;", "}    catch    ( CopyListing . DuplicateFileException   ignore )     {", "}", "Assert . assertEquals ( listing . getBytesToCopy (  )  ,     1  0  )  ;", "Assert . assertEquals ( listing . getNumberOfPaths (  )  ,     3  )  ;", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "try    {", "listing . buildListing ( listingFile ,    options )  ;", "Assert . fail (  \" Invalid   input   not   detected \"  )  ;", "}    catch    ( CopyListing . InvalidInputException   ignore )     {", "}", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  \" Test   build   listing   failed \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testBuildListing"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    null ;", "String   testRootString    =     \"  / singleFileListing \"  ;", "Path   testRoot    =    new   Path ( testRootString )  ;", "SequenceFile . Reader   reader    =    null ;", "try    {", "fs    =    FileSystem . get ( getConf (  )  )  ;", "if    ( fs . exists ( testRoot )  )", "TestDistCpUtils . delete ( fs ,    testRootString )  ;", "Path   sourceFile    =    new   Path ( testRoot ,     \"  / source / foo / bar / source . txt \"  )  ;", "Path   decoyFile    =    new   Path ( testRoot ,     \"  / target / moo / source . txt \"  )  ;", "Path   targetFile    =    new   Path ( testRoot ,     \"  / target / moo / target . txt \"  )  ;", "TestDistCpUtils . createFile ( fs ,    sourceFile . toString (  )  )  ;", "TestDistCpUtils . createFile ( fs ,    decoyFile . toString (  )  )  ;", "TestDistCpUtils . createFile ( fs ,    targetFile . toString (  )  )  ;", "List < Path >    srcPaths    =    new   ArrayList < Path >  (  )  ;", "srcPaths . add ( sourceFile )  ;", "DistCpOptions   options    =    new   DistCpOptions ( srcPaths ,    targetFile )  ;", "CopyListing   listing    =    new   SimpleCopyListing ( getConf (  )  ,     . CREDENTIALS )  ;", "final   Path   listFile    =    new   Path ( testRoot ,     \"  / tmp / fileList . seq \"  )  ;", "listing . buildListing ( listFile ,    options )  ;", "reader    =    new   SequenceFile . Reader ( getConf (  )  ,    Reader . file ( listFile )  )  ;", "CopyListingFileStatus   fileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   relativePath    =    new   Text (  )  ;", "Assert . assertTrue ( reader . next ( relativePath ,    fileStatus )  )  ;", "Assert . assertTrue ( relativePath . toString (  )  . equals (  \"  \"  )  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  \" Unexpected   exception   encountered .  \"  )  ;", ". LOG . error (  \" Unexpected   exception :     \"  ,    e )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,    testRootString )  ;", "IOUtils . closeStream ( reader )  ;", "}", "}", "METHOD_END"], "methodName": ["testBuildListingForSingleFile"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    null ;", "try    {", "fs    =    FileSystem . get ( getConf (  )  )  ;", "List < Path >    srcPaths    =    new   ArrayList < Path >  (  )  ;", "srcPaths . add ( new   Path (  \"  / tmp / in /  *  /  *  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in / src 1  /  1  . txt \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in / src 2  /  1  . txt \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / out \"  )  ;", "Path   listingFile    =    new   Path (  \"  / tmp / list \"  )  ;", "DistCpOptions   options    =    new   DistCpOptions ( srcPaths ,    target )  ;", "CopyListing   listing    =    CopyListing . getCopyListing ( getConf (  )  ,     . CREDENTIALS ,    options )  ;", "try    {", "listing . buildListing ( listingFile ,    options )  ;", "Assert . fail (  \" Duplicates   not   detected \"  )  ;", "}    catch    ( CopyListing . DuplicateFileException   ignore )     {", "}", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   in   test \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed    \"     +     ( e . getMessage (  )  )  )  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDuplicates"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "File   inFile    =    File . createTempFile (  \" TestCopyListingIn \"  ,    null )  ;", "inFile . deleteOnExit (  )  ;", "File   outFile    =    File . createTempFile (  \" TestCopyListingOut \"  ,    null )  ;", "outFile . deleteOnExit (  )  ;", "List < Path >    srcs    =    new   ArrayList < Path >  (  )  ;", "srcs . add ( new   Path ( inFile . toURI (  )  )  )  ;", "Exception   expectedEx    =    new   IOException (  \" boom \"  )  ;", "SequenceFile . Writer   writer    =    mock ( Writer . class )  ;", "doThrow ( expectedEx )  . when ( writer )  . close (  )  ;", "SimpleCopyListing   listing    =    new   SimpleCopyListing ( getConf (  )  ,    TestCopyListing . CREDENTIALS )  ;", "DistCpOptions   options    =    new   DistCpOptions ( srcs ,    new   Path ( outFile . toURI (  )  )  )  ;", "Exception   actualEx    =    null ;", "try    {", "listing . doBuildListing ( writer ,    options )  ;", "}    catch    ( Exception   e )     {", "actualEx    =    e ;", "}", "Assert . assertNotNull (  \" close   writer   didn ' t   fail \"  ,    actualEx )  ;", "Assert . assertEquals ( expectedEx ,    actualEx )  ;", "}", "METHOD_END"], "methodName": ["testFailOnCloseError"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    null ;", "try    {", "fs    =    FileSystem . get ( getConf (  )  )  ;", "List < Path >    srcPaths    =    new   ArrayList < Path >  (  )  ;", "srcPaths . add ( new   Path (  \"  / tmp / in /  1  \"  )  )  ;", "srcPaths . add ( new   Path (  \"  / tmp / in /  2  \"  )  )  ;", "Path   target    =    new   Path (  \"  / tmp / out /  1  \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in /  1  \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in /  2  \"  )  ;", "fs . mkdirs ( target )  ;", "DistCpOptions   options    =    new   DistCpOptions ( srcPaths ,    target )  ;", "validatePaths ( options )  ;", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "target    =    new   Path (  \"  / tmp / out /  1  \"  )  ;", "fs . create ( target )  . close (  )  ;", "options    =    new   DistCpOptions ( srcPaths ,    target )  ;", "try    {", "validatePaths ( options )  ;", "Assert . fail (  \" Invalid   inputs   accepted \"  )  ;", "}    catch    ( CopyListing . InvalidInputException   ignore )     {", "}", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "srcPaths . clear (  )  ;", "srcPaths . add ( new   Path (  \"  / tmp / in /  1  \"  )  )  ;", "fs . mkdirs ( new   Path (  \"  / tmp / in /  1  \"  )  )  ;", "target    =    new   Path (  \"  / tmp / out /  1  \"  )  ;", "fs . create ( target )  . close (  )  ;", "options    =    new   DistCpOptions ( srcPaths ,    target )  ;", "try    {", "validatePaths ( options )  ;", "Assert . fail (  \" Invalid   inputs   accepted \"  )  ;", "}    catch    ( CopyListing . InvalidInputException   ignore )     {", "}", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  \" Test   input   validation   failed \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultipleSrcToFile"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "SimpleCopyListing   listing    =    new   SimpleCopyListing ( getConf (  )  ,    TestCopyListing . CREDENTIALS )     {", "@ Override", "protected   boolean   shouldCopy ( Path   path ,    DistCpOptions   options )     {", "return    !  ( path . getName (  )  . equals ( SUCCEEDED _ FILE _ NAME )  )  ;", "}", "}  ;", "FileSystem   fs    =    FileSystem . get ( getConf (  )  )  ;", "List < Path >    srcPaths    =    new   ArrayList < Path >  (  )  ;", "srcPaths . add ( new   Path (  \"  / tmp / in 4  /  1  \"  )  )  ;", "srcPaths . add ( new   Path (  \"  / tmp / in 4  /  2  \"  )  )  ;", "Path   target    =    new   Path (  \"  / tmp / out 4  /  1  \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in 4  /  1  /  _ SUCCESS \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in 4  /  1  / file \"  )  ;", "TestDistCpUtils . createFile ( fs ,     \"  / tmp / in 4  /  2  \"  )  ;", "fs . mkdirs ( target )  ;", "DistCpOptions   options    =    new   DistCpOptions ( srcPaths ,    target )  ;", "Path   listingFile    =    new   Path (  \"  / tmp / list 4  \"  )  ;", "listing . buildListing ( listingFile ,    options )  ;", "Assert . assertEquals ( listing . getNumberOfPaths (  )  ,     3  )  ;", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader ( getConf (  )  ,    Reader . file ( listingFile )  )  ;", "CopyListingFileStatus   fileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   relativePath    =    new   Text (  )  ;", "Assert . assertTrue ( reader . next ( relativePath ,    fileStatus )  )  ;", "Assert . assertEquals ( relativePath . toString (  )  ,     \"  /  1  \"  )  ;", "Assert . assertTrue ( reader . next ( relativePath ,    fileStatus )  )  ;", "Assert . assertEquals ( relativePath . toString (  )  ,     \"  /  1  / file \"  )  ;", "Assert . assertTrue ( reader . next ( relativePath ,    fileStatus )  )  ;", "Assert . assertEquals ( relativePath . toString (  )  ,     \"  /  2  \"  )  ;", "Assert . assertFalse ( reader . next ( relativePath ,    fileStatus )  )  ;", "}", "METHOD_END"], "methodName": ["testSkipCopy"], "fileName": "org.apache.hadoop.tools.TestCopyListing"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( expected . getUserName (  )  ,    actual . getOwner (  )  )  ;", "assertEquals ( expected . getGroupName (  )  ,    actual . getGroup (  )  )  ;", "FsPermission   perm    =    expected . getPermission (  )  ;", "if    (  ( actual . isFile (  )  )     &  &     ( expected . defaultPerm )  )     {", "perm    =    perm . applyUMask (  . UMASK )  ;", "}", "assertEquals ( perm ,    actual . getPermission (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkFileStatus"], "fileName": "org.apache.hadoop.tools.TestDistCh"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  (  (  \" root =  \"     +    root )     +     \"  ,    returnvalue =  \"  )     +    returnvalue )  )  ;", "final   ByteArrayOutputStream   bytes    =    new   ByteArrayOutputStream (  )  ;", "final   PrintStream   out    =    new   PrintStream ( bytes )  ;", "final   PrintStream   oldOut    =    System . out ;", "final   PrintStream   oldErr    =    System . err ;", "System . setOut ( out )  ;", "System . setErr ( out )  ;", "final   String   results ;", "try    {", "assertEquals ( returnvalue ,    shell . run ( new   String [  ]  {     \"  - lsr \"  ,    root    }  )  )  ;", "results    =    bytesString (  )  ;", "}    finally    {", "IOUtils . closeStream ( out )  ;", "System . setOut ( oldOut )  ;", "System . setErr ( oldErr )  ;", "}", "System . out . println (  (  \" results :  \\ n \"     +    results )  )  ;", "return   results ;", "}", "METHOD_END"], "methodName": ["runLsr"], "fileName": "org.apache.hadoop.tools.TestDistCh"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  (  (  (  ( CapacitySchedulerConfiguration . PREFIX )     +     ( CapacitySchedulerConfiguration . ROOT )  )     +     \"  .  \"  )     +     ( CapacitySchedulerConfiguration . QUEUES )  )  ,     \" default \"  )  ;", "conf . set (  (  (  (  ( CapacitySchedulerConfiguration . PREFIX )     +     ( CapacitySchedulerConfiguration . ROOT )  )     +     \"  . default .  \"  )     +     ( CapacitySchedulerConfiguration . CAPACITY )  )  ,     \"  1  0  0  \"  )  ;", "final   MiniDFSCluster   cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . format ( true )  . build (  )  ;", "final   FileSystem   fs    =    cluster . getFileSystem (  )  ;", "final   FsShell   shell    =    new   FsShell ( conf )  ;", "try    {", "final    . FileTree   tree    =    new    . FileTree ( fs ,     \" testDistCh \"  )  ;", "final   FileStatus   rootstatus    =    fs . getFileStatus ( tree . rootdir )  ;", ". runLsr ( shell ,    tree . root ,     0  )  ;", "final   String [  ]    args    =    new   String [  . NUN _ SUBS ]  ;", "final    . ChPermissionStatus [  ]    newstatus    =    new    . ChPermissionStatus [  . NUN _ SUBS ]  ;", "args [  0  ]     =     \"  / test / testDistCh / sub 0  : sub 1  :  :  \"  ;", "newstatus [  0  ]     =    new    . ChPermissionStatus ( rootstatus ,     \" sub 1  \"  ,     \"  \"  ,     \"  \"  )  ;", "args [  1  ]     =     \"  / test / testDistCh / sub 1  :  : sub 2  :  \"  ;", "newstatus [  1  ]     =    new    . ChPermissionStatus ( rootstatus ,     \"  \"  ,     \" sub 2  \"  ,     \"  \"  )  ;", "args [  2  ]     =     \"  / test / testDistCh / sub 2  :  :  :  4  3  7  \"  ;", "newstatus [  2  ]     =    new    . ChPermissionStatus ( rootstatus ,     \"  \"  ,     \"  \"  ,     \"  4  3  7  \"  )  ;", "args [  3  ]     =     \"  / test / testDistCh / sub 3  : sub 1  : sub 2  :  4  4  7  \"  ;", "newstatus [  3  ]     =    new    . ChPermissionStatus ( rootstatus ,     \" sub 1  \"  ,     \" sub 2  \"  ,     \"  4  4  7  \"  )  ;", "args [  4  ]     =     \"  / test / testDistCh / sub 4  :  : sub 5  :  4  3  7  \"  ;", "newstatus [  4  ]     =    new    . ChPermissionStatus ( rootstatus ,     \"  \"  ,     \" sub 5  \"  ,     \"  4  3  7  \"  )  ;", "args [  5  ]     =     \"  / test / testDistCh / sub 5  : sub 1  : sub 5  :  \"  ;", "newstatus [  5  ]     =    new    . ChPermissionStatus ( rootstatus ,     \" sub 1  \"  ,     \" sub 5  \"  ,     \"  \"  )  ;", "args [  6  ]     =     \"  / test / testDistCh / sub 6  : sub 3  :  :  4  3  7  \"  ;", "newstatus [  6  ]     =    new    . ChPermissionStatus ( rootstatus ,     \" sub 3  \"  ,     \"  \"  ,     \"  4  3  7  \"  )  ;", "System . out . println (  (  \" args =  \"     +     ( Arrays . asList ( args )  . toString (  )  . replace (  \"  ,  \"  ,     \"  ,  \\ n       \"  )  )  )  )  ;", "System . out . println (  (  \" newstatus =  \"     +     ( Arrays . asList ( newstatus )  . toString (  )  . replace (  \"  ,  \"  ,     \"  ,  \\ n       \"  )  )  )  )  ;", "new   DistCh ( MiniMRClientClusterFactory . create ( this . getClass (  )  ,     2  ,    conf )  . getConfig (  )  )  . run ( args )  ;", ". runLsr ( shell ,    tree . root ,     0  )  ;", "for    ( int   i    =     0  ;    i    <     (  . NUN _ SUBS )  ;    i +  +  )     {", "Path   sub    =    new   Path (  (  (  ( tree . root )     +     \"  / sub \"  )     +    i )  )  ;", ". checkFileStatus ( newstatus [ i ]  ,    fs . getFileStatus ( sub )  )  ;", "for    ( FileStatus   status    :    fs . listStatus ( sub )  )     {", ". checkFileStatus ( newstatus [ i ]  ,    status )  ;", "}", "}", "}    finally    {", "cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDistCh"], "fileName": "org.apache.hadoop.tools.TestDistCh"}, {"methodBody": ["METHOD_START", "{", "for    ( TestDistCpSystem . FileEntry   entry    :    entries )     {", "Path   newpath    =    new   Path (  (  ( topdir    +     \"  /  \"  )     +     ( entry . getPath (  )  )  )  )  ;", "if    ( entry . isDirectory (  )  )     {", "fs . mkdirs ( newpath )  ;", "} else    {", "OutputStream   out    =    fs . create ( newpath )  ;", "try    {", "out . write (  (  ( topdir    +     \"  /  \"  )     +    entry )  . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}    finally    {", "out . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "fs . delete ( new   Path ( topdir )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["deldir"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    new   Path ( topdir )  ;", "L < FileStatus >    statuses    =    new   ArrayL < FileStatus >  (  )  ;", "for    ( int   idx    =     0  ;    idx    <     ( files . length )  ;     +  + idx )     {", "Path   newpath    =    new   Path ( root ,    files [ idx ]  . getPath (  )  )  ;", "statuses . add ( fs . getFileStatus ( newpath )  )  ;", "}", "return   statuses . toArray ( new   FileStatus [ statuses . size (  )  ]  )  ;", "}", "METHOD_END"], "methodName": ["getFileStatus"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "TestDistCpSystem . FileEntry [  ]    srcfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry ( TestDistCpSystem . SRCDAT ,    true )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . SRCDAT )     +     \"  / a \"  )  ,    false )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . SRCDAT )     +     \"  / b \"  )  ,    true )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . SRCDAT )     +     \"  / b / c \"  )  ,    false )     }  ;", "TestDistCpSystem . FileEntry [  ]    dstfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry ( TestDistCpSystem . DSTDAT ,    true )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . DSTDAT )     +     \"  / a \"  )  ,    false )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . DSTDAT )     +     \"  / b \"  )  ,    true )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . DSTDAT )     +     \"  / b / c \"  )  ,    false )     }  ;", "testPreserveUserHelper ( srcfiles ,    srcfiles ,    false ,    true ,    false )  ;", "testPreserveUserHelper ( srcfiles ,    dstfiles ,    false ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testPreserveUseNonEmptyDir"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "TestDistCpSystem . FileEntry [  ]    srcfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry ( TestDistCpSystem . SRCDAT ,    true )     }  ;", "TestDistCpSystem . FileEntry [  ]    dstfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry ( TestDistCpSystem . DSTDAT ,    true )     }  ;", "testPreserveUserHelper ( srcfiles ,    srcfiles ,    false ,    true ,    false )  ;", "testPreserveUserHelper ( srcfiles ,    dstfiles ,    false ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testPreserveUserEmptyDir"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    null ;", "MiniDFSCluster   cluster    =    null ;", "try    {", "final   String   testRoot    =     \"  / testdir \"  ;", "final   String   testSrcRel    =     . SRCDAT ;", "final   String   testSrc    =     ( testRoot    +     \"  /  \"  )     +    testSrcRel ;", "final   String   testDstRel    =     . DSTDAT ;", "final   String   testDst    =     ( testRoot    +     \"  /  \"  )     +    testDstRel ;", "conf    =    new   Configuration (  )  ;", "cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "String   nnUri    =    FileSystem . getDefaultUri ( conf )  . toString (  )  ;", "FileSystem   fs    =    FileSystem . get ( URI . create ( nnUri )  ,    conf )  ;", "fs . mkdirs ( new   Path ( testRoot )  )  ;", "if    ( createSrcDir )     {", "fs . mkdirs ( new   Path ( testSrc )  )  ;", "}", "if    ( createTgtDir )     {", "fs . mkdirs ( new   Path ( testDst )  )  ;", "}", "createFiles ( fs ,    testRoot ,    srcEntries )  ;", "FileStatus [  ]    srcstats    =     . getFileStatus ( fs ,    testRoot ,    srcEntries )  ;", "for    ( int   i    =     0  ;    i    <     ( srcEntries . length )  ;    i +  +  )     {", "fs . setOwner ( srcstats [ i ]  . getPath (  )  ,     (  \" u \"     +    i )  ,    null )  ;", "}", "String [  ]    args    =     ( update )     ?    new   String [  ]  {     \"  - pu \"  ,     \"  - update \"  ,    nnUri    +    testSrc ,    nnUri    +    testDst    }     :    new   String [  ]  {     \"  - pu \"  ,    nnUri    +    testSrc ,    nnUri    +    testDst    }  ;", "ToolRunner . run ( conf ,    new   DistCp (  )  ,    args )  ;", "String   realTgtPath    =    testDst ;", "if    (  ! createTgtDir )     {", "realTgtPath    =    testRoot ;", "}", "FileStatus [  ]    dststat    =     . getFileStatus ( fs ,    realTgtPath ,    dstEntries )  ;", "for    ( int   i    =     0  ;    i    <     ( dststat . length )  ;    i +  +  )     {", "assertEquals (  (  \" i =  \"     +    i )  ,     (  \" u \"     +    i )  ,    dststat [ i ]  . getOwner (  )  )  ;", "}", ". deldir ( fs ,    testRoot )  ;", "}    finally    {", "if    ( cluster    !  =    null )     {", "cluster . shutdown (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testPreserveUserHelper"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "TestDistCpSystem . FileEntry [  ]    srcfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . SRCDAT )     +     \"  / a \"  )  ,    false )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . SRCDAT )     +     \"  / b \"  )  ,    true )  ,    new   TestDistCpSystem . FileEntry (  (  ( TestDistCpSystem . SRCDAT )     +     \"  / b / c \"  )  ,    false )     }  ;", "TestDistCpSystem . FileEntry [  ]    dstfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry (  \" a \"  ,    false )  ,    new   TestDistCpSystem . FileEntry (  \" b \"  ,    true )  ,    new   TestDistCpSystem . FileEntry (  \" b / c \"  ,    false )     }  ;", "testPreserveUserHelper ( srcfiles ,    dstfiles ,    true ,    true ,    true )  ;", "}", "METHOD_END"], "methodName": ["testPreserveUserNonEmptyDirWithUpdate"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "TestDistCpSystem . FileEntry [  ]    srcfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry ( TestDistCpSystem . SRCDAT ,    false )     }  ;", "TestDistCpSystem . FileEntry [  ]    dstfiles    =    new   TestDistCpSystem . FileEntry [  ]  {    new   TestDistCpSystem . FileEntry ( TestDistCpSystem . DSTDAT ,    false )     }  ;", "testPreserveUserHelper ( srcfiles ,    srcfiles ,    false ,    true ,    false )  ;", "testPreserveUserHelper ( srcfiles ,    dstfiles ,    false ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testPreserveUserSingleFile"], "fileName": "org.apache.hadoop.tools.TestDistCpSystem"}, {"methodBody": ["METHOD_START", "{", "OutputStream   out    =    TestDistCpViewFs . fs . create ( listFile )  ;", "try    {", "for    ( String   entry    :    entries )     {", "out . write (  (  (  ( TestDistCpViewFs . root )     +     \"  /  \"  )     +    entry )  . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}", "}    finally    {", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addEntries"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     3  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiFileTargetMissing"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    sync )  ;", "checkResult (  . target ,     3  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiFileTargetPresent"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singledir \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     1  ,     \" dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singlefile 1  / file 1  \"  )  ;", "createFiles (  \" singlefile 1  / file 1  \"  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileMissingTarget"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singlefile 2  / file 2  \"  )  ;", "createFiles (  \" singlefile 2  / file 2  \"  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    sync )  ;", "checkResult (  . target ,     1  ,     \" file 2  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileTargetDir"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singlefile 1  / file 1  \"  )  ;", "createFiles (  \" singlefile 1  / file 1  \"  ,     . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileTargetFile"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( count ,    TestDistCpViewFs . fs . listStatus ( target )  . length )  ;", "if    (  ( relPaths    =  =    null )     |  |     (  ( relPaths . length )     =  =     0  )  )     {", "Assert . assertTrue ( target . toString (  )  ,    TestDistCpViewFs . fs . exists ( target )  )  ;", "return ;", "}", "for    ( String   relPath    :    relPaths )     {", "Assert . assertTrue ( new   Path ( target ,    relPath )  . toString (  )  ,    TestDistCpViewFs . fs . exists ( new   Path ( target ,    relPath )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkResult"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "String   e ;", "for    ( String   entry    :    entries )     {", "if    ( new   Path ( entry )  . isAbsolute (  )  )     {", "e    =    entry ;", "} else    {", "e    =     (  (  . root )     +     \"  /  \"  )     +    entry ;", "}", "OutputStream   out    =     . fs . create ( new   Path ( e )  )  ;", "try    {", "out . write ( e . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}    finally    {", "out . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  \" mapred . job . tracker \"  ,     \" local \"  )  ;", "conf . set (  \" fs . default . name \"  ,     \" file :  /  /  /  \"  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "for    ( String   entry    :    entries )     {", ". fs . mkdirs ( new   Path ( entry )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    new   DistCpOptions ( listFile ,    target )  ;", "options . setSyncFolder ( sync )  ;", "options . setTargetPathExists ( targetExists )  ;", "try    {", "new   DistCp (  . getConf (  )  ,    options )  . execute (  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["runTest"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   fswd    =    FileSystem . get (  . getConf (  )  )  . getWorkingDirectory (  )  ;", "Configuration   vConf    =    ViewFileSystemTestSetup . createConfig ( false )  ;", "ConfigUtil . addLink ( vConf ,     \"  / usr \"  ,    new   URI ( fswd . toString (  )  )  )  ;", ". fs    =    FileSystem . get ( VIEWFS _ URI ,    vConf )  ;", ". fs . setWorkingDirectory ( new   Path (  \"  / usr \"  )  )  ;", ". listFile    =    new   Path (  \" target / tmp / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", ". target    =    new   Path (  \" target / tmp / target \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", ". root    =    new   Path (  \" target / tmp \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  . toString (  )  ;", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  /  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir 1  / dir 3  / file 7  \"  ,     \" singledir 1  / dir 3  / file 8  \"  ,     \" singledir 1  / dir 3  / file 9  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    false )  ;", "checkResult (  . target ,     4  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" dir 3  / file 7  \"  ,     \" dir 3  / file 8  \"  ,     \" dir 3  / file 9  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGlobTargetMissingMultiLevel"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir / dir 2  / file 6  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    false )  ;", "checkResult (  . target ,     2  ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  ,     \" singledir / dir 2  / file 6  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGlobTargetMissingSingleLevel"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile \"  ,     \" singledir \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    false )  ;", "checkResult (  . target ,     2  ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  ,     \" singledir / dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultiDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile \"  ,     \" singledir \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  . target . toString (  )  ,     (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    false )  ;", "checkResult (  . target ,     2  ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  ,     \" singledir / dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultiDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "caseMultiFileTargetMissing ( false )  ;", "caseMultiFileTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiFileTargetMissing"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "caseMultiFileTargetPresent ( false )  ;", "caseMultiFileTargetPresent ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiFileTargetPresent"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "caseSingleDirTargetMissing ( false )  ;", "caseSingleDirTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singledir \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    false )  ;", "checkResult (  . target ,     1  ,     \" singledir / dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileMissingTarget ( false )  ;", "caseSingleFileMissingTarget ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileMissingTarget"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileTargetDir ( false )  ;", "caseSingleFileTargetDir ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileTargetDir"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileTargetFile ( false )  ;", "caseSingleFileTargetFile ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileTargetFile"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  /  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir 1  / dir 3  / file 7  \"  ,     \" singledir 1  / dir 3  / file 8  \"  ,     \" singledir 1  / dir 3  / file 9  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    true )  ;", "checkResult (  . target ,     6  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" file 7  \"  ,     \" file 8  \"  ,     \" file 9  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateGlobTargetMissingMultiLevel"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir / dir 2  / file 6  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    true )  ;", "checkResult (  . target ,     4  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" dir 2  / file 6  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateGlobTargetMissingSingleLevel"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile \"  ,     \" singledir \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    true )  ;", "checkResult (  . target ,     4  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateMultiDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" Umultifile \"  ,     \" Usingledir \"  )  ;", "createFiles (  \" Umultifile / Ufile 3  \"  ,     \" Umultifile / Ufile 4  \"  ,     \" Umultifile / Ufile 5  \"  )  ;", "mkdirs (  . target . toString (  )  ,     (  (  . root )     +     \"  / Usingledir / Udir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    true )  ;", "checkResult (  . target ,     4  ,     \" Ufile 3  \"  ,     \" Ufile 4  \"  ,     \" Ufile 5  \"  ,     \" Udir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateMultiDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" Usingledir \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / Usingledir / Udir 1  \"  )  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    true )  ;", "checkResult (  . target ,     1  ,     \" Udir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateSingleDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestDistCpViewFs"}, {"methodBody": ["METHOD_START", "{", "return   new   AclEntry . Builder (  )  . setScope ( scope )  . setType ( type )  . setName ( name )  . setPermission ( permission )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["aclEntry"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "return   new   AclEntry . Builder (  )  . setScope ( scope )  . setType ( type )  . setPermission ( permission )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["aclEntry"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "assertArrayEquals ( entries ,    TestDistCpWithAcls . fs . getAclStatus ( new   Path ( path )  )  . getEntries (  )  . toArray ( new   AclEntry [  0  ]  )  )  ;", "}", "METHOD_END"], "methodName": ["assertAclEntries"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( perm ,    TestDistCpWithAcls . fs . getFileStatus ( new   Path ( path )  )  . getPermission (  )  . toShort (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertPermission"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "DistCp   distCp    =    new   DistCp ( TestDistCpWithAcls . conf ,    null )  ;", "assertEquals ( exitCode ,    ToolRunner . run ( TestDistCpWithAcls . conf ,    distCp ,    new   String [  ]  {     \"  - pa \"  ,     \"  / src \"  ,    dst    }  )  )  ;", "}", "METHOD_END"], "methodName": ["assertRunDistCp"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithAcls . initCluster ( true ,    true )  ;", "TestDistCpWithAcls . fs . mkdirs ( new   Path (  \"  / src / dir 1  / subdir 1  \"  )  )  ;", "TestDistCpWithAcls . fs . mkdirs ( new   Path (  \"  / src / dir 2  \"  )  )  ;", "TestDistCpWithAcls . fs . create ( new   Path (  \"  / src / dir 2  / file 2  \"  )  )  . close (  )  ;", "TestDistCpWithAcls . fs . create ( new   Path (  \"  / src / dir 2  / file 3  \"  )  )  . close (  )  ;", "TestDistCpWithAcls . fs . mkdirs ( new   Path (  \"  / src / dir 3 sticky \"  )  )  ;", "TestDistCpWithAcls . fs . create ( new   Path (  \"  / src / file 1  \"  )  )  . close (  )  ;", "TestDistCpWithAcls . fs . modifyAclEntries ( new   Path (  \"  / src / dir 1  \"  )  ,    Arrays . asList ( TestDistCpWithAcls . aclEntry ( DEFAULT ,    USER ,     \" bruce \"  ,    ALL )  )  )  ;", "TestDistCpWithAcls . fs . modifyAclEntries ( new   Path (  \"  / src / dir 2  / file 2  \"  )  ,    Arrays . asList ( TestDistCpWithAcls . aclEntry ( ACCESS ,    GROUP ,     \" sales \"  ,    NONE )  )  )  ;", "TestDistCpWithAcls . fs . setPermission ( new   Path (  \"  / src / dir 2  / file 3  \"  )  ,    new   FsPermission (  (  ( short )     (  4  3  2  )  )  )  )  ;", "TestDistCpWithAcls . fs . modifyAclEntries ( new   Path (  \"  / src / file 1  \"  )  ,    Arrays . asList ( TestDistCpWithAcls . aclEntry ( ACCESS ,    USER ,     \" diana \"  ,    READ )  )  )  ;", "TestDistCpWithAcls . fs . setPermission ( new   Path (  \"  / src / dir 3 sticky \"  )  ,    new   FsPermission (  (  ( short )     (  1  0  2  3  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithAcls . conf    =    new   Configuration (  )  ;", "TestDistCpWithAcls . conf . setBoolean ( DFS _ NAMENODE _ ACLS _ ENABLED _ KEY ,    aclsEnabled )  ;", "TestDistCpWithAcls . conf . set ( FS _ DEFAULT _ NAME _ KEY ,     \" stubfs :  /  /  /  \"  )  ;", "TestDistCpWithAcls . conf . setClass (  \" fs . stubfs . impl \"  ,    TestDistCpWithAcls . StubFileSystem . class ,    FileSystem . class )  ;", "TestDistCpWithAcls . cluster    =    new   Builder ( TestDistCpWithAcls . conf )  . numDataNodes (  1  )  . format ( format )  . build (  )  ;", "TestDistCpWithAcls . cluster . waitActive (  )  ;", "TestDistCpWithAcls . fs    =    TestDistCpWithAcls . cluster . getFileSystem (  )  ;", "}", "METHOD_END"], "methodName": ["initCluster"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithAcls . shutdown (  )  ;", "TestDistCpWithAcls . initCluster ( false ,    aclsEnabled )  ;", "}", "METHOD_END"], "methodName": ["restart"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "IOUtils . cleanup ( null ,    TestDistCpWithAcls . fs )  ;", "if    (  ( TestDistCpWithAcls . cluster )     !  =    null )     {", "TestDistCpWithAcls . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "try    {", ". restart ( false )  ;", ". assertRunDistCp ( DistCpConstants . ACLS _ NOT _ SUPPORTED ,     \"  / dstAclsNotEnabled \"  )  ;", "}    finally    {", ". restart ( true )  ;", "}", "}", "METHOD_END"], "methodName": ["testAclsNotEnabled"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithAcls . assertRunDistCp ( DistCpConstants . ACLS _ NOT _ SUPPORTED ,     \" stubfs :  /  / dstAclsNotImplemented \"  )  ;", "}", "METHOD_END"], "methodName": ["testAclsNotImplemented"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithAcls . assertRunDistCp ( DistCpConstants . SUCCESS ,     \"  / dstPreserveAcls \"  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / dir 1  \"  ,    new   AclEntry [  ]  {    TestDistCpWithAcls . aclEntry ( DEFAULT ,    USER ,    ALL )  ,    TestDistCpWithAcls . aclEntry ( DEFAULT ,    USER ,     \" bruce \"  ,    ALL )  ,    TestDistCpWithAcls . aclEntry ( DEFAULT ,    GROUP ,    READ _ EXECUTE )  ,    TestDistCpWithAcls . aclEntry ( DEFAULT ,    MASK ,    ALL )  ,    TestDistCpWithAcls . aclEntry ( DEFAULT ,    OTHER ,    READ _ EXECUTE )     }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / dir 1  \"  ,     (  ( short )     (  4  9  3  )  )  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / dir 1  / subdir 1  \"  ,    new   AclEntry [  ]  {        }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / dir 1  / subdir 1  \"  ,     (  ( short )     (  4  9  3  )  )  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / dir 2  \"  ,    new   AclEntry [  ]  {        }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / dir 2  \"  ,     (  ( short )     (  4  9  3  )  )  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / dir 2  / file 2  \"  ,    new   AclEntry [  ]  {    TestDistCpWithAcls . aclEntry ( ACCESS ,    GROUP ,    READ )  ,    TestDistCpWithAcls . aclEntry ( ACCESS ,    GROUP ,     \" sales \"  ,    NONE )     }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / dir 2  / file 2  \"  ,     (  ( short )     (  4  2  0  )  )  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / dir 2  / file 3  \"  ,    new   AclEntry [  ]  {        }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / dir 2  / file 3  \"  ,     (  ( short )     (  4  3  2  )  )  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / dir 3 sticky \"  ,    new   AclEntry [  ]  {        }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / dir 3 sticky \"  ,     (  ( short )     (  1  0  2  3  )  )  )  ;", "TestDistCpWithAcls . assertAclEntries (  \"  / dstPreserveAcls / file 1  \"  ,    new   AclEntry [  ]  {    TestDistCpWithAcls . aclEntry ( ACCESS ,    USER ,     \" diana \"  ,    READ )  ,    TestDistCpWithAcls . aclEntry ( ACCESS ,    GROUP ,    READ )     }  )  ;", "TestDistCpWithAcls . assertPermission (  \"  / dstPreserveAcls / file 1  \"  ,     (  ( short )     (  4  2  0  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testPreserveAcls"], "fileName": "org.apache.hadoop.tools.TestDistCpWithAcls"}, {"methodBody": ["METHOD_START", "{", "initXAttrs (  )  ;", "DistCpTestUtils . assertRunDistCp ( expectedExitCode ,    src ,    dest ,    preserveOpts ,     . conf )  ;", "if    ( expectedExitCode    =  =     ( DistCpConstants . SUCCESS )  )     {", "Map < String ,    byte [  ]  >    xAttrs    =    Maps . newHashMap (  )  ;", "for    ( Path   p    :     . pathnames )     {", "xAttrs . clear (  )  ;", "if    ( expectRaw )     {", "xAttrs . put (  . rawName 1  ,     . rawValue 1  )  ;", "}", "if    ( expectUser )     {", "xAttrs . put (  . userName 1  ,     . userValue 1  )  ;", "}", "DistCpTestUtils . assertXAttrs ( new   Path ( dest ,    p )  ,     . fs ,    xAttrs )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["doTestPreserveRawXAttrs"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithRawXAttrs . conf    =    new   Configuration (  )  ;", "TestDistCpWithRawXAttrs . conf . setBoolean ( DFS _ NAMENODE _ XATTRS _ ENABLED _ KEY ,    true )  ;", "TestDistCpWithRawXAttrs . cluster    =    new   Builder ( TestDistCpWithRawXAttrs . conf )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "TestDistCpWithRawXAttrs . cluster . waitActive (  )  ;", "TestDistCpWithRawXAttrs . fs    =    TestDistCpWithRawXAttrs . cluster . getFileSystem (  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithRawXAttrs . makeFilesAndDirs ( TestDistCpWithRawXAttrs . fs )  ;", "for    ( Path   p    :    TestDistCpWithRawXAttrs . pathnames )     {", "TestDistCpWithRawXAttrs . fs . setXAttr ( new   Path (  (  ( TestDistCpWithRawXAttrs . rawRootName )     +     \"  / src \"  )  ,    p )  ,    TestDistCpWithRawXAttrs . rawName 1  ,    TestDistCpWithRawXAttrs . rawValue 1  )  ;", "TestDistCpWithRawXAttrs . fs . setXAttr ( new   Path (  (  ( TestDistCpWithRawXAttrs . rawRootName )     +     \"  / src \"  )  ,    p )  ,    TestDistCpWithRawXAttrs . userName 1  ,    TestDistCpWithRawXAttrs . userValue 1  )  ;", "}", "}", "METHOD_END"], "methodName": ["initXAttrs"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "fs . delete ( new   Path (  \"  / src \"  )  ,    true )  ;", "fs . delete ( new   Path (  \"  / dest \"  )  ,    true )  ;", "fs . mkdirs (  . subDir 1  )  ;", "fs . create (  . file 1  )  . close (  )  ;", "}", "METHOD_END"], "methodName": ["makeFilesAndDirs"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "IOUtils . cleanup ( null ,    TestDistCpWithRawXAttrs . fs )  ;", "if    (  ( TestDistCpWithRawXAttrs . cluster )     !  =    null )     {", "TestDistCpWithRawXAttrs . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "final   String   relSrc    =     \"  /  .  /  . reserved /  .  .  /  . reserved / raw /  .  .  / raw / src /  .  .  / src \"  ;", "final   String   relDst    =     \"  /  .  /  . reserved /  .  .  /  . reserved / raw /  .  .  / raw / dest /  .  .  / dest \"  ;", "doTestPreserveRawXAttrs ( relSrc ,    relDst ,     \"  - px \"  ,    true ,    true ,    DistCpConstants . SUCCESS )  ;", "doTestPreserveRawXAttrs (  . rootedSrcName ,     . rootedDestName ,     \"  - px \"  ,    false ,    true ,    DistCpConstants . SUCCESS )  ;", "doTestPreserveRawXAttrs (  . rootedSrcName ,     . rawDestName ,     \"  - px \"  ,    false ,    true ,    DistCpConstants . INVALID _ ARGUMENT )  ;", "doTestPreserveRawXAttrs (  . rawSrcName ,     . rootedDestName ,     \"  - px \"  ,    false ,    true ,    DistCpConstants . INVALID _ ARGUMENT )  ;", "doTestPreserveRawXAttrs (  . rawSrcName ,     . rawDestName ,     \"  - px \"  ,    true ,    true ,    DistCpConstants . SUCCESS )  ;", "final   Path   savedWd    =     . fs . getWorkingDirectory (  )  ;", "try    {", ". fs . setWorkingDirectory ( new   Path (  \"  /  . reserved / raw \"  )  )  ;", "doTestPreserveRawXAttrs (  (  \"  .  .  /  .  .  \"     +     (  . rawSrcName )  )  ,     (  \"  .  .  /  .  .  \"     +     (  . rawDestName )  )  ,     \"  - px \"  ,    true ,    true ,    DistCpConstants . SUCCESS )  ;", "}    finally    {", ". fs . setWorkingDirectory ( savedWd )  ;", "}", "}", "METHOD_END"], "methodName": ["testPreserveRawXAttrs1"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rootedSrcName ,    TestDistCpWithRawXAttrs . rootedDestName ,     \"  - p \"  ,    false ,    false ,    DistCpConstants . SUCCESS )  ;", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rootedSrcName ,    TestDistCpWithRawXAttrs . rawDestName ,     \"  - p \"  ,    false ,    false ,    DistCpConstants . INVALID _ ARGUMENT )  ;", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rawSrcName ,    TestDistCpWithRawXAttrs . rootedDestName ,     \"  - p \"  ,    false ,    false ,    DistCpConstants . INVALID _ ARGUMENT )  ;", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rawSrcName ,    TestDistCpWithRawXAttrs . rawDestName ,     \"  - p \"  ,    true ,    false ,    DistCpConstants . SUCCESS )  ;", "}", "METHOD_END"], "methodName": ["testPreserveRawXAttrs2"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rootedSrcName ,    TestDistCpWithRawXAttrs . rootedDestName ,    null ,    false ,    false ,    DistCpConstants . SUCCESS )  ;", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rootedSrcName ,    TestDistCpWithRawXAttrs . rawDestName ,    null ,    false ,    false ,    DistCpConstants . INVALID _ ARGUMENT )  ;", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rawSrcName ,    TestDistCpWithRawXAttrs . rootedDestName ,    null ,    false ,    false ,    DistCpConstants . INVALID _ ARGUMENT )  ;", "doTestPreserveRawXAttrs ( TestDistCpWithRawXAttrs . rawSrcName ,    TestDistCpWithRawXAttrs . rawDestName ,    null ,    true ,    false ,    DistCpConstants . SUCCESS )  ;", "}", "METHOD_END"], "methodName": ["testPreserveRawXAttrs3"], "fileName": "org.apache.hadoop.tools.TestDistCpWithRawXAttrs"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithXAttrs . initCluster ( true ,    true )  ;", "TestDistCpWithXAttrs . fs . mkdirs ( TestDistCpWithXAttrs . subDir 1  )  ;", "TestDistCpWithXAttrs . fs . create ( TestDistCpWithXAttrs . file 1  )  . close (  )  ;", "TestDistCpWithXAttrs . fs . mkdirs ( TestDistCpWithXAttrs . dir 2  )  ;", "TestDistCpWithXAttrs . fs . create ( TestDistCpWithXAttrs . file 2  )  . close (  )  ;", "TestDistCpWithXAttrs . fs . create ( TestDistCpWithXAttrs . file 3  )  . close (  )  ;", "TestDistCpWithXAttrs . fs . create ( TestDistCpWithXAttrs . file 4  )  . close (  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . dir 1  ,    TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . dir 1  ,    TestDistCpWithXAttrs . name 2  ,    TestDistCpWithXAttrs . value 2  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . subDir 1  ,    TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . subDir 1  ,    TestDistCpWithXAttrs . name 3  ,    TestDistCpWithXAttrs . value 3  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 1  ,    TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 1  ,    TestDistCpWithXAttrs . name 2  ,    TestDistCpWithXAttrs . value 2  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 1  ,    TestDistCpWithXAttrs . name 3  ,    TestDistCpWithXAttrs . value 3  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . dir 2  ,    TestDistCpWithXAttrs . name 2  ,    TestDistCpWithXAttrs . value 2  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 2  ,    TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 2  ,    TestDistCpWithXAttrs . name 4  ,    TestDistCpWithXAttrs . value 4  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 3  ,    TestDistCpWithXAttrs . name 3  ,    TestDistCpWithXAttrs . value 3  )  ;", "TestDistCpWithXAttrs . fs . setXAttr ( TestDistCpWithXAttrs . file 3  ,    TestDistCpWithXAttrs . name 4  ,    TestDistCpWithXAttrs . value 4  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithXAttrs . conf    =    new   Configuration (  )  ;", "TestDistCpWithXAttrs . conf . setBoolean ( DFS _ NAMENODE _ XATTRS _ ENABLED _ KEY ,    xAttrsEnabled )  ;", "TestDistCpWithXAttrs . conf . set ( FS _ DEFAULT _ NAME _ KEY ,     \" stubfs :  /  /  /  \"  )  ;", "TestDistCpWithXAttrs . conf . setClass (  \" fs . stubfs . impl \"  ,    TestDistCpWithXAttrs . StubFileSystem . class ,    FileSystem . class )  ;", "TestDistCpWithXAttrs . cluster    =    new   Builder ( TestDistCpWithXAttrs . conf )  . numDataNodes (  1  )  . format ( format )  . build (  )  ;", "TestDistCpWithXAttrs . cluster . waitActive (  )  ;", "TestDistCpWithXAttrs . fs    =    TestDistCpWithXAttrs . cluster . getFileSystem (  )  ;", "}", "METHOD_END"], "methodName": ["initCluster"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "TestDistCpWithXAttrs . shutdown (  )  ;", "TestDistCpWithXAttrs . initCluster ( false ,    xAttrsEnabled )  ;", "}", "METHOD_END"], "methodName": ["restart"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "IOUtils . cleanup ( null ,    TestDistCpWithXAttrs . fs )  ;", "if    (  ( TestDistCpWithXAttrs . cluster )     !  =    null )     {", "TestDistCpWithXAttrs . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "DistCpTestUtils . assertRunDistCp ( DistCpConstants . SUCCESS ,    TestDistCpWithXAttrs . rootedSrcName ,     \"  / dstPreserveXAttrs \"  ,     \"  - px \"  ,    TestDistCpWithXAttrs . conf )  ;", "Map < String ,    byte [  ]  >    xAttrs    =    Maps . newHashMap (  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 2  ,    TestDistCpWithXAttrs . value 2  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstDir 1  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "xAttrs . clear (  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 3  ,    new   byte [  0  ]  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstSubDir 1  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "xAttrs . clear (  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 2  ,    TestDistCpWithXAttrs . value 2  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 3  ,    new   byte [  0  ]  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstFile 1  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "xAttrs . clear (  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 2  ,    TestDistCpWithXAttrs . value 2  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstDir 2  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "xAttrs . clear (  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 1  ,    TestDistCpWithXAttrs . value 1  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 4  ,    new   byte [  0  ]  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstFile 2  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "xAttrs . clear (  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 3  ,    new   byte [  0  ]  )  ;", "xAttrs . put ( TestDistCpWithXAttrs . name 4  ,    new   byte [  0  ]  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstFile 3  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "xAttrs . clear (  )  ;", "DistCpTestUtils . assertXAttrs ( TestDistCpWithXAttrs . dstFile 4  ,    TestDistCpWithXAttrs . fs ,    xAttrs )  ;", "}", "METHOD_END"], "methodName": ["testPreserveXAttrs"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "try    {", ". restart ( false )  ;", "DistCpTestUtils . assertRunDistCp ( DistCpConstants . XATTRS _ NOT _ SUPPORTED ,     . rootedSrcName ,     \"  / dstXAttrsNotEnabled \"  ,     \"  - px \"  ,     . conf )  ;", "}    finally    {", ". restart ( true )  ;", "}", "}", "METHOD_END"], "methodName": ["testXAttrsNotEnabled"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "DistCpTestUtils . assertRunDistCp ( DistCpConstants . XATTRS _ NOT _ SUPPORTED ,    TestDistCpWithXAttrs . rootedSrcName ,     \" stubfs :  /  / dstXAttrsNotImplemented \"  ,     \"  - px \"  ,    TestDistCpWithXAttrs . conf )  ;", "}", "METHOD_END"], "methodName": ["testXAttrsNotImplemented"], "fileName": "org.apache.hadoop.tools.TestDistCpWithXAttrs"}, {"methodBody": ["METHOD_START", "{", "Path   result    =    new   Path (  (  (  ( TestExternalCall . root )     +     \"  /  \"  )     +    fname )  )  ;", "OutputStream   out    =    TestExternalCall . fs . create ( result )  ;", "try    {", "out . write (  (  (  ( TestExternalCall . root )     +     \"  /  \"  )     +    fname )  . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}    finally    {", "out . close (  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.TestExternalCall"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . t (  \" fdefault . name \"  ,     \" file :  /  /  /  \"  )  ;", "conf . t (  \" mapred . job . tracker \"  ,     \" local \"  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.tools.TestExternalCall"}, {"methodBody": ["METHOD_START", "{", "securityManager    =    System . getSecurityManager (  )  ;", "System . setSecurityManager ( new    . NoExitSecurityManager (  )  )  ;", "try    {", ". fs    =    FileSystem . get (  . getConf (  )  )  ;", ". root    =    new   Path (  \" target / tmp \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  . toString (  )  ;", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.TestExternalCall"}, {"methodBody": ["METHOD_START", "{", "System . setSecurityManager ( securityManager )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.tools.TestExternalCall"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    TestExternalCall . getConf (  )  ;", "Path   stagingDir    =    JobSubmissionFiles . getStagingDir ( new   mapreduce . Cluster ( conf )  ,    conf )  ;", "stagingDir . getFileSystem ( conf )  . mkdirs ( stagingDir )  ;", "Path   soure    =    createFile (  \" tmp . txt \"  )  ;", "Path   target    =    createFile (  \" target . txt \"  )  ;", "DistCp   distcp    =    new   DistCp ( conf ,    null )  ;", "String [  ]    arg    =    new   String [  ]  {    soure . toString (  )  ,    target . toString (  )     }  ;", "distcp . run ( arg )  ;", "Assert . assertTrue ( TestExternalCall . fs . exists ( target )  )  ;", "}", "METHOD_END"], "methodName": ["testCleanup"], "fileName": "org.apache.hadoop.tools.TestExternalCall"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    TestExternalCall . getConf (  )  ;", "Path   stagingDir    =    JobSubmissionFiles . getStagingDir ( new   mapreduce . Cluster ( conf )  ,    conf )  ;", "stagingDir . getFileSystem ( conf )  . mkdirs ( stagingDir )  ;", "Path   soure    =    createFile (  \" tmp . txt \"  )  ;", "Path   target    =    createFile (  \" target . txt \"  )  ;", "try    {", "String [  ]    arg    =    new   String [  ]  {    target . toString (  )  ,    soure . toString (  )     }  ;", "DistCp . main ( arg )  ;", "Assert . fail (  )  ;", "}    catch    ( TestExternalCall . ExitException   t )     {", "Assert . assertTrue ( TestExternalCall . fs . exists ( target )  )  ;", "Assert . assertEquals ( t . status ,     0  )  ;", "Assert . assertEquals ( stagingDir . getFileSystem ( conf )  . listStatus ( stagingDir )  . length ,     0  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCleanupTestViaToolRunner"], "fileName": "org.apache.hadoop.tools.TestExternalCall"}, {"methodBody": ["METHOD_START", "{", "OutputStream   out    =    TestFileBasedCopyListing . fs . create ( listFile )  ;", "try    {", "for    ( String   entry    :    entries )     {", "out . write ( entry . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}", "}    finally    {", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addEntries"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "TestFileBasedCopyListing . map . put (  \"  / file 1  \"  ,     \"  / tmp / singlefile 1  / file 1  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 2  \"  ,     \"  / tmp / singlefile 2  / file 2  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 3  \"  ,     \"  / tmp / multifile / file 3  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 4  \"  ,     \"  / tmp / multifile / file 4  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 5  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / multifile / file 3  \"  ,     \"  / tmp / multifile / file 3  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / multifile / file 4  \"  ,     \"  / tmp / multifile / file 4  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / multifile / file 5  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Ufile 3  \"  ,     \"  / tmp / Umultifile / Ufile 3  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Ufile 4  \"  ,     \"  / tmp / Umultifile / Ufile 4  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Ufile 5  \"  ,     \"  / tmp / Umultifile / Ufile 5  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / dir 1  \"  ,     \"  / tmp / singledir / dir 1  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / singledir / dir 1  \"  ,     \"  / tmp / singledir / dir 1  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / dir 2  \"  ,     \"  / tmp / singledir / dir 2  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / singledir / dir 2  \"  ,     \"  / tmp / singledir / dir 2  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Udir 1  \"  ,     \"  / tmp / Usingledir / Udir 1  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Udir 2  \"  ,     \"  / tmp / Usingledir / Udir 2  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / dir 2  / file 6  \"  ,     \"  / tmp / singledir / dir 2  / file 6  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / singledir / dir 2  / file 6  \"  ,     \"  / tmp / singledir / dir 2  / file 6  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 7  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 7  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 8  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 8  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / file 9  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 9  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / dir 3  / file 7  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 7  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / dir 3  / file 8  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 8  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / dir 3  / file 9  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 9  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Ufile 7  \"  ,     \"  / tmp / Usingledir 1  / Udir 3  / Ufile 7  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Ufile 8  \"  ,     \"  / tmp / Usingledir 1  / Udir 3  / Ufile 8  \"  )  ;", "TestFileBasedCopyListing . map . put (  \"  / Ufile 9  \"  ,     \"  / tmp / Usingledir 1  / Udir 3  / Ufile 9  \"  )  ;", "}", "METHOD_END"], "methodName": ["buildExpectedValuesMap"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp 1  / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp /  *  /  *  \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "createFiles (  \"  / tmp / singledir 1  / dir 3  / file 7  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 8  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 9  \"  )  ;", "runTest ( listFile ,    target ,    sync )  ;", "checkResult ( listFile ,     6  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "TestDistCpUtils . delete (  . fs ,     \"  / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseGlobTargetMissingMultiLevel"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp 1  / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp /  *  \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "createFiles (  \"  / tmp / singledir / dir 2  / file 6  \"  )  ;", "runTest ( listFile ,    target ,    sync )  ;", "checkResult ( listFile ,     5  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "TestDistCpUtils . delete (  . fs ,     \"  / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseGlobTargetMissingSingleLevel"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / multifile \"  ,     \"  / tmp / singledir \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "mkdirs (  \"  / tmp / singledir / dir 1  \"  )  ;", "runTest ( listFile ,    target ,    sync )  ;", "checkResult ( listFile ,     4  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "runTest ( listFile ,    target ,    false ,    sync )  ;", "checkResult ( listFile ,     3  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiFileTargetMissing"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "mkdirs ( target . toString (  )  )  ;", "runTest ( listFile ,    target ,    true ,    sync )  ;", "checkResult ( listFile ,     3  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiFileTargetPresent"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / singledir \"  )  ;", "mkdirs (  \"  / tmp / singledir / dir 1  \"  )  ;", "runTest ( listFile ,    target ,    false ,    sync )  ;", "checkResult ( listFile ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / singlefile 1  / file 1  \"  )  ;", "createFiles (  \"  / tmp / singlefile 1  / file 1  \"  )  ;", "runTest ( listFile ,    target ,    false ,    sync )  ;", "checkResult ( listFile ,     0  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileMissingTarget"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / singlefile 2  / file 2  \"  )  ;", "createFiles (  \"  / tmp / singlefile 2  / file 2  \"  )  ;", "mkdirs ( target . toString (  )  )  ;", "runTest ( listFile ,    target ,    true ,    sync )  ;", "checkResult ( listFile ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileTargetDir"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / singlefile 1  / file 1  \"  )  ;", "createFiles (  \"  / tmp / singlefile 1  / file 1  \"  ,    target . toString (  )  )  ;", "runTest ( listFile ,    target ,    false ,    sync )  ;", "checkResult ( listFile ,     0  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileTargetFile"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "if    ( count    =  =     0  )     {", "return ;", "}", "int   recCount    =     0  ;", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader (  . config ,    Reader . file ( listFile )  )  ;", "try    {", "Text   relPath    =    new   Text (  )  ;", "CopyListingFileStatus   fileStatus    =    new   CopyListingFileStatus (  )  ;", "while    ( reader . next ( relPath ,    fileStatus )  )     {", "if    (  ( fileStatus . isDirectory (  )  )     &  &     ( relPath . toString (  )  . equals (  \"  \"  )  )  )     {", "continue ;", "}", "Assert . assertEquals ( fileStatus . getPath (  )  . toUri (  )  . getPath (  )  ,     . map . get ( relPath . toString (  )  )  )  ;", "recCount +  +  ;", "}", "}    finally    {", "IOUtils . closeStream ( reader )  ;", "}", "Assert . assertEquals ( recCount ,    count )  ;", "}", "METHOD_END"], "methodName": ["checkResult"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "TestFileBasedCopyListing . cluster    =    new   Builder ( TestFileBasedCopyListing . config )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "TestFileBasedCopyListing . fs    =    TestFileBasedCopyListing . cluster . getFileSystem (  )  ;", "TestFileBasedCopyListing . buildExpectedValuesMap (  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "for    ( String   entry    :    entries )     {", "OutputStream   out    =     . fs . create ( new   Path ( entry )  )  ;", "try    {", "out . write ( entry . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}    finally    {", "out . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestFileBasedCopyListing . cluster )     !  =    null )     {", "TestFileBasedCopyListing . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["destroy"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "for    ( String   entry    :    entries )     {", ". fs . mkdirs ( new   Path ( entry )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "runTest ( listFile ,    target ,    targetExists ,    true )  ;", "}", "METHOD_END"], "methodName": ["runTest"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "CopyListing   listing    =    new   FileBasedCopyListing ( TestFileBasedCopyListing . config ,    TestFileBasedCopyListing . CREDENTIALS )  ;", "DistCpOptions   options    =    new   DistCpOptions ( listFile ,    target )  ;", "options . setSyncFolder ( sync )  ;", "options . setTargetPathExists ( targetExists )  ;", "listing . buildListing ( listFile ,    options )  ;", "}", "METHOD_END"], "methodName": ["runTest"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp 1  / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp /  *  /  *  \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "createFiles (  \"  / tmp / singledir 1  / dir 3  / file 7  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 8  \"  ,     \"  / tmp / singledir 1  / dir 3  / file 9  \"  )  ;", "mkdirs ( target . toString (  )  )  ;", "runTest ( listFile ,    target ,    true )  ;", "checkResult ( listFile ,     6  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "TestDistCpUtils . delete (  . fs ,     \"  / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGlobTargetDirMultiLevel"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseGlobTargetMissingMultiLevel ( false )  ;", "caseGlobTargetMissingMultiLevel ( true )  ;", "}", "METHOD_END"], "methodName": ["testGlobTargetMissingMultiLevel"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseGlobTargetMissingSingleLevel ( false )  ;", "caseGlobTargetMissingSingleLevel ( true )  ;", "}", "METHOD_END"], "methodName": ["testGlobTargetMissingSingleLevel"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseMultiDirTargetMissing ( false )  ;", "caseMultiDirTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / multifile \"  ,     \"  / tmp / singledir \"  )  ;", "createFiles (  \"  / tmp / multifile / file 3  \"  ,     \"  / tmp / multifile / file 4  \"  ,     \"  / tmp / multifile / file 5  \"  )  ;", "mkdirs ( target . toString (  )  ,     \"  / tmp / singledir / dir 1  \"  )  ;", "runTest ( listFile ,    target ,    true )  ;", "checkResult ( listFile ,     4  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultiDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseMultiFileTargetMissing ( false )  ;", "caseMultiFileTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiFileTargetMissing"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseMultiFileTargetPresent ( false )  ;", "caseMultiFileTargetPresent ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiFileTargetPresent"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseSingleDirTargetMissing ( false )  ;", "caseSingleDirTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / singledir \"  )  ;", "mkdirs (  \"  / tmp / singledir / dir 1  \"  )  ;", "mkdirs ( target . toString (  )  )  ;", "runTest ( listFile ,    target ,    true )  ;", "checkResult ( listFile ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileMissingTarget ( false )  ;", "caseSingleFileMissingTarget ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileMissingTarget"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileTargetDir ( false )  ;", "caseSingleFileTargetDir ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileTargetDir"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileTargetFile ( false )  ;", "caseSingleFileTargetFile ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileTargetFile"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp 1  / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp /  *  /  *  \"  )  ;", "createFiles (  \"  / tmp / Umultifile / Ufile 3  \"  ,     \"  / tmp / Umultifile / Ufile 4  \"  ,     \"  / tmp / Umultifile / Ufile 5  \"  )  ;", "createFiles (  \"  / tmp / Usingledir 1  / Udir 3  / Ufile 7  \"  ,     \"  / tmp / Usingledir 1  / Udir 3  / Ufile 8  \"  ,     \"  / tmp / Usingledir 1  / Udir 3  / Ufile 9  \"  )  ;", "mkdirs ( target . toString (  )  )  ;", "runTest ( listFile ,    target ,    true )  ;", "checkResult ( listFile ,     6  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "TestDistCpUtils . delete (  . fs ,     \"  / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateGlobTargetDirMultiLevel"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / Umultifile \"  ,     \"  / tmp / Usingledir \"  )  ;", "createFiles (  \"  / tmp / Umultifile / Ufile 3  \"  ,     \"  / tmp / Umultifile / Ufile 4  \"  ,     \"  / tmp / Umultifile / Ufile 5  \"  )  ;", "mkdirs ( target . toString (  )  ,     \"  / tmp / Usingledir / Udir 1  \"  )  ;", "runTest ( listFile ,    target ,    true )  ;", "checkResult ( listFile ,     4  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateMultiDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \"  / tmp / listing \"  )  ;", "Path   target    =    new   Path (  \"  / tmp / target \"  )  ;", "addEntries ( listFile ,     \"  / tmp / Usingledir \"  )  ;", "mkdirs (  \"  / tmp / Usingledir / Udir 1  \"  )  ;", "mkdirs ( target . toString (  )  )  ;", "runTest ( listFile ,    target ,    true ,    true )  ;", "checkResult ( listFile ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   build   listing \"  ,    e )  ;", "Assert . fail (  \" build   listing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     \"  / tmp \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateSingleDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestFileBasedCopyListing"}, {"methodBody": ["METHOD_START", "{", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  1  \"  )  ;", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  2  \"  )  ;", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  2  /  3  \"  )  ;", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  2  /  3  /  4  \"  )  ;", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  5  \"  )  ;", "TestGlobbedCopyListing . touchFile (  \"  / tmp / source /  5  /  6  \"  )  ;", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  7  \"  )  ;", "TestGlobbedCopyListing . mkdirs (  \"  / tmp / source /  7  /  8  \"  )  ;", "TestGlobbedCopyListing . touchFile (  \"  / tmp / source /  7  /  8  /  9  \"  )  ;", "}", "METHOD_END"], "methodName": ["createSourceData"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fileSystem    =    null ;", "try    {", "fileSystem    =     . cluster . getFileSystem (  )  ;", "fileSystem . mkdirs ( new   Path ( path )  )  ;", ". recordInExpectedValues ( path )  ;", "}    finally    {", "IOUtils . cleanup ( null ,    fileSystem )  ;", "}", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fileSystem    =    TestGlobbedCopyListing . cluster . getFileSystem (  )  ;", "Path   sourcePath    =    new   Path (  (  ( fileSystem . getUri (  )  . toString (  )  )     +    path )  )  ;", "TestGlobbedCopyListing . expectedValues . put ( sourcePath . toString (  )  ,    DistCpUtils . getRelativePath ( new   Path (  \"  / tmp / source \"  )  ,    sourcePath )  )  ;", "}", "METHOD_END"], "methodName": ["recordInExpectedValues"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "TestGlobbedCopyListing . cluster    =    new   Builder ( new   Configuration (  )  )  . build (  )  ;", "TestGlobbedCopyListing . createSourceData (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "TestGlobbedCopyListing . cluster . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "final   URI   uri    =    TestGlobbedCopyListing . cluster . getFileSystem (  )  . getUri (  )  ;", "final   String   pathString    =    uri . toString (  )  ;", "Path   fileSystemPath    =    new   Path ( pathString )  ;", "Path   source    =    new   Path (  (  ( fileSystemPath . toString (  )  )     +     \"  / tmp / source \"  )  )  ;", "Path   target    =    new   Path (  (  ( fileSystemPath . toString (  )  )     +     \"  / tmp / target \"  )  )  ;", "Path   listingPath    =    new   Path (  (  ( fileSystemPath . toString (  )  )     +     \"  / tmp / META / fileList . seq \"  )  )  ;", "DistCpOptions   options    =    new   DistCpOptions ( Arrays . asList ( source )  ,    target )  ;", "options . setTargetPathExists ( false )  ;", "new   GlobbedCopyListing ( new   Configuration (  )  ,    TestGlobbedCopyListing . CREDENTIALS )  . buildListing ( listingPath ,    options )  ;", "verifyContents ( listingPath )  ;", "}", "METHOD_END"], "methodName": ["testRun"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fileSystem    =    null ;", "DataOutputStream   outputStream    =    null ;", "try    {", "fileSystem    =     . cluster . getFileSystem (  )  ;", "outputStream    =    fileSystem . create ( new   Path ( path )  ,    true ,     0  )  ;", ". recordInExpectedValues ( path )  ;", "}    finally    {", "IOUtils . cleanup ( null ,    fileSystem ,    outputStream )  ;", "}", "}", "METHOD_END"], "methodName": ["touchFile"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader ( TestGlobbedCopyListing . cluster . getFileSystem (  )  ,    listingPath ,    new   Configuration (  )  )  ;", "Text   key    =    new   Text (  )  ;", "CopyListingFileStatus   value    =    new   CopyListingFileStatus (  )  ;", "Map < String ,    String >    actualValues    =    new   HashMap < String ,    String >  (  )  ;", "while    ( reader . next ( key ,    value )  )     {", "if    (  ( value . isDirectory (  )  )     &  &     ( key . toString (  )  . equals (  \"  \"  )  )  )     {", "continue ;", "}", "actualValues . put ( value . getPath (  )  . toString (  )  ,    key . toString (  )  )  ;", "}", "Assert . assertEquals ( TestGlobbedCopyListing . expectedValues . size (  )  ,    actualValues . size (  )  )  ;", "for    ( Map . Entry < String ,    String >    entry    :    actualValues . entrySet (  )  )     {", "Assert . assertEquals ( entry . getValue (  )  ,    TestGlobbedCopyListing . expectedValues . get ( entry . getKey (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyContents"], "fileName": "org.apache.hadoop.tools.TestGlobbedCopyListing"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    ( String   segment    :    dirsAndFile )     {", "if    (  ( sb . length (  )  )     >     0  )     {", "sbpend ( SEPARATOR )  ;", "}", "sbpend ( segment )  ;", "}", "final   Path   f    =    new   Path ( root ,    sb . toString (  )  )  ;", "final   FSDataOutputStream   out    =    fs . create ( f )  ;", "try    {", "out . write ( fileContent )  ;", "}    finally    {", "out . close (  )  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "String   fileBaseName    =    dirsAndFile [  (  ( dirsAndFile . length )     -     1  )  ]  ;", "return    . createFile ( root ,    fs ,    fileBaseName . getBytes (  \" UTF -  8  \"  )  ,    dirsAndFile )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "try    {", "fiek ( ekPo ;", "aertTrue (  (  (  ( meage    +     \"     ( Potion    =     \"  )     +     ( figetPo )  )  )     +     \"  )  \"  )  ,    fal )  ;", "}    catch    ( IOException   ioe )     {", "}", "}", "METHOD_END"], "methodName": ["expectSeekIOE"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  (  \" lsr   root =  \"     +    dir )  )  ;", "final   ByteArrayOutputStream   bytes    =    new   ByteArrayOutputStream (  )  ;", "final   PrintStream   out    =    new   PrintStream ( bytes )  ;", "final   PrintStream   oldOut    =    System . out ;", "final   PrintStream   oldErr    =    System . err ;", "System . setOut ( out )  ;", "System . setErr ( out )  ;", "final   String   results ;", "try    {", "Assert . assertEquals (  0  ,    shell . run ( new   String [  ]  {     \"  - lsr \"  ,    dir    }  )  )  ;", "results    =    bytesString (  )  ;", "}    finally    {", "IOUtils . closeStream ( out )  ;", "System . setOut ( oldOut )  ;", "System . setErr ( oldErr )  ;", "}", "System . out . println (  (  \" lsr   results :  \\ n \"     +    results )  )  ;", "String   dirname    =    dir ;", "if    (  ( dir . lastIndexOf ( SEPARATOR )  )     !  =     (  -  1  )  )     {", "dirname    =    dir . substring ( dir . lastIndexOf ( SEPARATOR )  )  ;", "}", "final   List < String >    paths    =    new   ArrayList < String >  (  )  ;", "for    ( StringTokenizer   t    =    new   StringTokenizer ( results ,     \"  \\ n \"  )  ;    t . hasMoreTokens (  )  ;  )     {", "final   String   s    =    t . nextToken (  )  ;", "final   int   i    =    s . indexOf ( dirname )  ;", "if    ( i    >  =     0  )     {", "paths . add ( s . substring (  ( i    +     ( dirname . length (  )  )  )  )  )  ;", "}", "}", "Collections . sort ( paths )  ;", "System . out . println (  (  \" lsr   paths    =     \"     +     ( pathsString (  )  . replace (  \"  ,     \"  ,     \"  ,  \\ n       \"  )  )  )  )  ;", "return   paths ;", "}", "METHOD_END"], "methodName": ["lsr"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   String   inputPathStr    =    inputPath . toUri (  )  . getPath (  )  ;", "System . out . println (  (  \" inputPathStr    =     \"     +    inputPathStr )  )  ;", "final   URI   uri    =    fs . getUri (  )  ;", "final   String   prefix    =     (  (  (  (  \" har :  /  / hdfs -  \"     +     ( uri . getHost (  )  )  )     +     \"  :  \"  )     +     ( uri . getPort (  )  )  )     +     ( archivePath . toUri (  )  . getPath (  )  )  )     +     ( Path . SEPARATOR )  ;", "final   String   harName    =     \" foo . har \"  ;", "final   String   fullHarPathStr    =    prefix    +    harName ;", "final   String [  ]    args    =    new   String [  ]  {     \"  - archiveName \"  ,    harName ,     \"  - p \"  ,    inputPathStr ,     \"  *  \"  ,    archivePath . toString (  )     }  ;", "System . setProperty ( HadoopArchives . TEST _ HADOOP _ ARCHIVES _ JAR _ PATH ,     . HADOOP _ ARCHIVES _ JAR )  ;", "final   HadoopArchives   har    =    new   HadoopArchives ( conf )  ;", "assertEquals (  0  ,    ToolRunner . run ( har ,    args )  )  ;", "return   fullHarPathStr ;", "}", "METHOD_END"], "methodName": ["makeArchive"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bb    =    new   byte [  7  7  7  7  7  ]  ;", "f    ( int   i    =     0  ;    i    <     ( bb . length )  ;    i +  +  )     {", "double   d    =    Math . log (  ( i    +     2  )  )  ;", "long   bits    =    Double . doubleToLongBits ( d )  ;", "bb [ i ]     =     (  ( byte )     ( bits )  )  ;", "}", "return   bb ;", "}", "METHOD_END"], "methodName": ["prepareBin"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "try    {", "int   b ;", "while    ( true )     {", "b    =    fsdis . read (  )  ;", "if    ( b    <     0  )     {", "break ;", "} else    {", "baos . write ( b )  ;", "}", "}", "baos . close (  )  ;", "return   baosByteArray (  )  ;", "}    finally    {", "if    ( close )     {", "fsdis . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readAllSimple"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   int   available    =    fsdis . available (  )  ;", "final   byte [  ]    buffer ;", "final   ByteArrayOutputStream   baos ;", "if    ( available    <     0  )     {", "buffer    =    new   byte [  1  0  2  4  ]  ;", "baos    =    new   ByteArrayOutputStream (  (  ( buffer . length )     *     2  )  )  ;", "} else    {", "buffer    =    new   byte [ available ]  ;", "baos    =    new   ByteArrayOutputStream ( available )  ;", "}", "int   readIBuffer    =     0  ;", "int   read ;", "while    ( true )     {", "read    =    fsdis . read ( buffer ,    readIBuffer ,     (  ( buffer . length )     -    readIBuffer )  )  ;", "if    ( read    <     0  )     {", "if    ( readIBuffer    >     0  )     {", "baos . write ( buffer ,     0  ,    readIBuffer )  ;", "}", "return   baosByteArray (  )  ;", "} else    {", "readIBuffer    +  =    read ;", "if    ( readIBuffer    =  =     ( buffer . length )  )     {", "baos . write ( buffer )  ;", "readIBuffer    =     0  ;", "} else", "if    ( readIBuffer    >     ( buffer . length )  )     {", "throw   new   IOException (  (  (  (  \" Read   more   than   the   buffer   length :     \"     +    readIBuffer )     +     \"  ,    buffer   length    =     \"  )     +     ( buffer . length )  )  )  ;", "}", "}", "}", "}    finally    {", "if    ( close )     {", "fsdis . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readAllWithBuffer"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "final   byte [  ]    buffer    =    new   byte [  1  7  ]  ;", "inttalRead    =     0  ;", "int   read ;", "while    ( true )     {", "read    =    fsdis . readtalRead ,    buffer ,     0  ,    buffer . length )  ;", "if    ( read    >     0  )     {", "talRead    +  =    read ;", "baos . write ( buffer ,     0  ,    read )  ;", "} else", "if    ( read    <     0  )     {", "break ;", "} else    {", "throw   new   AssertionError (  (  (  (  \" FSDataInputStream # read (  4  )    returned    0  ,    while    \"     +     \"    the    4 th   method   parameter   is    \"  )     +     ( buffer . length )  )     +     \"  .  \"  )  )  ;", "}", "}", "final   byte [  ]    result    =    baosByteArray (  )  ;", "return   result ;", "}    finally    {", "if    ( close )     {", "fsdis . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readAllWithRead4"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "final   byte [  ]    buffer    =    new   byte [  1  7  ]  ;", "final   int   times    = talLength    /     ( buffer . length )  ;", "final   int   remainder    = talLength    %     ( buffer . length )  ;", "int   position    =     0  ;", "try    {", "for    ( int   i    =     0  ;    i    <    times ;    i +  +  )     {", "fsdis . readFully ( position ,    buffer )  ;", "position    +  =    buffer . length ;", "baos . write ( buffer )  ;", "}", "if    ( remainder    >     0  )     {", "fsdis . readFully ( position ,    buffer ,     0  ,    remainder )  ;", "position    +  =    remainder ;", "baos . write ( buffer ,     0  ,    remainder )  ;", "}", "try    {", "fsdis . readFully ( position ,    buffer ,     0  ,     1  )  ;", "assertTrue ( false )  ;", "}    catch    ( IOException   ioe )     {", "}", "assertEqualstalLength ,    position )  ;", "final   byte [  ]    result    =    baosByteArray (  )  ;", "assertEqualstalLength ,    result . length )  ;", "return   result ;", "}    finally    {", "if    ( close )     {", "fsdis . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readAllWithReadFully"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   byte [  ]    result    =    new   byte [ totalLength ]  ;", "long   pos ;", "try    {", "final   byte [  ]    buffer    =    new   byte [  1  7  ]  ;", "final   int   times    =    totalLength    /     ( buffer . length )  ;", "int   read ;", "int   expectedRead ;", "for    ( int   i    =    times ;    i    >  =     0  ;    i -  -  )     {", "pos    =    i    *     ( buffer . length )  ;", "fsdis . seek ( pos )  ;", "assertEquals ( pos ,    fsdis . getPos (  )  )  ;", "read    =    fsdis . read ( buffer )  ;", "if    ( i    =  =    times )     {", "expectedRead    =    totalLength    %     ( buffer . length )  ;", "if    ( expectedRead    =  =     0  )     {", "expectedRead    =     -  1  ;", "}", "} else    {", "expectedRead    =    buffer . length ;", "}", "assertEquals ( expectedRead ,    read )  ;", "if    ( read    >     0  )     {", "System . arraycopy ( buffer ,     0  ,    result ,     (  ( int )     ( pos )  )  ,    read )  ;", "}", "}", ". expectSeekIOE ( fsdis ,    Long . MAX _ VALUE ,     \" Seek   to   Long . MAX _ VALUE   should   lead   to   IOE .  \"  )  ;", ". expectSeekIOE ( fsdis ,    Long . MIN _ VALUE ,     \" Seek   to   Long . MIN _ VALUE   should   lead   to   IOE .  \"  )  ;", "long   pp    =     -  1 L ;", ". expectSeekIOE ( fsdis ,    pp ,     (  (  \" Seek   to    \"     +    pp )     +     \"    should   lead   to   IOE .  \"  )  )  ;", "fsdis . seek ( totalLength )  ;", "assertEquals ( totalLength ,    fsdis . getPos (  )  )  ;", "pp    =    totalLength    +     1  ;", ". expectSeekIOE ( fsdis ,    pp ,     (  (  \" Seek   to   the   length   position    +     1     (  \"     +    pp )     +     \"  )    should   lead   to   IOE .  \"  )  )  ;", "return   result ;", "}    finally    {", "if    ( close )     {", "fsdis . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readAllWithSeek"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  0  ,    fsdis 1  . skip (  (  -  1  )  )  )  ;", "assertEquals (  0  ,    fsdis 1  . skip (  0  )  )  ;", "final   ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream ( totalLength )  ;", "try    {", "final   byte [  ]    buffer    =    new   byte [  1  7  ]  ;", "final   int   times    =    totalLength    /     ( buffer . length )  ;", "final   int   remainder    =    totalLength    %     ( buffer . length )  ;", "long   skipped ;", "long   expectedPosition ;", "int   toGo ;", "for    ( int   i    =     0  ;    i    <  =    times ;    i +  +  )     {", "toGo    =     ( i    <    times )     ?    buffer . length    :    remainder ;", "if    (  ( i    %     2  )     =  =     0  )     {", "fsdis 1  . readFully ( buffer ,     0  ,    toGo )  ;", "skipped    =     . skipUntilZero ( fsdis 2  ,    toGo )  ;", "} else    {", "fsdis 2  . readFully ( buffer ,     0  ,    toGo )  ;", "skipped    =     . skipUntilZero ( fsdis 1  ,    toGo )  ;", "}", "if    ( i    <    times )     {", "assertEquals ( buffer . length ,    skipped )  ;", "expectedPosition    =     ( i    +     1  )     *     ( buffer . length )  ;", "} else    {", "if    ( remainder    >     0  )     {", "assertEquals ( remainder ,    skipped )  ;", "} else    {", "assertEquals (  0  ,    skipped )  ;", "}", "expectedPosition    =    totalLength ;", "}", "assertEquals ( expectedPosition ,    fsdis 1  . getPos (  )  )  ;", "assertEquals ( expectedPosition ,    fsdis 2  . getPos (  )  )  ;", "if    ( toGo    >     0  )     {", "baos . write ( buffer ,     0  ,    toGo )  ;", "}", "}", "assertEquals (  0  ,    fsdis 1  . skip (  (  -  1  )  )  )  ;", "assertEquals (  0  ,    fsdis 1  . skip (  0  )  )  ;", "assertEquals (  0  ,    fsdis 1  . skip (  1  )  )  ;", "assertEquals (  0  ,    fsdis 1  . skip ( Long . MAX _ VALUE )  )  ;", "return   baos . toByteArray (  )  ;", "}    finally    {", "if    ( close )     {", "fsdis 1  . close (  )  ;", "fsdis 2  . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readAllWithSkip"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   Configuration (  )  ;", "conf . set (  (  (  (  ( CapacitySchedulerConfiguration . PREFIX )     +     ( CapacitySchedulerConfiguration . ROOT )  )     +     \"  .  \"  )     +     ( CapacitySchedulerConfiguration . QUEUES )  )  ,     \" default \"  )  ;", "conf . set (  (  (  (  ( CapacitySchedulerConfiguration . PREFIX )     +     ( CapacitySchedulerConfiguration . ROOT )  )     +     \"  . default .  \"  )     +     ( CapacitySchedulerConfiguration . CAPACITY )  )  ,     \"  1  0  0  \"  )  ;", "dfscluster    =    new   hdfs . MiniDFSCluster . Builder ( conf )  . checkExitOnShutdown ( true )  . numDataNodes (  2  )  . format ( true )  . racks ( null )  . build (  )  ;", "fs    =    dfscluster . getFileSystem (  )  ;", "archivePath    =    new   fs . Path ( fs . getHomeDirectory (  )  ,     \" archive \"  )  ;", "fs . delete ( archivePath ,    true )  ;", "inputPath    =    new   fs . Path ( fs . getHomeDirectory (  )  ,    TestHadoopArchives . inputDir )  ;", "fs . delete ( inputPath ,    true )  ;", "fs . mkdirs ( inputPath )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,     \" a \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,     \" b \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,     \" c \"  )  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "long   skipped    =     0  ;", "long   remainsToSkip    =    Skip ;", "long   s ;", "while    ( skipped    <    Skip )     {", "s    =    fis . skip ( remainsToSkip )  ;", "if    ( s    =  =     0  )     {", "return   skipped ;", "}", "skipped    +  =    s ;", "remainsToSkip    -  =    s ;", "}", "return   skipped ;", "}", "METHOD_END"], "methodName": ["skipUntilZero"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "if    (  ( dfscluster )     !  =    null )     {", "dfscluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   String   fullHarPathStr    =    makeArchive (  )  ;", "final   String   tmpDir    =     ( System . getProperty (  \" test . build . data \"  ,     \" build / test / data \"  )  )     +     \"  / work - dir / har - fs - tmp \"  ;", "final   Path   tmpPath    =    new   Path ( tmpDir )  ;", "final   LocalFileSystem   localFs    =    FileSystem . getLocal ( new   Configuration (  )  )  ;", "localFs . delete ( tmpPath ,    true )  ;", "localFs . mkdirs ( tmpPath )  ;", "assertTrue ( localFs . exists ( tmpPath )  )  ;", "final   HarFileSystem   harFileSystem    =    new   HarFileSystem ( fs )  ;", "try    {", "final   URI   harUri    =    new   URI ( fullHarPathStr )  ;", "harFileSystem . initialize ( harUri ,    fs . getConf (  )  )  ;", "final   Path   sourcePath    =    new   Path (  (  ( fullHarPathStr    +     ( Path . SEPARATOR )  )     +     \" a \"  )  )  ;", "final   Path   targetPath    =    new   Path ( tmpPath ,     \" straus \"  )  ;", "harFileSystem . copyToLocalFile ( false ,    sourcePath ,    targetPath )  ;", "FileStatus   straus    =    localFs . getFileStatus ( targetPath )  ;", "assertEquals (  1  ,    straus . getLen (  )  )  ;", "}    finally    {", "harFileSystem . close (  )  ;", "localFs . delete ( tmpPath ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopyToLocal"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "TestHadoopArchives . createFile ( inputPath ,    fs ,     \" c   c \"  )  ;", "final   Path   sub 1     =    new   Path ( inputPath ,     \" sub    1  \"  )  ;", "fs . mkdirs ( sub 1  )  ;", "TestHadoopArchives . createFile ( sub 1  ,    fs ,     \" file   x   y   z \"  )  ;", "TestHadoopArchives . createFile ( sub 1  ,    fs ,     \" file \"  )  ;", "TestHadoopArchives . createFile ( sub 1  ,    fs ,     \" x \"  )  ;", "TestHadoopArchives . createFile ( sub 1  ,    fs ,     \" y \"  )  ;", "TestHadoopArchives . createFile ( sub 1  ,    fs ,     \" z \"  )  ;", "final   Path   sub 2     =    new   Path ( inputPath ,     \" sub    1    with   suffix \"  )  ;", "fs . mkdirs ( sub 2  )  ;", "TestHadoopArchives . createFile ( sub 2  ,    fs ,     \" z \"  )  ;", "final   FsShell   shell    =    new   FsShell ( conf )  ;", "final   String   inputPathStr    =    inputPath . toUri (  )  . getPath (  )  ;", "final   List < String >    originalPaths    =    TestHadoopArchives . lsr ( shell ,    inputPathStr )  ;", "final   String   fullHarPathStr    =    makeArchive (  )  ;", "final   List < String >    harPaths    =    TestHadoopArchives . lsr ( shell ,    fullHarPathStr )  ;", "Assert . assertEquals ( originalPaths ,    harPaths )  ;", "}", "METHOD_END"], "methodName": ["testPathWithSpaces"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,     \" c   c \"  )  )  ;", "final   Path   sub 1     =    new   Path ( inputPath ,     \" sub    1  \"  )  ;", "fs . mkdirs ( sub 1  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    sub 1  . getName (  )  ,     \" file   x   y   z \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    sub 1  . getName (  )  ,     \" file \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    sub 1  . getName (  )  ,     \" x \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    sub 1  . getName (  )  ,     \" y \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    sub 1  . getName (  )  ,     \" z \"  )  )  ;", "final   Path   sub 2     =    new   Path ( inputPath ,     \" sub    1    with   suffix \"  )  ;", "fs . mkdirs ( sub 2  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    sub 2  . getName (  )  ,     \" z \"  )  )  ;", "final   byte [  ]    binContent    =    TestHadoopArchives . prepareBin (  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    binContent ,    sub 2  . getName (  )  ,     \" bin \"  )  )  ;", "fileList . add ( TestHadoopArchives . createFile ( inputPath ,    fs ,    new   byte [  0  ]  ,    sub 2  . getName (  )  ,     \" zero - length \"  )  )  ;", "final   String   fullHarPathStr    =    makeArchive (  )  ;", "final   HarFileSystem   harFileSystem    =    new   HarFileSystem ( fs )  ;", "try    {", "final   URI   harUri    =    new   URI ( fullHarPathStr )  ;", "harFileSystem . initialize ( harUri ,    fs . getConf (  )  )  ;", "int   readFileCount    =     0  ;", "for    ( final   String   pathStr 0     :    fileList )     {", "final   Path   path    =    new   Path (  (  ( fullHarPathStr    +     ( Path . SEPARATOR )  )     +    pathStr 0  )  )  ;", "final   String   baseName    =    path . getName (  )  ;", "final   FileStatus   status    =    harFileSystem . getFileStatus ( path )  ;", "if    ( status . isFile (  )  )     {", "final   byte [  ]    actualContentSimple    =    TestHadoopArchives . readAllSimple ( harFileSystem . open ( path )  ,    true )  ;", "final   byte [  ]    actualContentBuffer    =    TestHadoopArchives . readAllWithBuffer ( harFileSystem . open ( path )  ,    true )  ;", "assertArrayEquals ( actualContentSimple ,    actualContentBuffer )  ;", "final   byte [  ]    actualContentFully    =    TestHadoopArchives . readAllWithReadFully ( actualContentSimple . length ,    harFileSystem . open ( path )  ,    true )  ;", "assertArrayEquals ( actualContentSimple ,    actualContentFully )  ;", "final   byte [  ]    actualContentSeek    =    TestHadoopArchives . readAllWithSeek ( actualContentSimple . length ,    harFileSystem . open ( path )  ,    true )  ;", "assertArrayEquals ( actualContentSimple ,    actualContentSeek )  ;", "final   byte [  ]    actualContentRead 4     =    TestHadoopArchives . readAllWithRead 4  ( harFileSystem . open ( path )  ,    true )  ;", "assertArrayEquals ( actualContentSimple ,    actualContentRead 4  )  ;", "final   byte [  ]    actualContentSkip    =    TestHadoopArchives . readAllWithSkip ( actualContentSimple . length ,    harFileSystem . open ( path )  ,    harFileSystem . open ( path )  ,    true )  ;", "assertArrayEquals ( actualContentSimple ,    actualContentSkip )  ;", "if    (  \" bin \"  . equals ( baseName )  )     {", "assertArrayEquals ( binContent ,    actualContentSimple )  ;", "} else", "if    (  \" zero - length \"  . equals ( baseName )  )     {", "assertEquals (  0  ,    actualContentSimple . length )  ;", "} else    {", "String   actual    =    new   String ( actualContentSimple ,     \" UTF -  8  \"  )  ;", "assertEquals ( baseName ,    actual )  ;", "}", "readFileCount +  +  ;", "}", "}", "assertEquals ( fileList . size (  )  ,    readFileCount )  ;", "}    finally    {", "harFileSystem . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testReadFileContent"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "final   Path   sub 1     =    new   Path ( inputPath ,     \" dir 1  \"  )  ;", "fs . mkdirs ( sub 1  )  ;", ". createFile ( inputPath ,    fs ,    sub 1  . getName (  )  ,     \" a \"  )  ;", "final   FsShell   shell    =    new   FsShell ( conf )  ;", "final   List < String >    originalPaths    =     . lsr ( shell ,     \" input \"  )  ;", "System . out . println (  (  \" originalPaths :     \"     +    originalPaths )  )  ;", "final   String   fullHarPathStr    =    makeArchive (  )  ;", "final   List < String >    harPaths    =     . lsr ( shell ,    fullHarPathStr )  ;", "Assert . assertEquals ( originalPaths ,    harPaths )  ;", "}", "METHOD_END"], "methodName": ["testRelativePath"], "fileName": "org.apache.hadoop.tools.TestHadoopArchives"}, {"methodBody": ["METHOD_START", "{", "OutputStream   out    =    TestIntegration . fs . create ( listFile )  ;", "try    {", "for    ( String   entry    :    entries )     {", "out . write (  (  (  ( TestIntegration . root )     +     \"  /  \"  )     +    entry )  . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}", "}    finally    {", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addEntries"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     3  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiFileTargetMissing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    sync )  ;", "checkResult (  . target ,     3  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseMultiFileTargetPresent"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singledir \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     1  ,     \" dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singlefile 1  / file 1  \"  )  ;", "createFiles (  \" singlefile 1  / file 1  \"  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileMissingTarget"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singlefile 2  / file 2  \"  )  ;", "createFiles (  \" singlefile 2  / file 2  \"  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    sync )  ;", "checkResult (  . target ,     1  ,     \" file 2  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileTargetDir"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singlefile 1  / file 1  \"  )  ;", "createFiles (  \" singlefile 1  / file 1  \"  ,     \" target \"  )  ;", "runTest (  . listFile ,     . target ,    false ,    sync )  ;", "checkResult (  . target ,     1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["caseSingleFileTargetFile"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( count ,    TestIntegration . fs . listStatus ( target )  . length )  ;", "if    (  ( relPaths    =  =    null )     |  |     (  ( relPaths . length )     =  =     0  )  )     {", "Assert . assertTrue ( target . toString (  )  ,    TestIntegration . fs . exists ( target )  )  ;", "return ;", "}", "for    ( String   relPath    :    relPaths )     {", "Assert . assertTrue ( new   Path ( target ,    relPath )  . toString (  )  ,    TestIntegration . fs . exists ( new   Path ( target ,    relPath )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkResult"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "for    ( String   entry    :    entries )     {", "OutputStream   out    =     . fs . create ( new   Path (  (  (  (  . root )     +     \"  /  \"  )     +    entry )  )  )  ;", "try    {", "out . write (  (  (  (  . root )     +     \"  /  \"  )     +    entry )  . getBytes (  )  )  ;", "out . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}    finally    {", "out . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "OutputStream   out    =    TestIntegration . fs . create ( new   Path (  (  (  ( TestIntegration . root )     +     \"  /  \"  )     +    entry )  )  )  ;", "try    {", "out . write ( contents )  ;", "}    finally    {", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createWithContents"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  \" fs . default . name \"  ,     \" file :  /  /  /  \"  )  ;", "conf . set (  \" mapred . job . tracker \"  ,     \" local \"  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "for    ( String   entry    :    entries )     {", ". fs . mkdirs ( new   Path ( entry )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "runTest ( listFile ,    target ,    targetExists ,    sync ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["runTest"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    new   DistCpOptions ( listFile ,    target )  ;", "options . setSyncFolder ( sync )  ;", "options . setDeleteMissing ( delete )  ;", "options . setOverwrite ( overwrite )  ;", "options . setTargetPathExists ( targetExists )  ;", "try    {", "new   DistCp (  . getConf (  )  ,    options )  . execute (  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["runTest"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", ". fs    =    FileSystem . get (  . getConf (  )  )  ;", ". listFile    =    new   Path (  \" target / tmp / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", ". target    =    new   Path (  \" target / tmp / target \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", ". root    =    new   Path (  \" target / tmp \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  . toString (  )  ;", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   sourcePath    =    new   Path (  \" noscheme :  /  /  / file \"  )  ;", "List < Path >    sources    =    new   ArrayList < Path >  (  )  ;", "sources . add ( sourcePath )  ;", "DistCpOptions   options    =    new   DistCpOptions ( sources ,    TestIntegration . target )  ;", "Configuration   conf    =    TestIntegration . getConf (  )  ;", "Path   stagingDir    =    JobSubmissionFiles . getStagingDir ( new   mapreduce . Cluster ( conf )  ,    conf )  ;", "stagingDir . getFileSystem ( conf )  . mkdirs ( stagingDir )  ;", "try    {", "new   DistCp ( conf ,    options )  . execute (  )  ;", "}    catch    ( Throwable   t )     {", "Assert . assertEquals ( stagingDir . getFileSystem ( conf )  . listStatus ( stagingDir )  . length ,     0  )  ;", "}", "}    catch    ( Exception   e )     {", "TestIntegration . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" testCleanup   failed    \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCleanup"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile 1  / file 3  \"  ,     \" multifile 1  / file 4  \"  ,     \" multifile 1  / file 5  \"  )  ;", "createFiles (  \" multifile 1  / file 3  \"  ,     \" multifile 1  / file 4  \"  ,     \" multifile 1  / file 5  \"  )  ;", "mkdirs (  . target . toString (  )  )  ;", "Configuration   conf    =     . getConf (  )  ;", "try    {", "conf . setClass ( DistCpConstants . CONF _ LABEL _ COPY _ LISTING _ CLASS ,     . CustomCopyListing . class ,    CopyListing . class )  ;", "DistCpOptions   options    =    new   DistCpOptions ( Arrays . asList ( new   Path (  (  (  (  . root )     +     \"  /  \"  )     +     \" multifile 1  \"  )  )  )  ,     . target )  ;", "options . setSyncFolder ( true )  ;", "options . setDeleteMissing ( false )  ;", "options . setOverwrite ( false )  ;", "try    {", "new   DistCp ( conf ,    options )  . execute (  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new   IOException ( e )  ;", "}", "}    finally    {", "conf . unset ( DistCpConstants . CONF _ LABEL _ COPY _ LISTING _ CLASS )  ;", "}", "checkResult (  . target ,     2  ,     \" file 4  \"  ,     \" file 5  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testCustomCopyListing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" srcdir \"  )  ;", "createFiles (  \" srcdir / file 1  \"  ,     \" dstdir / file 1  \"  ,     \" dstdir / file 2  \"  )  ;", "Path   target    =    new   Path (  (  (  . root )     +     \"  / dstdir \"  )  )  ;", "runTest (  . listFile ,    target ,    false ,    true ,    true ,    false )  ;", "checkResult ( target ,     1  ,     \" file 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDeleteMissingInDestination"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  /  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir 1  / dir 3  / file 7  \"  ,     \" singledir 1  / dir 3  / file 8  \"  ,     \" singledir 1  / dir 3  / file 9  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    false )  ;", "checkResult (  . target ,     4  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" dir 3  / file 7  \"  ,     \" dir 3  / file 8  \"  ,     \" dir 3  / file 9  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGlobTargetMissingMultiLevel"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir / dir 2  / file 6  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    false )  ;", "checkResult (  . target ,     2  ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  ,     \" singledir / dir 2  / file 6  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGlobTargetMissingSingleLevel"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile \"  ,     \" singledir \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    false )  ;", "checkResult (  . target ,     2  ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  ,     \" singledir / dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultiDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile \"  ,     \" singledir \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  . target . toString (  )  ,     (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    false )  ;", "checkResult (  . target ,     2  ,     \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  ,     \" singledir / dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultiDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "caseMultiFileTargetMissing ( false )  ;", "caseMultiFileTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiFileTargetMissing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "caseMultiFileTargetPresent ( false )  ;", "caseMultiFileTargetPresent ( true )  ;", "}", "METHOD_END"], "methodName": ["testMultiFileTargetPresent"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    contents 1     =     \" contents 1  \"  . getBytes (  )  ;", "byte [  ]    contents 2     =     \" contents 2  \"  . getBytes (  )  ;", "Assert . assertEquals ( contents 1  . length ,    contents 2  . length )  ;", "try    {", "addEntries (  . listFile ,     \" srcdir \"  )  ;", "createWithContents (  \" srcdir / file 1  \"  ,    contents 1  )  ;", "createWithContents (  \" dstdir / file 1  \"  ,    contents 2  )  ;", "Path   target    =    new   Path (  (  (  . root )     +     \"  / dstdir \"  )  )  ;", "runTest (  . listFile ,    target ,    false ,    false ,    false ,    true )  ;", "checkResult ( target ,     1  ,     \" file 1  \"  )  ;", "FSDataInputStream   is    =     . fs . open ( new   Path (  (  (  . root )     +     \"  / dstdir / file 1  \"  )  )  )  ;", "byte [  ]    dstContents    =    new   byte [ contents 1  . length ]  ;", "is . readFully ( dstContents )  ;", "is . close (  )  ;", "Assert . assertArrayEquals ( contents 1  ,    dstContents )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testOverwrite"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "caseSingleDirTargetMissing ( false )  ;", "caseSingleDirTargetMissing ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" singledir \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    false )  ;", "checkResult (  . target ,     1  ,     \" singledir / dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileMissingTarget ( false )  ;", "caseSingleFileMissingTarget ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileMissingTarget"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileTargetDir ( false )  ;", "caseSingleFileTargetDir ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileTargetDir"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "caseSingleFileTargetFile ( false )  ;", "caseSingleFileTargetFile ( true )  ;", "}", "METHOD_END"], "methodName": ["testSingleFileTargetFile"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  /  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir 1  / dir 3  / file 7  \"  ,     \" singledir 1  / dir 3  / file 8  \"  ,     \" singledir 1  / dir 3  / file 9  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    true )  ;", "checkResult (  . target ,     6  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" file 7  \"  ,     \" file 8  \"  ,     \" file 9  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateGlobTargetMissingMultiLevel"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "Path   listFile    =    new   Path (  \" target / tmp 1  / listing \"  )  . makeQualified (  . fs . getUri (  )  ,     . fs . getWorkingDirectory (  )  )  ;", "addEntries ( listFile ,     \"  *  \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "createFiles (  \" singledir / dir 2  / file 6  \"  )  ;", "runTest ( listFile ,     . target ,    false ,    true )  ;", "checkResult (  . target ,     4  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" dir 2  / file 6  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "TestDistCpUtils . delete (  . fs ,     \" target / tmp 1  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateGlobTargetMissingSingleLevel"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" multifile \"  ,     \" singledir \"  )  ;", "createFiles (  \" multifile / file 3  \"  ,     \" multifile / file 4  \"  ,     \" multifile / file 5  \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / singledir / dir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    false ,    true )  ;", "checkResult (  . target ,     4  ,     \" file 3  \"  ,     \" file 4  \"  ,     \" file 5  \"  ,     \" dir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateMultiDirTargetMissing"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" Umultifile \"  ,     \" Usingledir \"  )  ;", "createFiles (  \" Umultifile / Ufile 3  \"  ,     \" Umultifile / Ufile 4  \"  ,     \" Umultifile / Ufile 5  \"  )  ;", "mkdirs (  . target . toString (  )  ,     (  (  . root )     +     \"  / Usingledir / Udir 1  \"  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    true )  ;", "checkResult (  . target ,     4  ,     \" Ufile 3  \"  ,     \" Ufile 4  \"  ,     \" Ufile 5  \"  ,     \" Udir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateMultiDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "try    {", "addEntries (  . listFile ,     \" Usingledir \"  )  ;", "mkdirs (  (  (  . root )     +     \"  / Usingledir / Udir 1  \"  )  )  ;", "mkdirs (  . target . toString (  )  )  ;", "runTest (  . listFile ,     . target ,    true ,    true )  ;", "checkResult (  . target ,     1  ,     \" Udir 1  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   distcp \"  ,    e )  ;", "Assert . fail (  \" distcp   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete (  . fs ,     . root )  ;", "}", "}", "METHOD_END"], "methodName": ["testUpdateSingleDirTargetPresent"], "fileName": "org.apache.hadoop.tools.TestIntegration"}, {"methodBody": ["METHOD_START", "{", "File   result    =    new   File (  (  (  ( TestLogalyzer . outdir . getAbsolutePath (  )  )     +     ( File . separator )  )     +     \" part -  0  0  0  0  0  \"  )  )  ;", "File   success    =    new   File (  (  (  ( TestLogalyzer . outdir . getAbsolutePath (  )  )     +     ( File . separator )  )     +     \"  _ SUCCESS \"  )  )  ;", "Assert . assertTrue ( success . exists (  )  )  ;", "FileInputStream   fis    =    new   FileInputStream ( result )  ;", "BufferedReader   br    =    new   BufferedReader ( new   InputStreamReader ( fis ,     \" UTF -  8  \"  )  )  ;", "String   line    =    br . readLine (  )  ;", "Assert . assertTrue (  (  (  \"  1     4  4  \"     +     ( TestLogalyzer . TAB )  )     +     \"  2  \"  )  . equals ( line )  )  ;", "line    =    br . readLine (  )  ;", "Assert . assertTrue (  (  (  \"  3     4  4  \"     +     ( TestLogalyzer . TAB )  )     +     \"  1  \"  )  . equals ( line )  )  ;", "line    =    br . readLine (  )  ;", "Assert . assertTrue (  (  (  \"  4     4  4  \"     +     ( TestLogalyzer . TAB )  )     +     \"  1  \"  )  . equals ( line )  )  ;", "br . close (  )  ;", "}", "METHOD_END"], "methodName": ["checkResult"], "fileName": "org.apache.hadoop.tools.TestLogalyzer"}, {"methodBody": ["METHOD_START", "{", "FileContext   files    =    FileContext . getLocalFSFileContext (  )  ;", "Path   ws    =    new   Path (  . workSpace . getAbsoluteFile (  )  . getAbsolutePath (  )  )  ;", "files . delete ( ws ,    true )  ;", "Path   workSpacePath    =    new   Path (  . workSpace . getAbsolutePath (  )  ,     \" log \"  )  ;", "files . mkdir ( workSpacePath ,    null ,    true )  ;", ". LOG . info (  \" create   logfile . log \"  )  ;", "Path   logfile 1     =    new   Path ( workSpacePath ,     \" logfile . log \"  )  ;", "FSDataOutputStream   os    =    files . create ( logfile 1  ,    EnumSet . of ( CREATE )  )  ;", "os . writeBytes (  (  (  (  (  (  \"  4     3  \"     +     (  . EL )  )     +     \"  1     3  \"  )     +     (  . EL )  )     +     \"  4     4  4  \"  )     +     (  . EL )  )  )  ;", "os . writeBytes (  (  (  (  (  (  \"  2     3  \"     +     (  . EL )  )     +     \"  1     3  \"  )     +     (  . EL )  )     +     \"  0     4  5  \"  )     +     (  . EL )  )  )  ;", "os . writeBytes (  (  (  (  (  (  \"  4     3  \"     +     (  . EL )  )     +     \"  1     3  \"  )     +     (  . EL )  )     +     \"  1     4  4  \"  )     +     (  . EL )  )  )  ;", "os . flush (  )  ;", "os . close (  )  ;", ". LOG . info (  \" create   logfile 1  . log \"  )  ;", "Path   logfile 2     =    new   Path ( workSpacePath ,     \" logfile 1  . log \"  )  ;", "os    =    files . create ( logfile 2  ,    EnumSet . of ( CREATE )  )  ;", "os . writeBytes (  (  (  (  (  (  \"  4     3  \"     +     (  . EL )  )     +     \"  1     3  \"  )     +     (  . EL )  )     +     \"  3     4  4  \"  )     +     (  . EL )  )  )  ;", "os . writeBytes (  (  (  (  (  (  \"  2     3  \"     +     (  . EL )  )     +     \"  1     3  \"  )     +     (  . EL )  )     +     \"  0     4  5  \"  )     +     (  . EL )  )  )  ;", "os . writeBytes (  (  (  (  (  (  \"  4     3  \"     +     (  . EL )  )     +     \"  1     3  \"  )     +     (  . EL )  )     +     \"  1     4  4  \"  )     +     (  . EL )  )  )  ;", "os . flush (  )  ;", "os . close (  )  ;", "return   workSpacePath ;", "}", "METHOD_END"], "methodName": ["createLogFile"], "fileName": "org.apache.hadoop.tools.TestLogalyzer"}, {"methodBody": ["METHOD_START", "{", "Path   f    =    createLogFile (  )  ;", "String [  ]    args    =    new   String [  1  0  ]  ;", "args [  0  ]     =     \"  - archiveDir \"  ;", "args [  1  ]     =    f . toString (  )  ;", "args [  2  ]     =     \"  - grep \"  ;", "args [  3  ]     =     \"  4  4  \"  ;", "args [  4  ]     =     \"  - sort \"  ;", "args [  5  ]     =     \"  0  \"  ;", "args [  6  ]     =     \"  - analysis \"  ;", "args [  7  ]     =     . outdir . getAbsolutePath (  )  ;", "args [  8  ]     =     \"  - separator \"  ;", "args [  9  ]     =     \"     \"  ;", "Logalyzer . main ( args )  ;", "checkResult (  )  ;", "}", "METHOD_END"], "methodName": ["testLogalyzer"], "fileName": "org.apache.hadoop.tools.TestLogalyzer"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Assert . assertFalse ( conf . getBoolean ( DistCpOptionSwitch . APPEND . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertFalse ( conf . getBoolean ( DistCpOptionSwitch . SYNC _ FOLDERS . getConfigLabel (  )  ,    false )  )  ;", "DistCpOptions   options    =     . parse ( new   String [  ]  {     \"  - update \"  ,     \"  - append \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "options . appendToConf ( conf )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpOptionSwitch . APPEND . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpOptionSwitch . SYNC _ FOLDERS . getConfigLabel (  )  ,    false )  )  ;", "try    {", "options    =     . parse ( new   String [  ]  {     \"  - append \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Append   should   fail   if   update   option   is   not   specified \"  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "GenericTestUtils . assertExceptionContains (  \" Append   is   valid   only   with   update   options \"  ,    e )  ;", "}", "try    {", "options    =     . parse ( new   String [  ]  {     \"  - append \"  ,     \"  - update \"  ,     \"  - skipcrccheck \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Append   should   fail   if   skipCrc   option   is   specified \"  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "GenericTestUtils . assertExceptionContains (  \" Append   is   disallowed   when   skipping   CRC \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppendOption"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \"  - strategy \"  ,     \" dynamic \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getCopyStrategy (  )  ,     \" dynamic \"  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getCopyStrategy (  )  ,    DistCpConstants . UNIFORMSIZE )  ;", "}", "METHOD_END"], "methodName": ["testCopyStrategy"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "try    {", ". parse ( new   String [  ]  {     \"  - m \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source \"     }  )  ;", "Assert . fail (  \" Missing   map   value \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testInvalidArgs"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertNull ( options . getLogPath (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - log \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / logs \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getLogPath (  )  ,    new   Path (  \" hdfs :  /  / localhost :  8  0  2  0  / logs \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testLogPath"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "try    {", ". parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Neither   source   listing   not   source   paths   present \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testMissingSourceInfo"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "try    {", ". parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source \"     }  )  ;", "Assert . fail (  \" Missing   target   allowed \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testMissingTarget"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Assert . assertFalse ( conf . getBoolean ( DistCpOptionSwitch . IGNORE _ FAILURES . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertFalse ( conf . getBoolean ( DistCpOptionSwitch . ATOMIC _ COMMIT . getConfigLabel (  )  ,    false )  )  ;", "DistCpOptions   options    =     . parse ( new   String [  ]  {     \"  - atomic \"  ,     \"  - i \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "options . appendToConf ( conf )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpOptionSwitch . IGNORE _ FAILURES . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpOptionSwitch . ATOMIC _ COMMIT . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertEquals ( conf . getInt ( DistCpOptionSwitch . BANDWIDTH . getConfigLabel (  )  ,     (  -  1  )  )  ,    DistCpConstants . DEFAULT _ BANDWIDTH _ MB )  ;", "conf    =    new   Configuration (  )  ;", "Assert . assertFalse ( conf . getBoolean ( DistCpOptionSwitch . SYNC _ FOLDERS . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertFalse ( conf . getBoolean ( DistCpOptionSwitch . DELETE _ MISSING . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertEquals ( conf . get ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  )  ,    null )  ;", "options    =     . parse ( new   String [  ]  {     \"  - update \"  ,     \"  - delete \"  ,     \"  - pu \"  ,     \"  - bandwidth \"  ,     \"  1  1  \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "options . appendToConf ( conf )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpOptionSwitch . SYNC _ FOLDERS . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpOptionSwitch . DELETE _ MISSING . getConfigLabel (  )  ,    false )  )  ;", "Assert . assertEquals ( conf . get ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  )  ,     \" U \"  )  ;", "Assert . assertEquals ( conf . getInt ( DistCpOptionSwitch . BANDWIDTH . getConfigLabel (  )  ,     (  -  1  )  )  ,     1  1  )  ;", "}", "METHOD_END"], "methodName": ["testOptionsAppendToConf"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Assert . assertNull ( conf . get ( DistCpSwitch . ATOMIC _ COMMIT . getConfigLabel (  )  )  )  ;", "DistCpSwitch . addToConf ( conf ,    DistCpSwitch . ATOMIC _ COMMIT )  ;", "Assert . assertTrue ( conf . getBoolean ( DistCpSwitch . ATOMIC _ COMMIT . getConfigLabel (  )  ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["testOptionsSwitchAddToConf"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldAtomicCommit (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - atomic \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldAtomicCommit (  )  )  ;", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - atomic \"  ,     \"  - update \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Atomic   and   sync   folders   were   allowed \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testParseAtomicCommit"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldBlock (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - async \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldBlock (  )  )  ;", "}", "METHOD_END"], "methodName": ["testParseBlokcing"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldDeleteMissing (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - update \"  ,     \"  - delete \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldSyncFolder (  )  )  ;", "Assert . assertTrue ( options . shouldDeleteMissing (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - overwrite \"  ,     \"  - delete \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldOverwrite (  )  )  ;", "Assert . assertTrue ( options . shouldDeleteMissing (  )  )  ;", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - atomic \"  ,     \"  - delete \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Atomic   and   delete   folders   were   allowed \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testParseDeleteMissing"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldIgnoreFailures (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - i \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldIgnoreFailures (  )  )  ;", "}", "METHOD_END"], "methodName": ["testParseIgnoreFailure"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getMaxMaps (  )  ,    DistCpConstants . DEFAULT _ MAPS )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - m \"  ,     \"  1  \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getMaxMaps (  )  ,     1  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - m \"  ,     \"  0  \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getMaxMaps (  )  ,     1  )  ;", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - m \"  ,     \" hello \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Non   numberic   map   parsed \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - mapredXslConf \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Non   numberic   map   parsed \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testParseMaps"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "OptionsParser . parse ( new   String [  ]  {     \"  - bandwidth \"  ,     \"  -  1  1  \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "}", "METHOD_END"], "methodName": ["testParseNonPositiveBandwidth"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldOverwrite (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - overwrite \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldOverwrite (  )  )  ;", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - update \"  ,     \"  - overwrite \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Update   and   overwrite   aren ' t   allowed   together \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testParseOverwrite"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertNull ( options . getSslConfigurationFile (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - mapredSslConf \"  ,     \"  / tmp / ssl - client . xml \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getSslConfigurationFile (  )  ,     \"  / tmp / ssl - client . xml \"  )  ;", "}", "METHOD_END"], "methodName": ["testParseSSLConf"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldSkipCRC (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - update \"  ,     \"  - skipcrccheck \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldSyncFolder (  )  )  ;", "Assert . assertTrue ( options . shouldSkipCRC (  )  )  ;", "}", "METHOD_END"], "methodName": ["testParseSkipCRC"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldSyncFolder (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - update \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldSyncFolder (  )  )  ;", "}", "METHOD_END"], "methodName": ["testParseSyncFolders"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertNull ( options . getAtomicWorkPath (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - atomic \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertNull ( options . getAtomicWorkPath (  )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - atomic \"  ,     \"  - tmp \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / work \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getAtomicWorkPath (  )  ,    new   Path (  \" hdfs :  /  / localhost :  8  0  2  0  / work \"  )  )  ;", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - tmp \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / work \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" work   path   was   allowed   without    - atomic   switch \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testParseWorkPath"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "OptionsParser . parse ( new   String [  ]  {     \"  - bandwidth \"  ,     \"  0  \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "}", "METHOD_END"], "methodName": ["testParseZeroBandwidth"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getMapBandwidth (  )  ,    DistCpConstants . DEFAULT _ BANDWIDTH _ MB )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - bandwidth \"  ,     \"  1  1  \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getMapBandwidth (  )  ,     1  1  )  ;", "}", "METHOD_END"], "methodName": ["testParsebandwidth"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - p \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - p \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - pbr \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - pbrgup \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - pbrgupcax \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - pc \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . REPLICATION )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . USER )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . GROUP )  )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . ACL )  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . XATTR )  )  ;", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - p \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "int   i    =     0  ;", "Iterator < DistCpOptions . FileAttribute >    attribIterator    =    options . preserveAttributes (  )  ;", "while    ( attribIterator . hasNext (  )  )     {", "attribIterator . next (  )  ;", "i +  +  ;", "}", "Assert . assertEquals ( i ,     6  )  ;", "try    {", "OptionsParser . parse ( new   String [  ]  {     \"  - pabcd \"  ,     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target \"     }  )  ;", "Assert . fail (  \" Invalid   preserve   attribute \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}    catch    ( NoSuchElementException   ignore )     {", "}", "options    =    OptionsParser . parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertFalse ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "options . preserve ( DistCpOptions . FileAttribute . PERMISSION )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "options . preserve ( DistCpOptions . FileAttribute . PERMISSION )  ;", "Assert . assertTrue ( options . shouldPreserve ( DistCpOptions . FileAttribute . PERMISSION )  )  ;", "}", "METHOD_END"], "methodName": ["testPreserve"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getSourceFileListing (  )  ,    new   Path (  \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSourceListing"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "try    {", ". parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . fail (  \" Both   source   listing    &    source   paths   allowed \"  )  ;", "}    catch    ( IllegalArgumentException   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testSourceListingAndSourcePath"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    OptionsParser . parse ( new   String [  ]  {     \"  - f \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / source / first \"  ,     \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"     }  )  ;", "Assert . assertEquals ( options . getTargetPath (  )  ,    new   Path (  \" hdfs :  /  / localhost :  8  0  2  0  / target /  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testTargetPath"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   option    =    new   DistCpOptions ( new   Path (  \" abc \"  )  ,    new   Path (  \" xyz \"  )  )  ;", "String   val    =     \" DistCpOptions { atomicCommit = false ,    syncFolder = false ,    deleteMissing = false ,     \"     +     (  (  \" ignoreFailures = false ,    maxMaps =  2  0  ,    sslConfigurationFile =  ' null '  ,    copyStrategy =  ' uniformsize '  ,     \"     +     \" sourceFileListing = abc ,    sourcePaths = null ,    targetPath = xyz ,    targetPathExists = true ,     \"  )     +     \" preserveRawXattrs = false }  \"  )  ;", "Assert . assertEquals ( val ,    option . toString (  )  )  ;", "Assert . assertNotSame ( DistCpOptionSwitch . ATOMIC _ COMMIT . toString (  )  ,    DistCpOptionSwitch . ATOMIC _ COMMIT . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["testToString"], "fileName": "org.apache.hadoop.tools.TestOptionsParser"}, {"methodBody": ["METHOD_START", "{", "Path   metaFolder    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ META _ FOLDER )  )  ;", "try    {", "FileSystem   fs    =    metaFolder . getFileSystem ( conf )  ;", ". LOG . info (  (  \" Cleaning   up   temporary   work   folder :     \"     +    metaFolder )  )  ;", "fs . delete ( metaFolder ,    true )  ;", "}    catch    ( IOException   ignore )     {", ". LOG . error (  \" Exception   encountered    \"  ,    ignore )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.tools.mapred.CopyCommitter"}, {"methodBody": ["METHOD_START", "{", "try    {", "Configuration   conf    =    context . getConfiguration (  )  ;", "Path   targetWorkPath    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH )  )  ;", "FileSystem   targetFS    =    targetWorkPath . getFileSystem ( conf )  ;", "String   jobId    =    context . getJobID (  )  . toString (  )  ;", "deleteAttemptTempFiles ( targetWorkPath ,    targetFS ,    jobId )  ;", "deleteAttemptTempFiles ( targetWorkPath . getParent (  )  ,    targetFS ,    jobId )  ;", "}    catch    ( Throwable   t )     {", ". LOG . warn (  \" Unable   to   cleanup   temp   files \"  ,    t )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanupTempFiles"], "fileName": "org.apache.hadoop.tools.mapred.CopyCommitter"}, {"methodBody": ["METHOD_START", "{", "Path   workDir    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH )  )  ;", "Path   finalDir    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH )  )  ;", "FileSystem   targetFS    =    workDir . getFileSystem ( conf )  ;", ". LOG . info (  (  (  (  \" Atomic   commit   enabled .    Moving    \"     +    workDir )     +     \"    to    \"  )     +    finalDir )  )  ;", "if    (  ( targetFS . exists ( finalDir )  )     &  &     ( targetFS . exists ( workDir )  )  )     {", ". LOG . error (  (  \" Pre - existing   final - path   found   at :     \"     +    finalDir )  )  ;", "throw   new   IOException (  (  (  (  (  (  \" Target - path   can ' t   be   committed   to   because   it    \"     +     \" exists   at    \"  )     +    finalDir )     +     \"  .    Copied   data   is   in   temp - dir :     \"  )     +    workDir )     +     \"  .     \"  )  )  ;", "}", "boolean   result    =    targetFS . rename ( workDir ,    finalDir )  ;", "if    (  ! result )     {", ". LOG . warn (  \" Rename   failed .    Perhaps   data   already   moved .    Verifying .  .  .  \"  )  ;", "result    =     ( targetFS . exists ( finalDir )  )     &  &     (  !  ( targetFS . exists ( workDir )  )  )  ;", "}", "if    ( result )     {", ". LOG . info (  (  \" Data   committed   successfully   to    \"     +    finalDir )  )  ;", "taskAttemptContext . setStatus (  (  \" Data   committed   successfully   to    \"     +    finalDir )  )  ;", "} else    {", ". LOG . error (  (  \" Unable   to   commit   data   to    \"     +    finalDir )  )  ;", "throw   new   IOException (  (  (  (  \" Atomic   commit   failed .    Temporary   data   in    \"     +    workDir )     +     \"  ,    Unable   to   move   to    \"  )     +    finalDir )  )  ;", "}", "}", "METHOD_END"], "methodName": ["commitData"], "fileName": "org.apache.hadoop.tools.mapred.CopyCommitter"}, {"methodBody": ["METHOD_START", "{", "FileStatus [  ]    tempFiles    =    targetFS . globStatus ( new   Path ( targetWorkPath ,     (  (  \"  . distcp . tmp .  \"     +     ( jobId . replaceAll (  \" job \"  ,     \" attempt \"  )  )  )     +     \"  *  \"  )  )  )  ;", "if    (  ( tempFiles    !  =    null )     &  &     (  ( tempFiles . length )     >     0  )  )     {", "for    ( FileStatus   file    :    tempFiles )     {", ". LOG . info (  (  \" Cleaning   up    \"     +     ( file . getPath (  )  )  )  )  ;", "targetFS . delete ( file . getPath (  )  ,    false )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["deleteAttemptTempFiles"], "fileName": "org.apache.hadoop.tools.mapred.CopyCommitter"}, {"methodBody": ["METHOD_START", "{", "CopyCommitter . LOG . info (  (  \"  - delete   option   is   enabled .    About   to   remove   entries   from    \"     +     \" target   that   are   missing   in   source \"  )  )  ;", "Path   sourceListing    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ LISTING _ FILE _ PATH )  )  ;", "FileSystem   clusterFS    =    sourceListing . getFileSystem ( conf )  ;", "Path   sortedSourceListing    =    DistCpUtils . sortListing ( clusterFS ,    conf ,    sourceListing )  ;", "Path   targetListing    =    new   Path ( sourceListing . getParent (  )  ,     \" targetListing . seq \"  )  ;", "CopyListing   target    =    new   GlobbedCopyListing ( new   Configuration ( conf )  ,    null )  ;", "List < Path >    targets    =    new   ArrayList < Path >  (  1  )  ;", "Path   targetFinalPath    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH )  )  ;", "targets . add ( targetFinalPath )  ;", "DistCpOptions   options    =    new   DistCpOptions ( targets ,    new   Path (  \"  / NONE \"  )  )  ;", "options . setOverwrite ( overwrite )  ;", "options . setSyncFolder ( syncFolder )  ;", "options . setTargetPathExists ( targetPathExists )  ;", "target . buildListing ( targetListing ,    options )  ;", "Path   sortedTargetListing    =    DistCpUtils . sortListing ( clusterFS ,    conf ,    targetListing )  ;", "long   totalLen    =    clusterFS . getFileStatus ( sortedTargetListing )  . getLen (  )  ;", "SequenceFile . Reader   sourceReader    =    new   SequenceFile . Reader ( conf ,    Reader . file ( sortedSourceListing )  )  ;", "SequenceFile . Reader   targetReader    =    new   SequenceFile . Reader ( conf ,    Reader . file ( sortedTargetListing )  )  ;", "long   deletedEntries    =     0  ;", "try    {", "CopyListingFileStatus   srcFileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   srcRelPath    =    new   Text (  )  ;", "CopyListingFileStatus   trgtFileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   trgtRelPath    =    new   Text (  )  ;", "FileSystem   targetFS    =    targetFinalPath . getFileSystem ( conf )  ;", "boolean   srcAvailable    =    sourceReader . next ( srcRelPath ,    srcFileStatus )  ;", "while    ( targetReader . next ( trgtRelPath ,    trgtFileStatus )  )     {", "while    ( srcAvailable    &  &     (  ( trgtRelPath . compareTo ( srcRelPath )  )     >     0  )  )     {", "srcAvailable    =    sourceReader . next ( srcRelPath ,    srcFileStatus )  ;", "}", "if    ( srcAvailable    &  &     ( trgtRelPath . equals ( srcRelPath )  )  )", "continue ;", "boolean   result    =     (  !  ( targetFS . exists ( trgtFileStatus . getPath (  )  )  )  )     |  |     ( targetFS . delete ( trgtFileStatus . getPath (  )  ,    true )  )  ;", "if    ( result )     {", "CopyCommitter . LOG . info (  (  (  \" Deleted    \"     +     ( trgtFileStatus . getPath (  )  )  )     +     \"     -    Missing   at   source \"  )  )  ;", "deletedEntries +  +  ;", "} else    {", "throw   new   IOException (  (  \" Unable   to   delete    \"     +     ( trgtFileStatus . getPath (  )  )  )  )  ;", "}", "taskAttemptContext . progress (  )  ;", "taskAttemptContext . setStatus (  (  (  \" Deleting   missing   files   from   target .     [  \"     +     (  (  ( targetReader . getPosition (  )  )     *     1  0  0  )     /    totalLen )  )     +     \"  %  ]  \"  )  )  ;", "}", "}    finally    {", "IOUtils . closeStream ( sourceReader )  ;", "IOUtils . closeStream ( targetReader )  ;", "}", "CopyCommitter . LOG . info (  (  (  (  \" Deleted    \"     +    deletedEntries )     +     \"    from   target :     \"  )     +     ( targets . get (  0  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["deleteMissing"], "fileName": "org.apache.hadoop.tools.mapred.CopyCommitter"}, {"methodBody": ["METHOD_START", "{", "String   attrSymbols    =    conf . get ( DistCpConstants . CONF _ LABEL _ PRESERVE _ STATUS )  ;", "final   boolean   syncOrOverwrite    =     ( syncFolder )     |  |     ( overwrite )  ;", ". LOG . info (  (  \" About   to   preserve   attributes :     \"     +    attrSymbols )  )  ;", "EnumSet < DistCpOptions . FileAttribute >    attributes    =    DistCpUtils . unpackAttributes ( attrSymbols )  ;", "final   boolean   preserveRawXattrs    =    conf . getBoolean ( DistCpConstants . CONF _ LABEL _ PRESERVE _ RAWXATTRS ,    false )  ;", "Path   sourceListing    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ LISTING _ FILE _ PATH )  )  ;", "FileSystem   clusterFS    =    sourceListing . getFileSystem ( conf )  ;", "SequenceFile . Reader   sourceReader    =    new   SequenceFile . Reader ( conf ,    Reader . file ( sourceListing )  )  ;", "long   totalLen    =    clusterFS . getFileStatus ( sourceListing )  . getLen (  )  ;", "Path   targetRoot    =    new   Path ( conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH )  )  ;", "long   preservedEntries    =     0  ;", "try    {", "CopyListingFileStatus   srcFileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   srcRelPath    =    new   Text (  )  ;", "while    ( sourceReader . next ( srcRelPath ,    srcFileStatus )  )     {", "if    (  !  ( srcFileStatus . isDirectory (  )  )  )", "continue ;", "Path   targetFile    =    new   Path (  (  (  ( targetRoot . toString (  )  )     +     \"  /  \"  )     +    srcRelPath )  )  ;", "if    (  ( targetRoot . equals ( targetFile )  )     &  &    syncOrOverwrite )", "continue ;", "FileSystem   targetFS    =    targetFile . getFileSystem ( conf )  ;", "DistCpUtils . preserve ( targetFS ,    targetFile ,    srcFileStatus ,    attributes ,    preserveRawXattrs )  ;", "taskAttemptContext . progress (  )  ;", "taskAttemptContext . setStatus (  (  (  \" Preserving   status   on   directory   entries .     [  \"     +     (  (  ( sourceReader . getPosition (  )  )     *     1  0  0  )     /    totalLen )  )     +     \"  %  ]  \"  )  )  ;", "}", "}    finally    {", "IOUtils . closeStream ( sourceReader )  ;", "}", ". LOG . info (  (  (  \" Preserved   status   on    \"     +    preservedEntries )     +     \"    dir   entries   on   target \"  )  )  ;", "}", "METHOD_END"], "methodName": ["preserveFileAttributesForDirectories"], "fileName": "org.apache.hadoop.tools.mapred.CopyCommitter"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( syncFolders )  )     {", "return   true ;", "}", "bean   sameLength    =     ( target . getLen (  )  )     =  =     ( source . getLen (  )  )  ;", "bean   sameBlockSize    =     (  ( source . getBlockSize (  )  )     =  =     ( target . getBlockSize (  )  )  )     |  |     (  !  ( preserve . contains ( DistCpOptions . FileAttribute . BLOCKSIZE )  )  )  ;", "if    ( sameLength    &  &    sameBlockSize )     {", "return    ( skipCrc )     |  |     ( DistCpUtils . checksumsAreEqual ( sourceFS ,    source . getPath (  )  ,    null ,    targetFS ,    target . getPath (  )  )  )  ;", "} else    {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["canSkip"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "final   FileStatus   targetFileStatus ;", "try    {", "targetFileStatus    =    targetFS . getFileStatus ( target )  ;", "}    catch    ( FileNotFoundException   e )     {", "return   CopyMapper . FileAction . OVERWRITE ;", "}", "if    (  ( targetFileStatus    !  =    null )     &  &     (  !  ( overWrite )  )  )     {", "if    ( canSkip ( sourceFS ,    source ,    targetFileStatus )  )     {", "return   CopyMapper . FileAction . SKIP ;", "} else", "if    ( append )     {", "long   targetLen    =    targetFileStatus . getLen (  )  ;", "if    ( targetLen    <     ( source . getLen (  )  )  )     {", "fs . FileChecksum   sourceChecksum    =    sourceFS . getFileChecksum ( source . getPath (  )  ,    targetLen )  ;", "if    (  ( sourceChecksum    !  =    null )     &  &     ( sourceChecksum . equals ( targetFS . getFileChecksum ( target )  )  )  )     {", "return   CopyMapper . FileAction . APPEND ;", "}", "}", "}", "}", "return   CopyMapper . FileAction . OVERWRITE ;", "}", "METHOD_END"], "methodName": ["checkUpdate"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "long   bytesCopied ;", "try    {", "bytesCopied    =     (  ( Long )     ( new   RetriableFileCopyCommand ( skipCrc ,    description ,    action )  . execute ( sourceFileStatus ,    target ,    context ,    fileAttributes )  )  )  ;", "}    catch    ( Exception   e )     {", "context . setStatus (  (  \" Copy   Failure :     \"     +     ( sourceFileStatus . getPath (  )  )  )  )  ;", "throw   new   IOException (  (  (  (  \" File   copy   failed :     \"     +     ( sourceFileStatus . getPath (  )  )  )     +     \"     -  -  >     \"  )     +    target )  ,    e )  ;", "}", ". incrementCounter ( context ,     . Counter . BYTESEXPECTED ,    sourceFileStatus . getLen (  )  )  ;", ". incrementCounter ( context ,     . Counter . BYTESCOPIED ,    bytesCopied )  ;", ". incrementCounter ( context ,     . Counter . COPY ,     1  )  ;", "}", "METHOD_END"], "methodName": ["copyFileWithRetry"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "new   RetriableDirectoryCreateCommand ( description )  . execute ( target ,    context )  ;", "}    catch    ( Exception   e )     {", "throw   new   IOException (  (  \" mkdir   failed   for    \"     +    target )  ,    e )  ;", "}", ". incrementCounter ( context ,     . Counter . COPY ,     1  )  ;", "}", "METHOD_END"], "methodName": ["createTargetDirsWithRetry"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "if    (  ( cacheFiles    !  =    null )     &  &     (  ( cacheFiles . length )     >     0  )  )     {", "for    ( Path   file    :    cacheFiles )     {", "if    ( file . getName (  )  . equals ( fileName )  )     {", "return   file ;", "}", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["findCacheFile"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "String   attributeString    =    context . getConfiguration (  )  . get ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  )  ;", "return   DistCpUtils . unkAttributes ( attributeString )  ;", "}", "METHOD_END"], "methodName": ["getFileAttributeSettings"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "return   fileStatus    =  =    null    ?     \" N / A \"     :    fileStatus . isDirectory (  )     ?     \" dir \"     :     \" file \"  ;", "}", "METHOD_END"], "methodName": ["getFileType"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "CopyMapper . LOG . error (  (  (  (  \" Failure   in   copying    \"     +     ( sourceFileStatus . getPath (  )  )  )     +     \"    to    \"  )     +    target )  ,    exception )  ;", "if    (  ( ignoreFailures )     &  &     (  ( exception . getCause (  )  )    instanceof   RetriableFileCopyCommand . CopyReadException )  )     {", "CopyMapper . incrementCounter ( context ,    CopyMapper . Counter . FAIL ,     1  )  ;", "CopyMapper . incrementCounter ( context ,    CopyMapper . Counter . BYTESFAILED ,    sourceFileStatus . getLen (  )  )  ;", "context . write ( null ,    new   io . Text (  (  (  (  \" FAIL :     \"     +     ( sourceFileStatus . getPath (  )  )  )     +     \"     -     \"  )     +     ( StringUtils . stringifyException ( exception )  )  )  )  )  ;", "} else", "throw   exception ;", "}", "METHOD_END"], "methodName": ["handleFailures"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "context . getCounter ( counter )  . increment ( value )  ;", "}", "METHOD_END"], "methodName": ["incrementCounter"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "CopyMapper . LOG . info (  \" Initializing   SSL   configuration \"  )  ;", "String   workDir    =     ( conf . get ( JOB _ LOCAL _ DIR )  )     +     \"  / work \"  ;", "Path [  ]    cacheFiles    =    context . getLocalCacheFiles (  )  ;", "Configuration   sslConfig    =    new   Configuration ( false )  ;", "String   sslConfFileName    =    conf . get ( DistCpConstants . CONF _ LABEL _ SSL _ CONF )  ;", "Path   sslClient    =    findCacheFile ( cacheFiles ,    sslConfFileName )  ;", "if    ( sslClient    =  =    null )     {", "CopyMapper . LOG . warn (  (  (  (  \" SSL   Client   config   file   not   found .    Was   looking   for    \"     +    sslConfFileName )     +     \"    in    \"  )     +     ( Arrays . toString ( cacheFiles )  )  )  )  ;", "return ;", "}", "sslConfig . addResource ( sslClient )  ;", "String   trustStoreFile    =    conf . get (  \" ssl . client . truststore . location \"  )  ;", "Path   trustStorePath    =    findCacheFile ( cacheFiles ,    trustStoreFile )  ;", "sslConfig . set (  \" ssl . client . truststore . location \"  ,    trustStorePath . toString (  )  )  ;", "String   keyStoreFile    =    conf . get (  \" ssl . client . keystore . location \"  )  ;", "Path   keyStorePath    =    findCacheFile ( cacheFiles ,    keyStoreFile )  ;", "sslConfig . set (  \" ssl . client . keystore . location \"  ,    keyStorePath . toString (  )  )  ;", "try    {", "OutputStream   out    =    new   FileOutputStream (  (  ( workDir    +     \"  /  \"  )     +    sslConfFileName )  )  ;", "try    {", "sslConfig . writeXml ( out )  ;", "}    finally    {", "out . close (  )  ;", "}", "conf . set ( DistCpConstants . CONF _ LABEL _ SSL _ KEYSTORE ,    sslConfFileName )  ;", "}    catch    ( IOException   e )     {", "CopyMapper . LOG . warn (  (  \" Unable   to   write   out   the   ssl   configuration .     \"     +     \" Will   fall   back   to   default   ssl - client . xml   in   class   path ,    if   there   is   one \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["initializeSSLConf"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "CopyMapper . incrementCounter ( context ,    CopyMapper . Counter . SKIP ,     1  )  ;", "CopyMapper . incrementCounter ( context ,    CopyMapper . Counter . BYTESSKIPPED ,    sourceFile . getLen (  )  )  ;", "}", "METHOD_END"], "methodName": ["updateSkipCounters"], "fileName": "org.apache.hadoop.tools.mapred.CopyMapper"}, {"methodBody": ["METHOD_START", "{", "String   commitDirectory    =    conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH )  ;", "if    (  ( commitDirectory    =  =    null )     |  |     ( commitDirectory . isEmpty (  )  )  )     {", "return   null ;", "} else    {", "return   new   Path ( commitDirectory )  ;", "}", "}", "METHOD_END"], "methodName": ["getCommitDirectory"], "fileName": "org.apache.hadoop.tools.mapred.CopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "return   CopyOutputFormat . getCommitDirectory ( job . getConfiguration (  )  )  ;", "}", "METHOD_END"], "methodName": ["getCommitDirectory"], "fileName": "org.apache.hadoop.tools.mapred.CopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "String   workingDirectory    =    conf . get ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH )  ;", "if    (  ( workingDirectory    =  =    null )     |  |     ( workingDirectory . isEmpty (  )  )  )     {", "return   null ;", "} else    {", "return   new   Path ( workingDirectory )  ;", "}", "}", "METHOD_END"], "methodName": ["getWorkingDirectory"], "fileName": "org.apache.hadoop.tools.mapred.CopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "return   CopyOutputFormat . getWorkingDirectory ( job . getConfiguration (  )  )  ;", "}", "METHOD_END"], "methodName": ["getWorkingDirectory"], "fileName": "org.apache.hadoop.tools.mapred.CopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "job . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    commitDirectory . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["setCommitDirectory"], "fileName": "org.apache.hadoop.tools.mapred.CopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "job . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    workingDirectory . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["setWorkingDirectory"], "fileName": "org.apache.hadoop.tools.mapred.CopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( DistCpUtils . checksumsAreEqual ( sourceFS ,    source ,    sourceChecksum ,    targetFS ,    target )  )  )     {", "StringBuilder   errorMessage    =    new   StringBuilder (  \" Check - sum   mismatch   between    \"  )  . append ( source )  . append (  \"    and    \"  )  . append ( target )  . append (  \"  .  \"  )  ;", "if    (  ( sourceFS . getStatus ( source )  . getBlockSize (  )  )     !  =     ( targetFS . getStatus ( target )  . getBlockSize (  )  )  )     {", "errorMessage . append (  \"    Source   and   target   differ   in   block - size .  \"  )  . append (  \"    Use    - pb   to   preserve   block - sizes   during   copy .  \"  )  . append (  \"    Alternatively ,    skip   checksum - checks   altogether ,    using    - skipCrc .  \"  )  . append (  \"     ( NOTE :    By   skipping   checksums ,    one   runs   the   risk   of   masking   data - corruption   during   file - transfer .  )  \"  )  ;", "}", "throw   new   IOException ( errorMessage . toString (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compareCheckSums"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "final   Path   sourcePath    =    sourceFileStatus . getPath (  )  ;", "FileSystem   fs    =    sourcePath . getFileSystem ( configuration )  ;", "if    (  ( fs . getFileStatus ( sourcePath )  . getLen (  )  )     !  =    targetLen )", "throw   new   IOException (  (  (  (  \" Mismatch   in   length   of   source :  \"     +    sourcePath )     +     \"    and   target :  \"  )     +    target )  )  ;", "}", "METHOD_END"], "methodName": ["compareFileLengths"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "Path   source    =    sourceFileStatus . getPath (  )  ;", "byte [  ]    buf    =    new   byte [ bufferSize ]  ;", "ThrottledInputStream   inStream    =    null ;", "long   totalBytesRead    =     0  ;", "try    {", "inStream    =     . getInputStream ( source ,    context . getConfiguration (  )  )  ;", "int   bytesRead    =     . readBytes ( inStream ,    buf ,    sourceOffset )  ;", "while    ( bytesRead    >  =     0  )     {", "totalBytesRead    +  =    bytesRead ;", "if    (  ( action )     =  =     ( CopyMapper . FileAction . APPEND )  )     {", "sourceOffset    +  =    bytesRead ;", "}", "outStream . write ( buf ,     0  ,    bytesRead )  ;", "updateContextStatus ( totalBytesRead ,    context ,    sourceFileStatus )  ;", "bytesRead    =     . readBytes ( inStream ,    buf ,    sourceOffset )  ;", "}", "outStream . close (  )  ;", "outStream    =    null ;", "}    finally    {", "IOUtils . cleanup (  . LOG ,    outStream ,    inStream )  ;", "}", "return   totalBytesRead ;", "}", "METHOD_END"], "methodName": ["copyBytes"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "FsPermission   permission    =    FsPermission . getFileDefault (  )  . applyUMask ( FsPermission . getUMask ( targetFS . getConf (  )  )  )  ;", "final   OutputStream   outStream ;", "if    (  ( action )     =  =     ( CopyMapper . FileAction . OVERWRITE )  )     {", "final   short   repl    =     . getReplicationFactor ( fileAttributes ,    sourceFileStatus ,    targetFS ,    targetPath )  ;", "final   long   blockSize    =     . getBlockSize ( fileAttributes ,    sourceFileStatus ,    targetFS ,    targetPath )  ;", "FSDataOutputStream   out    =    targetFS . create ( targetPath ,    permission ,    EnumSet . of ( CREATE ,    OVERWRITE )  ,     . BUFFER _ SIZE ,    repl ,    blockSize ,    context ,    getChecksumOpt ( fileAttributes ,    sourceChecksum )  )  ;", "outStream    =    new   BufferedOutputStream ( out )  ;", "} else    {", "outStream    =    new   BufferedOutputStream ( targetFS . append ( targetPath ,     . BUFFER _ SIZE )  )  ;", "}", "return   copyBytes ( sourceFileStatus ,    sourceOffset ,    outStream ,     . BUFFER _ SIZE ,    context )  ;", "}", "METHOD_END"], "methodName": ["copyToFile"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "final   boolean   toAppend    =     ( action )     =  =     ( CopyMapper . FileAction . APPEND )  ;", "Path   targetPath    =     ( toAppend )     ?    target    :    getTmpFile ( target ,    context )  ;", "final   Configuration   configuration    =    context . getConfiguration (  )  ;", "FileSystem   targetFS    =    target . getFileSystem ( configuration )  ;", "try    {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Copying    \"     +     ( sourceFileStatus . getPath (  )  )  )     +     \"    to    \"  )     +    target )  )  ;", ". LOG . debug (  (  \" Target   file   path :     \"     +    targetPath )  )  ;", "}", "final   Path   sourcePath    =    sourceFileStatus . getPath (  )  ;", "final   FileSystem   sourceFS    =    sourcePath . getFileSystem ( configuration )  ;", "final   FileChecksum   sourceChecksum    =     ( fileAttributes . contains ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )     ?    sourceFS . getFileChecksum ( sourcePath )     :    null ;", "final   long   offset    =     (  ( action )     =  =     ( CopyMapper . FileAction . APPEND )  )     ?    targetFS . getFileStatus ( target )  . getLen (  )     :     0  ;", "long   bytesRead    =    copyToFile ( targetPath ,    targetFS ,    sourceFileStatus ,    offset ,    context ,    fileAttributes ,    sourceChecksum )  ;", "compareFileLengths ( sourceFileStatus ,    targetPath ,    configuration ,     ( bytesRead    +    offset )  )  ;", "if    (  ( bytesRead    !  =     0  )     &  &     (  !  ( skipCrc )  )  )     {", "compareCheckSums ( sourceFS ,    sourceFileStatus . getPath (  )  ,    sourceChecksum ,    targetFS ,    targetPath )  ;", "}", "if    (  ! toAppend )     {", "promoteTmpToTarget ( targetPath ,    target ,    targetFS )  ;", "}", "return   bytesRead ;", "}    finally    {", "if    (  (  ! toAppend )     &  &     ( targetFS . exists ( targetPath )  )  )     {", "targetFS . delete ( targetPath ,    false )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["doCopy"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "boolean   preserve    =     ( fileAttributes . contains ( DistCpOptions . FileAttribute . BLOCKSIZE )  )     |  |     ( fileAttributes . contains ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )  ;", "return   preserve    ?    sourc . getBlockSize (  )     :    targetFS . getDefaultBlockSize ( tmpTargetPath )  ;", "}", "METHOD_END"], "methodName": ["getBlockSize"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "if    (  ( fileAttributes . contains ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  )     &  &     ( sourceChecksum    !  =    null )  )     {", "return   sourceChecksum . getChecksumOpt (  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getChecksumOpt"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileSystem   fs    =    path . getFileSystem ( conf )  ;", "long   bandwidthMB    =    conf . getInt ( DistCpConstants . CONF _ LABEL _ BANDWIDTH _ MB ,    DistCpConstants . DEFAULT _ BANDWIDTH _ MB )  ;", "FSDataInputStream   in    =    fs . open ( path )  ;", "return   new   ThrottledInputStream ( in ,     (  ( bandwidthMB    *     1  0  2  4  )     *     1  0  2  4  )  )  ;", "}    catch    ( IOException   e )     {", "throw   new    . CopyReadException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getInputStream"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "return   fileAttributes . contains ( DistCpOptions . FileAttribute . REPLICATION )     ?    sourceFile . getReplication (  )     :    targetFS . getDefaultReplication ( tmpTargetPath )  ;", "}", "METHOD_END"], "methodName": ["getReplicationFactor"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "Path   targetWorkPath    =    new   Path ( context . getConfiguration (  )  . get ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH )  )  ;", "Path   root    =     ( target . equals ( targetWorkPath )  )     ?    targetWorkPath . getParent (  )     :    targetWorkPath ;", ". LOG . info (  (  \" Creating   temp   file :     \"     +     ( new   Path ( root ,     (  \"  . distcp . tmp .  \"     +     ( context . getTaskAttemptID (  )  . toString (  )  )  )  )  )  )  )  ;", "return   new   Path ( root ,     (  \"  . distcp . tmp .  \"     +     ( context . getTaskAttemptID (  )  . toString (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getTmpFile"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  ( fs . exists ( target )  )     &  &     (  !  ( fs . delete ( target ,    false )  )  )  )     |  |     (  (  !  ( fs . exists ( target . getParent (  )  )  )  )     &  &     (  !  ( fs . mkdirs ( target . getParent (  )  )  )  )  )  )     |  |     (  !  ( fs . rename ( tmpTarget ,    target )  )  )  )     {", "throw   new   IOException (  (  (  (  \" Failed   to   promote   tmp - file :  \"     +    tmpTarget )     +     \"    to :     \"  )     +    target )  )  ;", "}", "}", "METHOD_END"], "methodName": ["promoteTmpToTarget"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    ( position    =  =     0  )     {", "return   inStream . read ( buf )  ;", "} else    {", "return   inStream . read ( position ,    buf ,     0  ,    buf . length )  ;", "}", "}    catch    ( IOException   e )     {", "throw   new    . CopyReadException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["readBytes"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   message    =    new   StringBuilder ( DistCpUtils . getFormatter (  )  . format (  (  ( totalBytesRead    *     1  0  0  .  0 F )     /     ( sourceFileStatus . getLen (  )  )  )  )  )  ;", "message . append (  \"  %     \"  )  . append ( description )  . append (  \"     [  \"  )  . append ( DistCpUtils . getStringDescriptionFor ( totalBytesRead )  )  . append (  '  /  '  )  . append ( DistCpUtils . getStringDescriptionFor ( sourceFileStatus . getLen (  )  )  )  . append (  '  ]  '  )  ;", "context . setStatus ( message . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["updateContextStatus"], "fileName": "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "Path   base    =    new   Path ( targetBase )  ;", "Stack < Path >    stack    =    new   Stack < Path >  (  )  ;", "stack . push ( base )  ;", "while    (  !  ( stack . isEmpty (  )  )  )     {", "Path   file    =    stack . pop (  )  ;", "if    (  !  ( fs . exists ( file )  )  )", "continue ;", "FileStatus [  ]    fStatus    =    fs . listStatus ( file )  ;", "if    (  ( fStatus    =  =    null )     |  |     (  ( fStatus . length )     =  =     0  )  )", "continue ;", "for    ( FileStatus   status    :    fStatus )     {", "if    ( status . isDirectory (  )  )     {", "stack . push ( status . getPath (  )  )  ;", "Assert . assertEquals ( status . getPermission (  )  ,    sourcePerm )  ;", "}", "}", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["checkDirectoryPermissions"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "Path   meta    =    new   Path (  \"  / meta \"  )  ;", "try    {", "if    (  . cluster . getFileSystem (  )  . exists ( meta )  )     {", ". cluster . getFileSystem (  )  . delete ( meta ,    true )  ;", "Assert . fail (  \" Expected   meta   folder   to   be   deleted \"  )  ;", "}", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   cleaning   up   folder \"  ,    e )  ;", "Assert . fail (  \" Unable   to   clean   up   meta   folder \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanupMetaFolder"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TestCopyCommitter . config    =    TestCopyCommitter . getJobForClient (  )  . getConfiguration (  )  ;", "TestCopyCommitter . config . setLong ( DistCpConstants . CONF _ LABEL _ TOTAL _ BYTES _ TO _ BE _ COPIED ,     0  )  ;", "TestCopyCommitter . cluster    =    new   hdfs . MiniDFSCluster . Builder ( TestCopyCommitter . config )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TestCopyCommitter . config . set ( DistCpConstants . CONF _ LABEL _ META _ FOLDER ,     \"  / meta \"  )  ;", "Path   meta    =    new   Path (  \"  / meta \"  )  ;", "try    {", "TestCopyCommitter . cluster . getFileSystem (  )  . mkdirs ( meta )  ;", "}    catch    ( IOException   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered   while   creating   meta   folder \"  ,    e )  ;", "Assert . fail (  \" Unable   to   create   meta   folder \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["createMetaFolder"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestCopyCommitter . cluster )     !  =    null )     {", "TestCopyCommitter . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["destroy"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "Job   job    =    Job . getInstance ( new   Configuration (  )  )  ;", "job . getConfiguration (  )  . set (  \" mapred . job . tracker \"  ,     (  \" localhost :  \"     +     (  . PORT )  )  )  ;", "job . setInputFormatClass (  . NullInputFormat . class )  ;", "job . setOutputFormatClass ( NullOutputFormat . class )  ;", "job . setNumReduceTasks (  0  )  ;", "return   job ;", "}", "METHOD_END"], "methodName": ["getJobForClient"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "return   new   TaskAttemptContextImpl ( conf ,    new   TaskAttemptID (  \"  2  0  0  7  0  7  1  2  1  7  3  3  \"  ,     1  ,    TaskType . MAP ,     1  ,     1  )  )  ;", "}", "METHOD_END"], "methodName": ["getTaskAttemptContext"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptContext   taskAttemptContext    =    getTaskAttemptContext ( TestCopyCommitter . config )  ;", "JobContext   jobContext    =    new   mapreduce . task . JobContextImpl ( taskAttemptContext . getConfiguration (  )  ,    taskAttemptContext . getTaskAttemptID (  )  . getJobID (  )  )  ;", "Configuration   conf    =    jobContext . getConfiguration (  )  ;", "String   workPath    =     \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  ;", "String   finalPath    =     \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  ;", "FileSystem   fs    =    null ;", "try    {", "OutputCommitter   committer    =    new   CopyCommitter ( null ,    taskAttemptContext )  ;", "fs    =    FileSystem . get ( conf )  ;", "fs . mkdirs ( new   Path ( workPath )  )  ;", "fs . mkdirs ( new   Path ( finalPath )  )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    workPath )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    finalPath )  ;", "conf . setBoolean ( DistCpConstants . CONF _ LABEL _ ATOMIC _ COPY ,    true )  ;", "Assert . assertTrue ( fs . exists ( new   Path ( workPath )  )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path ( finalPath )  )  )  ;", "try    {", "committer . commitJob ( jobContext )  ;", "Assert . fail (  \" Should   not   be   able   to   atomic - commit   to   pre - existing   path .  \"  )  ;", "}    catch    ( Exception   exception )     {", "Assert . assertTrue ( fs . exists ( new   Path ( workPath )  )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path ( finalPath )  )  )  ;", "TestCopyCommitter . LOG . info (  \" Atomic - commit   Test   pass .  \"  )  ;", "}", "}    catch    ( IOException   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered   while   testing   for   atomic   commit .  \"  ,    e )  ;", "Assert . fail (  \" Atomic   commit   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,    workPath )  ;", "TestDistCpUtils . delete ( fs ,    finalPath )  ;", "conf . setBoolean ( DistCpConstants . CONF _ LABEL _ ATOMIC _ COPY ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["testAtomicCommitExistingFinal"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptContext   taskAttemptContext    =    getTaskAttemptContext ( TestCopyCommitter . config )  ;", "JobContext   jobContext    =    new   mapreduce . task . JobContextImpl ( taskAttemptContext . getConfiguration (  )  ,    taskAttemptContext . getTaskAttemptID (  )  . getJobID (  )  )  ;", "Configuration   conf    =    jobContext . getConfiguration (  )  ;", "String   workPath    =     \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  ;", "String   finalPath    =     \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  ;", "FileSystem   fs    =    null ;", "try    {", "OutputCommitter   committer    =    new   CopyCommitter ( null ,    taskAttemptContext )  ;", "fs    =    FileSystem . get ( conf )  ;", "fs . mkdirs ( new   Path ( workPath )  )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    workPath )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    finalPath )  ;", "conf . setBoolean ( DistCpConstants . CONF _ LABEL _ ATOMIC _ COPY ,    true )  ;", "Assert . assertTrue ( fs . exists ( new   Path ( workPath )  )  )  ;", "Assert . assertFalse ( fs . exists ( new   Path ( finalPath )  )  )  ;", "committer . commitJob ( jobContext )  ;", "Assert . assertFalse ( fs . exists ( new   Path ( workPath )  )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path ( finalPath )  )  )  ;", "committer . commitJob ( jobContext )  ;", "Assert . assertFalse ( fs . exists ( new   Path ( workPath )  )  )  ;", "Assert . assertTrue ( fs . exists ( new   Path ( finalPath )  )  )  ;", "}    catch    ( IOException   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered   while   testing   for   preserve   status \"  ,    e )  ;", "Assert . fail (  \" Atomic   commit   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,    workPath )  ;", "TestDistCpUtils . delete ( fs ,    finalPath )  ;", "conf . setBoolean ( DistCpConstants . CONF _ LABEL _ ATOMIC _ COPY ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["testAtomicCommitMissingFinal"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptContext   taskAttemptContext    =    getTaskAttemptContext ( TestCopyCommitter . config )  ;", "JobContext   jobContext    =    new   mapreduce . task . JobContextImpl ( taskAttemptContext . getConfiguration (  )  ,    taskAttemptContext . getTaskAttemptID (  )  . getJobID (  )  )  ;", "Configuration   conf    =    jobContext . getConfiguration (  )  ;", "String   sourceBase ;", "String   targetBase ;", "FileSystem   fs    =    null ;", "try    {", "OutputCommitter   committer    =    new   CopyCommitter ( null ,    taskAttemptContext )  ;", "fs    =    FileSystem . get ( conf )  ;", "sourceBase    =    TestDistCpUtils . createTestSetup ( fs ,    FsPermission . getDefault (  )  )  ;", "targetBase    =    TestDistCpUtils . createTestSetup ( fs ,    FsPermission . getDefault (  )  )  ;", "String   targetBaseAdd    =    TestDistCpUtils . createTestSetup ( fs ,    FsPermission . getDefault (  )  )  ;", "fs . rename ( new   Path ( targetBaseAdd )  ,    new   Path ( targetBase )  )  ;", "DistCpOptions   options    =    new   DistCpOptions ( Arrays . asList ( new   Path ( sourceBase )  )  ,    new   Path (  \"  / out \"  )  )  ;", "options . setSyncFolder ( true )  ;", "options . setDeleteMissing ( true )  ;", "options . appendToConf ( conf )  ;", "CopyListing   listing    =    new   GlobbedCopyListing ( conf ,    TestCopyCommitter . CREDENTIALS )  ;", "Path   listingFile    =    new   Path (  (  \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  )  )  ;", "listing . buildListing ( listingFile ,    options )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    targetBase )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    targetBase )  ;", "committer . commitJob ( jobContext )  ;", "if    (  !  ( TestDistCpUtils . checkIfFoldersAreInSync ( fs ,    targetBase ,    sourceBase )  )  )     {", "Assert . fail (  \" Source   and   target   folders   are   not   in   sync \"  )  ;", "}", "if    (  !  ( TestDistCpUtils . checkIfFoldersAreInSync ( fs ,    sourceBase ,    targetBase )  )  )     {", "Assert . fail (  \" Source   and   target   folders   are   not   in   sync \"  )  ;", "}", "committer . commitJob ( jobContext )  ;", "if    (  !  ( TestDistCpUtils . checkIfFoldersAreInSync ( fs ,    targetBase ,    sourceBase )  )  )     {", "Assert . fail (  \" Source   and   target   folders   are   not   in   sync \"  )  ;", "}", "if    (  !  ( TestDistCpUtils . checkIfFoldersAreInSync ( fs ,    sourceBase ,    targetBase )  )  )     {", "Assert . fail (  \" Source   and   target   folders   are   not   in   sync \"  )  ;", "}", "}    catch    ( Throwable   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered   while   testing   for   delete   missing \"  ,    e )  ;", "Assert . fail (  \" Delete   missing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,     \"  / tmp 1  \"  )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ DELETE _ MISSING ,     \" false \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDeleteMissing"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptContext   taskAttemptContext    =    getTaskAttemptContext ( TestCopyCommitter . config )  ;", "JobContext   jobContext    =    new   mapreduce . task . JobContextImpl ( taskAttemptContext . getConfiguration (  )  ,    taskAttemptContext . getTaskAttemptID (  )  . getJobID (  )  )  ;", "Configuration   conf    =    jobContext . getConfiguration (  )  ;", "String   sourceBase ;", "String   targetBase ;", "FileSystem   fs    =    null ;", "try    {", "OutputCommitter   committer    =    new   CopyCommitter ( null ,    taskAttemptContext )  ;", "fs    =    FileSystem . get ( conf )  ;", "sourceBase    =     \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  ;", "targetBase    =     \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  1  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  3  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  4  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  5  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  7  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  8  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( sourceBase    +     \"  /  9  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( targetBase    +     \"  /  2  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( targetBase    +     \"  /  4  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( targetBase    +     \"  /  5  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( targetBase    +     \"  /  7  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( targetBase    +     \"  /  9  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( targetBase    +     \"  / A \"  )  )  ;", "DistCpOptions   options    =    new   DistCpOptions ( Arrays . asList ( new   Path ( sourceBase )  )  ,    new   Path (  \"  / out \"  )  )  ;", "options . setSyncFolder ( true )  ;", "options . setDeleteMissing ( true )  ;", "options . appendToConf ( conf )  ;", "CopyListing   listing    =    new   GlobbedCopyListing ( conf ,    TestCopyCommitter . CREDENTIALS )  ;", "Path   listingFile    =    new   Path (  (  \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  )  )  ;", "listing . buildListing ( listingFile ,    options )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    targetBase )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    targetBase )  ;", "committer . commitJob ( jobContext )  ;", "if    (  !  ( TestDistCpUtils . checkIfFoldersAreInSync ( fs ,    targetBase ,    sourceBase )  )  )     {", "Assert . fail (  \" Source   and   target   folders   are   not   in   sync \"  )  ;", "}", "Assert . assertEquals ( fs . listStatus ( new   Path ( targetBase )  )  . length ,     4  )  ;", "committer . commitJob ( jobContext )  ;", "if    (  !  ( TestDistCpUtils . checkIfFoldersAreInSync ( fs ,    targetBase ,    sourceBase )  )  )     {", "Assert . fail (  \" Source   and   target   folders   are   not   in   sync \"  )  ;", "}", "Assert . assertEquals ( fs . listStatus ( new   Path ( targetBase )  )  . length ,     4  )  ;", "}    catch    ( IOException   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered   while   testing   for   delete   missing \"  ,    e )  ;", "Assert . fail (  \" Delete   missing   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,     \"  / tmp 1  \"  )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ DELETE _ MISSING ,     \" false \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDeleteMissingFlatInterleavedFiles"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptContext   taskAttemptContext    =    getTaskAttemptContext ( TestCopyCommitter . config )  ;", "JobContext   jobContext    =    new   mapreduce . task . JobContextImpl ( taskAttemptContext . getConfiguration (  )  ,    taskAttemptContext . getTaskAttemptID (  )  . getJobID (  )  )  ;", "try    {", "OutputCommitter   committer    =    new   CopyCommitter ( null ,    taskAttemptContext )  ;", "committer . commitJob ( jobContext )  ;", "Assert . assertEquals ( taskAttemptContext . getStatus (  )  ,     \" Commit   Successful \"  )  ;", "committer . commitJob ( jobContext )  ;", "Assert . assertEquals ( taskAttemptContext . getStatus (  )  ,     \" Commit   Successful \"  )  ;", "}    catch    ( IOException   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  \" Commit   failed \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNoCommitAction"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "TaskAttemptContext   taskAttemptContext    =    getTaskAttemptContext ( TestCopyCommitter . config )  ;", "JobContext   jobContext    =    new   mapreduce . task . JobContextImpl ( taskAttemptContext . getConfiguration (  )  ,    taskAttemptContext . getTaskAttemptID (  )  . getJobID (  )  )  ;", "Configuration   conf    =    jobContext . getConfiguration (  )  ;", "String   sourceBase ;", "String   targetBase ;", "FileSystem   fs    =    null ;", "try    {", "OutputCommitter   committer    =    new   CopyCommitter ( null ,    taskAttemptContext )  ;", "fs    =    FileSystem . get ( conf )  ;", "FsPermission   sourcePerm    =    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  ;", "FsPermission   initialPerm    =    new   FsPermission (  (  ( short )     (  4  4  8  )  )  )  ;", "sourceBase    =    TestDistCpUtils . createTestSetup ( fs ,    sourcePerm )  ;", "targetBase    =    TestDistCpUtils . createTestSetup ( fs ,    initialPerm )  ;", "DistCpOptions   options    =    new   DistCpOptions ( Arrays . asList ( new   Path ( sourceBase )  )  ,    new   Path (  \"  / out \"  )  )  ;", "options . preserve ( DistCpOptions . FileAttribute . PERMISSION )  ;", "options . appendToConf ( conf )  ;", "options . setTargetPathExists ( false )  ;", "CopyListing   listing    =    new   GlobbedCopyListing ( conf ,    TestCopyCommitter . CREDENTIALS )  ;", "Path   listingFile    =    new   Path (  (  \"  / tmp 1  /  \"     +     ( String . valueOf ( TestCopyCommitter . rand . nextLong (  )  )  )  )  )  ;", "listing . buildListing ( listingFile ,    options )  ;", "conf . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    targetBase )  ;", "committer . commitJob ( jobContext )  ;", "if    (  !  ( checkDirectoryPermissions ( fs ,    targetBase ,    sourcePerm )  )  )     {", "Assert . fail (  \" Permission   don ' t   match \"  )  ;", "}", "committer . commitJob ( jobContext )  ;", "if    (  !  ( checkDirectoryPermissions ( fs ,    targetBase ,    sourcePerm )  )  )     {", "Assert . fail (  \" Permission   don ' t   match \"  )  ;", "}", "}    catch    ( IOException   e )     {", "TestCopyCommitter . LOG . error (  \" Exception   encountered   while   testing   for   preserve   status \"  ,    e )  ;", "Assert . fail (  \" Preserve   status   failure \"  )  ;", "}    finally    {", "TestDistCpUtils . delete ( fs ,     \"  / tmp 1  \"  )  ;", "conf . unset ( DistCpConstants . CONF _ LABEL _ PRESERVE _ STATUS )  ;", "}", "}", "METHOD_END"], "methodName": ["testPreserveStatus"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyCommitter"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    toAppend    =    new   byte [ length ]  ;", "Random   random    =    new   Random (  )  ;", "random . nextBytes ( toAppend )  ;", "FSDataOutputStream   out    =     . cluster . getFileSystem (  )  . append ( p )  ;", "try    {", "out . write ( toAppend )  ;", "}    finally    {", "IOUtils . closeStream ( out )  ;", "}", "}", "METHOD_END"], "methodName": ["appendFile"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "for    ( Path   source    :    TestCopyMapper . pathList )     {", "if    ( fs . getFileStatus ( source )  . isFile (  )  )     {", "TestCopyMapper . appendFile ( source ,     (  ( TestCopyMapper . DEFAULT _ FILE _ SIZE )     *     2  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["appendSourceData"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "FsPermission   changedPermission    =    new   FsPermission ( FsAction . ALL ,    FsAction . ALL ,    FsAction . ALL )  ;", "for    ( Path   path    :    TestCopyMapper . pathList )", "if    ( fs . isFile ( path )  )     {", "fs . setOwner ( path ,    user ,    group )  ;", "fs . setPermission ( path ,    changedPermission )  ;", "}", "}", "METHOD_END"], "methodName": ["changeUserGroup"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  1  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  /  3  /  4  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  /  3  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  5  \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  5  /  6  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  /  8  \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  /  8  /  9  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["createSourceData"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  1  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  /  3  /  4  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  /  3  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  5  \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  5  /  6  \"  )  ,    true ,    null )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  /  8  \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  /  8  /  9  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["createSourceDataWithDifferentBlockSize"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  1  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  /  3  /  4  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  2  /  3  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  5  \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  5  /  6  \"  )  ,    new   fs . Options . ChecksumOpt ( Type . CRC 3  2  ,     5  1  2  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  \"  )  )  ;", "TestCopyMapper . mkdirs (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  /  8  \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  7  /  8  /  9  \"  )  ,    new   fs . Options . ChecksumOpt ( Type . CRC 3  2 C ,     5  1  2  )  )  ;", "}", "METHOD_END"], "methodName": ["createSourceDataWithDifferentChecksumType"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . pathList . clear (  )  ;", "TestCopyMapper . nFiles    =     0  ;", "TestCopyMapper . cluster . getFileSystem (  )  . delete ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    true )  ;", "TestCopyMapper . cluster . getFileSystem (  )  . delete ( new   Path ( TestCopyMapper . TARGET _ PATH )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["deleteState"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", ". deleteState (  )  ;", ". createSourceData (  )  ;", "FileSystem   fs    =     . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext (  . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "Configuration   configuration    =    context . getConfiguration (  )  ;", "configuration . setBoolean ( DistCpOptionSwitch . IGNORE _ FAILURES . getConfigLabel (  )  ,    ignoreFailures )  ;", "configuration . setBoolean ( DistCpOptionSwitch . OVERWRITE . getConfigLabel (  )  ,    true )  ;", "configuration . setBoolean ( DistCpOptionSwitch . SKIP _ CRC . getConfigLabel (  )  ,    true )  ;", "copyMapper . setup ( context )  ;", "for    ( Path   path    :     . pathList )     {", "final   FileStatus   fileStatus    =    fs . getFileStatus ( path )  ;", "if    (  !  ( fileStatus . isDirectory (  )  )  )     {", "fs . delete ( path ,    true )  ;", "copyMapper . map ( new   Text ( DistCpUtils . getRelativePath ( new   Path (  . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( fileStatus )  ,    context )  ;", "}", "}", "if    ( ignoreFailures )     {", "for    ( Text   value    :    stubContext . getWriter (  )  . values (  )  )     {", "Assert . assertTrue (  (  ( value . toString (  )  )     +     \"    is   not   skipped \"  )  ,    value . toString (  )  . startsWith (  \" FAIL :  \"  )  )  ;", "}", "}", "Assert . assertTrue (  \" There   should   have   been   an   exception .  \"  ,    ignoreFailures )  ;", "}    catch    ( Exception   e )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +     ( e . getMessage (  )  )  )  ,     (  ! ignoreFailures )  )  ;", "e . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["doTestIgnoreFailures"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    TestCopyMapper . getConfigurationForCluster (  )  ;", "final   FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "Path   workPath    =    new   Path ( TestCopyMapper . TARGET _ PATH )  . makeQualified ( fs . getUri (  )  ,    fs . getWorkingDirectory (  )  )  ;", "configuration . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    workPath . toString (  )  )  ;", "configuration . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    workPath . toString (  )  )  ;", "configuration . setBoolean ( DistCpOptionSwitch . OVERWRITE . getConfigLabel (  )  ,    false )  ;", "configuration . setBoolean ( DistCpOptionSwitch . SKIP _ CRC . getConfigLabel (  )  ,    false )  ;", "configuration . setBoolean ( DistCpOptionSwitch . SYNC _ FOLDERS . getConfigLabel (  )  ,    true )  ;", "configuration . set ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  ,     \" br \"  )  ;", "return   configuration ;", "}", "METHOD_END"], "methodName": ["getConfiguration"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    new   Configuration (  )  ;", "System . setProperty (  \" test . build . data \"  ,     \" target / tmp / build / TEST _ COPY _ MAPPER / data \"  )  ;", "configuration . set (  \" hadoop . log . dir \"  ,     \" target / tmp \"  )  ;", "configuration . set (  \" dfs . namenode . fs - limits . min - block - size \"  ,     \"  0  \"  )  ;", ". LOG . debug (  (  \" fs . default . name       =  =     \"     +     ( configuration . get (  \" fs . default . name \"  )  )  )  )  ;", ". LOG . debug (  (  \" dfs . http . address    =  =     \"     +     ( configuration . get (  \" dfs . http . address \"  )  )  )  )  ;", "return   configuration ;", "}", "METHOD_END"], "methodName": ["getConfigurationForCluster"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fileSystem    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "final   Path   qualifiedPath    =    new   Path ( path )  . makeQualified ( fileSystem . getUri (  )  ,    fileSystem . getWorkingDirectory (  )  )  ;", "TestCopyMapper . pathList . add ( qualifiedPath )  ;", "fileSystem . mkdirs ( qualifiedPath )  ;", "}", "METHOD_END"], "methodName": ["mkdirs"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . configuration    =    TestCopyMapper . getConfigurationForCluster (  )  ;", "TestCopyMapper . cluster    =    new   hdfs . MiniDFSCluster . Builder ( TestCopyMapper . configuration )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . deleteState (  )  ;", "if    ( preserveChecksum )     {", "TestCopyMapper . createSourceDataWithDifferentChecksumType (  )  ;", "} else    {", "TestCopyMapper . createSourceData (  )  ;", "}", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "Configuration   configuration    =    context . getConfiguration (  )  ;", "EnumSet < DistCpOptions . FileAttribute >    fileAttributes    =    EnumSet . of ( DistCpOptions . FileAttribute . REPLICATION )  ;", "if    ( preserveChecksum )     {", "fileAttributes . add ( DistCpOptions . FileAttribute . CHECKSUMTYPE )  ;", "}", "configuration . set ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  ,    DistCpUtils . packAttributes ( fileAttributes )  )  ;", "copyMapper . setup ( context )  ;", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "copyMapper . map ( new   Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( fs . getFileStatus ( path )  )  ,    context )  ;", "}", "verifyCopy ( fs ,    preserveChecksum )  ;", "Assert . assertEquals ( TestCopyMapper . pathList . size (  )  ,    stubContext . getReporter (  )  . getCounter ( CopyMapper . Counter . COPY )  . getValue (  )  )  ;", "if    (  ! preserveChecksum )     {", "Assert . assertEquals (  (  ( TestCopyMapper . nFiles )     *     ( TestCopyMapper . DEFAULT _ FILE _ SIZE )  )  ,    stubContext . getReporter (  )  . getCounter ( CopyMapper . Counter . BYTESCOPIED )  . getValue (  )  )  ;", "} else    {", "Assert . assertEquals (  (  (  ( TestCopyMapper . nFiles )     *     ( TestCopyMapper . NON _ DEFAULT _ BLOCK _ SIZE )  )     *     2  )  ,    stubContext . getReporter (  )  . getCounter ( CopyMapper . Counter . BYTESCOPIED )  . getValue (  )  )  ;", "}", "testCopyingExistingFiles ( fs ,    copyMapper ,    context )  ;", "for    ( Text   value    :    stubContext . getWriter (  )  . values (  )  )     {", "Assert . assertTrue (  (  ( value . toString (  )  )     +     \"    is   not   skipped \"  )  ,    value . toString (  )  . startsWith (  \" SKIP :  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopy"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . createSourceDataWithDifferentBlockSize (  )  ;", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "Configuration   configuration    =    context . getConfiguration (  )  ;", "EnumSet < DistCpOptions . FileAttribute >    fileAttributes    =    EnumSet . noneOf ( DistCpOptions . FileAttribute . class )  ;", "configuration . set ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  ,    DistCpUtils . packAttributes ( fileAttributes )  )  ;", "copyMapper . setup ( context )  ;", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "final   FileStatus   fileStatus    =    fs . getFileStatus ( path )  ;", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( fileStatus )  ,    context )  ;", "}", "Assert . fail (  \" Copy   should   have   failed   because   of   block - size   difference .  \"  )  ;", "}    catch    ( Exception   exception )     {", "Assert . assertTrue (  \" Failure   exception   should   have   suggested   the   use   of    - pb .  \"  ,    exception . getCause (  )  . getCause (  )  . getMessage (  )  . contains (  \" pb \"  )  )  ;", "Assert . assertTrue (  \" Failure   exception   should   have   suggested   the   use   of    - skipCrc .  \"  ,    exception . getCause (  )  . getCause (  )  . getMessage (  )  . contains (  \" skipCrc \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopyFailOnBlockSizeDifference"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", ". deleteState (  )  ;", ". createSourceData (  )  ;", "UserGroupInformation   tmpUser    =    UserGroupInformation . createRemoteUser (  \" guest \"  )  ;", "final   CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "final   Context   context    =    tmpUser . doAs ( new   PrivilegedAction < Context >  (  )     {", "@ Override", "public   Context   run (  )     {", "try    {", "StubContext   stubContext    =    new   StubContext ( getConfiguration (  )  ,    null ,     0  )  ;", "return   stubContext . getContext (  )  ;", "}    catch    (    e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new    < e > RuntimeException (  )  ;", "}", "}", "}  )  ;", ". touchFile (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ;", ". mkdirs (  . TARGET _ PATH )  ;", ". cluster . getFileSystem (  )  . setPermission ( new   Path (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ,    new   FsPermission ( FsAction . READ ,    FsAction . READ ,    FsAction . READ )  )  ;", ". cluster . getFileSystem (  )  . setPermission ( new   Path (  . TARGET _ PATH )  ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "final   FileSystem   tmpFS    =    tmpUser . doAs ( new   PrivilegedAction < FileSystem >  (  )     {", "@ Override", "public   FileSystem   run (  )     {", "try    {", "return   FileSystem . get (  . configuration )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "throw   new   RuntimeException (  \" Test   ought   to   fail   here \"  )  ;", "}", "}", "}  )  ;", "tmpUser . doAs ( new   PrivilegedAction < Integer >  (  )     {", "@ Override", "public   Integer   run (  )     {", "try    {", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   Text (  \"  / src / file \"  )  ,    new   CopyListingFileStatus ( tmpFS . getFileStatus ( new   Path (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  )  ,    context )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "return   null ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopyReadableFiles"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "final   FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "testCopy ( false )  ;", "TestCopyMapper . appendSourceData (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "context . getConfiguration (  )  . setBoolean ( DistCpOptionSwitch . APPEND . getConfigLabel (  )  ,    true )  ;", "copyMapper . setup ( context )  ;", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( TestCopyMapper . cluster . getFileSystem (  )  . getFileStatus ( path )  )  ,    context )  ;", "}", "verifyCopy ( fs ,    false )  ;", "Assert . assertEquals (  (  (  ( TestCopyMapper . nFiles )     *     ( TestCopyMapper . DEFAULT _ FILE _ SIZE )  )     *     2  )  ,    stubContext . getReporter (  )  . getCounter ( CopyMapper . Counter . BYTESCOPIED )  . getValue (  )  )  ;", "Assert . assertEquals ( TestCopyMapper . pathList . size (  )  ,    stubContext . getReporter (  )  . getCounter ( CopyMapper . Counter . COPY )  . getValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCopyWithAppend"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "testCopy ( true )  ;", "}", "METHOD_END"], "methodName": ["testCopyWithDifferentChecksumType"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( fs . getFileStatus ( path )  )  ,    context )  ;", "}", "Assert . assertEquals ( TestCopyMapper . nFiles ,    context . getCounter ( CopyMapper . Counter . SKIP )  . getValue (  )  )  ;", "}    catch    ( Exception   exception )     {", "Assert . assertTrue (  (  \" Caught   unexpected   exception :  \"     +     ( exception . getMessage (  )  )  )  ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["testCopyingExistingFiles"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", ". deleteState (  )  ;", ". createSourceData (  )  ;", "FileSystem   fs    =     . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext (  . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", ". mkdirs (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ;", ". touchFile (  (  (  . TARGET _ PATH )     +     \"  / src / file \"  )  )  ;", "try    {", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   Text (  \"  / src / file \"  )  ,    new   CopyListingFileStatus ( fs . getFileStatus ( new   Path (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  )  ,    context )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . startsWith (  \" Can ' t   replace \"  )  )  ;", "}", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDirToFile"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . createSourceData (  )  ;", "UserGroupInformation   tmpUser    =    UserGroupInformation . createRemoteUser (  \" guest \"  )  ;", "final   CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "final   StubContext   stubContext    =    tmpUser . doAs ( new   PrivilegedAction < StubContext >  (  )     {", "@ Override", "public   StubContext   run (  )     {", "try    {", "return   new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "}    catch    ( Exception   e )     {", "TestCopyMapper . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new   RuntimeException ( e )  ;", "}", "}", "}  )  ;", "EnumSet < DistCpOptions . FileAttribute >    preserveStatus    =    EnumSet . allOf ( DistCpOptions . FileAttribute . class )  ;", "preserveStatus . remove ( DistCpOptions . FileAttribute . ACL )  ;", "preserveStatus . remove ( DistCpOptions . FileAttribute . XATTR )  ;", "final   Context   context    =    stubContext . getContext (  )  ;", "context . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ PRESERVE _ STATUS ,    DistCpUtils . packAttributes ( preserveStatus )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ;", "OutputStream   out    =    TestCopyMapper . cluster . getFileSystem (  )  . create ( new   Path (  (  ( TestCopyMapper . TARGET _ PATH )     +     \"  / src / file \"  )  )  )  ;", "out . write (  \" hello   world \"  . getBytes (  )  )  ;", "out . close (  )  ;", "TestCopyMapper . cluster . getFileSystem (  )  . setPermission ( new   Path (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ,    new   fs . permission . FsPermission ( FsAction . READ ,    FsAction . READ ,    FsAction . READ )  )  ;", "TestCopyMapper . cluster . getFileSystem (  )  . setPermission ( new   Path (  (  ( TestCopyMapper . TARGET _ PATH )     +     \"  / src / file \"  )  )  ,    new   fs . permission . FsPermission ( FsAction . READ ,    FsAction . READ ,    FsAction . READ )  )  ;", "final   FileSystem   tmpFS    =    tmpUser . doAs ( new   PrivilegedAction < FileSystem >  (  )     {", "@ Override", "public   FileSystem   run (  )     {", "try    {", "return   FileSystem . get ( TestCopyMapper . configuration )  ;", "}    catch    ( IOException   e )     {", "TestCopyMapper . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "throw   new   RuntimeException (  \" Test   ought   to   fail   here \"  )  ;", "}", "}", "}  )  ;", "tmpUser . doAs ( new   PrivilegedAction < Integer >  (  )     {", "@ Override", "public   Integer   run (  )     {", "try    {", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   Text (  \"  / src / file \"  )  ,    new   CopyListingFileStatus ( tmpFS . getFileStatus ( new   Path (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  )  ,    context )  ;", "Assert . fail (  \" Didn ' t   expect   the   file   to   be   copied \"  )  ;", "}    catch    ( AccessControlException   ignore )     {", "}    catch    ( Exception   e )     {", "if    (  (  (  ( e . getCause (  )  )     =  =    null )     |  |     (  ( e . getCause (  )  . getCause (  )  )     =  =    null )  )     |  |     (  !  (  ( e . getCause (  )  . getCause (  )  )    instanceof   AccessControlException )  )  )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "return   null ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "TestCopyMapper . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFailCopyWithAccessControlException"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", ". deleteState (  )  ;", ". createSourceData (  )  ;", "FileSystem   fs    =     . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext (  . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", ". touchFile (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ;", ". mkdirs (  (  (  . TARGET _ PATH )     +     \"  / src / file \"  )  )  ;", "try    {", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   Text (  \"  / src / file \"  )  ,    new   CopyListingFileStatus ( fs . getFileStatus ( new   Path (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  )  ,    context )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . startsWith (  \" Can ' t   replace \"  )  )  ;", "}", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFileToDir"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "doTestIgnoreFailures ( true )  ;", "doTestIgnoreFailures ( false )  ;", "}", "METHOD_END"], "methodName": ["testIgnoreFailures"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . createSourceData (  )  ;", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "Configuration   configuration    =    context . getConfiguration (  )  ;", "String   workPath    =    new   Path (  \" webhdfs :  /  / localhost :  1  2  3  4  /  *  /  *  /  *  /  ?  /  \"  )  . makeQualified ( fs . getUri (  )  ,    fs . getWorkingDirectory (  )  )  . toString (  )  ;", "configuration . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,    workPath )  ;", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    TestCopyMapper . pathList . get (  0  )  )  )  ,    new   CopyListingFileStatus ( fs . getFileStatus ( TestCopyMapper . pathList . get (  0  )  )  )  ,    context )  ;", "Assert . assertTrue (  \" There   should   have   been   an   exception .  \"  ,    false )  ;", "}    catch    ( Exception   ignore )     {", "}", "}", "METHOD_END"], "methodName": ["testMakeDirFailure"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", ". deleteState (  )  ;", ". createSourceData (  )  ;", "UserGroupInformation   tmpUser    =    UserGroupInformation . createRemoteUser (  \" guest \"  )  ;", "final   CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "final   Context   context    =    tmpUser . doAs ( new   PrivilegedAction < Context >  (  )     {", "@ Override", "public   Context   run (  )     {", "try    {", "StubContext   stubContext    =    new   StubContext ( getConfiguration (  )  ,    null ,     0  )  ;", "return   stubContext . getContext (  )  ;", "}    catch    (    e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new    < e > RuntimeException (  )  ;", "}", "}", "}  )  ;", "EnumSet < DistCpOptions . FileAttribute >    preserveStatus    =    EnumSet . allOf ( DistCpOptions . FileAttribute . class )  ;", "preserveStatus . remove ( DistCpOptions . FileAttribute . ACL )  ;", "preserveStatus . remove ( DistCpOptions . FileAttribute . XATTR )  ;", "context . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ PRESERVE _ STATUS ,    DistCpUtils . packAttributes ( preserveStatus )  )  ;", ". touchFile (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ;", ". mkdirs (  . TARGET _ PATH )  ;", ". cluster . getFileSystem (  )  . setPermission ( new   Path (  . TARGET _ PATH )  ,    new   FsPermission (  (  ( short )     (  5  1  1  )  )  )  )  ;", "final   FileSystem   tmpFS    =    tmpUser . doAs ( new   PrivilegedAction < FileSystem >  (  )     {", "@ Override", "public   FileSystem   run (  )     {", "try    {", "return   FileSystem . get (  . configuration )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "throw   new   RuntimeException (  \" Test   ought   to   fail   here \"  )  ;", "}", "}", "}  )  ;", "tmpUser . doAs ( new   PrivilegedAction < Integer >  (  )     {", "@ Override", "public   Integer   run (  )     {", "try    {", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   Text (  \"  / src / file \"  )  ,    new   CopyListingFileStatus ( tmpFS . getFileStatus ( new   Path (  (  (  . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  )  ,    context )  ;", "Assert . fail (  \" Expected   copy   to   fail \"  )  ;", "}    catch    ( AccessControlException   e )     {", "Assert . assertTrue (  (  \" Got   exception :     \"     +     ( e . getMessage (  )  )  )  ,    true )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "return   null ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPreserve"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "testPreserveBlockSizeAndReplicationImpl ( true )  ;", "testPreserveBlockSizeAndReplicationImpl ( false )  ;", "}", "METHOD_END"], "methodName": ["testPreserveBlockSizeAndReplication"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . createSourceData (  )  ;", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "Configuration   configuration    =    context . getConfiguration (  )  ;", "EnumSet < DistCpOptions . FileAttribute >    fileAttributes    =    EnumSet . noneOf ( DistCpOptions . FileAttribute . class )  ;", "if    ( preserve )     {", "fileAttributes . add ( DistCpOptions . FileAttribute . BLOCKSIZE )  ;", "fileAttributes . add ( DistCpOptions . FileAttribute . REPLICATION )  ;", "}", "configuration . set ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  ,    DistCpUtils . packAttributes ( fileAttributes )  )  ;", "copyMapper . setup ( context )  ;", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "final   FileStatus   fileStatus    =    fs . getFileStatus ( path )  ;", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( fileStatus )  ,    context )  ;", "}", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "final   Path   targetPath    =    new   Path ( path . toString (  )  . replaceAll ( TestCopyMapper . SOURCE _ PATH ,    TestCopyMapper . TARGET _ PATH )  )  ;", "final   FileStatus   source    =    fs . getFileStatus ( path )  ;", "final   FileStatus   target    =    fs . getFileStatus ( targetPath )  ;", "if    (  !  ( source . isDirectory (  )  )  )     {", "Assert . assertTrue (  ( preserve    |  |     (  ( source . getBlockSize (  )  )     !  =     ( target . getBlockSize (  )  )  )  )  )  ;", "Assert . assertTrue (  ( preserve    |  |     (  ( source . getReplication (  )  )     !  =     ( target . getReplication (  )  )  )  )  )  ;", "Assert . assertTrue (  (  (  ! preserve )     |  |     (  ( source . getBlockSize (  )  )     =  =     ( target . getBlockSize (  )  )  )  )  )  ;", "Assert . assertTrue (  (  (  ! preserve )     |  |     (  ( source . getReplication (  )  )     =  =     ( target . getReplication (  )  )  )  )  )  ;", "}", "}", "}    catch    ( Exception   e )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +     ( e . getMessage (  )  )  )  ,    false )  ;", "e . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPreserveBlockSizeAndReplicationImpl"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "testPreserveUserGroupImpl ( true )  ;", "testPreserveUserGroupImpl ( false )  ;", "}", "METHOD_END"], "methodName": ["testPreserveUserGroup"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . createSourceData (  )  ;", "TestCopyMapper . changeUserGroup (  \" Michael \"  ,     \" Corleone \"  )  ;", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "Configuration   configuration    =    context . getConfiguration (  )  ;", "EnumSet < DistCpOptions . FileAttribute >    fileAttributes    =    EnumSet . noneOf ( DistCpOptions . FileAttribute . class )  ;", "if    ( preserve )     {", "fileAttributes . add ( DistCpOptions . FileAttribute . USER )  ;", "fileAttributes . add ( DistCpOptions . FileAttribute . GROUP )  ;", "fileAttributes . add ( DistCpOptions . FileAttribute . PERMISSION )  ;", "}", "configuration . set ( DistCpOptionSwitch . PRESERVE _ STATUS . getConfigLabel (  )  ,    DistCpUtils . packAttributes ( fileAttributes )  )  ;", "copyMapper . setup ( context )  ;", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "final   FileStatus   fileStatus    =    fs . getFileStatus ( path )  ;", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    path )  )  ,    new   CopyListingFileStatus ( fileStatus )  ,    context )  ;", "}", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "final   Path   targetPath    =    new   Path ( path . toString (  )  . replaceAll ( TestCopyMapper . SOURCE _ PATH ,    TestCopyMapper . TARGET _ PATH )  )  ;", "final   FileStatus   source    =    fs . getFileStatus ( path )  ;", "final   FileStatus   target    =    fs . getFileStatus ( targetPath )  ;", "if    (  !  ( source . isDirectory (  )  )  )     {", "Assert . assertTrue (  (  (  ! preserve )     |  |     ( source . getOwner (  )  . equals ( target . getOwner (  )  )  )  )  )  ;", "Assert . assertTrue (  (  (  ! preserve )     |  |     ( source . getGroup (  )  . equals ( target . getGroup (  )  )  )  )  )  ;", "Assert . assertTrue (  (  (  ! preserve )     |  |     ( source . getPermission (  )  . equals ( target . getPermission (  )  )  )  )  )  ;", "Assert . assertTrue (  ( preserve    |  |     (  !  ( source . getOwner (  )  . equals ( target . getOwner (  )  )  )  )  )  )  ;", "Assert . assertTrue (  ( preserve    |  |     (  !  ( source . getGroup (  )  . equals ( target . getGroup (  )  )  )  )  )  )  ;", "Assert . assertTrue (  ( preserve    |  |     (  !  ( source . getPermission (  )  . equals ( target . getPermission (  )  )  )  )  )  )  ;", "Assert . assertTrue (  (  ( source . isDirectory (  )  )     |  |     (  ( source . getReplication (  )  )     !  =     ( target . getReplication (  )  )  )  )  )  ;", "}", "}", "}    catch    ( Exception   e )     {", "Assert . assertTrue (  (  \" Unexpected   exception :     \"     +     ( e . getMessage (  )  )  )  ,    false )  ;", "e . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPreserveUserGroupImpl"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "testCopy ( false )  ;", "}", "METHOD_END"], "methodName": ["testRun"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  /  1  \"  )  )  ;", "Path   sourceFilePath    =    TestCopyMapper . pathList . get (  0  )  ;", "Path   targetFilePath    =    new   Path ( sourceFilePath . toString (  )  . replaceAll ( TestCopyMapper . SOURCE _ PATH ,    TestCopyMapper . TARGET _ PATH )  )  ;", "TestCopyMapper . touchFile ( targetFilePath . toString (  )  )  ;", "FileSystem   fs    =    TestCopyMapper . cluster . getFileSystem (  )  ;", "CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "StubContext   stubContext    =    new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "Context   context    =    stubContext . getContext (  )  ;", "context . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    targetFilePath . getParent (  )  . toString (  )  )  ;", "copyMapper . setup ( context )  ;", "final   CopyListingFileStatus   sourceFileStatus    =    new   CopyListingFileStatus ( fs . getFileStatus ( sourceFilePath )  )  ;", "long   before    =    fs . getFileStatus ( targetFilePath )  . getModificationTime (  )  ;", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    sourceFilePath )  )  ,    sourceFileStatus ,    context )  ;", "long   after    =    fs . getFileStatus ( targetFilePath )  . getModificationTime (  )  ;", "Assert . assertTrue (  \" File   should   have   been   skipped \"  ,     ( before    =  =    after )  )  ;", "context . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,    targetFilePath . toString (  )  )  ;", "copyMapper . setup ( context )  ;", "before    =    fs . getFileStatus ( targetFilePath )  . getModificationTime (  )  ;", "try    {", "Thread . sleep (  2  )  ;", "}    catch    ( Throwable   ignore )     {", "}", "copyMapper . map ( new   io . Text ( DistCpUtils . getRelativePath ( new   Path ( TestCopyMapper . SOURCE _ PATH )  ,    sourceFilePath )  )  ,    sourceFileStatus ,    context )  ;", "after    =    fs . getFileStatus ( targetFilePath )  . getModificationTime (  )  ;", "Assert . assertTrue (  \" File   should   have   been   overwritten .  \"  ,     ( before    <    after )  )  ;", "}    catch    ( Exception   exception )     {", "Assert . fail (  (  \" Unexpected   exception :     \"     +     ( exception . getMessage (  )  )  )  )  ;", "exception . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleFileCopy"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestCopyMapper . deleteState (  )  ;", "TestCopyMapper . createSourceData (  )  ;", "UserGroupInformation   tmpUser    =    UserGroupInformation . createRemoteUser (  \" guest \"  )  ;", "final   CopyMapper   copyMapper    =    new   CopyMapper (  )  ;", "final   StubContext   stubContext    =    tmpUser . doAs ( new   PrivilegedAction < StubContext >  (  )     {", "@ Override", "public   StubContext   run (  )     {", "try    {", "return   new   StubContext ( TestCopyMapper . getConfiguration (  )  ,    null ,     0  )  ;", "}    catch    ( Exception   e )     {", "TestCopyMapper . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "throw   new   RuntimeException ( e )  ;", "}", "}", "}  )  ;", "final   Context   context    =    stubContext . getContext (  )  ;", "EnumSet < DistCpOptions . FileAttribute >    preserveStatus    =    EnumSet . allOf ( DistCpOptions . FileAttribute . class )  ;", "preserveStatus . remove ( DistCpOptions . FileAttribute . ACL )  ;", "preserveStatus . remove ( DistCpOptions . FileAttribute . XATTR )  ;", "context . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ PRESERVE _ STATUS ,    DistCpUtils . packAttributes ( preserveStatus )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ;", "TestCopyMapper . touchFile (  (  ( TestCopyMapper . TARGET _ PATH )     +     \"  / src / file \"  )  )  ;", "TestCopyMapper . cluster . getFileSystem (  )  . setPermission ( new   Path (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  ,    new   fs . permission . FsPermission ( FsAction . READ ,    FsAction . READ ,    FsAction . READ )  )  ;", "TestCopyMapper . cluster . getFileSystem (  )  . setPermission ( new   Path (  (  ( TestCopyMapper . TARGET _ PATH )     +     \"  / src / file \"  )  )  ,    new   fs . permission . FsPermission ( FsAction . READ ,    FsAction . READ ,    FsAction . READ )  )  ;", "final   FileSystem   tmpFS    =    tmpUser . doAs ( new   PrivilegedAction < FileSystem >  (  )     {", "@ Override", "public   FileSystem   run (  )     {", "try    {", "return   FileSystem . get ( TestCopyMapper . configuration )  ;", "}    catch    ( IOException   e )     {", "TestCopyMapper . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "throw   new   RuntimeException (  \" Test   ought   to   fail   here \"  )  ;", "}", "}", "}  )  ;", "tmpUser . doAs ( new   PrivilegedAction < Integer >  (  )     {", "@ Override", "public   Integer   run (  )     {", "try    {", "copyMapper . setup ( context )  ;", "copyMapper . map ( new   Text (  \"  / src / file \"  )  ,    new   CopyListingFileStatus ( tmpFS . getFileStatus ( new   Path (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  )  ,    context )  ;", "Assert . assertEquals ( stubContext . getWriter (  )  . values (  )  . size (  )  ,     1  )  ;", "Assert . assertTrue ( stubContext . getWriter (  )  . values (  )  . get (  0  )  . toString (  )  . startsWith (  \" SKIP \"  )  )  ;", "Assert . assertTrue ( stubContext . getWriter (  )  . values (  )  . get (  0  )  . toString (  )  . contains (  (  ( TestCopyMapper . SOURCE _ PATH )     +     \"  / src / file \"  )  )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "return   null ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "TestCopyMapper . LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  (  \" Test   failed :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSkipCopyNoPerms"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . touchFile ( path ,    false ,    null )  ;", "}", "METHOD_END"], "methodName": ["touchFile"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs ;", "DataOutputStream   outputStream    =    null ;", "try    {", "fs    =     . cluster . getFileSystem (  )  ;", "final   Path   qualifiedPath    =    new   Path ( path )  . makeQualified ( fs . getUri (  )  ,    fs . getWorkingDirectory (  )  )  ;", "final   long   blockSize    =     ( createMultipleBlocks )     ?     . NON _ DEFAULT _ BLOCK _ SIZE    :     ( fs . getDefaultBlockSize ( qualifiedPath )  )     *     2  ;", "FsPermission   permission    =    FsPermission . getFileDefault (  )  . applyUMask ( FsPermission . getUMask ( fs . getConf (  )  )  )  ;", "outputStream    =    fs . create ( qualifiedPath ,    permission ,    EnumSet . of ( CREATE ,    OVERWRITE )  ,     0  ,     (  ( short )     (  ( fs . getDefaultReplication ( qualifiedPath )  )     *     2  )  )  ,    blockSize ,    null ,    checksumOpt )  ;", "byte [  ]    bytes    =    new   byte [  . DEFAULT _ FILE _ SIZE ]  ;", "outputStream . write ( bytes )  ;", "long   fileSize    =     . DEFAULT _ FILE _ SIZE ;", "if    ( createMultipleBlocks )     {", "while    ( fileSize    <     (  2     *    blockSize )  )     {", "outputStream . write ( bytes )  ;", "outputStream . flush (  )  ;", "fileSize    +  =     . DEFAULT _ FILE _ SIZE ;", "}", "}", ". pathList . add ( qualifiedPath )  ;", "+  +  (  . nFiles )  ;", "FileStatus   fileStatus    =    fs . getFileStatus ( qualifiedPath )  ;", "System . out . println ( fileStatus . getBlockSize (  )  )  ;", "System . out . println ( fileStatus . getReplication (  )  )  ;", "}    finally    {", "IOUtils . cleanup ( null ,    outputStream )  ;", "}", "}", "METHOD_END"], "methodName": ["touchFile"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "TestCopyMapper . touchFile ( path ,    true ,    checksumOpt )  ;", "}", "METHOD_END"], "methodName": ["touchFile"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "for    ( Path   path    :    TestCopyMapper . pathList )     {", "final   Path   targetPath    =    new   Path ( path . toString (  )  . replaceAll ( TestCopyMapper . SOURCE _ PATH ,    TestCopyMapper . TARGET _ PATH )  )  ;", "Assert . assertTrue ( fs . exists ( targetPath )  )  ;", "Assert . assertTrue (  (  ( fs . isFile ( targetPath )  )     =  =     ( fs . isFile ( path )  )  )  )  ;", "FileStatus   sourceStatus    =    fs . getFileStatus ( path )  ;", "FileStatus   targetStatus    =    fs . getFileStatus ( targetPath )  ;", "Assert . assertEquals ( sourceStatus . getReplication (  )  ,    targetStatus . getReplication (  )  )  ;", "if    ( preserveChecksum )     {", "Assert . assertEquals ( sourceStatus . getBlockSize (  )  ,    targetStatus . getBlockSize (  )  )  ;", "}", "Assert . assertTrue (  (  (  !  ( fs . isFile ( targetPath )  )  )     |  |     ( fs . getFileChecksum ( targetPath )  . equals ( fs . getFileChecksum ( path )  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyCopy"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyMapper"}, {"methodBody": ["METHOD_START", "{", "try    {", "OutputFormat   outputFormat    =    new   CopyOutputFormat (  )  ;", "Job   job    =    Job . getInstance ( new   Configuration (  )  )  ;", "JobID   jobID    =    new   JobID (  \"  2  0  0  7  0  7  1  2  1  7  3  3  \"  ,     1  )  ;", "try    {", "JobContext   context    =    new   JobContextImpl ( job . getConfiguration (  )  ,    jobID )  ;", "outputFormat . checkOutputSpecs ( context )  ;", "Assert . fail (  \" No   checking   for   invalid   work / commit   path \"  )  ;", "}    catch    ( IllegalStateException   ignore )     {", "}", "CopyOutputFormat . setWorkingDirectory ( job ,    new   Path (  \"  / tmp / work \"  )  )  ;", "try    {", "JobContext   context    =    new   JobContextImpl ( job . getConfiguration (  )  ,    jobID )  ;", "outputFormat . checkOutputSpecs ( context )  ;", "Assert . fail (  \" No   checking   for   invalid   commit   path \"  )  ;", "}    catch    ( IllegalStateException   ignore )     {", "}", "job . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,     \"  \"  )  ;", "CopyOutputFormat . setCommitDirectory ( job ,    new   Path (  \"  / tmp / commit \"  )  )  ;", "try    {", "JobContext   context    =    new   JobContextImpl ( job . getConfiguration (  )  ,    jobID )  ;", "outputFormat . checkOutputSpecs ( context )  ;", "Assert . fail (  \" No   checking   for   invalid   work   path \"  )  ;", "}    catch    ( IllegalStateException   ignore )     {", "}", "CopyOutputFormat . setWorkingDirectory ( job ,    new   Path (  \"  / tmp / work \"  )  )  ;", "CopyOutputFormat . setCommitDirectory ( job ,    new   Path (  \"  / tmp / commit \"  )  )  ;", "try    {", "JobContext   context    =    new   JobContextImpl ( job . getConfiguration (  )  ,    jobID )  ;", "outputFormat . checkOutputSpecs ( context )  ;", "}    catch    ( IllegalStateException   ignore )     {", "Assert . fail (  \" Output   spec   check   failed .  \"  )  ;", "}", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   checkoutput   specs \"  ,    e )  ;", "Assert . fail (  \" Checkoutput   Spec   failure \"  )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . error (  \" Exception   encountered   while   testing   checkoutput   specs \"  ,    e )  ;", "Assert . fail (  \" Checkoutput   Spec   failure \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCheckOutputSpecs"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "try    {", "TaskAttemptContext   context    =    new   TaskAttemptContextImpl ( new   Configuration (  )  ,    new   TaskAttemptID (  \"  2  0  0  7  0  7  1  2  1  7  3  3  \"  ,     1  ,    TaskType . MAP ,     1  ,     1  )  )  ;", "context . getConfiguration (  )  . set (  \" mapred . output . dir \"  ,     \"  / out \"  )  ;", "Assert . assertTrue (  (  ( new   CopyOutputFormat (  )  . getOutputCommitter ( context )  )    instanceof   CopyCommitter )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  \" Unable   to   get   output   committer \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetOutputCommitter"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "try    {", "Job   job    =    Job . getInstance ( new   Configuration (  )  )  ;", "Assert . assertEquals ( null ,    CopyOutputFormat . getCommitDirectory ( job )  )  ;", "job . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH ,     \"  \"  )  ;", "Assert . assertEquals ( null ,    CopyOutputFormat . getCommitDirectory ( job )  )  ;", "Path   directory    =    new   Path (  \"  / tmp / test \"  )  ;", "CopyOutputFormat . setCommitDirectory ( job ,    directory )  ;", "Assert . assertEquals ( directory ,    CopyOutputFormat . getCommitDirectory ( job )  )  ;", "Assert . assertEquals ( directory . toString (  )  ,    job . getConfiguration (  )  . get ( DistCpConstants . CONF _ LABEL _ TARGET _ FINAL _ PATH )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   test \"  ,    e )  ;", "Assert . fail (  \" Failed   while   testing   for   set   Commit   Directory \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSetCommitDirectory"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "try    {", "Job   job    =    Job . getInstance ( new   Configuration (  )  )  ;", "Assert . assertEquals ( null ,    CopyOutputFormat . getWorkingDirectory ( job )  )  ;", "job . getConfiguration (  )  . set ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH ,     \"  \"  )  ;", "Assert . assertEquals ( null ,    CopyOutputFormat . getWorkingDirectory ( job )  )  ;", "Path   directory    =    new   Path (  \"  / tmp / test \"  )  ;", "CopyOutputFormat . setWorkingDirectory ( job ,    directory )  ;", "Assert . assertEquals ( directory ,    CopyOutputFormat . getWorkingDirectory ( job )  )  ;", "Assert . assertEquals ( directory . toString (  )  ,    job . getConfiguration (  )  . get ( DistCpConstants . CONF _ LABEL _ TARGET _ WORK _ PATH )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered   while   running   test \"  ,    e )  ;", "Assert . fail (  \" Failed   while   testing   for   set   Working   Directory \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSetWorkingDirectory"], "fileName": "org.apache.hadoop.tools.mapred.TestCopyOutputFormat"}, {"methodBody": ["METHOD_START", "{", "Mapper . Context   context    =    mock ( Context . class )  ;", "doReturn ( new   Configuration (  )  )  . when ( context )  . getConfiguration (  )  ;", "Exception   expectedEx    =    new   IOException (  \" boom \"  )  ;", "OutputStream   out    =    mock ( OutputStream . class )  ;", "doThrow ( expectedEx )  . when ( out )  . close (  )  ;", "File   f    =    File . createTempFile ( this . getClass (  )  . getSimpleName (  )  ,    null )  ;", "f . deleteOnExit (  )  ;", "FileStatus   stat    =    new   FileStatus (  1 L ,    false ,     1  ,     1  0  2  4  ,     0  ,    new   Path ( f . toURI (  )  )  )  ;", "Exception   actualEx    =    null ;", "try    {", "new    (  \" testFailOnCloseError \"  ,    CopyMapper . FileAction . OVERWRITE )  . copyBytes ( stat ,     0  ,    out ,     5  1  2  ,    context )  ;", "}    catch    ( Exception   e )     {", "actualEx    =    e ;", "}", "assertNotNull (  \" close   didn ' t   fail \"  ,    actualEx )  ;", "assertEquals ( expectedEx ,    actualEx )  ;", "}", "METHOD_END"], "methodName": ["testFailOnCloseError"], "fileName": "org.apache.hadoop.tools.mapred.TestRetriableFileCopyCommand"}, {"methodBody": ["METHOD_START", "{", "long   lastEnd    =     0  ;", "for    ( InputSplit   split    :    splits )     {", "FileSplit   fileSplit    =     (  ( FileSplit )     ( split )  )  ;", "long   start    =    fileSplit . getStart (  )  ;", "Assert . assertEquals ( lastEnd ,    start )  ;", "lastEnd    =    start    +     ( fileSplit . getLength (  )  )  ;", "}", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader (  . cluster . getFileSystem (  )  . getConf (  )  ,    Reader . file ( listFile )  )  ;", "try    {", "reader . seek ( lastEnd )  ;", "CopyListingFileStatus   srcFileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   srcRelPath    =    new   Text (  )  ;", "Assert . assertFalse ( reader . next ( srcRelPath ,    srcFileStatus )  )  ;", "}    finally    {", "IOUtils . closeStream ( reader )  ;", "}", "}", "METHOD_END"], "methodName": ["checkSplits"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fileSystem    =    null ;", "DataOutputStream   outputStream    =    null ;", "try    {", "fileSystem    =     . cluster . getFileSystem (  )  ;", "outputStream    =    fileSystem . create ( new   Path ( path )  ,    true ,     0  )  ;", "int   size    =     (  ( int )     ( Math . ceil (  ( fileSize    +     (  (  1     -     (  . random . nextFloat (  )  )  )     *    fileSize )  )  )  )  )  ;", "outputStream . write ( new   byte [ size ]  )  ;", "return   size ;", "}    finally    {", "IOUtils . cleanup ( null ,    fileSystem ,    outputStream )  ;", "}", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "Path   sourcePath    =    new   Path (  (  ( TestUniformSizeInputFormat . cluster . getFileSystem (  )  . getUri (  )  . toString (  )  )     +     \"  / tmp / source \"  )  )  ;", "Path   targetPath    =    new   Path (  (  ( TestUniformSizeInputFormat . cluster . getFileSystem (  )  . getUri (  )  . toString (  )  )     +     \"  / tmp / target \"  )  )  ;", "List < Path >    sourceList    =    new   ArrayList < Path >  (  )  ;", "sourceList . add ( sourcePath )  ;", "final   DistCpOptions   distCpOptions    =    new   DistCpOptions ( sourceList ,    targetPath )  ;", "distCpOptions . setMaxMaps ( nMaps )  ;", "return   distCpOptions ;", "}", "METHOD_END"], "methodName": ["getOptions"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "TestUniformSizeInputFormat . cluster    =    new   Builder ( new   Configuration (  )  )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "TestUniformSizeInputFormat . totalFileSize    =     0  ;", "for    ( int   i    =     0  ;    i    <     ( TestUniformSizeInputFormat . N _ FILES )  ;     +  + i )", "TestUniformSizeInputFormat . totalFileSize    +  =    TestUniformSizeInputFormat . createFile (  (  \"  / tmp / source /  \"     +     ( String . valueOf ( i )  )  )  ,    TestUniformSizeInputFormat . SIZEOF _ EACH _ FILE )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "TestUniformSizeInputFormat . cluster . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "testGetSplits (  9  )  ;", "for    ( int   i    =     1  ;    i    <     (  . N _ FILES )  ;     +  + i )", "testGetSplits ( i )  ;", "}", "METHOD_END"], "methodName": ["testGetSplits"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    TestUniformSizeInputFormat . getOptions ( nMaps )  ;", "Configuration   configuration    =    new   Configuration (  )  ;", "configuration . set (  \" mapred . map . tasks \"  ,    String . valueOf ( options . getMaxMaps (  )  )  )  ;", "Path   listFile    =    new   Path (  (  ( TestUniformSizeInputFormat . cluster . getFileSystem (  )  . getUri (  )  . toString (  )  )     +     \"  / tmp / testGetSplits _  1  / fileList . seq \"  )  )  ;", "CopyListing . getCopyListing ( configuration ,    TestUniformSizeInputFormat . CREDENTIALS ,    options )  . buildListing ( listFile ,    options )  ;", "JobContext   jobContext    =    new   JobContextImpl ( configuration ,    new   JobID (  )  )  ;", "UniformSizeInputFormat   uniformSizeInputFormat    =    new   UniformSizeInputFormat (  )  ;", "List < InputSplit >    splits    =    uniformSizeInputFormat . getSplits ( jobContext )  ;", "int   sizePerMap    =     ( TestUniformSizeInputFormat . totalFileSize )     /    nMaps ;", "checkSplits ( listFile ,    splits )  ;", "int   doubleCheckedTotalSize    =     0  ;", "int   previousSplitSize    =     -  1  ;", "for    ( int   i    =     0  ;    i    <     ( splits . size (  )  )  ;     +  + i )     {", "InputSplit   split    =    splits . get ( i )  ;", "int   currentSplitSize    =     0  ;", "RecordReader < Text ,    CopyListingFileStatus >    recordReader    =    uniformSizeInputFormat . createRecordReader ( split ,    null )  ;", "StubContext   stubContext    =    new   StubContext ( jobContext . getConfiguration (  )  ,    recordReader ,     0  )  ;", "final   TaskAttemptContext   taskAttemptContext    =    stubContext . getContext (  )  ;", "recordReader . initialize ( split ,    taskAttemptContext )  ;", "while    ( recordReader . nextKeyValue (  )  )     {", "Path   sourcePath    =    recordReader . getCurrentValue (  )  . getPath (  )  ;", "FileSystem   fs    =    sourcePath . getFileSystem ( configuration )  ;", "FileStatus [  ]    fileStatus    =    fs . listStatus ( sourcePath )  ;", "if    (  ( fileStatus . length )     >     1  )     {", "continue ;", "}", "currentSplitSize    +  =    fileStatus [  0  ]  . getLen (  )  ;", "}", "Assert . assertTrue (  (  (  ( previousSplitSize    =  =     (  -  1  )  )     |  |     (  ( Math . abs (  ( currentSplitSize    -    previousSplitSize )  )  )     <     (  0  .  1     *    sizePerMap )  )  )     |  |     ( i    =  =     (  ( splits . size (  )  )     -     1  )  )  )  )  ;", "doubleCheckedTotalSize    +  =    currentSplitSize ;", "}", "Assert . assertEquals ( TestUniformSizeInputFormat . totalFileSize ,    doubleCheckedTotalSize )  ;", "}", "METHOD_END"], "methodName": ["testGetSplits"], "fileName": "org.apache.hadoop.tools.mapred.TestUniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "final   String   listingFilePathString    =    configuration . get ( DistCpConstants . CONF _ LABEL _ LISTING _ FILE _ PATH ,     \"  \"  )  ;", "assert    !  ( listingFilePathString . equals (  \"  \"  )  )     :     \" Couldn ' t   find   listing   file .    Invalid   i .  \"  ;", "return   new   Path ( listingFilePathString )  ;", "}", "METHOD_END"], "methodName": ["getListingFilePath"], "fileName": "org.apache.hadoop.tools.mapred.UniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "final   Path   listingFilePath    =    UniformSizeInputFormat . getListingFilePath ( configuration )  ;", "try    {", "final   FileSystem   fileSystem    =    listingFilePath . getFileSystem ( configuration )  ;", "if    (  !  ( fileSystem . exists ( listingFilePath )  )  )", "throw   new   IllegalArgumentException (  (  \" Listing   file   doesn ' t   exist   at :     \"     +    listingFilePath )  )  ;", "return   new   Reader ( configuration ,    Reader . file ( listingFilePath )  )  ;", "}    catch    ( IOException   exception )     {", "UniformSizeInputFormat . LOG . error (  (  \" Couldn ' t   find   listing   file   at :     \"     +    listingFilePath )  ,    exception )  ;", "throw   new   IllegalArgumentException (  (  \" Couldn ' t   find   listing - file   at :     \"     +    listingFilePath )  ,    exception )  ;", "}", "}", "METHOD_END"], "methodName": ["getListingFileReader"], "fileName": "org.apache.hadoop.tools.mapred.UniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "List < InputSplit >    splits    =    new   ArrayList < InputSplit >  ( numSplits )  ;", "long   nBytesPerSplit    =     (  ( long )     ( Math . ceil (  (  ( totalSizeBytes    *     1  .  0  )     /    numSplits )  )  )  )  ;", "CopyListingFileStatus   srcFileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   srcRelPath    =    new   Text (  )  ;", "long   currentSplitSize    =     0  ;", "long   lastSplitStart    =     0  ;", "long   lastPosition    =     0  ;", "final   Path   listingFilePath    =     . getListingFilePath ( configuration )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  \" Average   bytes   per   map :     \"     +    nBytesPerSplit )     +     \"  ,    Number   of   maps :     \"  )     +    numSplits )     +     \"  ,    total   size :     \"  )     +    totalSizeBytes )  )  ;", "}", "SequenceFile . Reader   reader    =    null ;", "try    {", "reader    =    getListingFileReader ( configuration )  ;", "while    ( reader . next ( srcRelPath ,    srcFileStatus )  )     {", "if    (  (  ( currentSplitSize    +     ( srcFileStatus . getLen (  )  )  )     >    nBytesPerSplit )     &  &     ( lastPosition    !  =     0  )  )     {", "FileSplit   split    =    new   FileSplit ( listingFilePath ,    lastSplitStart ,     ( lastPosition    -    lastSplitStart )  ,    null )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Creating   split    :     \"     +    split )     +     \"  ,    bytes   in   split :     \"  )     +    currentSplitSize )  )  ;", "}", "splits . add ( split )  ;", "lastSplitStart    =    lastPosition ;", "currentSplitSize    =     0  ;", "}", "currentSplitSize    +  =    srcFileStatus . getLen (  )  ;", "lastPosition    =    reader . getPosition (  )  ;", "}", "if    ( lastPosition    >    lastSplitStart )     {", "FileSplit   split    =    new   FileSplit ( listingFilePath ,    lastSplitStart ,     ( lastPosition    -    lastSplitStart )  ,    null )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . info (  (  (  (  \" Creating   split    :     \"     +    split )     +     \"  ,    bytes   in   split :     \"  )     +    currentSplitSize )  )  ;", "}", "splits . add ( split )  ;", "}", "}    finally    {", "IOUtils . closeStream ( reader )  ;", "}", "return   splits ;", "}", "METHOD_END"], "methodName": ["getSplits"], "fileName": "org.apache.hadoop.tools.mapred.UniformSizeInputFormat"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( DynamicInputChunk . areInvariantsInitialized (  )  )  )", "DynamicInputChunk . initializeChunkInvariants ( taskAttemptContext . getConfiguration (  )  )  ;", "String   taskId    =    taskAttemptContext . getTaskAttemptID (  )  . getTaskID (  )  . toString (  )  ;", "Path   acquiredFilePath    =    new   Path ( DynamicInputChunk . chunkRootPath ,    taskId )  ;", "if    ( DynamicInputChunk . fs . exists ( acquiredFilePath )  )     {", "DynamicInputChunk . LOG . info (  (  \" Acquiring   pre - assigned   chunk :     \"     +    acquiredFilePath )  )  ;", "return   new   DynamicInputChunk ( acquiredFilePath ,    taskAttemptContext )  ;", "}", "for    ( FileStatus   chunkFile    :    DynamicInputChunk . getListOfChunkFiles (  )  )     {", "if    ( DynamicInputChunk . fs . rename ( chunkFile . getPath (  )  ,    acquiredFilePath )  )     {", "DynamicInputChunk . LOG . info (  (  ( taskId    +     \"    acquired    \"  )     +     ( chunkFile . getPath (  )  )  )  )  ;", "return   new   DynamicInputChunk ( acquiredFilePath ,    taskAttemptContext )  ;", "} else", "DynamicInputChunk . LOG . warn (  (  ( taskId    +     \"    could   not   acquire    \"  )     +     ( chunkFile . getPath (  )  )  )  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["acquire"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "return    ( DynamicInputChunk . chunkRootPath )     !  =    null ;", "}", "METHOD_END"], "methodName": ["areInvariantsInitialized"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "Path   newPath    =    new   Path ( DynamicInputChunk . chunkRootPath ,    taskId . toString (  )  )  ;", "if    (  !  ( DynamicInputChunk . fs . rename ( chunkFilePath ,    newPath )  )  )     {", "DynamicInputChunk . LOG . warn (  (  (  ( chunkFilePath )     +     \"    could   not   be   assigned   to    \"  )     +    taskId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["assignTo"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "IOUtils . cleanup ( DynamicInputChunk . LOG ,    reader ,    writer )  ;", "}", "METHOD_END"], "methodName": ["close"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "return   new   DynamicInputChunk ( chunkId ,    configuration )  ;", "}", "METHOD_END"], "methodName": ["createChunkForWrite"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "Path   chunkFilePattern    =    new   Path ( DynamicInputChunk . chunkRootPath ,     (  ( DynamicInputChunk . chunkFilePrefix )     +     \"  *  \"  )  )  ;", "FileStatus [  ]    chunkFiles    =    DynamicInputChunk . fs . globStatus ( chunkFilePattern )  ;", "DynamicInputChunk . numChunksLeft    =    chunkFiles . length ;", "return   chunkFiles ;", "}", "METHOD_END"], "methodName": ["getListOfChunkFiles"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "final   String   listingFileString    =    configuration . get ( DistCpConstants . CONF _ LABEL _ LISTING _ FILE _ PATH ,     \"  \"  )  ;", "assert    !  ( listingFileString . equals (  \"  \"  )  )     :     \" Listing   file   not   found .  \"  ;", "return   listingFileString ;", "}", "METHOD_END"], "methodName": ["getListingFilePath"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "return   DynamicInputChunk . numChunksLeft ;", "}", "METHOD_END"], "methodName": ["getNumChunksLeft"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "return   chunkFilePath ;", "}", "METHOD_END"], "methodName": ["getPath"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "assert    ( reader )     !  =    null    :     \" Reader   un - initialized !  \"  ;", "return   reader ;", "}", "METHOD_END"], "methodName": ["getReader"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "DynamicInputChunk . configuration    =    config ;", "Path   listingFilePath    =    new   Path ( DynamicInputChunk . getListingFilePath ( DynamicInputChunk . configuration )  )  ;", "DynamicInputChunk . chunkRootPath    =    new   Path ( listingFilePath . getParent (  )  ,     \" chunkDir \"  )  ;", "DynamicInputChunk . fs    =    DynamicInputChunk . chunkRootPath . getFileSystem ( DynamicInputChunk . configuration )  ;", "DynamicInputChunk . chunkFilePrefix    =     ( listingFilePath . getName (  )  )     +     \"  . chunk .  \"  ;", "}", "METHOD_END"], "methodName": ["initializeChunkInvariants"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "reader    =    new   SequenceFileRecordReader < K ,    V >  (  )  ;", "reader . initialize ( new   FileSplit ( chunkFilePath ,     0  ,    DistCpUtils . getFileSize ( chunkFilePath ,    DynamicInputChunk . configuration )  ,    null )  ,    taskAttemptContext )  ;", "}", "METHOD_END"], "methodName": ["openForRead"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "writer    =    SequenceFile . createWriter ( chunkFilePath . getFileSystem ( DynamicInputChunk . configuration )  ,    DynamicInputChunk . configuration ,    chunkFilePath ,    Text . class ,    CopyListingFileStatus . class ,    NONE )  ;", "}", "METHOD_END"], "methodName": ["openForWrite"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "close (  )  ;", "if    (  !  (  . fs . delete ( chunkFilePath ,    false )  )  )     {", ". LOG . error (  (  \" Unable   to   release   chunk   at   path :     \"     +     ( chunkFilePath )  )  )  ;", "throw   new   IOException (  (  \" Unable   to   release   chunk   at   path :     \"     +     ( chunkFilePath )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["release"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "writer . append ( key ,    value )  ;", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputChunk"}, {"methodBody": ["METHOD_START", "{", "for    ( DynamicInputChunk   chunk    :    chunks )", "chunk . close (  )  ;", "}", "METHOD_END"], "methodName": ["closeAll"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   DynamicInputChunk . createChunkForWrite ( String . format (  \"  %  0  5 d \"  ,    chunkId )  ,    config )  ;", "}", "METHOD_END"], "methodName": ["createChunk"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "List < DynamicInputChunk >    chunks    =    new   ArrayList < DynamicInputChunk >  (  )  ;", "int   chunkIdUpperBound    =    Math . min ( nChunksTotal ,     ( chunkCount    +    nChunksOpenAtOnce )  )  ;", "if    (  ( nChunksTotal    -    chunkIdUpperBound )     <    nChunksOpenAtOnce )", "chunkIdUpperBound    =    nChunksTotal ;", "for    ( int   i    =    chunkCount ;    i    <    chunkIdUpperBound ;     +  + i )", "chunks . add (  . createChunk ( i ,    config )  )  ;", "return   chunks ;", "}", "METHOD_END"], "methodName": ["createChunks"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "int   numMaps    =    DynamicInputFormat . getNumMapTasks ( jobContext . getConfiguration (  )  )  ;", "final   int   nSplits    =    Math . min ( numMaps ,    chunks . size (  )  )  ;", "List < InputSplit >    splits    =    new   ArrayList < InputSplit >  ( nSplits )  ;", "for    ( int   i    =     0  ;    i    <    nSplits ;     +  + i )     {", "TaskID   taskId    =    new   TaskID ( jobContext . getJobID (  )  ,    TaskType . MAP ,    i )  ;", "chunks . get ( i )  . assignTo ( taskId )  ;", "splits . add ( new   FileSplit ( chunks . get ( i )  . getPath (  )  ,     0  ,    DynamicInputFormat . getMinRecordsPerChunk ( jobContext . getConfiguration (  )  )  ,    null )  )  ;", "}", "DistCpUtils . publish ( jobContext . getConfiguration (  )  ,    DynamicInputFormat . CONF _ LABEL _ NUM _ SPLITS ,    splits . size (  )  )  ;", "return   splits ;", "}", "METHOD_END"], "methodName": ["createSplits"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "String   listingFilePathString    =    configuration . get ( DistCpConstants . CONF _ LABEL _ LISTING _ FILE _ PATH ,     \"  \"  )  ;", "assert    !  ( listingFilePathString . equals (  \"  \"  )  )     :     \" Listing   file   not   found .  \"  ;", "Path   listingFilePath    =    new   Path ( listingFilePathString )  ;", "try    {", "assert   listingFilePath . getFileSystem ( configuration )  . exists ( listingFilePath )     :     (  \" Listing   file :     \"     +    listingFilePath )     +     \"    not   found .  \"  ;", "}    catch    ( IOException   e )     {", "assert   false    :     (  (  \" Listing   file :     \"     +    listingFilePath )     +     \"    couldn ' t   be   access    \"  )     +     ( e . getMessage (  )  )  ;", "}", "return   listingFilePath ;", "}", "METHOD_END"], "methodName": ["getListingFilePath"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   configuration . getInt ( DynamicInputFormat . CONF _ LABEL _ LISTING _ SPLIT _ RATIO ,    DynamicInputFormat . getSplitRatio ( numMaps ,    numPaths ,    configuration )  )  ;", "}", "METHOD_END"], "methodName": ["getListingSplitRatio"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "int   maxChunksIdeal    =    conf . getInt ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ IDEAL ,    DistCpConstants . MAX _ CHUNKS _ IDEAL _ DEFAULT )  ;", "if    ( maxChunksIdeal    <  =     0  )     {", ". LOG . warn (  (  (  ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ IDEAL )     +     \"    should   be   positive .    Fall   back   to   default   value :     \"  )     +     ( DistCpConstants . MAX _ CHUNKS _ IDEAL _ DEFAULT )  )  )  ;", "maxChunksIdeal    =    DistCpConstants . MAX _ CHUNKS _ IDEAL _ DEFAULT ;", "}", "return   maxChunksIdeal ;", "}", "METHOD_END"], "methodName": ["getMaxChunksIdeal"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "int   maxChunksTolerable    =    conf . getInt ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ TOLERABLE ,    DistCpConstants . MAX _ CHUNKS _ TOLERABLE _ DEFAULT )  ;", "if    ( maxChunksTolerable    <  =     0  )     {", ". LOG . warn (  (  (  ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ TOLERABLE )     +     \"    should   be   positive .    Fall   back   to   default   value :     \"  )     +     ( DistCpConstants . MAX _ CHUNKS _ TOLERABLE _ DEFAULT )  )  )  ;", "maxChunksTolerable    =    DistCpConstants . MAX _ CHUNKS _ TOLERABLE _ DEFAULT ;", "}", "return   maxChunksTolerable ;", "}", "METHOD_END"], "methodName": ["getMaxChunksTolerable"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "int   minRecordsPerChunk    =    conf . getInt ( DistCpConstants . CONF _ LABEL _ MIN _ RECORDS _ PER _ CHUNK ,    DistCpConstants . MIN _ RECORDS _ PER _ CHUNK _ DEFAULT )  ;", "if    ( minRecordsPerChunk    <  =     0  )     {", ". LOG . warn (  (  (  ( DistCpConstants . CONF _ LABEL _ MIN _ RECORDS _ PER _ CHUNK )     +     \"    should   be   positive .    Fall   back   to   default   value :     \"  )     +     ( DistCpConstants . MIN _ RECORDS _ PER _ CHUNK _ DEFAULT )  )  )  ;", "minRecordsPerChunk    =    DistCpConstants . MIN _ RECORDS _ PER _ CHUNK _ DEFAULT ;", "}", "return   minRecordsPerChunk ;", "}", "METHOD_END"], "methodName": ["getMinRecordsPerChunk"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   DistCpUtils . getInt ( configuration ,    DynamicInputFormat . CONF _ LABEL _ NUM _ ENTRIES _ PER _ CHUNK )  ;", "}", "METHOD_END"], "methodName": ["getNumEntriesPerChunk"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   DistCpUtils . getInt ( configuration ,    NUM _ MAPS )  ;", "}", "METHOD_END"], "methodName": ["getNumMapTasks"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   DistCpUtils . getInt ( configuration ,    DistCpConstants . CONF _ LABEL _ TOTAL _ NUMBER _ OF _ RECORDS )  ;", "}", "METHOD_END"], "methodName": ["getNumberOfRecords"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "return   DynamicInputFormat . getSplitRatio ( nMaps ,    nRecords ,    new   Configuration (  )  )  ;", "}", "METHOD_END"], "methodName": ["getSplitRatio"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "int   maxChunksIdeal    =    DynamicInputFormat . getMaxChunksIdeal ( conf )  ;", "int   minRecordsPerChunk    =    DynamicInputFormat . getMinRecordsPerChunk ( conf )  ;", "int   splitRatio    =    DynamicInputFormat . getSplitRatio ( conf )  ;", "if    ( nMaps    =  =     1  )     {", "DynamicInputFormat . LOG . warn (  \" nMaps    =  =     1  .    Why   use   DynamicInputFormat ?  \"  )  ;", "return    1  ;", "}", "if    ( nMaps    >    maxChunksIdeal )", "return   splitRatio ;", "int   nPickups    =     (  ( int )     ( Math . ceil (  (  (  ( float )     ( maxChunksIdeal )  )     /    nMaps )  )  )  )  ;", "int   nRecordsPerChunk    =     (  ( int )     ( Math . ceil (  (  (  ( float )     ( nRecords )  )     /     ( nMaps    *    nPickups )  )  )  )  )  ;", "return   nRecordsPerChunk    <    minRecordsPerChunk    ?    splitRatio    :    nPickups ;", "}", "METHOD_END"], "methodName": ["getSplitRatio"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "int   splitRatio    =    conf . getInt ( DistCpConstants . CONF _ LABEL _ SPLIT _ RATIO ,    DistCpConstants . SPLIT _ RATIO _ DEFAULT )  ;", "if    ( splitRatio    <  =     0  )     {", ". LOG . warn (  (  (  ( DistCpConstants . CONF _ LABEL _ SPLIT _ RATIO )     +     \"    should   be   positive .    Fall   back   to   default   value :     \"  )     +     ( DistCpConstants . SPLIT _ RATIO _ DEFAULT )  )  )  ;", "splitRatio    =    DistCpConstants . SPLIT _ RATIO _ DEFAULT ;", "}", "return   splitRatio ;", "}", "METHOD_END"], "methodName": ["getSplitRatio"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   configuration    =    context . getConfiguration (  )  ;", "int   numRecords    =     . getNumberOfRecords ( configuration )  ;", "int   numMaps    =     . getNumMapTasks ( configuration )  ;", "int   maxChunksTolerable    =     . getMaxChunksTolerable ( configuration )  ;", "int   splitRatio    =     . getListingSplitRatio ( configuration ,    numMaps ,    numRecords )  ;", ". validateNumChunksUsing ( splitRatio ,    numMaps ,    maxChunksTolerable )  ;", "int   numEntriesPerChunk    =     (  ( int )     ( Math . ceil (  (  (  ( float )     ( numRecords )  )     /     ( splitRatio    *    numMaps )  )  )  )  )  ;", "DistCpUtils . publish ( context . getConfiguration (  )  ,     . CONF _ LABEL _ NUM _ ENTRIES _ PER _ CHUNK ,    numEntriesPerChunk )  ;", "final   int   nChunksTotal    =     (  ( int )     ( Math . ceil (  (  (  ( float )     ( numRecords )  )     /    numEntriesPerChunk )  )  )  )  ;", "int   nChunksOpenAtOnce    =    Math . min (  . N _ CHUNKS _ OPEN _ AT _ ONCE _ DEFAULT ,    nChunksTotal )  ;", "Path   listingPath    =     . getListingFilePath ( configuration )  ;", "SequenceFile . Reader   reader    =    new   SequenceFile . Reader ( configuration ,    Reader . file ( listingPath )  )  ;", "List < DynamicInputChunk >    openChunks    =    new   ArrayList < DynamicInputChunk >  (  )  ;", "List < DynamicInputChunk >    chunksFinal    =    new   ArrayList < DynamicInputChunk >  (  )  ;", "CopyListingFileStatus   fileStatus    =    new   CopyListingFileStatus (  )  ;", "Text   relPath    =    new   Text (  )  ;", "int   recordCounter    =     0  ;", "int   chunkCount    =     0  ;", "try    {", "while    ( reader . next ( relPath ,    fileStatus )  )     {", "if    (  ( recordCounter    %     ( nChunksOpenAtOnce    *    numEntriesPerChunk )  )     =  =     0  )     {", ". closeAll ( openChunks )  ;", "chunksFinal . addAll ( openChunks )  ;", "openChunks    =     . createChunks ( configuration ,    chunkCount ,    nChunksTotal ,    nChunksOpenAtOnce )  ;", "chunkCount    +  =    openChunks . size (  )  ;", "nChunksOpenAtOnce    =    openChunks . size (  )  ;", "recordCounter    =     0  ;", "}", "openChunks . get (  ( recordCounter    %    nChunksOpenAtOnce )  )  . write ( relPath ,    fileStatus )  ;", "+  + recordCounter ;", "}", "}    finally    {", ". closeAll ( openChunks )  ;", "chunksFinal . addAll ( openChunks )  ;", "IOUtils . closeStream ( reader )  ;", "}", ". LOG . info (  (  \" Number   of   dynamic - chunk - files   created :     \"     +     ( chunksFinal . size (  )  )  )  )  ;", "return   chunksFinal ;", "}", "METHOD_END"], "methodName": ["splitCopyListingIntoChunksWithShuffle"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "if    (  ( splitRatio    *    numMaps )     >    maxChunksTolerable )", "throw   new   IOException (  (  (  (  (  \" Too   many   chunks   creatwith   splitRatio :  \"     +    splitRatio )     +     \"  ,    numMaps :  \"  )     +    numMaps )     +     \"  .    Rce   numMaps   or   decrease   split - ratio   to   proce \"  )  )  ;", "}", "METHOD_END"], "methodName": ["validateNumChunksUsing"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "boolean   tooLongSinceLastDirScan    =     ( now    -     ( timeOfLastChunkDirScan )  )     >     (  . TIME _ THRESHOLD _ FOR _ DIR _ SCANS )  ;", "if    ( tooLongSinceLastDirScan    |  |     (  (  !  ( isChunkDirAlreadyScanned )  )     &  &     (  (  ( numRecordsProcessedByThisMap )     %     ( numRecordsPerChunk )  )     >     (  ( numRecordsPerChunk )     /     2  )  )  )  )     {", "DynamicInputChunk . getListOfChunkFiles (  )  ;", "isChunkDirAlreadyScanned    =    true ;", "timeOfLastChunkDirScan    =    now ;", "}", "return   DynamicInputChunk . getNumChunksLeft (  )  ;", "}", "METHOD_END"], "methodName": ["getNumChunksLeft"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicRecordReader"}, {"methodBody": ["METHOD_START", "{", "return   DistCpUtils . getInt ( configuration ,    DistCpConstants . CONF _ LABEL _ TOTAL _ NUMBER _ OF _ RECORDS )  ;", "}", "METHOD_END"], "methodName": ["getTotalNumRecords"], "fileName": "org.apache.hadoop.tools.mapred.lib.DynamicRecordReader"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fileSystem    =    null ;", "DataOutputStream   outputStream    =    null ;", "try    {", "fileSystem    =     . cluster . getFileSystem (  )  ;", "outputStream    =    fileSystem . create ( new   Path ( path )  ,    true ,     0  )  ;", ". expectedFilePaths . add ( fileSystem . listStatus ( new   Path ( path )  )  [  0  ]  . getPath (  )  . toString (  )  )  ;", "}    finally    {", "IOUtils . cleanup ( null ,    fileSystem ,    outputStream )  ;", "}", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    new   Configuration (  )  ;", "System . setProperty (  \" test . build . data \"  ,     \" target / tmp / build / TEST _ DYNAMIC _ INPUT _ FORMAT / data \"  )  ;", "configuration . set (  \" hadoop . log . dir \"  ,     \" target / tmp \"  )  ;", ". LOG . debug (  (  \" fs . default . name       =  =     \"     +     ( configuration . get (  \" fs . default . name \"  )  )  )  )  ;", ". LOG . debug (  (  \" dfs . http . address    =  =     \"     +     ( configuration . get (  \" dfs . http . address \"  )  )  )  )  ;", "return   configuration ;", "}", "METHOD_END"], "methodName": ["getConfigurationForCluster"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "Path   sourcePath    =    new   Path (  (  ( TestDynamicInputFormat . cluster . getFileSystem (  )  . getUri (  )  . toString (  )  )     +     \"  / tmp / source \"  )  )  ;", "Path   targetPath    =    new   Path (  (  ( TestDynamicInputFormat . cluster . getFileSystem (  )  . getUri (  )  . toString (  )  )     +     \"  / tmp / target \"  )  )  ;", "List < Path >    sourceList    =    new   ArrayList < Path >  (  )  ;", "sourceList . add ( sourcePath )  ;", "DistCpOptions   options    =    new   DistCpOptions ( sourceList ,    targetPath )  ;", "options . setMaxMaps ( TestDynamicInputFormat . NUM _ SPLITS )  ;", "return   options ;", "}", "METHOD_END"], "methodName": ["getOptions"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "TestDynamicInputFormat . cluster    =    new   Builder ( TestDynamicInputFormat . getConfigurationForCluster (  )  )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "for    ( int   i    =     0  ;    i    <     ( TestDynamicInputFormat . N _ FILES )  ;     +  + i )", "TestDynamicInputFormat . createFile (  (  \"  / tmp / source /  \"     +     ( String . valueOf ( i )  )  )  )  ;", "FileSystem   fileSystem    =    TestDynamicInputFormat . cluster . getFileSystem (  )  ;", "TestDynamicInputFormat . expectedFilePaths . add ( fileSystem . listStatus ( new   Path (  \"  / tmp / source /  0  \"  )  )  [  0  ]  . getPath (  )  . getParent (  )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "TestDynamicInputFormat . cluster . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals (  1  ,    DynamicInputFormat . getSplitRatio (  1  ,     1  0  0  0  0  0  0  0  0  0  )  )  ;", "Assert . assertEquals (  2  ,    DynamicInputFormat . getSplitRatio (  1  1  0  0  0  0  0  0  ,     1  0  )  )  ;", "Assert . assertEquals (  4  ,    DynamicInputFormat . getSplitRatio (  3  0  ,     7  0  0  )  )  ;", "Assert . assertEquals (  2  ,    DynamicInputFormat . getSplitRatio (  3  0  ,     2  0  0  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ TOLERABLE ,     (  -  1  )  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ IDEAL ,     (  -  1  )  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ MIN _ RECORDS _ PER _ CHUNK ,     (  -  1  )  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ SPLIT _ RATIO ,     (  -  1  )  )  ;", "Assert . assertEquals (  1  ,    DynamicInputFormat . getSplitRatio (  1  ,     1  0  0  0  0  0  0  0  0  0  ,    conf )  )  ;", "Assert . assertEquals (  2  ,    DynamicInputFormat . getSplitRatio (  1  1  0  0  0  0  0  0  ,     1  0  ,    conf )  )  ;", "Assert . assertEquals (  4  ,    DynamicInputFormat . getSplitRatio (  3  0  ,     7  0  0  ,    conf )  )  ;", "Assert . assertEquals (  2  ,    DynamicInputFormat . getSplitRatio (  3  0  ,     2  0  0  ,    conf )  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ TOLERABLE ,     1  0  0  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ MAX _ CHUNKS _ IDEAL ,     3  0  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ MIN _ RECORDS _ PER _ CHUNK ,     1  0  )  ;", "conf . setInt ( DistCpConstants . CONF _ LABEL _ SPLIT _ RATIO ,     5  3  )  ;", "Assert . assertEquals (  5  3  ,    DynamicInputFormat . getSplitRatio (  3  ,     2  0  0  ,    conf )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSplitRatio"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "DistCpOptions   options    =    TestDynamicInputFormat . getOptions (  )  ;", "Configuration   configuration    =    new   Configuration (  )  ;", "configuration . set (  \" mapred . map . tasks \"  ,    String . valueOf ( options . getMaxMaps (  )  )  )  ;", "CopyListing . getCopyListing ( configuration ,    TestDynamicInputFormat . CREDENTIALS ,    options )  . buildListing ( new   Path (  (  ( TestDynamicInputFormat . cluster . getFileSystem (  )  . getUri (  )  . toString (  )  )     +     \"  / tmp / testDynInputFormat / fileList . seq \"  )  )  ,    options )  ;", "JobContext   jobContext    =    new   JobContextImpl ( configuration ,    new   JobID (  )  )  ;", "DynamicInputFormat < Text ,    CopyListingFileStatus >    inputFormat    =    new   DynamicInputFormat < Text ,    CopyListingFileStatus >  (  )  ;", "List < InputSplit >    splits    =    inputFormat . getSplits ( jobContext )  ;", "int   nFiles    =     0  ;", "int   taskId    =     0  ;", "for    ( InputSplit   split    :    splits )     {", "RecordReader < Text ,    CopyListingFileStatus >    recordReader    =    inputFormat . createRecordReader ( split ,    null )  ;", "StubContext   stubContext    =    new   StubContext ( jobContext . getConfiguration (  )  ,    recordReader ,    taskId )  ;", "final   TaskAttemptContext   taskAttemptContext    =    stubContext . getContext (  )  ;", "recordReader . initialize ( splits . get (  0  )  ,    taskAttemptContext )  ;", "float   previousProgressValue    =     0  .  0 F ;", "while    ( recordReader . nextKeyValue (  )  )     {", "CopyListingFileStatus   fileStatus    =    recordReader . getCurrentValue (  )  ;", "String   source    =    fileStatus . getPath (  )  . toString (  )  ;", "System . out . println ( source )  ;", "Assert . assertTrue ( TestDynamicInputFormat . expectedFilePaths . contains ( source )  )  ;", "final   float   progress    =    recordReader . getProgress (  )  ;", "Assert . assertTrue (  ( progress    >  =    previousProgressValue )  )  ;", "Assert . assertTrue (  ( progress    >  =     0  .  0 F )  )  ;", "Assert . assertTrue (  ( progress    <  =     1  .  0 F )  )  ;", "previousProgressValue    =    progress ;", "+  + nFiles ;", "}", "Assert . assertTrue (  (  ( recordReader . getProgress (  )  )     =  =     1  .  0 F )  )  ;", "+  + taskId ;", "}", "Assert . assertEquals ( TestDynamicInputFormat . expectedFilePaths . size (  )  ,    nFiles )  ;", "}", "METHOD_END"], "methodName": ["testGetSplits"], "fileName": "org.apache.hadoop.tools.mapred.lib.TestDynamicInputFormat"}, {"methodBody": ["METHOD_START", "{", "if    (  ( mNodeMap )     =  =    null )     {", "mNodeMap    =    new   HashMap < String ,    MineNode >  ( mineNodes . size (  )  )  ;", "for    ( MineNode   mn    :    mineNodes )     {", "mNodeMap . put ( mn . getName (  )  ,    mn )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["buildMachineNodeMap"], "fileName": "org.apache.hadoop.tools.rumen.AbstractClusterStory"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rNodeMap )     =  =    null )     {", "rNodeMap    =    new   HashMap < String ,    RackNode >  ( kNodes . size (  )  )  ;", "for    ( RackNode   rn    :    kNodes )     {", "rNodeMap . put ( rn . getName (  )  ,    rn )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["buildRackNodeMap"], "fileName": "org.apache.hadoop.tools.rumen.AbstractClusterStory"}, {"methodBody": ["METHOD_START", "{", "if    (  ( machineNodes )     =  =    null )     {", "Node   root    =    geTopology (  )  ;", "SortedSet < MachineNode >    mNodes    =    new   TreeSet < MachineNode >  (  )  ;", "SortedSet < RackNode >    rNodes    =    new   TreeSet < RackNode >  (  )  ;", "Deque < Node >    unvisited    =    new   ArrayDeque < Node >  (  )  ;", "Deque < Integer >    distUnvisited    =    new   ArrayDeque < Integer >  (  )  ;", "unvisited . add ( root )  ;", "distUnvisited . add (  0  )  ;", "for    ( Node   n    =    unvisited . poll (  )  ;    n    !  =    null ;    n    =    unvisited . poll (  )  )     {", "int   distance    =    distUnvisited . poll (  )  ;", "if    ( n   instanceof   RackNode )     {", "rNodes . add (  (  ( RackNode )     ( n )  )  )  ;", "mNodes . addAll (  (  ( RackNode )     ( n )  )  . getMachinesInRack (  )  )  ;", "if    (  ( distance    +     1  )     >     ( maximumDistance )  )     {", "maximumDistance    =    distance    +     1  ;", "}", "} else", "if    ( n   instanceof   MachineNode )     {", "mNodes . add (  (  ( MachineNode )     ( n )  )  )  ;", "if    ( distance    >     ( maximumDistance )  )     {", "maximumDistance    =    distance ;", "}", "} else    {", "for    ( Node   child    :    n . getChildren (  )  )     {", "unvisited . addFirst ( child )  ;", "distUnvisited . addFirst (  ( distance    +     1  )  )  ;", "}", "}", "}", "machineNodes    =    Collections . unmodifiableSortedSet ( mNodes )  ;", "rackNodes    =    Collections . unmodifiableSortedSet ( rNodes )  ;", "}", "}", "METHOD_END"], "methodName": ["parseTopologyTree"], "fileName": "org.apache.hadoop.tools.rumen.AbstractClusterStory"}, {"methodBody": ["METHOD_START", "{", "if    ( anonymizeTopology )     {", "System . out . println (  (  \" Anonymizing   topology   file :     \"     +     ( inputTopologyPath )  )  )  ;", "ClusterTopologyReader   reader    =    null ;", "JsonGenerator   outGen    =    null ;", "Configuration   conf    =    getConf (  )  ;", "try    {", "outGen    =    createJsonGenerator ( conf ,    outputTopologyPath )  ;", "reader    =    new   ClusterTopologyReader ( inputTopologyPath ,    conf )  ;", "LoggedNetworkTopology   job    =    reader . get (  )  ;", "outGen . writeObject ( job )  ;", "System . out . println (  (  \" d   topology   file :     \"     +     ( outputTopologyPath )  )  )  ;", "}    finally    {", "if    ( outGen    !  =    null )     {", "outGen . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["anonymizeTopology"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "if    ( anonymizeTrace )     {", "System . out . println (  (  \" Anonymizing   trace   file :     \"     +     ( inputTracePath )  )  )  ;", "JobTraceReader   reader    =    null ;", "JsonGenerator   outGen    =    null ;", "Configuration   conf    =    getConf (  )  ;", "try    {", "outGen    =    createJsonGenerator ( conf ,    outputTracePath )  ;", "reader    =    new   JobTraceReader ( inputTracePath ,    conf )  ;", "LoggedJob   job    =    reader . getNext (  )  ;", "while    ( job    !  =    null )     {", "outGen . writeObject ( job )  ;", "job    =    reader . getNext (  )  ;", "}", "System . out . println (  (  \" d   trace   file :     \"     +     ( outputTracePath )  )  )  ;", "}    finally    {", "if    ( outGen    !  =    null )     {", "outGen . close (  )  ;", "}", "if    ( reader    !  =    null )     {", "reader . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["anonymizeTrace"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "FileSystem   outFS    =    path . getFileSystem ( conf )  ;", "CompressionCodec   codec    =    new   io . compress . CompressionCodecFactory ( conf )  . getCodec ( path )  ;", "OutputStream   output ;", "Compressor   compressor    =    null ;", "if    ( codec    !  =    null )     {", "compressor    =    CodecPool . getCompressor ( codec )  ;", "output    =    codec . createOutputStream ( outFS . create ( path )  ,    compressor )  ;", "} else    {", "output    =    outFS . create ( path )  ;", "}", "JsonGenerator   outGen    =    outFactory . createJsonGenerator ( output ,    UTF 8  )  ;", "outGen . useDefaultPrettyPrinter (  )  ;", "return   outGen ;", "}", "METHOD_END"], "methodName": ["createJsonGenerator"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "try    {", "for    ( int   i    =     0  ;    i    <     ( args . length )  ;     +  + i )     {", "if    (  \"  - trace \"  . equals ( args [ i ]  )  )     {", "aTrace    =    true ;", "inputTracePath    =    new   Path ( args [  ( i    +     1  )  ]  )  ;", "outputTracePath    =    new   Path ( args [  ( i    +     2  )  ]  )  ;", "i    +  =     2  ;", "}", "if    (  \"  - topology \"  . equals ( args [ i ]  )  )     {", "aTopology    =    true ;", "inputTopologyPath    =    new   Path ( args [  ( i    +     1  )  ]  )  ;", "outputTopologyPath    =    new   Path ( args [  ( i    +     2  )  ]  )  ;", "i    +  =     2  ;", "}", "}", "}    catch    ( Exception   e )     {", "throw   new   IllegalArgumentException (  \" Illegal   arguments   list !  \"  ,    e )  ;", "}", "if    (  (  !  ( aTopology )  )     &  &     (  !  ( aTrace )  )  )     {", "throw   new   IllegalArgumentException (  \" Invalid   arguments   list !  \"  )  ;", "}", "statePool    =    new   StatePool (  )  ;", "statePool . initialize ( getConf (  )  )  ;", "outMapper    =    new   ObjectMapper (  )  ;", "SimpleModule   module    =    new   SimpleModule (  \" Anonymization   Serializer \"  ,    new   Version (  0  ,     1  ,     1  ,     \" FINAL \"  )  )  ;", "module . addSerializer ( DataType . class ,    new   DefaultRumenSerializer (  )  )  ;", "module . addSerializer ( String . class ,    new   BlockingSerializer (  )  )  ;", "module . addSerializer ( ID . class ,    new   ObjectStringSerializer < ID >  (  )  )  ;", "module . addSerializer ( AnonymizableDataType . class ,    new   DefaultAnonymizingRumenSerializer ( statePool ,    getConf (  )  )  )  ;", "outMapper . registerModule ( module )  ;", "outFactory    =    outMapper . getJsonFactory (  )  ;", "}", "METHOD_END"], "methodName": ["initialize"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "Anonymizer   instance    =    new   Anonymizer (  )  ;", "int   result    =     0  ;", "try    {", "result    =    ToolRunner . run ( instance ,    args )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace ( System . err )  ;", "System . exit (  (  -  1  )  )  ;", "}", "if    ( result    !  =     0  )     {", "System . exit ( result )  ;", "}", "return ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  \"  \\ nUsage :  -  \"  )  ;", "System . out . print (  \"        \"  )  ;", "System . out . print (  \"     [  - trace    < input - trace - path >     < output - trace - path >  ]  \"  )  ;", "System . out . println (  (  \"     [  - topology    < input - topology - path >     \"     +     \"  < output - topology - path >  ]     \"  )  )  ;", "System . out . print (  \"  \\ n \"  )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "try    {", "anonymizeTrace (  )  ;", "}    catch    ( IOException   ioe )     {", "System . err . println (  \" Error   running   the   trace   a !  \"  )  ;", "ioe . printStackTrace (  )  ;", "System . out . println (  \"  \\ n \\ nAnonymization   unsuccessful !  \"  )  ;", "return    -  1  ;", "}", "try    {", "anonymizeTopology (  )  ;", "}    catch    ( IOException   ioe )     {", "System . err . println (  \" Error   running   the   cluster   topology   a !  \"  )  ;", "ioe . printStackTrace (  )  ;", "System . out . println (  \"  \\ n \\ nAnonymization   unsuccessful !  \"  )  ;", "return    -  1  ;", "}", "statePool . persist (  )  ;", "System . out . println (  \" Anonymization   completed   successfully !  \"  )  ;", "return    0  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.rumen.Anonymizer"}, {"methodBody": ["METHOD_START", "{", "int   result    =    Arrays . binarySearch ( rankings ,    probe )  ;", "return    ( Math . abs (  ( result    +     1  )  )  )     -     1  ;", "}", "METHOD_END"], "methodName": ["floorIndex"], "fileName": "org.apache.hadoop.tools.rumen.CDFRandomGenerator"}, {"methodBody": ["METHOD_START", "{", "return   values [ index ]  ;", "}", "METHOD_END"], "methodName": ["getDatumAt"], "fileName": "org.apache.hadoop.tools.rumen.CDFRandomGenerator"}, {"methodBody": ["METHOD_START", "{", "return   rankings [ index ]  ;", "}", "METHOD_END"], "methodName": ["getRankingAt"], "fileName": "org.apache.hadoop.tools.rumen.CDFRandomGenerator"}, {"methodBody": ["METHOD_START", "{", "rankings [  0  ]     =     0  .  0  ;", "values [  0  ]     =    cdf . getMinimum (  )  ;", "rankings [  (  ( rankings . length )     -     1  )  ]     =     1  .  0  ;", "values [  (  ( rankings . length )     -     1  )  ]     =    cdf . getMaximum (  )  ;", "List < LoggedSingleRelativeking >    subjects    =    cdf . getkings (  )  ;", "for    ( int   i    =     0  ;    i    <     ( subjects . size (  )  )  ;     +  + i )     {", "rankings [  ( i    +     1  )  ]     =    subjects . get ( i )  . getRelativeking (  )  ;", "values [  ( i    +     1  )  ]     =    subjects . get ( i )  . getDatum (  )  ;", "}", "}", "METHOD_END"], "methodName": ["initializeTables"], "fileName": "org.apache.hadoop.tools.rumen.CDFRandomGenerator"}, {"methodBody": ["METHOD_START", "{", "return   valueAt ( random . nextDouble (  )  )  ;", "}", "METHOD_END"], "methodName": ["randomValue"], "fileName": "org.apache.hadoop.tools.rumen.CDFRandomGenerator"}, {"methodBody": ["METHOD_START", "{", "return   topology ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.tools.rumen.ClusterTopologyReader"}, {"methodBody": ["METHOD_START", "{", "try    {", "t    =    parser . getNext (  )  ;", "if    (  ( t )     =  =    null )     {", "throw   new   IOException (  \" Input   file   does   not   contain   valid   t   data .  \"  )  ;", "}", "}    finally    {", "parser . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["readTopology"], "fileName": "org.apache.hadoop.tools.rumen.ClusterTopologyReader"}, {"methodBody": ["METHOD_START", "{", "final   DataInputStream   in    =    new   CurrentJHParser . ForkedDataInputStream ( input )  ;", "try    {", "final   EventReader   reader    =    new   EventReader ( in )  ;", "try    {", "reader . getNextEvent (  )  ;", "}    catch    ( IOException   e )     {", "return   false ;", "}    finally    {", "reader . close (  )  ;", "}", "}    catch    ( IOException   e )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["canParse"], "fileName": "org.apache.hadoop.tools.rumen.CurrentJHParser"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( skewBufferLength )  ;     +  + i )     {", "Logg   newJob    =    rawNextJob (  )  ;", "if    ( newJob    =  =    null )     {", "return ;", "}", "skewBuffer . add ( newJob )  ;", "}", "}", "METHOD_END"], "methodName": ["fillSkewBuffer"], "fileName": "org.apache.hadoop.tools.rumen.DeskewedJobTraceReader"}, {"methodBody": ["METHOD_START", "{", "return   maxSkewBufferNeeded ;", "}", "METHOD_END"], "methodName": ["neededSkewBufferSize"], "fileName": "org.apache.hadoop.tools.rumen.DeskewedJobTraceReader"}, {"methodBody": ["METHOD_START", "{", "LoggedJob   newJob    =    rawNextJob (  )  ;", "if    ( newJob    !  =    null )     {", "skewBuffer . add ( newJob )  ;", "}", "LoggedJob   result    =    skewBuffer . poll (  )  ;", "while    (  ( result    !  =    null )     &  &     (  ( result . getSubmitTime (  )  )     <     ( returnedLatestSubmitTime )  )  )     {", ". LOG . error (  \" The   current   job   was   submitted   earlier   than   the   previous   one \"  )  ;", ". LOG . error (  (  \" Its   jobID   is    \"     +     ( result . getJobID (  )  )  )  )  ;", ". LOG . error (  (  (  (  \" Its   submit   time   is    \"     +     ( result . getSubmitTime (  )  )  )     +     \"  , but   the   previous   one   was    \"  )     +     ( returnedLatestSubmitTime )  )  )  ;", "if    ( abortOnUnfixableSkew )     {", "throw   new    . OutOfOrderException (  (  (  (  \" Job   submit   time   is    \"     +     ( result . getSubmitTime (  )  )  )     +     \"  , but   the   previous   one   was    \"  )     +     ( returnedLatestSubmitTime )  )  )  ;", "}", "result    =    rawNextJob (  )  ;", "}", "if    ( result    !  =    null )     {", "returnedLatestSubmitTime    =    result . getSubmitTime (  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["nextJob"], "fileName": "org.apache.hadoop.tools.rumen.DeskewedJobTraceReader"}, {"methodBody": ["METHOD_START", "{", "LoggedJob   result    =    reader . getNext (  )  ;", "if    (  (  (  !  ( abortOnUnfixableSkew )  )     |  |     (  ( skewBufferLength )     >     0  )  )     &  &     ( result    !  =    null )  )     {", "long   thisTime    =    result . getSubmitTime (  )  ;", "if    ( submitTimesSoFar . contains ( thisTime )  )     {", "Integer   myCount    =    countedRepeatedSubmitTimesSoFar . get ( thisTime )  ;", "countedRepeatedSubmitTimesSoFar . put ( thisTime ,     ( myCount    =  =    null    ?     2     :    myCount    +     1  )  )  ;", "} else    {", "submitTimesSoFar . add ( thisTime )  ;", "}", "if    ( thisTime    <     ( skewMeasurementLatestSubmitTime )  )     {", "Iterator < Long >    endCursor    =    submitTimesSoFar . descendingIterator (  )  ;", "int   thisJobNeedsSkew    =     0  ;", "Long   keyNeedingSkew ;", "while    (  ( endCursor . hasNext (  )  )     &  &     (  ( keyNeedingSkew    =    endCursor . next (  )  )     >    thisTime )  )     {", "Integer   keyNeedsSkewAmount    =    countedRepeatedSubmitTimesSoFar . get ( keyNeedingSkew )  ;", "thisJobNeedsSkew    +  =     ( keyNeedsSkewAmount    =  =    null )     ?     1     :    keyNeedsSkewAmount ;", "}", "maxSkewBufferNeeded    =    Math . max ( maxSkewBufferNeeded ,    thisJobNeedsSkew )  ;", "}", "skewMeasurementLatestSubmitTime    =    Math . max ( thisTime ,    skewMeasurementLatestSubmitTime )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["rawNextJob"], "fileName": "org.apache.hadoop.tools.rumen.DeskewedJobTraceReader"}, {"methodBody": ["METHOD_START", "{", "long   offsetInCycle    =     (  ( adjustee . getSubmitTime (  )  )     -     ( firstJobSubmitTime )  )     %     ( inputCycle )  ;", "long   outputOffset    =     (  ( long )     (  (  ( double )     ( offsetInCycle )  )     *     ( timeDilation )  )  )  ;", "long   adjustt    =     (  ( firstJobSubmitTime )     +    outputOffset )     -     ( adjustee . getSubmitTime (  )  )  ;", "adjustee . adjustTimes ( adjustt )  ;", "}", "METHOD_END"], "methodName": ["adjustJobTimes"], "fileName": "org.apache.hadoop.tools.rumen.Folder"}, {"methodBody": ["METHOD_START", "{", "String   tempDirName    =    null ;", "String   inputPathName    =    null ;", "String   outputPathName    =    null ;", "for    ( int   i    =     0  ;    i    <     ( args . length )  ;     +  + i )     {", "String   thisArg    =    args [ i ]  ;", "if    ( thisArg . equalsIgnoreCase (  \"  - starts - after \"  )  )     {", "startsAfter    =     . parseDuration ( args [  (  +  + i )  ]  )  ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - output - duration \"  )  )     {", "outputDuration    =     . parseDuration ( args [  (  +  + i )  ]  )  ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - input - cycle \"  )  )     {", "inputCycle    =     . parseDuration ( args [  (  +  + i )  ]  )  ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - concentration \"  )  )     {", "concentration    =    Double . parseDouble ( args [  (  +  + i )  ]  )  ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - debug \"  )  )     {", "debug    =    true ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - allow - missorting \"  )  )     {", "allowMissorting    =    true ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - seed \"  )  )     {", "seeded    =    true ;", "randomSeed    =    Long . parseLong ( args [  (  +  + i )  ]  )  ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - skew - buffer - length \"  )  )     {", "skewBufferLength    =    Integer . parseInt ( args [  (  +  + i )  ]  )  ;", "} else", "if    ( thisArg . equalsIgnoreCase (  \"  - temp - directory \"  )  )     {", "tempDirName    =    args [  (  +  + i )  ]  ;", "} else", "if    (  ( thisArg . equals (  \"  \"  )  )     |  |     ( thisArg . startsWith (  \"  -  \"  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Illegal   switch   argument ,     \"     +    thisArg )     +     \"    at   position    \"  )     +    i )  )  ;", "} else    {", "inputPathName    =    thisArg ;", "outputPathName    =    args [  (  +  + i )  ]  ;", "if    ( i    !  =     (  ( args . length )     -     1  )  )     {", "throw   new   IllegalArgumentException (  \" Too   many   non - switch   arguments \"  )  ;", "}", "}", "}", "try    {", "Configuration   conf    =    getConf (  )  ;", "Path   inPath    =    new   Path ( inputPathName )  ;", "reader    =    new   DeskewedJobTraceReader ( new   JobTraceReader ( inPath ,    conf )  ,    skewBufferLength ,     (  !  ( allowMissorting )  )  )  ;", "Path   outPath    =    new   Path ( outputPathName )  ;", "outGen    =    new   DefaultOutputter < LoggedJob >  (  )  ;", "outGen . init ( outPath ,    conf )  ;", "tempDir    =     ( tempDirName    =  =    null )     ?    outPath . getParent (  )     :    new   Path ( tempDirName )  ;", "FileSystem   fs    =    tempDir . getFileSystem ( getConf (  )  )  ;", "if    (  !  ( fs . getFileStatus ( tempDir )  . isDirectory (  )  )  )     {", "throw   new   IOException (  \" Your   temp   directory   is   not   a   directory \"  )  ;", "}", "if    (  ( inputCycle )     <  =     0  )     {", ". LOG . error (  \" You   must   have   an   input   cycle   length .  \"  )  ;", "return    . NO _ INPUT _ CYCLE _ LENGTH ;", "}", "if    (  ( outputDuration )     <  =     0  )     {", "outputDuration    =     (  6  0 L    *     6  0 L )     *     (  . TICKS _ PER _ SECOND )  ;", "}", "if    (  ( inputCycle )     <  =     0  )     {", "inputCycle    =    outputDuration ;", "}", "timeDilation    =     (  ( double )     ( outputDuration )  )     /     (  ( double )     ( inputCycle )  )  ;", "random    =     ( seeded )     ?    new   Random ( randomSeed )     :    new   Random (  )  ;", "if    ( debug )     {", "randomSeed    =    random . nextLong (  )  ;", ". LOG . warn (  (  \" This   run   effectively   has   a    - seed   of    \"     +     ( randomSeed )  )  )  ;", "random    =    new   Random ( randomSeed )  ;", "seeded    =    true ;", "}", "}    catch    ( IOException   e )     {", "e . printStackTrace ( System . err )  ;", "return    . NON _ EXISTENT _ FILES ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["initialize"], "fileName": "org.apache.hadoop.tools.rumen.Folder"}, {"methodBody": ["METHOD_START", "{", "Folder   instance    =    new   Folder (  )  ;", "int   result    =     0  ;", "try    {", "result    =    ToolRunner . run ( instance ,    args )  ;", "}    catch    ( IOException   e )     {", "e . printStackTrace ( System . err )  ;", "System . exit ( Folder . IO _ ERROR )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace ( System . err )  ;", "System . exit ( Folder . OTHER _ ERROR )  ;", "}", "if    ( result    !  =     0  )     {", "System . exit ( result )  ;", "}", "return ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.rumen.Folder"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( transcriptionRateInteger )  ;     +  + i )     {", "outGoutput ( job )  ;", "}", "if    (  ( random . nextDouble (  )  )     <     ( transcriptionRateFraction )  )     {", "outGoutput ( job )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeOutput"], "fileName": "org.apache.hadoop.tools.rumen.Folder"}, {"methodBody": ["METHOD_START", "{", "String   numeral    =    durationString . substring (  0  ,     (  ( durationString . length (  )  )     -     1  )  )  ;", "char   durationCode    =    durationString . charAt (  (  ( durationString . length (  )  )     -     1  )  )  ;", "long   result    =    Integer . parseInt ( numeral )  ;", "if    ( result    <  =     0  )     {", "throw   new   IllegalArgumentException (  \" Negative   durations   are   not   allowed \"  )  ;", "}", "switch    ( durationCode )     {", "case    ' D '     :", "case    ' d '     :", "return    (  (  (  2  4 L    *     6  0 L )     *     6  0 L )     *     (  . TICKS _ PER _ SECOND )  )     *    result ;", "case    ' H '     :", "case    ' h '     :", "return    (  (  6  0 L    *     6  0 L )     *     (  . TICKS _ PER _ SECOND )  )     *    result ;", "case    ' M '     :", "case    ' m '     :", "return    (  6  0 L    *     (  . TICKS _ PER _ SECOND )  )     *    result ;", "case    ' S '     :", "case    ' s '     :", "return    (  . TICKS _ PER _ SECOND )     *    result ;", "default    :", "throw   new   IllegalArgumentException (  \" Missing   or   invalid   duration   code \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["parseDuration"], "fileName": "org.apache.hadoop.tools.rumen.Folder"}, {"methodBody": ["METHOD_START", "{", "class   JobEntryComparator   implements   Comparator < Pair < LoggedJob ,    JobTraceReader >  >     {", "public   int   compare ( Pair < LoggedJob ,    JobTraceReader >    p 1  ,    Pair < LoggedJob ,    JobTraceReader >    p 2  )     {", "LoggedJob   j 1     =    p 1  . first (  )  ;", "LoggedJob   j 2     =    p 2  . first (  )  ;", "return    ( j 1  . getSubmitTime (  )  )     <     ( j 2  . getSubmitTime (  )  )     ?     -  1     :     ( j 1  . getSubmitTime (  )  )     =  =     ( j 2  . getSubmitTime (  )  )     ?     0     :     1  ;", "}", "}", "Queue < Pair < LoggedJob ,    JobTraceReader >  >    heap    =    new   PriorityQueue < Pair < LoggedJob ,    JobTraceReader >  >  (  )  ;", "try    {", "LoggedJob   job    =    reader . nextJob (  )  ;", "if    ( job    =  =    null )     {", ". LOG . error (  \" The   job   trace   is   empty \"  )  ;", "return    . EMPTY _ JOB _ TRACE ;", "}", "if    (  ( startsAfter )     >     0  )     {", ". LOG . info (  (  \" starts - after   time   is   specified .    Initial   job   submit   time    :     \"     +     ( job . getSubmitTime (  )  )  )  )  ;", "long   approximateTime    =     ( job . getSubmitTime (  )  )     +     ( startsAfter )  ;", "job    =    reader . nextJob (  )  ;", "long   skippedCount    =     0  ;", "while    (  ( job    !  =    null )     &  &     (  ( job . getSubmitTime (  )  )     <    approximateTime )  )     {", "job    =    reader . nextJob (  )  ;", "skippedCount +  +  ;", "}", ". LOG . debug (  (  (  (  (  \" Considering   jobs   with   submit   time   greater   than    \"     +     ( startsAfter )  )     +     \"    ms .    Skipped    \"  )     +    skippedCount )     +     \"    jobs .  \"  )  )  ;", "if    ( job    =  =    null )     {", ". LOG . error (  (  (  (  \" No   more   jobs   to   process   in   the   trace   with    ' starts - after '  \"     +     \"    set   to    \"  )     +     ( startsAfter )  )     +     \" ms .  \"  )  )  ;", "return    . EMPTY _ JOB _ TRACE ;", "}", ". LOG . info (  (  \" The   first   job   has   a   submit   time   of    \"     +     ( job . getSubmitTime (  )  )  )  )  ;", "}", "firstJobSubmitTime    =    job . getSubmitTime (  )  ;", "long   lastJobSubmitTime    =    firstJobSubmitTime ;", "int   numberJobs    =     0  ;", "long   currentIntervalEnd    =    Long . MIN _ VALUE ;", "Path   nextSegment    =    null ;", "Outputter < LoggedJob >    tempGen    =    null ;", "if    ( debug )     {", ". LOG . debug (  (  \" The   first   job   has   a   submit   time   of    \"     +     ( firstJobSubmitTime )  )  )  ;", "}", "final   Configuration   conf    =    getConf (  )  ;", "try    {", "while    ( job    !  =    null )     {", "final   Random   tempNameGenerator    =    new   Random (  )  ;", "lastJobSubmitTime    =    job . getSubmitTime (  )  ;", "+  + numberJobs ;", "if    (  ( job . getSubmitTime (  )  )     >  =    currentIntervalEnd )     {", "if    ( tempGen    !  =    null )     {", "tempGen . close (  )  ;", "}", "nextSegment    =    null ;", "for    ( int   i    =     0  ;     ( i    <     3  )     &  &     ( nextSegment    =  =    null )  ;     +  + i )     {", "try    {", "nextSegment    =    new   Path ( tempDir ,     (  (  \" segment -  \"     +     ( tempNameGenerator . nextLong (  )  )  )     +     \"  . json . gz \"  )  )  ;", "if    ( debug )     {", ". LOG . debug (  (  \" The   next   segment   name   is    \"     +    nextSegment )  )  ;", "}", "FileSystem   fs    =    nextSegment . getFileSystem ( conf )  ;", "try    {", "if    (  !  ( fs . exists ( nextSegment )  )  )     {", "break ;", "}", "continue ;", "}    catch    ( IOException   e )     {", "}", "}    catch    ( IOException   e )     {", "}", "}", "if    ( nextSegment    =  =    null )     {", "throw   new   RuntimeException (  \" Failed   to   create   a   new   file !  \"  )  ;", "}", "if    ( debug )     {", ". LOG . debug (  (  (  (  \" Creating    \"     +    nextSegment )     +     \"    for   a   job   with   a   submit   time   of    \"  )     +     ( job . getSubmitTime (  )  )  )  )  ;", "}", "deletees . add ( nextSegment )  ;", "tempPaths . add ( nextSegment )  ;", "tempGen    =    new   DefaultOutputter < LoggedJob >  (  )  ;", "tempGen . init ( nextSegment ,    conf )  ;", "long   currentIntervalNumber    =     (  ( job . getSubmitTime (  )  )     -     ( firstJobSubmitTime )  )     /     ( inputCycle )  ;", "currentIntervalEnd    =     ( firstJobSubmitTime )     +     (  ( currentIntervalNumber    +     1  )     *     ( inputCycle )  )  ;", "}", "if    ( tempGen    !  =    null )     {", "tempGen . output ( job )  ;", "}", "job    =    reader . nextJob (  )  ;", "}", "}    catch    ( DeskewedJobTraceReader . OutOfOrderException   e )     {", "return    . OUT _ OF _ ORDER _ JOBS ;", "}    finally    {", "if    ( tempGen    !  =    null )     {", "tempGen . close (  )  ;", "}", "}", "if    ( lastJobSubmitTime    <  =     ( firstJobSubmitTime )  )     {", ". LOG . error (  (  \" All   of   your   job [ s ]    have   the   same   submit   time .  \"     +     \"       Please   just   use   your   input   file .  \"  )  )  ;", "return    . ALL _ JOBS _ SIMULTANEOUS ;", "}", "double   submitTimeSpan    =    lastJobSubmitTime    -     ( firstJobSubmitTime )  ;", ". LOG . warn (  (  (  \" Your   input   trace   spans    \"     +     ( lastJobSubmitTime    -     ( firstJobSubmitTime )  )  )     +     \"    ticks .  \"  )  )  ;", "double   foldingRatio    =     (  ( submitTimeSpan    *     ( numberJobs    +     1  )  )     /    numberJobs )     /     ( inputCycle )  ;", "if    ( debug )     {", ". LOG . warn (  (  (  (  (  (  \" run :    submitTimeSpan    =     \"     +    submitTimeSpan )     +     \"  ,    numberJobs    =     \"  )     +    numberJobs )     +     \"  ,    inputCycle    =     \"  )     +     ( inputCycle )  )  )  ;", "}", "if    (  ( reader . neededSkewBufferSize (  )  )     >     0  )     {", ". LOG . warn (  (  (  \" You   needed   a    - skew - buffer - length   of    \"     +     ( reader . neededSkewBufferSize (  )  )  )     +     \"    but   no   more ,    for   this   input .  \"  )  )  ;", "}", "double   tProbability    =     (  ( timeDilation )     *     ( concentration )  )     /    foldingRatio ;", "if    ( debug )     {", ". LOG . warn (  (  (  (  (  (  \" run :    timeDilation    =     \"     +     ( timeDilation )  )     +     \"  ,    concentration    =     \"  )     +     ( concentration )  )     +     \"  ,    foldingRatio    =     \"  )     +    foldingRatio )  )  ;", ". LOG . warn (  (  \" The   transcription   probability   is    \"     +    tProbability )  )  ;", "}", "transcriptionRateInteger    =     (  ( int )     ( Math . floor ( tProbability )  )  )  ;", "transcriptionRateFraction    =    tProbability    -     ( Math . floor ( tProbability )  )  ;", "heap    =    new   PriorityQueue < Pair < LoggedJob ,    JobTraceReader >  >  ( tempPaths . size (  )  ,    new   JobEntryComparator (  )  )  ;", "for    ( Path   tempPath    :    tempPaths )     {", "JobTraceReader   thisReader    =    new   JobTraceReader ( tempPath ,    conf )  ;", "closees . add ( thisReader )  ;", "LoggedJob   streamFirstJob    =    thisReader . getNext (  )  ;", "long   thisIndex    =     (  ( streamFirstJob . getSubmitTime (  )  )     -     ( firstJobSubmitTime )  )     /     ( inputCycle )  ;", "if    ( debug )     {", ". LOG . debug (  (  (  (  \" A   job   with   submit   time   of    \"     +     ( streamFirstJob . getSubmitTime (  )  )  )     +     \"    is   in   interval    #     \"  )     +    thisIndex )  )  ;", "}", "adjustJobTimes ( streamFirstJob )  ;", "if    ( debug )     {", ". LOG . debug (  (  \" That   job ' s   submit   time   is   adjusted   to    \"     +     ( streamFirstJob . getSubmitTime (  )  )  )  )  ;", "}", "heap . add ( new   Pair < LoggedJob ,    JobTraceReader >  ( streamFirstJob ,    thisReader )  )  ;", "}", "Pair < LoggedJob ,    JobTraceReader >    next    =    heap . poll (  )  ;", "while    ( next    !  =    null )     {", "maybeOutput ( next . first (  )  )  ;", "if    ( debug )     {", ". LOG . debug (  (  \" The   most   recent   job   has   an   adjusted   submit   time   of    \"     +     ( next . first (  )  . getSubmitTime (  )  )  )  )  ;", ". LOG . debug (  (  \"    Its   replacement   in   the   heap   will   come   from   input   engine    \"     +     ( next . second (  )  )  )  )  ;", "}", "LoggedJob   replacement    =    next . second (  )  . getNext (  )  ;", "if    ( replacement    =  =    null )     {", "next . second (  )  . close (  )  ;", "if    ( debug )     {", ". LOG . debug (  \" That   input   engine   is   depleted .  \"  )  ;", "}", "} else    {", "adjustJobTimes ( replacement )  ;", "if    ( debug )     {", ". LOG . debug (  (  \" The   replacement   has   an   adjusted   submit   time   of    \"     +     ( replacement . getSubmitTime (  )  )  )  )  ;", "}", "heap . add ( new   Pair < LoggedJob ,    JobTraceReader >  ( replacement ,    next . second (  )  )  )  ;", "}", "next    =    heap . poll (  )  ;", "}", "}    finally    {", "IOUtils . cleanup ( null ,    reader )  ;", "if    (  ( outGen )     !  =    null )     {", "outGen . close (  )  ;", "}", "for    ( Pair < LoggedJob ,    JobTraceReader >    heapEntry    :    heap )     {", "heapEntry . second (  )  . close (  )  ;", "}", "for    ( Closeable   closee    :    closees )     {", "closee . close (  )  ;", "}", "if    (  !  ( debug )  )     {", "Configuration   conf    =    getConf (  )  ;", "for    ( Path   deletee    :    deletees )     {", "FileSystem   fs    =    deletee . getFileSystem ( conf )  ;", "try    {", "fs . delete ( deletee ,    false )  ;", "}    catch    ( IOException   e )     {", "}", "}", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.rumen.Folder"}, {"methodBody": ["METHOD_START", "{", "try    {", "LineReader   reader    =    new   LineReader ( input )  ;", "Text   buffer    =    new   Text (  )  ;", "return    (  ( reader . readLine ( buffer )  )     !  =     0  )     &  &     ( bufferString (  )  . equals (  \" Meta   VERSION =  \\  \"  1  \\  \"     .  \"  )  )  ;", "}    catch    ( EOFException   e )     {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["canParse"], "fileName": "org.apache.hadoop.tools.rumen.Hadoop20JHParser"}, {"methodBody": ["METHOD_START", "{", "HistoryEventEmitter   result    =    liveEmitters . get ( name )  ;", "if    ( result    =  =    null )     {", "result    =    type . createEmitter (  )  ;", "liveEmitters . put ( name ,    result )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["findOrMakeEmitter"], "fileName": "org.apache.hadoop.tools.rumen.Hadoop20JHParser"}, {"methodBody": ["METHOD_START", "{", "String   line    =    getOneLine (  )  ;", "while    (  ( line . length (  )  )     <     (  . endLineString . length (  )  )  )     {", "line    =    getOneLine (  )  ;", "}", "if    ( line . endsWith (  . endLineString )  )     {", "return   line ;", "}", "StringBuilder   sb    =    new   StringBuilder ( line )  ;", "String   addedLine ;", "do    {", "addedLine    =    getOneLine (  )  ;", "if    ( addedLine    =  =    null )     {", "return   sb . toString (  )  ;", "}", "sb . append (  \"  \\ n \"  )  ;", "sb . append ( addedLine )  ;", "}    while    (  (  ( addedLine . length (  )  )     <     (  . endLineString . length (  )  )  )     |  |     (  !  (  . endLineString . equals ( addedLine . substring (  (  ( addedLine . length (  )  )     -     (  . endLineString . length (  )  )  )  )  )  )  )     )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getFullLine"], "fileName": "org.apache.hadoop.tools.rumen.Hadoop20JHParser"}, {"methodBody": ["METHOD_START", "{", "Text   resultText    =    new   Text (  )  ;", "if    (  ( reader . readLine ( resultText )  )     =  =     0  )     {", "throw   new   EOFException (  \" apparent   bad   line \"  )  ;", "}", "return   resultTextString (  )  ;", "}", "METHOD_END"], "methodName": ["getOneLine"], "fileName": "org.apache.hadoop.tools.rumen.Hadoop20JHParser"}, {"methodBody": ["METHOD_START", "{", "return   HadoopLogsAnalyzer . confFileHeader . matcher ( header )  . find (  )  ;", "}", "METHOD_END"], "methodName": ["apparentConfFileHeader"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "return   HadoopLogsAnalyzer . xmlFilePrefix . matcher ( line )  . lookingAt (  )  ;", "}", "METHOD_END"], "methodName": ["apparentXMLFileStart"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "getDistribution ( block ,    outcome ,    type )  . enter ( value )  ;", "getDistribution ( block ,     . JobOutcome . OVERALL ,    type )  . enter ( value )  ;", "getDistribution ( block ,    outcome ,    LoggedJob . JobType . OVERALL )  . enter ( value )  ;", "getDistribution ( block ,     . JobOutcome . OVERALL ,    LoggedJob . JobType . OVERALL )  . enter ( value )  ;", "}", "METHOD_END"], "methodName": ["canonicalDistributionsEnter"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "Pattern   result    =    counterPatterns . get ( counterName )  ;", "if    ( result    =  =    null )     {", "String   namePatternRegex    =     (  \"  \\  \\  [  \\  \\  (  \"     +    counterName )     +     \"  \\  \\  )  \\  \\  (  [  ^  )  ]  +  \\  \\  )  \\  \\  (  (  [  0  -  9  ]  +  )  \\  \\  )  \\  \\  ]  \"  ;", "result    =    Pattern . compile ( namePatternRegex )  ;", "counterPatterns . put ( counterName ,    result )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["counterPattern"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( jobBeingTraced )     !  =    null )     {", "if    ( omitTaskDetails )     {", "jobBeingTraced . setMapTasks ( null )  ;", "jobBeingTraced . setReduceTasks ( null )  ;", "jobBeingTraced . setOtherTasks ( null )  ;", "}", "jobBeingTraced . setSuccessfulMapAttemptCDFs ( mapCDFArrayList ( successfulMapAttemptTimes )  )  ;", "jobBeingTraced . setFailedMapAttemptCDFs ( mapCDFArrayList ( failedMapAttemptTimes )  )  ;", "gedDiscreteCDF   discCDF    =    new   gedDiscreteCDF (  )  ;", "discCDF . setCDF ( successfulReduceAttemptTimes ,    attemptTimesPercentiles ,     1  0  0  )  ;", "jobBeingTraced . setSuccessfulReduceAttemptCDF ( discCDF )  ;", "discCDF    =    new   gedDiscreteCDF (  )  ;", "discCDF . setCDF ( failedReduceAttemptTimes ,    attemptTimesPercentiles ,     1  0  0  )  ;", "jobBeingTraced . setFailedReduceAttemptCDF ( discCDF )  ;", "long   totalSuccessfulAttempts    =     0 L ;", "long   maxTriesToSucceed    =     0 L ;", "for    ( Map . Entry < Long ,    Long >    ent    :    successfulNthMapperAttempts )     {", "totalSuccessfulAttempts    +  =    ent . getValue (  )  ;", "maxTriesToSucceed    =    Math . max ( maxTriesToSucceed ,    ent . getKey (  )  )  ;", "}", "if    ( totalSuccessfulAttempts    >     0 L )     {", "double [  ]    successAfterI    =    new   double [  (  ( int )     ( maxTriesToSucceed )  )     +     1  ]  ;", "for    ( int   i    =     0  ;    i    <     ( successAfterI . length )  ;     +  + i )     {", "successAfterI [ i ]     =     0  .  0  ;", "}", "for    ( Map . Entry < Long ,    Long >    ent    :    successfulNthMapperAttempts )     {", "successAfterI [ ent . getKey (  )  . intValue (  )  ]     =     (  ( double )     ( ent . getValue (  )  )  )     /    totalSuccessfulAttempts ;", "}", "jobBeingTraced . setMapperTriesToSucceed ( successAfterI )  ;", "} else    {", "jobBeingTraced . setMapperTriesToSucceed ( null )  ;", "}", "jobTraceGen . output ( jobBeingTraced )  ;", "jobBeingTraced    =    null ;", "}", "}", "METHOD_END"], "methodName": ["finalizeJob"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "ParsedHost   result    =    ParsedHost . parse ( hostName )  ;", "if    (  ( result    !  =    null )     &  &     (  !  ( allHosts . contains ( result )  )  )  )     {", "allHosts . add ( result )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["getAndRecordParsedHost"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "return   block [ outcome . ordinal (  )  ]  [ type . ordinal (  )  ]  ;", "}", "METHOD_END"], "methodName": ["getDistribution"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "String   valueString    =    parseCounter ( counterString ,    counterName )  ;", "if    ( valueString    !  =    null )     {", "thunk . set ( Long . parseLong ( valueString )  )  ;", "}", "}", "METHOD_END"], "methodName": ["incorporateCounter"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . hdfsBytesRead    =    val ;", "}", "}  ,    counterString ,     \" HDFS _ BYTES _ READ \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . hdfsBytesWritten    =    val ;", "}", "}  ,    counterString ,     \" HDFS _ BYTES _ WRITTEN \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . fileBytesRead    =    val ;", "}", "}  ,    counterString ,     \" FILE _ BYTES _ READ \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . fileBytesWritten    =    val ;", "}", "}  ,    counterString ,     \" FILE _ BYTES _ WRITTEN \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapInputBytes    =    val ;", "}", "}  ,    counterString ,     \" MAP _ INPUT _ BYTES \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapInputRecords    =    val ;", "}", "}  ,    counterString ,     \" MAP _ INPUT _ RECORDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapOutputBytes    =    val ;", "}", "}  ,    counterString ,     \" MAP _ OUTPUT _ BYTES \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapOutputRecords    =    val ;", "}", "}  ,    counterString ,     \" MAP _ OUTPUT _ RECORDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . combineInputRecords    =    val ;", "}", "}  ,    counterString ,     \" COMBINE _ INPUT _ RECORDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceInputGroups    =    val ;", "}", "}  ,    counterString ,     \" REDUCE _ INPUT _ GROUPS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceInputRecords    =    val ;", "}", "}  ,    counterString ,     \" REDUCE _ INPUT _ RECORDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceShuffleBytes    =    val ;", "}", "}  ,    counterString ,     \" REDUCE _ SHUFFLE _ BYTES \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceOutputRecords    =    val ;", "}", "}  ,    counterString ,     \" REDUCE _ OUTPUT _ RECORDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . spilledRecords    =    val ;", "}", "}  ,    counterString ,     \" SPILLED _ RECORDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . getResourceUsageMetrics (  )  . setCumulativeCpuUsage ( val )  ;", "}", "}  ,    counterString ,     \" CPU _ MILLISECONDS \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . getResourceUsageMetrics (  )  . setVirtualMemoryUsage ( val )  ;", "}", "}  ,    counterString ,     \" VIRTUAL _ MEMORY _ BYTES \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . getResourceUsageMetrics (  )  . setPhysicalMemoryUsage ( val )  ;", "}", "}  ,    counterString ,     \" PHYSICAL _ MEMORY _ BYTES \"  )  ;", "incorporateCounter ( new   HadoopLogsAnalyzer . SetField ( attempt 2  )     {", "@ Override", "void   set ( long   val )     {", "attempt . getResourceUsageMetrics (  )  . setHeapUsage ( val )  ;", "}", "}  ,    counterString ,     \" COMMITTED _ HEAP _ BYTES \"  )  ;", "}", "METHOD_END"], "methodName": ["incorporateCounters"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( spreading )  )     {", "return ;", "}", "if    (  ( taskTimes . getTotalCount (  )  )     <  =     1  )     {", "return ;", "}", "int [  ]    endpoints    =    new   int [  2  ]  ;", "endpoints [  0  ]     =    spreadMin ;", "endpoints [  1  ]     =    spreadMax ;", "long [  ]    endpointKeys    =    taskTimes . getCDF (  1  0  0  0  ,    endpoints )  ;", "int   smallResultOffset    =     (  ( taskTimes . getTotalCount (  )  )     <     (  . SMALL _ SPREAD _ COMPENSATION _ THRESHOLD )  )     ?     1     :     0  ;", "Histogram   myTotal    =    spreadTo [ outcome . ordinal (  )  ]  [ jtype . ordinal (  )  ]  ;", "long   dividend    =    endpointKeys [  (  2     +    smallResultOffset )  ]  ;", "long   divisor    =    endpointKeys [  (  1     -    smallResultOffset )  ]  ;", "if    ( divisor    >     0  )     {", "long   mytotalRatio    =     ( dividend    *     1  0  0  0  0  0  0 L )     /    divisor ;", "myTotal . enter ( mytotalRatio )  ;", "}", "}", "METHOD_END"], "methodName": ["incorporateSpread"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "Path   jobTraceFilename    =    null ;", "Path   topologyFilename    =    null ;", "if    (  (  ( args . length )     =  =     0  )     |  |     (  ( args [  (  ( args . length )     -     1  )  ]  . charAt (  0  )  )     =  =     '  -  '  )  )     {", "throw   new   IllegalArgumentException (  \" No   input   specified .  \"  )  ;", "} else    {", "inputFilename    =    args [  (  ( args . length )     -     1  )  ]  ;", "}", "for    ( int   i    =     0  ;    i    <     (  ( args . length )     -     (  ( inputFilename )     =  =    null    ?     0     :     1  )  )  ;     +  + i )     {", "if    (  (  \"  - h \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     |  |     (  \"  - help \"  . equals ( args [ i ]  . toLowerCase (  )  )  )  )     {", "usage (  )  ;", "return    0  ;", "}", "if    (  (  \"  - c \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     |  |     (  \"  - collect - prefixes \"  . equals ( args [ i ]  . toLowerCase (  )  )  )  )     {", "collecting    =    true ;", "continue ;", "}", "if    (  \"  - write - job - trace \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "+  + i ;", "jobTraceFilename    =    new   Path ( args [ i ]  )  ;", "continue ;", "}", "if    (  \"  - single - line - job - traces \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "prettyprintTrace    =    false ;", "continue ;", "}", "if    (  \"  - omit - task - details \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "omitTaskDetails    =    true ;", "continue ;", "}", "if    (  \"  - write - topology \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "+  + i ;", "topologyFilename    =    new   Path ( args [ i ]  )  ;", "continue ;", "}", "if    (  \"  - job - digest - spectra \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "ArrayList < Integer >    values    =    new   ArrayList < Integer >  (  )  ;", "+  + i ;", "while    (  ( i    <     ( args . length )  )     &  &     ( Character . isDigit ( args [ i ]  . charAt (  0  )  )  )  )     {", "values . add ( Integer . parseInt ( args [ i ]  )  )  ;", "+  + i ;", "}", "if    (  ( values . size (  )  )     =  =     0  )     {", "throw   new   IllegalArgumentException (  \" Empty    - job - digest - spectra   list \"  )  ;", "}", "attemptTimesPercentiles    =    new   int [ values . size (  )  ]  ;", "int   lastValue    =     0  ;", "for    ( int   j    =     0  ;    j    <     ( attemptTimesPercentiles . length )  ;     +  + j )     {", "if    (  (  ( values . get ( j )  )     <  =    lastValue )     |  |     (  ( values . get ( j )  )     >  =     1  0  0  )  )     {", "throw   new   IllegalArgumentException (  \" Bad    - job - digest - spectra   percentiles   list \"  )  ;", "}", "attemptTimesPercentiles [ j ]     =    values . get ( j )  ;", "}", "-  - i ;", "continue ;", "}", "if    (  (  \"  - d \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     |  |     (  \"  - debug \"  . equals ( args [ i ]  . toLowerCase (  )  )  )  )     {", "debug    =    true ;", "continue ;", "}", "if    (  \"  - spreads \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "int   min    =    Integer . parseInt ( args [  ( i    +     1  )  ]  )  ;", "int   max    =    Integer . parseInt ( args [  ( i    +     2  )  ]  )  ;", "if    (  (  ( min    <    max )     &  &     ( min    <     1  0  0  0  )  )     &  &     ( max    <     1  0  0  0  )  )     {", "spreadMin    =    min ;", "spreadMax    =    max ;", "spreading    =    true ;", "i    +  =     2  ;", "}", "continue ;", "}", "if    (  \"  - delays \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "delays    =    true ;", "continue ;", "}", "if    (  \"  - runtimes \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "runtimes    =    true ;", "continue ;", "}", "if    (  \"  - tasktimes \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "collectTaskTimes    =    true ;", "continue ;", "}", "if    (  \"  - v 1  \"  . equals ( args [ i ]  . toLowerCase (  )  )  )     {", "version    =     1  ;", "continue ;", "}", "throw   new   IllegalArgumentException (  (  \" Unrecognized   argument :     \"     +     ( args [ i ]  )  )  )  ;", "}", "runTimeDists    =    newDistributionBlock (  )  ;", "delayTimeDists    =    newDistributionBlock (  )  ;", "mapTimeSpreadDists    =    newDistributionBlock (  \" map - time - spreads \"  )  ;", "shuffleTimeSpreadDists    =    newDistributionBlock (  )  ;", "sortTimeSpreadDists    =    newDistributionBlock (  )  ;", "reduceTimeSpreadDists    =    newDistributionBlock (  )  ;", "mapTimeDists    =    newDistributionBlock (  )  ;", "shuffleTimeDists    =    newDistributionBlock (  )  ;", "sortTimeDists    =    newDistributionBlock (  )  ;", "reduceTimeDists    =    newDistributionBlock (  )  ;", "taskAttemptStartTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskReduceAttemptShuffleEndTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskReduceAttemptSortEndTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskMapAttemptFinishTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskReduceAttemptFinishTimes    =    new   HashMap < String ,    Long >  (  )  ;", "final   Path   inputPath    =    new   Path ( inputFilename )  ;", "inputIsDirectory    =    pathIsDirectory ( inputPath )  ;", "if    (  ( jobTraceFilename    !  =    null )     &  &     (  ( attemptTimesPercentiles )     =  =    null )  )     {", "attemptTimesPercentiles    =    new   int [  1  9  ]  ;", "for    ( int   i    =     0  ;    i    <     1  9  ;     +  + i )     {", "attemptTimesPercentiles [ i ]     =     ( i    +     1  )     *     5  ;", "}", "}", "if    (  !  ( inputIsDirectory )  )     {", "input    =    maybeUncompressedPath ( inputPath )  ;", "} else    {", "inputDirectoryPath    =    inputPath ;", "FileSystem   fs    =    inputPath . getFileSystem ( getConf (  )  )  ;", "FileStatus [  ]    statuses    =    fs . listStatus ( inputPath )  ;", "inputDirectoryFiles    =    new   String [ statuses . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( statuses . length )  ;     +  + i )     {", "inputDirectoryFiles [ i ]     =    statuses [ i ]  . getPath (  )  . getName (  )  ;", "}", "int   dropPoint    =     0  ;", "for    ( int   i    =     0  ;    i    <     ( inputDirectoryFiles . length )  ;     +  + i )     {", "String   name    =    inputDirectoryFiles [ i ]  ;", "if    (  !  (  (  ( name . length (  )  )     >  =     4  )     &  &     (  \"  . crc \"  . equals ( name . substring (  (  ( name . length (  )  )     -     4  )  )  )  )  )  )     {", "inputDirectoryFiles [  ( dropPoint +  +  )  ]     =    name ;", "}", "}", ". LOG . info (  (  (  \" We   dropped    \"     +     (  ( inputDirectoryFiles . length )     -    dropPoint )  )     +     \"    crc   files .  \"  )  )  ;", "String [  ]    new _ inputDirectoryFiles    =    new   String [ dropPoint ]  ;", "System . arraycopy ( inputDirectoryFiles ,     0  ,    new _ inputDirectoryFiles ,     0  ,    dropPoint )  ;", "inputDirectoryFiles    =    new _ inputDirectoryFiles ;", "Arrays . sort ( inputDirectoryFiles )  ;", "if    (  !  ( setNextDirectoryInputStream (  )  )  )     {", "throw   new   FileNotFoundException (  \" Empty   directory   specified .  \"  )  ;", "}", "}", "if    ( jobTraceFilename    !  =    null )     {", "jobTraceGen    =    new   DefaultOutputter < LoggedJob >  (  )  ;", "jobTraceGen . init ( jobTraceFilename ,    getConf (  )  )  ;", "if    ( topologyFilename    !  =    null )     {", "topologyGen    =    new   DefaultOutputter < LoggedNetworkTopology >  (  )  ;", "topologyGen . init ( topologyFilename ,    getConf (  )  )  ;", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["initializeHadoopLogsAnalyzer"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "try    {", "analyzer    =    new    (  )  ;", "int   result    =    ToolRunner . run ( analyzer ,    args )  ;", "if    ( result    =  =     0  )     {", "return ;", "}", "System . exit ( result )  ;", "}    catch    ( FileNotFoundException   e )     {", ". LOG . error (  \"  \"  ,    e )  ;", "e . printStackTrace (  . staticDebugOutput )  ;", "System . exit (  1  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \"  \"  ,    e )  ;", "e . printStackTrace (  . staticDebugOutput )  ;", "System . exit (  2  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \"  \"  ,    e )  ;", "e . printStackTrace (  . staticDebugOutput )  ;", "System . exit (  3  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "ArrayList < LoggedDiscreteCDF >    result    =    new   ArrayList < LoggedDiscreteCDF >  (  )  ;", "for    ( Histogram   hist    :    data )     {", "LoggedDiscreteCDF   discCDF    =    new   LoggedDiscreteCDF (  )  ;", "discCDF . setCDF ( hist ,    attemptTimesPercentiles ,     1  0  0  )  ;", "result . add ( discCDF )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["mapCDFArrayList"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  ( jobBeingTraced )     !  =    null )     &  &     (  ( jobconf )     !  =    null )  )     &  &     ( jobBeingTraced . getJobID (  )  . toString (  )  . equals ( jobconf . jobID )  )  )     {", "jobBeingTraced . setHeapMegabytes ( jobconf . heapMegabytes )  ;", "jobBeingTraced . setQueue ( jobconf . queue )  ;", "jobBeingTraced . setJobName ( jobconf . jobName )  ;", "jobBeingTraced . setClusterMapMB ( jobconf . clusterMapMB )  ;", "jobBeingTraced . setClusterReduceMB ( jobconf . clusterReduceMB )  ;", "jobBeingTraced . setJobMapMB ( jobconf . jobMapMB )  ;", "jobBeingTraced . setJobReduceMB ( jobconf . jobReduceMB )  ;", "jobBeingTraced . setJobProperties ( jobconf . properties )  ;", "jobconf    =    null ;", "finalizeJob (  )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeMateJobAndConf"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "CompressionCodecFactory   codecs    =    new   CompressionCodecFactory ( getConf (  )  )  ;", "inputCodec    =    codecs . getCodec ( p )  ;", "FileSystem   fs    =    p . getFileSystem ( getConf (  )  )  ;", "FSDataInputStream   fileIn    =    fs . open ( p )  ;", "if    (  ( inputCodec )     =  =    null )     {", "return   new   util . LineReader ( fileIn ,    getConf (  )  )  ;", "} else    {", "inputDecompressor    =    CodecPool . getDecompressor ( inputCodec )  ;", "return   new   util . LineReader ( inputCodec . createInputStream ( fileIn ,    inputDecompressor )  ,    getConf (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeUncompressedPath"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "return   newDistributionBlock ( null )  ;", "}", "METHOD_END"], "methodName": ["newDistributionBlock"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "Histogram [  ]  [  ]    result    =    new   Histogram [ HadoopLogsAnalyzer . JobOutcome . values (  )  . length ]  [  ]  ;", "for    ( int   i    =     0  ;    i    <     ( HadoopLogsAnalyzer . JobOutcome . values (  )  . length )  ;     +  + i )     {", "result [ i ]     =    new   Histogram [ LoggedJob . JobType . values (  )  . length ]  ;", "for    ( int   j    =     0  ;    j    <     ( LoggedJob . JobType . values (  )  . length )  ;     +  + j )     {", "result [ i ]  [ j ]     =     ( blockname    =  =    null )     ?    new   Histogram (  )     :    new   Histogram ( blockname )  ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["newDistributionBlock"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    ( counterString    =  =    null )     {", "return   null ;", "}", "Matr   mat    =    counterPattern ( counterName )  . matr ( counterString )  ;", "if    ( mat . find (  )  )     {", "return   mat . group (  1  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["parseCounter"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    p . getFileSystem ( getConf (  )  )  ;", "return   fs . getFileStatus ( p )  . isDirectory (  )  ;", "}", "METHOD_END"], "methodName": ["pathIsDirectory"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "statisticalOutput . print (  ( title    +     \"  \\ n \\ n \"  )  )  ;", "for    ( int   i    =     0  ;    i    <     (  . JobOutcome . values (  )  . length )  ;     +  + i )     {", "for    ( int   j    =     0  ;    j    <     ( LoggedJob . JobType . values (  )  . length )  ;     +  + j )     {", ". JobOutcome   thisOutcome    =     . JobOutcome . values (  )  [ i ]  ;", "LoggedJob . JobType   thisType    =    LoggedJob . JobType . values (  )  [ j ]  ;", "statisticalOutput . print (  \" outcome    =     \"  )  ;", "statisticalOutput . print ( thisOutcome . toString (  )  )  ;", "statisticalOutput . print (  \"  ,    and   type    =     \"  )  ;", "statisticalOutput . print ( thisType . toString (  )  )  ;", "statisticalOutput . print (  \"  .  \\ n \\ n \"  )  ;", "Histogram   dist    =    distSet [ i ]  [ j ]  ;", "printSingleDistributionData ( dist )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["printDistributionSet"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "int [  ]    percentiles    =    new   int [ numberBuckets ]  ;", "for    ( int   k    =     0  ;    k    <     ( numberBuckets )  ;     +  + k )     {", "percentiles [ k ]     =    k    +     1  ;", "}", "long [  ]    cdf    =    dist . getCDF (  (  ( numberBuckets )     +     1  )  ,    percentiles )  ;", "if    ( cdf    =  =    null )     {", "statisticalOutput . print (  \"  ( No   data )  \\ n \"  )  ;", "} else    {", "statisticalOutput . print (  \" min :        \"  )  ;", "statisticalOutput . print ( cdf [  0  ]  )  ;", "statisticalOutput . print (  \"  \\ n \"  )  ;", "for    ( int   k    =     0  ;    k    <     ( numberBuckets )  ;     +  + k )     {", "statisticalOutput . print ( percentiles [ k ]  )  ;", "statisticalOutput . print (  \"  %           \"  )  ;", "statisticalOutput . print ( cdf [  ( k    +     1  )  ]  )  ;", "statisticalOutput . print (  \"  \\ n \"  )  ;", "}", "statisticalOutput . print (  \" max :        \"  )  ;", "statisticalOutput . print ( cdf [  (  ( numberBuckets )     +     1  )  ]  )  ;", "statisticalOutput . print (  \"  \\ n \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["printSingleDistributionData"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  (  ( version )     =  =     0  )     |  |     (  ( version )     =  =     1  )  )     {", "String   jobID    =    line . get (  \" JOBID \"  )  ;", "String   user    =    line . get (  \" USER \"  )  ;", "String   jobPriority    =    line . get (  \" JOB _ PRIORITY \"  )  ;", "String   submitTime    =    line . get (  \" SUBMIT _ TIME \"  )  ;", "String   jobName    =    line . get (  \" JOBNAME \"  )  ;", "String   launchTime    =    line . get (  \" LAUNCH _ TIME \"  )  ;", "String   finishTime    =    line . get (  \" FINISH _ TIME \"  )  ;", "String   status    =    line . get (  \" JOB _ STATUS \"  )  ;", "String   totalMaps    =    line . get (  \" TOTAL _ MAPS \"  )  ;", "String   totalReduces    =    line . get (  \" TOTAL _ REDUCES \"  )  ;", "if    (  (  ( jobID    !  =    null )     &  &     (  ( jobTraceGen )     !  =    null )  )     &  &     (  (  ( jobBeingTraced )     =  =    null )     |  |     (  !  ( jobID . equals ( jobBeingTraced . getJobID (  )  . toString (  )  )  )  )  )  )     {", "finalizeJob (  )  ;", "jobBeingTraced    =    new   LoggedJob ( jobID )  ;", "tasksInCurrentJob    =    new   HashMap < String ,    LoggedTask >  (  )  ;", "attemptsInCurrentJob    =    new   HashMap < String ,    LoggedTaskAttempt >  (  )  ;", "successfulMapAttemptTimes    =    new   Histogram [  ( ParsedHost . numberOfDistances (  )  )     +     1  ]  ;", "for    ( int   i    =     0  ;    i    <     ( successfulMapAttemptTimes . length )  ;     +  + i )     {", "successfulMapAttemptTimes [ i ]     =    new   Histogram (  )  ;", "}", "successfulReduceAttemptTimes    =    new   Histogram (  )  ;", "failedMapAttemptTimes    =    new   Histogram [  ( ParsedHost . numberOfDistances (  )  )     +     1  ]  ;", "for    ( int   i    =     0  ;    i    <     ( failedMapAttemptTimes . length )  ;     +  + i )     {", "failedMapAttemptTimes [ i ]     =    new   Histogram (  )  ;", "}", "failedReduceAttemptTimes    =    new   Histogram (  )  ;", "successfulNthMapperAttempts    =    new   Histogram (  )  ;", "successfulNthReducerAttempts    =    new   Histogram (  )  ;", "mapperLocality    =    new   Histogram (  )  ;", "}", "if    (  ( jobBeingTraced )     !  =    null )     {", "if    ( user    !  =    null )     {", "jobBeingTraced . setUser ( user )  ;", "}", "if    ( jobPriority    !  =    null )     {", "jobBeingTraced . setPriority ( LoggedJob . JobPriority . valueOf ( jobPriority )  )  ;", "}", "if    ( totalMaps    !  =    null )     {", "jobBeingTraced . setTotalMaps ( Integer . parseInt ( totalMaps )  )  ;", "}", "if    ( totalReduces    !  =    null )     {", "jobBeingTraced . setTotalReduces ( Integer . parseInt ( totalReduces )  )  ;", "}", "if    ( submitTime    !  =    null )     {", "jobBeingTraced . setSubmitTime ( Long . parseLong ( submitTime )  )  ;", "}", "if    ( launchTime    !  =    null )     {", "jobBeingTraced . setLaunchTime ( Long . parseLong ( launchTime )  )  ;", "}", "if    ( finishTime    !  =    null )     {", "jobBeingTraced . setFinishTime ( Long . parseLong ( finishTime )  )  ;", "if    ( status    !  =    null )     {", "jobBeingTraced . setOutcome ( Pre 2  1 JobHistoryConstants . Values . valueOf ( status )  )  ;", "}", "maybeMateJobAndConf (  )  ;", "}", "}", "if    ( jobName    !  =    null )     {", "Matcher   m    =     . streamingJobnamePattern . matcher ( jobName )  ;", "thisJobType    =    LoggedJob . JobType . JAVA ;", "if    ( m . matches (  )  )     {", "thisJobType    =    LoggedJob . JobType . STREAMING ;", "}", "}", "if    ( submitTime    !  =    null )     {", "submitTimeCurrentJob    =    Long . parseLong ( submitTime )  ;", "currentJobID    =    jobID ;", "taskAttemptStartTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskReduceAttemptShuffleEndTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskReduceAttemptSortEndTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskMapAttemptFinishTimes    =    new   HashMap < String ,    Long >  (  )  ;", "taskReduceAttemptFinishTimes    =    new   HashMap < String ,    Long >  (  )  ;", "launchTimeCurrentJob    =     0 L ;", "} else", "if    (  (  ( launchTime    !  =    null )     &  &     ( jobID    !  =    null )  )     &  &     ( currentJobID . equals ( jobID )  )  )     {", "launchTimeCurrentJob    =    Long . parseLong ( launchTime )  ;", "} else", "if    (  (  ( finishTime    !  =    null )     &  &     ( jobID    !  =    null )  )     &  &     ( currentJobID . equals ( jobID )  )  )     {", "long   endTime    =    Long . parseLong ( finishTime )  ;", "if    (  ( launchTimeCurrentJob )     !  =     0  )     {", "String   jobResultText    =    line . get (  \" JOB _ STATUS \"  )  ;", ". JobOutcome   thisOutcome    =     (  ( jobResultText    !  =    null )     &  &     (  \" SUCCESS \"  . equals ( jobResultText )  )  )     ?     . JobOutcome . SUCCESS    :     . JobOutcome . FAILURE ;", "if    (  ( submitTimeCurrentJob )     !  =     0 L )     {", "canonicalDistributionsEnter ( delayTimeDists ,    thisOutcome ,    thisJobType ,     (  ( launchTimeCurrentJob )     -     ( submitTimeCurrentJob )  )  )  ;", "}", "if    (  ( launchTimeCurrentJob )     !  =     0 L )     {", "canonicalDistributionsEnter ( runTimeDists ,    thisOutcome ,    thisJobType ,     ( endTime    -     ( launchTimeCurrentJob )  )  )  ;", "}", "Histogram   currentJobMapTimes    =    new   Histogram (  )  ;", "Histogram   currentJobShuffleTimes    =    new   Histogram (  )  ;", "Histogram   currentJobSortTimes    =    new   Histogram (  )  ;", "Histogram   currentJobReduceTimes    =    new   Histogram (  )  ;", "Iterator < Map . Entry < String ,    Long >  >    taskIter    =    taskAttemptStartTimes . entrySet (  )  . iterator (  )  ;", "while    ( taskIter . hasNext (  )  )     {", "Map . Entry < String ,    Long >    entry    =    taskIter . next (  )  ;", "long   startTime    =    entry . getValue (  )  ;", "Long   mapEndTime    =    taskMapAttemptFinishTimes . get ( entry . getKey (  )  )  ;", "if    ( mapEndTime    !  =    null )     {", "currentJobMapTimes . enter (  ( mapEndTime    -    startTime )  )  ;", "canonicalDistributionsEnter ( mapTimeDists ,    thisOutcome ,    thisJobType ,     ( mapEndTime    -    startTime )  )  ;", "}", "Long   shuffleEnd    =    taskReduceAttemptShuffleEndTimes . get ( entry . getKey (  )  )  ;", "Long   sortEnd    =    taskReduceAttemptSortEndTimes . get ( entry . getKey (  )  )  ;", "Long   reduceEnd    =    taskReduceAttemptFinishTimes . get ( entry . getKey (  )  )  ;", "if    (  (  ( shuffleEnd    !  =    null )     &  &     ( sortEnd    !  =    null )  )     &  &     ( reduceEnd    !  =    null )  )     {", "currentJobShuffleTimes . enter (  ( shuffleEnd    -    startTime )  )  ;", "currentJobSortTimes . enter (  ( sortEnd    -    shuffleEnd )  )  ;", "currentJobReduceTimes . enter (  ( reduceEnd    -    sortEnd )  )  ;", "canonicalDistributionsEnter ( shuffleTimeDists ,    thisOutcome ,    thisJobType ,     ( shuffleEnd    -    startTime )  )  ;", "canonicalDistributionsEnter ( sortTimeDists ,    thisOutcome ,    thisJobType ,     ( sortEnd    -    shuffleEnd )  )  ;", "canonicalDistributionsEnter ( reduceTimeDists ,    thisOutcome ,    thisJobType ,     ( reduceEnd    -    sortEnd )  )  ;", "}", "}", "incorporateSpread ( currentJobMapTimes ,    mapTimeSpreadDists ,    thisOutcome ,    thisJobType )  ;", "incorporateSpread ( currentJobShuffleTimes ,    shuffleTimeSpreadDists ,    thisOutcome ,    thisJobType )  ;", "incorporateSpread ( currentJobSortTimes ,    sortTimeSpreadDists ,    thisOutcome ,    thisJobType )  ;", "incorporateSpread ( currentJobReduceTimes ,    reduceTimeSpreadDists ,    thisOutcome ,    thisJobType )  ;", "}", "}", "}", "}    catch    ( NumberFormatException   e )     {", ". LOG . warn (  (  (  \"  . processJobLine :    bad   numerical   format ,    at   line    \"     +     ( lineNumber )  )     +     \"  .  \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["processJobLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "String   attemptID    =    line . get (  \" TASK _ ATTEMPT _ ID \"  )  ;", "String   taskID    =    line . get (  \" TASKID \"  )  ;", "String   status    =    line . get (  \" TASK _ STATUS \"  )  ;", "String   attemptStartTime    =    line . get (  \" START _ TIME \"  )  ;", "String   attemptFinishTime    =    line . get (  \" FINISH _ TIME \"  )  ;", "String   hostName    =    line . get (  \" HOSTNAME \"  )  ;", "String   counters    =    line . get (  \" COUNTERS \"  )  ;", "if    (  (  ( jobBeingTraced )     !  =    null )     &  &     ( taskID    !  =    null )  )     {", "LoggedTask   task    =    tasksInCurrentJob . get ( taskID )  ;", "if    ( task    =  =    null )     {", "task    =    new   LoggedTask (  )  ;", "task . setTaskID ( taskID )  ;", "jobBeingTraced . getMapTasks (  )  . add ( task )  ;", "tasksInCurrentJob . put ( taskID ,    task )  ;", "}", "task . setTaskID ( taskID )  ;", "LoggedTaskAttempt   attempt    =    attemptsInCurrentJob . get ( attemptID )  ;", "boolean   attemptAlreadyExists    =    attempt    !  =    null ;", "if    ( attempt    =  =    null )     {", "attempt    =    new   LoggedTaskAttempt (  )  ;", "attempt . setAttemptID ( attemptID )  ;", "}", "if    (  ! attemptAlreadyExists )     {", "attemptsInCurrentJob . put ( attemptID ,    attempt )  ;", "task . getAttempts (  )  . add ( attempt )  ;", "}", "Pre 2  1 JobHistoryConstants . Values   stat    =    null ;", "try    {", "stat    =     ( status    =  =    null )     ?    null    :    Pre 2  1 JobHistoryConstants . Values . valueOf ( status )  ;", "}    catch    ( IllegalArgumentException   e )     {", ". LOG . error (  (  (  \" A   map   attempt   status   you   don \\  ' t   know   about   is    \\  \"  \"     +    status )     +     \"  \\  \"  .  \"  )  ,    e )  ;", "stat    =    null ;", "}", "incorporateCounters ( attempt ,    counters )  ;", "attempt . setResult ( stat )  ;", "if    ( attemptStartTime    !  =    null )     {", "attempt . setStartTime ( Long . parseLong ( attemptStartTime )  )  ;", "}", "if    ( attemptFinishTime    !  =    null )     {", "attempt . setFinishTime ( Long . parseLong ( attemptFinishTime )  )  ;", "}", "int   distance    =    Integer . MAX _ VALUE ;", "if    ( hostName    !  =    null )     {", "ParsedHost   host    =    getAndRecordParsedHost ( hostName )  ;", "if    ( host    !  =    null )     {", "attempt . setHostName ( host . getNodeName (  )  ,    host . getRackName (  )  )  ;", "attempt . setLocation ( host . makeLoggedLocation (  )  )  ;", "} else    {", "attempt . setHostName ( hostName ,    null )  ;", "}", "List < LoggedLocation >    locs    =    task . getPreferredLocations (  )  ;", "if    (  ( host    !  =    null )     &  &     ( locs    !  =    null )  )     {", "for    ( LoggedLocation   loc    :    locs )     {", "ParsedHost   preferedLoc    =    new   ParsedHost ( loc )  ;", "distance    =    Math . min ( distance ,    preferedLoc . distance ( host )  )  ;", "}", "}", "mapperLocality . enter ( distance )  ;", "}", "distance    =    Math . min ( distance ,     (  ( successfulMapAttemptTimes . length )     -     1  )  )  ;", "if    (  (  ( attempt . getStartTime (  )  )     >     0  )     &  &     (  ( attempt . getFinishTime (  )  )     >     0  )  )     {", "long   runtime    =     ( attempt . getFinishTime (  )  )     -     ( attempt . getStartTime (  )  )  ;", "if    ( stat    =  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )     {", "successfulMapAttemptTimes [ distance ]  . enter ( runtime )  ;", "}", "if    ( stat    =  =     ( Pre 2  1 JobHistoryConstants . Values . FAILED )  )     {", "failedMapAttemptTimes [ distance ]  . enter ( runtime )  ;", "}", "}", "if    ( attemptID    !  =    null )     {", "Matcher   matcher    =     . taskAttemptIDPattern . matcher ( attemptID )  ;", "if    ( matcher . matches (  )  )     {", "String   attemptNumberString    =    matcher . group (  1  )  ;", "if    ( attemptNumberString    !  =    null )     {", "int   attemptNumber    =    Integer . parseInt ( attemptNumberString )  ;", "successfulNthMapperAttempts . enter ( attemptNumber )  ;", "}", "}", "}", "}", "try    {", "if    ( attemptStartTime    !  =    null )     {", "long   startTimeValue    =    Long . parseLong ( attemptStartTime )  ;", "if    (  ( startTimeValue    !  =     0  )     &  &     (  ( startTimeValue    +     (  . MAXIMUM _ CLOCK _ SKEW )  )     >  =     ( launchTimeCurrentJob )  )  )     {", "taskAttemptStartTimes . put ( attemptID ,    startTimeValue )  ;", "} else    {", "taskAttemptStartTimes . remove ( attemptID )  ;", "}", "} else", "if    (  ( status    !  =    null )     &  &     ( attemptFinishTime    !  =    null )  )     {", "long   finishTime    =    Long . parseLong ( attemptFinishTime )  ;", "if    ( status . equals (  \" SUCCESS \"  )  )     {", "taskMapAttemptFinishTimes . put ( attemptID ,    finishTime )  ;", "}", "}", "}    catch    ( NumberFormatException   e )     {", ". LOG . warn (  (  (  \"  . processMapAttemptLine :    bad   numerical   format ,    at   line \"     +     ( lineNumber )  )     +     \"  .  \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["processMapAttemptLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( collecting )  )     {", "RecordType   myType    =    line . getType (  )  ;", "if    ( myType    =  =     ( canonicalJob )  )     {", "processJobLine ( line )  ;", "} else", "if    ( myType    =  =     ( canonicalTask )  )     {", "processTaskLine ( line )  ;", "} else", "if    ( myType    =  =     ( canonicalMapAttempt )  )     {", "processMapAttemptLine ( line )  ;", "} else", "if    ( myType    =  =     ( canonicalReduceAttempt )  )     {", "processReduceAttemptLine ( line )  ;", "} else    {", "}", "}", "}", "METHOD_END"], "methodName": ["processParsedLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "String   attemptID    =    line . get (  \" TASK _ ATTEMPT _ ID \"  )  ;", "String   taskID    =    line . get (  \" TASKID \"  )  ;", "String   status    =    line . get (  \" TASK _ STATUS \"  )  ;", "String   attemptStartTime    =    line . get (  \" START _ TIME \"  )  ;", "String   attemptFinishTime    =    line . get (  \" FINISH _ TIME \"  )  ;", "String   attemptShuffleFinished    =    line . get (  \" SHUFFLE _ FINISHED \"  )  ;", "String   attemptSortFinished    =    line . get (  \" SORT _ FINISHED \"  )  ;", "String   counters    =    line . get (  \" COUNTERS \"  )  ;", "String   hostName    =    line . get (  \" HOSTNAME \"  )  ;", "if    (  ( hostName    !  =    null )     &  &     (  !  ( hostNames . contains ( hostName )  )  )  )     {", "hostNames . add ( hostName )  ;", "}", "if    (  (  ( jobBeingTraced )     !  =    null )     &  &     ( taskID    !  =    null )  )     {", "LoggedTask   task    =    tasksInCurrentJob . get ( taskID )  ;", "if    ( task    =  =    null )     {", "task    =    new   LoggedTask (  )  ;", "task . setTaskID ( taskID )  ;", "jobBeingTraced . getReduceTasks (  )  . add ( task )  ;", "tasksInCurrentJob . put ( taskID ,    task )  ;", "}", "task . setTaskID ( taskID )  ;", "LoggedTaskAttempt   attempt    =    attemptsInCurrentJob . get ( attemptID )  ;", "boolean   attemptAlreadyExists    =    attempt    !  =    null ;", "if    ( attempt    =  =    null )     {", "attempt    =    new   LoggedTaskAttempt (  )  ;", "attempt . setAttemptID ( attemptID )  ;", "}", "if    (  ! attemptAlreadyExists )     {", "attemptsInCurrentJob . put ( attemptID ,    attempt )  ;", "task . getAttempts (  )  . add ( attempt )  ;", "}", "Pre 2  1 JobHistoryConstants . Values   stat    =    null ;", "try    {", "stat    =     ( status    =  =    null )     ?    null    :    Pre 2  1 JobHistoryConstants . Values . valueOf ( status )  ;", "}    catch    ( IllegalArgumentException   e )     {", ". LOG . warn (  (  (  \" A   map   attempt   status   you   don \\  ' t   know   about   is    \\  \"  \"     +    status )     +     \"  \\  \"  .  \"  )  ,    e )  ;", "stat    =    null ;", "}", "incorporateCounters ( attempt ,    counters )  ;", "attempt . setResult ( stat )  ;", "if    ( attemptStartTime    !  =    null )     {", "attempt . setStartTime ( Long . parseLong ( attemptStartTime )  )  ;", "}", "if    ( attemptFinishTime    !  =    null )     {", "attempt . setFinishTime ( Long . parseLong ( attemptFinishTime )  )  ;", "}", "if    ( attemptShuffleFinished    !  =    null )     {", "attempt . setShuffleFinished ( Long . parseLong ( attemptShuffleFinished )  )  ;", "}", "if    ( attemptSortFinished    !  =    null )     {", "attempt . setSortFinished ( Long . parseLong ( attemptSortFinished )  )  ;", "}", "if    (  (  ( attempt . getStartTime (  )  )     >     0  )     &  &     (  ( attempt . getFinishTime (  )  )     >     0  )  )     {", "long   runtime    =     ( attempt . getFinishTime (  )  )     -     ( attempt . getStartTime (  )  )  ;", "if    ( stat    =  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )     {", "successfulReduceAttemptTimes . enter ( runtime )  ;", "}", "if    ( stat    =  =     ( Pre 2  1 JobHistoryConstants . Values . FAILED )  )     {", "failedReduceAttemptTimes . enter ( runtime )  ;", "}", "}", "if    ( hostName    !  =    null )     {", "ParsedHost   host    =    getAndRecordParsedHost ( hostName )  ;", "if    ( host    !  =    null )     {", "attempt . setHostName ( host . getNodeName (  )  ,    host . getRackName (  )  )  ;", "} else    {", "attempt . setHostName ( hostName ,    null )  ;", "}", "}", "if    ( attemptID    !  =    null )     {", "Matcher   matcher    =     . taskAttemptIDPattern . matcher ( attemptID )  ;", "if    ( matcher . matches (  )  )     {", "String   attemptNumberString    =    matcher . group (  1  )  ;", "if    ( attemptNumberString    !  =    null )     {", "int   attemptNumber    =    Integer . parseInt ( attemptNumberString )  ;", "successfulNthReducerAttempts . enter ( attemptNumber )  ;", "}", "}", "}", "}", "try    {", "if    ( attemptStartTime    !  =    null )     {", "long   startTimeValue    =    Long . parseLong ( attemptStartTime )  ;", "if    (  ( startTimeValue    !  =     0  )     &  &     (  ( startTimeValue    +     (  . MAXIMUM _ CLOCK _ SKEW )  )     >  =     ( launchTimeCurrentJob )  )  )     {", "taskAttemptStartTimes . put ( attemptID ,    startTimeValue )  ;", "}", "} else", "if    (  (  ( status    !  =    null )     &  &     ( status . equals (  \" SUCCESS \"  )  )  )     &  &     ( attemptFinishTime    !  =    null )  )     {", "long   finishTime    =    Long . parseLong ( attemptFinishTime )  ;", "taskReduceAttemptFinishTimes . put ( attemptID ,    finishTime )  ;", "if    ( attemptShuffleFinished    !  =    null )     {", "taskReduceAttemptShuffleEndTimes . put ( attemptID ,    Long . parseLong ( attemptShuffleFinished )  )  ;", "}", "if    ( attemptSortFinished    !  =    null )     {", "taskReduceAttemptSortEndTimes . put ( attemptID ,    Long . parseLong ( attemptSortFinished )  )  ;", "}", "}", "}    catch    ( NumberFormatException   e )     {", ". LOG . error (  (  (  \"  . processReduceAttemptLine :    bad   numerical   format ,    at   line \"     +     ( lineNumber )  )     +     \"  .  \"  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["processReduceAttemptLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( jobBeingTraced )     !  =    null )     {", "String   taskID    =    line . get (  \" TASKID \"  )  ;", "String   taskType    =    line . get (  \" TASK _ TYPE \"  )  ;", "String   startTime    =    line . get (  \" START _ TIME \"  )  ;", "String   status    =    line . get (  \" TASK _ STATUS \"  )  ;", "String   finishTime    =    line . get (  \" FINISH _ TIME \"  )  ;", "String   splits    =    line . get (  \" SPLITS \"  )  ;", "LoggedTask   task    =    tasksInCurrentJob . get ( taskID )  ;", "boolean   taskAlreadyLogged    =    task    !  =    null ;", "if    ( task    =  =    null )     {", "task    =    new   LoggedTask (  )  ;", "}", "if    ( splits    !  =    null )     {", "ArrayList < LoggedLocation >    locations    =    null ;", "StringTokenizer   tok    =    new   StringTokenizer ( splits ,     \"  ,  \"  ,    false )  ;", "if    (  ( tok . countTokens (  )  )     <  =     (  . MAXIMUM _ PREFERRED _ LOCATIONS )  )     {", "locations    =    new   ArrayList < LoggedLocation >  (  )  ;", "}", "while    ( tok . hasMoreTokens (  )  )     {", "String   nextSplit    =    tok . nextToken (  )  ;", "ParsedHost   node    =    getAndRecordParsedHost ( nextSplit )  ;", "if    (  ( locations    !  =    null )     &  &     ( node    !  =    null )  )     {", "locations . add ( node . makeLoggedLocation (  )  )  ;", "}", "}", "task . setPreferredLocations ( locations )  ;", "}", "task . setTaskID ( taskID )  ;", "if    ( startTime    !  =    null )     {", "task . setStartTime ( Long . parseLong ( startTime )  )  ;", "}", "if    ( finishTime    !  =    null )     {", "task . setFinishTime ( Long . parseLong ( finishTime )  )  ;", "}", "Pre 2  1 JobHistoryConstants . Values   typ ;", "Pre 2  1 JobHistoryConstants . Values   stat ;", "try    {", "stat    =     ( status    =  =    null )     ?    null    :    Pre 2  1 JobHistoryConstants . Values . valueOf ( status )  ;", "}    catch    ( IllegalArgumentException   e )     {", ". LOG . error (  (  (  \" A   task   status   you   don \\  ' t   know   about   is    \\  \"  \"     +    status )     +     \"  \\  \"  .  \"  )  ,    e )  ;", "stat    =    null ;", "}", "task . setTaskStatus ( stat )  ;", "try    {", "typ    =     ( taskType    =  =    null )     ?    null    :    Pre 2  1 JobHistoryConstants . Values . valueOf ( taskType )  ;", "}    catch    ( IllegalArgumentException   e )     {", ". LOG . error (  (  (  \" A   task   type   you   don \\  ' t   know   about   is    \\  \"  \"     +    taskType )     +     \"  \\  \"  .  \"  )  ,    e )  ;", "typ    =    null ;", "}", "if    ( typ    =  =    null )     {", "return ;", "}", "task . setTaskType ( typ )  ;", "List < LoggedTask >    vec    =     ( typ    =  =     ( Pre 2  1 JobHistoryConstants . Values . MAP )  )     ?    jobBeingTraced . getMapTasks (  )     :    typ    =  =     ( Pre 2  1 JobHistoryConstants . Values . REDUCE )     ?    jobBeingTraced . getReduceTasks (  )     :    jobBeingTraced . getOtherTasks (  )  ;", "if    (  ! taskAlreadyLogged )     {", "vec . add ( task )  ;", "tasksInCurrentJob . put ( taskID ,    task )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["processTaskLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "String   line    =    readCountedLine (  )  ;", "if    ( line    =  =    null )     {", "return   null ;", "}", "while    (  ( line . indexOf (  '  \\ f '  )  )     >     0  )     {", "line    =    line . substring ( line . indexOf (  '  \\ f '  )  )  ;", "}", "if    (  (  ( line . length (  )  )     !  =     0  )     &  &     (  ( line . charAt (  0  )  )     =  =     '  \\ f '  )  )     {", "String   subjectLine    =    readCountedLine (  )  ;", "if    (  (  (  ( subjectLine    !  =    null )     &  &     (  ( subjectLine . length (  )  )     !  =     0  )  )     &  &    parentConfFileHeader ( line )  )  )     &  &    parentXMLFileStart ( subjectLine )  )  )     {", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "while    (  ( subjectLine    !  =    null )     &  &     (  ( subjectLine . indexOf (  '  \\ f '  )  )     >     0  )  )     {", "subjectLine    =    subjectLine . substring ( subjectLine . indexOf (  '  \\ f '  )  )  ;", "}", "while    (  ( subjectLine    !  =    null )     &  &     (  (  ( subjectLine . length (  )  )     =  =     0  )     |  |     (  ( subjectLine . charAt (  0  )  )     !  =     '  \\ f '  )  )  )     {", "sbpend ( subjectLine )  ;", "subjectLine    =    readCountedLine (  )  ;", "}", "if    ( subjectLine    !  =    null )     {", "unreadCountedLine ( subjectLine )  ;", "}", "return   new   Pair < String ,    String >  ( line ,    sb . toString (  )  )  ;", "}", "return   readBalancedLine (  )  ;", "}", "String   endlineString    =     (  ( version )     =  =     0  )     ?     \"     \"     :     \"     .  \"  ;", "if    (  ( line . length (  )  )     <     ( endlineString . length (  )  )  )     {", "return   new   Pair < String ,    String >  ( null ,    line )  ;", "}", "if    (  !  ( endlineString . equals ( line . substring (  (  ( line . length (  )  )     -     ( endlineString . length (  )  )  )  )  )  )  )     {", "StringBuilder   sb    =    new   StringBuilder ( line )  ;", "String   addedLine ;", "do    {", "addedLine    =    readCountedLine (  )  ;", "if    ( addedLine    =  =    null )     {", "return   new   Pair < String ,    String >  ( null ,    sb . toString (  )  )  ;", "}", "while    (  ( addedLine . indexOf (  '  \\ f '  )  )     >     0  )     {", "addedLine    =    addedLine . substring ( addedLine . indexOf (  '  \\ f '  )  )  ;", "}", "if    (  (  ( addedLine . length (  )  )     >     0  )     &  &     (  ( addedLine . charAt (  0  )  )     =  =     '  \\ f '  )  )     {", "unreadCountedLine ( addedLine )  ;", "return   new   Pair < String ,    String >  ( null ,    sb . toString (  )  )  ;", "}", "sbpend (  \"  \\ n \"  )  ;", "sbpend ( addedLine )  ;", "}    while    (  !  ( endlineString . equals ( addedLine . substring (  (  ( addedLine . length (  )  )     -     ( endlineString . length (  )  )  )  )  )  )     )  ;", "line    =    sb . toString (  )  ;", "}", "return   new   Pair < String ,    String >  ( null ,    line )  ;", "}", "METHOD_END"], "methodName": ["readBalancedLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rereadableLine )     !  =    null )     {", "String   result    =    rereadableLine ;", "rereadableLine    =    null ;", "return   result ;", "}", "String   result    =    readInputLine (  )  ;", "if    ( result    !  =    null )     {", "if    (  ( fileFirstLine )     &  &     (  ( result . equals (  \"  \"  )  )     |  |     (  ( result . charAt (  0  )  )     !  =     '  \\ f '  )  )  )     {", "fileFirstLine    =    false ;", "rereadableLine    =    result ;", "return    (  \"  \\ f !  ! FILE    \"     +     ( currentFileName )  )     +     \"  !  !  \\ n \"  ;", "}", "fileFirstLine    =    false ;", "+  +  ( lineNumber )  ;", "} else", "if    (  ( inputIsDirectory )     &  &     ( setNextDirectoryInputStream (  )  )  )     {", "result    =    readCountedLine (  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["readCountedLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  ( input )     =  =    null )     {", "return   null ;", "}", "inputLineText . clear (  )  ;", "if    (  ( input . readLine ( inputLineText )  )     =  =     0  )     {", "return   null ;", "}", "return   inputLineTextString (  )  ;", "}    catch    ( EOFException   e )     {", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["readInputLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "Pair < String ,    String >    line    =    readBalancedLine (  )  ;", "while    ( line    !  =    null )     {", "if    (  ( debug )     &  &     (  (  (  ( lineNumber )     <     1  0  0  0  0  0  0 L )     &  &     (  (  ( lineNumber )     %     1  0  0  0 L )     =  =     0  )  )     |  |     (  (  ( lineNumber )     %     1  0  0  0  0  0  0 L )     =  =     0  )  )  )     {", ". LOG . debug (  (  (  (  \"  \"     +     ( lineNumber )  )     +     \"     \"  )     +     ( line . second (  )  )  )  )  ;", "}", "if    (  ( line . first (  )  )     =  =    null )     {", "try    {", "processParsedLine ( new   ParsedLine ( line . second (  )  ,    version )  )  ;", "}    catch    ( StringIndexOutOfBoundsException   e )     {", ". LOG . warn (  (  (  (  \" anomalous   line    #  \"     +     ( lineNumber )  )     +     \"  :  \"  )     +    line )  ,    e )  ;", "}", "} else    {", "jobconf    =    new   ParsedConfigFile ( line . first (  )  ,    line . second (  )  )  ;", "if    (  ( jobconf . valid )     =  =    false )     {", "jobconf    =    null ;", "}", "maybeMateJobAndConf (  )  ;", "}", "line    =    readBalancedLine (  )  ;", "}", "finalizeJob (  )  ;", "if    ( collecting )     {", "String [  ]    typeNames    =    LogRecordType . lineTypes (  )  ;", "for    ( int   i    =     0  ;    i    <     ( typeNames . length )  ;     +  + i )     {", "statisticalOutput . print ( typeNames [ i ]  )  ;", "statisticalOutput . print (  '  \\ n '  )  ;", "}", "} else    {", "if    ( delays )     {", "printDistributionSet (  \" Job   start   delay   spectrum :  \"  ,    delayTimeDists )  ;", "}", "if    ( runtimes )     {", "printDistributionSet (  \" Job   run   time   spectrum :  \"  ,    runTimeDists )  ;", "}", "if    ( spreading )     {", "String   ratioDescription    =     (  (  (  \"  (  \"     +     ( spreadMax )  )     +     \"  /  1  0  0  0     % ile )    to    (  \"  )     +     ( spreadMin )  )     +     \"  /  1  0  0  0     % ile )    scaled   by    1  0  0  0  0  0  0  \"  ;", "printDistributionSet (  (  (  \" Map   task   success   times    \"     +    ratioDescription )     +     \"  :  \"  )  ,    mapTimeSpreadDists )  ;", "printDistributionSet (  (  (  \" Shuffle   success   times    \"     +    ratioDescription )     +     \"  :  \"  )  ,    shuffleTimeSpreadDists )  ;", "printDistributionSet (  (  (  \" Sort   success   times    \"     +    ratioDescription )     +     \"  :  \"  )  ,    sortTimeSpreadDists )  ;", "printDistributionSet (  (  (  \" Reduce   success   times    \"     +    ratioDescription )     +     \"  :  \"  )  ,    reduceTimeSpreadDists )  ;", "}", "if    ( collectTaskTimes )     {", "printDistributionSet (  \" Global   map   task   success   times :  \"  ,    mapTimeDists )  ;", "printDistributionSet (  \" Global   shuffle   task   success   times :  \"  ,    shuffleTimeDists )  ;", "printDistributionSet (  \" Global   sort   task   success   times :  \"  ,    sortTimeDists )  ;", "printDistributionSet (  \" Global   reduce   task   success   times :  \"  ,    reduceTimeDists )  ;", "}", "}", "if    (  ( topologyGen )     !  =    null )     {", "LoggedNetworkTopology   topo    =    new   LoggedNetworkTopology ( allHosts ,     \"  < root >  \"  ,     0  )  ;", "topologyGen . output ( topo )  ;", "topologyGen . close (  )  ;", "}", "if    (  ( jobTraceGen )     !  =    null )     {", "jobTraceGen . close (  )  ;", "}", "if    (  ( input )     !  =    null )     {", "input . close (  )  ;", "input    =    null ;", "}", "if    (  ( inputCodec )     !  =    null )     {", "CodecPool . returnDecompressor ( inputDecompressor )  ;", "inputDecompressor    =    null ;", "inputCodec    =    null ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "int   result    =    initializeHadoopLogsAnalyzer ( args )  ;", "if    ( result    !  =     0  )     {", "return   result ;", "}", "return   run (  )  ;", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( input )     !  =    null )     {", "input . close (  )  ;", ". LOG . info (  (  \" File   closed :     \"     +     ( currentFileName )  )  )  ;", "input    =    null ;", "}", "if    (  ( inputCodec )     !  =    null )     {", "CodecPool . returnDecompressor ( inputDecompressor )  ;", "inputDecompressor    =    null ;", "inputCodec    =    null ;", "}", "+  +  ( inputDirectoryCursor )  ;", "if    (  ( inputDirectoryCursor )     >  =     ( inputDirectoryFiles . length )  )     {", "return   false ;", "}", "fileFirstLine    =    true ;", "currentFileName    =    inputDirectoryFiles [ inputDirectoryCursor ]  ;", ". LOG . info (  (  (  \"  \\ nOpening   file    \"     +     ( currentFileName )  )     +     \"        *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *  *     .  \"  )  )  ;", ". LOG . info (  (  (  (  (  (  (  \" This   file ,     \"     +     (  ( inputDirectoryCursor )     +     1  )  )     +     \"  /  \"  )     +     ( inputDirectoryFiles . length )  )     +     \"  ,    starts   with   line    \"  )     +     ( lineNumber )  )     +     \"  .  \"  )  )  ;", "input    =    maybeUncompressedPath ( new   Path ( inputDirectoryPath ,    currentFileName )  )  ;", "return    ( input )     !  =    null ;", "}", "METHOD_END"], "methodName": ["setNextDirectoryInputStream"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rereadableLine )     =  =    null )     {", "rereadableLine    =    unreadee ;", "}", "}", "METHOD_END"], "methodName": ["unreadCountedLine"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "statusOutput . print (  (  \" Usage :     \\ n \"     +     (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" administrative   subcommands :  \\ n \"     +     \"  - v 1                                                       specify   version    1    of   the   jt   logs \\ n \"  )     +     \"  - h   or    - help                              print   this   message \\ n \"  )     +     \"  - d   or    - debug                           print   voluminous   debug   info   during   processing \\ n \"  )     +     \"  - collect - prefixes            collect   the   prefixes   of   log   lines \\ n \\ n \"  )     +     \"       job   trace   subcommands \\ n \"  )     +     \"  - write - job - trace               takes   a   filename .  \\ n \"  )     +     \"                                                                writes   job   trace   in   JSON   to   that   filename \\ n \"  )     +     \"  - single - line - job - traces      omit   prettyprinting   of   job   trace \\ n \"  )     +     \"  - omit - task - details         leave   out   info   about   each   task   and   attempt ,  \\ n \"  )     +     \"                                                                so   only   statistical   info   is   added   to   each   job \\ n \"  )     +     \"  - write - topology                  takes   a   filename .  \\ n \"  )     +     \"                                                                writes   JSON   file   giving   network   topology \\ n \"  )     +     \"  - job - digest - spectra      takes   a   list   of   percentile   points \\ n \"  )     +     \"                                                                writes   CDFs   with   min ,    max ,    and   those   percentiles \\ n \\ n \"  )     +     \" subcommands   for   task   statistical   info \\ n \"  )     +     \"  - spreads                                       we   have   a   mode   where ,    for   each   job ,    we   can \\ n \"  )     +     \"                                                                develop   the   ratio   of   percentile   B   to   percentile   A \\ n \"  )     +     \"                                                                of   task   run   times .       Having   developed   that   ratio ,  \\ n \"  )     +     \"                                                                we   can   consider   it   to   be   a   datum   and   we   can \\ n \"  )     +     \"                                                                build   a   CDF   of   those   ratios .        - spreads   turns \\ n \"  )     +     \"                                                                this   option   on ,    and   takes   A   and   B \\ n \"  )     +     \"  - delays                                          tells   us   to   gather   and   print   CDFs   for   delays \\ n \"  )     +     \"                                                                from   job   submit   to   job   start \\ n \"  )     +     \"  - runtimes                                    prints   CDFs   of   job   wallclock   times    [ launch \\ n \"  )     +     \"                                                                to   finish ]  \\ n \"  )     +     \"  - tasktimes                                 prints   CDFs   of   job   wallclock   times    [ launch \\ n \"  )     +     \"                                                                to   finish ]  \\ n \\ n \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["usage"], "fileName": "org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer"}, {"methodBody": ["METHOD_START", "{", "stream . print (  (  (  \" dumping   Histogram    \"     +     ( name )  )     +     \"  :  \\ n \"  )  )  ;", "Iterator < Map . Entry < Long ,    Long >  >    iter    =    iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < Long ,    Long >    ent    =    iter . next (  )  ;", "stream . print (  (  (  (  (  \" val / count   pair :     \"     +     (  ( long )     ( ent . getKey (  )  )  )  )     +     \"  ,     \"  )     +     (  ( long )     ( ent . getValue (  )  )  )  )     +     \"  \\ n \"  )  )  ;", "}", "stream . print (  \"  *  *  *    end    *  *  *     \\ n \"  )  ;", "}", "METHOD_END"], "methodName": ["dump"], "fileName": "org.apache.hadoop.tools.rumen.Histogram"}, {"methodBody": ["METHOD_START", "{", "Long   existingValue    =    content . get ( value )  ;", "if    ( existingValue    =  =    null )     {", "content . put ( value ,     1 L )  ;", "} else    {", "content . put ( value ,     ( existingValue    +     1 L )  )  ;", "}", "+  +  ( totalCount )  ;", "}", "METHOD_END"], "methodName": ["enter"], "fileName": "org.apache.hadoop.tools.rumen.Histogram"}, {"methodBody": ["METHOD_START", "{", "Long   result    =    content . get ( key )  ;", "return   result    =  =    null    ?     0     :    result ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.tools.rumen.Histogram"}, {"methodBody": ["METHOD_START", "{", "if    (  ( totalCount )     =  =     0  )     {", "return   null ;", "}", "long [  ]    result    =    new   long [  ( buckets . length )     +     2  ]  ;", "result [  0  ]     =    content . firstEntry (  )  . getKey (  )  ;", "result [  (  ( buckets . length )     +     1  )  ]     =    content . lastEntry (  )  . getKey (  )  ;", "Iterator < Map . Entry < Long ,    Long >  >    iter    =    content . entrySet (  )  . iterator (  )  ;", "long   cumulativeCount    =     0  ;", "int   bucketCursor    =     0  ;", "while    ( itersNext (  )  )     {", "long   targetCumulativeCount    =     (  ( buckets [ bucketCursor ]  )     *     ( totalCount )  )     /    scale ;", "Map . Entry < Long ,    Long >    elt    =    iter . next (  )  ;", "cumulativeCount    +  =    elt . getValue (  )  ;", "while    ( cumulativeCount    >  =    targetCumulativeCount )     {", "result [  ( bucketCursor    +     1  )  ]     =    elt . getKey (  )  ;", "+  + bucketCursor ;", "if    ( bucketCursor    <     ( buckets . length )  )     {", "targetCumulativeCount    =     (  ( buckets [ bucketCursor ]  )     *     ( totalCount )  )     /    scale ;", "} else    {", "break ;", "}", "}", "if    ( bucketCursor    =  =     ( buckets . length )  )     {", "break ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["getCDF"], "fileName": "org.apache.hadoop.tools.rumen.Histogram"}, {"methodBody": ["METHOD_START", "{", "return   totalCount ;", "}", "METHOD_END"], "methodName": ["getTotalCount"], "fileName": "org.apache.hadoop.tools.rumen.Histogram"}, {"methodBody": ["METHOD_START", "{", "return   content . entrySet (  )  . iterator (  )  ;", "}", "METHOD_END"], "methodName": ["iterator"], "fileName": "org.apache.hadoop.tools.rumen.Histogram"}, {"methodBody": ["METHOD_START", "{", "return   data ;", "}", "METHOD_END"], "methodName": ["getData"], "fileName": "org.apache.hadoop.tools.rumen.HistogramRawTestData"}, {"methodBody": ["METHOD_START", "{", "return   percentiles ;", "}", "METHOD_END"], "methodName": ["getPercentiles"], "fileName": "org.apache.hadoop.tools.rumen.HistogramRawTestData"}, {"methodBody": ["METHOD_START", "{", "return   scale ;", "}", "METHOD_END"], "methodName": ["getScale"], "fileName": "org.apache.hadoop.tools.rumen.HistogramRawTestData"}, {"methodBody": ["METHOD_START", "{", "this . data    =    data ;", "}", "METHOD_END"], "methodName": ["setData"], "fileName": "org.apache.hadoop.tools.rumen.HistogramRawTestData"}, {"methodBody": ["METHOD_START", "{", "this . percentiles    =    percentiles ;", "}", "METHOD_END"], "methodName": ["setPercentiles"], "fileName": "org.apache.hadoop.tools.rumen.HistogramRawTestData"}, {"methodBody": ["METHOD_START", "{", "this . scale    =    scale ;", "}", "METHOD_END"], "methodName": ["setScale"], "fileName": "org.apache.hadoop.tools.rumen.HistogramRawTestData"}, {"methodBody": ["METHOD_START", "{", "Queue < HistoryEvent >    results    =    new   LinkedList < HistoryEvent >  (  )  ;", ". PostEmitAction   removeEmitter    =     . PostEmitAction . NONE ;", "for    ( SingleEventEmitter   see    :    nonFinalSEEs (  )  )     {", "HistoryEvent   event    =    see . maybeEmitEvent ( line ,    name ,    this )  ;", "if    ( event    !  =    null )     {", "results . add ( event )  ;", "}", "}", "for    ( SingleEventEmitter   see    :    finalSEEs (  )  )     {", "HistoryEvent   event    =    see . maybeEmitEvent ( line ,    name ,    this )  ;", "if    ( event    !  =    null )     {", "results . add ( event )  ;", "removeEmitter    =     . PostEmitAction . REMOVE _ HEE ;", "break ;", "}", "}", "return   new   Pair < Queue < HistoryEvent >  ,     . PostEmitAction >  ( results ,    removeEmitter )  ;", "}", "METHOD_END"], "methodName": ["emitterCore"], "fileName": "org.apache.hadoop.tools.rumen.HistoryEventEmitter"}, {"methodBody": ["METHOD_START", "{", "try    {", "return    . parseCounters ( counters )  ;", "}    catch    ( ParseException   e )     {", ". LOG . warn (  (  (  \" The   counter   string ,     \\  \"  \"     +    counters )     +     \"  \\  \"    is   badly   formatted .  \"  )  )  ;", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["maybeParseCounters"], "fileName": "org.apache.hadoop.tools.rumen.HistoryEventEmitter"}, {"methodBody": ["METHOD_START", "{", "if    ( counters    =  =    null )     {", ". LOG . warn (  \" s :    null   counter   detected :  \"  )  ;", "return   null ;", "}", "counters    =    counters . replace (  \"  \\  \\  .  \"  ,     \"  \\  \\  \\  \\  .  \"  )  ;", "counters    =    counters . replace (  \"  \\  \\  \\  \\  {  \"  ,     \"  \\  \\  {  \"  )  ;", "counters    =    counters . replace (  \"  \\  \\  \\  \\  }  \"  ,     \"  \\  \\  }  \"  )  ;", "counters    =    counters . replace (  \"  \\  \\  \\  \\  (  \"  ,     \"  \\  \\  (  \"  )  ;", "counters    =    counters . replace (  \"  \\  \\  \\  \\  )  \"  ,     \"  \\  \\  )  \"  )  ;", "counters    =    counters . replace (  \"  \\  \\  \\  \\  [  \"  ,     \"  \\  \\  [  \"  )  ;", "counters    =    counters . replace (  \"  \\  \\  \\  \\  ]  \"  ,     \"  \\  \\  ]  \"  )  ;", "Counters   depForm    =    fromEscapedCompactString ( counters )  ;", "return   new   Counters ( depForm )  ;", "}", "METHOD_END"], "methodName": ["parseCounters"], "fileName": "org.apache.hadoop.tools.rumen.HistoryEventEmitter"}, {"methodBody": ["METHOD_START", "{", "finalized    =    true ;", "if    (  ( jobConfigurationParameters )     !  =    null )     {", "result . setProperties ( jobConfigurationParameters )  ;", "}", "Histogram [  ]    successfulMapAttemptTimes    =    new   Histogram [  ( ParsedHost . numberOfDistances (  )  )     +     1  ]  ;", "for    ( int   i    =     0  ;    i    <     ( successfulMapAttemptTimes . length )  ;     +  + i )     {", "successfulMapAttemptTimes [ i ]     =    new   Histogram (  )  ;", "}", "Histogram   successfulReduceAttemptTimes    =    new   Histogram (  )  ;", "Histogram [  ]    failedMapAttemptTimes    =    new   Histogram [  ( ParsedHost . numberOfDistances (  )  )     +     1  ]  ;", "for    ( int   i    =     0  ;    i    <     ( failedMapAttemptTimes . length )  ;     +  + i )     {", "failedMapAttemptTimes [ i ]     =    new   Histogram (  )  ;", "}", "Histogram   failedReduceAttemptTimes    =    new   Histogram (  )  ;", "Histogram   successfulNthMapperAttempts    =    new   Histogram (  )  ;", "for    ( LoggedTask   task    :    result . getMapTasks (  )  )     {", "for    ( LoggedTaskAttempt   attempt    :    task . getAttempts (  )  )     {", "int   distance    =     ( successfulMapAttemptTimes . length )     -     1  ;", "Long   runtime    =    null ;", "if    (  (  ( attempt . getFinishTime (  )  )     >     0  )     &  &     (  ( attempt . getStartTime (  )  )     >     0  )  )     {", "runtime    =     ( attempt . getFinishTime (  )  )     -     ( attempt . getStartTime (  )  )  ;", "if    (  ( attempt . getResult (  )  )     =  =     ( Pre 2  1 HistoryConstants . Values . SUCCESS )  )     {", "LoggedLocation   host    =    attempt . getLocation (  )  ;", "List < LoggedLocation >    locs    =    task . getPreferredLocations (  )  ;", "if    (  ( host    !  =    null )     &  &     ( locs    !  =    null )  )     {", "for    ( LoggedLocation   loc    :    locs )     {", "ParsedHost   preferedLoc    =    new   ParsedHost ( loc )  ;", "distance    =    Math . min ( distance ,    preferedLoc . distance ( new   ParsedHost ( host )  )  )  ;", "}", "}", "if    (  (  ( attempt . getStartTime (  )  )     >     0  )     &  &     (  ( attempt . getFinishTime (  )  )     >     0  )  )     {", "if    ( runtime    !  =    null )     {", "successfulMapAttemptTimes [ distance ]  . enter ( runtime )  ;", "}", "}", "TaskAttemptID   attemptID    =    attempt . getAttemptID (  )  ;", "if    ( attemptID    !  =    null )     {", "successfulNthMapperAttempts . enter ( attemptID . getId (  )  )  ;", "}", "} else    {", "if    (  ( attempt . getResult (  )  )     =  =     ( Pre 2  1 HistoryConstants . Values . FAILED )  )     {", "if    ( runtime    !  =    null )     {", "failedMapAttemptTimes [ distance ]  . enter ( runtime )  ;", "}", "}", "}", "}", "}", "}", "for    ( LoggedTask   task    :    result . getReduceTasks (  )  )     {", "for    ( LoggedTaskAttempt   attempt    :    task . getAttempts (  )  )     {", "Long   runtime    =     ( attempt . getFinishTime (  )  )     -     ( attempt . getStartTime (  )  )  ;", "if    (  (  ( attempt . getFinishTime (  )  )     >     0  )     &  &     (  ( attempt . getStartTime (  )  )     >     0  )  )     {", "runtime    =     ( attempt . getFinishTime (  )  )     -     ( attempt . getStartTime (  )  )  ;", "}", "if    (  ( attempt . getResult (  )  )     =  =     ( Pre 2  1 HistoryConstants . Values . SUCCESS )  )     {", "if    ( runtime    !  =    null )     {", "successfulReduceAttemptTimes . enter ( runtime )  ;", "}", "} else", "if    (  ( attempt . getResult (  )  )     =  =     ( Pre 2  1 HistoryConstants . Values . FAILED )  )     {", "failedReduceAttemptTimes . enter ( runtime )  ;", "}", "}", "}", "result . setFailedMapAttemptCDFs ( mapCDFArrayList ( failedMapAttemptTimes )  )  ;", "LoggedDiscreteCDF   failedReduce    =    new   LoggedDiscreteCDF (  )  ;", "failedReduce . setCDF ( failedReduceAttemptTimes ,    attemptTimesPercentiles ,     1  0  0  )  ;", "result . setFailedReduceAttemptCDF ( failedReduce )  ;", "result . setSuccessfulMapAttemptCDFs ( mapCDFArrayList ( successfulMapAttemptTimes )  )  ;", "LoggedDiscreteCDF   succReduce    =    new   LoggedDiscreteCDF (  )  ;", "succReduce . setCDF ( successfulReduceAttemptTimes ,    attemptTimesPercentiles ,     1  0  0  )  ;", "result . setSuccessfulReduceAttemptCDF ( succReduce )  ;", "long   totalSuccessfulAttempts    =     0 L ;", "long   maxTriesToSucceed    =     0 L ;", "for    ( Map . Entry < Long ,    Long >    ent    :    successfulNthMapperAttempts )     {", "totalSuccessfulAttempts    +  =    ent . getValue (  )  ;", "maxTriesToSucceed    =    Math . max ( maxTriesToSucceed ,    ent . getKey (  )  )  ;", "}", "if    ( totalSuccessfulAttempts    >     0 L )     {", "double [  ]    successAfterI    =    new   double [  (  ( int )     ( maxTriesToSucceed )  )     +     1  ]  ;", "for    ( int   i    =     0  ;    i    <     ( successAfterI . length )  ;     +  + i )     {", "successAfterI [ i ]     =     0  .  0  ;", "}", "for    ( Map . Entry < Long ,    Long >    ent    :    successfulNthMapperAttempts )     {", "successAfterI [ ent . getKey (  )  . intValue (  )  ]     =     (  ( double )     ( ent . getValue (  )  )  )     /    totalSuccessfulAttempts ;", "}", "result . setMapperTriesToSucceed ( successAfterI )  ;", "} else    {", "result . setMapperTriesToSucceed ( null )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["build"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "for    ( String   name    :    names )     {", "String   result    =    conf . getProperty ( name )  ;", "if    ( result    !  =    null )     {", "return   result ;", "}", "}", "return   defaultValue ;", "}", "METHOD_END"], "methodName": ["extract"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "String   javaOptions    =    JobBuilder . extract ( conf ,    names ,    null )  ;", "if    ( javaOptions    =  =    null )     {", "return   null ;", "}", "Matcher   matcher    =    JobBuilder . heapPattern . matcher ( javaOptions )  ;", "Integer   heapMegabytes    =    null ;", "while    ( matcher . find (  )  )     {", "String   heapSize    =    matcher . group (  1  )  ;", "heapMegabytes    =     (  ( int )     (  ( TraditionalBinaryPrefix . string 2 long ( heapSize )  )     /     ( JobBuilder . BYTES _ IN _ MEG )  )  )  ;", "}", "return   heapMegabytes ;", "}", "METHOD_END"], "methodName": ["extractMegabytes"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "return   getAndRecordParsedHost ( null ,    hostName )  ;", "}", "METHOD_END"], "methodName": ["getAndRecordParsedHost"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedHost   result    =    null ;", "if    ( rkName    =  =    null )     {", "result    =    ParsedHost . parse ( hostName )  ;", "} else    {", "result    =    new   ParsedHost ( rkName ,    hostName )  ;", "}", "if    ( result    !  =    null )     {", "ParsedHost   canonicalResult    =    allHosts . get ( result )  ;", "if    ( canonicalResult    !  =    null )     {", "return   canonicalResult ;", "}", "allHosts . put ( result ,    result )  ;", "return   result ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getAndRecordParsedHost"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "return   jobID ;", "}", "METHOD_END"], "methodName": ["getJobID"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    ParsedTask >    taskMap    =    otherTasks ;", "List < LoggedTask >    tasks    =    this . result . getOtherTasks (  )  ;", "switch    ( type )     {", "case   MAP    :", "taskMap    =    mapTasks ;", "tasks    =    this . result . getMapTasks (  )  ;", "break ;", "case   REDUCE    :", "taskMap    =    reduceTasks ;", "tasks    =    this . result . getReduceTasks (  )  ;", "break ;", "default    :", "}", "ParsedTask   result    =    taskMap . get ( taskIDname )  ;", "if    (  ( result    =  =    null )     &  &    allowCreate )     {", "result    =    new   ParsedTask (  )  ;", "result . setTaskType (  . getPre 2  1 Value ( type . toString (  )  )  )  ;", "result . setTaskID ( taskIDname )  ;", "taskMap . put ( taskIDname ,    result )  ;", "tasks . add ( result )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["getOrMakeTask"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTask   task    =    getOrMakeTask ( type ,    taskIDName ,    false )  ;", "ParsedTaskAttempt   result    =    attempts . get ( taskAttemptName )  ;", "if    (  ( result    =  =    null )     &  &     ( task    !  =    null )  )     {", "result    =    new   ParsedTaskAttempt (  )  ;", "result . setAttemptID ( taskAttemptName )  ;", "attempts . put ( taskAttemptName ,    result )  ;", "task . getAttempts (  )  . add ( result )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["getOrMakeTaskAttempt"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( name . equalsIgnoreCase (  \" JOB _ CLEANUP \"  )  )     {", "return   Pre 2  1 HistoryConstants . Values . CLEANUP ;", "}", "if    ( name . equalsIgnoreCase (  \" JOB _ SETUP \"  )  )     {", "return   Pre 2  1 HistoryConstants . Values . SETUP ;", "}", "if    ( name . equalsIgnoreCase ( SUCCEEDED . toString (  )  )  )     {", "return   Pre 2  1 HistoryConstants . Values . SUCCESS ;", "}", "return   Pre 2  1 HistoryConstants . Values . valueOf ( name . toUpperCase (  )  )  ;", "}", "METHOD_END"], "methodName": ["getPre21Value"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTask   result    =    mapTasks . get ( taskIDname )  ;", "if    ( result    !  =    null )     {", "return   result ;", "}", "result    =    reduceTasks . get ( taskIDname )  ;", "if    ( result    !  =    null )     {", "return   result ;", "}", "return   otherTasks . get ( taskIDname )  ;", "}", "METHOD_END"], "methodName": ["getTask"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ArrayList < LoggedDiscreteCDF >    result    =    new   ArrayList < LoggedDiscreteCDF >  (  )  ;", "f    ( Histogram   hist    :    data )     {", "LoggedDiscreteCDF   discCDF    =    new   LoggedDiscreteCDF (  )  ;", "discCDF . setCDF ( hist ,    attemptTimesPercentiles ,     1  0  0  )  ;", "result . add ( discCDF )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["mapCDFArrayList"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( megabytes    !  =    null )     {", "result . setHeapMegabytes ( megabytes )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeSetHeapMegabytes"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( megabytes    !  =    null )     {", "result . setMapMB ( megabytes )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeSetJobMapMB"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( megabytes    !  =    null )     {", "result . setReduceMB ( megabytes )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeSetJobReduceMB"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( splits    !  =    null )     {", "ArrayList < LoggedLocation >    locations    =    null ;", "StringTokenizer   tok    =    new   StringTokenizer ( splits ,     \"  ,  \"  ,    false )  ;", "if    (  ( tok . countTokens (  )  )     <  =     (  . MAXIMUM _ PREFERRED _ LOCATIONS )  )     {", "locations    =    new   ArrayList < LoggedLocation >  (  )  ;", "while    ( tok . hasMoreTokens (  )  )     {", "String   nextSplit    =    tok . nextToken (  )  ;", "ParsedHost   node    =    getAndRecordParsedHost ( nextSplit )  ;", "if    (  ( locations    !  =    null )     &  &     ( node    !  =    null )  )     {", "locations . add ( node . makeLoggedLocation (  )  )  ;", "}", "}", "return   locations ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["preferredLocationForSplits"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( finalized )     {", "throw   new   IllegalStateException (  \"  . process ( Properties   conf )    called   after   ParsedJob   built \"  )  ;", "}", "String   queue    =     . extract ( conf ,    JobConfPropertyNames . QUEUE _ NAMES . getCandidates (  )  ,    null )  ;", "if    ( queue    !  =    null )     {", "result . setQueue ( queue )  ;", "}", "result . setJobName (  . extract ( conf ,    JobConfPropertyNames . JOB _ NAMES . getCandidates (  )  ,    null )  )  ;", "maybeSetHeapMegabytes ( extractMegabytes ( conf ,    JobConfPropertyNames . TASK _ JAVA _ OPTS _ S . getCandidates (  )  )  )  ;", "maybeSetJobMapMB ( extractMegabytes ( conf ,    JobConfPropertyNames . MAP _ JAVA _ OPTS _ S . getCandidates (  )  )  )  ;", "maybeSetJobReduceMB ( extractMegabytes ( conf ,    JobConfPropertyNames . REDUCE _ JAVA _ OPTS _ S . getCandidates (  )  )  )  ;", "this . jobConfigurationParameters    =    conf ;", "}", "METHOD_END"], "methodName": ["process"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( finalized )     {", "throw   new   IllegalStateException (  \" JobBuilder . process ( HistoryEvent   event )    called   after   ParsedJob   built \"  )  ;", "}", "if    ( event   instanceof   AMStartedEvent )     {", "return ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobFinishedEvent )     {", "processJobFinishedEvent (  (  ( mapreduce . jobhistory . JobFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobInfoChangeEvent )     {", "processJobInfoChangeEvent (  (  ( mapreduce . jobhistory . JobInfoChangeEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobInitedEvent )     {", "processJobInitedEvent (  (  ( mapreduce . jobhistory . JobInitedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobPriorityChangeEvent )     {", "processJobPriorityChangeEvent (  (  ( mapreduce . jobhistory . JobPriorityChangeEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobQueueChangeEvent )     {", "processJobQueueChangeEvent (  (  ( mapreduce . jobhistory . JobQueueChangeEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobStatusChangedEvent )     {", "processJobStatusChangedEvent (  (  ( mapreduce . jobhistory . JobStatusChangedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobSubmittedEvent )     {", "processJobSubmittedEvent (  (  ( mapreduce . jobhistory . JobSubmittedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . JobUnsuccessfulCompletionEvent )     {", "processJobUnsuccessfulCompletionEvent (  (  ( mapreduce . jobhistory . JobUnsuccessfulCompletionEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . MapAttemptFinishedEvent )     {", "processMapAttemptFinishedEvent (  (  ( mapreduce . jobhistory . MapAttemptFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . ReduceAttemptFinishedEvent )     {", "processReduceAttemptFinishedEvent (  (  ( mapreduce . jobhistory . ReduceAttemptFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskAttemptFinishedEvent )     {", "processTaskAttemptFinishedEvent (  (  ( mapreduce . jobhistory . TaskAttemptFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskAttemptStartedEvent )     {", "processTaskAttemptStartedEvent (  (  ( mapreduce . jobhistory . TaskAttemptStartedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskAttemptUnsuccessfulCompletionEvent )     {", "processTaskAttemptUnsuccessfulCompletionEvent (  (  ( mapreduce . jobhistory . TaskAttemptUnsuccessfulCompletionEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskFailedEvent )     {", "processTaskFailedEvent (  (  ( mapreduce . jobhistory . TaskFailedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskFinishedEvent )     {", "processTaskFinishedEvent (  (  ( mapreduce . jobhistory . TaskFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskStartedEvent )     {", "processTaskStartedEvent (  (  ( mapreduce . jobhistory . TaskStartedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskUpdatedEvent )     {", "processTaskUpdatedEvent (  (  ( mapreduce . jobhistory . TaskUpdatedEvent )     ( event )  )  )  ;", "} else", "throw   new   IllegalArgumentException (  \" JobBuilder . process ( HistoryEvent )  :    unknown   event   type \"  )  ;", "}", "METHOD_END"], "methodName": ["process"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setFinishTime ( event . getFinishTime (  )  )  ;", "result . setID ( jobID )  ;", "result . setOutcome ( Pre 2  1 HistoryConstants . Values . SUCCESS )  ;", "Finished   job    =     (  ( Finished )     ( event . getDatum (  )  )  )  ;", "Map < String ,    Long >    countersMap    =    HistoryUtils . extractCounters ( job . totalCounters )  ;", "result . putTotalCounters ( countersMap )  ;", "countersMap    =    HistoryUtils . extractCounters ( job . mapCounters )  ;", "result . putMapCounters ( countersMap )  ;", "countersMap    =    HistoryUtils . extractCounters ( job . reduceCounters )  ;", "result . putReduceCounters ( countersMap )  ;", "}", "METHOD_END"], "methodName": ["processJobFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setLaunchTime ( event . getLaunchTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["processJobInfoChangeEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setLaunchTime ( event . getLaunchTime (  )  )  ;", "result . setTotalMaps ( event . getTotalMaps (  )  )  ;", "result . setTotalReduces ( event . getTotalReduces (  )  )  ;", "}", "METHOD_END"], "methodName": ["processJobInitedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setPriority ( LoggedJob . JobPriority . valueOf ( event . getPriority (  )  . toString (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["processJobPriorityChangeEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "String   queue    =    event . getJobQueueName (  )  ;", "if    ( queue    !  =    null )     {", "result . setQueue ( queue )  ;", "}", "}", "METHOD_END"], "methodName": ["processJobQueueChangeEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setOutcome ( Pre 2  1 JobHistoryConstants . Values . valueOf ( event . getStatus (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["processJobStatusChangedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setJobID ( event . getJobId (  )  . toString (  )  )  ;", "result . setJobName ( event . getJobName (  )  )  ;", "result . setUser ( event . getUserName (  )  )  ;", "result . setSubmitTime ( event . getSubmitTime (  )  )  ;", "result . putJobConfPath ( event . getJobConfPath (  )  )  ;", "result . putJobAcls ( event . getJobAcls (  )  )  ;", "String   queue    =    event . getJobQueueName (  )  ;", "if    ( queue    !  =    null )     {", "result . setQueue ( queue )  ;", "}", "}", "METHOD_END"], "methodName": ["processJobSubmittedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "result . setOutcome ( Pre 2  1 JobHistoryConstants . Values . valueOf ( event . getStatus (  )  )  )  ;", "result . setFinishTime ( event . getFinishTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["processJobUnsuccessfulCompletionEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTaskAttempt   attempt    =    getOrMakeTaskAttempt ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    event . getAttemptId (  )  . toString (  )  )  ;", "if    ( attempt    =  =    null )     {", "return ;", "}", "attempt . setResult (  . getPre 2  1 Value ( event . getTaskStatus (  )  )  )  ;", "attempt . setHostName ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "ParsedHost   pHost    =    getAndRecordParsedHost ( event . getRackName (  )  ,    event . getHostname (  )  )  ;", "if    ( pHost    !  =    null )     {", "attempt . setLocation ( pHost . makeLoggedLocation (  )  )  ;", "}", "attempt . setFinishTime ( event . getFinishTime (  )  )  ;", "attempt . incorporateCounters (  (  ( MapAttemptFinished )     ( event . getDatum (  )  )  )  . counters )  ;", "attempt . arraySetClockSplits ( event . getClockSplits (  )  )  ;", "attempt . arraySetCpuUsages ( event . getCpuUsages (  )  )  ;", "attempt . arraySetVMemKbytes ( event . getVMemKbytes (  )  )  ;", "attempt . arraySetPhysMemKbytes ( event . getPhysMemKbytes (  )  )  ;", "}", "METHOD_END"], "methodName": ["processMapAttemptFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTaskAttempt   attempt    =    getOrMakeTaskAttempt ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    event . getAttemptId (  )  . toString (  )  )  ;", "if    ( attempt    =  =    null )     {", "return ;", "}", "attempt . setResult (  . getPre 2  1 Value ( event . getTaskStatus (  )  )  )  ;", "attempt . setHostName ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "ParsedHost   pHost    =    getAndRecordParsedHost ( event . getRackName (  )  ,    event . getHostname (  )  )  ;", "if    ( pHost    !  =    null )     {", "attempt . setLocation ( pHost . makeLoggedLocation (  )  )  ;", "}", "attempt . setFinishTime ( event . getFinishTime (  )  )  ;", "attempt . setShuffleFinished ( event . getShuffleFinishTime (  )  )  ;", "attempt . setSortFinished ( event . getSortFinishTime (  )  )  ;", "attempt . incorporateCounters (  (  ( ReduceAttemptFinished )     ( event . getDatum (  )  )  )  . counters )  ;", "attempt . arraySetClockSplits ( event . getClockSplits (  )  )  ;", "attempt . arraySetCpuUsages ( event . getCpuUsages (  )  )  ;", "attempt . arraySetVMemKbytes ( event . getVMemKbytes (  )  )  ;", "attempt . arraySetPhysMemKbytes ( event . getPhysMemKbytes (  )  )  ;", "}", "METHOD_END"], "methodName": ["processReduceAttemptFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTaskAttempt   attempt    =    getOrMakeTaskAttempt ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    event . getAttemptId (  )  . toString (  )  )  ;", "if    ( attempt    =  =    null )     {", "return ;", "}", "attempt . setResult (  . getPre 2  1 Value ( event . getTaskStatus (  )  )  )  ;", "ParsedHost   pHost    =    getAndRecordParsedHost ( event . getRackName (  )  ,    event . getHostname (  )  )  ;", "if    ( pHost    !  =    null )     {", "attempt . setLocation ( pHost . makeLoggedLocation (  )  )  ;", "}", "attempt . setFinishTime ( event . getFinishTime (  )  )  ;", "attempt . incorporateCounters (  (  ( TaskAttemptFinished )     ( event . getDatum (  )  )  )  . counters )  ;", "}", "METHOD_END"], "methodName": ["processTaskAttemptFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTaskAttempt   attempt    =    getOrMakeTaskAttempt ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    event . getTaskAttemptId (  )  . toString (  )  )  ;", "if    ( attempt    =  =    null )     {", "return ;", "}", "attempt . setStartTime ( event . getStartTime (  )  )  ;", "attempt . putTrackerName ( event . getTrackerName (  )  )  ;", "attempt . putHttpPort ( event . getHttpPort (  )  )  ;", "attempt . putShufflePort ( event . getShufflePort (  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskAttemptStartedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTaskAttempt   attempt    =    getOrMakeTaskAttempt ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    event . getTaskAttemptId (  )  . toString (  )  )  ;", "if    ( attempt    =  =    null )     {", "return ;", "}", "attempt . setResult (  . getPre 2  1 Value ( event . getTaskStatus (  )  )  )  ;", "attempt . setHostName ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "ParsedHost   pHost    =    getAndRecordParsedHost ( event . getRackName (  )  ,    event . getHostname (  )  )  ;", "if    ( pHost    !  =    null )     {", "attempt . setLocation ( pHost . makeLoggedLocation (  )  )  ;", "}", "attempt . setFinishTime ( event . getFinishTime (  )  )  ;", "JhCounters   counters    =     (  ( TaskAttemptUnsuccessfulCompletion )     ( event . getDatum (  )  )  )  . counters ;", "attempt . incorporateCounters (  ( counters    =  =    null    ?    EMPTY _ COUNTERS    :    counters )  )  ;", "attempt . arraySetClockSplits ( event . getClockSplits (  )  )  ;", "attempt . arraySetCpuUsages ( event . getCpuUsages (  )  )  ;", "attempt . arraySetVMemKbytes ( event . getVMemKbytes (  )  )  ;", "attempt . arraySetPhysMemKbytes ( event . getPhysMemKbytes (  )  )  ;", "TaskAttemptUnsuccessfulCompletion   t    =     (  ( TaskAttemptUnsuccessfulCompletion )     ( event . getDatum (  )  )  )  ;", "attempt . putDiagnosticInfo ( t . error . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskAttemptUnsuccessfulCompletionEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTask   task    =    getOrMakeTask ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    false )  ;", "if    ( task    =  =    null )     {", "return ;", "}", "task . setFinishTime ( event . getFinishTime (  )  )  ;", "task . setTaskStatus (  . getPre 2  1 Value ( event . getTaskStatus (  )  )  )  ;", "TaskFailed   t    =     (  ( TaskFailed )     ( event . getDatum (  )  )  )  ;", "task . putDiagnosticInfo ( t . error . toString (  )  )  ;", "task . putFailedDueToAttemptId ( t . failedDueToAttempt . toString (  )  )  ;", "JhCounters   counters    =     (  ( TaskFailed )     ( event . getDatum (  )  )  )  . counters ;", "task . incorporateCounters (  ( counters    =  =    null    ?    EMPTY _ COUNTERS    :    counters )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskFailedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTask   task    =    getOrMakeTask ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    false )  ;", "if    ( task    =  =    null )     {", "return ;", "}", "task . setFinishTime ( event . getFinishTime (  )  )  ;", "task . setTaskStatus (  . getPre 2  1 Value ( event . getTaskStatus (  )  )  )  ;", "task . incorporateCounters (  (  ( TaskFinished )     ( event . getDatum (  )  )  )  . counters )  ;", "}", "METHOD_END"], "methodName": ["processTaskFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTask   task    =    getOrMakeTask ( event . getTaskType (  )  ,    event . getTaskId (  )  . toString (  )  ,    true )  ;", "task . setStartTime ( event . getStartTime (  )  )  ;", "task . setPreferredLocations ( preferredLocationForSplits ( event . getSplitLocations (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskStartedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedTask   task    =    getTask ( event . getTaskId (  )  . toString (  )  )  ;", "if    ( task    =  =    null )     {", "return ;", "}", "task . setFinishTime ( event . getFinishTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskUpdatedEvent"], "fileName": "org.apache.hadoop.tools.rumen.JobBuilder"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . copyOf ( candidates ,    candidates . length )  ;", "}", "METHOD_END"], "methodName": ["getCandidates"], "fileName": "org.apache.hadoop.tools.rumen.JobConfPropertyNames"}, {"methodBody": ["METHOD_START", "{", "Properties   result    =    new   Properties (  )  ;", "try    {", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "Document   doc    =    db . parse ( input )  ;", "Element   root    =    doc . getDocumentElement (  )  ;", "if    (  !  (  \" configuration \"  . equals ( root . getTagName (  )  )  )  )     {", "System . out . print (  \" root   is   not   a   configuration   node \"  )  ;", "return   null ;", "}", "NodeList   props    =    root . getChildNodes (  )  ;", "for    ( int   i    =     0  ;    i    <     ( props . getLength (  )  )  ;     +  + i )     {", "Node   propNode    =    props . item ( i )  ;", "if    (  !  ( propNode   instanceof   Element )  )", "continue ;", "Element   prop    =     (  ( Element )     ( propNode )  )  ;", "if    (  !  (  \" property \"  . equals ( prop . getTagName (  )  )  )  )     {", "System . out . print (  \" bad   conf   file :    element   not    < property >  \"  )  ;", "}", "NodeList   fields    =    prop . getChildNodes (  )  ;", "String   attr    =    null ;", "String   value    =    null ;", "@ SuppressWarnings (  \" unused \"  )", "boolean   finalParameter    =    false ;", "for    ( int   j    =     0  ;    j    <     ( fields . getLength (  )  )  ;    j +  +  )     {", "Node   fieldNode    =    fields . item ( j )  ;", "if    (  !  ( fieldNode   instanceof   Element )  )     {", "continue ;", "}", "Element   field    =     (  ( Element )     ( fieldNode )  )  ;", "if    (  (  \" name \"  . equals ( field . getTagName (  )  )  )     &  &     ( field . hasChildNodes (  )  )  )     {", "attr    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "}", "if    (  (  \" value \"  . equals ( field . getTagName (  )  )  )     &  &     ( field . hasChildNodes (  )  )  )     {", "value    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  ;", "}", "if    (  (  \" final \"  . equals ( field . getTagName (  )  )  )     &  &     ( field . hasChildNodes (  )  )  )     {", "finalParameter    =     \" true \"  . equals (  (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  )  ;", "}", "}", "if    (  ( attr    !  =    null )     &  &     ( value    !  =    null )  )     {", "result . put ( attr ,    value )  ;", "}", "}", "}    catch    ( ParserException   e )     {", "return   null ;", "}    catch    ( SAXException   e )     {", "return   null ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.apache.hadoop.tools.rumen.JobConfigurationParser"}, {"methodBody": ["METHOD_START", "{", "for    ( JobHistoryParserFactory . VersionDetector   vd    :    JobHistoryParserFactory . VersionDetector . values (  )  )     {", "boolean   canParse    =    vd . canParse ( ris )  ;", "ris . rewind (  )  ;", "if    ( canParse )     {", "return   vd . newInstance ( ris )  ;", "}", "}", "throw   new   IOException (  \" No   suitable   parser .  \"  )  ;", "}", "METHOD_END"], "methodName": ["getParser"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryParserFactory"}, {"methodBody": ["METHOD_START", "{", "Matcher   matcher    =    pattern . matcher ( fileName )  ;", "if    (  !  ( matcher . matches (  )  )  )     {", "return   null ;", "}", "return   matcher . group (  1  )  ;", "}", "METHOD_END"], "methodName": ["applyParser"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Long >    countersMap    =    new   HashMap < String ,    Long >  (  )  ;", "if    ( counters    !  =    null )     {", "for    ( JhCounterGroup   group    :    counters . groups )     {", "for    ( JhCounter   counter    :    group . counts )     {", "countersMap . put ( counter . nameString (  )  ,    counter . value )  ;", "}", "}", "}", "return   countersMap ;", "}", "METHOD_END"], "methodName": ["extractCounters"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "String   jobId    =    JobHistoryUtils . extractJobIDFromConfFileName ( fileName )  ;", "if    ( jobId    =  =    null )     {", "jobId    =    JobHistoryUtils . extractJobIDFromHistoryFileName ( fileName )  ;", "}", "return   jobId ;", "}", "METHOD_END"], "methodName": ["extractJobID"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "String   pre 2  1 JobID    =    JobHistoryUtils . applyParser ( fileName ,    Pre 2  1 JobHistoryConstants . CONF _ FILENAME _ REGEX _ V 1  )  ;", "if    ( pre 2  1 JobID    =  =    null )     {", "pre 2  1 JobID    =    JobHistoryUtils . applyParser ( fileName ,    Pre 2  1 JobHistoryConstants . CONF _ FILENAME _ REGEX _ V 2  )  ;", "}", "if    ( pre 2  1 JobID    !  =    null )     {", "return   pre 2  1 JobID ;", "}", "return   JobHistoryUtils . applyParser ( fileName ,    CONF _ FILENAME _ REGEX )  ;", "}", "METHOD_END"], "methodName": ["extractJobIDFromConfFileName"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "JobID   id    =    null ;", "if    ( mapreduce . v 2  . jobhistory . JobHistoryUtils . isValidJobHistoryFileName ( fileName )  )     {", "try    {", "id    =    mapreduce . v 2  . jobhistory . JobHistoryUtils . getJobIDFromHistoryFilePath ( fileName )  ;", "}    catch    ( IOException   e )     {", "}", "}", "if    ( id    !  =    null )     {", "return   id . toString (  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["extractJobIDFromCurrentHistoryFile"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "String   jobID    =    JobHistoryUtils . extractJobIDFromCurrentHistoryFile ( fileName )  ;", "if    ( jobID    !  =    null )     {", "return   jobID ;", "}", "String   pre 2  1 JobID    =    JobHistoryUtils . applyParser ( fileName ,    Pre 2  1 JobHistoryConstants . JOBHISTORY _ FILENAME _ REGEX _ V 1  )  ;", "if    ( pre 2  1 JobID    =  =    null )     {", "pre 2  1 JobID    =    JobHistoryUtils . applyParser ( fileName ,    Pre 2  1 JobHistoryConstants . JOBHISTORY _ FILENAME _ REGEX _ V 2  )  ;", "}", "return   pre 2  1 JobID ;", "}", "METHOD_END"], "methodName": ["extractJobIDFromHistoryFileName"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "String   jobId    =    JobHistoryUtils . extractJobIDFromConfFileName ( fileName )  ;", "return   jobId    !  =    null ;", "}", "METHOD_END"], "methodName": ["isJobConfXml"], "fileName": "org.apache.hadoop.tools.rumen.JobHistoryUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   mapper . readValue ( json ,    clazz )  ;", "}    catch    ( EOFException   e )     {", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["getNext"], "fileName": "org.apache.hadoop.tools.rumen.JsonObjectMapperParser"}, {"methodBody": ["METHOD_START", "{", "writer . writeObject ( object )  ;", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.tools.rumen.JsonObjectMapperWriter"}, {"methodBody": ["METHOD_START", "{", "LogRecordType   result    =    LogRecordType . internees . get ( typeName )  ;", "if    ( result    =  =    null )     {", "result    =    new   LogRecordType ( typeName )  ;", "LogRecordType . internees . put ( typeName ,    result )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["intern"], "fileName": "org.apache.hadoop.tools.rumen.LogRecordType"}, {"methodBody": ["METHOD_START", "{", "return   LogRecordType . internees . get ( typeName )  ;", "}", "METHOD_END"], "methodName": ["internSoft"], "fileName": "org.apache.hadoop.tools.rumen.LogRecordType"}, {"methodBody": ["METHOD_START", "{", "Iterator < Map . Entry < String ,    LogRecordType >  >    iter    =    LogRecordType . internees . entrySet (  )  . iterator (  )  ;", "String [  ]    result    =    new   String [ LogRecordType . internees . size (  )  ]  ;", "for    ( int   i    =     0  ;    i    <     ( LogRecordType . internees . size (  )  )  ;     +  + i )     {", "result [ i ]     =    iter . next (  )  . getKey (  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["lineTypes"], "fileName": "org.apache.hadoop.tools.rumen.LogRecordType"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "urn ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "c 1  . get ( i )  . deepCompare ( c 2  . get ( i )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    mompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedDiscreteCDF )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedDiscreteCDF   other    =     (  ( LoggedDiscreteCDF )     ( comparand )  )  ;", "compare 1  ( numberValues ,    other . numberValues ,    loc ,     \" numberValues \"  )  ;", "compare 1  ( minimum ,    other . minimum ,    loc ,     \" minimum \"  )  ;", "compare 1  ( maximum ,    other . maximum ,    loc ,     \" maximum \"  )  ;", "compare 1  ( rankings ,    other . rankings ,    loc ,     \" rankings \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "return   maximum ;", "}", "METHOD_END"], "methodName": ["getMaximum"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "return   minimum ;", "}", "METHOD_END"], "methodName": ["getMinimum"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "return   numberValues ;", "}", "METHOD_END"], "methodName": ["getNumberValues"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "return   rankings ;", "}", "METHOD_END"], "methodName": ["getRankings"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "numberValues    =    data . getTotalCount (  )  ;", "long [  ]    CDF    =    data . getCDF ( modulus ,    steps )  ;", "if    ( CDF    !  =    null )     {", "minimum    =    CDF [  0  ]  ;", "maximum    =    CDF [  (  ( CDF . length )     -     1  )  ]  ;", "rankings    =    new   ArrayList < SingleRelativeRanking >  (  )  ;", "for    ( int   i    =     1  ;    i    <     (  ( CDF . length )     -     1  )  ;     +  + i )     {", "SingleRelativeRanking   srr    =    new   SingleRelativeRanking (  )  ;", "srr . setRelativeRanking (  (  (  ( double )     ( steps [  ( i    -     1  )  ]  )  )     /    modulus )  )  ;", "srr . setDatum ( CDF [ i ]  )  ;", "rankings . add ( srr )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setCDF"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "this . maximum    =    maximum ;", "}", "METHOD_END"], "methodName": ["setMaximum"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "this . minimum    =    minimum ;", "}", "METHOD_END"], "methodName": ["setMinimum"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "this . numberValues    =    numberValues ;", "}", "METHOD_END"], "methodName": ["setNumberValues"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "this . rankings    =    rankings ;", "}", "METHOD_END"], "methodName": ["setRankings"], "fileName": "org.apache.hadoop.tools.rumen.LoggedDiscreteCDF"}, {"methodBody": ["METHOD_START", "{", "submitTime    +  =    adjustment ;", "launchTime    +  =    adjustment ;", "finishTime    +  =    adjustment ;", "for    ( Task   task    :    mapTasks )     {", "task . adjustTimes ( adjustment )  ;", "}", "for    ( Task   task    :    reduceTasks )     {", "task . adjustTimes ( adjustment )  ;", "}", "for    ( Task   task    :    otherTasks )     {", "task . adjustTimes ( adjustment )  ;", "}", "}", "METHOD_END"], "methodName": ["adjustTimes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "TreePath   recursePath    =    new   TreePath ( loc ,    eltna )  ;", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . length )     !  =     ( c 2  . length )  )  )     {", "throw   new   DeepInequalityException (  ( eltna    +     \"    miscompared \"  )  ,    recursePath )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . length )  ;     +  + i )     {", "if    (  ( c 1  [ i ]  )     !  =     ( c 2  [ i ]  )  )     {", "throw   new   DeepInequalityException (  ( eltna    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltna ,    i )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  !  ( c 1  . equa ( c 2  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "TreePath   recursePath    =    new   TreePath ( loc ,    eltna ,    index )  ;", "if    (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     {", "if    ( index    =  =     (  -  1  )  )     {", "throw   new   DeepInequalityException (  ( eltna    +     \"    miscompared \"  )  ,    recursePath )  ;", "} else    {", "throw   new   DeepInequalityException (  (  (  ( eltna    +     \"  [  \"  )     +    index )     +     \"  ]    miscompared \"  )  ,    recursePath )  ;", "}", "}", "c 1  . deepCompare ( c 2  ,    recursePath )  ;", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "TreePath   dtPath    =    new   TreePath ( loc ,    eltname )  ;", "if    (  !  ( c 1  . getClass (  )  . getName (  )  . equals ( c 2  . getClass (  )  . getName (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( dtPath ,     \" class \"  )  )  ;", "}", "comre 1  ( c 1  . getValue (  )  ,    c 2  . getValue (  )  ,    dtPath ,     \" value \"  )  ;", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "c 1  . get ( i )  . deepComre ( c 2  . get ( i )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compareCDFs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( jprop 1     =  =    null )     &  &     ( jprop 2     =  =    null )  )     {", "return ;", "}", "if    (  ( jprop 1     =  =    null )     |  |     ( jprop 2     =  =    null )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "Properties   prop 1     =    jprop 1  . getValue (  )  ;", "Properties   prop 2     =    jprop 2  . getValue (  )  ;", "if    (  ( prop 1  . size (  )  )     !  =     ( prop 2  . size (  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared    [ size ]  \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( Map . Entry < Object ,    Object >    entry    :    prop 1  . entrySet (  )  )     {", "String   v 1     =    entry . getValue (  ) String (  )  ;", "String   v 2     =    prop 2  . get ( entry . getKey (  )  ) String (  )  ;", "compare 1  ( v 1  ,    v 2  ,    new   TreePath ( loc ,    eltname )  ,     (  \" key :  \"     +     ( entry . getKey (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compareJobProperties"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "c 1  . get ( i )  . deepComre ( c 2  . get ( i )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compareLoggedTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "TreePath   recursePath    =    new   TreePath ( loc ,    eltna )  ;", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  !  ( c 1  . equals ( c 2  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltna    +     \"    miscompared \"  )  ,    recursePath )  ;", "}", "}", "METHOD_END"], "methodName": ["compareStrings"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedJob )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedJob   other    =     (  ( LoggedJob )     ( comparand )  )  ;", "compare 1  ( jobID . toString (  )  ,    other . jobID . toString (  )  ,    loc ,     \" jobID \"  )  ;", "compare 1  ( user ,    other . user ,    loc ,     \" user \"  )  ;", "compare 1  ( computonsPerMapInputByte ,    other . computonsPerMapInputByte ,    loc ,     \" computonsPerMapInputByte \"  )  ;", "compare 1  ( computonsPerMapOutputByte ,    other . computonsPerMapOutputByte ,    loc ,     \" computonsPerMapOutputByte \"  )  ;", "compare 1  ( computonsPerReduceInputByte ,    other . computonsPerReduceInputByte ,    loc ,     \" computonsPerReduceInputByte \"  )  ;", "compare 1  ( computonsPerReduceOutputByte ,    other . computonsPerReduceOutputByte ,    loc ,     \" computonsPerReduceOutputByte \"  )  ;", "compare 1  ( submitTime ,    other . submitTime ,    loc ,     \" submitTime \"  )  ;", "compare 1  ( launchTime ,    other . launchTime ,    loc ,     \" launchTime \"  )  ;", "compare 1  ( finishTime ,    other . finishTime ,    loc ,     \" finishTime \"  )  ;", "compare 1  ( heapMegabytes ,    other . heapMegabytes ,    loc ,     \" heapMegabytes \"  )  ;", "compare 1  ( totalMaps ,    other . totalMaps ,    loc ,     \" totalMaps \"  )  ;", "compare 1  ( totalReduces ,    other . totalReduces ,    loc ,     \" totalReduces \"  )  ;", "compare 1  ( outcome ,    other . outcome ,    loc ,     \" outcome \"  )  ;", "compare 1  ( jobtype ,    other . jobtype ,    loc ,     \" jobtype \"  )  ;", "compare 1  ( priority ,    other . priority ,    loc ,     \" priority \"  )  ;", "compareStrings ( directDependantJobs ,    other . directDependantJobs ,    loc ,     \" directDependantJobs \"  )  ;", "compareLoggedTasks ( mapTasks ,    other . mapTasks ,    loc ,     \" mapTasks \"  )  ;", "compareLoggedTasks ( reduceTasks ,    other . reduceTasks ,    loc ,     \" reduceTasks \"  )  ;", "compareLoggedTasks ( otherTasks ,    other . otherTasks ,    loc ,     \" otherTasks \"  )  ;", "compare 1  ( relativeTime ,    other . relativeTime ,    loc ,     \" relativeTime \"  )  ;", "compareCDFs ( successfulMapAttemptCDFs ,    other . successfulMapAttemptCDFs ,    loc ,     \" successfulMapAttemptCDFs \"  )  ;", "compareCDFs ( failedMapAttemptCDFs ,    other . failedMapAttemptCDFs ,    loc ,     \" failedMapAttemptCDFs \"  )  ;", "compare 1  ( successfulReduceAttemptCDF ,    other . successfulReduceAttemptCDF ,    loc ,     \" successfulReduceAttemptCDF \"  ,     (  -  1  )  )  ;", "compare 1  ( failedReduceAttemptCDF ,    other . failedReduceAttemptCDF ,    loc ,     \" failedReduceAttemptCDF \"  ,     (  -  1  )  )  ;", "compare 1  ( mapperTriesToSucceed ,    other . mapperTriesToSucceed ,    loc ,     \" mapperTriesToSucceed \"  )  ;", "compare 1  ( failedMapperFraction ,    other . failedMapperFraction ,    loc ,     \" failedMapperFraction \"  )  ;", "compare 1  ( queue ,    other . queue ,    loc ,     \" queue \"  )  ;", "compare 1  ( jobName ,    other . jobName ,    loc ,     \" jobName \"  )  ;", "compare 1  ( clusterMapMB ,    other . clusterMapMB ,    loc ,     \" clusterMapMB \"  )  ;", "compare 1  ( clusterReduceMB ,    other . clusterReduceMB ,    loc ,     \" clusterReduceMB \"  )  ;", "compare 1  ( jobMapMB ,    other . jobMapMB ,    loc ,     \" jobMapMB \"  )  ;", "compare 1  ( jobReduceMB ,    other . jobReduceMB ,    loc ,     \" jobReduceMB \"  )  ;", "compareJobProperties ( jobProperties ,    other . getJobProperties (  )  ,    loc ,     \" JobProperties \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   clusterMapMB ;", "}", "METHOD_END"], "methodName": ["getClusterMapMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   clusterReduceMB ;", "}", "METHOD_END"], "methodName": ["getClusterReduceMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   computonsPerMapInputByte ;", "}", "METHOD_END"], "methodName": ["getComputonsPerMapInputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   computonsPerMapOutputByte ;", "}", "METHOD_END"], "methodName": ["getComputonsPerMapOutputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   computonsPerReduceInputByte ;", "}", "METHOD_END"], "methodName": ["getComputonsPerReduceInputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   computonsPerReduceOutputByte ;", "}", "METHOD_END"], "methodName": ["getComputonsPerReduceOutputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   directDependantJobs ;", "}", "METHOD_END"], "methodName": ["getDirectDependantJobs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   failedMapAttemptCDFs ;", "}", "METHOD_END"], "methodName": ["getFailedMapAttemptCDFs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   failedMapperFraction ;", "}", "METHOD_END"], "methodName": ["getFailedMapperFraction"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   failedReduceAttemptCDF ;", "}", "METHOD_END"], "methodName": ["getFailedReduceAttemptCDF"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   finishTime ;", "}", "METHOD_END"], "methodName": ["getFinishTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   heapMegabytes ;", "}", "METHOD_END"], "methodName": ["getHeapMegabytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobID ;", "}", "METHOD_END"], "methodName": ["getJobID"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobMapMB ;", "}", "METHOD_END"], "methodName": ["getJobMapMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobName ;", "}", "METHOD_END"], "methodName": ["getJobName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobProperties ;", "}", "METHOD_END"], "methodName": ["getJobProperties"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobReduceMB ;", "}", "METHOD_END"], "methodName": ["getJobReduceMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobtype ;", "}", "METHOD_END"], "methodName": ["getJobtype"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   launchTime ;", "}", "METHOD_END"], "methodName": ["getLaunchTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   mapTasks ;", "}", "METHOD_END"], "methodName": ["getMapTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   mapperTriesToSucceed ;", "}", "METHOD_END"], "methodName": ["getMapperTriesToSucceed"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   otherTasks ;", "}", "METHOD_END"], "methodName": ["getOtherTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   outcome ;", "}", "METHOD_END"], "methodName": ["getOutcome"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   reduceTasks ;", "}", "METHOD_END"], "methodName": ["getReduceTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   relativeTime ;", "}", "METHOD_END"], "methodName": ["getRelativeTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   submitTime ;", "}", "METHOD_END"], "methodName": ["getSubmitTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   successfulMapAttemptCDFs ;", "}", "METHOD_END"], "methodName": ["getSuccessfulMapAttemptCDFs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   successfulReduceAttemptCDF ;", "}", "METHOD_END"], "methodName": ["getSuccessfulReduceAttemptCDF"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   totalMaps ;", "}", "METHOD_END"], "methodName": ["getTotalMaps"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   totalReduces ;", "}", "METHOD_END"], "methodName": ["getTotalReduces"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . clusterMapMB    =    clusterMapMB ;", "}", "METHOD_END"], "methodName": ["setClusterMapMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . clusterReduceMB    =    clusterReduceMB ;", "}", "METHOD_END"], "methodName": ["setClusterReduceMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . computonsPerMapInputByte    =    computonsPerMapInputByte ;", "}", "METHOD_END"], "methodName": ["setComputonsPerMapInputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . computonsPerMapOutputByte    =    computonsPerMapOutputByte ;", "}", "METHOD_END"], "methodName": ["setComputonsPerMapOutputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . computonsPerReduceInputByte    =    computonsPerReduceInputByte ;", "}", "METHOD_END"], "methodName": ["setComputonsPerReduceInputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . computonsPerReduceOutputByte    =    computonsPerReduceOutputByte ;", "}", "METHOD_END"], "methodName": ["setComputonsPerReduceOutputByte"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . directDependantJobs    =    directDependantJobs ;", "}", "METHOD_END"], "methodName": ["setDirectDependantJobs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . failedMapAttemptCDFs    =    failedMapAttemptCDFs ;", "}", "METHOD_END"], "methodName": ["setFailedMapAttemptCDFs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . failedMapperFraction    =    failedMapperFraction ;", "}", "METHOD_END"], "methodName": ["setFailedMapperFraction"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . failedReduceAttemptCDF    =    failedReduceAttemptCDF ;", "}", "METHOD_END"], "methodName": ["setFailedReduceAttemptCDF"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . finishTime    =    finishTime ;", "}", "METHOD_END"], "methodName": ["setFinishTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . heapMegabytes    =    heapMegabytes ;", "}", "METHOD_END"], "methodName": ["setHeapMegabytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . jobID    =    JobID . forName ( jobID )  ;", "}", "METHOD_END"], "methodName": ["setJobID"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . jobMapMB    =    jobMapMB ;", "}", "METHOD_END"], "methodName": ["setJobMapMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . jobName    =    new   JobName ( jobName )  ;", "}", "METHOD_END"], "methodName": ["setJobName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . jobProperties    =    new   JobProperties ( conf )  ;", "}", "METHOD_END"], "methodName": ["setJobProperties"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . jobReduceMB    =    jobReduceMB ;", "}", "METHOD_END"], "methodName": ["setJobReduceMB"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . jobtype    =    jobtype ;", "}", "METHOD_END"], "methodName": ["setJobtype"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . launchTime    =    startTime ;", "}", "METHOD_END"], "methodName": ["setLaunchTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . mapTasks    =    mapTasks ;", "}", "METHOD_END"], "methodName": ["setMapTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . mapperTriesToSucceed    =    mapperTriesToSucceed ;", "}", "METHOD_END"], "methodName": ["setMapperTriesToSucceed"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . otherTasks    =    otherTasks ;", "}", "METHOD_END"], "methodName": ["setOtherTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . outcome    =    outcome ;", "}", "METHOD_END"], "methodName": ["setOutcome"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . priority    =    priority ;", "}", "METHOD_END"], "methodName": ["setPriority"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . queue    =    new   QueueName ( queue )  ;", "}", "METHOD_END"], "methodName": ["setQueue"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . reduceTasks    =    reduceTasks ;", "}", "METHOD_END"], "methodName": ["setReduceTasks"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . relativeTime    =    relativeTime ;", "}", "METHOD_END"], "methodName": ["setRelativeTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . submitTime    =    submitTime ;", "}", "METHOD_END"], "methodName": ["setSubmitTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . successfulMapAttemptCDFs    =    successfulMapAttemptCDFs ;", "}", "METHOD_END"], "methodName": ["setSuccessfulMapAttemptCDFs"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . successfulReduceAttemptCDF    =    successfulReduceAttemptCDF ;", "}", "METHOD_END"], "methodName": ["setSuccessfulReduceAttemptCDF"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . totalMaps    =    totalMaps ;", "}", "METHOD_END"], "methodName": ["setTotalMaps"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . totalReduces    =    totalReduces ;", "}", "METHOD_END"], "methodName": ["setTotalReduces"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( LoggedJob . alreadySeenAnySetterAttributes . contains ( attributeName )  )  )     {", "LoggedJob . alreadySeenAnySetterAttributes . add ( attributeName )  ;", "System . err . println (  (  (  \" In   LoggedJob ,    we   saw   the   unknown   attribute    \"     +    attributeName )     +     \"  .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUnknownAttribute"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "this . user    =    new   UserName ( user )  ;", "}", "METHOD_END"], "methodName": ["setUser"], "fileName": "org.apache.hadoop.tools.rumen.LoggedJob"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "TreePath   recursePath    =    new   TreePath ( loc ,    eltname )  ;", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityExcep (  ( eltname    +     \"    miscompared \"  )  ,    recursePath )  ;", "}", "for    ( NodeName   n 1     :    c 1  )     {", "boolean   found    =    false ;", "for    ( NodeName   n 2     :    c 2  )     {", "if    ( n 1  . getValue (  )  . equals ( n 2  . getValue (  )  )  )     {", "found    =    true ;", "break ;", "}", "}", "if    (  ! found )     {", "throw   new   DeepInequalityExcep (  (  (  ( eltname    +     \"    miscompared    [  \"  )     +     ( n 1  . getValue (  )  )  )     +     \"  ]  \"  )  ,    recursePath )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["compareStrings"], "fileName": "org.apache.hadoop.tools.rumen.LoggedLocation"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedLocation )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedLocation   other    =     (  ( LoggedLocation )     ( comparand )  )  ;", "compareStrings ( layers ,    other . layers ,    loc ,     \" layers \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedLocation"}, {"methodBody": ["METHOD_START", "{", "return   layers ;", "}", "METHOD_END"], "methodName": ["getLayers"], "fileName": "org.apache.hadoop.tools.rumen.LoggedLocation"}, {"methodBody": ["METHOD_START", "{", "if    (  ( layers    =  =    null )     |  |     ( layers . isEmpty (  )  )  )     {", "this . layers    =    Collections . emptyList (  )  ;", "} else    {", "synchronized (  . layersCache )     {", "List < NodeName >    found    =     . layersCache . get ( layers )  ;", "if    ( found    =  =    null )     {", "List < NodeName >    clone    =    new   ArrayList < NodeName >  ( layers . size (  )  )  ;", "clone . add ( new   NodeName ( layers . get (  0  )  . intern (  )  ,    null )  )  ;", "clone . add ( new   NodeName ( null ,    layers . get (  1  )  . intern (  )  )  )  ;", "List < NodeName >    readonlyLayers    =    Collections . unmodifiableList ( clone )  ;", "List < String >    readonlyLayersKey    =    Collections . unmodifiableList ( layers )  ;", ". layersCache . put ( readonlyLayersKey ,    readonlyLayers )  ;", "this . layers    =    readonlyLayers ;", "} else    {", "this . layers    =    found ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["setLayers"], "fileName": "org.apache.hadoop.tools.rumen.LoggedLocation"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( LoggedLocation . alreadySeenAnySetterAttributes . contains ( attributeName )  )  )     {", "LoggedLocation . alreadySeenAnySetterAttributes . add ( attributeName )  ;", "System . err . println (  (  (  \" In   LoggedJob ,    we   saw   the   unknown   attribute    \"     +    attributeName )     +     \"  .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUnknownAttribute"], "fileName": "org.apache.hadoop.tools.rumen.LoggedLocation"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "Collections . sort ( c 1  ,    new    . TopoSort (  )  )  ;", "Collections . sort ( c 2  ,    new    . TopoSort (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "c 1  . get ( i )  . deepCompare ( c 2  . get ( i )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedNetworkTopology )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedNetworkTopology   other    =     (  ( LoggedNetworkTopology )     ( comparand )  )  ;", "compare 1  ( children ,    other . children ,    loc ,     \" children \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "return   children ;", "}", "METHOD_END"], "methodName": ["getChildren"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "this . children    =    children ;", "}", "METHOD_END"], "methodName": ["setChildren"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "this . name    =    new   NodeName ( name )  ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( LoggedNetworkTopology . alreadySeenAnySetterAttributes . contains ( attributeName )  )  )     {", "LoggedNetworkTopology . alreadySeenAnySetterAttributes . add ( attributeName )  ;", "System . err . println (  (  (  \" In   LoggedJob ,    we   saw   the   unknown   attribute    \"     +    attributeName )     +     \"  .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUnknownAttribute"], "fileName": "org.apache.hadoop.tools.rumen.LoggedNetworkTopology"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedSingleRelativeRanking )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedSingleRelativeRanking   other    =     (  ( LoggedSingleRelativeRanking )     ( comparand )  )  ;", "compare 1  ( relativeRanking ,    other . relativeRanking ,    loc ,     \" relativeRanking \"  )  ;", "compare 1  ( datum ,    other . datum ,    loc ,     \" datum \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "return   datum ;", "}", "METHOD_END"], "methodName": ["getDatum"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "return   relativeRanking ;", "}", "METHOD_END"], "methodName": ["getRelativeRanking"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "this . datum    =    datum ;", "}", "METHOD_END"], "methodName": ["setDatum"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "this . relativeRanking    =    relativeRanking ;", "}", "METHOD_END"], "methodName": ["setRelativeRanking"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( LoggedSingleRelativeRanking . alreadySeenAnySetterAttributes . contains ( attributeName )  )  )     {", "LoggedSingleRelativeRanking . alreadySeenAnySetterAttributes . add ( attributeName )  ;", "System . err . println (  (  (  \" In   LoggedJob ,    we   saw   the   unknown   attribute    \"     +    attributeName )     +     \"  .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUnknownAttribute"], "fileName": "org.apache.hadoop.tools.rumen.LoggedSingleRelativeRanking"}, {"methodBody": ["METHOD_START", "{", "startTime    +  =    adjustment ;", "finishTime    +  =    adjustment ;", "for    ( Attempt   attempt    :    attempts )     {", "attempt . adjustTimes ( adjustment )  ;", "}", "}", "METHOD_END"], "methodName": ["adjustTimes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "String   result    =    nonCanonicalName . toLowerCase (  )  ;", "result    =    result . replace (  '     '  ,     '  |  '  )  ;", "result    =    result . replace (  '  -  '  ,     '  |  '  )  ;", "result    =    result . replace (  '  _  '  ,     '  |  '  )  ;", "result    =    result . replace (  '  .  '  ,     '  |  '  )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["canonicalizeCounterName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  !  ( c 1  . equa ( c 2  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  !  ( c 1  . equa ( c 2  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "c 1  . get ( i )  . deepComre ( c 2  . get ( i )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compareLoggedLocations"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "c 1  . get ( i )  . deepComre ( c 2  . get ( i )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compareLoggedTaskAttempts"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedTask )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedTask   other    =     (  ( LoggedTask )     ( comparand )  )  ;", "compare 1  ( inputBytes ,    other . inputBytes ,    loc ,     \" inputBytes \"  )  ;", "compare 1  ( inputRecords ,    other . inputRecords ,    loc ,     \" inputRecords \"  )  ;", "compare 1  ( outputBytes ,    other . outputBytes ,    loc ,     \" outputBytes \"  )  ;", "compare 1  ( outputRecords ,    other . outputRecords ,    loc ,     \" outputRecords \"  )  ;", "compare 1  ( taskID . toString (  )  ,    other . taskID . toString (  )  ,    loc ,     \" taskID \"  )  ;", "compare 1  ( startTime ,    other . startTime ,    loc ,     \" startTime \"  )  ;", "compare 1  ( finishTime ,    other . finishTime ,    loc ,     \" finishTime \"  )  ;", "compare 1  ( taskType ,    other . taskType ,    loc ,     \" taskType \"  )  ;", "compare 1  ( taskStatus ,    other . taskStatus ,    loc ,     \" taskStatus \"  )  ;", "compareLoggedTaskAttempts ( attempts ,    other . attempts ,    loc ,     \" attempts \"  )  ;", "compareLoggedLocations ( preferredLocations ,    other . preferredLocations ,    loc ,     \" preferredLocations \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   attempts ;", "}", "METHOD_END"], "methodName": ["getAttempts"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   finishTime ;", "}", "METHOD_END"], "methodName": ["getFinishTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   inputBytes ;", "}", "METHOD_END"], "methodName": ["getInputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   inputRecords ;", "}", "METHOD_END"], "methodName": ["getInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   outputBytes ;", "}", "METHOD_END"], "methodName": ["getOutputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   outputRecords ;", "}", "METHOD_END"], "methodName": ["getOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   preferredLocations ;", "}", "METHOD_END"], "methodName": ["getPreferredLocations"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   startTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   taskID ;", "}", "METHOD_END"], "methodName": ["getTaskID"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   taskStatus ;", "}", "METHOD_END"], "methodName": ["getTaskStatus"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "return   taskType ;", "}", "METHOD_END"], "methodName": ["getTaskType"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "counterName    =    LoggedTask . canonicalizeCounterName ( counterName )  ;", "for    ( JhCounterGroup   group    :    counters . groups )     {", "for    ( JhCounter   counter    :    group . counts )     {", "if    ( counterName . equals ( LoggedTask . canonicalizeCounterName ( counter . name . toString (  )  )  )  )     {", "thunk . set ( counter . value )  ;", "return ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["incorporateCounter"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "switch    ( taskType )     {", "case   MAP    :", "incorporateMapCounters ( counters )  ;", "return ;", "case   REDUCE    :", "incorporateReduceCounters ( counters )  ;", "return ;", "}", "}", "METHOD_END"], "methodName": ["incorporateCounters"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . inputBytes    =    val ;", "}", "}  ,    counters ,     \" HDFS _ BYTES _ READ \"  )  ;", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . outputBytes    =    val ;", "}", "}  ,    counters ,     \" FILE _ BYTES _ WRITTEN \"  )  ;", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . inputRecords    =    val ;", "}", "}  ,    counters ,     \" MAP _ INPUT _ RECORDS \"  )  ;", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . outputRecords    =    val ;", "}", "}  ,    counters ,     \" MAP _ OUTPUT _ RECORDS \"  )  ;", "}", "METHOD_END"], "methodName": ["incorporateMapCounters"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . inputBytes    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ SHUFFLE _ BYTES \"  )  ;", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . outputBytes    =    val ;", "}", "}  ,    counters ,     \" HDFS _ BYTES _ WRITTEN \"  )  ;", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . inputRecords    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ INPUT _ RECORDS \"  )  ;", "LoggedTask . incorporateCounter ( new   LoggedTask . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "task . outputRecords    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ OUTPUT _ RECORDS \"  )  ;", "}", "METHOD_END"], "methodName": ["incorporateReduceCounters"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    ( attempts    =  =    null )     {", "this . attempts    =    new   ArrayList < Attempt >  (  )  ;", "} else    {", "this . attempts    =    attempts ;", "}", "}", "METHOD_END"], "methodName": ["setAttempts"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . finishTime    =    finishTime ;", "}", "METHOD_END"], "methodName": ["setFinishTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . inputBytes    =    inputBytes ;", "}", "METHOD_END"], "methodName": ["setInputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . inputRecords    =    inputRecords ;", "}", "METHOD_END"], "methodName": ["setInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . outputBytes    =    outputBytes ;", "}", "METHOD_END"], "methodName": ["setOutputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . outputRecords    =    outputRecords ;", "}", "METHOD_END"], "methodName": ["setOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  ( preferredLocations    =  =    null )     |  |     ( preferredLocations . isEmpty (  )  )  )     {", "this . preferredLocations    =    Collections . emptyList (  )  ;", "} else    {", "this . preferredLocations    =    preferredLocations ;", "}", "}", "METHOD_END"], "methodName": ["setPreferredLocations"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . startTime    =    startTime ;", "}", "METHOD_END"], "methodName": ["setStartTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . taskID    =    TaskID . forName ( taskID )  ;", "}", "METHOD_END"], "methodName": ["setTaskID"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . taskStatus    =    taskStatus ;", "}", "METHOD_END"], "methodName": ["setTaskStatus"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "this . taskType    =    taskType ;", "}", "METHOD_END"], "methodName": ["setTaskType"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( LoggedTask . alreadySeenAnySetterAttributes . contains ( attributeName )  )  )     {", "LoggedTask . alreadySeenAnySetterAttributes . add ( attributeName )  ;", "System . err . println (  (  (  \" In   LoggedJob ,    we   saw   the   unknown   attribute    \"     +    attributeName )     +     \"  .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUnknownAttribute"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTask"}, {"methodBody": ["METHOD_START", "{", "startTime    +  =    adjustment ;", "finishTime    +  =    adjustment ;", "if    (  ( sortFinished )     >  =     0  )     {", "shuffleFinished    +  =    adjustment ;", "sortFinished    +  =    adjustment ;", "}", "}", "METHOD_END"], "methodName": ["adjustTimes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "List < List < Integer >  >    result    =    new   ArrayList < List < Integer >  >  ( LoggedTaskAttempt . SplitVectorKind . values (  )  . length )  ;", "for    ( LoggedTaskAttempt . SplitVectorKind   kind    :    LoggedTaskAttempt . SplitVectorKind . values (  )  )     {", "result . add ( kind . get ( this )  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["allSplitVectors"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "List < Integer >    result    =    new   ArrayList < Integer >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( clockSplits . length )  ;     +  + i )     {", "result . add ( clockSplits [ i ]  )  ;", "}", "this . clockSplits    =    result ;", "}", "METHOD_END"], "methodName": ["arraySetClockSplits"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "List < Integer >    result    =    new   ArrayList < Integer >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( cpuUsages . length )  ;     +  + i )     {", "result . add ( cpuUsages [ i ]  )  ;", "}", "this . cpuUsages    =    result ;", "}", "METHOD_END"], "methodName": ["arraySetCpuUsages"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "List < Integer >    result    =    new   ArrayList < Integer >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( physMemKbytes . length )  ;     +  + i )     {", "result . add ( physMemKbytes [ i ]  )  ;", "}", "this . physMemKbytes    =    result ;", "}", "METHOD_END"], "methodName": ["arraySetPhysMemKbytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "List < Integer >    result    =    new   ArrayList < Integer >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( vMemKbytes . length )  ;     +  + i )     {", "result . add ( vMemKbytes [ i ]  )  ;", "}", "this . vMemKbytes    =    result ;", "}", "METHOD_END"], "methodName": ["arraySetVMemKbytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "String   result    =    nonCanonicalName . toLowerCase (  )  ;", "result    =    result . replace (  '     '  ,     '  |  '  )  ;", "result    =    result . replace (  '  -  '  ,     '  |  '  )  ;", "result    =    result . replace (  '  _  '  ,     '  |  '  )  ;", "result    =    result . replace (  '  .  '  ,     '  |  '  )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["canonicalizeCounterName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  !  ( c 1  . equa ( c 2  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscompared \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     |  |     (  ( c 1  . size (  )  )     !  =     ( c 2  . size (  )  )  )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( c 1  . size (  )  )  ;     +  + i )     {", "if    (  !  ( c 1  . get ( i )  . equals ( c 2  . get ( i )  )  )  )     {", "throw   new   DeepInequalityException (  (  (  (  \"  \"     +     ( c 1  . get ( i )  )  )     +     \"     !  =     \"  )     +     ( c 2  . get ( i )  )  )  ,    new   TreePath ( loc ,    eltname ,    i )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "TreePath   recurse    =    new   TreePath ( loc ,    eltna )  ;", "if    (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     {", "throw   new   DeepInequalityException (  ( eltna    +     \"    miscompared \"  )  ,    recurse )  ;", "}", "c 1  . deepCompare ( c 2  ,    recurse )  ;", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( c 1     !  =    c 2  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( c 1     =  =    null )     &  &     ( c 2     =  =    null )  )     {", "return ;", "}", "if    (  ( c 1     =  =    null )     |  |     ( c 2     =  =    null )  )     {", "throw   new   DeepInequalityException (  ( eltname    +     \"    miscomred \"  )  ,    new   TreePath ( loc ,    eltname )  )  ;", "}", "comre 1  ( c 1  . getValue (  )  ,    c 2  . getValue (  )  ,    new   TreePath ( loc ,    eltname )  ,     \" value \"  )  ;", "}", "METHOD_END"], "methodName": ["compare1"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( comparand   instanceof   LoggedTaskAttempt )  )     {", "throw   new   DeepInequalityException (  \" comparand   has   wrong   type \"  ,    loc )  ;", "}", "LoggedTaskAttempt   other    =     (  ( LoggedTaskAttempt )     ( comparand )  )  ;", "compare 1  ( attemptID . toString (  )  ,    other . attemptID . toString (  )  ,    loc ,     \" attemptID \"  )  ;", "compare 1  ( result ,    other . result ,    loc ,     \" result \"  )  ;", "compare 1  ( startTime ,    other . startTime ,    loc ,     \" startTime \"  )  ;", "compare 1  ( finishTime ,    other . finishTime ,    loc ,     \" finishTime \"  )  ;", "compare 1  ( hostName ,    other . hostName ,    loc ,     \" hostName \"  )  ;", "compare 1  ( hdfsBytesRead ,    other . hdfsBytesRead ,    loc ,     \" hdfsBytesRead \"  )  ;", "compare 1  ( hdfsBytesWritten ,    other . hdfsBytesWritten ,    loc ,     \" hdfsBytesWritten \"  )  ;", "compare 1  ( fileBytesRead ,    other . fileBytesRead ,    loc ,     \" fileBytesRead \"  )  ;", "compare 1  ( fileBytesWritten ,    other . fileBytesWritten ,    loc ,     \" fileBytesWritten \"  )  ;", "compare 1  ( mapInputBytes ,    other . mapInputBytes ,    loc ,     \" mapInputBytes \"  )  ;", "compare 1  ( mapInputRecords ,    other . mapInputRecords ,    loc ,     \" mapInputRecords \"  )  ;", "compare 1  ( mapOutputBytes ,    other . mapOutputBytes ,    loc ,     \" mapOutputBytes \"  )  ;", "compare 1  ( mapOutputRecords ,    other . mapOutputRecords ,    loc ,     \" mapOutputRecords \"  )  ;", "compare 1  ( combineInputRecords ,    other . combineInputRecords ,    loc ,     \" combineInputRecords \"  )  ;", "compare 1  ( reduceInputGroups ,    other . reduceInputGroups ,    loc ,     \" reduceInputGroups \"  )  ;", "compare 1  ( reduceInputRecords ,    other . reduceInputRecords ,    loc ,     \" reduceInputRecords \"  )  ;", "compare 1  ( reduceShuffleBytes ,    other . reduceShuffleBytes ,    loc ,     \" reduceShuffleBytes \"  )  ;", "compare 1  ( reduceOutputRecords ,    other . reduceOutputRecords ,    loc ,     \" reduceOutputRecords \"  )  ;", "compare 1  ( spilledRecords ,    other . spilledRecords ,    loc ,     \" spilledRecords \"  )  ;", "compare 1  ( shuffleFinished ,    other . shuffleFinished ,    loc ,     \" shuffleFinished \"  )  ;", "compare 1  ( sortFinished ,    other . sortFinished ,    loc ,     \" sortFinished \"  )  ;", "compare 1  ( location ,    other . location ,    loc ,     \" location \"  )  ;", "compare 1  ( clockSplits ,    other . clockSplits ,    loc ,     \" clockSplits \"  )  ;", "compare 1  ( cpuUsages ,    other . cpuUsages ,    loc ,     \" cpuUsages \"  )  ;", "compare 1  ( vMemKbytes ,    other . vMemKbytes ,    loc ,     \" vMemKbytes \"  )  ;", "compare 1  ( physMemKbytes ,    other . physMemKbytes ,    loc ,     \" physMemKbytes \"  )  ;", "}", "METHOD_END"], "methodName": ["deepCompare"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   attemptID ;", "}", "METHOD_END"], "methodName": ["getAttemptID"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   clockSplits ;", "}", "METHOD_END"], "methodName": ["getClockSplits"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   combineInputRecords ;", "}", "METHOD_END"], "methodName": ["getCombineInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   cpuUsages ;", "}", "METHOD_END"], "methodName": ["getCpuUsages"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   fileBytesRead ;", "}", "METHOD_END"], "methodName": ["getFileBytesRead"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   fileBytesWritten ;", "}", "METHOD_END"], "methodName": ["getFileBytesWritten"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   finishTime ;", "}", "METHOD_END"], "methodName": ["getFinishTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   hdfsBytesRead ;", "}", "METHOD_END"], "methodName": ["getHdfsBytesRead"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   hdfsBytesWritten ;", "}", "METHOD_END"], "methodName": ["getHdfsBytesWritten"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   hostName ;", "}", "METHOD_END"], "methodName": ["getHostName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   location ;", "}", "METHOD_END"], "methodName": ["getLocation"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   mapInputBytes ;", "}", "METHOD_END"], "methodName": ["getMapInputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   mapInputRecords ;", "}", "METHOD_END"], "methodName": ["getMapInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   mapOutputBytes ;", "}", "METHOD_END"], "methodName": ["getMapOutputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   mapOutputRecords ;", "}", "METHOD_END"], "methodName": ["getMapOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   physMemKbytes ;", "}", "METHOD_END"], "methodName": ["getPhysMemKbytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   reduceInputGroups ;", "}", "METHOD_END"], "methodName": ["getReduceInputGroups"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   reduceInputRecords ;", "}", "METHOD_END"], "methodName": ["getReduceInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   reduceOutputRecords ;", "}", "METHOD_END"], "methodName": ["getReduceOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   reduceShuffleBytes ;", "}", "METHOD_END"], "methodName": ["getReduceShuffleBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   metrics ;", "}", "METHOD_END"], "methodName": ["getResourceUsageMetrics"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   result ;", "}", "METHOD_END"], "methodName": ["getResult"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   shuffleFinished ;", "}", "METHOD_END"], "methodName": ["getShuffleFinished"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   sortFinished ;", "}", "METHOD_END"], "methodName": ["getSortFinished"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   spilledRecords ;", "}", "METHOD_END"], "methodName": ["getSpilledRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   startTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   vMemKbytes ;", "}", "METHOD_END"], "methodName": ["getVMemKbytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "counterName    =    LoggedTaskAttempt . canonicalizeCounterName ( counterName )  ;", "for    ( JhCounterGroup   group    :    counters . groups )     {", "for    ( JhCounter   counter    :    group . counts )     {", "if    ( counterName . equals ( LoggedTaskAttempt . canonicalizeCounterName ( counter . name . toString (  )  )  )  )     {", "thunk . set ( counter . value )  ;", "return ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["incorporateCounter"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . hdfsBytesRead    =    val ;", "}", "}  ,    counters ,     \" HDFS _ BYTES _ READ \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . hdfsBytesWritten    =    val ;", "}", "}  ,    counters ,     \" HDFS _ BYTES _ WRITTEN \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . fileBytesRead    =    val ;", "}", "}  ,    counters ,     \" FILE _ BYTES _ READ \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . fileBytesWritten    =    val ;", "}", "}  ,    counters ,     \" FILE _ BYTES _ WRITTEN \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapInputBytes    =    val ;", "}", "}  ,    counters ,     \" MAP _ INPUT _ BYTES \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapInputRecords    =    val ;", "}", "}  ,    counters ,     \" MAP _ INPUT _ RECORDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapOutputBytes    =    val ;", "}", "}  ,    counters ,     \" MAP _ OUTPUT _ BYTES \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . mapOutputRecords    =    val ;", "}", "}  ,    counters ,     \" MAP _ OUTPUT _ RECORDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . combineInputRecords    =    val ;", "}", "}  ,    counters ,     \" COMBINE _ INPUT _ RECORDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceInputGroups    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ INPUT _ GROUPS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceInputRecords    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ INPUT _ RECORDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceShuffleBytes    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ SHUFFLE _ BYTES \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . reduceOutputRecords    =    val ;", "}", "}  ,    counters ,     \" REDUCE _ OUTPUT _ RECORDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "attempt . spilledRecords    =    val ;", "}", "}  ,    counters ,     \" SPILLED _ RECORDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "metrics . setCumulativeCpuUsage ( val )  ;", "}", "}  ,    counters ,     \" CPU _ MILLISECONDS \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "metrics . setVirtualMemoryUsage ( val )  ;", "}", "}  ,    counters ,     \" VIRTUAL _ MEMORY _ BYTES \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "metrics . setPhysicalMemoryUsage ( val )  ;", "}", "}  ,    counters ,     \" PHYSICAL _ MEMORY _ BYTES \"  )  ;", "LoggedTaskAttempt . incorporateCounter ( new   LoggedTaskAttempt . SetField ( this )     {", "@ Override", "void   set ( long   val )     {", "metrics . setHeapUsage ( val )  ;", "}", "}  ,    counters ,     \" COMMITTED _ HEAP _ BYTES \"  )  ;", "}", "METHOD_END"], "methodName": ["incorporateCounters"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . attemptID    =    TaskAttemptID . forName ( attemptID )  ;", "}", "METHOD_END"], "methodName": ["setAttemptID"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . clockSplits    =    clockSplits ;", "}", "METHOD_END"], "methodName": ["setClockSplits"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . combineInputRecords    =    combineInputRecords ;", "}", "METHOD_END"], "methodName": ["setCombineInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . cpuUsages    =    cpuUsages ;", "}", "METHOD_END"], "methodName": ["setCpuUsages"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . fileBytesRead    =    fileBytesRead ;", "}", "METHOD_END"], "methodName": ["setFileBytesRead"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . fileBytesWritten    =    fileBytesWritten ;", "}", "METHOD_END"], "methodName": ["setFileBytesWritten"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . finishTime    =    finishTime ;", "}", "METHOD_END"], "methodName": ["setFinishTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . hdfsBytesRead    =    hdfsBytesRead ;", "}", "METHOD_END"], "methodName": ["setHdfsBytesRead"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . hdfsBytesWritten    =    hdfsBytesWritten ;", "}", "METHOD_END"], "methodName": ["setHdfsBytesWritten"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . hostName    =     ( hostName    =  =    null )     ?    null    :    new   NodeName ( hostName )  ;", "}", "METHOD_END"], "methodName": ["setHostName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( hostName    =  =    null )     |  |     (  ( hostName . length (  )  )     =  =     0  )  )     {", "throw   new   RuntimeException (  \" Invalid   entry !    Missing   hostname \"  )  ;", "} else", "if    (  ( rackName    =  =    null )     |  |     (  ( rackName . length (  )  )     =  =     0  )  )     {", "setHostName ( hostName )  ;", "} else    {", "if    (  !  ( rackName . startsWith (  \"  /  \"  )  )  )     {", "rackName    =     \"  /  \"     +    rackName ;", "}", "if    (  !  ( hostName . startsWith (  \"  /  \"  )  )  )     {", "hostName    =     \"  /  \"     +    hostName ;", "}", "setHostName (  (  ( rackName . intern (  )  )     +     ( hostName . intern (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setHostName"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . location    =    location ;", "}", "METHOD_END"], "methodName": ["setLocation"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . mapInputBytes    =    mapInputBytes ;", "}", "METHOD_END"], "methodName": ["setMapInputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . mapInputRecords    =    mapInputRecords ;", "}", "METHOD_END"], "methodName": ["setMapInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . mapOutputBytes    =    mapOutputBytes ;", "}", "METHOD_END"], "methodName": ["setMapOutputBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . mapOutputRecords    =    mapOutputRecords ;", "}", "METHOD_END"], "methodName": ["setMapOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . physMemKbytes    =    physMemKbytes ;", "}", "METHOD_END"], "methodName": ["setPhysMemKbytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . reduceInputGroups    =    reduceInputGroups ;", "}", "METHOD_END"], "methodName": ["setReduceInputGroups"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . reduceInputRecords    =    reduceInputRecords ;", "}", "METHOD_END"], "methodName": ["setReduceInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . reduceOutputRecords    =    reduceOutputRecords ;", "}", "METHOD_END"], "methodName": ["setReduceOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . reduceShuffleBytes    =    reduceShuffleBytes ;", "}", "METHOD_END"], "methodName": ["setReduceShuffleBytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . metrics    =    metrics ;", "}", "METHOD_END"], "methodName": ["setResourceUsageMetrics"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . result    =    result ;", "}", "METHOD_END"], "methodName": ["setResult"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . shuffleFinished    =    shuffleFinished ;", "}", "METHOD_END"], "methodName": ["setShuffleFinished"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . sortFinished    =    sortFinished ;", "}", "METHOD_END"], "methodName": ["setSortFinished"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . spilledRecords    =    spilledRecords ;", "}", "METHOD_END"], "methodName": ["setSpilledRecords"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . startTime    =    startTime ;", "}", "METHOD_END"], "methodName": ["setStartTime"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( LoggedTaskAttempt . alreadySeenAnySetterAttributes . contains ( attributeName )  )  )     {", "LoggedTaskAttempt . alreadySeenAnySetterAttributes . add ( attributeName )  ;", "System . err . println (  (  (  \" In   LoggedJob ,    we   saw   the   unknown   attribute    \"     +    attributeName )     +     \"  .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setUnknownAttribute"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . vMemKbytes    =    vMemKbytes ;", "}", "METHOD_END"], "methodName": ["setVMemKbytes"], "fileName": "org.apache.hadoop.tools.rumen.LoggedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   mapSlots ;", "}", "METHOD_END"], "methodName": ["getMapSlots"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return   memory ;", "}", "METHOD_END"], "methodName": ["getMemory"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return   memoryPerMapSlot ;", "}", "METHOD_END"], "methodName": ["getMemoryPerMapSlot"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return   memoryPerReduceSlot ;", "}", "METHOD_END"], "methodName": ["getMemoryPerReduceSlot"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return   numCores ;", "}", "METHOD_END"], "methodName": ["getNumCores"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return    (  ( RackNode )     ( getParent (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getRackNode"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return   reduceSlots ;", "}", "METHOD_END"], "methodName": ["getReduceSlots"], "fileName": "org.apache.hadoop.tools.rumen.MachineNode"}, {"methodBody": ["METHOD_START", "{", "return   runtime ;", "}", "METHOD_END"], "methodName": ["getMapRuntime"], "fileName": "org.apache.hadoop.tools.rumen.MapTaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "if    (  ( child . parent )     !  =    null )     {", "throw   new   IllegalArgtException (  (  \" The   child   is   already   under   another   node :  \"     +     ( child . parent )  )  )  ;", "}", "checkChildren (  )  ;", "boolean   retval    =    children . add ( child )  ;", "if    ( retval )", "child . parent    =    this ;", "return   retval ;", "}", "METHOD_END"], "methodName": ["addChild"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "if    (  ( children )     =  =    null )     {", "children    =    new   TreeSet <  >  (  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkChildren"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "return    ( children )     =  =    null    ?    Node . EMPTY _ SET    :    Collections . unmodifiableSortedSet ( children )  ;", "}", "METHOD_END"], "methodName": ["getChildren"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "return   level ;", "}", "METHOD_END"], "methodName": ["getLevel"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "return   parent ;", "}", "METHOD_END"], "methodName": ["getParent"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "return    (  ( children )     !  =    null )     &  &     (  !  ( children . isEmpty (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["hasChildren"], "fileName": "org.apache.hadoop.tools.rumen.Node"}, {"methodBody": ["METHOD_START", "{", "return   car ;", "}", "METHOD_END"], "methodName": ["first"], "fileName": "org.apache.hadoop.tools.rumen.Pair"}, {"methodBody": ["METHOD_START", "{", "return   cdr ;", "}", "METHOD_END"], "methodName": ["second"], "fileName": "org.apache.hadoop.tools.rumen.Pair"}, {"methodBody": ["METHOD_START", "{", "if    (  ( propName . equals ( attr )  )     &  &     ( value    !  =    null )  )     {", "try    {", "return   Integer . pInt ( value )  ;", "}    catch    ( NumberFormatException   e )     {", "return   oldValue ;", "}", "}", "return   oldValue ;", "}", "METHOD_END"], "methodName": ["maybeGetIntValue"], "fileName": "org.apache.hadoop.tools.rumen.ParsedConfigFile"}, {"methodBody": ["METHOD_START", "{", "if    ( nodeName . equals ( other . nodeName )  )     {", "return    0  ;", "}", "if    ( rackName . equals ( other . rackName )  )     {", "return    1  ;", "}", "return    2  ;", "}", "METHOD_END"], "methodName": ["distance"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "return   nodeName ;", "}", "METHOD_END"], "methodName": ["getNodeName"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "return   rackName ;", "}", "METHOD_END"], "methodName": ["getRackName"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "LoggedLocation   result    =    new   LoggedLocation (  )  ;", "List < String >    coordinates    =    new   ArrayList < String >  (  )  ;", "coordinates . add ( rackName )  ;", "coordinates . add ( nodeName )  ;", "result . setLayers ( coordinates )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["makeLoggedLocation"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "switch    ( i )     {", "case    0     :", "return   rackName ;", "case    1     :", "return   nodeName ;", "default    :", "throw   new   IllegalArgtException (  \" Host   location   component   index   out   of   range .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["nameComponent"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "return    3  ;", "}", "METHOD_END"], "methodName": ["numberOfDistances"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "Matcher   matcher    =    ParsedHost . splitPattern . matcher ( name )  ;", "if    (  !  ( matcher . matches (  )  )  )", "return   null ;", "return   new   ParsedHost ( matcher . group (  1  )  ,    matcher . group (  2  )  )  ;", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "return   name    =  =    null    ?    null    :    name . startsWith (  \"  /  \"  )     ?    name . substring (  1  )     :    name ;", "}", "METHOD_END"], "methodName": ["process"], "fileName": "org.apache.hadoop.tools.rumen.ParsedHost"}, {"methodBody": ["METHOD_START", "{", "List < ParsedTask >    result    =    new   ArrayList < ParsedTask >  (  )  ;", "for    ( LoggedTask   t    :    tasks )     {", "if    ( t   instanceof   ParsedTask )     {", "result . add (  (  ( ParsedTask )     ( t )  )  )  ;", "} else    {", "throw   new   RuntimeException (  \" Unexpected   type   of   tasks   in   the   list .  .  .  \"  )  ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["convertTasks"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "ParsedJob . LOG . info (  (  (  (  (  (  (  (  (  (  (  (  \" ParsedJob   details :  \"     +     ( obtainTotalCounters (  )  )  )     +     \"  ;  \"  )     +     ( obtainMapCounters (  )  )  )     +     \"  ;  \"  )     +     ( obtainReduceCounters (  )  )  )     +     \"  \\ n \"  )     +     ( obtainJobConfpath (  )  )  )     +     \"  \\ n \"  )     +     ( obtainJobAcls (  )  )  )     +     \"  ; Q =  \"  )     +     (  ( getQueue (  )  )     =  =    null    ?     \" null \"     :    getQueue (  )  . getValue (  )  )  )  )  ;", "List < ParsedTask >    maps    =    obtainMapTasks (  )  ;", "for    ( ParsedTask   task    :    maps )     {", "task . dumpParsedTask (  )  ;", "}", "List < ParsedTask >    reduces    =    obtainReduceTasks (  )  ;", "for    ( ParsedTask   task    :    reduces )     {", "task . dumpParsedTask (  )  ;", "}", "List < ParsedTask >    others    =    obtainOtherTasks (  )  ;", "for    ( ParsedTask   task    :    others )     {", "task . dumpParsedTask (  )  ;", "}", "}", "METHOD_END"], "methodName": ["dumpParsedJob"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobAcls ;", "}", "METHOD_END"], "methodName": ["obtainJobAcls"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "return   jobConfPath ;", "}", "METHOD_END"], "methodName": ["obtainJobConfpath"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "return   mapCountersMap ;", "}", "METHOD_END"], "methodName": ["obtainMapCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "List < LoggedTask >    tasks    =    super . getMapTasks (  )  ;", "return   convertTasks ( tasks )  ;", "}", "METHOD_END"], "methodName": ["obtainMapTasks"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "List < LoggedTask >    tasks    =    super . getOtherTasks (  )  ;", "return   convertTasks ( tasks )  ;", "}", "METHOD_END"], "methodName": ["obtainOtherTasks"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "return   reduceCountersMap ;", "}", "METHOD_END"], "methodName": ["obtainReduceCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "List < LoggedTask >    tasks    =    super . getReduceTasks (  )  ;", "return   convertTasks ( tasks )  ;", "}", "METHOD_END"], "methodName": ["obtainReduceTasks"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "return   totalCountersMap ;", "}", "METHOD_END"], "methodName": ["obtainTotalCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "jobAcls    =    acls ;", "}", "METHOD_END"], "methodName": ["putJobAcls"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "jobConfPath    =    confPath ;", "}", "METHOD_END"], "methodName": ["putJobConfPath"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "this . mapCountersMap    =    mapCounters ;", "}", "METHOD_END"], "methodName": ["putMapCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "this . reduceCountersMap    =    reduceCounters ;", "}", "METHOD_END"], "methodName": ["putReduceCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "this . totalCountersMap    =    totalCounters ;", "}", "METHOD_END"], "methodName": ["putTotalCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedJob"}, {"methodBody": ["METHOD_START", "{", "return   content . getProperty ( key )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.tools.rumen.ParsedLine"}, {"methodBody": ["METHOD_START", "{", "String   val    =    get ( key )  ;", "return   Long . pLong ( val )  ;", "}", "METHOD_END"], "methodName": ["getLong"], "fileName": "org.apache.hadoop.tools.rumen.ParsedLine"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.tools.rumen.ParsedLine"}, {"methodBody": ["METHOD_START", "{", "List < ParsedTaskAttempt >    result    =    new   ArrayList < ParsedTaskAttempt >  (  )  ;", "for    ( LoggedTaskAttempt   t    :    attempts )     {", "if    ( t   instanceof   ParsedTaskAttempt )     {", "result . add (  (  ( ParsedTaskAttempt )     ( t )  )  )  ;", "} else    {", "throw   new   RuntimeException (  \" Unexpected   type   of   taskAttempts   in   the   list .  .  .  \"  )  ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["convertTaskAttempts"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "ParsedTask . LOG . info (  (  (  (  (  \" ParsedTask   details :  \"     +     ( obtainCounters (  )  )  )     +     \"  \\ n \"  )     +     ( obtainFailedDueToAttemptId (  )  )  )     +     \"  \\ nPreferred   Locations   are :  \"  )  )  ;", "List < LoggedLocation >    loc    =    getPreferredLocations (  )  ;", "for    ( LoggedLocation   l    :    loc )     {", "ParsedTask . LOG . info (  (  (  ( l . getLayers (  )  )     +     \"  ;  \"  )     +     ( l . toString (  )  )  )  )  ;", "}", "List < ParsedTaskAttempt >    attempts    =    obtainTaskAttempts (  )  ;", "for    ( ParsedTaskAttempt   attempt    :    attempts )     {", "attempt . dumpParsedTaskAttempt (  )  ;", "}", "}", "METHOD_END"], "methodName": ["dumpParsedTask"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Long >    countersMap    =    JobHistoryUtils . extractCounters ( counters )  ;", "putCounters ( countersMap )  ;", "super . incorporateCounters ( counters )  ;", "}", "METHOD_END"], "methodName": ["incorporateCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "return   countersMap ;", "}", "METHOD_END"], "methodName": ["obtainCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticInfo ;", "}", "METHOD_END"], "methodName": ["obtainDiagnosticInfo"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "return   failedDueToAttempt ;", "}", "METHOD_END"], "methodName": ["obtainFailedDueToAttemptId"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "List < LoggedTaskAttempt >    attempts    =    getAttempts (  )  ;", "return   convertTaskAttempts ( attempts )  ;", "}", "METHOD_END"], "methodName": ["obtainTaskAttempts"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "this . countersMap    =    counters ;", "}", "METHOD_END"], "methodName": ["putCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "diagnosticInfo    =    msg ;", "}", "METHOD_END"], "methodName": ["putDiagnosticInfo"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "failedDueToAttempt    =    attempt ;", "}", "METHOD_END"], "methodName": ["putFailedDueToAttemptId"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTask"}, {"methodBody": ["METHOD_START", "{", "ParsedTaskAttempt . LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" ParsedTaskAttempt   details :  \"     +     ( obtainCounters (  )  )  )     +     \"  ; DiagnosticInfo =  \"  )     +     ( obtainDiagnosticInfo (  )  )  )     +     \"  \\ n \"  )     +     ( obtainTrackerName (  )  )  )     +     \"  ;  \"  )     +     ( obtainHttpPort (  )  )  )     +     \"  ;  \"  )     +     ( obtainShufflePort (  )  )  )     +     \"  ; rack =  \"  )     +     ( getHostName (  )  . getRackName (  )  )  )     +     \"  ; host =  \"  )     +     ( getHostName (  )  . getHostName (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["dumpParsedTaskAttempt"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Long >    countersMap    =    JobHistoryUtils . extractCounters ( counters )  ;", "putCounters ( countersMap )  ;", "super . incorporateCounters ( counters )  ;", "}", "METHOD_END"], "methodName": ["incorporateCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   countersMap ;", "}", "METHOD_END"], "methodName": ["obtainCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticInfo ;", "}", "METHOD_END"], "methodName": ["obtainDiagnosticInfo"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   httpPort ;", "}", "METHOD_END"], "methodName": ["obtainHttpPort"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   shufflePort ;", "}", "METHOD_END"], "methodName": ["obtainShufflePort"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return   trackerName ;", "}", "METHOD_END"], "methodName": ["obtainTrackerName"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . countersMap    =    counters ;", "}", "METHOD_END"], "methodName": ["putCounters"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "diagnosticInfo    =    msg ;", "}", "METHOD_END"], "methodName": ["putDiagnosticInfo"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "httpPort    =    port ;", "}", "METHOD_END"], "methodName": ["putHttpPort"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "shufflePort    =    port ;", "}", "METHOD_END"], "methodName": ["putShufflePort"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "this . trackerName    =    trackerName ;", "}", "METHOD_END"], "methodName": ["putTrackerName"], "fileName": "org.apache.hadoop.tools.rumen.ParsedTaskAttempt"}, {"methodBody": ["METHOD_START", "{", "return    (  ( Set < MachineNode >  )     (  ( Set )     ( getChildren (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getMachinesInRack"], "fileName": "org.apache.hadoop.tools.rumen.RackNode"}, {"methodBody": ["METHOD_START", "{", "MessageDigest   md 5     =    RandomSeedGenerator . md 5 Holder . get (  )  ;", "md 5  . reset (  )  ;", "String   str    =     ( streamId    +     '  /  '  )     +    masterSeed ;", "byte [  ]    digest    =    md 5  . digest ( str . getBytes (  )  )  ;", "long   seed    =     0  ;", "for    ( int   i    =     0  ;    i    <     8  ;    i +  +  )     {", "seed    =     ( seed    <  <     8  )     +     (  (  ( int )     ( digest [ i ]  )  )     +     1  2  8  )  ;", "}", "return   seed ;", "}", "METHOD_END"], "methodName": ["getSeed"], "fileName": "org.apache.hadoop.tools.rumen.RandomSeedGenerator"}, {"methodBody": ["METHOD_START", "{", "return   mergeTime ;", "}", "METHOD_END"], "methodName": ["getMergeRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   reduceTime ;", "}", "METHOD_END"], "methodName": ["getReduceRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   shuffleTime ;", "}", "METHOD_END"], "methodName": ["getShuffleRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "if    ( m 1     !  =    m 2  )     {", "throw   new   DeepInequalityException (  (  \" Value   miscompared :  \"     +     ( locString (  )  )  )  ,    loc )  ;", "}", "}", "METHOD_END"], "methodName": ["compareMetric"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "if    (  ( m 1  . size (  )  )     !  =     ( m 2  . size (  )  )  )     {", "throw   new   DeepInequalityException (  (  \" Size   miscompared :     \"     +     ( locString (  )  )  )  ,    loc )  ;", "}", "}", "METHOD_END"], "methodName": ["compareSize"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "return   cumulativeCpuUsage ;", "}", "METHOD_END"], "methodName": ["getCumulativeCpuUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "return   heapUsage ;", "}", "METHOD_END"], "methodName": ["getHeapUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "return   physicalMemoryUsage ;", "}", "METHOD_END"], "methodName": ["getPhysicalMemoryUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "return   virtualMemoryUsage ;", "}", "METHOD_END"], "methodName": ["getVirtualMemoryUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "cumulativeCpuUsage    =    usage ;", "}", "METHOD_END"], "methodName": ["setCumulativeCpuUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "heapUsage    =    usage ;", "}", "METHOD_END"], "methodName": ["setHeapUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "physicalMemoryUsage    =    usage ;", "}", "METHOD_END"], "methodName": ["setPhysicalMemoryUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "virtualMemoryUsage    =    usage ;", "}", "METHOD_END"], "methodName": ["setVirtualMemoryUsage"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "int   size    =     0  ;", "size    +  =    WritableUtils . getVIntSize ( cumulativeCpu )  ;", "size    +  =    WritableUtils . getVIntSize ( virtualMemory )  ;", "size    +  =    WritableUtils . getVIntSize ( physicalMemory )  ;", "size    +  =    WritableUtils . getVIntSize ( heap )  ;", "return   size ;", "}", "METHOD_END"], "methodName": ["size"], "fileName": "org.apache.hadoop.tools.rumen.ResourceUsageMetrics"}, {"methodBody": ["METHOD_START", "{", "try    {", "input . reset (  )  ;", "return   this ;", "}    catch    ( IOException   e )     {", "throw   new   IOException (  \" Unable   to   r   the   stream \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["rewind"], "fileName": "org.apache.hadoop.tools.rumen.RewindableInputStream"}, {"methodBody": ["METHOD_START", "{", "return   state ;", "}", "METHOD_END"], "methodName": ["getRunState"], "fileName": "org.apache.hadoop.tools.rumen.TaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   kind . get ( allSplits )  ;", "}", "METHOD_END"], "methodName": ["getSplitVector"], "fileName": "org.apache.hadoop.tools.rumen.TaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   taskInfo ;", "}", "METHOD_END"], "methodName": ["getTaskInfo"], "fileName": "org.apache.hadoop.tools.rumen.TaskAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   bytesIn ;", "}", "METHOD_END"], "methodName": ["getInputBytes"], "fileName": "org.apache.hadoop.tools.rumen.TaskInfo"}, {"methodBody": ["METHOD_START", "{", "return   recsIn ;", "}", "METHOD_END"], "methodName": ["getInputRecords"], "fileName": "org.apache.hadoop.tools.rumen.TaskInfo"}, {"methodBody": ["METHOD_START", "{", "return   bytesOut ;", "}", "METHOD_END"], "methodName": ["getOutputBytes"], "fileName": "org.apache.hadoop.tools.rumen.TaskInfo"}, {"methodBody": ["METHOD_START", "{", "return   recsOut ;", "}", "METHOD_END"], "methodName": ["getOutputRecords"], "fileName": "org.apache.hadoop.tools.rumen.TaskInfo"}, {"methodBody": ["METHOD_START", "{", "return   metrics ;", "}", "METHOD_END"], "methodName": ["getResourceUsageMetrics"], "fileName": "org.apache.hadoop.tools.rumen.TaskInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxMemory ;", "}", "METHOD_END"], "methodName": ["getTaskMemory"], "fileName": "org.apache.hadoop.tools.rumen.TaskInfo"}, {"methodBody": ["METHOD_START", "{", "FSDataInputStream   dataStream    =    fs . open ( path )  ;", "JsonObjectMapperParser < RawTestData >    parser    =    new   JsonObjectMapperParser < RawTestData >  ( dataStream ,    RawTestData . class )  ;", "RawTestData   data ;", "try    {", "data    =    parser . getNext (  )  ;", "}    finally    {", "parser . close (  )  ;", "}", "hist    =    new    (  )  ;", "List < Long >    measurements    =    data . getData (  )  ;", "List < Long >    typeProbeData    =    new   RawTestData (  )  . getData (  )  ;", "assertTrue (  (  (  (  (  \" The   data   attribute   of   a   jackson - reconstructed   RawTestData    \"     +     \"    should   be   a    \"  )     +     ( typeProbeData . getClass (  )  . getName (  )  )  )     +     \"  ,    like   a   virgin   RawTestData ,    but   it ' s   a    \"  )     +     ( measurements . getClass (  )  . getName (  )  )  )  ,     (  ( measurements . getClass (  )  )     =  =     ( typeProbeData . getClass (  )  )  )  )  ;", "for    ( int   j    =     0  ;    j    <     ( measurements . size (  )  )  ;     +  + j )     {", "hist . enter ( measurements . get ( j )  )  ;", "}", "LoggedDiscreteCDF   result    =    new   LoggedDiscreteCDF (  )  ;", "int [  ]    percentiles    =    new   int [ data . getPercentiles (  )  . size (  )  ]  ;", "for    ( int   j    =     0  ;    j    <     ( data . getPercentiles (  )  . size (  )  )  ;     +  + j )     {", "percentiles [ j ]     =    data . getPercentiles (  )  . get ( j )  ;", "}", "result . setCDF ( hist ,    percentiles ,    data . getScale (  )  )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["histogramFileToCDF"], "fileName": "org.apache.hadoop.tools.rumen.TestHistograms"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "for    ( String   arg    :    args )     {", "Path   filePath    =    new   Path ( arg )  . makeQualified ( lfs )  ;", "String   fileName    =    filePath . getName (  )  ;", "if    ( fileName . startsWith (  \" input \"  )  )     {", "LoggedDiscreteCDF   newResult    =     . histogramFileToCDF ( filePath ,    lfs )  ;", "String   testName    =    fileName . substring (  \" input \"  . length (  )  )  ;", "Path   goldFilePath    =    new   Path ( filePath . getParent (  )  ,     (  \" gold \"     +    testName )  )  ;", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "JsonFactory   factory    =    mapper . getJsonFactory (  )  ;", "FSDataOutputStream   ostream    =    lfs . create ( goldFilePath ,    true )  ;", "JsonGenerator   gen    =    factory . createJsonGenerator ( ostream ,    UTF 8  )  ;", "gen . useDefaultPrettyPrinter (  )  ;", "gen . writeObject ( newResult )  ;", "gen . close (  )  ;", "} else    {", "System . err . println (  (  (  \" Input   file   not   started   with    \\  \" input \\  \"  .    File    \"     +    fileName )     +     \"    skipped .  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.rumen.TestHistograms"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "final   FileSystem   lfs    =    FileSystem . getLocal ( conf )  ;", "final   Path   rootInputDir    =    new   Path ( System . getProperty (  \" test . tools . input . dir \"  ,     \"  \"  )  )  . makeQualified ( lfs )  ;", "final   Path   rootInputFile    =    new   Path ( rootInputDir ,     \" rumen / histogram - tests \"  )  ;", "FileStatus [  ]    tests    =    lfs . listStatus ( rootInputFile )  ;", "for    ( int   i    =     0  ;    i    <     ( tests . length )  ;     +  + i )     {", "Path   filePath    =    tests [ i ]  . getPath (  )  ;", "String   fileName    =    filePath . getName (  )  ;", "if    ( fileName . startsWith (  \" input \"  )  )     {", "String   testName    =    fileName . substring (  \" input \"  . length (  )  )  ;", "Path   goldFilePath    =    new   Path ( rootInputFile ,     (  \" gold \"     +    testName )  )  ;", "assertTrue (  \" Gold   file   dies   not   exist \"  ,    lfs . exists ( goldFilePath )  )  ;", "LoggedDiscreteCDF   newResult    =     . histogramFileToCDF ( filePath ,    lfs )  ;", "System . out . println (  (  \" Testing   a   Histogram   for    \"     +    fileName )  )  ;", "FSDataInputStream   goldStream    =    lfs . open ( goldFilePath )  ;", "JsonObjectMapperParser < LoggedDiscreteCDF >    parser    =    new   JsonObjectMapperParser < LoggedDiscreteCDF >  ( goldStream ,    LoggedDiscreteCDF . class )  ;", "try    {", "LoggedDiscreteCDF   dcdf    =    parser . getNext (  )  ;", "dcdf . deepCompare ( newResult ,    new   TreePath ( null ,     \"  < root >  \"  )  )  ;", "}    catch    ( DeepInequalityException   e )     {", "fail ( e . path . toString (  )  )  ;", "}    finally    {", "parser . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["testHistograms"], "fileName": "org.apache.hadoop.tools.rumen.TestHistograms"}, {"methodBody": ["METHOD_START", "{", "LoggedSingleRelativeRanking   result    =    new   LoggedSingleRelativeRanking (  )  ;", "result . setDatum ( datum )  ;", "result . setRelativeRanking ( ranking )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["makeRR"], "fileName": "org.apache.hadoop.tools.rumen.TestPiecewiseLinearInterpolation"}, {"methodBody": ["METHOD_START", "{", "LoggedDiscreteCDF   input    =    new   LoggedDiscreteCDF (  )  ;", "input . setMinimum (  1  0  0  0  0  0 L )  ;", "input . setMaximum (  1  1  0  0  0  0  0 L )  ;", "ArrayList < LoggedSingleRelativeRanking >    rankings    =    new   ArrayList < LoggedSingleRelativeRanking >  (  )  ;", "rankings . add (  . makeRR (  0  .  1  ,     2  0  0  0  0  0 L )  )  ;", "rankings . add (  . makeRR (  0  .  5  ,     8  0  0  0  0  0 L )  )  ;", "rankings . add (  . makeRR (  0  .  9  ,     1  0  0  0  0  0  0 L )  )  ;", "input . setRankings ( rankings )  ;", "input . setNumberValues (  3  )  ;", "CDFRandomGenerator   gen    =    new   CDFPiecewiseLinearRandomGenerator ( input )  ;", "Histogram   values    =    new   Histogram (  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  0  0  0  0  ;     +  + i )     {", "long   value    =    gen . randomValue (  )  ;", "values . enter ( value )  ;", "}", "int [  ]    percentiles    =    new   int [  9  9  ]  ;", "for    ( int   i    =     0  ;    i    <     9  9  ;     +  + i )     {", "percentiles [ i ]     =    i    +     1  ;", "}", "long [  ]    result    =    values . getCDF (  1  0  0  ,    percentiles )  ;", "long   sumErrorSquares    =     0 L ;", "for    ( int   i    =     0  ;    i    <     1  0  ;     +  + i )     {", "long   error    =     ( result [ i ]  )     -     (  (  1  0  0  0  0 L    *    i )     +     1  0  0  0  0  0 L )  ;", "System . out . println (  (  (  (  (  (  (  (  \" element    \"     +    i )     +     \"  ,    got    \"  )     +     ( result [ i ]  )  )     +     \"  ,    expected    \"  )     +     (  (  1  0  0  0  0 L    *    i )     +     1  0  0  0  0  0 L )  )     +     \"  ,    error    =     \"  )     +    error )  )  ;", "sumErrorSquares    +  =    error    *    error ;", "}", "for    ( int   i    =     1  0  ;    i    <     5  0  ;     +  + i )     {", "long   error    =     ( result [ i ]  )     -     (  (  1  5  0  0  0 L    *    i )     +     5  0  0  0  0 L )  ;", "System . out . println (  (  (  (  (  (  (  (  \" element    \"     +    i )     +     \"  ,    got    \"  )     +     ( result [ i ]  )  )     +     \"  ,    expected    \"  )     +     (  (  1  5  0  0  0 L    *    i )     +     5  0  0  0  0 L )  )     +     \"  ,    error    =     \"  )     +    error )  )  ;", "sumErrorSquares    +  =    error    *    error ;", "}", "for    ( int   i    =     5  0  ;    i    <     9  0  ;     +  + i )     {", "long   error    =     ( result [ i ]  )     -     (  (  5  0  0  0 L    *    i )     +     5  5  0  0  0  0 L )  ;", "System . out . println (  (  (  (  (  (  (  (  \" element    \"     +    i )     +     \"  ,    got    \"  )     +     ( result [ i ]  )  )     +     \"  ,    expected    \"  )     +     (  (  5  0  0  0 L    *    i )     +     5  5  0  0  0  0 L )  )     +     \"  ,    error    =     \"  )     +    error )  )  ;", "sumErrorSquares    +  =    error    *    error ;", "}", "for    ( int   i    =     9  0  ;    i    <  =     1  0  0  ;     +  + i )     {", "long   error    =     ( result [ i ]  )     -     (  (  1  0  0  0  0 L    *    i )     +     1  0  0  0  0  0 L )  ;", "System . out . println (  (  (  (  (  (  (  (  \" element    \"     +    i )     +     \"  ,    got    \"  )     +     ( result [ i ]  )  )     +     \"  ,    expected    \"  )     +     (  (  1  0  0  0  0 L    *    i )     +     1  0  0  0  0  0 L )  )     +     \"  ,    error    =     \"  )     +    error )  )  ;", "sumErrorSquares    +  =    error    *    error ;", "}", "double   realSumErrorSquares    =     (  ( double )     ( sumErrorSquares )  )  ;", "double   normalizedError    =     (  ( realSumErrorSquares    /     1  0  0  )     /     ( rankings . get (  1  )  . getDatum (  )  )  )     /     ( rankings . get (  1  )  . getDatum (  )  )  ;", "double   RMSNormalizedError    =    Math . sqrt ( normalizedError )  ;", "System . out . println (  (  \" sumErrorSquares    =     \"     +    sumErrorSquares )  )  ;", "System . out . println (  (  (  (  \" normalizedError :     \"     +    normalizedError )     +     \"  ,    RMSNormalizedError :     \"  )     +    RMSNormalizedError )  )  ;", "System . out . println (  (  \" Cumulative   error   is    \"     +    RMSNormalizedError )  )  ;", "assertTrue (  (  (  (  \" The   RMS   relative   error   per   bucket ,     \"     +    RMSNormalizedError )     +     \"  ,    exceeds   our   tolerance   of    \"  )     +     (  . maximumRelativeError )  )  ,     ( RMSNormalizedError    <  =     (  . maximumRelativeError )  )  )  ;", "}", "METHOD_END"], "methodName": ["testOneRun"], "fileName": "org.apache.hadoop.tools.rumen.TestPiecewiseLinearInterpolation"}, {"methodBody": ["METHOD_START", "{", "long   masterSeed 1     =     4  2  ;", "long   masterSeed 2     =     4  3  ;", "assertTrue (  \" Deterministic   seeding \"  ,     (  (  . getSeed (  \" stream 1  \"  ,    masterSeed 1  )  )     =  =     (  . getSeed (  \" stream 1  \"  ,    masterSeed 1  )  )  )  )  ;", "assertTrue (  \" Deterministic   seeding \"  ,     (  (  . getSeed (  \" stream 2  \"  ,    masterSeed 2  )  )     =  =     (  . getSeed (  \" stream 2  \"  ,    masterSeed 2  )  )  )  )  ;", "assertTrue (  \" Different   streams \"  ,     (  (  . getSeed (  \" stream 1  \"  ,    masterSeed 1  )  )     !  =     (  . getSeed (  \" stream 2  \"  ,    masterSeed 1  )  )  )  )  ;", "assertTrue (  \" Different   master   seeds \"  ,     (  (  . getSeed (  \" stream 1  \"  ,    masterSeed 1  )  )     !  =     (  . getSeed (  \" stream 1  \"  ,    masterSeed 2  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSeedGeneration"], "fileName": "org.apache.hadoop.tools.rumen.TestRandomSeedGenerator"}, {"methodBody": ["METHOD_START", "{", "return   new   LoggedNetworkTopology ( allHosts )  ;", "}", "METHOD_END"], "methodName": ["build"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( splits    !  =    null )     {", "StringTokenizer   tok    =    new   StringTokenizer ( splits ,     \"  ,  \"  ,    false )  ;", "while    ( toksMoreTokens (  )  )     {", "String   nextSplit    =    tok . nextToken (  )  ;", "recordParsedHost ( nextSplit )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["preferredLocationForSplits"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( event   instanceof   TaskAttemptFinishedEvent )     {", "processTaskAttemptFinishedEvent (  (  ( TaskAttemptFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskAttemptUnsuccessfulCompletionEvent )     {", "processTaskAttemptUnsuccessfulCompletionEvent (  (  ( mapreduce . jobhistory . TaskAttemptUnsuccessfulCompletionEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . TaskStartedEvent )     {", "processTaskStartedEvent (  (  ( mapreduce . jobhistory . TaskStartedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . MapAttemptFinishedEvent )     {", "processMapAttemptFinishedEvent (  (  ( mapreduce . jobhistory . MapAttemptFinishedEvent )     ( event )  )  )  ;", "} else", "if    ( event   instanceof   mapreduce . jobhistory . ReduceAttemptFinishedEvent )     {", "processReduceAttemptFinishedEvent (  (  ( mapreduce . jobhistory . ReduceAttemptFinishedEvent )     ( event )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["process"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "recordParsedHost ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "}", "METHOD_END"], "methodName": ["processMapAttemptFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "recordParsedHost ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "}", "METHOD_END"], "methodName": ["processReduceAttemptFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "recordParsedHost ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskAttemptFinishedEvent"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "recordParsedHost ( event . getHostname (  )  ,    event . getRackName (  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskAttemptUnsuccessfulCompletionEvent"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "preferredLocationForSplits ( event . getSplitLocations (  )  )  ;", "}", "METHOD_END"], "methodName": ["processTaskStartedEvent"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "ParsedHost   result    =    ParsedHost . parse ( nodeName )  ;", "if    (  ( result    !  =    null )     &  &     (  !  ( allHosts . contains ( result )  )  )  )     {", "allHosts . add ( result )  ;", "}", "}", "METHOD_END"], "methodName": ["recordParsedHost"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "if    ( hostName    =  =    null )     {", "return ;", "}", "ParsedHost   result    =    null ;", "if    ( rackName    =  =    null )     {", "result    =    ParsedHost . parse ( hostName )  ;", "} else    {", "result    =    new   ParsedHost ( rackName ,    hostName )  ;", "}", "if    (  ( result    !  =    null )     &  &     (  !  ( allHosts . contains ( result )  )  )  )     {", "allHosts . add ( result )  ;", "}", "}", "METHOD_END"], "methodName": ["recordParsedHost"], "fileName": "org.apache.hadoop.tools.rumen.TopologyBuilder"}, {"methodBody": ["METHOD_START", "{", "IOUtils . cleanup ( TraceBuilder . LOG ,    traceWriter ,    topologyWriter )  ;", "}", "METHOD_END"], "methodName": ["finish"], "fileName": "org.apache.hadoop.tools.rumen.TraceBuilder"}, {"methodBody": ["METHOD_START", "{", "TraceBuilder   builder    =    new   TraceBuilder (  )  ;", "int   result    =    TraceBuilder . RUN _ METHOD _ FAILED _ EXIT _ CODE ;", "try    {", "result    =    ToolRunner . run ( builder ,    args )  ;", "}    catch    ( Throwable   t )     {", "t . printStackTrace ( System . err )  ;", "}    finally    {", "try    {", "builder . finish (  )  ;", "}    finally    {", "if    ( result    =  =     0  )     {", "return ;", "}", "System . exit ( result )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.tools.rumen.TraceBuilder"}, {"methodBody": ["METHOD_START", "{", "jobBuilder . process ( properties )  ;", "topologyBuilder . process ( properties )  ;", "}", "METHOD_END"], "methodName": ["processJobConf"], "fileName": "org.apache.hadoop.tools.rumen.TraceBuilder"}, {"methodBody": ["METHOD_START", "{", "HistoryEvent   e ;", "while    (  ( e    =    parser . nextEvent (  )  )     !  =    null )     {", "job . process ( e )  ;", "topology . process ( e )  ;", "}", "parser . close (  )  ;", "}", "METHOD_END"], "methodName": ["processJobHistory"], "fileName": "org.apache.hadoop.tools.rumen.TraceBuilder"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   TaskType . valueOf ( taskType )  ;", "}    catch    ( IllegalArgtException   e )     {", "if    (  \" CLEANUP \"  . equals ( taskType )  )     {", "return   TaskType . JOB _ CLEANUP ;", "}", "if    (  \" SETUP \"  . equals ( taskType )  )     {", "return   TaskType . JOB _ SETUP ;", "}", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["get20TaskType"], "fileName": "org.apache.hadoop.tools.rumen.Version20LogInterfaceUtils"}, {"methodBody": ["METHOD_START", "{", "Map < LoggedNetworkTopology ,    Integer >    levelMapping    =    new   IdentityHashMap < LoggedNetworkTopology ,    Integer >  (  )  ;", "Deque < LoggedNetworkTopology >    unvisited    =    new   ArrayDeque < LoggedNetworkTopology >  (  )  ;", "unvisited . add ( topology )  ;", "levelMapping . put ( topology ,     0  )  ;", "int   leafLevel    =     -  1  ;", "for    ( LoggedNetworkTopology   n    =    unvisited . poll (  )  ;    n    !  =    null ;    n    =    unvisited . poll (  )  )     {", "int   level    =    levelMapping . get ( n )  ;", "List < LoggedNetworkTopology >    children    =    n . getChildren (  )  ;", "if    (  ( children    =  =    null )     |  |     ( children . isEmpty (  )  )  )     {", "if    ( leafLevel    =  =     (  -  1  )  )     {", "leafLevel    =    level ;", "} else", "if    ( leafLevel    !  =    level )     {", "throw   new   IllegalArgtException (  \" Leaf   nodes   are   not   on   the   same   level \"  )  ;", "}", "} else    {", "for    ( LoggedNetworkTopology   child    :    children )     {", "levelMapping . put ( child ,     ( level    +     1  )  )  ;", "unvisited . addFirst ( child )  ;", "}", "}", "}", "Node [  ]    path    =    new   Node [ leafLevel ]  ;", "unvisited . add ( topology )  ;", "for    ( LoggedNetworkTopology   n    =    unvisited . poll (  )  ;    n    !  =    null ;    n    =    unvisited . poll (  )  )     {", "int   level    =    levelMapping . get ( n )  ;", "Node   current ;", "if    ( level    =  =    leafLevel )     {", "MachineNode . Builder   builder    =    new   MachineNode . Builder ( n . getName (  )  . getValue (  )  ,    level )  ;", "if    ( defaultNode    !  =    null )     {", "builder . cloneFrom ( defaultNode )  ;", "}", "current    =    builder . build (  )  ;", "} else    {", "current    =     ( level    =  =     ( leafLevel    -     1  )  )     ?    new   RackNode ( n . getName (  )  . getValue (  )  ,    level )     :    new   Node ( n . getName (  )  . getValue (  )  ,    level )  ;", "path [ level ]     =    current ;", "for    ( LoggedNetworkTopology   child    :    n . getChildren (  )  )     {", "unvisited . addFirst ( child )  ;", "}", "}", "if    ( level    !  =     0  )     {", "path [  ( level    -     1  )  ]  . addChild ( current )  ;", "}", "}", "root    =    path [  0  ]  ;", "}", "METHOD_END"], "methodName": ["buildCluster"], "fileName": "org.apache.hadoop.tools.rumen.ZombieCluster"}, {"methodBody": ["METHOD_START", "{", "if    (  ( loggedTaskMap )     =  =    null )     {", "loggedTaskMap    =    new   HashMap < mapreduce . TaskID ,    LoggedTask >  (  )  ;", "loggedTaskAttemptMap    =    new   HashMap < TaskAttemptID ,    LoggedTaskAttempt >  (  )  ;", "for    ( LoggedTask   map    :    job . getMapTasks (  )  )     {", "map    =    sanitizeLoggedTask ( map )  ;", "if    ( map    !  =    null )     {", "loggedTaskMap . put ( maskTaskID ( map . taskID )  ,    map )  ;", "for    ( LoggedTaskAttempt   mapAttempt    :    map . getAttempts (  )  )     {", "mapAttempt    =    sanitizeLoggedTaskAttempt ( mapAttempt )  ;", "if    ( mapAttempt    !  =    null )     {", "TaskAttemptID   id    =    mapAttempt . getAttemptID (  )  ;", "loggedTaskAttemptMap . put ( maskAttemptID ( id )  ,    mapAttempt )  ;", "}", "}", "}", "}", "for    ( LoggedTask   reduce    :    job . getReduceTasks (  )  )     {", "reduce    =    sanitizeLoggedTask ( reduce )  ;", "if    ( reduce    !  =    null )     {", "loggedTaskMap . put ( maskTaskID ( reduce . taskID )  ,    reduce )  ;", "for    ( LoggedTaskAttempt   reduceAttempt    :    reduce . getAttempts (  )  )     {", "reduceAttempt    =    sanitizeLoggedTaskAttempt ( reduceAttempt )  ;", "if    ( reduceAttempt    !  =    null )     {", "TaskAttemptID   id    =    reduceAttempt . getAttemptID (  )  ;", "loggedTaskAttemptMap . put ( maskAttemptID ( id )  ,    reduceAttempt )  ;", "}", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["buildMaps"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( status    =  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )     {", "return   State . SUCCEEDED ;", "} else", "if    ( status    =  =     ( Pre 2  1 JobHistoryConstants . Values . FAILED )  )     {", "return   State . FAILED ;", "} else", "if    ( status    =  =     ( Pre 2  1 JobHistoryConstants . Values . KILLED )  )     {", "return   State . KILLED ;", "} else    {", "throw   new   IllegalArgtException (  (  \" unknown   status    \"     +    status )  )  ;", "}", "}", "METHOD_END"], "methodName": ["convertState"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "long   reduceTime ;", "try    {", "if    ( state    =  =     ( State . SUCCEEDED )  )     {", "reduceTime    =    makeUpRuntime ( job . getSuccessfulReduceAttemptCDF (  )  )  ;", "} else", "if    ( state    =  =     ( State . FAILED )  )     {", "reduceTime    =    makeUpRuntime ( job . getFailedReduceAttemptCDF (  )  )  ;", "} else    {", "throw   new   IllegalArgumentException (  (  \" state   is   neither   SUCCEEDED   nor   FAILED :     \"     +    state )  )  ;", "}", "return   reduceTime ;", "}    catch    (  . NoValueToMakeUpRuntime   e )     {", "return    0  ;", "}", "}", "METHOD_END"], "methodName": ["doMakeUpReduceRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "int   distance    =    cluster . getMaximumDistance (  )  ;", "String   rackHostName    =    loggedAttempt . getHostName (  )  . getValue (  )  ;", "if    ( rackHostName    =  =    null )     {", "return   distance ;", "}", "MineNode   mn    =    getMineNode ( rackHostName )  ;", "if    ( mn    =  =    null )     {", "return   distance ;", "}", "List < LoggedLocation >    locations    =    loggedTask . getPreferredLocations (  )  ;", "if    ( locations    !  =    null )     {", "for    ( LoggedLocation   location    :    locations )     {", "List < NodeName >    layers    =    location . getLayers (  )  ;", "if    (  ( layers    =  =    null )     |  |     ( layers . isEmpty (  )  )  )     {", "continue ;", "}", "String   dataNodeName    =    layers . get (  (  ( layers . size (  )  )     -     1  )  )  . getValue (  )  ;", "MineNode   dataNode    =    cluster . getMineByName ( dataNodeName )  ;", "if    ( dataNode    !  =    null )     {", "distance    =    Math . min ( distance ,    cluster . distance ( mn ,    dataNode )  )  ;", "}", "}", "}", "return   distance ;", "}", "METHOD_END"], "methodName": ["getLocality"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "return   job ;", "}", "METHOD_END"], "methodName": ["getLoggedJob"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "buildMaps (  )  ;", "return   loggedTaskMap . get ( getMaskedTaskID ( taskType ,    taskNumber )  )  ;", "}", "METHOD_END"], "methodName": ["getLoggedTask"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "buildMaps (  )  ;", "TaskAttemptID   id    =    new   TaskAttemptID ( getMaskedTaskID ( taskType ,    taskNumber )  ,    taskAttemptNumber )  ;", "return   loggedTaskAttemptMap . get ( id )  ;", "}", "METHOD_END"], "methodName": ["getLoggedTaskAttempt"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "ParsedHost   parsedHost    =    ParsedHost . parse ( rackHostName )  ;", "String   hostName    =     ( parsedHost    =  =    null )     ?    rackHostName    :    parsedHost . getNodeName (  )  ;", "if    ( hostName    =  =    null )     {", "return   null ;", "}", "return    ( cluster )     =  =    null    ?    null    :    cluster . getMineByName ( hostName )  ;", "}", "METHOD_END"], "methodName": ["getMachineNode"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "return   new   TaskID ( new   JobID (  )  ,    taskType ,    taskNumber )  ;", "}", "METHOD_END"], "methodName": ["getMaskedTaskID"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "( numRandomSeeds )  +  +  ;", "return   RandomSeedGenerator . getSeed (  (  \" for \"     +     ( job . getJobID (  )  )  )  ,    numRandomSeeds )  ;", "}", "METHOD_END"], "methodName": ["getNextRandomSeed"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "return   job . getMapTasks (  )  . size (  )  ;", "}", "METHOD_END"], "methodName": ["getNumLoggedMaps"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "return   job . getReduceTasks (  )  . size (  )  ;", "}", "METHOD_END"], "methodName": ["getNumLoggedReduces"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "int   locality    =     0  ;", "LoggedTask   loggedTask    =    getLoggedTask ( taskType ,    taskNumber )  ;", "if    ( loggedTask    =  =    null )     {", "TaskInfo   taskInfo    =    new   TaskInfo (  0  ,     0  ,     0  ,     0  ,     0  )  ;", "return   makeUpTaskAttemptInfo ( taskType ,    taskInfo ,    taskAttemptNumber ,    taskNumber ,    locality )  ;", "}", "LoggedTaskAttempt   loggedAttempt    =    getLoggedTaskAttempt ( taskType ,    taskNumber ,    taskAttemptNumber )  ;", "if    ( loggedAttempt    =  =    null )     {", "TaskInfo   taskInfo    =    getTaskInfo ( loggedTask )  ;", "return   makeUpTaskAttemptInfo ( taskType ,    taskInfo ,    taskAttemptNumber ,    taskNumber ,    locality )  ;", "} else    {", "if    (  ( loggedAttempt . getResult (  )  )     =  =     ( Pre 2  1 HistoryConstants . Values . KILLED )  )     {", "TaskInfo   taskInfo    =    getTaskInfo ( loggedTask )  ;", "return   makeUpTaskAttemptInfo ( taskType ,    taskInfo ,    taskAttemptNumber ,    taskNumber ,    locality )  ;", "} else    {", "return   getTaskAttemptInfo ( loggedTask ,    loggedAttempt )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["getTaskAttemptInfo"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "TaskInfo   taskInfo    =    getTaskInfo ( loggedTask )  ;", "List < List < Integer >  >    allSplitVectors    =    loggedAttempt . allSplitVectors (  )  ;", "State   state    =     . convertState ( loggedAttempt . getResult (  )  )  ;", "if    (  ( loggedTask . getTaskType (  )  )     =  =     ( Pre 2  1 JobHistoryConstants . Values . MAP )  )     {", "long   taskTime ;", "if    (  ( loggedAttempt . getStartTime (  )  )     =  =     0  )     {", "int   locality    =    getLocality ( loggedTask ,    loggedAttempt )  ;", "taskTime    =    makeUpMapRuntime ( state ,    locality )  ;", "} else    {", "taskTime    =     ( loggedAttempt . getFinishTime (  )  )     -     ( loggedAttempt . getStartTime (  )  )  ;", "}", "taskTime    =    sanitizeTaskRuntime ( taskTime ,    loggedAttempt . getAttemptID (  )  )  ;", "return   new   MapTaskAttemptInfo ( state ,    taskInfo ,    taskTime ,    allSplitVectors )  ;", "} else", "if    (  ( loggedTask . getTaskType (  )  )     =  =     ( Pre 2  1 JobHistoryConstants . Values . REDUCE )  )     {", "long   startTime    =    loggedAttempt . getStartTime (  )  ;", "long   mergeDone    =    loggedAttempt . getSortFinished (  )  ;", "long   shuffleDone    =    loggedAttempt . getShuffleFinished (  )  ;", "long   finishTime    =    loggedAttempt . getFinishTime (  )  ;", "if    (  ( startTime    <  =     0  )     |  |     ( startTime    >  =    finishTime )  )     {", "long   reduceTime    =    makeUpReduceRuntime ( state )  ;", "return   new   ReduceTaskAttemptInfo ( state ,    taskInfo ,     0  ,     0  ,    reduceTime ,    allSplitVectors )  ;", "} else    {", "if    ( shuffleDone    <  =     0  )     {", "shuffleDone    =    startTime ;", "}", "if    ( mergeDone    <  =     0  )     {", "mergeDone    =    finishTime ;", "}", "long   shuffleTime    =    shuffleDone    -    startTime ;", "long   mergeTime    =    mergeDone    -    shuffleDone ;", "long   reduceTime    =    finishTime    -    mergeDone ;", "reduceTime    =    sanitizeTaskRuntime ( reduceTime ,    loggedAttempt . getAttemptID (  )  )  ;", "return   new   ReduceTaskAttemptInfo ( state ,    taskInfo ,    shuffleTime ,    mergeTime ,    reduceTime ,    allSplitVectors )  ;", "}", "} else    {", "throw   new   IllegalArgumentException (  (  (  (  \" taskType   for    \"     +     ( loggedTask . getTaskID (  )  )  )     +     \"    is   neither   MAP   nor   REDUCE :     \"  )     +     ( loggedTask . getTaskType (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getTaskAttemptInfo"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( loggedTask    =  =    null )     {", "return   new   TaskInfo (  0  ,     0  ,     0  ,     0  ,     0  )  ;", "}", "List < LoggedTaskAttempt >    attempts    =    loggedTask . getAttempts (  )  ;", "long   inputBytes    =     -  1  ;", "long   inputRecords    =     -  1  ;", "long   outputBytes    =     -  1  ;", "long   outputRecords    =     -  1  ;", "long   heapMegabytes    =     -  1  ;", "ResourceUsageMetrics   metrics    =    new   ResourceUsageMetrics (  )  ;", "Pre 2  1 JobHistoryConstants . Values   type    =    loggedTask . getTaskType (  )  ;", "if    (  ( type    !  =     ( Pre 2  1 JobHistoryConstants . Values . MAP )  )     &  &     ( type    !  =     ( Pre 2  1 JobHistoryConstants . Values . REDUCE )  )  )     {", "throw   new   IllegalArgtException (  (  (  (  \" getTaskInfo   only   supports   MAP   or   REDUCE   tasks :     \"     +     ( type . toString (  )  )  )     +     \"    for   task    =     \"  )     +     ( loggedTask . getTaskID (  )  )  )  )  ;", "}", "for    ( LoggedTaskAttempt   attempt    :    attempts )     {", "attempt    =    sanitizeLoggedTaskAttempt ( attempt )  ;", "if    (  ( attempt    =  =    null )     |  |     (  ( attempt . getResult (  )  )     !  =     ( Pre 2  1 JobHistoryConstants . Values . SUCCESS )  )  )     {", "continue ;", "}", "if    ( type    =  =     ( Pre 2  1 JobHistoryConstants . Values . MAP )  )     {", "inputBytes    =    attempt . getHdfsBytesRead (  )  ;", "inputRecords    =    attempt . getMapInputRecords (  )  ;", "outputBytes    =     (  ( job . getTotalReduces (  )  )     >     0  )     ?    attempt . getMapOutputBytes (  )     :    attempt . getHdfsBytesWritten (  )  ;", "outputRecords    =    attempt . getMapOutputRecords (  )  ;", "heapMegabytes    =     (  ( job . getJobMapMB (  )  )     >     0  )     ?    job . getJobMapMB (  )     :    job . getHeapMegabytes (  )  ;", "} else    {", "inputBytes    =    attempt . getReduceShuffleBytes (  )  ;", "inputRecords    =    attempt . getReduceInputRecords (  )  ;", "outputBytes    =    attempt . getHdfsBytesWritten (  )  ;", "outputRecords    =    attempt . getReduceOutputRecords (  )  ;", "heapMegabytes    =     (  ( job . getJobReduceMB (  )  )     >     0  )     ?    job . getJobReduceMB (  )     :    job . getHeapMegabytes (  )  ;", "}", "metrics    =    attempt . getResourceUsageMetrics (  )  ;", "break ;", "}", "TaskInfo   taskInfo    =    new   TaskInfo ( inputBytes ,     (  ( int )     ( inputRecords )  )  ,    outputBytes ,     (  ( int )     ( outputRecords )  )  ,     (  ( int )     ( heapMegabytes )  )  ,    metrics )  ;", "return   taskInfo ;", "}", "METHOD_END"], "methodName": ["getTaskInfo"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "return   new   TaskAttemptID ( new   TaskID ( job . getJobID (  )  ,    taskType ,    taskNumber )  ,    taskAttemptNumber )  ;", "}", "METHOD_END"], "methodName": ["makeTaskAttemptID"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "long   runtime ;", "if    (  ( state    =  =     ( State . SUCCEEDED )  )     |  |     ( state    =  =     ( State . FAILED )  )  )     {", "List < LoggedDiscreteCDF >    cdfList    =     ( state    =  =     ( State . SUCCEEDED )  )     ?    job . getSuccessfulMapAttemptCDFs (  )     :    job . getFailedMapAttemptCDFs (  )  ;", "if    ( cdfList    =  =    null )     {", "runtime    =     -  1  ;", "return   runtime ;", "}", "try    {", "runtime    =    makeUpRuntime ( cdfList . get ( locality )  )  ;", "}    catch    (  . NoValueToMakeUpRuntime   e )     {", "runtime    =    makeUpRuntime ( cdfList )  ;", "}", "} else    {", "throw   new   IllegalArgumentException (  (  \" state   is   neither   SUCCEEDED   nor   FAILED :     \"     +    state )  )  ;", "}", "return   runtime ;", "}", "METHOD_END"], "methodName": ["makeUpMapRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "long   reduceTime    =     0  ;", "for    ( int   i    =     0  ;    i    <     5  ;    i +  +  )     {", "reduceTime    =    doMakeUpReduceRuntime ( state )  ;", "if    ( reduceTime    >  =     0  )     {", "return   reduceTime ;", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["makeUpReduceRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "int   total    =     0  ;", "if    ( mapAttemptCDFs    =  =    null )     {", "return    -  1  ;", "}", "for    ( LoggedDiscreteCDF   cdf    :    mapAttemptCDFs )     {", "total    +  =    cdf . getNumberValues (  )  ;", "}", "if    ( total    =  =     0  )     {", "return    -  1  ;", "}", "int   index    =    random . nextInt ( total )  ;", "for    ( LoggedDiscreteCDF   cdf    :    mapAttemptCDFs )     {", "if    ( index    >  =     ( cdf . getNumberValues (  )  )  )     {", "index    -  =    cdf . getNumberValues (  )  ;", "} else    {", "if    ( index    <     0  )     {", "throw   new   IllegalStateException (  \" application   error \"  )  ;", "}", "return   makeUpRuntime ( cdf )  ;", "}", "}", "throw   new   IllegalStateException (  \" not   possible   to   get   here \"  )  ;", "}", "METHOD_END"], "methodName": ["makeUpRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( hasRandomSeed )     {", "synchronized ( interpolatorMap )     {", "return   makeUpRuntimeCore ( loggedDiscreteCDF )  ;", "}", "}", "return   makeUpRuntimeCore ( loggedDiscreteCDF )  ;", "}", "METHOD_END"], "methodName": ["makeUpRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "CDFRandomGenerator   interpolator ;", "synchronized ( interpolatorMap )     {", "interpolator    =    interpolatorMap . get ( loggedDiscreteCDF )  ;", "}", "if    ( interpolator    =  =    null )     {", "if    (  ( loggedDiscreteCDF . getNumberValues (  )  )     =  =     0  )     {", "throw   new    . NoValueToMakeUpRuntime (  \" no   value   to   use   to   make   up   runtime \"  )  ;", "}", "interpolator    =     ( hasRandomSeed )     ?    new   CDFPiecewiseLinearRandomGenerator ( loggedDiscreteCDF ,    getNextRandomSeed (  )  )     :    new   CDFPiecewiseLinearRandomGenerator ( loggedDiscreteCDF )  ;", "synchronized ( interpolatorMap )     {", "interpolatorMap . put ( loggedDiscreteCDF ,    interpolator )  ;", "}", "}", "return   interpolator . randomValue (  )  ;", "}", "METHOD_END"], "methodName": ["makeUpRuntimeCore"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( numAttempts    =  =    null )     {", "return   State . FAILED ;", "}", "if    ( taskAttemptNumber    >  =     (  ( numAttempts . length )     -     1  )  )     {", "return   State . SUCCEEDED ;", "} else    {", "double   pSucceed    =    numAttempts [ taskAttemptNumber ]  ;", "double   pFail    =     0  ;", "for    ( int   i    =    taskAttemptNumber    +     1  ;    i    <     ( numAttempts . length )  ;    i +  +  )     {", "pFail    +  =    numAttempts [ i ]  ;", "}", "return    ( random . nextDouble (  )  )     <     ( pSucceed    /     ( pSucceed    +    pFail )  )     ?    State . SUCCEEDED    :    State . FAILED ;", "}", "}", "METHOD_END"], "methodName": ["makeUpState"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( taskType    =  =     ( TaskType . MAP )  )     {", "State   state    =    State . SUCCEEDED ;", "long   runtime    =     0  ;", "state    =    makeUpState ( taskAttemptNumber ,    job . getMapperTriesToSucceed (  )  )  ;", "runtime    =    makeUpMapRuntime ( state ,    locality )  ;", "runtime    =    sanitizeTaskRuntime ( runtime ,    makeTaskAttemptID ( taskType ,    taskNumber ,    taskAttemptNumber )  )  ;", "TaskAttemptInfo   tai    =    new   MapTaskAttemptInfo ( state ,    taskInfo ,    runtime ,    null )  ;", "return   tai ;", "} else", "if    ( taskType    =  =     ( TaskType . REDUCE )  )     {", "State   state    =    State . SUCCEEDED ;", "long   shuffleTime    =     0  ;", "long   sortTime    =     0  ;", "long   reduceTime    =     0  ;", "reduceTime    =    makeUpReduceRuntime ( state )  ;", "TaskAttemptInfo   tai    =    new   ReduceTaskAttemptInfo ( state ,    taskInfo ,    shuffleTime ,    sortTime ,    reduceTime ,    null )  ;", "return   tai ;", "}", "throw   new   IllegalArgtException (  (  \" taskType   is   neither   MAP   nor   REDUCE :     \"     +    taskType )  )  ;", "}", "METHOD_END"], "methodName": ["makeUpTaskAttemptInfo"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "JobID   jobId    =    new   JobID (  )  ;", "TaskType   taskType    =    attemptId . getTaskType (  )  ;", "TaskID   taskId    =    attemptId . getTaskID (  )  ;", "return   new   TaskAttemptID ( jobId . getJtIdentifier (  )  ,    jobId . getId (  )  ,    taskType ,    taskId . getId (  )  ,    attemptId . getId (  )  )  ;", "}", "METHOD_END"], "methodName": ["maskAttemptID"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "JobID   jobId    =    new   JobID (  )  ;", "TaskType   taskType    =    taskId . getTaskType (  )  ;", "return   new   TaskID ( jobId ,    taskType ,    taskId . getId (  )  )  ;", "}", "METHOD_END"], "methodName": ["maskTaskID"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( task    =  =    null )     {", "return   null ;", "}", "if    (  ( task . getTaskType (  )  )     =  =    null )     {", ". LOG . warn (  (  (  \" Task    \"     +     ( task . getTaskID (  )  )  )     +     \"    has   nulll   TaskType \"  )  )  ;", "return   null ;", "}", "if    (  ( task . getTaskStatus (  )  )     =  =    null )     {", ". LOG . warn (  (  (  \" Task    \"     +     ( task . getTaskID (  )  )  )     +     \"    has   nulll   TaskStatus \"  )  )  ;", "return   null ;", "}", "return   task ;", "}", "METHOD_END"], "methodName": ["sanitizeLoggedTask"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( attempt    =  =    null )     {", "return   null ;", "}", "if    (  ( attempt . getResult (  )  )     =  =    null )     {", ". LOG . warn (  (  (  \" TaskAttempt    \"     +     ( attempt . getResult (  )  )  )     +     \"    has   nulll   Result \"  )  )  ;", "return   null ;", "}", "return   attempt ;", "}", "METHOD_END"], "methodName": ["sanitizeLoggedTaskAttempt"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( time    <     0  )     {", ". LOG . warn (  (  (  (  \" Negative   running   time   for   task    \"     +    id )     +     \"  :     \"  )     +    time )  )  ;", "return    1  0  0 L ;", "}", "return   time ;", "}", "METHOD_END"], "methodName": ["sanitizeTaskRuntime"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    ( oldVal    =  =     (  -  1  )  )     {", ". LOG . warn (  (  ( name    +     \"    not   defined   for    \"  )     +    id )  )  ;", "return   defaultVal ;", "}", "return   oldVal ;", "}", "METHOD_END"], "methodName": ["sanitizeValue"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "TaskInfo   taskInfo    =    getTaskInfo ( loggedTask )  ;", "double [  ]    factors    =    new   double [  ]  {     1  .  0  ,    rackLocalOverNodeLocal ,    rackRemoteOverNodeLocal    }  ;", "double   scaleFactor    =     ( factors [ locality ]  )     /     ( factors [ loggedLocality ]  )  ;", "State   state    =     . convertState ( loggedAttempt . getResult (  )  )  ;", "if    (  ( loggedTask . getTaskType (  )  )     =  =     ( Pre 2  1 JobHistoryConstants . Values . MAP )  )     {", "long   taskTime    =     0  ;", "if    (  ( loggedAttempt . getStartTime (  )  )     =  =     0  )     {", "taskTime    =    makeUpMapRuntime ( state ,    locality )  ;", "} else    {", "taskTime    =     ( loggedAttempt . getFinishTime (  )  )     -     ( loggedAttempt . getStartTime (  )  )  ;", "}", "taskTime    =    sanitizeTaskRuntime ( taskTime ,    loggedAttempt . getAttemptID (  )  )  ;", "taskTime    *  =    scaleFactor ;", "return   new   MapTaskAttemptInfo ( state ,    taskInfo ,    taskTime ,    loggedAttempt . allSplitVectors (  )  )  ;", "} else    {", "throw   new   IllegalArgumentException (  (  \" taskType   can   only   be   MAP :     \"     +     ( loggedTask . getTaskType (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["scaleInfo"], "fileName": "org.apache.hadoop.tools.rumen.ZombieJob"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( contains ( word )  )  )     {", "int   index    =    getSize (  )  ;", "list . put ( word ,    index )  ;", "isUpdated    =    true ;", "}", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "return   list . containsKey ( word )  ;", "}", "METHOD_END"], "methodName": ["contains"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "return   list . size (  )  ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "return   list ;", "}", "METHOD_END"], "methodName": ["getWords"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "return   list . get ( word )  ;", "}", "METHOD_END"], "methodName": ["indexOf"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "list    =    new   HashMap < String ,    Integer >  ( size )  ;", "}", "METHOD_END"], "methodName": ["setSize"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "this . list    =    list ;", "}", "METHOD_END"], "methodName": ["setWords"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordList"}, {"methodBody": ["METHOD_START", "{", "String   suffix    =     \"  \"  ;", "for    ( String   ks    :    suffixes )     {", "if    ( data . endsWith ( ks )  )     {", "suffix    =    ks ;", "data    =    data . substring (  0  ,     (  ( data . length (  )  )     -     ( suffix . length (  )  )  )  )  ;", "return   new   String [  ]  {    data ,    suffix    }  ;", "}", "}", "throw   new   RuntimeExcep (  (  (  (  (  (  \" Data    [  \"     +    data )     +     \"  ]    doesn ' t   have   a   suffix   from \"  )     +     \"    known   suffixes    [  \"  )     +     ( StringUtils . join ( suffixes ,     '  ,  '  )  )  )     +     \"  ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["extractSuffix"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility"}, {"methodBody": ["METHOD_START", "{", "for    ( String   ks    :    suffixes )     {", "if    ( data . endsWith ( ks )  )     {", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["hasSuffix"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility"}, {"methodBody": ["METHOD_START", "{", "return   WordListAnonymizerUtility . isKnownData ( data ,    WordListAnonymizerUtility . KNOWN _ WORDS )  ;", "}", "METHOD_END"], "methodName": ["isKnownData"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility"}, {"methodBody": ["METHOD_START", "{", "for    ( String   kd    :    knownWords )     {", "if    ( data . equals ( kd )  )     {", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isKnownData"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility"}, {"methodBody": ["METHOD_START", "{", "if    ( StringUtils . isNumeric ( data )  )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["needsAnonymization"], "fileName": "org.apache.hadoop.tools.rumen.anonymization.WordListAnonymizerUtility"}, {"methodBody": ["METHOD_START", "{", "if    ( data    =  =    null )     {", "return   null ;", "}", "if    (  !  ( wordList . contains ( data )  )  )     {", "wordList . add ( data )  ;", "}", "return    ( wordList . getName (  )  )     +     ( wordList . indexOf ( data )  )  ;", "}", "METHOD_END"], "methodName": ["anonymize"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType"}, {"methodBody": ["METHOD_START", "{", "return   DefaultAnonymizableDataType . DEFAULT _ PREFIX ;", "}", "METHOD_END"], "methodName": ["getPrefix"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType"}, {"methodBody": ["METHOD_START", "{", "return   true ;", "}", "METHOD_END"], "methodName": ["needsAnonymization"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.DefaultAnonymizableDataType"}, {"methodBody": ["METHOD_START", "{", "if    ( data    =  =    null )     {", "return   null ;", "}", "if    ( WordListAnonymizerUtility . needsAnonymization ( data )  )     {", "String   suffix    =     \"  \"  ;", "String   coreData    =    data ;", "if    ( WordListAnonymizerUtility . hasSuffix ( data ,     . KNOWN _ SUFFIXES )  )     {", "String [  ]    split    =    WordListAnonymizerUtility . extractSuffix ( data ,     . KNOWN _ SUFFIXES )  ;", "suffix    =    split [  1  ]  ;", "coreData    =    split [  0  ]  ;", "}", "String   anonymizedData    =    coreData ;", "if    (  !  ( WordListAnonymizerUtility . isKnownData ( coreData )  )  )     {", "if    (  !  ( wordList . contains ( coreData )  )  )     {", "wordList . add ( coreData )  ;", "}", "anonymizedData    =     ( wordList . getName (  )  )     +     ( wordList . indexOf ( coreData )  )  ;", "}", "return   anonymizedData    +    suffix ;", "} else    {", "return   data ;", "}", "}", "METHOD_END"], "methodName": ["anonymize"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.FileName"}, {"methodBody": ["METHOD_START", "{", "FileName . FileNameState   fState    =     (  ( FileName . FileNameState )     ( statePool . getState ( getClass (  )  )  )  )  ;", "if    ( fState    =  =    null )     {", "fState    =    new   FileName . FileNameState (  )  ;", "statePool . addState ( getClass (  )  ,    fState )  ;", "}", "String [  ]    files    =    StringUtils . split ( fileName )  ;", "String [  ]    anonymizedFileNames    =    new   String [ files . length ]  ;", "int   i    =     0  ;", "for    ( String   f    :    files )     {", "anonymizedFileNames [  ( i +  +  )  ]     =    FileName . anonymize ( statePool ,    conf ,    fState ,    f )  ;", "}", "anonymizedFileName    =    StringUtils . arrayToString ( anonymizedFileNames )  ;", "}", "METHOD_END"], "methodName": ["anonymize"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.FileName"}, {"methodBody": ["METHOD_START", "{", "String   ret    =    null ;", "try    {", "URI   uri    =    new   URI ( fileName )  ;", "ret    =     . anonymizePath ( uri . getPath (  )  ,    fState . getDirectoryState (  )  ,    fState . getState (  )  )  ;", "String   authority    =    uri . getAuthority (  )  ;", "String   scheme    =    uri . getScheme (  )  ;", "if    ( scheme    !  =    null )     {", "String   anonymizedAuthority    =     \"  \"  ;", "if    ( authority    !  =    null )     {", "NodeName   hostName    =    new   NodeName ( null ,    uri . getHost (  )  )  ;", "anonymizedAuthority    =    hostName . getAnonymizedValue ( statePool ,    conf )  ;", "}", "ret    =     (  ( scheme    +     \"  :  /  /  \"  )     +    anonymizedAuthority )     +    ret ;", "}", "}    catch    ( URISyntaxException   use )     {", "throw   new   RuntimeException ( use )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["anonymize"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.FileName"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   buffer    =    new   StringBuilder (  )  ;", "StringTokenizer   tokenizer    =    new   StringTokenizer ( path ,    Path . SEPARATOR ,    true )  ;", "while    ( tokenizer . hasMoreTokens (  )  )     {", "String   token    =    tokenizer . nextToken (  )  ;", "if    ( SEPARATOR . equals ( token )  )     {", "buffer . append ( token )  ;", "} else", "if    ( Path . CUR _ DIR . equals ( token )  )     {", "buffer . append ( token )  ;", "} else", "if    (  . PREV _ DIR . equals ( token )  )     {", "buffer . append ( token )  ;", "} else", "if    ( tokenizer . hasMoreTokens (  )  )     {", "buffer . append (  . anonymize ( token ,    dState )  )  ;", "} else    {", "buffer . append (  . anonymize ( token ,    fState )  )  ;", "}", "}", "return   buffer . toString (  )  ;", "}", "METHOD_END"], "methodName": ["anonymizePath"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.FileName"}, {"methodBody": ["METHOD_START", "{", "return   jobProperties ;", "}", "METHOD_END"], "methodName": ["getValue"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.JobProperties"}, {"methodBody": ["METHOD_START", "{", "if    ( data    =  =    null )     {", "return   null ;", "}", "if    (  !  ( wordList . contains ( data )  )  )     {", "wordList . add ( data )  ;", "}", "return    ( wordList . getName (  )  )     +     ( wordList . indexOf ( data )  )  ;", "}", "METHOD_END"], "methodName": ["anonymize"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.NodeName"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   buf    =    new   StringBuffer (  )  ;", "State   state    =     ( State )     ( pool . getState ( getClass (  )  )  )  )  ;", "if    ( state    =  =    null )     {", "state    =    newState (  )  ;", "pool . addState ( getClass (  )  ,    state )  ;", "}", "if    (  (  ( rackName )     !  =    null )     &  &     (  ( hostName )     !  =    null )  )     {", "buf . append (  '  /  '  )  ;", "buf . append . anonymize ( rackName ,    state . getRackNameState (  )  )  )  ;", "buf . append (  '  /  '  )  ;", "buf . append . anonymize ( hostName ,    state . getHostNameState (  )  )  )  ;", "} else    {", "if    (  ( state . getRackNameState (  )  . contains ( nodeName )  )     |  |     (  ( rackName )     !  =    null )  )     {", "buf . append . anonymize ( nodeName ,    state . getRackNameState (  )  )  )  ;", "} else    {", "buf . append . anonymize ( nodeName ,    state . getHostNameState (  )  )  )  ;", "}", "}", "anonymize    =    buf . toString (  )  ;", "}", "METHOD_END"], "methodName": ["anonymize"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.NodeName"}, {"methodBody": ["METHOD_START", "{", "return   hostName ;", "}", "METHOD_END"], "methodName": ["getHostName"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.NodeName"}, {"methodBody": ["METHOD_START", "{", "return   rackName ;", "}", "METHOD_END"], "methodName": ["getRackName"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.NodeName"}, {"methodBody": ["METHOD_START", "{", "return    ( getLatestKeyName ( key )  )     !  =    null ;", "}", "METHOD_END"], "methodName": ["accept"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser"}, {"methodBody": ["METHOD_START", "{", "for    ( String   opt    :    split (  \"     \"  )  )     {", "Matcher   matcher    =     . MAX _ HEAP _ PATTERN . matcher ( opt )  ;", "if    ( matcher . find (  )  )     {", "heapOpts . add ( opt )  ;", "} else    {", "others . add ( opt )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["extractMaxHeapOpts"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser"}, {"methodBody": ["METHOD_START", "{", "for    ( String   opt    :    split (  \"     \"  )  )     {", "Matcher   matcher    =     . MIN _ HEAP _ PATTERN . matcher ( opt )  ;", "if    ( matcher . find (  )  )     {", "heapOpts . add ( opt )  ;", "} else    {", "others . add ( opt )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["extractMinHeapOpts"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser"}, {"methodBody": ["METHOD_START", "{", "if    ( value    !  =    null )     {", "String   latestKey    =    getLatestKeyName ( key )  ;", "if    ( JOB _ NAME . equals ( latestKey )  )     {", "return   new   JobName ( value )  ;", "}", "if    ( USER _ NAME . equals ( latestKey )  )     {", "return   new   UserName ( value )  ;", "}", "if    ( QUEUE _ NAME . equals ( latestKey )  )     {", "return   new   QueueName ( value )  ;", "}", "if    (  ( MAP _ JAVA _ OPTS . equals ( latestKey )  )     |  |     ( REDUCE _ JAVA _ OPTS . equals ( latestKey )  )  )     {", "List < String >    heapOptions    =    new   ArrayList < String >  (  )  ;", ". extractMaxHeapOpts ( value ,    heapOptions ,    new   ArrayList < String >  (  )  )  ;", ". extractMinHeapOpts ( value ,    heapOptions ,    new   ArrayList < String >  (  )  )  ;", "return   new   DefaultDataType ( StringUtils . join ( heapOptions ,     '     '  )  )  ;", "}", "try    {", "format . parse ( value )  ;", "return   new   DefaultDataType ( value )  ;", "}    catch    ( ParseException   pe )     {", "}", "if    (  (  \" true \"  . equals ( value )  )     |  |     (  \" false \"  . equals ( value )  )  )     {", "Boolean . parseBoolean ( value )  ;", "return   new   DefaultDataType ( value )  ;", "}", "if    (  ( latestKey . endsWith (  \"  . class \"  )  )     |  |     ( latestKey . endsWith (  \"  . codec \"  )  )  )     {", "return   new   ClassName ( value )  ;", "}", "if    (  ( latestKey . endsWith (  \" sizes \"  )  )     |  |     ( latestKey . endsWith (  \"  . timestamps \"  )  )  )     {", "new   DefaultDataType ( value )  ;", "}", "if    (  (  (  (  (  (  (  ( latestKey . endsWith (  \"  . dir \"  )  )     |  |     ( latestKey . endsWith (  \"  . location \"  )  )  )     |  |     ( latestKey . endsWith (  \"  . jar \"  )  )  )     |  |     ( latestKey . endsWith (  \"  . path \"  )  )  )     |  |     ( latestKey . endsWith (  \"  . logfile \"  )  )  )     |  |     ( latestKey . endsWith (  \"  . file \"  )  )  )     |  |     ( latestKey . endsWith (  \"  . files \"  )  )  )     |  |     ( latestKey . endsWith (  \"  . archives \"  )  )  )     {", "try    {", "return   new   FileName ( value )  ;", "}    catch    ( Exception   ioe )     {", "}", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["fromString"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( key ,    key )  ;", "try    {", "for    ( Field   f    :    mrFields )     {", "String   mrKey    =    f . get ( f . getName (  )  ) String (  )  ;", "if    (  ( configuration . get ( mrKey )  )     !  =    null )     {", "return   mrKey ;", "}", "}", "return   null ;", "}    catch    ( IllegalAccessException   iae )     {", "throw   new   RuntimeException ( iae )  ;", "}    finally    {", "configuration . clear (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getLatestKeyName"], "fileName": "org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser"}, {"methodBody": ["METHOD_START", "{", "jGen . writeNull (  )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.tools.rumen.serializers.BlockingSerializer"}, {"methodBody": ["METHOD_START", "{", "Object   val    =    object . getAnonymizedValue ( statePool ,    conf )  ;", "if    ( val   instanceof   String )     {", "jGen . writeString ( val . toString (  )  )  ;", "} else    {", "jGen . writeObject ( val )  ;", "}", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.tools.rumen.serializers.DefaultAnonymizingRumenSerializer"}, {"methodBody": ["METHOD_START", "{", "Object   data    =    object . getValue (  )  ;", "if    ( data   instanceof   String )     {", "jGwriteString ( data . toString (  )  )  ;", "} else    {", "jGwriteObject ( data )  ;", "}", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.tools.rumen.serializers.DefaultRumenSerializer"}, {"methodBody": ["METHOD_START", "{", "jGen . writeString ( object . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["serialize"], "fileName": "org.apache.hadoop.tools.rumen.serializers.ObjectStringSerializer"}, {"methodBody": ["METHOD_START", "{", "if    ( pool . containsKey ( id . getName (  )  )  )     {", "throw   new   RuntimeException (  (  (  (  (  (  \" State    '  \"     +     ( state . getName (  )  )  )     +     \"  '    added   for   the \"  )     +     \"    class    \"  )     +     ( id . getName (  )  )  )     +     \"    already   exists !  \"  )  )  ;", "}", "isUpdated    =    true ;", "pool . put ( id . getName (  )  ,    new    . StatePair ( state )  )  ;", "}", "METHOD_END"], "methodName": ["addState"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "return   pool . containsKey ( clazz . getName (  )  )     ?    pool . get ( clazz . getName (  )  )  . getState (  )     :    null ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "return   pool ;", "}", "METHOD_END"], "methodName": ["getStates"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "return   StatePool . VERSION ;", "}", "METHOD_END"], "methodName": ["getVersion"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "if    ( isInitialized )     {", "throw   new   RuntimeException (  \"    is   already   initialized !  \"  )  ;", "}", "this . conf    =    conf ;", "String   persistDir    =    conf . get (  . DIR _ CONFIG )  ;", "reload    =    conf . getBoolean (  . RELOAD _ CONFIG ,    false )  ;", "persist    =    conf . getBoolean (  . PERSIST _ CONFIG ,    false )  ;", "if    (  ( reload )     |  |     ( persist )  )     {", "System . out . println (  (  \" State   Manager   initializing .    State   directory    :     \"     +    persistDir )  )  ;", "System . out . println (  (  (  (  \" Reload :  \"     +     ( reload )  )     +     \"    Persist :  \"  )     +     ( persist )  )  )  ;", "if    ( persistDir    =  =    null )     {", "throw   new   RuntimeException (  (  \" No   state   persist   directory   configured !  \"     +     \"    Disable   persistence .  \"  )  )  ;", "} else    {", "this . persistDirPath    =    new   Path ( persistDir )  ;", "}", "} else    {", "System . out . println (  \" State   Manager   disabled .  \"  )  ;", "}", "reload (  )  ;", "DateFormat   formatter    =    new   SimpleDateFormat (  \" dd - MMM - yyyy - hh ' H '  - mm ' M '  - ss ' S '  \"  )  ;", "Calendar   calendar    =    Calendar . getInstance (  )  ;", "calendar . setTimeInMillis ( System . currentTimeMillis (  )  )  ;", "timeStamp    =    formatter . format ( calendar . getTime (  )  )  ;", "isInitialized    =    true ;", "}", "METHOD_END"], "methodName": ["initialize"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isUpdated )  )     {", "for    (  . StatePair   statePair    :    pool . values (  )  )     {", "if    ( statePair . getState (  )  . isUpdated (  )  )     {", "isUpdated    =    true ;", "return   true ;", "}", "}", "}", "return   isUpdated ;", "}", "METHOD_END"], "methodName": ["isUpdated"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( persist )  )     {", "return ;", "}", "if    ( isUpdated (  )  )     {", "System . out . println (  \" State   is   updated !    Committing .  \"  )  ;", "Path   currStateFile    =    new   Path ( persistDirPath ,     . CURRENT _ STATE _ FILENAME )  ;", "Path   commitStateFile    =    new   Path ( persistDirPath ,     . COMMIT _ STATE _ FILENAME )  ;", "FileSystem   fs    =    currStateFile . getFileSystem ( conf )  ;", "System . out . println (  (  \" Starting   the   persist   phase .    Persisting   to    \"     +     ( currStateFile . toString (  )  )  )  )  ;", "FSDataOutputStream   out    =    fs . create ( currStateFile ,    true )  ;", "write ( out )  ;", "out . close (  )  ;", "System . out . println (  (  (  \" Persist   phase   over .    The   best   known   un - committed   state \"     +     \"    is   located   at    \"  )     +     ( currStateFile . toString (  )  )  )  )  ;", "if    ( fs . exists ( commitStateFile )  )     {", "Path   commitRelocationFile    =    new   Path ( persistDirPath ,    timeStamp )  ;", "System . out . println (  (  (  \" Starting   the   pre - commit   phase .    Moving   the   previous    \"     +     \" best   known   state   to    \"  )     +     ( commitRelocationFile . toString (  )  )  )  )  ;", "FileUtil . copy ( fs ,    commitStateFile ,    fs ,    commitRelocationFile ,    false ,    conf )  ;", "}", "System . out . println (  (  \" Starting   the   commit   phase .    Committing   the   states   in    \"     +     ( currStateFile . toString (  )  )  )  )  ;", "FileUtil . copy ( fs ,    currStateFile ,    fs ,    commitStateFile ,    true ,    true ,    conf )  ;", "System . out . println (  (  (  \" Commit   phase   successful !    The   best   known   committed    \"     +     \" state   is   located   at    \"  )     +     ( commitStateFile . toString (  )  )  )  )  ;", "} else    {", "System . out . println (  \" State   not   updated !    No   commit   required .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["persist"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "mapper . configure ( CAN _ OVERRIDE _ ACCESS _ MODIFIERS ,    true )  ;", "SimpleModule   module    =    new   SimpleModule (  \" State   Serializer \"  ,    new   Version (  0  ,     1  ,     1  ,     \" FINAL \"  )  )  ;", "module . addDeserializer (  . StatePair . class ,    new   StateDeserializer (  )  )  ;", "mapper . registerModule ( module )  ;", "JsonParser   parser    =    mapper . getJsonFactory (  )  . createJsonParser (  (  ( DataInputStream )     ( in )  )  )  ;", "statePool    =    mapper . readValue ( parser ,     . class )  ;", "this . setStates ( statePool . getStates (  )  )  ;", "parser . close (  )  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "if    ( reload )     {", "Path   stateFilename    =    new   Path ( persistDirPath ,     . COMMIT _ STATE _ FILENAME )  ;", "FileSystem   fs    =    stateFilename . getFileSystem ( conf )  ;", "if    ( fs . exists ( stateFilename )  )     {", "reloadState ( stateFilename ,    conf )  ;", "} else    {", "throw   new   RuntimeException (  (  \" No   latest   state   persist   directory   found !  \"     +     \"    Disable   persistence   and   run .  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["reload"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    stateFile . getFileSystem ( conf )  ;", "if    ( fs . exists ( stateFile )  )     {", "System . out . println (  (  \" Reading   state   from    \"     +     ( stateFile . toString (  )  )  )  )  ;", "FSDataInputStream   in    =    fs . open ( stateFile )  ;", "read ( in )  ;", "in . close (  )  ;", "} else    {", "System . out . println (  (  \" No   state   information   found   for    \"     +    stateFile )  )  ;", "}", "}", "METHOD_END"], "methodName": ["reloadState"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "if    (  ( pool . size (  )  )     >     0  )     {", "throw   new   RuntimeException (  \" Pool   not   empty !  \"  )  ;", "}", "this . pool    =    s ;", "}", "METHOD_END"], "methodName": ["setStates"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "if    ( version    !  =     ( StatePool . VERSION )  )     {", "throw   new   RuntimeException (  (  (  (  \" Version   mismatch !    Expected    \"     +     ( StatePool . VERSION )  )     +     \"    got    \"  )     +    version )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setVersion"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  \" Dumping   the   StatePool ' s   in   JSON   format .  \"  )  ;", "ObjectMapper   outMapper    =    new   ObjectMapper (  )  ;", "outMapper . configure ( CAN _ OVERRIDE _ ACCESS _ MODIFIERS ,    true )  ;", "SimpleModule   module    =    new   SimpleModule (  \" State   Serializer \"  ,    new   Version (  0  ,     1  ,     1  ,     \" FINAL \"  )  )  ;", "outMapper . registerModule ( module )  ;", "JsonFactory   outFactory    =    outMapper . getJsonFactory (  )  ;", "JsonGenerator   jGen    =    outFactory . createJsonGenerator (  (  ( DataOutputStream )     ( out )  )  ,    UTF 8  )  ;", "jGen . useDefaultPrettyPrinter (  )  ;", "jGen . writeObject ( this )  ;", "jGen . close (  )  ;", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.tools.rumen.state.StatePool"}, {"methodBody": ["METHOD_START", "{", "DistCp   distCp    =    new   DistCp ( conf ,    null )  ;", "String [  ]    optsArr    =     ( options    =  =    null )     ?    new   String [  ]  {    src ,    dst    }     :    new   String [  ]  {    options ,    src ,    dst    }  ;", "Assert . assertEquals ( exitCode ,    ToolRunner . run ( conf ,    distCp ,    optsArr )  )  ;", "}", "METHOD_END"], "methodName": ["assertRunDistCp"], "fileName": "org.apache.hadoop.tools.util.DistCpTestUtils"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    byte [  ]  >    xAttrs    =    fs . getXAttrs ( path )  ;", "Assert . assertEquals ( pathString (  )  ,    expectedXAttrs . size (  )  ,    xAttrs . size (  )  )  ;", "Iterr < Map . Entry < String ,    byte [  ]  >  >    i    =    expectedXAttrs . entrySet (  )  . iterr (  )  ;", "while    ( i . hasNext (  )  )     {", "Map . Entry < String ,    byte [  ]  >    e    =    i . next (  )  ;", "String   name    =    e . getKey (  )  ;", "byte [  ]    value    =    e . getValue (  )  ;", "if    ( value    =  =    null )     {", "Assert . assertTrue (  (  ( xAttrs . containsKey ( name )  )     &  &     (  ( xAttrs . get ( name )  )     =  =    null )  )  )  ;", "} else    {", "Assert . assertArrayEquals ( value ,    xAttrs . get ( name )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["assertXAttrs"], "fileName": "org.apache.hadoop.tools.util.DistCpTestUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "fs . getAclStatus ( new   Path ( Path . SEPARATOR )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   CopyLing . AclsNotSupportedException (  (  \" ACLs   not   supported   for   file   system :     \"     +     ( fs . getUri (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkFileSystemAclSupport"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "fs . getXAttrs ( new   Path ( Path . SEPARATOR )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   CopyLing . XAttrsNotSupportedException (  (  \" XAttrs   not   supported   for   file   system :     \"     +     ( fs . getUri (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkFileSystemXAttrSupport"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "FileChecksum   targetChecksum    =    null ;", "try    {", "sourceChecksum    =     ( sourceChecksum    !  =    null )     ?    sourceChecksum    :    sourceFS . getFileChecksum ( source )  ;", "targetChecksum    =    targetFS . getFileChecksum ( target )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  (  \" Unable   to   retrieve   checksum   for    \"     +    source )     +     \"    or    \"  )     +    target )  ,    e )  ;", "}", "return    (  ( sourceChecksum    =  =    null )     |  |     ( targetChecksum    =  =    null )  )     |  |     ( sourceChecksum . equals ( targetChecksum )  )  ;", "}", "METHOD_END"], "methodName": ["checksumsAreEqual"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "URI   srcUri    =    srcFs . getUri (  )  ;", "URI   dstUri    =    destFs . getUri (  )  ;", "if    (  ( srcUri . getScheme (  )  )     =  =    null )     {", "return   false ;", "}", "if    (  !  ( srcUri . getScheme (  )  . equals ( dstUri . getScheme (  )  )  )  )     {", "return   false ;", "}", "String   srcHost    =    srcUri . getHost (  )  ;", "String   dstHost    =    dstUri . getHost (  )  ;", "if    (  ( srcHost    !  =    null )     &  &     ( dstHost    !  =    null )  )     {", "try    {", "srcHost    =    InetAddress . getByName ( srcHost )  . getCanonicalHostName (  )  ;", "dstHost    =    InetAddress . getByName ( dstHost )  . getCanonicalHostName (  )  ;", "}    catch    ( UnknownHostException   ue )     {", "if    (  . LOG . isDebugEnabled (  )  )", ". LOG . debug (  \" Could   not   compare   file - systems .    Unknown   host :     \"  ,    ue )  ;", "return   false ;", "}", "if    (  !  ( srcHost . equals ( dstHost )  )  )     {", "return   false ;", "}", "} else", "if    (  ( srcHost    =  =    null )     &  &     ( dstHost    !  =    null )  )     {", "return   false ;", "} else", "if    ( srcHost    !  =    null )     {", "return   false ;", "}", "return    ( srcUri . getPort (  )  )     =  =     ( dstUri . getPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["compareFs"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "List < AclEntry >    entries    =    fileSystem . getAclStatus ( fileStatus . getPath (  )  )  . getEntries (  )  ;", "return   AclUgetAclFromPermAndEntries ( fileStatus . getPermission (  )  ,    entries )  ;", "}", "METHOD_END"], "methodName": ["getAcl"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( DistCpUtils . LOG . isDebugEnabled (  )  )", "DistCpUtils . LOG . debug (  (  \" Retrieving   file   size   for :     \"     +    path )  )  ;", "return   path . getFileSystem ( configuration )  . getFileStatus ( path )  . getLen (  )  ;", "}", "METHOD_END"], "methodName": ["getFileSize"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "return   DistCpUtils . FORMATTER . get (  )  ;", "}", "METHOD_END"], "methodName": ["getFormatter"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "int   value    =    configuration . getInt ( label ,     (  -  1  )  )  ;", "assert   value    >  =     0     :     \" Couldn ' t   find    \"     +    label ;", "return   value ;", "}", "METHOD_END"], "methodName": ["getInt"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "long   value    =    configuration . getLong ( label ,     (  -  1  )  )  ;", "assert   value    >  =     0     :     \" Couldn ' t   find    \"     +    label ;", "return   value ;", "}", "METHOD_END"], "methodName": ["getLong"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "String   childPathString    =    childPath . toUri (  )  . getPath (  )  ;", "String   sourceRootPathString    =    sourceRootPath . toUri (  )  . getPath (  )  ;", "return   sourceRootPathString . equals (  \"  /  \"  )     ?    childPathString    :    childPathString . substring ( sourceRootPathString . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["getRelativePath"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "String   confLabel    =     (  \" distcp .  \"     +     ( options . getCopyStrategy (  )  . toLowerCase ( Locale . getDefault (  )  )  )  )     +     \"  . strategy . impl \"  ;", "return   conf . getClass ( confLabel ,    UniformSizeInputFormat . class ,    InputFormat . class )  ;", "}", "METHOD_END"], "methodName": ["getStrategy"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "char [  ]    units    =    new   char [  ]  {     ' B '  ,     ' K '  ,     ' M '  ,     ' G '  ,     ' T '  ,     ' P '     }  ;", "double   current    =    nBytes ;", "double   prev    =    current ;", "int   index    =     0  ;", "while    (  ( current    =    current    /     1  0  2  4  )     >  =     1  )     {", "prev    =    current ;", "+  + index ;", "}", "assert   index    <     ( units . length )     :     \" Too   large   a   number .  \"  ;", "return    (  . getFormatter (  )  . format ( prev )  )     +     ( units [ index ]  )  ;", "}", "METHOD_END"], "methodName": ["getStringDescriptionFor"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "return   fileSystem . getXAttrs ( path )  ;", "}", "METHOD_END"], "methodName": ["getXAttrs"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   buffer    =    new   StringBuffer ( DistCpOptions . FileAttribute . values (  )  . length )  ;", "int   len    =     0  ;", "for    ( DistCpOptions . FileAttribute   attribute    :    attributes )     {", "buffer . append ( attribute . name (  )  . charAt (  0  )  )  ;", "len +  +  ;", "}", "return   buffer . substring (  0  ,    len )  ;", "}", "METHOD_END"], "methodName": ["packAttributes"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "FileStatus   targetFileStatus    =    targetFS . getFileStatus ( path )  ;", "String   group    =    targetFileStatus . getGroup (  )  ;", "String   user    =    targetFileStatus . getOwner (  )  ;", "boolean   chown    =    false ;", "if    ( attributes . contains ( DistCpOptions . FileAttribute . ACL )  )     {", "List < AclEntry >    srcAcl    =    srcFileStatus . getAclEntries (  )  ;", "List < AclEntry >    targetAcl    =     . getAcl ( targetFS ,    targetFileStatus )  ;", "if    (  !  ( srcAcl . equals ( targetAcl )  )  )     {", "targetFS . setAcl ( path ,    srcAcl )  ;", "}", "if    (  ( srcFileStatus . getPermission (  )  . getStickyBit (  )  )     !  =     ( targetFileStatus . getPermission (  )  . getStickyBit (  )  )  )     {", "targetFS . setPermission ( path ,    srcFileStatus . getPermission (  )  )  ;", "}", "} else", "if    (  ( attributes . contains ( DistCpOptions . FileAttribute . PERMISSION )  )     &  &     (  !  ( srcFileStatus . getPermission (  )  . equals ( targetFileStatus . getPermission (  )  )  )  )  )     {", "targetFS . setPermission ( path ,    srcFileStatus . getPermission (  )  )  ;", "}", "final   boolean   preserveXAttrs    =    attributes . contains ( DistCpOptions . FileAttribute . XATTR )  ;", "if    ( preserveXAttrs    |  |    preserveRawXattrs )     {", "final   String   rawNS    =    RAW . name (  )  . toLowerCase (  )  ;", "Map < String ,    byte [  ]  >    srcXAttrs    =    srcFileStatus . getXAttrs (  )  ;", "Map < String ,    byte [  ]  >    targetXAttrs    =     . getXAttrs ( targetFS ,    path )  ;", "if    (  ( srcXAttrs    !  =    null )     &  &     (  !  ( srcXAttrs . equals ( targetXAttrs )  )  )  )     {", "Iterator < Map . Entry < String ,    byte [  ]  >  >    iter    =    srcXAttrs . entrySet (  )  . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < String ,    byte [  ]  >    entry    =    iter . next (  )  ;", "final   String   xattrName    =    entry . getKey (  )  ;", "if    (  ( xattrName . startsWith ( rawNS )  )     |  |    preserveXAttrs )     {", "targetFS . setXAttr ( path ,    entry . getKey (  )  ,    entry . getValue (  )  )  ;", "}", "}", "}", "}", "if    (  (  ( attributes . contains ( DistCpOptions . FileAttribute . REPLICATION )  )     &  &     (  !  ( targetFileStatus . isDirectory (  )  )  )  )     &  &     (  ( srcFileStatus . getReplication (  )  )     !  =     ( targetFileStatus . getReplication (  )  )  )  )     {", "targetFS . setReplication ( path ,    srcFileStatus . getReplication (  )  )  ;", "}", "if    (  ( attributes . contains ( DistCpOptions . FileAttribute . GROUP )  )     &  &     (  !  ( group . equals ( srcFileStatus . getGroup (  )  )  )  )  )     {", "group    =    srcFileStatus . getGroup (  )  ;", "chown    =    true ;", "}", "if    (  ( attributes . contains ( DistCpOptions . FileAttribute . USER )  )     &  &     (  !  ( user . equals ( srcFileStatus . getOwner (  )  )  )  )  )     {", "user    =    srcFileStatus . getOwner (  )  ;", "chown    =    true ;", "}", "if    ( chown )     {", "targetFS . setOwner ( path ,    user ,    group )  ;", "}", "}", "METHOD_END"], "methodName": ["preserve"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( label ,    String . valueOf ( value )  )  ;", "}", "METHOD_END"], "methodName": ["publish"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "SequenceFile . Sorter   sorter    =    new   SequenceFile . Sorter ( fs ,    Text . class ,    CopyListingFileStatus . class ,    conf )  ;", "Path   output    =    new   Path (  (  ( sourceListing . toString (  )  )     +     \"  _ sorted \"  )  )  ;", "if    ( fs . exists ( output )  )     {", "fs . delete ( output ,    false )  ;", "}", "sorter . sort ( sourceListing ,    output )  ;", "return   output ;", "}", "METHOD_END"], "methodName": ["sortListing"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "CopyListingFileStatus   copyListingFileStatus    =    new   CopyListingFileStatus ( fileStatus )  ;", "if    ( preserveAcls )     {", "FsPermission   perm    =    fileStatus . getPermission (  )  ;", "if    ( perm . getAclBit (  )  )     {", "List < AclEntry >    aclEntries    =    fileSystem . getAclStatus ( fileStatus . getPath (  )  )  . getEntries (  )  ;", "copyListingFileStatus . setAclEntries ( aclEntries )  ;", "}", "}", "if    ( preserveXAttrs    |  |    preserveRawXAttrs )     {", "Map < String ,    byte [  ]  >    srcXAttrs    =    fileSystem . getXAttrs ( fileStatus . getPath (  )  )  ;", "if    ( preserveXAttrs    &  &    preserveRawXAttrs )     {", "copyListingFileStatus . setXAttrs ( srcXAttrs )  ;", "} else    {", "Map < String ,    byte [  ]  >    trgXAttrs    =    Maps . newHashMap (  )  ;", "final   String   rawNS    =    RAW . name (  )  . toLowerCase (  )  ;", "for    ( Map . Entry < String ,    byte [  ]  >    ent    :    srcXAttrs . entrySet (  )  )     {", "final   String   xattrName    =    ent . getKey (  )  ;", "if    ( xattrName . startsWith ( rawNS )  )     {", "if    ( preserveRawXAttrs )     {", "trgXAttrs . put ( xattrName ,    ent . getValue (  )  )  ;", "}", "} else", "if    ( preserveXAttrs )     {", "trgXAttrs . put ( xattrName ,    ent . getValue (  )  )  ;", "}", "}", "copyListingFileStatus . setXAttrs ( trgXAttrs )  ;", "}", "}", "return   copyListingFileStatus ;", "}", "METHOD_END"], "methodName": ["toCopyListingFileStatus"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "EnumSet < DistCpOptions . FileAttribute >    retValue    =    EnumSet . noneOf ( DistCpOptions . FileAttribute . class )  ;", "if    ( attributes    !  =    null )     {", "for    ( int   index    =     0  ;    index    <     ( attributes . length (  )  )  ;    index +  +  )     {", "retValue . add ( DistCpOptions . FileAttribute . getAttribute ( attributes . charAt ( index )  )  )  ;", "}", "}", "return   retValue ;", "}", "METHOD_END"], "methodName": ["unpackAttributes"], "fileName": "org.apache.hadoop.tools.util.DistCpUtils"}, {"methodBody": ["METHOD_START", "{", "Exception   latestException ;", "int   counter    =     0  ;", "while    ( true )     {", "try    {", "return   doExecute ( arguments )  ;", "}    catch    ( Exception   exception )     {", ". LOG . error (  (  \" Failure   in   Retriable   command :     \"     +     ( description )  )  ,    exception )  ;", "latestException    =    exception ;", "}", "counter +  +  ;", "RetryAction   action    =    retryPolicy . shouldRetry ( latestException ,    counter ,     0  ,    true )  ;", "if    (  ( action . action )     =  =     ( RetryDecision . RETRY )  )     {", "ThreadUtil . sleepAtLeastIgnoreInterrupts ( action . delayMillis )  ;", "} else    {", "break ;", "}", "}", "throw   new   IOException (  (  \" Couldn ' t   run   retriable - command :     \"     +     ( description )  )  ,    latestException )  ;", "}", "METHOD_END"], "methodName": ["execute"], "fileName": "org.apache.hadoop.tools.util.RetriableCommand"}, {"methodBody": ["METHOD_START", "{", "this . retryPolicy    =    retryHandler ;", "return   this ;", "}", "METHOD_END"], "methodName": ["setRetryPolicy"], "fileName": "org.apache.hadoop.tools.util.RetriableCommand"}, {"methodBody": ["METHOD_START", "{", "Path   base    =    new   Path ( targetBase )  ;", "Stack < Path >    stack    =    new   Stack < Path >  (  )  ;", "stack . push ( base )  ;", "while    (  !  ( stack . isEmpty (  )  )  )     {", "Path   file    =    stack . pop (  )  ;", "if    (  !  ( fs . exists ( file )  )  )", "continue ;", "FileStatus [  ]    fStatus    =    fs . listStatus ( file )  ;", "if    (  ( fStatus    =  =    null )     |  |     (  ( fStatus . length )     =  =     0  )  )", "continue ;", "for    ( FileStatus   status    :    fStatus )     {", "if    ( status . isDirectory (  )  )     {", "stack . push ( status . getPath (  )  )  ;", "}", "Assert . assertTrue ( fs . exists ( new   Path (  (  ( sourceBase    +     \"  /  \"  )     +     (  . getRelativePath ( new   Path ( targetBase )  ,    status . getPath (  )  )  )  )  )  )  )  ;", "}", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["checkIfFoldersAreInSync"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "TestDistCpUtils . cluster    =    new   Builder ( TestDistCpUtils . config )  . numDataNodes (  1  )  . format ( true )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "OutputStream   out    =    fs . create ( new   Path ( filePath )  )  ;", "IO . closeStream ( out )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "String   base    =    TestDistCpUtils . getBase ( baseDir )  ;", "fs . mkdirs ( new   Path (  ( base    +     \"  / newTest / hello / world 1  \"  )  )  )  ;", "fs . mkdirs ( new   Path (  ( base    +     \"  / newTest / hello / world 2  / newworld \"  )  )  )  ;", "fs . mkdirs ( new   Path (  ( base    +     \"  / newTest / hello / world 3  / oldworld \"  )  )  )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest \"  )  )  ,    perm )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest / hello \"  )  )  ,    perm )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest / hello / world 1  \"  )  )  ,    perm )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest / hello / world 2  \"  )  )  ,    perm )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest / hello / world 2  / newworld \"  )  )  ,    perm )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest / hello / world 3  \"  )  )  ,    perm )  ;", "fs . setPermission ( new   Path (  ( base    +     \"  / newTest / hello / world 3  / oldworld \"  )  )  ,    perm )  ;", "TestDistCpUtils . createFile ( fs ,     ( base    +     \"  / newTest /  1  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( base    +     \"  / newTest / hello /  2  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( base    +     \"  / newTest / hello / world 3  / oldworld /  3  \"  )  )  ;", "TestDistCpUtils . createFile ( fs ,     ( base    +     \"  / newTest / hello / world 2  /  4  \"  )  )  ;", "return   base ;", "}", "METHOD_END"], "methodName": ["createTestSetup"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "return   TestDistCpUtils . createTestSetup (  \"  / tmp 1  \"  ,    fs ,    FsPermission . getDefault (  )  )  ;", "}", "METHOD_END"], "methodName": ["createTestSetup"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "return   TestDistCpUtils . createTestSetup (  \"  / tmp 1  \"  ,    fs ,    perm )  ;", "}", "METHOD_END"], "methodName": ["createTestSetup"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    ( fs    !  =    null )     {", "if    ( path    !  =    null )     {", "fs . delete ( new   Path ( path )  ,    true )  ;", "}", "}", "}    catch    ( IOException   e )     {", ". LOG . warn (  \" Exception   encountered    \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestDistCpUtils . cluster )     !  =    null )     {", "TestDistCpUtils . cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["destroy"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "String   location    =    String . valueOf ( TestDistCpUtils . rand . nextLong (  )  )  ;", "return    ( base    +     \"  /  \"  )     +    location ;", "}", "METHOD_END"], "methodName": ["getBase"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "Path   root    =    new   Path (  \"  / tmp / abc \"  )  ;", "Path   child    =    new   Path (  \"  / tmp / abc / xyz / file \"  )  ;", "Assert . assertEquals (  . getRelativePath ( root ,    child )  ,     \"  / xyz / file \"  )  ;", "root    =    new   Path (  \"  /  \"  )  ;", "child    =    new   Path (  \"  / a \"  )  ;", "Assert . assertEquals (  . getRelativePath ( root ,    child )  ,     \"  / a \"  )  ;", "}", "METHOD_END"], "methodName": ["testGetRelativePathRoot"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "EnumSet < DistCpOptions . FileAttribute >    attributes    =    EnumSet . noneOf ( DistCpOptions . FileAttribute . class )  ;", "Assert . assertEquals (  . packAttributes ( attributes )  ,     \"  \"  )  ;", "attributes . add ( DistCpOptions . FileAttribute . REPLICATION )  ;", "Assert . assertEquals (  . packAttributes ( attributes )  ,     \" R \"  )  ;", "Assert . assertEquals ( attributes ,     . unpackAttributes (  \" R \"  )  )  ;", "attributes . add ( DistCpOptions . FileAttribute . BLOCKSIZE )  ;", "Assert . assertEquals (  . packAttributes ( attributes )  ,     \" RB \"  )  ;", "Assert . assertEquals ( attributes ,     . unpackAttributes (  \" RB \"  )  )  ;", "attributes . add ( DistCpOptions . FileAttribute . USER )  ;", "Assert . assertEquals (  . packAttributes ( attributes )  ,     \" RBU \"  )  ;", "Assert . assertEquals ( attributes ,     . unpackAttributes (  \" RBU \"  )  )  ;", "attributes . add ( DistCpOptions . FileAttribute . GROUP )  ;", "Assert . assertEquals (  . packAttributes ( attributes )  ,     \" RBUG \"  )  ;", "Assert . assertEquals ( attributes ,     . unpackAttributes (  \" RBUG \"  )  )  ;", "attributes . add ( DistCpOptions . FileAttribute . PERMISSION )  ;", "Assert . assertEquals (  . packAttributes ( attributes )  ,     \" RBUGP \"  )  ;", "Assert . assertEquals ( attributes ,     . unpackAttributes (  \" RBUGP \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testPackAttributes"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileSystem   fs    =    FileSystem . get (  . config )  ;", "EnumSet < DistCpOptions . FileAttribute >    attributes    =    EnumSet . noneOf ( DistCpOptions . FileAttribute . class )  ;", "Path   path    =    new   Path (  \"  / tmp / abc \"  )  ;", "Path   src    =    new   Path (  \"  / tmp / src \"  )  ;", "fs . mkdirs ( path )  ;", "fs . mkdirs ( src )  ;", "CopyListingFileStatus   srcStatus    =    new   CopyListingFileStatus ( fs . getFileStatus ( src )  )  ;", "FsPermission   noPerm    =    new   FsPermission (  (  ( short )     (  0  )  )  )  ;", "fs . setPermission ( path ,    noPerm )  ;", "fs . setOwner ( path ,     \" nobody \"  ,     \" nobody \"  )  ;", "DistCpUtils . preserve ( fs ,    path ,    srcStatus ,    attributes ,    false )  ;", "FileStatus   target    =    fs . getFileStatus ( path )  ;", "Assert . assertEquals ( target . getPermission (  )  ,    noPerm )  ;", "Assert . assertEquals ( target . getOwner (  )  ,     \" nobody \"  )  ;", "Assert . assertEquals ( target . getGroup (  )  ,     \" nobody \"  )  ;", "attributes . add ( DistCpOptions . FileAttribute . PERMISSION )  ;", "DistCpUtils . preserve ( fs ,    path ,    srcStatus ,    attributes ,    false )  ;", "target    =    fs . getFileStatus ( path )  ;", "Assert . assertEquals ( target . getPermission (  )  ,    srcStatus . getPermission (  )  )  ;", "Assert . assertEquals ( target . getOwner (  )  ,     \" nobody \"  )  ;", "Assert . assertEquals ( target . getGroup (  )  ,     \" nobody \"  )  ;", "attributes . add ( DistCpOptions . FileAttribute . GROUP )  ;", "attributes . add ( DistCpOptions . FileAttribute . USER )  ;", "DistCpUtils . preserve ( fs ,    path ,    srcStatus ,    attributes ,    false )  ;", "target    =    fs . getFileStatus ( path )  ;", "Assert . assertEquals ( target . getPermission (  )  ,    srcStatus . getPermission (  )  )  ;", "Assert . assertEquals ( target . getOwner (  )  ,    srcStatus . getOwner (  )  )  ;", "Assert . assertEquals ( target . getGroup (  )  ,    srcStatus . getGroup (  )  )  ;", "fs . delete ( path ,    true )  ;", "fs . delete ( src ,    true )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "Assert . fail (  \" Preserve   test   failure \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPreserve"], "fileName": "org.apache.hadoop.tools.util.TestDistCpUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "new    . MyRetriableCommand (  5  )  . execute (  0  )  ;", "Assert . assertTrue ( false )  ;", "}    catch    ( Exception   e )     {", "Assert . assertTrue ( true )  ;", "}", "try    {", "new    . MyRetriableCommand (  3  )  . execute (  0  )  ;", "Assert . assertTrue ( true )  ;", "}    catch    ( Exception   e )     {", "Assert . assertTrue ( false )  ;", "}", "try    {", "new    . MyRetriableCommand (  5  ,    RetryPolicies . retryUpToMaximumCountWithFixedSleep (  5  ,     0  ,    TimeUnit . MILLISECONDS )  )  . execute (  0  )  ;", "Assert . assertTrue ( true )  ;", "}    catch    ( Exception   e )     {", "Assert . assertTrue ( false )  ;", "}", "}", "METHOD_END"], "methodName": ["testRetriableCommand"], "fileName": "org.apache.hadoop.tools.util.TestRetriableCommand"}, {"methodBody": ["METHOD_START", "{", "long   bandwidth ;", "ThrottledInputStream   in ;", "long   maxBPS    =     (  ( long )     ( maxBandwidth    /    factor )  )  ;", "if    ( maxBandwidth    =  =     0  )     {", "in    =    new   ThrottledInputStream ( new   FileInputStream ( tmpFile )  )  ;", "} else    {", "in    =    new   ThrottledInputStream ( new   FileInputStream ( tmpFile )  ,    maxBPS )  ;", "}", "OutputStream   out    =    new   FileOutputStream ( outFile )  ;", "try    {", "if    ( flag    =  =     (  . CB . BUFFER )  )     {", ". copyBytes ( in ,    out ,     . BUFF _ SIZE )  ;", "} else", "if    ( flag    =  =     (  . CB . BUFF _ OFFSET )  )     {", ". copyBytesWithOffset ( in ,    out ,     . BUFF _ SIZE )  ;", "} else    {", ". copyByteByByte ( in ,    out )  ;", "}", ". LOG . info ( in )  ;", "bandwidth    =    in . getBytesPerSec (  )  ;", "Assert . assertEquals ( in . getTotalBytesRead (  )  ,    tmpFile . length (  )  )  ;", "Assert . assertTrue (  (  ( in . getBytesPerSec (  )  )     >     ( maxBandwidth    /     ( factor    *     1  .  2  )  )  )  )  ;", "Assert . assertTrue (  (  (  ( in . getTotalSleepTime (  )  )     >    sleepTime )     |  |     (  ( in . getBytesPerSec (  )  )     <  =    maxBPS )  )  )  ;", "}    finally    {", "IOUtils . closeStream ( in )  ;", "IOUtils . closeStream ( out )  ;", "}", "return   bandwidth ;", "}", "METHOD_END"], "methodName": ["copyAndAssert"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "int   ch    =    in . read (  )  ;", "while    ( ch    >  =     0  )     {", "out . write ( ch )  ;", "ch    =    in . read (  )  ;", "}", "}", "METHOD_END"], "methodName": ["copyByteByByte"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buf    =    new   byte [ buffSize ]  ;", "int   bytesRead    =    in . d ( buf )  ;", "while    ( bytesRead    >  =     0  )     {", "out . write ( buf ,     0  ,    bytesRead )  ;", "bytesRead    =    in . d ( buf )  ;", "}", "}", "METHOD_END"], "methodName": ["copyBytes"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    buf    =    new   byte [ buffSize ]  ;", "int   bytesRead    =    in . d ( buf ,     0  ,    buffSize )  ;", "while    ( bytesRead    >  =     0  )     {", "out . write ( buf ,     0  ,    bytesRead )  ;", "bytesRead    =    in . d ( buf )  ;", "}", "}", "METHOD_END"], "methodName": ["copyBytesWithOffset"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "return   File . createTempFile (  \" tmp \"  ,     \" dat \"  )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "File   tmpFile    =    createFile (  )  ;", "writeToFile ( tmpFile ,    sizeInKB )  ;", "return   tmpFile ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "File   tmpFile ;", "File   outFile ;", "try    {", "tmpFile    =    createFile (  1  0  2  4  )  ;", "outFile    =    createFile (  )  ;", "tmpFile . deleteOnExit (  )  ;", "outFile . deleteOnExit (  )  ;", "long   maxBandwidth    =    copyAndAssert ( tmpFile ,    outFile ,     0  ,     1  ,     (  -  1  )  ,     . CB . BUFFER )  ;", "copyAndAssert ( tmpFile ,    outFile ,    maxBandwidth ,     2  0  ,     0  ,     . CB . BUFFER )  ;", "copyAndAssert ( tmpFile ,    outFile ,    maxBandwidth ,     2  0  ,     0  ,     . CB . BUFF _ OFFSET )  ;", "copyAndAssert ( tmpFile ,    outFile ,    maxBandwidth ,     2  0  ,     0  ,     . CB . ONE _ C )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Exception   encountered    \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["testRead"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "OutputStream   out    =    new   FileOutputStream ( tmpFile )  ;", "try    {", "byte [  ]    buffer    =    new   byte [  1  0  2  4  ]  ;", "for    ( long   index    =     0  ;    index    <    sizeInKB ;    index +  +  )     {", "out . write ( buffer )  ;", "}", "}    finally    {", "IOUtils . closeStream ( out )  ;", "}", "}", "METHOD_END"], "methodName": ["writeToFile"], "fileName": "org.apache.hadoop.tools.util.TestThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =     (  ( System . currentTimeMillis (  )  )     -     ( startTime )  )     /     1  0  0  0  ;", "if    ( elapsed    =  =     0  )     {", "return   bytesRead ;", "} else    {", "return    ( bytesRead )     /    elapsed ;", "}", "}", "METHOD_END"], "methodName": ["getBytesPerSec"], "fileName": "org.apache.hadoop.tools.util.ThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "return   bytesRead ;", "}", "METHOD_END"], "methodName": ["getTotalBytesRead"], "fileName": "org.apache.hadoop.tools.util.ThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "return   totalSleepTime ;", "}", "METHOD_END"], "methodName": ["getTotalSleepTime"], "fileName": "org.apache.hadoop.tools.util.ThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "if    (  !  (  ( rawStream )    instanceof   PositionedReadable )  )     {", "throw   new   UnsupportedOperationException (  \" positioned   read   is   not   supported   by   the   internal   stream \"  )  ;", "}", "t (  )  ;", "int   readLen    =     (  ( PositionedReadable )     ( rawStream )  )  . read ( position ,    buffer ,    offset ,    length )  ;", "if    ( readLen    !  =     (  -  1  )  )     {", "bytesRead    +  =    readLen ;", "}", "return   readLen ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.tools.util.ThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "if    (  ( getBytesPerSec (  )  )     >     ( maxBytesPerSec )  )     {", "try    {", "Thread . sleep (  . SLEEP _ DURATION _ MS )  ;", "totalSleepTime    +  =     . SLEEP _ DURATION _ MS ;", "}    catch    ( InterruptedException   e )     {", "throw   new   IOException (  \" Thread   aborted \"  ,    e )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["throttle"], "fileName": "org.apache.hadoop.tools.util.ThrottledInputStream"}, {"methodBody": ["METHOD_START", "{", "TypedBytesWritable   tbw    =    new   TypedBytesWritable (  )  ;", "tbw . setValue (  1  2  3  4  5  )  ;", "ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "DataOutput   dout    =    new   DataOutputStream ( baos )  ;", "tbw . write ( dout )  ;", "ByteArrayInputStream   bais    =    new   ByteArrayInputStream ( baos . toByteArray (  )  )  ;", "DataInput   din    =    new   DataInputStream ( bais )  ;", "TypedBytesWritable   readTbw    =    new   TypedBytesWritable (  )  ;", "readTbw . readFields ( din )  ;", "assertEquals ( tbw ,    readTbw )  ;", "}", "METHOD_END"], "methodName": ["testIO"], "fileName": "org.apache.hadoop.typedbytes.TestTypedBytesWritable"}, {"methodBody": ["METHOD_START", "{", "TypedBytesWritable   tbw    =    new   TypedBytesWritable (  )  ;", "tbw . setValue ( true )  ;", "assertEquals (  \" true \"  ,    tbw . toString (  )  )  ;", "tbw . setValue (  1  2  3  4  5  )  ;", "assertEquals (  \"  1  2  3  4  5  \"  ,    tbw . toString (  )  )  ;", "tbw . setValue (  1  2  3  4  5  6  7  8  9 L )  ;", "assertEquals (  \"  1  2  3  4  5  6  7  8  9  \"  ,    tbw . toString (  )  )  ;", "tbw . setValue (  (  ( float )     (  1  .  2  3  )  )  )  ;", "assertEquals (  \"  1  .  2  3  \"  ,    tbw . toString (  )  )  ;", "tbw . setValue (  1  .  2  3  4  5  6  7  8  9  )  ;", "assertEquals (  \"  1  .  2  3  4  5  6  7  8  9  \"  ,    tbw . toString (  )  )  ;", "tbw . setValue (  \" random   text \"  )  ;", "assertEquals (  \" random   text \"  ,    tbw . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testToString"], "fileName": "org.apache.hadoop.typedbytes.TestTypedBytesWritable"}, {"methodBody": ["METHOD_START", "{", "TypedBytesInput   bin    =     (  ( TypedBytesInput )     ( TypedBytesInput . tbIn . get (  )  )  )  ;", "bin . setDataInput ( in )  ;", "return   bin ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   code    =     1  ;", "try    {", "code    =    in . readUnsign (  )  ;", "}    catch    ( EOFException   eof )     {", "return   null ;", "}", "if    ( code    =  =     ( Type . BYTES . code )  )     {", "return   new   Buffer ( readBytes (  )  )  ;", "} else", "if    ( code    =  =     ( Type . BYTE . code )  )     {", "return   readByte (  )  ;", "} else", "if    ( code    =  =     ( Type . BOOL . code )  )     {", "return   readBool (  )  ;", "} else", "if    ( code    =  =     ( Type . INT . code )  )     {", "return   readInt (  )  ;", "} else", "if    ( code    =  =     ( Type . LONG . code )  )     {", "return   readLong (  )  ;", "} else", "if    ( code    =  =     ( Type . FLOAT . code )  )     {", "return   readFloat (  )  ;", "} else", "if    ( code    =  =     ( Type . DOUBLE . code )  )     {", "return   readDouble (  )  ;", "} else", "if    ( code    =  =     ( Type . STRING . code )  )     {", "return   readString (  )  ;", "} else", "if    ( code    =  =     ( Type . VECTOR . code )  )     {", "return   readVector (  )  ;", "} else", "if    ( code    =  =     ( Type . LIST . code )  )     {", "return   readList (  )  ;", "} else", "if    ( code    =  =     ( Type . MAP . code )  )     {", "return   readMap (  )  ;", "} else", "if    ( code    =  =     ( Type . MARKER . code )  )     {", "return   null ;", "} else", "if    (  (  5  0     <  =    code )     &  &     ( code    <  =     2  0  0  )  )     {", "return   new   Buffer ( readBytes (  )  )  ;", "} else    {", "throw   new   RuntimeException (  \" unknown   type \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readBoolean (  )  ;", "}", "METHOD_END"], "methodName": ["readBool"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readByte (  )  ;", "}", "METHOD_END"], "methodName": ["readByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   length    =    in . readInt (  )  ;", "byte [  ]        =    new   byte [ length ]  ;", "in . readFully (  )  ;", "return    ;", "}", "METHOD_END"], "methodName": ["readBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readDouble (  )  ;", "}", "METHOD_END"], "methodName": ["readDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readFloat (  )  ;", "}", "METHOD_END"], "methodName": ["readFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readInt (  )  ;", "}", "METHOD_END"], "methodName": ["readInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "List   list    =    new   ArrayList (  )  ;", "Object   obj    =    re (  )  ;", "while    ( obj    !  =    null )     {", "list . d ( obj )  ;", "obj    =    re (  )  ;", "}", "return   list ;", "}", "METHOD_END"], "methodName": ["readList"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readLong (  )  ;", "}", "METHOD_END"], "methodName": ["readLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   length    =    readMapHeader (  )  ;", "TreeMap   result    =    new   TreeMap (  )  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "Object   key    =    read (  )  ;", "Object   value    =    read (  )  ;", "result .  ( key ,    value )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["readMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readInt (  )  ;", "}", "METHOD_END"], "methodName": ["readMapHeader"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   code    =     -  1  ;", "try    {", "code    =    in . readUnsign (  )  ;", "}    catch    ( EOFException   eof )     {", "return   null ;", "}", "if    ( code    =  =     ( Type . BYTES . code )  )     {", "return   readRawBytes (  )  ;", "} else", "if    ( code    =  =     ( Type . BYTE . code )  )     {", "return   readRawByte (  )  ;", "} else", "if    ( code    =  =     ( Type . BOOL . code )  )     {", "return   readRawBool (  )  ;", "} else", "if    ( code    =  =     ( Type . INT . code )  )     {", "return   readRawInt (  )  ;", "} else", "if    ( code    =  =     ( Type . LONG . code )  )     {", "return   readRawLong (  )  ;", "} else", "if    ( code    =  =     ( Type . FLOAT . code )  )     {", "return   readRawFloat (  )  ;", "} else", "if    ( code    =  =     ( Type . DOUBLE . code )  )     {", "return   readRawDouble (  )  ;", "} else", "if    ( code    =  =     ( Type . STRING . code )  )     {", "return   readRawString (  )  ;", "} else", "if    ( code    =  =     ( Type . VECTOR . code )  )     {", "return   readRawVector (  )  ;", "} else", "if    ( code    =  =     ( Type . LIST . code )  )     {", "return   readRawList (  )  ;", "} else", "if    ( code    =  =     ( Type . MAP . code )  )     {", "return   readRawMap (  )  ;", "} else", "if    ( code    =  =     ( Type . MARKER . code )  )     {", "return   null ;", "} else", "if    (  (  5  0     <  =    code )     &  &     ( code    <  =     2  0  0  )  )     {", "return   readRawBytes ( code )  ;", "} else    {", "throw   new   RuntimeException (  \" unknown   type \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["readRaw"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [  2  ]  ;", "bytes [  0  ]     =     (  ( byte )     ( Type . BOOL . code )  )  ;", "in . readFully ( bytes ,     1  ,     1  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readRawBool"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [  2  ]  ;", "bytes [  0  ]     =     (  ( byte )     ( Type . BYTE . code )  )  ;", "in . readFully ( bytes ,     1  ,     1  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readRawByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   readRawBytes ( Type . BYTES . code )  ;", "}", "METHOD_END"], "methodName": ["readRawBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   length    =    in . readInt (  )  ;", "byte [  ]        =    new   byte [  5     +    length ]  ;", "[  0  ]     =     (  ( byte )     ( code )  )  ;", "[  1  ]     =     (  ( byte )     (  2  5  5     &     ( length    >  >     2  4  )  )  )  ;", "[  2  ]     =     (  ( byte )     (  2  5  5     &     ( length    >  >     1  6  )  )  )  ;", "[  3  ]     =     (  ( byte )     (  2  5  5     &     ( length    >  >     8  )  )  )  ;", "[  4  ]     =     (  ( byte )     (  2  5  5     &    length )  )  ;", "in . readFully (  ,     5  ,    length )  ;", "return    ;", "}", "METHOD_END"], "methodName": ["readRawBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [  9  ]  ;", "bytes [  0  ]     =     (  ( byte )     ( Type . DOUBLE . code )  )  ;", "in . readFully ( bytes ,     1  ,     8  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readRawDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [  5  ]  ;", "bytes [  0  ]     =     (  ( byte )     ( Type . FLOAT . code )  )  ;", "in . readFully ( bytes ,     1  ,     4  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readRawFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [  5  ]  ;", "bytes [  0  ]     =     (  ( byte )     ( Type . INT . code )  )  ;", "in . readFully ( bytes ,     1  ,     4  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readRawInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "Buffer   buffer    =    new   Buffer ( new   byte [  ]  {     (  ( byte )     ( Type . LIST . code )  )     }  )  ;", "byte [  ]        =    readRaw (  )  ;", "while    (     !  =    null )     {", "buffer . append (  )  ;", "=    readRaw (  )  ;", "}", "buffer . append ( new   byte [  ]  {     (  ( byte )     ( Type . MARKER . code )  )     }  )  ;", "return   buffer . get (  )  ;", "}", "METHOD_END"], "methodName": ["readRawList"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [  9  ]  ;", "bytes [  0  ]     =     (  ( byte )     ( Type . LONG . code )  )  ;", "in . readFully ( bytes ,     1  ,     8  )  ;", "return   bytes ;", "}", "METHOD_END"], "methodName": ["readRawLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "Buffer   buffer    =    new   Buffer (  )  ;", "int   length    =    readMapHeader (  )  ;", "buffer . append ( new    [  ]  {     (  (  )     ( Type . MAP . code )  )  ,     (  (  )     (  2  5  5     &     ( length    >  >     2  4  )  )  )  ,     (  (  )     (  2  5  5     &     ( length    >  >     1  6  )  )  )  ,     (  (  )     (  2  5  5     &     ( length    >  >     8  )  )  )  ,     (  (  )     (  2  5  5     &    length )  )     }  )  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "buffer . append ( readRaw (  )  )  ;", "buffer . append ( readRaw (  )  )  ;", "}", "return   buffer . get (  )  ;", "}", "METHOD_END"], "methodName": ["readRawMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   length    =    in . readInt (  )  ;", "byte [  ]        =    new   byte [  5     +    length ]  ;", "[  0  ]     =     (  ( byte )     ( Type . STRING . code )  )  ;", "[  1  ]     =     (  ( byte )     (  2  5  5     &     ( length    >  >     2  4  )  )  )  ;", "[  2  ]     =     (  ( byte )     (  2  5  5     &     ( length    >  >     1  6  )  )  )  ;", "[  3  ]     =     (  ( byte )     (  2  5  5     &     ( length    >  >     8  )  )  )  ;", "[  4  ]     =     (  ( byte )     (  2  5  5     &    length )  )  ;", "in . readFully (  ,     5  ,    length )  ;", "return    ;", "}", "METHOD_END"], "methodName": ["readRawString"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "Buffer   buffer    =    new   Buffer (  )  ;", "int   length    =    readVectorHeader (  )  ;", "buffer . append ( new    [  ]  {     (  (  )     ( Type . VECTOR . code )  )  ,     (  (  )     (  2  5  5     &     ( length    >  >     2  4  )  )  )  ,     (  (  )     (  2  5  5     &     ( length    >  >     1  6  )  )  )  ,     (  (  )     (  2  5  5     &     ( length    >  >     8  )  )  )  ,     (  (  )     (  2  5  5     &    length )  )     }  )  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "buffer . append ( readRaw (  )  )  ;", "}", "return   buffer . get (  )  ;", "}", "METHOD_END"], "methodName": ["readRawVector"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   WritableUtils . readString ( in )  ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   code    =     -  1  ;", "try    {", "code    =    in . readUnsign (  )  ;", "}    catch    ( EOFException   eof )     {", "return   null ;", "}", "for    ( Type   type    :    Type . values (  )  )     {", "if    (  ( type . code )     =  =    code )     {", "return   type ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["readType"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "int   length    =    readVectorHeader (  )  ;", "ArrayList   result    =    new   ArrayList ( length )  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "result . add ( read (  )  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["readVector"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "return   in . readInt (  )  ;", "}", "METHOD_END"], "methodName": ["readVectorHeader"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "this . in    =    in ;", "}", "METHOD_END"], "methodName": ["setDataInput"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "try    {", "in . rea (  )  ;", "return   true ;", "}    catch    ( EOFException   eof )     {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["skipType"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesInput"}, {"methodBody": ["METHOD_START", "{", "TypedBytesOutput   bout    =     (  ( TypedBytesOutput )     ( TypedBytesOutput . tbOut . get (  )  )  )  ;", "bout . setDataOutput ( out )  ;", "return   bout ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "this . out    =    out ;", "}", "METHOD_END"], "methodName": ["setDataOutput"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "if    ( obj   instanceof   Buffer )     {", "write (  (  ( Buffer )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Byte )     {", "writeByte (  (  ( Byte )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Boolean )     {", "writeBool (  (  ( Boolean )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Integer )     {", "writeInt (  (  ( Integer )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Long )     {", "writeLong (  (  ( Long )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Float )     {", "writeFloat (  (  ( Float )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Double )     {", "writeDouble (  (  ( Double )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   String )     {", "writeString (  (  ( String )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   ArrayList )     {", "writeVector (  (  ( ArrayList )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   List )     {", "writeList (  (  ( List )     ( obj )  )  )  ;", "} else", "if    ( obj   instanceof   Map )     {", "writeMap (  (  ( Map )     ( obj )  )  )  ;", "} else    {", "throw   new   RuntimeException (  \" cannot   write   objects   of   this   type \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . BOOL . code )  ;", "out . writeBoolean ( b )  ;", "}", "METHOD_END"], "methodName": ["writeBool"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . BYTE . code )  ;", "out . write ( b )  ;", "}", "METHOD_END"], "methodName": ["writeByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "writeBytes ( bytes ,    Type . BYTES . code )  ;", "}", "METHOD_END"], "methodName": ["writeBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "writeBytes ( bytes ,    code ,    bytes . length )  ;", "}", "METHOD_END"], "methodName": ["writeBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( code )  ;", "out . writeInt ( length )  ;", "out . write (  ,     0  ,    length )  ;", "}", "METHOD_END"], "methodName": ["writeBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "writeBytes ( buffer . get (  )  ,    Type . BYTES . code ,    buffer . getCount (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . DOUBLE . code )  ;", "out . writeDouble ( d )  ;", "}", "METHOD_END"], "methodName": ["writeDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . FLOAT . code )  ;", "out . writeFloat ( f )  ;", "}", "METHOD_END"], "methodName": ["writeFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . INT . code )  ;", "out . writeInt ( i )  ;", "}", "METHOD_END"], "methodName": ["writeInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "writeListHeader (  )  ;", "for    ( Object   obj    :    list )     {", "write ( obj )  ;", "}", "writeListFooter (  )  ;", "}", "METHOD_END"], "methodName": ["writeList"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . MARKER . code )  ;", "}", "METHOD_END"], "methodName": ["writeListFooter"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . LIST . code )  ;", "}", "METHOD_END"], "methodName": ["writeListHeader"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . LONG . code )  ;", "out . writeLong ( l )  ;", "}", "METHOD_END"], "methodName": ["writeLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "writeMapHeader ( map . size (  )  )  ;", "Set < Map . Entry >    entries    =    map . entrySet (  )  ;", "for    ( Map . Entry   entry    :    entries )     {", "write ( entry . getKey (  )  )  ;", "write ( entry . getValue (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["writeMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . MAP . code )  ;", "out . writeInt ( length )  ;", "}", "METHOD_END"], "methodName": ["writeMapHeader"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( bytes )  ;", "}", "METHOD_END"], "methodName": ["writeRaw"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( bytes ,    offset ,    length )  ;", "}", "METHOD_END"], "methodName": ["writeRaw"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . STRING . code )  ;", "WritableUtils . writeString ( out ,    s )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "writeVectorHeader ( vector . size (  )  )  ;", "for    ( Object   obj    :    vector )     {", "write ( obj )  ;", "}", "}", "METHOD_END"], "methodName": ["writeVector"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "out . write ( Type . VECTOR . code )  ;", "out . writeInt ( length )  ;", "}", "METHOD_END"], "methodName": ["writeVectorHeader"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesOutput"}, {"methodBody": ["METHOD_START", "{", "return   TypedBytesRecordInput . get ( TypedBytesInput . get ( in )  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "TypedBytesRecordInput   bin    =     (  ( TypedBytesRecordInput )     ( TypedBytesRecordInput . tbIn . get (  )  )  )  ;", "bin . setTypedBytesInput ( in )  ;", "return   bin ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . readBool (  )  ;", "}", "METHOD_END"], "methodName": ["readBool"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   new   Buffer ( in . rea (  )  )  ;", "}", "METHOD_END"], "methodName": ["readBuffer"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . rea (  )  ;", "}", "METHOD_END"], "methodName": ["readByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . readDouble (  )  ;", "}", "METHOD_END"], "methodName": ["readDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . readFloat (  )  ;", "}", "METHOD_END"], "methodName": ["readFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . readInt (  )  ;", "}", "METHOD_END"], "methodName": ["readInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . readLong (  )  ;", "}", "METHOD_END"], "methodName": ["readLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   in . readString (  )  ;", "}", "METHOD_END"], "methodName": ["readString"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "this . in    =    in ;", "}", "METHOD_END"], "methodName": ["setTypedBytesInput"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   new    . TypedBytesIndex ( in . readMapHeader (  )  )  ;", "}", "METHOD_END"], "methodName": ["startMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "}", "METHOD_END"], "methodName": ["startRecord"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "in . skipType (  )  ;", "return   new    . TypedBytesIndex ( in . readVectorHeader (  )  )  ;", "}", "METHOD_END"], "methodName": ["startVector"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordInput"}, {"methodBody": ["METHOD_START", "{", "out . writeListFooter (  )  ;", "}", "METHOD_END"], "methodName": ["endRecord"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "return   TypedBytesRecordOutput . get ( TypedBytesOutput . get ( out )  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "TypedBytesRecordOutput   bout    =     (  ( TypedBytesRecordOutput )     ( TypedBytesRecordOutput . tbOut . get (  )  )  )  ;", "bout . setTypedBytesOutput ( out )  ;", "return   bout ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "this . out    =    out ;", "}", "METHOD_END"], "methodName": ["setTypedBytesOutput"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeMapHeader ( m . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["startMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeListHeader (  )  ;", "}", "METHOD_END"], "methodName": ["startRecord"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeVectorHeader ( v . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["startVector"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeBool ( b )  ;", "}", "METHOD_END"], "methodName": ["writeBool"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeBytes ( buf . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeBuffer"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeByte ( b )  ;", "}", "METHOD_END"], "methodName": ["writeByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeDouble ( d )  ;", "}", "METHOD_END"], "methodName": ["writeDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeFloat ( f )  ;", "}", "METHOD_END"], "methodName": ["writeFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeInt ( i )  ;", "}", "METHOD_END"], "methodName": ["writeInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeLong ( l )  ;", "}", "METHOD_END"], "methodName": ["writeLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeString ( s )  ;", "}", "METHOD_END"], "methodName": ["writeString"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesRecordOutput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]     =    getBytes (  )  ;", "if    (  (  =  =    null )     |  |     (  ( length )     =  =     0  )  )     {", "return   null ;", "}", "for    ( Type   type    :    Type . values (  )  )     {", "if    (  ( type . code )     =  =     (  ( int )     (  0  ]  )  )  )     {", "return   type ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritable"}, {"methodBody": ["METHOD_START", "{", "try    {", "ByteArrayInputStream   bais    =    new   ByteArrayInputStream ( getBytes (  )  )  ;", "Input   tbi    =    Input . get ( new   DataInputStream ( bais )  )  ;", "Object   obj    =    tbi . read (  )  ;", "return   obj ;", "}    catch    ( IOException   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getValue"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritable"}, {"methodBody": ["METHOD_START", "{", "try    {", "ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "Output   tbo    =    Output . get ( new   DataOutputStream ( baos )  )  ;", "tbo . write ( obj )  ;", "byte [  ]    bytes    =    baos . toByteArray (  )  ;", "set ( bytes ,     0  ,    bytes . length )  ;", "}    catch    ( IOException   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["setValue"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritable"}, {"methodBody": ["METHOD_START", "{", "return   getValue (  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritable"}, {"methodBody": ["METHOD_START", "{", "return   TypedBytesWritableInput . get ( TypedBytesInput . get ( in )  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "TypedBytesWritableInput   bin    =     (  ( TypedBytesWritableInput )     ( TypedBytesWritableInput . tbIn . get (  )  )  )  ;", "bin . setTypedBytesInput ( in )  ;", "return   bin ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "Type   type    =    in . readType (  )  ;", "if    ( type    =  =    null )     {", "return   null ;", "}", "switch    ( type )     {", "case   BYTES    :", "return   readBytes (  )  ;", "case   BYTE    :", "return   readByte (  )  ;", "case   BOOL    :", "return   readBoolean (  )  ;", "case   INT    :", "return   readVInt (  )  ;", "case   LONG    :", "return   readVLong (  )  ;", "case   FLOAT    :", "return   readFloat (  )  ;", "case   DOUBLE    :", "return   readDouble (  )  ;", "case   STRING    :", "return   readText (  )  ;", "case   VECTOR    :", "return   readArray (  )  ;", "case   MAP    :", "return   readMap (  )  ;", "case   WRITABLE    :", "return   read (  )  ;", "default    :", "throw   new   RuntimeException (  \" unknown   type \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readArray ( null )  ;", "}", "METHOD_END"], "methodName": ["readArray"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( aw    =  =    null )     {", "aw    =    new   ArrayWritable (  . class )  ;", "} else", "if    (  !  ( aw . getValueClass (  )  . equals (  . class )  )  )     {", "throw   new   RuntimeException (  \" value   class   has   to   be    \"  )  ;", "}", "int   length    =    in . readVectorHeader (  )  ;", "Writable [  ]    writables    =    new   Writable [ length ]  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "writables [ i ]     =    new    ( in . readRaw (  )  )  ;", "}", "aw . set ( writables )  ;", "return   aw ;", "}", "METHOD_END"], "methodName": ["readArray"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readBoolean ( null )  ;", "}", "METHOD_END"], "methodName": ["readBoolean"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( bw    =  =    null )     {", "bw    =    new   Boolean (  )  ;", "}", "bw . set ( in . readBool (  )  )  ;", "return   bw ;", "}", "METHOD_END"], "methodName": ["readBoolean"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readByte ( null )  ;", "}", "METHOD_END"], "methodName": ["readByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( bw    =  =    null )     {", "bw    =    new   Byte (  )  ;", "}", "bw . set ( in . readByte (  )  )  ;", "return   bw ;", "}", "METHOD_END"], "methodName": ["readByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readBytes ( null )  ;", "}", "METHOD_END"], "methodName": ["readBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    in . readBytes (  )  ;", "if    ( bw    =  =    null )     {", "bw    =    new    ( bytes )  ;", "} else    {", "bw . set ( bytes ,     0  ,    bytes . length )  ;", "}", "return   bw ;", "}", "METHOD_END"], "methodName": ["readBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readDouble ( null )  ;", "}", "METHOD_END"], "methodName": ["readDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( dw    =  =    null )     {", "dw    =    new   Double (  )  ;", "}", "dw . set ( in . readDouble (  )  )  ;", "return   dw ;", "}", "METHOD_END"], "methodName": ["readDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readFloat ( null )  ;", "}", "METHOD_END"], "methodName": ["readFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( fw    =  =    null )     {", "fw    =    new   Float (  )  ;", "}", "fw . set ( in . readFloat (  )  )  ;", "return   fw ;", "}", "METHOD_END"], "methodName": ["readFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readInt ( null )  ;", "}", "METHOD_END"], "methodName": ["readInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( iw    =  =    null )     {", "iw    =    new   Int (  )  ;", "}", "iw . set ( in . readInt (  )  )  ;", "return   iw ;", "}", "METHOD_END"], "methodName": ["readInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readLong ( null )  ;", "}", "METHOD_END"], "methodName": ["readLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( lw    =  =    null )     {", "lw    =    new   Long (  )  ;", "}", "lw . set ( in . readLong (  )  )  ;", "return   lw ;", "}", "METHOD_END"], "methodName": ["readLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readMap ( null )  ;", "}", "METHOD_END"], "methodName": ["readMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( mw    =  =    null )     {", "mw    =    new   Map (  )  ;", "}", "int   length    =    in . readMapHeader (  )  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "key    =    read (  )  ;", "value    =    read (  )  ;", "mw . put ( key ,    value )  ;", "}", "return   mw ;", "}", "METHOD_END"], "methodName": ["readMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readSortedMap ( null )  ;", "}", "METHOD_END"], "methodName": ["readSortedMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( mw    =  =    null )     {", "mw    =    new   SortedMap (  )  ;", "}", "int   length    =    in . readMapHeader (  )  ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "Comparable   key    =     (  ( Comparable )     ( read (  )  )  )  ;", "value    =    read (  )  ;", "mw . put ( key ,    value )  ;", "}", "return   mw ;", "}", "METHOD_END"], "methodName": ["readSortedMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readText ( null )  ;", "}", "METHOD_END"], "methodName": ["readText"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( t    =  =    null )     {", "t    =    new   Text (  )  ;", "}", "t . set ( in . reString (  )  )  ;", "return   t ;", "}", "METHOD_END"], "methodName": ["readText"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "Type   type    =    in . readType (  )  ;", "if    ( type    =  =    null )     {", "return   null ;", "}", "switch    ( type )     {", "case   BYTES    :", "return    . class ;", "case   BYTE    :", "return   ByteWritable . class ;", "case   BOOL    :", "return   BooleanWritable . class ;", "case   INT    :", "return   VIntWritable . class ;", "case   LONG    :", "return   VLongWritable . class ;", "case   FLOAT    :", "return   FloatWritable . class ;", "case   DOUBLE    :", "return   DoubleWritable . class ;", "case   STRING    :", "return   Text . class ;", "case   VECTOR    :", "return   ArrayWritable . class ;", "case   MAP    :", "return   MapWritable . class ;", "case   WRITABLE    :", "return   Writable . class ;", "default    :", "throw   new   RuntimeException (  \" unknown   type \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["readType"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readVInt ( null )  ;", "}", "METHOD_END"], "methodName": ["readVInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( iw    =  =    null )     {", "iw    =    new   VInt (  )  ;", "}", "iw . set ( in . readInt (  )  )  ;", "return   iw ;", "}", "METHOD_END"], "methodName": ["readVInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readVLong ( null )  ;", "}", "METHOD_END"], "methodName": ["readVLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "if    ( lw    =  =    null )     {", "lw    =    new   VLong (  )  ;", "}", "lw . set ( in . readLong (  )  )  ;", "return   lw ;", "}", "METHOD_END"], "methodName": ["readVLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   readWritable ( null )  ;", "}", "METHOD_END"], "methodName": ["readWritable"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "ByteArrayInputStream   bais    =    new   ByteArrayInputStream ( in . readBytes (  )  )  ;", "DataInputStream   dis    =    new   DataInputStream ( bais )  ;", "String   className    =    Utils . readString ( dis )  ;", "if    ( writable    =  =    null )     {", "try    {", "Class <  ?    extends    >    cls    =    conf . getClassByName ( className )  . asSubclass (  . class )  ;", "writable    =     (  (  )     ( ReflectionUtils . newInstance ( cls ,    conf )  )  )  ;", "}    catch    ( ClassNotFoundException   e )     {", "throw   new   IOException ( e )  ;", "}", "} else", "if    (  !  ( writable . getClass (  )  . getName (  )  . equals ( className )  )  )     {", "throw   new   IOException (  \" wrong      class   given \"  )  ;", "}", "writable . readFields ( dis )  ;", "return   writable ;", "}", "METHOD_END"], "methodName": ["readWritable"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    conf ;", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "this . in    =    in ;", "}", "METHOD_END"], "methodName": ["setTypedBytesInput"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableInput"}, {"methodBody": ["METHOD_START", "{", "return   TypedBytesWritableOutput . get ( TypedBytesOutput . get ( out )  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "TypedBytesWritableOutput   bout    =     (  ( TypedBytesWritableOutput )     ( TypedBytesWritableOutput . tbOut . get (  )  )  )  ;", "bout . setTypedBytesOutput ( out )  ;", "return   bout ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "this . out    =    out ;", "}", "METHOD_END"], "methodName": ["setTypedBytesOutput"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "if    ( w   instanceof   TypedBytesWritable )     {", "writeTypedBytes (  (  ( TypedBytesWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   BytesWritable )     {", "writeBytes (  (  ( BytesWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   ByteWritable )     {", "writeByte (  (  ( ByteWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   BooleanWritable )     {", "writeBoolean (  (  ( BooleanWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   IntWritable )     {", "writeInt (  (  ( IntWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   VIntWritable )     {", "writeVInt (  (  ( VIntWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   LongWritable )     {", "writeLong (  (  ( LongWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   VLongWritable )     {", "writeVLong (  (  ( VLongWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   FloatWritable )     {", "writeFloat (  (  ( FloatWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   DoubleWritable )     {", "writeDouble (  (  ( DoubleWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   Text )     {", "writeText (  (  ( Text )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   ArrayWritable )     {", "writeArray (  (  ( ArrayWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   MapWritable )     {", "writeMap (  (  ( MapWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   SortedMapWritable )     {", "writeSortedMap (  (  ( SortedMapWritable )     ( w )  )  )  ;", "} else", "if    ( w   instanceof   Record )     {", "writeRecord (  (  ( Record )     ( w )  )  )  ;", "} else    {", "writeWritable ( w )  ;", "}", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "Writable [  ]    writables    =    aw . get (  )  ;", "out . writeVectorHeader ( writables . length )  ;", "for    ( Writable   writable    :    writables )     {", "write ( writable )  ;", "}", "}", "METHOD_END"], "methodName": ["writeArray"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeBool ( bw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeBoolean"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeByte ( bw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeByte"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    Arrays . copyOfRange ( bw . getBytes (  )  ,     0  ,    bw . getLength (  )  )  ;", "out . writeBytes ( bytes )  ;", "}", "METHOD_END"], "methodName": ["writeBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeDouble ( dw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeDouble"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeFloat ( fw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeFloat"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeInt ( iw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeLong ( lw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeMapHeader ( mw . size (  )  )  ;", "for    ( Map . Entry <  ,     >    entry    :    mw . entrySet (  )  )     {", "write ( entry . getKey (  )  )  ;", "write ( entry . getValue (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["writeMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "r . serialize ( TypedBytesRecordOutput . get ( out )  )  ;", "}", "METHOD_END"], "methodName": ["writeRecord"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeMapHeader ( smw . size (  )  )  ;", "for    ( Map . Entry < Comparable ,     >    entry    :    smw . entrySet (  )  )     {", "write ( entry . getKey (  )  )  ;", "write ( entry . getValue (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["writeSortedMap"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeString ( t . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeText"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeRaw ( tbw . getBytes (  )  ,     0  ,    tbw . getLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeTypedBytes"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeInt ( viw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeVInt"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "out . writeLong ( vlw . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["writeVLong"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   dos    =    new   DataOutputStream ( baos )  ;", "Utils . writeString ( dos ,    w . getClass (  )  . getName (  )  )  ;", "w . write ( dos )  ;", "dos . close (  )  ;", "out . writeBytes ( baos . toByteArray (  )  ,    Type . WRITABLE . code )  ;", "}", "METHOD_END"], "methodName": ["writeWritable"], "fileName": "org.apache.hadoop.typedbytes.TypedBytesWritableOutput"}, {"methodBody": ["METHOD_START", "{", "Map   json    =    new   LinkedHashMap (  )  ;", "long   jobStart    =     (  ( Long )     ( rumenJob . get (  \" submitTime \"  )  )  )  ;", "long   jobFinish    =     (  ( Long )     ( rumenJob . get (  \" finishTime \"  )  )  )  ;", "String   jobId    =    rumenJob . get (  \" jobID \"  )  . toString (  )  ;", "String   queue    =    rumenJob . get (  \" queue \"  )  . toString (  )  ;", "String   user    =    rumenJob . get (  \" user \"  )  . toString (  )  ;", "if    (  (  . baseline )     =  =     0  )     {", ". baseline    =    jobStart ;", "}", "jobStart    -  =     . baseline ;", "jobFinish    -  =     . baseline ;", "long   offset    =     0  ;", "if    ( jobStart    <     0  )     {", "System . out . println (  (  (  \" Warning :    reset   job    \"     +    jobId )     +     \"    start   time   to    0  .  \"  )  )  ;", "offset    =     - jobStart ;", "jobFinish    =    jobFinish    -    jobStart ;", "jobStart    =     0  ;", "}", "json . put (  \" am . type \"  ,     \" mapreduce \"  )  ;", "json . put (  \" job . start . ms \"  ,    jobStart )  ;", "json . put (  \" job . end . ms \"  ,    jobFinish )  ;", "json . put (  \" job . queue . name \"  ,    queue )  ;", "json . put (  \" job . id \"  ,    jobId )  ;", "json . put (  \" job . user \"  ,    user )  ;", "List   maps    =     . createSLSTasks (  \" map \"  ,     (  ( List )     ( rumenJob . get (  \" mapTasks \"  )  )  )  ,    offset )  ;", "List   reduces    =     . createSLSTasks (  \" reduce \"  ,     (  ( List )     ( rumenJob . get (  \" reduceTasks \"  )  )  )  ,    offset )  ;", "List   tasks    =    new   ArrayList (  )  ;", "tasks . addAll ( maps )  ;", "tasks . addAll ( reduces )  ;", "json . put (  \" job . tasks \"  ,    tasks )  ;", "return   json ;", "}", "METHOD_END"], "methodName": ["createSLSJob"], "fileName": "org.apache.hadoop.yarn.sls.RumenToSLSConverter"}, {"methodBody": ["METHOD_START", "{", "int   priority    =     ( taskType . equals (  \" reduce \"  )  )     ?     1  0     :     2  0  ;", "List   array    =    new   ArrayList (  )  ;", "for    ( Object   e    :    rumenTasks )     {", "Map   rumenTask    =     (  ( Map )     ( e )  )  ;", "for    ( Object   ee    :     (  ( List )     ( rumenTask . get (  \" attempts \"  )  )  )  )     {", "Map   rumenAttempt    =     (  ( Map )     ( ee )  )  ;", "long   taskStart    =     (  ( Long )     ( rumenAttempt . get (  \" startTime \"  )  )  )  ;", "long   taskFinish    =     (  ( Long )     ( rumenAttempt . get (  \" finishTime \"  )  )  )  ;", "String   hostname    =     (  ( String )     ( rumenAttempt . get (  \" hostName \"  )  )  )  ;", "taskStart    =     ( taskStart    -     (  . baseline )  )     +    offset ;", "taskFinish    =     ( taskFinish    -     (  . baseline )  )     +    offset ;", "Map   task    =    new   LinkedHashMap (  )  ;", "task . put (  \" container . host \"  ,    hostname )  ;", "task . put (  \" container . start . ms \"  ,    taskStart )  ;", "task . put (  \" container . end . ms \"  ,    taskFinish )  ;", "task . put (  \" container . priority \"  ,    priority )  ;", "task . put (  \" container . type \"  ,    taskType )  ;", "array . add ( task )  ;", "String [  ]    rackHost    =    SLSUtils . getRackHostName ( hostname )  ;", "if    (  . rackNodeMap . containsKey ( rackHost [  0  ]  )  )     {", ". rackNodeMap . get ( rackHost [  0  ]  )  . add ( rackHost [  1  ]  )  ;", "} else    {", "Set < String >    hosts    =    new   TreeSet < String >  (  )  ;", "hosts . add ( rackHost [  1  ]  )  ;", ". rackNodeMap . put ( rackHost [  0  ]  ,    hosts )  ;", "}", "}", "}", "return   array ;", "}", "METHOD_END"], "methodName": ["createSLSTasks"], "fileName": "org.apache.hadoop.yarn.sls.RumenToSLSConverter"}, {"methodBody": ["METHOD_START", "{", "Reader   input    =    new   FileReader ( inputFile )  ;", "try    {", "Writer   output    =    new   FileWriter ( outputFile )  ;", "try    {", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "ObjectWriter   writer    =    mapper . writerWithDefaultPrettyPrinter (  )  ;", "Iterator < Map >    i    =    mapper . readValues ( new   JsonFactory (  )  . createJsonParser ( input )  ,    Map . class )  ;", "while    ( i . hasNext (  )  )     {", "Map   m    =    i . next (  )  ;", "output . write (  (  ( writer . writeValueAsString (  . createSLSJob ( m )  )  )     +     (  . EOL )  )  )  ;", "}", "}    finally    {", "output . close (  )  ;", "}", "}    finally    {", "input . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["generateSLSLoadFile"], "fileName": "org.apache.hadoop.yarn.sls.RumenToSLSConverter"}, {"methodBody": ["METHOD_START", "{", "Writer   output    =    new   FileWriter ( outputFile )  ;", "try    {", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "ObjectWriter   writer    =    mapper . writerWithDefaultPrettyPrinter (  )  ;", "for    ( Map . Entry < String ,    Set < String >  >    entry    :     . rackNodeMap . entrySet (  )  )     {", "Map   rack    =    new   LinkedHashMap (  )  ;", "rack . put (  \" rack \"  ,    entry . getKey (  )  )  ;", "List   nodes    =    new   ArrayList (  )  ;", "for    ( String   name    :    entry . getValue (  )  )     {", "Map   node    =    new   LinkedHashMap (  )  ;", "node . put (  \" node \"  ,    name )  ;", "nodes . add ( node )  ;", "}", "rack . put (  \" nodes \"  ,    nodes )  ;", "output . write (  (  ( writer . writeValueAsString ( rack )  )     +     (  . EOL )  )  )  ;", "}", "}    finally    {", "output . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["generateSLSNodeFile"], "fileName": "org.apache.hadoop.yarn.sls.RumenToSLSConverter"}, {"methodBody": ["METHOD_START", "{", "Options   options    =    new   Options (  )  ;", "options . addOption (  \" input \"  ,    true ,     \" input   rumen   json   file \"  )  ;", "options . addOption (  \" outputJobs \"  ,    true ,     \" output   jobs   file \"  )  ;", "options . addOption (  \" outputNodes \"  ,    true ,     \" output   nodes   file \"  )  ;", "CommandLineParser   parser    =    new   GnuParser (  )  ;", "CommandLine   cmd    =    parser . parse ( options ,    args )  ;", "if    (  (  (  !  ( cmd . hasOption (  \" input \"  )  )  )     |  |     (  !  ( cmd . hasOption (  \" outputJobs \"  )  )  )  )     |  |     (  !  ( cmd . hasOption (  \" outputNodes \"  )  )  )  )     {", "System . err . println (  )  ;", "System . err . println (  \" ERROR :    Missing   input   or   output   file \"  )  ;", "System . err . println (  )  ;", "System . err . println (  (  \" LoadGenerator   creates   a   SLS   script    \"     +     \" from   a   Hadoop   Rumen   output \"  )  )  ;", "System . err . println (  )  ;", "System . err . println (  (  \" Options :     - input   FILE    - outputJobs   FILE    \"     +     \"  - outputNodes   FILE \"  )  )  ;", "System . err . println (  )  ;", "System . exit (  1  )  ;", "}", "String   inputFile    =    cmd . getOptionValue (  \" input \"  )  ;", "String   outputJsonFile    =    cmd . getOptionValue (  \" outputJobs \"  )  ;", "String   outputNodeFile    =    cmd . getOptionValue (  \" outputNodes \"  )  ;", "if    (  !  ( new   File ( inputFile )  . exists (  )  )  )     {", "System . err . println (  )  ;", "System . err . println (  \" ERROR :    input   does   not   exist \"  )  ;", "System . exit (  1  )  ;", "}", "if    ( new   File ( outputJsonFile )  . exists (  )  )     {", "System . err . println (  )  ;", "System . err . println (  \" ERROR :    output   job   file   is   existing \"  )  ;", "System . exit (  1  )  ;", "}", "if    ( new   File ( outputNodeFile )  . exists (  )  )     {", "System . err . println (  )  ;", "System . err . println (  \" ERROR :    output   node   file   is   existing \"  )  ;", "System . exit (  1  )  ;", "}", "File   jsonFile    =    new   File ( outputJsonFile )  ;", "if    (  (  !  ( jsonFile . getParentFile (  )  . exists (  )  )  )     &  &     (  !  ( jsonFile . getParentFile (  )  . mkdirs (  )  )  )  )     {", "System . err . println (  (  \" ERROR :    Cannot   create   output   directory   in   path :     \"     +     ( jsonFile . getParentFile (  )  . getAbsoluteFile (  )  )  )  )  ;", "System . exit (  1  )  ;", "}", "File   nodeFile    =    new   File ( outputNodeFile )  ;", "if    (  (  !  ( nodeFile . getParentFile (  )  . exists (  )  )  )     &  &     (  !  ( nodeFile . getParentFile (  )  . mkdirs (  )  )  )  )     {", "System . err . println (  (  \" ERROR :    Cannot   create   output   directory   in   path :     \"     +     ( jsonFile . getParentFile (  )  . getAbsoluteFile (  )  )  )  )  ;", "System . exit (  1  )  ;", "}", ". generateSLSLoadFile ( inputFile ,    outputJsonFile )  ;", ". generateSLSNodeFile ( outputNodeFile )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.sls.RumenToSLSConverter"}, {"methodBody": ["METHOD_START", "{", "( SLSRunner . remainingApps )  -  -  ;", "if    (  ( SLSRunner . remainingApps )     =  =     0  )     {", "SLSRunner . LOG . info (  \" SLSRunner   tears   down .  \"  )  ;", "System . exit (  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["decreaseRemainingApps"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "return   nmMap ;", "}", "METHOD_END"], "methodName": ["getNmMap"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "return   SLSRunner . runner ;", "}", "METHOD_END"], "methodName": ["getRunner"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "Options   options    =    new   Options (  )  ;", "options . addOption (  \" inputrumen \"  ,    true ,     \" input   rumen   files \"  )  ;", "options . addOption (  \" inputsls \"  ,    true ,     \" input   sls   files \"  )  ;", "options . addOption (  \" nodes \"  ,    true ,     \" input   topology \"  )  ;", "options . addOption (  \" output \"  ,    true ,     \" output   directory \"  )  ;", "options . addOption (  \" trackjobs \"  ,    true ,     \" jobs   to   be   tracked   during   simulating \"  )  ;", "options . addOption (  \" printsimulation \"  ,    false ,     \" print   out   simulation   information \"  )  ;", "CommandLineParser   parser    =    new   GnuParser (  )  ;", "CommandLine   cmd    =    parser . parse ( options ,    args )  ;", "String   inputRumen    =    cmd . getOptionValue (  \" inputrumen \"  )  ;", "String   inputSLS    =    cmd . getOptionValue (  \" inputsls \"  )  ;", "String   output    =    cmd . getOptionValue (  \" output \"  )  ;", "if    (  (  ( inputRumen    =  =    null )     &  &     ( inputSLS    =  =    null )  )     |  |     ( output    =  =    null )  )     {", "System . err . println (  )  ;", "System . err . println (  \" ERROR :    Missing   input   or   output   file \"  )  ;", "System . err . println (  )  ;", "System . err . println (  (  \" Options :     - inputrumen |  - inputsls   FILE , FILE .  .  .     \"     +     (  \"  - output   FILE    [  - nodes   FILE ]     [  - trackjobs   JobId , JobId .  .  .  ]     \"     +     \"  [  - printsimulation ]  \"  )  )  )  ;", "System . err . println (  )  ;", "System . exit (  1  )  ;", "}", "File   outputFile    =    new   File ( output )  ;", "if    (  (  !  ( outputFile . exists (  )  )  )     &  &     (  !  ( outputFile . mkdirs (  )  )  )  )     {", "System . err . println (  (  \" ERROR :    Cannot   create   output   directory    \"     +     ( outputFile . getAbsolutePath (  )  )  )  )  ;", "System . exit (  1  )  ;", "}", "Set < String >    trackedJobSet    =    new   HashSet < String >  (  )  ;", "if    ( cmd . hasOption (  \" trackjobs \"  )  )     {", "String   trackjobs    =    cmd . getOptionValue (  \" trackjobs \"  )  ;", "String [  ]    jobIds    =    trackjobs . split (  \"  ,  \"  )  ;", "trackedJobSet . addAll ( Arrays . asList ( jobIds )  )  ;", "}", "String   nodeFile    =     ( cmd . hasOption (  \" nodes \"  )  )     ?    cmd . getOptionValue (  \" nodes \"  )     :     \"  \"  ;", "boolean   isSLS    =    inputSLS    !  =    null ;", "String [  ]    inputFiles    =     ( isSLS )     ?    inputSLS . split (  \"  ,  \"  )     :    inputRumen . split (  \"  ,  \"  )  ;", "sls    =    new    ( isSLS ,    inputFiles ,    nodeFile ,    output ,    trackedJobSet ,    cmd . hasOption (  \" printsimulation \"  )  )  ;", "sls . start (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "if    ( printSimulation )     {", ". LOG . info (  \"  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \"  )  ;", ". LOG . info ( MessageFormat . format (  (  \"  #    nodes    =     {  0  }  ,     #    racks    =     {  1  }  ,    capacity    \"     +     \" of   each   node    {  2  }    MB   memory   and    {  3  }    vcores .  \"  )  ,    numNMs ,    numRacks ,    nmMemoryMB ,    nmVCores )  )  ;", ". LOG . info (  \"  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \"  )  ;", ". LOG . info ( MessageFormat . format (  (  \"  #    applications    =     {  0  }  ,     #    total    \"     +     \" tasks    =     {  1  }  ,    average    #    tasks   per   application    =     {  2  }  \"  )  ,    numAMs ,    numTasks ,     (  ( int )     ( Math . ceil (  (  (  ( numTasks )     +     0  .  0  )     /     ( numAMs )  )  )  )  )  )  )  ;", ". LOG . info (  \" JobId \\ tQueue \\ tAMType \\ tDuration \\ t # Tasks \"  )  ;", "for    ( Map . Entry < String ,    AMSimulator >    entry    :    amMap . entrySet (  )  )     {", "AMSimulator   am    =    entry . getValue (  )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  ( entry . getKey (  )  )     +     \"  \\ t \"  )     +     ( am . getQueue (  )  )  )     +     \"  \\ t \"  )     +     ( am . getAMType (  )  )  )     +     \"  \\ t \"  )     +     ( am . getDuration (  )  )  )     +     \"  \\ t \"  )     +     ( am . getNumTasks (  )  )  )  )  ;", "}", ". LOG . info (  \"  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \"  )  ;", ". LOG . info ( MessageFormat . format (  (  \" number   of   queues    =     {  0  }       average    \"     +     \" number   of   apps    =     {  1  }  \"  )  ,    queueAppNumMap . size (  )  ,     (  ( int )     ( Math . ceil (  (  (  ( numAMs )     +     0  .  0  )     /     ( queueAppNumMap . size (  )  )  )  )  )  )  )  )  ;", ". LOG . info (  \"  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \"  )  ;", ". LOG . info ( MessageFormat . format (  (  \" estimated   simulation   time   is    {  0  }  \"     +     \"    seconds \"  )  ,     (  ( long )     ( Math . ceil (  (  ( maxRuntime )     /     1  0  0  0  .  0  )  )  )  )  )  )  ;", ". LOG . info (  \"  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  -  \"  )  ;", "}", ". simulateInfoMap . put (  \" Number   of   racks \"  ,    numRacks )  ;", ". simulateInfoMap . put (  \" Number   of   nodes \"  ,    numNMs )  ;", ". simulateInfoMap . put (  \" Node   memory    ( MB )  \"  ,    nmMemoryMB )  ;", ". simulateInfoMap . put (  \" Node   VCores \"  ,    nmVCores )  ;", ". simulateInfoMap . put (  \" Number   of   applications \"  ,    numAMs )  ;", ". simulateInfoMap . put (  \" Number   of   tasks \"  ,    numTasks )  ;", ". simulateInfoMap . put (  \" Average   tasks   per   applicaion \"  ,     (  ( int )     ( Math . ceil (  (  (  ( numTasks )     +     0  .  0  )     /     ( numAMs )  )  )  )  )  )  ;", ". simulateInfoMap . put (  \" Number   of   queues \"  ,    queueAppNumMap . size (  )  )  ;", ". simulateInfoMap . put (  \" Average   applications   per   queue \"  ,     (  ( int )     ( Math . ceil (  (  (  ( numAMs )     +     0  .  0  )     /     ( queueAppNumMap . size (  )  )  )  )  )  )  )  ;", ". simulateInfoMap . put (  \" Estimated   simulate   time    ( s )  \"  ,     (  ( long )     ( Math . ceil (  (  ( maxRuntime )     /     1  0  0  0  .  0  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["printSimulationInfo"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "startRM (  )  ;", "startNM (  )  ;", "startAM (  )  ;", "(  ( SchedulerWrapper )     ( rm . getResourceScheduler (  )  )  )  . setQueueSet ( this . queueAppNumMap . keySet (  )  )  ;", "(  ( SchedulerWrapper )     ( rm . getResourceScheduler (  )  )  )  . setTrackedAppSet ( this . trackedApps )  ;", "printSimulationInfo (  )  ;", "waitForNodesRunning (  )  ;", ". runner . start (  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "int   heartbeatInterval    =    conf . getInt ( SLSConfiguration . AM _ HEARTBEAT _ INTERVAL _ MS ,    SLSConfiguration . AM _ HEARTBEAT _ INTERVAL _ MS _ DEFAULT )  ;", "int   containerMemoryMB    =    conf . getInt ( SLSConfiguration . CONTAINER _ MEMORY _ MB ,    SLSConfiguration . CONTAINER _ MEMORY _ MB _ DEFAULT )  ;", "int   containerVCores    =    conf . getInt ( SLSConfiguration . CONTAINER _ VCORES ,    SLSConfiguration . CONTAINER _ VCORES _ DEFAULT )  ;", "Resource   containerResource    =    BuilderUtils . newResource ( containerMemoryMB ,    containerVCores )  ;", "if    ( isSLS )     {", "startAMFromSLSTraces ( containerResource ,    heartbeatInterval )  ;", "} else    {", "startAMFromRumenTraces ( containerResource ,    heartbeatInterval )  ;", "}", "numAMs    =    amMap . size (  )  ;", ". remainingApps    =    numAMs ;", "}", "METHOD_END"], "methodName": ["startAM"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  \" fs . defaultFS \"  ,     \" file :  /  /  /  \"  )  ;", "long   baselineTimeMS    =     0  ;", "for    ( String   inputTrace    :    inputTraces )     {", "File   fin    =    new   File ( inputTrace )  ;", "JobTraceReader   reader    =    new   JobTraceReader ( new   Path ( fin . getAbsolutePath (  )  )  ,    conf )  ;", "try    {", "LoggedJob   job    =    null ;", "while    (  ( job    =    reader . getNext (  )  )     !  =    null )     {", "String   jobType    =     \" mapreduce \"  ;", "String   user    =     (  ( job . getUser (  )  )     =  =    null )     ?     \" default \"     :    job . getUser (  )  . getValue (  )  ;", "String   jobQueue    =    job . getQueue (  )  . getValue (  )  ;", "String   oldJobId    =    job . getJobID (  )  . toString (  )  ;", "long   jobStartTimeMS    =    job . getSubmitTime (  )  ;", "long   jobFinishTimeMS    =    job . getFinishTime (  )  ;", "if    ( baselineTimeMS    =  =     0  )     {", "baselineTimeMS    =    jobStartTimeMS ;", "}", "jobStartTimeMS    -  =    baselineTimeMS ;", "jobFinishTimeMS    -  =    baselineTimeMS ;", "if    ( jobStartTimeMS    <     0  )     {", ". LOG . warn (  (  (  \" Warning :    reset   job    \"     +    oldJobId )     +     \"    start   time   to    0  .  \"  )  )  ;", "jobFinishTimeMS    =    jobFinishTimeMS    -    jobStartTimeMS ;", "jobStartTimeMS    =     0  ;", "}", "boolean   isTracked    =    trackedApps . contains ( oldJobId )  ;", "int   queueSize    =     ( queueAppNumMap . containsKey ( jobQueue )  )     ?    queueAppNumMap . get ( jobQueue )     :     0  ;", "queueSize +  +  ;", "queueAppNumMap . put ( jobQueue ,    queueSize )  ;", "List < ContainerSimulator >    containerList    =    new   ArrayList < ContainerSimulator >  (  )  ;", "for    ( LoggedTask   mapTask    :    job . getMapTasks (  )  )     {", "LoggedTaskAttempt   taskAttempt    =    mapTask . getAttempts (  )  . get (  (  ( mapTask . getAttempts (  )  . size (  )  )     -     1  )  )  ;", "String   hostname    =    taskAttempt . getHostName (  )  . getValue (  )  ;", "long   containerLifeTime    =     ( taskAttempt . getFinishTime (  )  )     -     ( taskAttempt . getStartTime (  )  )  ;", "containerList . add ( new   ContainerSimulator ( containerResource ,    containerLifeTime ,    hostname ,     1  0  ,     \" map \"  )  )  ;", "}", "for    ( LoggedTask   reduceTask    :    job . getReduceTasks (  )  )     {", "LoggedTaskAttempt   taskAttempt    =    reduceTask . getAttempts (  )  . get (  (  ( reduceTask . getAttempts (  )  . size (  )  )     -     1  )  )  ;", "String   hostname    =    taskAttempt . getHostName (  )  . getValue (  )  ;", "long   containerLifeTime    =     ( taskAttempt . getFinishTime (  )  )     -     ( taskAttempt . getStartTime (  )  )  ;", "containerList . add ( new   ContainerSimulator ( containerResource ,    containerLifeTime ,    hostname ,     2  0  ,     \" reduce \"  )  )  ;", "}", "AMSimulator   amSim    =     (  ( AMSimulator )     ( ReflectionUtils . newInstance ( amClassMap . get ( jobType )  ,    conf )  )  )  ;", "if    ( amSim    !  =    null )     {", "amSim . init (  (  ( AM _ ID )  +  +  )  ,    heartbeatInterval ,    containerList ,    rm ,    this ,    jobStartTimeMS ,    jobFinishTimeMS ,    user ,    jobQueue ,    isTracked ,    oldJobId )  ;", ". runner . schedule ( amSim )  ;", "maxRuntime    =    Math . max ( maxRuntime ,    jobFinishTimeMS )  ;", "numTasks    +  =    containerList . size (  )  ;", "amMap . put ( oldJobId ,    amSim )  ;", "}", "}", "}    finally    {", "reader . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["startAMFromRumenTraces"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "JsonFactory   jsonF    =    new   JsonFactory (  )  ;", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "for    ( String   inputTrace    :    inputTraces )     {", "Reader   input    =    new   FileReader ( inputTrace )  ;", "try    {", "Iterator < Map >    i    =    mapper . readValues ( jsonF . createJsonParser ( input )  ,    Map . class )  ;", "while    ( i . hasNext (  )  )     {", "Map   jsonJob    =    i . next (  )  ;", "long   jobStartTime    =    Long . parseLong ( jsonJob . get (  \" job . start . ms \"  )  . toString (  )  )  ;", "long   jobFinishTime    =    Long . parseLong ( jsonJob . get (  \" job . end . ms \"  )  . toString (  )  )  ;", "String   user    =     (  ( String )     ( jsonJob . get (  \" job . user \"  )  )  )  ;", "if    ( user    =  =    null )", "user    =     \" default \"  ;", "String   queue    =    jsonJob . get (  \" job . queue . name \"  )  . toString (  )  ;", "String   oldAppId    =    jsonJob . get (  \" job . id \"  )  . toString (  )  ;", "boolean   isTracked    =    trackedApps . contains ( oldAppId )  ;", "int   queueSize    =     ( queueAppNumMap . containsKey ( queue )  )     ?    queueAppNumMap . get ( queue )     :     0  ;", "queueSize +  +  ;", "queueAppNumMap . put ( queue ,    queueSize )  ;", "List   tasks    =     (  ( List )     ( jsonJob . get (  \" job . tasks \"  )  )  )  ;", "if    (  ( tasks    =  =    null )     |  |     (  ( tasks . size (  )  )     =  =     0  )  )     {", "continue ;", "}", "List < ContainerSimulator >    containerList    =    new   ArrayList < ContainerSimulator >  (  )  ;", "for    ( Object   o    :    tasks )     {", "Map   jsonTask    =     (  ( Map )     ( o )  )  ;", "String   hostname    =    jsonTask . get (  \" container . host \"  )  . toString (  )  ;", "long   taskStart    =    Long . parseLong ( jsonTask . get (  \" container . start . ms \"  )  . toString (  )  )  ;", "long   taskFinish    =    Long . parseLong ( jsonTask . get (  \" container . end . ms \"  )  . toString (  )  )  ;", "long   lifeTime    =    taskFinish    -    taskStart ;", "int   priority    =    Integer . parseInt ( jsonTask . get (  \" container . priority \"  )  . toString (  )  )  ;", "String   type    =    jsonTask . get (  \" container . type \"  )  . toString (  )  ;", "containerList . add ( new   ContainerSimulator ( containerResource ,    lifeTime ,    hostname ,    priority ,    type )  )  ;", "}", "String   amType    =    jsonJob . get (  \" am . type \"  )  . toString (  )  ;", "AMSimulator   amSim    =     (  ( AMSimulator )     ( ReflectionUtils . newInstance ( amClassMap . get ( amType )  ,    new   Configuration (  )  )  )  )  ;", "if    ( amSim    !  =    null )     {", "amSim . init (  (  ( AM _ ID )  +  +  )  ,    heartbeatInterval ,    containerList ,    rm ,    this ,    jobStartTime ,    jobFinishTime ,    user ,    queue ,    isTracked ,    oldAppId )  ;", ". runner . schedule ( amSim )  ;", "maxRuntime    =    Math . max ( maxRuntime ,    jobFinishTime )  ;", "numTasks    +  =    containerList . size (  )  ;", "amMap . put ( oldAppId ,    amSim )  ;", "}", "}", "}    finally    {", "input . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["startAMFromSLSTraces"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "nmMemoryMB    =    conf . getInt ( SLSConfiguration . NM _ MEMORY _ MB ,    SLSConfiguration . NM _ MEMORY _ MB _ DEFAULT )  ;", "nmVCores    =    conf . getInt ( SLSConfiguration . NM _ VCORES ,    SLSConfiguration . NM _ VCORES _ DEFAULT )  ;", "int   heartbeatInterval    =    conf . getInt ( SLSConfiguration . NM _ HEARTBEAT _ INTERVAL _ MS ,    SLSConfiguration . NM _ HEARTBEAT _ INTERVAL _ MS _ DEFAULT )  ;", "Set < String >    nodeSet    =    new   HashSet < String >  (  )  ;", "if    ( nodeFile . isEmpty (  )  )     {", "if    ( isSLS )     {", "for    ( String   inputTrace    :    inputTraces )     {", "nodeSet . addAll ( SLSUtils . parseNodesFromSLSTrace ( inputTrace )  )  ;", "}", "} else    {", "for    ( String   inputTrace    :    inputTraces )     {", "nodeSet . addAll ( SLSUtils . parseNodesFromRumenTrace ( inputTrace )  )  ;", "}", "}", "} else    {", "nodeSet . addAll ( SLSUtils . parseNodesFromNodeFile ( nodeFile )  )  ;", "}", "Random   random    =    new   Random (  )  ;", "Set < String >    rackSet    =    new   HashSet < String >  (  )  ;", "for    ( String   hostName    :    nodeSet )     {", "NMSimulator   nm    =    new   NMSimulator (  )  ;", "nm . init ( hostName ,    nmMemoryMB ,    nmVCores ,    random . nextInt ( heartbeatInterval )  ,    heartbeatInterval ,    rm )  ;", "nmMap . put ( nm . getNode (  )  . getNodeID (  )  ,    nm )  ;", ". runner . schedule ( nm )  ;", "rackSet . add ( nm . getNode (  )  . getRackName (  )  )  ;", "}", "numRacks    =    rackSet . size (  )  ;", "numNMs    =    nmMap . size (  )  ;", "}", "METHOD_END"], "methodName": ["startNM"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "Configuration   rmConf    =    new   YarnConfiguration (  )  ;", "String   schedulerClass    =    rmConf . get ( RM _ SCHEDULER )  ;", "if    (  ( Class . forName ( schedulerClass )  )     =  =     ( CityScheduler . class )  )     {", "rmConf . set ( RM _ SCHEDULER ,    SLSCityScheduler . class . getName (  )  )  ;", "} else    {", "rmConf . set ( RM _ SCHEDULER ,    ResourceSchedulerWrapper . class . getName (  )  )  ;", "rmConf . set ( SLSConfiguration . RM _ SCHEDULER ,    schedulerClass )  ;", "}", "rmConf . set ( SLSConfiguration . METRICS _ OUTPUT _ DIR ,    metricsOutputDir )  ;", "rm    =    new   ResourceManager (  )  ;", "rm . init ( rmConf )  ;", "rm . start (  )  ;", "}", "METHOD_END"], "methodName": ["startRM"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "long   startTimeMS    =    System . currentTimeMillis (  )  ;", "while    ( true )     {", "int   numRunningNodes    =     0  ;", "for    ( RMNode   node    :    rm . getRMContext (  )  . getRMNodes (  )  . values (  )  )     {", "if    (  ( node . getState (  )  )     =  =     ( NodeState . RUNNING )  )     {", "numRunningNodes +  +  ;", "}", "}", "if    ( numRunningNodes    =  =     ( numNMs )  )     {", "break ;", "}", ". LOG . info ( MessageFormat . format (  (  \"    is   waiting   for   all    \"     +     \" nodes   RUNNING .     {  0  }    of    {  1  }    NMs   initialized .  \"  )  ,    numRunningNodes ,    numNMs )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", ". LOG . info ( MessageFormat . format (  \"    takes    {  0  }    ms   to   launch   all   nodes .  \"  ,     (  ( System . currentTimeMillis (  )  )     -    startTimeMS )  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForNodesRunning"], "fileName": "org.apache.hadoop.yarn.sls.SLSRunner"}, {"methodBody": ["METHOD_START", "{", "File   tempDir    =    new   File (  \" target \"  ,    UUID . randomUUID (  )  . toString (  )  )  ;", "final   List < Throwable >    exceptionList    =    Collections . synchronizedList ( new   ArrayList < Throwable >  (  )  )  ;", "Thread . setDefaultUncaughtExceptionHandler ( new   Thread . UncaughtExceptionHandler (  )     {", "@ Override", "public   void   uncaughtException ( Thread   t ,    Throwable   e )     {", "exceptionList . add ( e )  ;", "}", "}  )  ;", "File   slsOutputDir    =    new   File (  (  ( tempDir . getAbsolutePath (  )  )     +     \"  / slsoutput /  \"  )  )  ;", "String [  ]    args    =    new   String [  ]  {     \"  - inputrumen \"  ,     \" src / main / data /  2 jobs 2 min - rumen - jh . json \"  ,     \"  - output \"  ,    slsOutputDir . getAbsolutePath (  )     }  ;", "SLSRunner . main ( args )  ;", "int   count    =     2  0  ;", "while    ( count    >  =     0  )     {", "Thread . sleep (  1  0  0  0  )  ;", "if    (  !  ( exceptionList . isEmpty (  )  )  )     {", "SLSRunner . getRunner (  )  . stop (  )  ;", "Assert . fail (  (  (  \"    catched   exception   from   child   thread    \"     +     \"  ( TaskRunner . Task )  :     \"  )     +     ( exceptionList . get (  0  )  . getMessage (  )  )  )  )  ;", "break ;", "}", "count -  -  ;", "}", "SLSRunner . getRunner (  )  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSimulatorRunning"], "fileName": "org.apache.hadoop.yarn.sls.TestSLSRunner"}, {"methodBody": ["METHOD_START", "{", "return   createAllocateRequest ( ask ,    new   ArrayList < ContainerId >  (  )  )  ;", "}", "METHOD_END"], "methodName": ["createAllocateRequest"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "AllocateRequest   allocateRequest    =    AMSimulator . recordFactory . newRecordInstance ( AllocateRequest . class )  ;", "allocateRequest . setResponseId (  (  ( RESPONSE _ ID )  +  +  )  )  ;", "allocateRequest . setAskList ( ask )  ;", "allocateRequest . setReleaseList ( toRelease )  ;", "return   allocateRequest ;", "}", "METHOD_END"], "methodName": ["createAllocateRequest"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    AMSimulator . recordFactory . newRecordInstance ( ResourceRequest . class )  ;", "request . setCapability ( resource )  ;", "request . setResourceName ( host )  ;", "request . setNumContainers ( numContainers )  ;", "Priority   prio    =    AMSimulator . recordFactory . newRecordInstance ( Priority . class )  ;", "prio . setPriority ( priority )  ;", "request . setPriority ( prio )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["createResourceRequest"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   amtype ;", "}", "METHOD_END"], "methodName": ["getAMType"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "return    ( simulateFinishTimeMS )     -     ( simulateStartTimeMS )  ;", "}", "METHOD_END"], "methodName": ["getDuration"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   totalContainers ;", "}", "METHOD_END"], "methodName": ["getNumTasks"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "super . init ( traceStartTime ,     ( traceStartTime    +     (  1  0  0  0  0  0  0 L    *    heartbeatInterval )  )  ,    heartbeatInterval )  ;", "this . user    =    user ;", "this . rm    =    rm ;", "this . se    =    se ;", "this . user    =    user ;", "this . queue    =    queue ;", "this . oldAppId    =    oldAppId ;", "this . isTracked    =    isTracked ;", "this . traceStartTimeMS    =    traceStartTime ;", "this . traceFinishTimeMS    =    traceFinishTime ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    ResourceRequest >    rackLocalRequestMap    =    new   HashMap < String ,    ResourceRequest >  (  )  ;", "Map < String ,    ResourceRequest >    nodeLocalRequestMap    =    new   HashMap < String ,    ResourceRequest >  (  )  ;", "ResourceRequest   anyRequest    =    null ;", "for    ( Container   cs    :    csList )     {", "String [  ]    rackHostNames    =    SLSUtils . getRackHostName ( cs . getHostname (  )  )  ;", "String   rackname    =    rackHostNames [  0  ]  ;", "if    ( rackLocalRequestMap . containsKey ( rackname )  )     {", "rackLocalRequestMap . get ( rackname )  . setNumContainers (  (  ( rackLocalRequestMap . get ( rackname )  . getNumContainers (  )  )     +     1  )  )  ;", "} else    {", "ResourceRequest   request    =    createResourceRequest ( cs . getResource (  )  ,    rackname ,    priority ,     1  )  ;", "rackLocalRequestMap . put ( rackname ,    request )  ;", "}", "String   hostname    =    rackHostNames [  1  ]  ;", "if    ( nodeLocalRequestMap . containsKey ( hostname )  )     {", "nodeLocalRequestMap . get ( hostname )  . setNumContainers (  (  ( nodeLocalRequestMap . get ( hostname )  . getNumContainers (  )  )     +     1  )  )  ;", "} else    {", "ResourceRequest   request    =    createResourceRequest ( cs . getResource (  )  ,    hostname ,    priority ,     1  )  ;", "nodeLocalRequestMap . put ( hostname ,    request )  ;", "}", "if    ( anyRequest    =  =    null )     {", "anyRequest    =    createResourceRequest ( cs . getResource (  )  ,    ANY ,    priority ,     1  )  ;", "} else    {", "anyRequest . setNumContainers (  (  ( anyRequest . getNumContainers (  )  )     +     1  )  )  ;", "}", "}", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ask . addAll ( nodeLocalRequestMap . values (  )  )  ;", "ask . addAll ( rackLocalRequestMap . values (  )  )  ;", "if    ( anyRequest    !  =    null )     {", "ask . add ( anyRequest )  ;", "}", "return   ask ;", "}", "METHOD_END"], "methodName": ["packageRequests"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "final   RegisterApplicationMasterRequest   amRegisterRequest    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "amRegisterRequest . setHost (  \" localhost \"  )  ;", "amRegisterRequest . setRpcPort (  1  0  0  0  )  ;", "amRegisterRequest . setTrackingUrl (  \" localhost :  1  0  0  0  \"  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( appAttemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    rm . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getRMAppAttempt ( appAttemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "ugi . doAs ( new   PrivilegedExceptionAction < RegisterApplicationMasterResponse >  (  )     {", "@ Override", "public   RegisterApplicationMasterResponse   run (  )    throws   Exception    {", "return   rm . getApplicationMasterService (  )  . registerApplicationMaster ( amRegisterRequest )  ;", "}", "}  )  ;", "LOG . info ( MessageFormat . format (  \" Register   the   application      for   application    {  0  }  \"  ,    appId )  )  ;", "}", "METHOD_END"], "methodName": ["registerAM"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "GetNewApplicationRequest   newAppRequest    =    Records . newRecord ( GetNewApplicationRequest . class )  ;", "GetNewApplicationResponse   newAppResponse    =    rm . getClientRMService (  )  . getNewApplication ( newAppRequest )  ;", "appId    =    newAppResponse . getApplicationId (  )  ;", "final   SubmitApplicationRequest   subAppRequest    =    Records . newRecord ( SubmitApplicationRequest . class )  ;", "ApplicationSubmissionContext   appSubContext    =    Records . newRecord ( ApplicationSubmissionContext . class )  ;", "appSubContext . setApplicationId ( appId )  ;", "appSubContext . setMaxAppAttempts (  1  )  ;", "appSubContext . setQueue ( queue )  ;", "appSubContext . setPriority ( Priority . newInstance (  0  )  )  ;", "ContainerLaunchContext   conLauContext    =    Records . newRecord ( ContainerLaunchContext . class )  ;", "conLauContext . setApplicationACLs ( new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  )  ;", "conLauContext . setCommands ( new   ArrayList < String >  (  )  )  ;", "conLauContext . setEnvironment ( new   HashMap < String ,    String >  (  )  )  ;", "conLauContext . setLocalResources ( new   HashMap < String ,    api . records . LocalResource >  (  )  )  ;", "conLauContext . setServiceData ( new   HashMap < String ,    ByteBuffer >  (  )  )  ;", "appSubContext . setAMContainerSpec ( conLauContext )  ;", "appSubContext . setUnmanagedAM ( true )  ;", "subAppRequest . setApplicationSubmissionContext ( appSubContext )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( user )  ;", "ugi . doAs ( new   PrivilegedExceptionAction < Object >  (  )     {", "@ Override", "public   Object   run (  )    throws   YarnException    {", "rm . getClientRMService (  )  . submitApplication ( subAppRequest )  ;", "return   null ;", "}", "}  )  ;", "LOG . info ( MessageFormat . format (  \" Submit   a   new   application    {  0  }  \"  ,    appId )  )  ;", "RMApp   app    =    rm . getRMContext (  )  . getRMApps (  )  . get ( appId )  ;", "while    (  ( app . getState (  )  )     !  =     ( RMAppState . ACCEPTED )  )     {", "Thread . sleep (  1  0  )  ;", "}", "this . appAttemptId    =    rm . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ;", "RMAppAttempt   rmAppAttempt    =    rm . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getCurrentAppAttempt (  )  ;", "while    (  ( rmAppAttempt . getAppAttemptState (  )  )     !  =     ( RMAppAttemptState . LAUNCHED )  )     {", "Thread . sleep (  1  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "if    ( isTracked )     {", "(  ( SdulerWrapper )     ( rm . getResourceSduler (  )  )  )  . addTrackedApp ( appAttemptId ,    oldAppId )  ;", "}", "}", "METHOD_END"], "methodName": ["trackApp"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "if    ( isTracked )     {", "(  ( SdulerWrapper )     ( rm . getResourceSduler (  )  )  )  . removeTrackedApp ( appAttemptId ,    oldAppId )  ;", "}", "}", "METHOD_END"], "methodName": ["untrackApp"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.AMSimulator"}, {"methodBody": ["METHOD_START", "{", "super . init ( id ,    heartbeatInterval ,    containerList ,    rm ,    se ,    traceStartTime ,    traceFinishTime ,    user ,    queue ,    isTracked ,    oldAppId )  ;", "amtype    =     \" mapreduce \"  ;", "for    ( ContainerSimulator   cs    :    containerList )     {", "if    ( cs . getType (  )  . equals (  \" map \"  )  )     {", "cs . setPriority (  . PRIORITY _ MAP )  ;", "pendingMaps . add ( cs )  ;", "} else", "if    ( cs . getType (  )  . equals (  \" reduce \"  )  )     {", "cs . setPriority (  . PRIORITY _ REDUCE )  ;", "pendingReduces . add ( cs )  ;", "}", "}", "allMaps . addAll ( pendingMaps )  ;", "allReduces . addAll ( pendingReduces )  ;", "mapTotal    =    pendingMaps . size (  )  ;", "reduceTotal    =    pendingReduces . size (  )  ;", "totalContainers    =     ( mapTotal )     +     ( reduceTotal )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   amRequest    =    createResourceRequest ( BuilderUtils . newResource (  . MR _ AM _ CONTAINER _ RESOURCE _ MEMORY _ MB ,     . MR _ AM _ CONTAINER _ RESOURCE _ VCORES )  ,    ANY ,     1  ,     1  )  ;", "ask . add ( amRequest )  ;", "LOG . debug ( MessageFormat . format (  (  \" Application    {  0  }    sends   out   allocate    \"     +     \" request   for   its   AM \"  )  ,    appId )  )  ;", "final   AllocateRequest   request    =    this . createAllocateRequest ( ask )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( appAttemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    rm . getRMContext (  )  . getRMApps (  )  . get ( appAttemptId . getApplicationId (  )  )  . getRMAppAttempt ( appAttemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "AllocateResponse   response    =    ugi . doAs ( new   PrivilegedExceptionAction < AllocateResponse >  (  )     {", "@ Override", "public   AllocateResponse   run (  )    throws   Exception    {", "return   rm . getApplicationMasterService (  )  . allocate ( request )  ;", "}", "}  )  ;", "while    ( true )     {", "if    (  ( response    !  =    null )     &  &     (  !  ( response . getAllocatedContainers (  )  . isEmpty (  )  )  )  )     {", "Container   container    =    response . getAllocatedContainers (  )  . get (  0  )  ;", "se . getNmMap (  )  . get ( container . getNodeId (  )  )  . addNewContainer ( container ,     (  -  1 L )  )  ;", "amContainer    =    container ;", "LOG . debug ( MessageFormat . format (  (  \" Application    {  0  }    starts   its    \"     +     \" AM   container    (  {  1  }  )  .  \"  )  ,    appId ,    amContainer . getId (  )  )  )  ;", "isAMContainerRunning    =    true ;", "break ;", "}", "Thread . sleep (  1  0  0  0  )  ;", "sendContainerRequest (  )  ;", "response    =    responseQueue . take (  )  ;", "}", "}", "METHOD_END"], "methodName": ["requestAMContainer"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator"}, {"methodBody": ["METHOD_START", "{", "finishedContainers    =     0  ;", "isFinished    =    false ;", "mapFinished    =     0  ;", "reduceFinished    =     0  ;", "pendingFailedMaps . clear (  )  ;", "pendingMaps . clear (  )  ;", "pendingReduces . clear (  )  ;", "pendingFailedReduces . clear (  )  ;", "pendingMapddAll ( allMaps )  ;", "pendingReduceddAll ( pendingReduces )  ;", "isAMContainerRunning    =    false ;", "amContainer    =    null ;", "requestAMContainer (  )  ;", "}", "METHOD_END"], "methodName": ["restart"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.MRAMSimulator"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ SCHEDULER ,     \" scheduler . ResourceSchedulerWrapper \"  )  ;", "conf . set ( SLSConfiguration . RM _ SCHEDULER ,     \" FairScheduler \"  )  ;", "conf . setBoolean ( SLSConfiguration . METRICS _ SWITCH ,    false )  ;", "rm    =    new   ResourceManager (  )  ;", "rm . init ( conf )  ;", "rm . start (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.TestAMSimulator"}, {"methodBody": ["METHOD_START", "{", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.TestAMSimulator"}, {"methodBody": ["METHOD_START", "{", "TestAMSimulator . MockAMSimulator   app    =    new   TestAMSimulator . MockAMSimulator (  )  ;", "List < ContainerSimulator >    containers    =    new   ArrayList < ContainerSimulator >  (  )  ;", "app . init (  1  ,     1  0  0  0  ,    containers ,    rm ,    null ,     0  ,     1  0  0  0  0  0  0 L ,     \" user 1  \"  ,     \" default \"  ,    false ,     \" app 1  \"  )  ;", "app . firstStep (  )  ;", "Assert . assertEquals (  1  ,    rm . getRMContext (  )  . getRMApps (  )  . size (  )  )  ;", "Assert . assertNotNull ( rm . getRMContext (  )  . getRMApps (  )  . get ( app . appId )  )  ;", "app . lastStep (  )  ;", "}", "METHOD_END"], "methodName": ["testAMSimulator"], "fileName": "org.apache.hadoop.yarn.sls.appmaster.TestAMSimulator"}, {"methodBody": ["METHOD_START", "{", "NMSimulator . LOG . debug ( MessageFormat . format (  (  \" NodeManager    {  0  }    launches   a   new    \"     +     \" container    (  {  1  }  )  .  \"  )  ,    node . getNodeID (  )  ,    container . getId (  )  )  )  ;", "if    ( lifeTimeMS    !  =     (  -  1  )  )     {", "ContainerSimulator   cs    =    new   ContainerSimulator ( container . getId (  )  ,    container . getResource (  )  ,     ( lifeTimeMS    +     ( System . currentTimeMillis (  )  )  )  ,    lifeTimeMS )  ;", "containerQueue . add ( cs )  ;", "runningContainers . put ( cs . getId (  )  ,    cs )  ;", "} else    {", "synchronized ( amContainerList )     {", "amContainerList . add ( container . getId (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addNewContainer"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "synchronized ( amContainerList )     {", "amContainerList . remove ( containerId )  ;", "}", "synchronized ( completedContainerList )     {", "completedContainerList . add ( containerId )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanupContainer"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "ArrayList < ContainerStatus >    csList    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( ContainerSimulator   container    :    runningContainers . values (  )  )     {", "csList . add ( newContainerStatus ( container . getId (  )  ,    RUNNING ,    SUCCESS )  )  ;", "}", "synchronized ( amContainerList )     {", "for    ( ContainerId   cId    :    amContainerList )     {", "csList . add ( newContainerStatus ( cId ,    RUNNING ,    SUCCESS )  )  ;", "}", "}", "synchronized ( completedContainerList )     {", "for    ( ContainerId   cId    :    completedContainerList )     {", ". LOG . debug ( MessageFormat . format (  (  \" NodeManager    {  0  }    completed \"     +     \"    container    (  {  1  }  )  .  \"  )  ,    node . getNodeID (  )  ,    cId )  )  ;", "csList . add ( newContainerStatus ( cId ,    COMPLETE ,    SUCCESS )  )  ;", "}", "completedContainerList . clear (  )  ;", "}", "synchronized ( releasedContainerList )     {", "for    ( ContainerId   cId    :    releasedContainerList )     {", ". LOG . debug ( MessageFormat . format (  (  \" NodeManager    {  0  }    released   container \"     +     \"     (  {  1  }  )  .  \"  )  ,    node . getNodeID (  )  ,    cId )  )  ;", "csList . add ( newContainerStatus ( cId ,    COMPLETE ,    ABORTED )  )  ;", "}", "releasedContainerList . clear (  )  ;", "}", "return   csList ;", "}", "METHOD_END"], "methodName": ["generateContainerStatusList"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   amContainerList ;", "}", "METHOD_END"], "methodName": ["getAMContainers"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   completedContainerList ;", "}", "METHOD_END"], "methodName": ["getCompletedContainers"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   node ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   runningContainers ;", "}", "METHOD_END"], "methodName": ["getRunningContainers"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "super . init ( dispatchTime ,     ( dispatchTime    +     (  1  0  0  0  0  0  0 L    *    heartBeatInterval )  )  ,    heartBeatInterval )  ;", "String [  ]    rackHostName    =    SLSUtils . getRackHostName ( nodeIdStr )  ;", "this . node    =    NodeInfo . newNodeInfo ( rackHostName [  0  ]  ,    rackHostName [  1  ]  ,    BuilderUtils . newResource ( memory ,    cores )  )  ;", "this . rm    =    rm ;", "completedContainerList    =    Collections . synchronizedList ( new   ArrayList < api . records . ContainerId >  (  )  )  ;", "releasedContainerList    =    Collections . synchronizedList ( new   ArrayList < api . records . ContainerId >  (  )  )  ;", "containerQueue    =    new   DelayQueue < ContainerSimulator >  (  )  ;", "amContainerList    =    Collections . synchronizedList ( new   ArrayList < api . records . ContainerId >  (  )  )  ;", "runningContainers    =    new   ConcurrentHashMap < api . records . ContainerId ,    ContainerSimulator >  (  )  ;", "RegisterNodeManagerRequest   req    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "req . setNodeId ( node . getNodeID (  )  )  ;", "req . setResource ( node . getTotalCapability (  )  )  ;", "req . setHttpPort (  8  0  )  ;", "RegisterNodeManagerResponse   response    =    rm . getResourceTrackerService (  )  . registerNodeManager ( req )  ;", "masterKey    =    response . getNMTokenMasterKey (  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   cs    =    Records . newRecord ( ContainerStatus . class )  ;", "cs . setContainerId ( cId )  ;", "cs . setState ( state )  ;", "cs . setExitStatus ( exitState )  ;", "return   cs ;", "}", "METHOD_END"], "methodName": ["newContainerStatus"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   NodeId . newInstance ( host ,    port )  ;", "}", "METHOD_END"], "methodName": ["newNodeID"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   NodeInfo . newNodeInfo ( rackName ,    hostName ,    ResourceOption . newInstance ( resource ,    OVER _ COMMIT _ TIMEOUT _ MILLIS _ DEFAULT )  ,     (  ( NodeInfo . NODE _ ID )  +  +  )  )  ;", "}", "METHOD_END"], "methodName": ["newNodeInfo"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "final   NodeId   nodeId    =    NodeInfo . newNodeID ( hostName ,    port )  ;", "final   String   nodeAddr    =     ( hostName    +     \"  :  \"  )     +    port ;", "final   String   httpAddress    =    hostName ;", "return   new   NodeInfo . FakeRMNodeImpl ( nodeId ,    nodeAddr ,    httpAddress ,    resourceOption ,    rackName ,     \" Me   good \"  ,    port ,    hostName ,    null )  ;", "}", "METHOD_END"], "methodName": ["newNodeInfo"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,    appId )  ,    appAttemptId )  ,    cId )  ;", "}", "METHOD_END"], "methodName": ["newContainerId"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.TestNMSimulator"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ SCHEDULER ,     \" scheduler . ResourceSchedulerWrapper \"  )  ;", "conf . set ( SLSConfiguration . RM _ SCHEDULER ,     \" FairScheduler \"  )  ;", "conf . setBoolean ( SLSConfiguration . METRICS _ SWITCH ,    false )  ;", "rm    =    new   ResourceManager (  )  ;", "rm . init ( conf )  ;", "rm . start (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.TestNMSimulator"}, {"methodBody": ["METHOD_START", "{", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.TestNMSimulator"}, {"methodBody": ["METHOD_START", "{", "NMSimulator   node 1     =    new   NMSimulator (  )  ;", "node 1  . init (  \" rack 1  / node 1  \"  ,     (  ( GB )     *     1  0  )  ,     1  0  ,     0  ,     1  0  0  0  ,    rm )  ;", "node 1  . middleStep (  )  ;", "Assert . assertEquals (  1  ,    rm . getResourceScheduler (  )  . getNumClusterNodes (  )  )  ;", "Assert . assertEquals (  (  ( GB )     *     1  0  )  ,    rm . getResourceScheduler (  )  . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "Assert . assertEquals (  1  0  ,    rm . getResourceScheduler (  )  . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "ContainerId   cId 1     =    newContainerId (  1  ,     1  ,     1  )  ;", "Container   container 1     =    Container . newInstance ( cId 1  ,    null ,    null ,    Resources . createResource ( GB ,     1  )  ,    null ,    null )  ;", "node 1  . addNewContainer ( container 1  ,     1  0  0  0  0  0 L )  ;", "Assert . assertTrue (  \" Node 1    should   have   one   running   container .  \"  ,    node 1  . getRunningContainers (  )  . containsKey ( cId 1  )  )  ;", "ContainerId   cId 2     =    newContainerId (  2  ,     1  ,     1  )  ;", "Container   container 2     =    Container . newInstance ( cId 2  ,    null ,    null ,    Resources . createResource ( GB ,     1  )  ,    null ,    null )  ;", "node 1  . addNewContainer ( container 2  ,     (  -  1 L )  )  ;", "Assert . assertTrue (  \" Node 1    should   have   one   running   AM   container \"  ,    node 1  . getAMContainers (  )  . contains ( cId 2  )  )  ;", "node 1  . cleanupContainer ( cId 1  )  ;", "Assert . assertTrue (  \" Container 1    should   be   removed   from   Node 1  .  \"  ,    node 1  . getCompletedContainers (  )  . contains ( cId 1  )  )  ;", "node 1  . cleanupContainer ( cId 2  )  ;", "Assert . assertFalse (  \" Container 2    should   be   removed   from   Node 1  .  \"  ,    node 1  . getAMContainers (  )  . contains ( cId 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNMSimulator"], "fileName": "org.apache.hadoop.yarn.sls.nodemanager.TestNMSimulator"}, {"methodBody": ["METHOD_START", "{", "return   endTime ;", "}", "METHOD_END"], "methodName": ["getEndTime"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   hostname ;", "}", "METHOD_END"], "methodName": ["getHostname"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   lifeTime ;", "}", "METHOD_END"], "methodName": ["getLifeTime"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   resource ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "priority    =    p ;", "}", "METHOD_END"], "methodName": ["setPriority"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ContainerSimulator"}, {"methodBody": ["METHOD_START", "{", "return   updates ;", "}", "METHOD_END"], "methodName": ["getContainerUpdates"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.RMNodeWrapper"}, {"methodBody": ["METHOD_START", "{", "if    ( metricsON )     {", "try    {", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append ( appId )  . append (  \"  ,  \"  )  . append ( traceStartTimeMS )  . append (  \"  ,  \"  )  . append ( traceEndTimeMS )  . append (  \"  ,  \"  )  . append ( simulateStartTimeMS )  . append (  \"  ,  \"  )  . append ( simulateEndTimeMS )  ;", "jobRuntimeLogBW . write (  (  ( sb . toString (  )  )     +     (  . EOL )  )  )  ;", "jobRuntimeLogBW . flush (  )  ;", "}    catch    ( IOException   e )     {", "e . printStackTrace (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addAMRuntime"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "if    ( metricsON )     {", "Metrics . trackApp ( appAttemptId ,    oldAppId )  ;", "}", "}", "METHOD_END"], "methodName": ["addTrackedApp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "return   metrics ;", "}", "METHOD_END"], "methodName": ["getMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "return   this . queueSet ;", "}", "METHOD_END"], "methodName": ["getQueueSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "return   schedulerMetrics ;", "}", "METHOD_END"], "methodName": ["getSchedulerMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "return   this . trackedAppSet ;", "}", "METHOD_END"], "methodName": ["getTrackedAppSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "metrics    =    new   MetricRegistry (  )  ;", "metricsOutputDir    =    conf . get ( SLSConfiguration . METRICS _ OUTPUT _ DIR )  ;", "int   metricsWebAddressPort    =    conf . getInt ( SLSConfiguration . METRICS _ WEB _ ADDRESS _ PORT ,    SLSConfiguration . METRICS _ WEB _ ADDRESS _ PORT _ DEFAULT )  ;", "String   schedulerMetricsType    =    conf . get ( scheduler . getClass (  )  . getName (  )  )  ;", "Class   schedulerMetricsClass    =     ( schedulerMetricsType    =  =    null )     ?     . defaultSchedulerMetricsMap . get ( scheduler . getClass (  )  )     :    Class . forName ( schedulerMetricsType )  ;", "schedulerMetrics    =     (  ( SchedulerMetrics )     ( ReflectionUtils . newInstance ( schedulerMetricsClass ,    new   Configuration (  )  )  )  )  ;", "schedulerMetrics . init ( scheduler ,    metrics )  ;", "registerJvmMetrics (  )  ;", "registerClusterResourceMetrics (  )  ;", "registerContainerAppNumMetrics (  )  ;", "registerSchedulerMetrics (  )  ;", "initMetricsCSVOutput (  )  ;", "web    =    new   SLSWebApp ( this ,    metricsWebAddressPort )  ;", "web . start (  )  ;", "pool    =    new   ScheduledThreadPoolExecutor (  2  )  ;", "pool . scheduleAtFixedRate ( new    . HistogramsRunnable (  )  ,     0  ,     1  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "pool . scheduleAtFixedRate ( new    . MetricsLogRunnable (  )  ,     0  ,     1  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "jobRuntimeLogBW    =    new   BufferedWriter ( new   FileWriter (  (  ( metricsOutputDir )     +     \"  / jobruntime . csv \"  )  )  )  ;", "jobRuntimeLogBW . write (  (  (  \" JobID , real _ start _ time , real _ end _ time ,  \"     +     \" simulate _ start _ time , simulate _ end _ time \"  )     +     (  . EOL )  )  )  ;", "jobRuntimeLogBW . flush (  )  ;", "}", "METHOD_END"], "methodName": ["initMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "int   timeIntalMS    =    conf . getInt ( SLSConfiguration . METRICS _ RECORD _ INTERVAL _ MS ,    SLSConfiguration . METRICS _ RECORD _ INTERVAL _ MS _ DEFAULT )  ;", "File   dir    =    new   File (  (  ( metricsOutputDir )     +     \"  / metrics \"  )  )  ;", "if    (  (  !  ( dir . exists (  )  )  )     &  &     (  !  ( dir . mkdirs (  )  )  )  )     {", "LOG . or (  (  \" Cannot   create   directory    \"     +     ( dir . getAbsoluteFile (  )  )  )  )  ;", "}", "final   CsvReportreport =    CsvReportforRegistry ( metrics )  . formatFor ( Locale . US )  . convRatesTo ( TimeUnit . SECONDS )  . convDurationsTo ( TimeUnit . MILLISECONDS )  . build ( new   File (  (  ( metricsOutputDir )     +     \"  / metrics \"  )  )  )  ;", "reportstart ( timeIntalMS ,    TimeUnit . MILLISECONDS )  ;", "}", "METHOD_END"], "methodName": ["initMetricsCSVOutput"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  \" variable . cluster . allocated . memory \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  (  (     =  =    null )     |  |     (  ( getRootQueueMetrics (  )  )     =  =    null )  )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAllocatedMB (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . cluster . allocated . vcores \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  (  (     =  =    null )     |  |     (  ( getRootQueueMetrics (  )  )     =  =    null )  )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAllocatedVirtualCores (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . cluster . available . memory \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  (  (     =  =    null )     |  |     (  ( getRootQueueMetrics (  )  )     =  =    null )  )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAvailableMB (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . cluster . available . vcores \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  (  (     =  =    null )     |  |     (  ( getRootQueueMetrics (  )  )     =  =    null )  )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["registerClusterResourceMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  \" variable . running . application \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  (  (     =  =    null )     |  |     (  ( getRootQueueMetrics (  )  )     =  =    null )  )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAppsRunning (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . running . container \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  (  (     =  =    null )     |  |     (  ( getRootQueueMetrics (  )  )     =  =    null )  )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAllocatedContainers (  )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["registerContainerAppNumMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  \" variable . jvm . free . memory \"  ,    new   com . codahale . metrics . Gauge < Long >  (  )     {", "@ Override", "public   Long   getValue (  )     {", "return   Runtime . getRuntime (  )  . freeMemory (  )  ;", "}", "}  )  ;", "metrics . register (  \" variable . jvm . max . memory \"  ,    new   com . codahale . metrics . Gauge < Long >  (  )     {", "@ Override", "public   Long   getValue (  )     {", "return   Runtime . getRuntime (  )  . maxMemory (  )  ;", "}", "}  )  ;", "metrics . register (  \" variable . jvm . total . memory \"  ,    new   com . codahale . metrics . Gauge < Long >  (  )     {", "@ Override", "public   Long   getValue (  )     {", "return   Runtime . getRuntime (  )  . totalMemory (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["registerJvmMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "samplerLock . lock (  )  ;", "try    {", "schedulerAllocateCounter    =    metrics . counter (  \" counter . scheduler . operation . allocate \"  )  ;", "schedulerHandleCounter    =    metrics . counter (  \" counter . scheduler . operation . handle \"  )  ;", "schedulerHandleCounterMap    =    new   HashMap < SchedulerEventType ,    Counter >  (  )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "Counter   counter    =    metrics . counter (  (  \" counter . scheduler . operation . handle .  \"     +    e )  )  ;", "schedulerHandleCounterMap . put ( e ,    counter )  ;", "}", "int   timeWindowSize    =    conf . getInt ( SLSConfiguration . METRICS _ TIMER _ WINDOW _ SIZE ,    SLSConfiguration . METRICS _ TIMER _ WINDOW _ SIZE _ DEFAULT )  ;", "schedulerAllocateTimer    =    new   Timer ( new   SlidingWindowReservoir ( timeWindowSize )  )  ;", "schedulerHandleTimer    =    new   Timer ( new   SlidingWindowReservoir ( timeWindowSize )  )  ;", "schedulerHandleTimerMap    =    new   HashMap < SchedulerEventType ,    Timer >  (  )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "Timer   timer    =    new   Timer ( new   SlidingWindowReservoir ( timeWindowSize )  )  ;", "schedulerHandleTimerMap . put ( e ,    timer )  ;", "}", "schedulerHistogramList    =    new   ArrayList < Histogram >  (  )  ;", "histogramTimerMap    =    new   HashMap < Histogram ,    Timer >  (  )  ;", "Histogram   schedulerAllocateHistogram    =    new   Histogram ( new   SlidingWindowReservoir (  . SAMPLING _ SIZE )  )  ;", "metrics . register (  \" sampler . scheduler . operation . allocate . timecost \"  ,    schedulerAllocateHistogram )  ;", "schedulerHistogramList . add ( schedulerAllocateHistogram )  ;", "histogramTimerMap . put ( schedulerAllocateHistogram ,    schedulerAllocateTimer )  ;", "Histogram   schedulerHandleHistogram    =    new   Histogram ( new   SlidingWindowReservoir (  . SAMPLING _ SIZE )  )  ;", "metrics . register (  \" sampler . scheduler . operation . handle . timecost \"  ,    schedulerHandleHistogram )  ;", "schedulerHistogramList . add ( schedulerHandleHistogram )  ;", "histogramTimerMap . put ( schedulerHandleHistogram ,    schedulerHandleTimer )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "Histogram   histogram    =    new   Histogram ( new   SlidingWindowReservoir (  . SAMPLING _ SIZE )  )  ;", "metrics . register (  (  (  \" sampler . scheduler . operation . handle .  \"     +    e )     +     \"  . timecost \"  )  ,    histogram )  ;", "schedulerHistogramList . add ( histogram )  ;", "histogramTimerMap . put ( histogram ,    schedulerHandleTimerMap . get ( e )  )  ;", "}", "}    finally    {", "samplerLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["registerSchedulerMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "if    ( metricsON )     {", "Metrics . untrackApp ( appAttemptId ,    oldAppId )  ;", "}", "}", "METHOD_END"], "methodName": ["removeTrackedApp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "this . queueSet    =    queues ;", "}", "METHOD_END"], "methodName": ["setQueueSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "this . trackedAppSet    =    apps ;", "}", "METHOD_END"], "methodName": ["setTrackedAppSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "if    (  ( jobRuntimeLogBW )     !  =    null )     {", "jobRuntimeLogBW . close (  )  ;", "}", "if    (  ( pool )     !  =    null )", "pool . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "SortedMap < String ,    Count   countap    =    metrics . getCount (  )  ;", "if    ( releasedMemory    !  =     0  )     {", "String   name    =     (  \" countqueue .  \"     +    queue )     +     \"  . allocated . memory \"  ;", "if    (  !  ( countap . containsKey ( name )  )  )     {", "metrics . countname )  ;", "countap    =    metrics . getCount (  )  ;", "}", "countap . get ( name )  . inc (  (  - releasedMemory )  )  ;", "}", "if    ( releasedVCores    !  =     0  )     {", "String   name    =     (  \" countqueue .  \"     +    queue )     +     \"  . allocated . cores \"  ;", "if    (  !  ( countap . containsKey ( name )  )  )     {", "metrics . countname )  ;", "countap    =    metrics . getCount (  )  ;", "}", "countap . get ( name )  . inc (  (  - releasedVCores )  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateQueueMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "Resource   pendingResource    =    Resources . createResource (  0  ,     0  )  ;", "Resource   allocatedResource    =    Resources . createResource (  0  ,     0  )  ;", "String   queueName    =    appQueueMap . get ( attemptId . getApplicationId (  )  )  ;", "for    ( ResourceRequest   request    :    resourceRequests )     {", "if    ( request . getResourceName (  )  . equals ( ANY )  )     {", "Resources . addTo ( pendingResource ,    Resources . multiply ( request . getCapability (  )  ,    request . getNumContainers (  )  )  )  ;", "}", "}", "for    ( Container   container    :    allocation . getContainers (  )  )     {", "Resources . addTo ( allocatedResource ,    container . getResource (  )  )  ;", "Resources . subtractFrom ( pendingResource ,    container . getResource (  )  )  ;", "}", "SchedulerAppReport   report    =    getSchedulerAppInfo ( attemptId )  ;", "for    ( ContainerId   containerId    :    containerIds )     {", "Container   container    =    null ;", "for    ( RMContainer   c    :    report . getLiveContainers (  )  )     {", "if    ( c . getContainerId (  )  . equals ( containerId )  )     {", "container    =    c . getContainer (  )  ;", "break ;", "}", "}", "if    ( container    !  =    null )     {", "Resources . subtractFrom ( allocatedResource ,    container . getResource (  )  )  ;", "} else    {", "for    ( RMContainer   c    :    report . getReservedContainers (  )  )     {", "if    ( c . getContainerId (  )  . equals ( containerId )  )     {", "container    =    c . getContainer (  )  ;", "break ;", "}", "}", "if    ( container    !  =    null )     {", "Resources . subtractFrom ( pendingResource ,    container . getResource (  )  )  ;", "}", "}", "}", "Set < ContainerId >    preemptionContainers    =    new   HashSet < ContainerId >  (  )  ;", "if    (  ( allocation . getContainerPreemptions (  )  )     !  =    null )     {", "preemptionContainers . addAll ( allocation . getContainerPreemptions (  )  )  ;", "}", "if    (  ( allocation . getStrictContainerPreemptions (  )  )     !  =    null )     {", "preemptionContainers . addAll ( allocation . getStrictContainerPreemptions (  )  )  ;", "}", "if    (  !  ( preemptionContainers . isEmpty (  )  )  )     {", "for    ( ContainerId   containerId    :    preemptionContainers )     {", "if    (  !  ( preemptionContainerMap . containsKey ( containerId )  )  )     {", "Container   container    =    null ;", "for    ( RMContainer   c    :    report . getLiveContainers (  )  )     {", "if    ( c . getContainerId (  )  . equals ( containerId )  )     {", "container    =    c . getContainer (  )  ;", "break ;", "}", "}", "if    ( container    !  =    null )     {", "preemptionContainerMap . put ( containerId ,    container . getResource (  )  )  ;", "}", "}", "}", "}", "SortedMap < String ,    Counter >    counterMap    =    metrics . getCounters (  )  ;", "String [  ]    names    =    new   String [  ]  {     (  \" counter . queue .  \"     +    queueName )     +     \"  . pending . memory \"  ,     (  \" counter . queue .  \"     +    queueName )     +     \"  . pending . cores \"  ,     (  \" counter . queue .  \"     +    queueName )     +     \"  . allocated . memory \"  ,     (  \" counter . queue .  \"     +    queueName )     +     \"  . allocated . cores \"     }  ;", "int [  ]    values    =    new   int [  ]  {    pendingResource . getMemory (  )  ,    pendingResource . getVirtualCores (  )  ,    allocatedResource . getMemory (  )  ,    allocatedResource . getVirtualCores (  )     }  ;", "for    ( int   i    =     ( names . length )     -     1  ;    i    >  =     0  ;    i -  -  )     {", "if    (  !  ( counterMap . containsKey ( names [ i ]  )  )  )     {", "metrics . counter ( names [ i ]  )  ;", "counterMap    =    metrics . getCounters (  )  ;", "}", "counterMap . get ( names [ i ]  )  . inc ( values [ i ]  )  ;", "}", "queueLock . lock (  )  ;", "try    {", "if    (  !  ( etrics . isTracked ( queueName )  )  )     {", "etrics . trackQueue ( queueName )  ;", "}", "}    finally    {", "queueLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateQueueWithAllocateRequest"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "RMNodeWrapper   node    =     (  ( RMNodeWrapper )     ( eventWrapper . getRMNode (  )  )  )  ;", "List < UpdatedContainerInfo >    containerList    =    node . getContainerUpdates (  )  ;", "for    ( UpdatedContainerInfo   info    :    containerList )     {", "for    ( ContainerStatus   status    :    info . getCompletedContainers (  )  )     {", "ContainerId   containerId    =    status . getContainerId (  )  ;", "SchedulerAppReport   app    =    scheduler . getSchedulerAppInfo ( containerId . getApplicationAttemptId (  )  )  ;", "if    ( app    =  =    null )     {", "continue ;", "}", "String   queue    =    appQueueMap . get ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  ;", "int   releasedMemory    =     0  ;", "int   releasedVCores    =     0  ;", "if    (  ( status . getExitStatus (  )  )     =  =     ( ContainerExitStatus . SUCCESS )  )     {", "for    ( RMContainer   rmc    :    app . getLiveContainers (  )  )     {", "if    (  ( rmc . getContainerId (  )  )     =  =    containerId )     {", "releasedMemory    +  =    rmc . getContainer (  )  . getResource (  )  . getMemory (  )  ;", "releasedVCores    +  =    rmc . getContainer (  )  . getResource (  )  . getVirtualCores (  )  ;", "break ;", "}", "}", "} else", "if    (  ( status . getExitStatus (  )  )     =  =     ( ContainerExitStatus . ABORTED )  )     {", "if    ( preemptionContainerMap . containsKey ( containerId )  )     {", "api . records . Resource   preResource    =    preemptionContainerMap . get ( containerId )  ;", "releasedMemory    +  =    preResource . getMemory (  )  ;", "releasedVCores    +  =    preResource . getVirtualCores (  )  ;", "preemptionContainerMap . remove ( containerId )  ;", "}", "}", "updateQueueMetrics ( queue ,    releasedMemory ,    releasedVCores )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updateQueueWithNodeUpdate"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper"}, {"methodBody": ["METHOD_START", "{", "if    ( metricsON )     {", "try    {", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append ( appId )  . append (  \"  ,  \"  )  . append ( traceStartTimeMS )  . append (  \"  ,  \"  )  . append ( traceEndTimeMS )  . append (  \"  ,  \"  )  . append ( simulateStartTimeMS )  . append (  \"  ,  \"  )  . append ( simulateEndTimeMS )  ;", "jobRuntimeLogBW . write (  (  ( sb . toString (  )  )     +     (  . EOL )  )  )  ;", "jobRuntimeLogBW . flush (  )  ;", "}    catch    ( IOException   e )     {", "e . printStackTrace (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addAMRuntime"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( metricsON )     {", "Metrics . trackApp ( appAttemptId ,    oldAppId )  ;", "}", "}", "METHOD_END"], "methodName": ["addTrackedApp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   metrics ;", "}", "METHOD_END"], "methodName": ["getMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   this . queueSet ;", "}", "METHOD_END"], "methodName": ["getQueueSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   schedulerMetrics ;", "}", "METHOD_END"], "methodName": ["getSchedulerMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   this . trackedAppSet ;", "}", "METHOD_END"], "methodName": ["getTrackedAppSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "metrics    =    new   MetricRegistry (  )  ;", "metricsOutputDir    =    conf . get ( SLSConfiguration . METRICS _ OUTPUT _ DIR )  ;", "int   metricsWebAddressPort    =    conf . getInt ( SLSConfiguration . METRICS _ WEB _ ADDRESS _ PORT ,    SLSConfiguration . METRICS _ WEB _ ADDRESS _ PORT _ DEFAULT )  ;", "String   schedulerMetricsType    =    conf . get ( CapacityScheduler . class . getName (  )  )  ;", "Class   schedulerMetricsClass    =     ( schedulerMetricsType    =  =    null )     ?     . defaultSchedulerMetricsMap . get ( CapacityScheduler . class )     :    Class . forName ( schedulerMetricsType )  ;", "schedulerMetrics    =     (  ( SchedulerMetrics )     ( ReflectionUtils . newInstance ( schedulerMetricsClass ,    new   Configuration (  )  )  )  )  ;", "schedulerMetrics . init ( this ,    metrics )  ;", "registerJvmMetrics (  )  ;", "registerClusterResourceMetrics (  )  ;", "registerContainerAppNumMetrics (  )  ;", "registerSchedulerMetrics (  )  ;", "initMetricsCSVOutput (  )  ;", "web    =    new   SLSWebApp ( this ,    metricsWebAddressPort )  ;", "web . start (  )  ;", "pool    =    new   ScheduledThreadPoolExecutor (  2  )  ;", "pool . scheduleAtFixedRate ( new    . HistogramsRunnable (  )  ,     0  ,     1  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "pool . scheduleAtFixedRate ( new    . MetricsLogRunnable (  )  ,     0  ,     1  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "jobRuntimeLogBW    =    new   BufferedWriter ( new   FileWriter (  (  ( metricsOutputDir )     +     \"  / jobruntime . csv \"  )  )  )  ;", "jobRuntimeLogBW . write (  (  (  \" JobID , real _ start _ time , real _ end _ time ,  \"     +     \" simulate _ start _ time , simulate _ end _ time \"  )     +     (  . EOL )  )  )  ;", "jobRuntimeLogBW . flush (  )  ;", "}", "METHOD_END"], "methodName": ["initMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "int   timeIntervalMS    =    conf . getInt ( SLSConfiguration . METRICS _ RECORD _ INTERVAL _ MS ,    SLSConfiguration . METRICS _ RECORD _ INTERVAL _ MS _ DEFAULT )  ;", "File   dir    =    new   File (  (  ( metricsOutputDir )     +     \"  / metrics \"  )  )  ;", "if    (  (  !  ( dir . exists (  )  )  )     &  &     (  !  ( dir . mkdirs (  )  )  )  )     {", "LOG . error (  (  \" Cannot   create   directory    \"     +     ( dir . getAbsoluteFile (  )  )  )  )  ;", "}", "final   CsvReporter   reporter    =    CsvReporter . forRegistry ( metrics )  . formatFor ( Locale . US )  . convertRatesTo ( TimeUnit . SECONDS )  . convertDurationsTo ( TimeUnit . MILLISECONDS )  . build ( new   File (  (  ( metricsOutputDir )     +     \"  / metrics \"  )  )  )  ;", "reporter . start ( timeIntervalMS ,    TimeUnit . MILLISECONDS )  ;", "}", "METHOD_END"], "methodName": ["initMetricsCSVOutput"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  \" variable . cluster . allocated . memory \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  ( getRootQueueMetrics (  )  )     =  =    null )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAllocatedMB (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . cluster . allocated . vcores \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  ( getRootQueueMetrics (  )  )     =  =    null )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAllocatedVirtualCores (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . cluster . available . memory \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  ( getRootQueueMetrics (  )  )     =  =    null )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAvailableMB (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . cluster . available . vcores \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  ( getRootQueueMetrics (  )  )     =  =    null )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["registerClusterResourceMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  \" variable . running . application \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  ( getRootQueueMetrics (  )  )     =  =    null )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAppsRunning (  )  ;", "}", "}", "}  )  ;", "metrics . register (  \" variable . running . container \"  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "if    (  ( getRootQueueMetrics (  )  )     =  =    null )     {", "return    0  ;", "} else    {", "return   getRootQueueMetrics (  )  . getAllocatedContainers (  )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["registerContainerAppNumMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  \" variable . jvm . free . memory \"  ,    new   com . codahale . metrics . Gauge < Long >  (  )     {", "@ Override", "public   Long   getValue (  )     {", "return   Runtime . getRuntime (  )  . freeMemory (  )  ;", "}", "}  )  ;", "metrics . register (  \" variable . jvm . max . memory \"  ,    new   com . codahale . metrics . Gauge < Long >  (  )     {", "@ Override", "public   Long   getValue (  )     {", "return   Runtime . getRuntime (  )  . maxMemory (  )  ;", "}", "}  )  ;", "metrics . register (  \" variable . jvm . total . memory \"  ,    new   com . codahale . metrics . Gauge < Long >  (  )     {", "@ Override", "public   Long   getValue (  )     {", "return   Runtime . getRuntime (  )  . totalMemory (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["registerJvmMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "samplerLock . lock (  )  ;", "try    {", "schedulerAllocateCounter    =    metrics . counter (  \" counter . scheduler . operation . allocate \"  )  ;", "schedulerHandleCounter    =    metrics . counter (  \" counter . scheduler . operation . handle \"  )  ;", "schedulerHandleCounterMap    =    new   HashMap < SchedulerEventType ,    Counter >  (  )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "Counter   counter    =    metrics . counter (  (  \" counter . scheduler . operation . handle .  \"     +    e )  )  ;", "schedulerHandleCounterMap . put ( e ,    counter )  ;", "}", "int   timeWindowSize    =    conf . getInt ( SLSConfiguration . METRICS _ TIMER _ WINDOW _ SIZE ,    SLSConfiguration . METRICS _ TIMER _ WINDOW _ SIZE _ DEFAULT )  ;", "schedulerAllocateTimer    =    new   Timer ( new   SlidingWindowReservoir ( timeWindowSize )  )  ;", "schedulerHandleTimer    =    new   Timer ( new   SlidingWindowReservoir ( timeWindowSize )  )  ;", "schedulerHandleTimerMap    =    new   HashMap < SchedulerEventType ,    Timer >  (  )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "Timer   timer    =    new   Timer ( new   SlidingWindowReservoir ( timeWindowSize )  )  ;", "schedulerHandleTimerMap . put ( e ,    timer )  ;", "}", "schedulerHistogramList    =    new   ArrayList < Histogram >  (  )  ;", "histogramTimerMap    =    new   HashMap < Histogram ,    Timer >  (  )  ;", "Histogram   schedulerAllocateHistogram    =    new   Histogram ( new   SlidingWindowReservoir (  . SAMPLING _ SIZE )  )  ;", "metrics . register (  \" sampler . scheduler . operation . allocate . timecost \"  ,    schedulerAllocateHistogram )  ;", "schedulerHistogramList . add ( schedulerAllocateHistogram )  ;", "histogramTimerMap . put ( schedulerAllocateHistogram ,    schedulerAllocateTimer )  ;", "Histogram   schedulerHandleHistogram    =    new   Histogram ( new   SlidingWindowReservoir (  . SAMPLING _ SIZE )  )  ;", "metrics . register (  \" sampler . scheduler . operation . handle . timecost \"  ,    schedulerHandleHistogram )  ;", "schedulerHistogramList . add ( schedulerHandleHistogram )  ;", "histogramTimerMap . put ( schedulerHandleHistogram ,    schedulerHandleTimer )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "Histogram   histogram    =    new   Histogram ( new   SlidingWindowReservoir (  . SAMPLING _ SIZE )  )  ;", "metrics . register (  (  (  \" sampler . scheduler . operation . handle .  \"     +    e )     +     \"  . timecost \"  )  ,    histogram )  ;", "schedulerHistogramList . add ( histogram )  ;", "histogramTimerMap . put ( histogram ,    schedulerHandleTimerMap . get ( e )  )  ;", "}", "}    finally    {", "samplerLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["registerSchedulerMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( metricsON )     {", "Metrics . untrackApp ( appAttemptId ,    oldAppId )  ;", "}", "}", "METHOD_END"], "methodName": ["removeTrackedApp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "this . queueSet    =    queues ;", "}", "METHOD_END"], "methodName": ["setQueueSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "this . trackedAppSet    =    apps ;", "}", "METHOD_END"], "methodName": ["setTrackedAppSet"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    (  ( jobRuntimeLogBW )     !  =    null )     {", "jobRuntimeLogBW . close (  )  ;", "}", "if    (  ( pool )     !  =    null )", "pool . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "SortedMap < String ,    Count   countap    =    metrics . getCount (  )  ;", "if    ( releasedMemory    !  =     0  )     {", "String   name    =     (  \" countqueue .  \"     +    queue )     +     \"  . allocated . memory \"  ;", "if    (  !  ( countap . containsKey ( name )  )  )     {", "metrics . countname )  ;", "countap    =    metrics . getCount (  )  ;", "}", "countap . get ( name )  . inc (  (  - releasedMemory )  )  ;", "}", "if    ( releasedVCores    !  =     0  )     {", "String   name    =     (  \" countqueue .  \"     +    queue )     +     \"  . allocated . cores \"  ;", "if    (  !  ( countap . containsKey ( name )  )  )     {", "metrics . countname )  ;", "countap    =    metrics . getCount (  )  ;", "}", "countap . get ( name )  . inc (  (  - releasedVCores )  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateQueueMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Resource   pendingResource    =    Resources . createResource (  0  ,     0  )  ;", "Resource   allocatedResource    =    Resources . createResource (  0  ,     0  )  ;", "String   queueName    =    appQueueMap . get ( attemptId )  ;", "for    ( ResourceRequest   request    :    resourceRequests )     {", "if    ( request . getResourceName (  )  . equals ( ANY )  )     {", "Resources . addTo ( pendingResource ,    Resources . multiply ( request . getCapability (  )  ,    request . getNumContainers (  )  )  )  ;", "}", "}", "for    ( Container   container    :    allocation . getContainers (  )  )     {", "Resources . addTo ( allocatedResource ,    container . getResource (  )  )  ;", "Resources . subtractFrom ( pendingResource ,    container . getResource (  )  )  ;", "}", "AppReport   report    =    super . getAppInfo ( attemptId )  ;", "for    ( ContainerId   containerId    :    containerIds )     {", "Container   container    =    null ;", "for    ( RMContainer   c    :    report . getLiveContainers (  )  )     {", "if    ( c . getContainerId (  )  . equals ( containerId )  )     {", "container    =    c . getContainer (  )  ;", "break ;", "}", "}", "if    ( container    !  =    null )     {", "Resources . subtractFrom ( allocatedResource ,    container . getResource (  )  )  ;", "} else    {", "for    ( RMContainer   c    :    report . getReservedContainers (  )  )     {", "if    ( c . getContainerId (  )  . equals ( containerId )  )     {", "container    =    c . getContainer (  )  ;", "break ;", "}", "}", "if    ( container    !  =    null )     {", "Resources . subtractFrom ( pendingResource ,    container . getResource (  )  )  ;", "}", "}", "}", "Set < ContainerId >    preemptionContainers    =    new   HashSet < ContainerId >  (  )  ;", "if    (  ( allocation . getContainerPreemptions (  )  )     !  =    null )     {", "preemptionContainers . addAll ( allocation . getContainerPreemptions (  )  )  ;", "}", "if    (  ( allocation . getStrictContainerPreemptions (  )  )     !  =    null )     {", "preemptionContainers . addAll ( allocation . getStrictContainerPreemptions (  )  )  ;", "}", "if    (  !  ( preemptionContainers . isEmpty (  )  )  )     {", "for    ( ContainerId   containerId    :    preemptionContainers )     {", "if    (  !  ( preemptionContainerMap . containsKey ( containerId )  )  )     {", "Container   container    =    null ;", "for    ( RMContainer   c    :    report . getLiveContainers (  )  )     {", "if    ( c . getContainerId (  )  . equals ( containerId )  )     {", "container    =    c . getContainer (  )  ;", "break ;", "}", "}", "if    ( container    !  =    null )     {", "preemptionContainerMap . put ( containerId ,    container . getResource (  )  )  ;", "}", "}", "}", "}", "SortedMap < String ,    Counter >    counterMap    =    metrics . getCounters (  )  ;", "String [  ]    names    =    new   String [  ]  {     (  \" counter . queue .  \"     +    queueName )     +     \"  . pending . memory \"  ,     (  \" counter . queue .  \"     +    queueName )     +     \"  . pending . cores \"  ,     (  \" counter . queue .  \"     +    queueName )     +     \"  . allocated . memory \"  ,     (  \" counter . queue .  \"     +    queueName )     +     \"  . allocated . cores \"     }  ;", "int [  ]    values    =    new   int [  ]  {    pendingResource . getMemory (  )  ,    pendingResource . getVirtualCores (  )  ,    allocatedResource . getMemory (  )  ,    allocatedResource . getVirtualCores (  )     }  ;", "for    ( int   i    =     ( names . length )     -     1  ;    i    >  =     0  ;    i -  -  )     {", "if    (  !  ( counterMap . containsKey ( names [ i ]  )  )  )     {", "metrics . counter ( names [ i ]  )  ;", "counterMap    =    metrics . getCounters (  )  ;", "}", "counterMap . get ( names [ i ]  )  . inc ( values [ i ]  )  ;", "}", "queueLock . lock (  )  ;", "try    {", "if    (  !  ( schedulerMetrics . isTracked ( queueName )  )  )     {", "schedulerMetrics . trackQueue ( queueName )  ;", "}", "}    finally    {", "queueLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateQueueWithAllocateRequest"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "RMNodeWrapper   node    =     (  ( RMNodeWrapper )     ( eventWrapper . getRMNode (  )  )  )  ;", "List < UpdatedContainerInfo >    containerList    =    node . getContainerUpdates (  )  ;", "for    ( UpdatedContainerInfo   info    :    containerList )     {", "for    ( ContainerStatus   status    :    info . getCompletedContainers (  )  )     {", "ContainerId   containerId    =    status . getContainerId (  )  ;", "SchedulerAppReport   app    =    super . getSchedulerAppInfo ( containerId . getApplicationAttemptId (  )  )  ;", "if    ( app    =  =    null )     {", "continue ;", "}", "String   queue    =    appQueueMap . get ( containerId . getApplicationAttemptId (  )  )  ;", "int   releasedMemory    =     0  ;", "int   releasedVCores    =     0  ;", "if    (  ( status . getExitStatus (  )  )     =  =     ( ContainerExitStatus . SUCCESS )  )     {", "for    ( RMContainer   rmc    :    app . getLiveContainers (  )  )     {", "if    (  ( rmc . getContainerId (  )  )     =  =    containerId )     {", "releasedMemory    +  =    rmc . getContainer (  )  . getResource (  )  . getMemory (  )  ;", "releasedVCores    +  =    rmc . getContainer (  )  . getResource (  )  . getVirtualCores (  )  ;", "break ;", "}", "}", "} else", "if    (  ( status . getExitStatus (  )  )     =  =     ( ContainerExitStatus . ABORTED )  )     {", "if    ( preemptionContainerMap . containsKey ( containerId )  )     {", "api . records . Resource   preResource    =    preemptionContainerMap . get ( containerId )  ;", "releasedMemory    +  =    preResource . getMemory (  )  ;", "releasedVCores    +  =    preResource . getVirtualCores (  )  ;", "preemptionContainerMap . remove ( containerId )  ;", "}", "}", "updateQueueMetrics ( queue ,    releasedMemory ,    releasedVCores )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updateQueueWithNodeUpdate"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SLSCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   appTrackedMetrics ;", "}", "METHOD_END"], "methodName": ["getAppTrackedMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "return   queueTrackedMetrics ;", "}", "METHOD_END"], "methodName": ["getQueueTrackedMetrics"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "this . scheduler    =    scheduler ;", "this . trackedQueues    =    new   HashSet < String >  (  )  ;", "this . metrics    =    metrics ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "return   trackedQueues . contains ( queueName )  ;", "}", "METHOD_END"], "methodName": ["isTracked"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "metrics . register (  (  (  \" variable . app .  \"     +    oldAppId )     +     \"  . live . containers \"  )  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "SchedulerAppReport   app    =    getSchedulerAppInfo ( appAttemptId )  ;", "return   app . getLiveContainers (  )  . size (  )  ;", "}", "}  )  ;", "metrics . register (  (  (  \" variable . app .  \"     +    oldAppId )     +     \"  . reserved . containers \"  )  ,    new   com . codahale . metrics . Gauge < Integer >  (  )     {", "@ Override", "public   Integer   getValue (  )     {", "SchedulerAppReport   app    =    getSchedulerAppInfo ( appAttemptId )  ;", "return   app . getReservedContainers (  )  . size (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["trackApp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "for    ( String   m    :    appTrackedMetrics )     {", "metrics . remove (  (  (  (  \" variable . app .  \"     +    oldAppId )     +     \"  .  \"  )     +    m )  )  ;", "}", "}", "METHOD_END"], "methodName": ["untrackApp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "for    ( String   m    :    queueTrackedMetrics )     {", "metrics . remove (  (  (  (  \" variable . queue .  \"     +    queueName )     +     \"  .  \"  )     +    m )  )  ;", "}", "}", "METHOD_END"], "methodName": ["untrackQueue"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.SchedulerMetrics"}, {"methodBody": ["METHOD_START", "{", "return   this . startTimeMS ;", "}", "METHOD_END"], "methodName": ["getStartTimeMS"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TaskRunner"}, {"methodBody": ["METHOD_START", "{", "schedule ( task ,    System . currentTimeMillis (  )  )  ;", "}", "METHOD_END"], "methodName": ["schedule"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TaskRunner"}, {"methodBody": ["METHOD_START", "{", "task . timeRebase ( timeNow )  ;", "task . setQueue ( queue )  ;", "queue . add ( task )  ;", "}", "METHOD_END"], "methodName": ["schedule"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TaskRunner"}, {"methodBody": ["METHOD_START", "{", "this . threadPoolSize    =    threadPoolSize ;", "}", "METHOD_END"], "methodName": ["setQueueSize"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TaskRunner"}, {"methodBody": ["METHOD_START", "{", "if    (  ( executor )     !  =    null )     {", "throw   new   IllegalStateException (  \" Already   started \"  )  ;", "}", "DelayQueue   preStartQueue    =    queue ;", "queue    =    new   DelayQueue (  )  ;", "executor    =    new   ThreadPoolExecutor ( threadPoolSize ,    threadPoolSize ,     0  ,    TimeUnit . MILLISECONDS ,    queue )  ;", "executor . prestartAllCoreThreads (  )  ;", "startTimeMS    =    System . currentTimeMillis (  )  ;", "for    ( Object   d    :    preStartQueue )     {", "schedule (  (  (  . Task )     ( d )  )  ,    startTimeMS )  ;", "}", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TaskRunner"}, {"methodBody": ["METHOD_START", "{", "executor . shutdownNow (  )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner . stop (  )  ;", "}", "METHOD_END"], "methodName": ["cleanUp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner    =    new   TaskRunner (  )  ;", "runner . setQueueSize (  5  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner . start (  )  ;", "runner . schedule ( new    . DualTask (  0  ,     1  0  ,     1  0  )  )  ;", ". DualTask . latch . await (  5  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "Assert . assertTrue (  . DualTask . first )  ;", "Assert . assertTrue (  . DualTask . last )  ;", "}", "METHOD_END"], "methodName": ["testDualTask"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner . start (  )  ;", "runner . schedule ( new    . MultiTask (  0  ,     2  0  ,     5  )  )  ;", ". MultiTask . latch . await (  5  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "Assert . assertTrue (  . MultiTask . first )  ;", "Assert . assertEquals (  (  (  (  (  2  0     -     0  )     /     5  )     -     2  )     +     1  )  ,     . MultiTask . middle )  ;", "Assert . assertTrue (  . MultiTask . last )  ;", "}", "METHOD_END"], "methodName": ["testMultiTask"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner . schedule ( new   TestTaskRunner . PreStartTask (  2  1  0  )  )  ;", "Thread . sleep (  2  1  0  )  ;", "runner . start (  )  ;", "long   startedAt    =    System . currentTimeMillis (  )  ;", "TestTaskRunner . PreStartTask . latch . await (  5  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "long   runAt    =    System . currentTimeMillis (  )  ;", "Assert . assertTrue ( TestTaskRunner . PreStartTask . first )  ;", "Assert . assertTrue (  (  ( runAt    -    startedAt )     >  =     2  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testPreStartQueueing"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner . start (  )  ;", "runner . schedule ( new    . SingleTask (  0  )  )  ;", ". SingleTask . latch . await (  5  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "Assert . assertTrue (  . SingleTask . first )  ;", "}", "METHOD_END"], "methodName": ["testSingleTask"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "runner . start (  )  ;", "runner . schedule ( new    . TriTask (  0  ,     1  0  ,     5  )  )  ;", ". TriTask . latch . await (  5  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "Assert . assertTrue (  . TriTask . first )  ;", "Assert . assertTrue (  . TriTask . middle )  ;", "Assert . assertTrue (  . TriTask . last )  ;", "}", "METHOD_END"], "methodName": ["testTriTask"], "fileName": "org.apache.hadoop.yarn.sls.scheduler.TestTaskRunner"}, {"methodBody": ["METHOD_START", "{", "hostname    =    hostname . substring (  1  )  ;", "return   hostname . split (  \"  /  \"  )  ;", "}", "METHOD_END"], "methodName": ["getRackHostName"], "fileName": "org.apache.hadoop.yarn.sls.utils.SLSUtils"}, {"methodBody": ["METHOD_START", "{", "Set < String >    nodeSet    =    new   HashSet < String >  (  )  ;", "JsonFactory   jsonF    =    new   JsonFactory (  )  ;", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "Reader   input    =    new   FileReader ( nodeFile )  ;", "try    {", "Iterator < Map >    i    =    mapper . readValues ( jsonF . createJsonParser ( input )  ,    Map . class )  ;", "while    ( isNext (  )  )     {", "Map   jsonE    =    i . next (  )  ;", "String   rack    =     \"  /  \"     +     ( jsonE . get (  \" rack \"  )  )  ;", "List   tasks    =     (  ( List )     ( jsonE . get (  \" nodes \"  )  )  )  ;", "for    ( Object   o    :    tasks )     {", "Map   jsonNode    =     (  ( Map )     ( o )  )  ;", "nodeSet . add (  (  ( rack    +     \"  /  \"  )     +     ( jsonNode . get (  \" node \"  )  )  )  )  ;", "}", "}", "}    finally    {", "input . close (  )  ;", "}", "return   nodeSet ;", "}", "METHOD_END"], "methodName": ["parseNodesFromNodeFile"], "fileName": "org.apache.hadoop.yarn.sls.utils.SLSUtils"}, {"methodBody": ["METHOD_START", "{", "Set < String >    nodeSet    =    new   HashSet < String >  (  )  ;", "Fe   fin    =    new   Fe ( jobTrace )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  \" fs . defaultFS \"  ,     \" fe :  /  /  /  \"  )  ;", "JobTraceReader   reader    =    new   JobTraceReader ( new   Path ( fin . getAbsolutePath (  )  )  ,    conf )  ;", "try    {", "LoggedJob   job    =    null ;", "whe    (  ( job    =    reader . getNext (  )  )     !  =    null )     {", "for    ( LoggedTask   mapTask    :    job . getMapTasks (  )  )     {", "LoggedTaskAttempt   taskAttempt    =    mapTask . getAttempts (  )  . get (  (  ( mapTask . getAttempts (  )  . size (  )  )     -     1  )  )  ;", "nodeSet . add ( taskAttempt . getHostName (  )  . getValue (  )  )  ;", "}", "for    ( LoggedTask   reduceTask    :    job . getReduceTasks (  )  )     {", "LoggedTaskAttempt   taskAttempt    =    reduceTask . getAttempts (  )  . get (  (  ( reduceTask . getAttempts (  )  . size (  )  )     -     1  )  )  ;", "nodeSet . add ( taskAttempt . getHostName (  )  . getValue (  )  )  ;", "}", "}", "}    finally    {", "reader . close (  )  ;", "}", "return   nodeSet ;", "}", "METHOD_END"], "methodName": ["parseNodesFromRumenTrace"], "fileName": "org.apache.hadoop.yarn.sls.utils.SLSUtils"}, {"methodBody": ["METHOD_START", "{", "Set < String >    nodeSet    =    new   HashSet < String >  (  )  ;", "JsonFactory   jsonF    =    new   JsonFactory (  )  ;", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "Reader   input    =    new   FileReader ( jobTrace )  ;", "try    {", "Iterator < Map >    i    =    mapper . readValues ( jsonF . createJsonParser ( input )  ,    Map . class )  ;", "while    ( isNext (  )  )     {", "Map   jsonE    =    i . next (  )  ;", "List   tasks    =     (  ( List )     ( jsonE . get (  \" job . tasks \"  )  )  )  ;", "for    ( Object   o    :    tasks )     {", "Map   jsonTask    =     (  ( Map )     ( o )  )  ;", "String   hostname    =    jsonTask . get (  \" container . host \"  )  . toString (  )  ;", "nodeSet . add ( hostname )  ;", "}", "}", "}    finally    {", "input . close (  )  ;", "}", "return   nodeSet ;", "}", "METHOD_END"], "methodName": ["parseNodesFromSLSTrace"], "fileName": "org.apache.hadoop.yarn.sls.utils.SLSUtils"}, {"methodBody": ["METHOD_START", "{", "String   str    =     \"  / rack 1  / node 1  \"  ;", "String [  ]    rackHostname    =     . getRackHostName ( str )  ;", "Assert . assertEquals ( rackHostname [  0  ]  ,     \" rack 1  \"  )  ;", "Assert . assertEquals ( rackHostname [  1  ]  ,     \" node 1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testGetRackHostname"], "fileName": "org.apache.hadoop.yarn.sls.utils.TestSLSUtils"}, {"methodBody": ["METHOD_START", "{", "double   jvmFreeMemoryGB ;", "double   jvmMaxMemoryGB ;", "double   jvmTotalMemoryGB ;", "if    (  (  ( jvmFreeMemoryGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . jvm . free . memory \"  )  )  )     {", "jvmFreeMemoryGauge    =    metrics . getGauges (  )  . get (  \" variable . jvm . free . memory \"  )  ;", "}", "if    (  (  ( jvmMaxMemoryGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . jvm . max . memory \"  )  )  )     {", "jvmMaxMemoryGauge    =    metrics . getGauges (  )  . get (  \" variable . jvm . max . memory \"  )  ;", "}", "if    (  (  ( jvmTotalMemoryGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . jvm . total . memory \"  )  )  )     {", "jvmTotalMemoryGauge    =    metrics . getGauges (  )  . get (  \" variable . jvm . total . memory \"  )  ;", "}", "jvmFreeMemoryGB    =     (  ( jvmFreeMemoryGauge )     =  =    null )     ?     0     :     (  (  ( Double . parseDouble ( jvmFreeMemoryGauge . getValue (  )  . toString (  )  )  )     /     1  0  2  4  )     /     1  0  2  4  )     /     1  0  2  4  ;", "jvmMaxMemoryGB    =     (  ( jvmMaxMemoryGauge )     =  =    null )     ?     0     :     (  (  ( Double . parseDouble ( jvmMaxMemoryGauge . getValue (  )  . toString (  )  )  )     /     1  0  2  4  )     /     1  0  2  4  )     /     1  0  2  4  ;", "jvmTotalMemoryGB    =     (  ( jvmTotalMemoryGauge )     =  =    null )     ?     0     :     (  (  ( Double . parseDouble ( jvmTotalMemoryGauge . getValue (  )  . toString (  )  )  )     /     1  0  2  4  )     /     1  0  2  4  )     /     1  0  2  4  ;", "String   numRunningApps ;", "String   numRunningContainers ;", "if    (  (  ( numRunningAppsGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . runninplication \"  )  )  )     {", "numRunningAppsGauge    =    metrics . getGauges (  )  . get (  \" variable . runninplication \"  )  ;", "}", "if    (  (  ( numRunningContainersGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . running . container \"  )  )  )     {", "numRunningContainersGauge    =    metrics . getGauges (  )  . get (  \" variable . running . container \"  )  ;", "}", "numRunningApps    =     (  ( numRunningAppsGauge )     =  =    null )     ?     \"  0  \"     :    numRunningAppsGauge . getValue (  )  . toString (  )  ;", "numRunningContainers    =     (  ( numRunningContainersGauge )     =  =    null )     ?     \"  0  \"     :    numRunningContainersGauge . getValue (  )  . toString (  )  ;", "double   allocatedMemoryGB ;", "double   allocatedVCoresGB ;", "double   availableMemoryGB ;", "double   availableVCoresGB ;", "if    (  (  ( allocatedMemoryGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . cluster . allocated . memory \"  )  )  )     {", "allocatedMemoryGauge    =    metrics . getGauges (  )  . get (  \" variable . cluster . allocated . memory \"  )  ;", "}", "if    (  (  ( allocatedVCoresGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . cluster . allocated . vcores \"  )  )  )     {", "allocatedVCoresGauge    =    metrics . getGauges (  )  . get (  \" variable . cluster . allocated . vcores \"  )  ;", "}", "if    (  (  ( availableMemoryGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . cluster . available . memory \"  )  )  )     {", "availableMemoryGauge    =    metrics . getGauges (  )  . get (  \" variable . cluster . available . memory \"  )  ;", "}", "if    (  (  ( availableVCoresGauge )     =  =    null )     &  &     ( metrics . getGauges (  )  . containsKey (  \" variable . cluster . available . vcores \"  )  )  )     {", "availableVCoresGauge    =    metrics . getGauges (  )  . get (  \" variable . cluster . available . vcores \"  )  ;", "}", "allocatedMemoryGB    =     (  ( allocatedMemoryGauge )     =  =    null )     ?     0     :     ( Double . parseDouble ( allocatedMemoryGauge . getValue (  )  . toString (  )  )  )     /     1  0  2  4  ;", "allocatedVCoresGB    =     (  ( allocatedVCoresGauge )     =  =    null )     ?     0     :    Double . parseDouble ( allocatedVCoresGauge . getValue (  )  . toString (  )  )  ;", "availableMemoryGB    =     (  ( availableMemoryGauge )     =  =    null )     ?     0     :     ( Double . parseDouble ( availableMemoryGauge . getValue (  )  . toString (  )  )  )     /     1  0  2  4  ;", "availableVCoresGB    =     (  ( availableVCoresGauge )     =  =    null )     ?     0     :    Double . parseDouble ( availableVCoresGauge . getValue (  )  . toString (  )  )  ;", "double   allocateTimecost ;", "double   handleTimecost ;", "if    (  (  ( allocateTimecostHistogram )     =  =    null )     &  &     ( metrics . getHistograms (  )  . containsKey (  \" sampler . scheduler . operation . allocate . timecost \"  )  )  )     {", "allocateTimecostHistogram    =    metrics . getHistograms (  )  . get (  \" sampler . scheduler . operation . allocate . timecost \"  )  ;", "}", "if    (  (  ( handleTimecostHistogram )     =  =    null )     &  &     ( metrics . getHistograms (  )  . containsKey (  \" sampler . scheduler . operation . handle . timecost \"  )  )  )     {", "handleTimecostHistogram    =    metrics . getHistograms (  )  . get (  \" sampler . scheduler . operation . handle . timecost \"  )  ;", "}", "allocateTimecost    =     (  ( allocateTimecostHistogram )     =  =    null )     ?     0  .  0     :     ( allocateTimecostHistogram . getSnapshot (  )  . getMean (  )  )     /     1  0  0  0  0  0  0  ;", "handleTimecost    =     (  ( handleTimecostHistogram )     =  =    null )     ?     0  .  0     :     ( handleTimecostHistogram . getSnapshot (  )  . getMean (  )  )     /     1  0  0  0  0  0  0  ;", "Map < SchedulerEventType ,    Double >    handleOperTimecostMap    =    new   HashMap < SchedulerEventType ,    Double >  (  )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "String   key    =     (  \" sampler . scheduler . operation . handle .  \"     +    e )     +     \"  . timecost \"  ;", "if    (  (  !  ( handleOperTimecostHistogramMap . containsKey ( e )  )  )     &  &     ( metrics . getHistograms (  )  . containsKey ( key )  )  )     {", "handleOperTimecostHistogramMap . put ( e ,    metrics . getHistograms (  )  . get ( key )  )  ;", "}", "double   timecost    =     ( handleOperTimecostHistogramMap . containsKey ( e )  )     ?     ( handleOperTimecostHistogramMap . get ( e )  . getSnapshot (  )  . getMean (  )  )     /     1  0  0  0  0  0  0     :     0  ;", "handleOperTimecostMap . put ( e ,    timecost )  ;", "}", "Map < String ,    Double >    queueAllocatedMemoryMap    =    new   HashMap < String ,    Double >  (  )  ;", "Map < String ,    Long >    queueAllocatedVCoresMap    =    new   HashMap < String ,    Long >  (  )  ;", "for    ( String   queue    :    wrapper . getQueueSet (  )  )     {", "String   key    =     (  \" counter . queue .  \"     +    queue )     +     \"  . allocated . memory \"  ;", "if    (  (  !  ( queueAllocatedMemoryCounterMap . containsKey ( queue )  )  )     &  &     ( metrics . getCounters (  )  . containsKey ( key )  )  )     {", "queueAllocatedMemoryCounterMap . put ( queue ,    metrics . getCounters (  )  . get ( key )  )  ;", "}", "double   queueAllocatedMemoryGB    =     ( queueAllocatedMemoryCounterMap . containsKey ( queue )  )     ?     ( queueAllocatedMemoryCounterMap . get ( queue )  . getCount (  )  )     /     1  0  2  4  .  0     :     0  ;", "queueAllocatedMemoryMap . put ( queue ,    queueAllocatedMemoryGB )  ;", "key    =     (  \" counter . queue .  \"     +    queue )     +     \"  . allocated . cores \"  ;", "if    (  (  !  ( queueAllocatedVCoresCounterMap . containsKey ( queue )  )  )     &  &     ( metrics . getCounters (  )  . containsKey ( key )  )  )     {", "queueAllocatedVCoresCounterMap . put ( queue ,    metrics . getCounters (  )  . get ( key )  )  ;", "}", "long   queueAllocatedVCores    =     ( queueAllocatedVCoresCounterMap . containsKey ( queue )  )     ?    queueAllocatedVCoresCounterMap . get ( queue )  . getCount (  )     :     0  ;", "queueAllocatedVCoresMap . put ( queue ,    queueAllocatedVCores )  ;", "}", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append (  \"  {  \"  )  ;", "sb . append (  \"  \\  \" time \\  \"  :  \"  )  . append ( System . currentTimeMillis (  )  )  . append (  \"  ,  \\  \" jvm . free . memory \\  \"  :  \"  )  . append ( jvmFreeMemoryGB )  . append (  \"  ,  \\  \" jvm . max . memory \\  \"  :  \"  )  . append ( jvmMaxMemoryGB )  . append (  \"  ,  \\  \" jvm . total . memory \\  \"  :  \"  )  . append ( jvmTotalMemoryGB )  . append (  \"  ,  \\  \" runninplications \\  \"  :  \"  )  . append ( numRunningApps )  . append (  \"  ,  \\  \" running . containers \\  \"  :  \"  )  . append ( numRunningContainers )  . append (  \"  ,  \\  \" cluster . allocated . memory \\  \"  :  \"  )  . append ( allocatedMemoryGB )  . append (  \"  ,  \\  \" cluster . allocated . vcores \\  \"  :  \"  )  . append ( allocatedVCoresGB )  . append (  \"  ,  \\  \" cluster . available . memory \\  \"  :  \"  )  . append ( availableMemoryGB )  . append (  \"  ,  \\  \" cluster . available . vcores \\  \"  :  \"  )  . append ( availableVCoresGB )  ;", "for    ( String   queue    :    wrapper . getQueueSet (  )  )     {", "sb . append (  \"  ,  \\  \" queue .  \"  )  . append ( queue )  . append (  \"  . allocated . memory \\  \"  :  \"  )  . append ( queueAllocatedMemoryMap . get ( queue )  )  ;", "sb . append (  \"  ,  \\  \" queue .  \"  )  . append ( queue )  . append (  \"  . allocated . vcores \\  \"  :  \"  )  . append ( queueAllocatedVCoresMap . get ( queue )  )  ;", "}", "sb . append (  \"  ,  \\  \" scheduler . allocate . timecost \\  \"  :  \"  )  . append ( allocateTimecost )  ;", "sb . append (  \"  ,  \\  \" scheduler . handle . timecost \\  \"  :  \"  )  . append ( handleTimecost )  ;", "for    ( SchedulerEventType   e    :    SchedulerEventType . values (  )  )     {", "sb . append (  \"  ,  \\  \" scheduler . handle -  \"  )  . append ( e )  . append (  \"  . timecost \\  \"  :  \"  )  . append ( handleOperTimecostMap . get ( e )  )  ;", "}", "sb . append (  \"  }  \"  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["generateRealTimeTrackingMetrics"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "response . setContentType (  \" text / json \"  )  ;", "response . setStatus ( SC _ OK )  ;", "response . getWriter (  )  . println ( generateRealTimeTrackingMetrics (  )  )  ;", "(  ( Request )     ( request )  )  . setHandled ( true )  ;", "}", "METHOD_END"], "methodName": ["printJsonMetrics"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "response . setContentType (  \" text / json \"  )  ;", "response . setStatus ( SC _ OK )  ;", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "if    (  ( sdulerMetrics )    instanceof   FairSdulerMetrics )     {", "String   para    =    request . getParameter (  \" t \"  )  ;", "if    ( para . startsWith (  \" Job    \"  )  )     {", "String   appId    =    para . substring (  \" Job    \"  . length (  )  )  ;", "sb . append (  \"  {  \"  )  ;", "sb . append (  \"  \\  \" time \\  \"  :     \"  )  . append ( System . currentTimeMillis (  )  )  . append (  \"  ,  \"  )  ;", "sb . append (  \"  \\  \" appId \\  \"  :     \\  \"  \"  )  . append ( appId )  . append (  \"  \\  \"  \"  )  ;", "for    ( String   metric    :    this . sdulerMetrics . getAppTrackedMetrics (  )  )     {", "String   key    =     (  (  \" variable . app .  \"     +    appId )     +     \"  .  \"  )     +    metric ;", "sb . append (  \"  ,  \\  \"  \"  )  . append ( metric )  . append (  \"  \\  \"  :     \"  )  ;", "if    ( metrics . getGauges (  )  . containsKey ( key )  )     {", "double   memoryGB    =     ( Double . parseDouble ( metrics . getGauges (  )  . get ( key )  . getValue (  )  . toString (  )  )  )     /     1  0  2  4  ;", "sb . append ( memoryGB )  ;", "} else    {", "sb . append (  (  -  1  )  )  ;", "}", "}", "sb . append (  \"  }  \"  )  ;", "} else", "if    ( para . startsWith (  \" Queue    \"  )  )     {", "String   queueName    =    para . substring (  \" Queue    \"  . length (  )  )  ;", "sb . append (  \"  {  \"  )  ;", "sb . append (  \"  \\  \" time \\  \"  :     \"  )  . append ( System . currentTimeMillis (  )  )  . append (  \"  ,  \"  )  ;", "sb . append (  \"  \\  \" queueName \\  \"  :     \\  \"  \"  )  . append ( queueName )  . append (  \"  \\  \"  \"  )  ;", "for    ( String   metric    :    this . sdulerMetrics . getQueueTrackedMetrics (  )  )     {", "String   key    =     (  (  \" variable . queue .  \"     +    queueName )     +     \"  .  \"  )     +    metric ;", "sb . append (  \"  ,  \\  \"  \"  )  . append ( metric )  . append (  \"  \\  \"  :     \"  )  ;", "if    ( metrics . getGauges (  )  . containsKey ( key )  )     {", "double   memoryGB    =     ( Double . parseDouble ( metrics . getGauges (  )  . get ( key )  . getValue (  )  . toString (  )  )  )     /     1  0  2  4  ;", "sb . append ( memoryGB )  ;", "} else    {", "sb . append (  (  -  1  )  )  ;", "}", "}", "sb . append (  \"  }  \"  )  ;", "}", "}", "String   output    =    sb . toString (  )  ;", "if    ( output . isEmpty (  )  )     {", "output    =     \"  [  ]  \"  ;", "}", "response . getWriter (  )  . println ( output )  ;", "(  ( Request )     ( request )  )  . setHandled ( true )  ;", "}", "METHOD_END"], "methodName": ["printJsonTrack"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "response . setContentType (  \" text / html \"  )  ;", "response . setStatus ( SC _ OK )  ;", "String   simulateInfo ;", "if    ( Runner . simulateInfoMap . isEmpty (  )  )     {", "String   empty    =     \"  < tr >  < td   colspan =  '  2  '    align =  ' center '  >  \"     +     \" No   information   available <  / td >  <  / tr >  \"  ;", "simulateInfo    =    MessageFormat . format ( simulateInfoTemplate ,    empty )  ;", "} else    {", "StringBuilder   info    =    new   StringBuilder (  )  ;", "for    ( Map . Entry < String ,    Object >    entry    :    Runner . simulateInfoMap . entrySet (  )  )     {", "info . append (  \"  < tr >  \"  )  ;", "info . append (  \"  < td   class =  ' td 1  '  >  \"  )  . append ( entry . getKey (  )  )  . append (  \"  <  / td >  \"  )  ;", "info . append (  \"  < td   class =  ' td 2  '  >  \"  )  . append ( entry . getValue (  )  )  . append (  \"  <  / td >  \"  )  ;", "info . append (  \"  <  / tr >  \"  )  ;", "}", "simulateInfo    =    MessageFormat . format ( simulateInfoTemplate ,    info . toString (  )  )  ;", "}", "response . getWriter (  )  . println ( simulateInfo )  ;", "(  ( Request )     ( request )  )  . setHandled ( true )  ;", "}", "METHOD_END"], "methodName": ["printPageIndex"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "response . setContentType (  \" text / html \"  )  ;", "response . setStatus ( SC _ OK )  ;", "Set < String >    queues    =    wper . getQueueSet (  )  ;", "StringBuilder   queueInfo    =    new   StringBuilder (  )  ;", "int   i    =     0  ;", "for    ( String   queue    :    queues )     {", "queueInfopend (  \" legends [  4  ]  [  \"  ) pend ( i ) pend (  \"  ]     =     ' queue .  \"  ) pend ( queue ) pend (  \"  . allocated . memory '  ;  \"  )  ;", "queueInfopend (  \" legends [  5  ]  [  \"  ) pend ( i ) pend (  \"  ]     =     ' queue .  \"  ) pend ( queue ) pend (  \"  . allocated . vcores '  ;  \"  )  ;", "i +  +  ;", "}", "String   simulateInfo    =    MessageFormat . format ( simulateTemplate ,    queueInfo . toString (  )  ,    timeunitLabel ,     (  \"  \"     +    timeunit )  ,     (  \"  \"     +     ( ajaxUpdateTimeMS )  )  )  ;", "response . getWriter (  )  . println ( simulateInfo )  ;", "(  ( Request )     ( request )  )  . setHandled ( true )  ;", "}", "METHOD_END"], "methodName": ["printPageSimulate"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "response . setContentType (  \" text / html \"  )  ;", "response . setStatus ( SC _ OK )  ;", "StringBuilder   trackedQueueInfo    =    new   StringBuilder (  )  ;", "Set < String >    trackedQueues    =    wper . getQueueSet (  )  ;", "for    ( String   queue    :    trackedQueues )     {", "trackedQueueInfopend (  \"  < option   value =  ' Queue    \"  ) pend ( queue ) pend (  \"  '  >  \"  ) pend ( queue ) pend (  \"  <  / option >  \"  )  ;", "}", "StringBuilder   trackedAppInfo    =    new   StringBuilder (  )  ;", "Set < String >    trackedApps    =    wper . getTrackedAppSet (  )  ;", "for    ( String   job    :    trackedApps )     {", "trackedAppInfopend (  \"  < option   value =  ' Job    \"  ) pend ( job ) pend (  \"  '  >  \"  ) pend ( job ) pend (  \"  <  / option >  \"  )  ;", "}", "String   trackInfo    =    MessageFormat . format ( trackTemplate ,    trackedQueueInfo . toString (  )  ,    trackedAppInfo . toString (  )  ,    timeunitLabel ,     (  \"  \"     +    timeunit )  ,     (  \"  \"     +     ( ajaxUpdateTimeMS )  )  )  ;", "response . getWriter (  )  . println ( trackInfo )  ;", "(  ( Request )     ( request )  )  . setHandled ( true )  ;", "}", "METHOD_END"], "methodName": ["printPageTrack"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "final   ResourceHandler   staticHandler    =    new   ResourceHandler (  )  ;", "staticHandler . setResourceBase (  \" html \"  )  ;", "Handlerndler    =    new   AbstractHandler (  )     {", "@ Override", "public   voidndle ( String   target ,    HttpServletRequest   request ,    HttpServletResponse   response ,    int   dispatch )     {", "try    {", "int   timeunit    =     1  0  0  0  ;", "String   timeunitLabel    =     \" second \"  ;", "if    (  (  ( request . getParameter (  \" u \"  )  )     !  =    null )     &  &     ( request . getParameter (  \" u \"  )  . equalsIgnoreCase (  \" m \"  )  )  )     {", "timeunit    =     1  0  0  0     *     6  0  ;", "timeunitLabel    =     \" minute \"  ;", "}", "if    ( target . equals (  \"  /  \"  )  )     {", "printPageIndex ( request ,    response )  ;", "} else", "if    ( target . equals (  \"  / simulate \"  )  )     {", "printPageSimulate ( request ,    response ,    timeunit ,    timeunitLabel )  ;", "} else", "if    ( target . equals (  \"  / track \"  )  )     {", "printPageTrack ( request ,    response ,    timeunit ,    timeunitLabel )  ;", "} else", "if    (  ( target . startsWith (  \"  / js \"  )  )     |  |     ( target . startsWith (  \"  / css \"  )  )  )     {", "response . setracterEncoding (  \" utf -  8  \"  )  ;", "staticHandlerndle ( target ,    request ,    response ,    dispatch )  ;", "} else", "if    ( target . equals (  \"  / simulateMetrics \"  )  )     {", "printJsonMetrics ( request ,    response )  ;", "} else", "if    ( target . equals (  \"  / trackMetrics \"  )  )     {", "printJsonTrack ( request ,    response )  ;", "}", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "}", "}", "}  ;", "server    =    new   Server ( port )  ;", "server . setHandlerndler )  ;", "server . start (  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "if    ( erver )     !  =    null )     {", "ervertop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.sls.web.SLSWebApp"}, {"methodBody": ["METHOD_START", "{", "String   simulateInfoTemplate    =    FileUtils . readFileToString ( new   File (  \" src / main / html / simulate . info . html . template \"  )  )  ;", "SLSRunner . simulateInfoMap . put (  \" Number   of   racks \"  ,     1  0  )  ;", "SLSRunner . simulateInfoMap . put (  \" Number   of   nodes \"  ,     1  0  0  )  ;", "SLSRunner . simulateInfoMap . put (  \" Node   memory    ( MB )  \"  ,     1  0  2  4  )  ;", "SLSRunner . simulateInfoMap . put (  \" Node   VCores \"  ,     1  )  ;", "SLSRunner . simulateInfoMap . put (  \" Number   of   applications \"  ,     1  0  0  )  ;", "SLSRunner . simulateInfoMap . put (  \" Number   of   tasks \"  ,     1  0  0  0  )  ;", "SLSRunner . simulateInfoMap . put (  \" Average   tasks   per   applicaion \"  ,     1  0  )  ;", "SLSRunner . simulateInfoMap . put (  \" Number   of   queues \"  ,     4  )  ;", "SLSRunner . simulateInfoMap . put (  \" Average   applications   per   queue \"  ,     2  5  )  ;", "SLSRunner . simulateInfoMap . put (  \" Estimated   simulate   time    ( s )  \"  ,     1  0  0  0  0  )  ;", "StringBuilder   info    =    new   StringBuilder (  )  ;", "for    ( Map . Entry < String ,    Object >    entry    :    SLSRunner . simulateInfoMap . entrySet (  )  )     {", "info . append (  \"  < tr >  \"  )  ;", "info . append (  (  (  \"  < td   class =  ' td 1  '  >  \"     +     ( entry . getKey (  )  )  )     +     \"  <  / td >  \"  )  )  ;", "info . append (  (  (  \"  < td   class =  ' td 2  '  >  \"     +     ( entry . getValue (  )  )  )     +     \"  <  / td >  \"  )  )  ;", "info . append (  \"  <  / tr >  \"  )  ;", "}", "String   simulateInfo    =    MessageFormat . format ( simulateInfoTemplate ,    info . toString (  )  )  ;", "Assert . assertTrue (  \" The   simulate   info   html   page   should   not   be   empty \"  ,     (  ( simulateInfo . length (  )  )     >     0  )  )  ;", "for    ( Map . Entry < String ,    Object >    entry    :    SLSRunner . simulateInfoMap . entrySet (  )  )     {", "Assert . assertTrue (  (  (  \" The   simulate   info   html   page   should   have   information    \"     +     \" of    \"  )     +     ( entry . getKey (  )  )  )  ,    simulateInfo . contains (  (  (  (  (  \"  < td   class =  ' td 1  '  >  \"     +     ( entry . getKey (  )  )  )     +     \"  <  / td >  < td   class =  ' td 2  '  >  \"  )     +     ( entry . getValue (  )  )  )     +     \"  <  / td >  \"  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSimulateInfoPageHtmlTemplate"], "fileName": "org.apache.hadoop.yarn.sls.web.TestSLSWebApp"}, {"methodBody": ["METHOD_START", "{", "String   simulateTemplate    =    FileUtils . readFileToString ( new   File (  \" src / main / html / simulate . html . template \"  )  )  ;", "Set < String >    queues    =    new   HashSet < String >  (  )  ;", "queues . add (  \" sls _ queue _  1  \"  )  ;", "queues . add (  \" sls _ queue _  2  \"  )  ;", "queues . add (  \" sls _ queue _  3  \"  )  ;", "String   queueInfo    =     \"  \"  ;", "int   i    =     0  ;", "for    ( String   queue    :    queues )     {", "queueInfo    +  =     (  (  (  \" legends [  4  ]  [  \"     +    i )     +     \"  ]     =     ' queue \"  )     +    queue )     +     \"  . allocated . memory '  \"  ;", "queueInfo    +  =     (  (  (  \" legends [  5  ]  [  \"     +    i )     +     \"  ]     =     ' queue \"  )     +    queue )     +     \"  . allocated . vcores '  \"  ;", "i +  +  ;", "}", "String   simulateInfo    =    MessageFormat . format ( simulateTemplate ,    queueInfo ,     \" s \"  ,     1  0  0  0  ,     1  0  0  0  )  ;", "Assert . assertTrue (  \" The   simulate   page   html   page   should   not   be   empty \"  ,     (  ( simulateInfo . length (  )  )     >     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimulatePageHtmlTemplate"], "fileName": "org.apache.hadoop.yarn.sls.web.TestSLSWebApp"}, {"methodBody": ["METHOD_START", "{", "String   trackTemplate    =    FileUtils . readFileToString ( new   File (  \" src / main / html / track . html . template \"  )  )  ;", "String   trackedQueueInfo    =     \"  \"  ;", "Set < String >    trackedQueues    =    new   HashSet < String >  (  )  ;", "trackedQueues . add (  \" sls _ queue _  1  \"  )  ;", "trackedQueues . add (  \" sls _ queue _  2  \"  )  ;", "trackedQueues . add (  \" sls _ queue _  3  \"  )  ;", "for    ( String   queue    :    trackedQueues )     {", "trackedQueueInfo    +  =     (  (  (  \"  < option   value =  ' Queue    \"     +    queue )     +     \"  '  >  \"  )     +    queue )     +     \"  <  / option >  \"  ;", "}", "String   trackedAppInfo    =     \"  \"  ;", "Set < String >    trackedApps    =    new   HashSet < String >  (  )  ;", "trackedApps . add (  \" app _  1  \"  )  ;", "trackedApps . add (  \" app _  2  \"  )  ;", "for    ( String   job    :    trackedApps )     {", "trackedAppInfo    +  =     (  (  (  \"  < option   value =  ' Job    \"     +    job )     +     \"  '  >  \"  )     +    job )     +     \"  <  / option >  \"  ;", "}", "String   trackInfo    =    MessageFormat . format ( trackTemplate ,    trackedQueueInfo ,    trackedAppInfo ,     \" s \"  ,     1  0  0  0  ,     1  0  0  0  )  ;", "Assert . assertTrue (  \" The   queue / app   tracking   html   page   should   not   be   empty \"  ,     (  ( trackInfo . length (  )  )     >     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testTrackPageHtmlTemplate"], "fileName": "org.apache.hadoop.yarn.sls.web.TestSLSWebApp"}]