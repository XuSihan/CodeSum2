[{"methodBody": ["METHOD_START", "{", "InetSocketAddress   address    =    new   InetSocketAddress (  0  )  ;", "Configuration   configuration    =    new   Configuration (  )  ;", "ResourceTracker   instance    =    new    . ResourceTrackerTestImpl (  )  ;", ". server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( ResourceTracker . class ,    instance ,    address ,    configuration ,    null ,     1  )  ;", ". server . start (  )  ;", ". client    =     (  ( ResourceTracker )     ( RpcClientFactoryPBImpl . get (  )  . getClient ( ResourceTracker . class ,     1  ,    NetUtils . getConnectAddress (  . server )  ,    configuration )  )  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.TestResourceTrackerPBClientImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestResourceTrackerPBClientImpl . server )     !  =    null )     {", "TestResourceTrackerPBClientImpl . server . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.TestResourceTrackerPBClientImpl"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatRequest   request    =    TestResourceTrackerPBClientImpl . recordFactory . newRecordInstance ( NodeHeartbeatRequest . class )  ;", "assertNotNull ( TestResourceTrackerPBClientImpl . client . nodeHeartbeat ( request )  )  ;", "TestResourceTrackerPBClientImpl . ResourceTrackerTestImpl . exception    =    true ;", "try    {", "TestResourceTrackerPBClientImpl . client . nodeHeartbeat ( request )  ;", "fail (  \" there      should   be   YarnException \"  )  ;", "}    catch    ( YarnException   e )     {", "assertTrue ( e . getMessage (  )  . startsWith (  \" testMessage \"  )  )  ;", "}    finally    {", "TestResourceTrackerPBClientImpl . ResourceTrackerTestImpl . exception    =    false ;", "}", "}", "METHOD_END"], "methodName": ["testNodeHeartbeat"], "fileName": "org.apache.hadoop.yarn.TestResourceTrackerPBClientImpl"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerRequest   request    =    TestResourceTrackerPBClientImpl . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "assertNotNull ( TestResourceTrackerPBClientImpl . client . registerNodeManager ( request )  )  ;", "TestResourceTrackerPBClientImpl . ResourceTrackerTestImpl . exception    =    true ;", "try    {", "TestResourceTrackerPBClientImpl . client . registerNodeManager ( request )  ;", "fail (  \" there      should   be   YarnException \"  )  ;", "}    catch    ( YarnException   e )     {", "assertTrue ( e . getMessage (  )  . startsWith (  \" testMessage \"  )  )  ;", "}    finally    {", "TestResourceTrackerPBClientImpl . ResourceTrackerTestImpl . exception    =    false ;", "}", "}", "METHOD_END"], "methodName": ["testResourceTrackerPBClientImpl"], "fileName": "org.apache.hadoop.yarn.TestResourceTrackerPBClientImpl"}, {"methodBody": ["METHOD_START", "{", "testPbServerFactory (  )  ;", "testPbClientFactory (  )  ;", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.TestYSCRPCFactories"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    new   InetSocketAddress (  0  )  ;", "System . err . println (  (  ( addr . getHostName (  )  )     +     ( addr . getPort (  )  )  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "ResourceTracker   instance    =    new    . ResourceTrackerTestImpl (  )  ;", "Server   server    =    null ;", "try    {", "server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( ResourceTracker . class ,    instance ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "System . err . println ( server . getListenerAddress (  )  )  ;", "System . err . println ( NetUtils . getConnectAddress ( server )  )  ;", "ResourceTracker   client    =    null ;", "try    {", "client    =     (  ( ResourceTracker )     ( RpcClientFactoryPBImpl . get (  )  . getClient ( ResourceTracker . class ,     1  ,    NetUtils . getConnectAddress ( server )  ,    conf )  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   client \"  )  ;", "}", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   server \"  )  ;", "}    finally    {", "server . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPbClientFactory"], "fileName": "org.apache.hadoop.yarn.TestYSCRPCFactories"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    new   InetSocketAddress (  0  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "ResourceTracker   instance    =    new    . ResourceTrackerTestImpl (  )  ;", "Server   server    =    null ;", "try    {", "server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( ResourceTracker . class ,    instance ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   server \"  )  ;", "}    finally    {", "server . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPbServerFactory"], "fileName": "org.apache.hadoop.yarn.TestYSCRPCFactories"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   pbRecordFactory    =    RecordFactoryPBImpl . get (  )  ;", "try    {", "NodeHeartbeatRequest   request    =    pbRecordFactory . newRecordInstance ( NodeHeartbeatRequest . class )  ;", "Assert . assertEquals ( NodeHeartbeatRequestPBImpl . class ,    request . getClass (  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   crete   record \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPbRecordFactory"], "fileName": "org.apache.hadoop.yarn.TestYSCRecordFactory"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   result    =    ApplicationAttemptIdPBImpl . newInstance ( getApplicationId ( appAttemptId )  ,    appAttemptId )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "ApplicationIdPBImpl   appId    =    new   ApplicationIdPBImpl (  )     {", "public   ApplicationIdPBImpl   setParameters ( int   id ,    long   timamp )     {", "setClusterTimamp ( timamp )  ;", "setId ( id )  ;", "build (  )  ;", "return   this ;", "}", "}  . setParameters ( applicationId ,     1  0  0  0  )  ;", "return   new   ApplicationIdPBImpl ( appId . getProto (  )  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    ContainerIdPBImpl . newInstance ( getApplicationAttemptId ( appAttemptId )  ,    containerID )  ;", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   status    =    TestYarnServerApiClasses . recordFactory . newRecordInstance ( ContainerStatus . class )  ;", "status . setContainerId ( getContainerId ( containerID ,    appAttemptId )  )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["getContainerStatus"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "MasterKey   key    =    TestYarnServerApiClasses . recordFactory . newRecordInstance ( MasterKey . class )  ;", "key . setBytes ( ByteBuffer . allocate (  0  )  )  ;", "key . setKeyId (  1  )  ;", "return   key ;", "}", "METHOD_END"], "methodName": ["getMasterKey"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "NodeHealthStatus   healStatus    =    TestYarnServerApiClasses . recordFactory . newRecordInstance ( NodeHealthStatus . class )  ;", "healStatus . setHealthReport (  \" healthReport \"  )  ;", "healStatus . setIsNodeHealthy ( true )  ;", "healStatus . setLastHealthReportTime (  1  0  0  0  )  ;", "return   healStatus ;", "}", "METHOD_END"], "methodName": ["getNodeHealthStatus"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "return   NodeId . newInstance (  \" localhost \"  ,     9  0  9  0  )  ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "NodeStatus   status    =    TestYarnServerApiClasses . recordFactory . newRecordInstance ( NodeStatus . class )  ;", "status . setContainersStatuses ( new   ArrayList < ContainerStatus >  (  )  )  ;", "status . setKeepAliveApplications ( new   ArrayList < ApplicationId >  (  )  )  ;", "status . setNodeHealthStatus ( getNodeHealthStatus (  )  )  ;", "status . setNodeId ( getNodeId (  )  )  ;", "status . setResponseId (  1  )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["getNodeStatus"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "MasterKeyPBImpl   original    =    new   MasterKeyPBImpl (  )  ;", "original . setBytes ( ByteBuffer . allocate (  0  )  )  ;", "original . setKeyId (  1  )  ;", "MasterKeyPBImpl   copy    =    new   MasterKeyPBImpl ( original . getProto (  )  )  ;", "rtEquals (  1  ,    copy . getKeyId (  )  )  ;", "rtTrue ( original . equals ( copy )  )  ;", "rtEquals ( original . hashCode (  )  ,    copy . hashCode (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMasterKeyPBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatRequestPBImpl   original    =    new   NodeHeartbeatRequestPBImpl (  )  ;", "original . setLastKnownContainerTokenMasterKey ( getMasterKey (  )  )  ;", "original . setLastKnownNMTokenMasterKey ( getMasterKey (  )  )  ;", "original . setNodeStatus ( getNodeStatus (  )  )  ;", "NodeHeartbeatRequestPBImpl   copy    =    new   NodeHeartbeatRequestPBImpl ( original . getProto (  )  )  ;", "rtEquals (  1  ,    copy . getLastKnownContainerTokenMasterKey (  )  . getKeyId (  )  )  ;", "rtEquals (  1  ,    copy . getLastKnownNMTokenMasterKey (  )  . getKeyId (  )  )  ;", "rtEquals (  \" localhost \"  ,    copy . getNodeStatus (  )  . getNodeId (  )  . getHost (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeHeartbeatRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatResponsePBImpl   original    =    new   NodeHeartbeatResponsePBImpl (  )  ;", "original . setDiagnosticsMessage (  \" testDiagnosticMessage \"  )  ;", "original . setContainerTokenMasterKey ( getMasterKey (  )  )  ;", "original . setNMTokenMasterKey ( getMasterKey (  )  )  ;", "original . setNextHeartBeatInterval (  1  0  0  0  )  ;", "original . setNodeAction ( NodeAction . NORMAL )  ;", "original . setResponseId (  1  0  0  )  ;", "NodeHeartbeatResponsePBImpl   copy    =    new   NodeHeartbeatResponsePBImpl ( original . getProto (  )  )  ;", "rtEquals (  1  0  0  ,    copy . getResponseId (  )  )  ;", "rtEquals ( NodeAction . NORMAL ,    copy . getNodeAction (  )  )  ;", "rtEquals (  1  0  0  0  ,    copy . getNextHeartBeatInterval (  )  )  ;", "rtEquals (  1  ,    copy . getContainerTokenMasterKey (  )  . getKeyId (  )  )  ;", "rtEquals (  1  ,    copy . getNMTokenMasterKey (  )  . getKeyId (  )  )  ;", "rtEquals (  \" testDiagnosticMessage \"  ,    copy . getDiagnosticsMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeHeartbeatResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "NodeStatusPBImpl   original    =    new   NodeStatusPBImpl (  )  ;", "original . setContainersStatuses ( Arrays . asList ( getContainerStatus (  1  ,     2  ,     1  )  ,    getContainerStatus (  2  ,     3  ,     1  )  )  )  ;", "original . setKeepAliveApplications ( Arrays . asList ( getApplicationId (  3  )  ,    getApplicationId (  4  )  )  )  ;", "original . setNodeHealthStatus ( getNodeHealthStatus (  )  )  ;", "original . setNodeId ( getNodeId (  )  )  ;", "original . setResponseId (  1  )  ;", "NodeStatusPBImpl   copy    =    new   NodeStatusPBImpl ( original . getProto (  )  )  ;", "rtEquals (  3  ,    copy . getContainersStatuses (  )  . get (  1  )  . getContainerId (  )  . getId (  )  )  ;", "rtEquals (  3  ,    copy . getKeepAliveApplications (  )  . get (  0  )  . getId (  )  )  ;", "rtEquals (  1  0  0  0  ,    copy . getNodeHealthStatus (  )  . getLastHealthReportTime (  )  )  ;", "rtEquals (  9  0  9  0  ,    copy . getNodeId (  )  . getPort (  )  )  ;", "rtEquals (  1  ,    copy . getResponseId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeStatusPBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerRequestPBImpl   original    =    new   RegisterNodeManagerRequestPBImpl (  )  ;", "original . setHttpPort (  8  0  8  0  )  ;", "original . setNodeId ( getNodeId (  )  )  ;", "Resource   resource    =     . recordFactory . newRecordInstance ( Resource . class )  ;", "resource . setMemory (  1  0  0  0  0  )  ;", "resource . setVirtualCores (  2  )  ;", "original . setResource ( resource )  ;", "RegisterNodeManagerRequestPBImpl   copy    =    new   RegisterNodeManagerRequestPBImpl ( original . getProto (  )  )  ;", "assertEquals (  8  0  8  0  ,    copy . getHttpPort (  )  )  ;", "assertEquals (  9  0  9  0  ,    copy . getNodeId (  )  . getPort (  )  )  ;", "assertEquals (  1  0  0  0  0  ,    copy . getResource (  )  . getMemory (  )  )  ;", "assertEquals (  2  ,    copy . getResource (  )  . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegisterNodeManagerRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerResponsePBImpl   original    =    new   RegisterNodeManagerResponsePBImpl (  )  ;", "original . setContainerTokenMasterKey ( getMasterKey (  )  )  ;", "original . setNMTokenMasterKey ( getMasterKey (  )  )  ;", "original . setNodeAction ( NodeAction . NORMAL )  ;", "original . setDiagnosticsMessage (  \" testDiagnosticMessage \"  )  ;", "RegisterNodeManagerResponsePBImpl   copy    =    new   RegisterNodeManagerResponsePBImpl ( original . getProto (  )  )  ;", "rtEquals (  1  ,    copy . getContainerTokenMasterKey (  )  . getKeyId (  )  )  ;", "rtEquals (  1  ,    copy . getNMTokenMasterKey (  )  . getKeyId (  )  )  ;", "rtEquals ( NodeAction . NORMAL ,    copy . getNodeAction (  )  )  ;", "rtEquals (  \" testDiagnosticMessage \"  ,    copy . getDiagnosticsMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegisterNodeManagerResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "SerializedExceptionPBImpl   original    =    new   SerializedExceptionPBImpl (  )  ;", "original . init (  \" testMessage \"  )  ;", "SerializedExceptionPBImpl   copy    =    new   SerializedExceptionPBImpl ( original . getProto (  )  )  ;", "rtEquals (  \" testMessage \"  ,    copy . getMessage (  )  )  ;", "original    =    new   SerializedExceptionPBImpl (  )  ;", "original . init (  \" testMessage \"  ,    new   Throwable ( new   Throwable (  \" parent \"  )  )  )  ;", "copy    =    new   SerializedExceptionPBImpl ( original . getProto (  )  )  ;", "rtEquals (  \" testMessage \"  ,    copy . getMessage (  )  )  ;", "rtEquals (  \" parent \"  ,    copy . getCause (  )  . getMessage (  )  )  ;", "rtTrue ( copy . getRemoteTrace (  )  . startsWith (  \" Throwable :    Throwable :    parent \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSerializedExceptionPBImpl"], "fileName": "org.apache.hadoop.yarn.TestYarnServerApiClasses"}, {"methodBody": ["METHOD_START", "{", "File   tmpFile    =    File . createTempFile (  \" test \"  ,     \"  . junit \"  ,    parentDir )  ;", "File   tmpDir    =    new   File (  ( tmpFile    +     \"  . dir \"  )  )  ;", "Assert . assertFalse ( tmpDir . exists (  )  )  ;", "Assert . assertTrue ( tmpDir . mkdirs (  )  )  ;", "return   tmpDir ;", "}", "METHOD_END"], "methodName": ["createTmpDir"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "Socket   sock    =    new   Socket ( host ,    port )  ;", "BufferedReader   reader    =    null ;", "try    {", "OutputStream   outstream    =    sock . getOutputStream (  )  ;", "outstream . write ( cmd . getBytes (  )  )  ;", "outstream . flush (  )  ;", "sock . shutdownOutput (  )  ;", "reader    =    new   BufferedReader ( new   InputStreamReader ( sock . getInputStream (  )  )  )  ;", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "String   line ;", "while    (  ( line    =    reader . readLine (  )  )     !  =    null )     {", "sbpend (  ( line    +     \"  \\ n \"  )  )  ;", "}", "return   sb . toString (  )  ;", "}    finally    {", "sock . close (  )  ;", "if    ( reader    !  =    null )     {", "reader . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["send4LetterWord"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "System . setProperty (  \" zookeeper . preAllocSize \"  ,     \"  1  0  0  \"  )  ;", "FileTxnLog . setPreallocSize (  (  1  0  0     *     1  0  2  4  )  )  ;", "if    (  !  (  . BASETEST . exists (  )  )  )     {", ". BASETEST . mkdirs (  )  ;", "}", "File   dataDir    =     . createTmpDir (  . BASETEST )  ;", "zks    =    new   ZooKeeperServer ( dataDir ,    dataDir ,     3  0  0  0  )  ;", "final   int   PORT    =    Integer . parseInt ( hostPort . split (  \"  :  \"  )  [  1  ]  )  ;", "if    (  ( factory )     =  =    null )     {", "factory    =    new   NIOServerCnxnFactory (  )  ;", "factory . configure ( new   InetSocketAddress ( PORT )  ,    maxCnxns )  ;", "}", "factory . startup ( zks )  ;", "Assert . assertTrue (  \" waiting   for   server   up \"  ,     . waitForServerUp (  (  \"  1  2  7  .  0  .  0  .  1  :  \"     +    PORT )  ,     . CONNECTION _ TIMEOUT )  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "if    (  ( zks )     !  =    null )     {", "ZKDatabase   zkDb    =    zks . getZKDatabase (  )  ;", "factory . shutdown (  )  ;", "try    {", "zkDb . close (  )  ;", "}    catch    ( IOException   ie )     {", "}", "final   int   PORT    =    Integer . parseInt ( hostPort . split (  \"  :  \"  )  [  1  ]  )  ;", "Assert . assertTrue (  \" waiting   for   server   down \"  ,     . waitForServerDown (  (  \"  1  2  7  .  0  .  0  .  1  :  \"     +    PORT )  ,     . CONNECTION _ TIMEOUT )  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "ZKClient   client    =    new   ZKClient ( hostPort )  ;", "client . registerService (  \"  / nodemanager \"  ,     \" hostPort \"  )  ;", "client . unregisterService (  \"  / nodemanager \"  )  ;", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "test (  \"  / some / test \"  )  ;", "}", "METHOD_END"], "methodName": ["testzkClient"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "long   start    =    System . currentTimeMillis (  )  ;", "while    ( true )     {", "try    {", "String   host    =    hp . split (  \"  :  \"  )  [  0  ]  ;", "int   port    =    Integer . parseInt ( hp . split (  \"  :  \"  )  [  1  ]  )  ;", ". send 4 LetterWord ( host ,    port ,     \" stat \"  )  ;", "}    catch    ( IOException   e )     {", "return   true ;", "}", "if    (  ( System . currentTimeMillis (  )  )     >     ( start    +    timeout )  )     {", "break ;", "}", "try    {", "Thread . sleep (  2  5  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["waitForServerDown"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "long   start    =    System . currentTimeMillis (  )  ;", "while    ( true )     {", "try    {", "String   host    =    hp . split (  \"  :  \"  )  [  0  ]  ;", "int   port    =    Integer . parseInt ( hp . split (  \"  :  \"  )  [  1  ]  )  ;", "String   result    =     . send 4 LetterWord ( host ,    port ,     \" stat \"  )  ;", "if    ( result . startsWith (  \" Zookeeper   version :  \"  )  )     {", "return   true ;", "}", "}    catch    ( IOException   e )     {", "}", "if    (  ( System . currentTimeMillis (  )  )     >     ( start    +    timeout )  )     {", "break ;", "}", "try    {", "Thread . sleep (  2  5  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["waitForServerUp"], "fileName": "org.apache.hadoop.yarn.lib.TestZKClient"}, {"methodBody": ["METHOD_START", "{", "String   data ;", "try    {", "Stat   stat    =    new   Stat (  )  ;", "byte [  ]    byteData    =    zk . getData ( path ,    false ,    stat )  ;", "data    =    new   String ( byteData )  ;", "}    catch    ( KeeperException   ke )     {", "throw   new   IOException ( ke )  ;", "}", "return   data ;", "}", "METHOD_END"], "methodName": ["getServiceData"], "fileName": "org.apache.hadoop.yarn.lib.ZKClient"}, {"methodBody": ["METHOD_START", "{", "List < String >    children    =    null ;", "try    {", "children    =    zk . getChildren ( path ,    false )  ;", "}    catch    ( KeeperException   ke )     {", "throw   new   IOException ( ke )  ;", "}", "return   children ;", "}", "METHOD_END"], "methodName": ["listServices"], "fileName": "org.apache.hadoop.yarn.lib.ZKClient"}, {"methodBody": ["METHOD_START", "{", "try    {", "zk . create ( path ,    data . getBytes (  )  ,    OPEN _ ACL _ UNSAFE ,    EPHEMERAL )  ;", "}    catch    ( KeeperException   ke )     {", "throw   new   IOException ( ke )  ;", "}", "}", "METHOD_END"], "methodName": ["registerService"], "fileName": "org.apache.hadoop.yarn.lib.ZKClient"}, {"methodBody": ["METHOD_START", "{", "try    {", "zk . delete ( path ,     (  -  1  )  )  ;", "}    catch    ( KeeperException   ke )     {", "throw   new   IOException ( ke )  ;", "}", "}", "METHOD_END"], "methodName": ["unregisterService"], "fileName": "org.apache.hadoop.yarn.lib.ZKClient"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceManager (  )     {", "@ Oride", "protected   void   doSecureLogin (  )    throws   IOException    {", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManagers . length )     =  =     1  )     {", "return    0  ;", "}", "int   numRetriesForRMBecomingActive    =     ( failoTimeout )     /     1  0  0  ;", "while    (  ( numRetriesForRMBecomingActive -  -  )     >     0  )     {", "for    ( int   i    =     0  ;    i    <     ( resourceManagers . length )  ;    i +  +  )     {", "if    (  ( resourceManagers [ i ]  )     =  =    null )     {", "continue ;", "}", "try    {", "if    (  ( HAServiceState . ACTIVE )     =  =     ( resourceManagers [ i ]  . getRMContext (  )  . getRMAdminService (  )  . getServiceStatus (  )  . getState (  )  )  )     {", "return   i ;", "}", "}    catch    ( IOException   e )     {", "throw   new   YarnRuntimeException (  (  \" Couldn ' t   read   the   status   of    \"     +     \" a   ResourceManger   in   the   HA   ensemble .  \"  )  ,    e )  ;", "}", "}", "try    {", "Thread . sleep (  1  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", "throw   new   YarnRuntimeException (  (  \" Interrupted   while   waiting   for   one    \"     +     \" of   the   ResourceManagers   to   become   active \"  )  )  ;", "}", "}", "return    -  1  ;", "}", "METHOD_END"], "methodName": ["getActiveRMIndex"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "return   this . appHistoryServer ;", "}", "METHOD_END"], "methodName": ["getApplicationHistoryServer"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "try    {", "retu   InetAddress . getLocalHost (  )  . getHostName (  )  ;", "}    catch    ( UnknownHostException   ex )     {", "throw   new   RuntimeException ( ex )  ;", "}", "}", "METHOD_END"], "methodName": ["getHostname"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeManagers [ i ]  ;", "}", "METHOD_END"], "methodName": ["getNodeManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceManagers . length ;", "}", "METHOD_END"], "methodName": ["getNumOfResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "int   activeRMIndex    =    getActiveRMIndex (  )  ;", "return   activeRMIndex    =  =     (  -  1  )     ?    null    :    this . resourceManagers [ activeRMIndex ]  ;", "}", "METHOD_END"], "methodName": ["getResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceManagers [ i ]  ;", "}", "METHOD_END"], "methodName": ["getResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "return   testWorkDir ;", "}", "METHOD_END"], "methodName": ["getTestWorkDir"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "if    ( HAUtil . isHAEnabled ( conf )  )     {", "conf . set ( RM _ HA _ ID ,    rmIds [ index ]  )  ;", "}", "resourceManagers [ index ]  . init ( conf )  ;", "resourceManagers [ index ]  . getRMContext (  )  . getDispatcher (  )  . register ( RMAppAttemptEventType . class ,    new   EventHandler < RMAppAttemptEvent >  (  )     {", "public   void   handle ( RMAppAttemptEvent   event )     {", "if    ( event   instanceof   RMAppAttemptRegistrationEvent )     {", "appMasters . put ( event . getApplicationAttemptId (  )  ,    event . getTimestamp (  )  )  ;", "} else", "if    ( event   instanceof   resourcemanager . rmapp . attempt . event . RMAppAttemptUnregistrationEvent )     {", "appMasters . remove ( event . getApplicationAttemptId (  )  )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["initResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManagers [ index ]  )     !  =    null )     {", "resourceManagers [ index ]  . stop (  )  ;", "resourceManagers [ index ]     =    null ;", "}", "Configuration   conf    =    getConfig (  )  ;", "resourceManagers [ index ]     =    new   ResourceManager (  )  ;", "tResourceManager ( index ,    getConfig (  )  )  ;", "startResourceManager ( index )  ;", "}", "METHOD_END"], "methodName": ["restartResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "String   hostname    =    MiniYARNCluster . getHostname (  )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "conf . set ( HAUtil . addSuffix ( confKey ,    rmIds [ index ]  )  ,     ( hostname    +     \"  :  0  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setHARMConfiguration"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "String   hostname    =    MiniYARNCluster . getHostname (  )  ;", "conf . set ( RM _ ADDRESS ,     ( hostname    +     \"  :  0  \"  )  )  ;", "conf . set ( RM _ ADMIN _ ADDRESS ,     ( hostname    +     \"  :  0  \"  )  )  ;", "conf . set ( RM _ SCHEDULER _ ADDRESS ,     ( hostname    +     \"  :  0  \"  )  )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     ( hostname    +     \"  :  0  \"  )  )  ;", "WebAppUtils . setRMWebAppHostnameAndPort ( conf ,    hostname ,     0  )  ;", "}", "METHOD_END"], "methodName": ["setNonHARMConfiguration"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "try    {", "Thread   rmThread    =    new   Thread (  )     {", "public   void   run (  )     {", "resourceManagers [ index ]  . start (  )  ;", "}", "}  ;", "rmThread . setName (  (  \" RM -  \"     +    index )  )  ;", "rmThread . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( resourceManagers [ index ]  . getServiceState (  )  )     =  =     ( STATE . INITED )  )     &  &     (  ( waitCount +  +  )     <     6  0  )  )     {", ". LOG . info (  \" Waiting   for   RM   to   start .  .  .  \"  )  ;", "Thread . sleep (  1  5  0  0  )  ;", "}", "if    (  ( resourceManagers [ index ]  . getServiceState (  )  )     !  =     ( STATE . STARTED )  )     {", "throw   new   IOException (  (  \" ResourceManager   failed   to   start .    Final   state   is    \"     +     ( resourceManagers [ index ]  . getServiceState (  )  )  )  )  ;", "}", "}    catch    ( Throwable   t )     {", "throw   new   YarnRuntimeException ( t )  ;", "}", ". LOG . info (  (  \" MiniYARN   ResourceManager   address :     \"     +     ( getConfig (  )  . get ( RM _ ADDRESS )  )  )  )  ;", ". LOG . info (  (  \" MiniYARN   ResourceManager   web   address :     \"     +     ( WebAppUtils . getRMWebAppURLWithoutScheme ( getConfig (  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["startResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManagers [ index ]  )     !  =    null )     {", "resourceManagers [ index ]  . stop (  )  ;", "resourceManagers [ index ]     =    null ;", "}", "}", "METHOD_END"], "methodName": ["stopResourceManager"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "GetClusterMetricsRequest   req    =    GetClusterMetricsRequest . newInstance (  )  ;", "for    ( int   i    =     0  ;    i    <     ( timeout    /     1  0  0  )  ;    i +  +  )     {", "ResourceManager   rm    =    getResourceManager (  )  ;", "if    ( rm    =  =    null )     {", "throw   new   YarnException (  \" Can   not   find   the   active   RM .  \"  )  ;", "} else", "if    (  ( nodeManagers . length )     =  =     ( rm . getClientRMService (  )  . getClusterMetrics ( req )  . getClusterMetrics (  )  . getNumNodeManagers (  )  )  )     {", "return   true ;", "}", "Thread . sleep (  1  0  0  )  ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["waitForNodeManagersToConnect"], "fileName": "org.apache.hadoop.yarn.server.MiniYARNCluster"}, {"methodBody": ["METHOD_START", "{", "Configuration   configurationWithoutSecurity    =    new   Configuration (  )  ;", "configurationWithoutSecurity . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" simple \"  )  ;", "Configuration   configurationWithSecurity    =    new   Configuration (  )  ;", "configurationWithSecurity . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "configurationWithSecurity . set ( RM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY ,     . httpSpnegoPrincipal )  ;", "configurationWithSecurity . set ( RM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "configurationWithSecurity . set ( NM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY ,     . httpSpnegoPrincipal )  ;", "configurationWithSecurity . set ( NM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {    configurationWithoutSecurity    }  ,    new   Object [  ]  {    configurationWithSecurity    }     }  )  ;", "}", "METHOD_END"], "methodName": ["configs"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "ContainerManagementProtocol   proxy ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( user )  ;", "final   InetSocketAddress   addr    =    NetUtils . createSocketAddr ( nodeId . getHost (  )  ,    nodeId . getPort (  )  )  ;", "if    ( nmToken    !  =    null )     {", "ugi . addToken ( ConverterUtils . convertFromYarn ( nmToken ,    addr )  )  ;", "}", "proxy    =    ugi . doAs ( new   PrivilegedAction < ContainerManagementProtocol >  (  )     {", "@ Override", "public   ContainerManagementProtocol   run (  )     {", "return    (  ( ContainerManagementProtocol )     ( rpc . getProxy ( ContainerManagementProtocol . class ,    addr ,    conf )  )  )  ;", "}", "}  )  ;", "return   proxy ;", "}", "METHOD_END"], "methodName": ["getContainerManagementProtocolProxy"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( containerId )  ;", "GetContainerStatusesRequest   request    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerManagementProtocol   proxy    =    null ;", "try    {", "proxy    =    gementProtocolProxy ( rpc ,    nmToken ,    nodeId ,    appAttemptId . toString (  )  )  ;", "GetContainerStatusesResponse   statuses    =    proxy . getContainerStatuses ( request )  ;", "if    (  (  ( statuses . getFailedRequests (  )  )     !  =    null )     &  &     ( statuses . getFailedRequests (  )  . containsKey ( containerId )  )  )     {", "parseAndThrowException ( statuses . getFailedRequests (  )  . get ( containerId )  . deSerialize (  )  )  ;", "}", "}    finally    {", "if    ( proxy    !  =    null )     {", "rpc . stopProxy ( proxy ,    conf )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["getContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "if    ( t   instanceof   YarnException )     {", "throw    (  ( YarnException )     ( t )  )  ;", "} else", "if    ( t   instanceof   security . token . SecretManager . InvalidToken )     {", "throw    (  ( security . token . SecretManager . InvalidToken )     ( t )  )  ;", "} else    {", "throw    (  ( IOException )     ( t )  )  ;", "}", "}", "METHOD_END"], "methodName": ["parseAndThrowException"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "int   oldKeyId    =    nmTokenSecretManagerRM . getCurrentKey (  )  . getKeyId (  )  ;", "nmTokenSecretManagerRM . rollMasterKey (  )  ;", "int   interval    =     4  0  ;", "while    (  (  ( nmTokenSecretManagerNM . getCurrentKey (  )  . getKeyId (  )  )     =  =    oldKeyId )     &  &     (  ( interval -  -  )     >     0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "nmTokenSecretManagerRM . activateNextMasterKey (  )  ;", "Assert . assertTrue (  (  ( nmTokenSecretManagerNM . getCurrentKey (  )  . getKeyId (  )  )     =  =     ( nmTokenSecretManagerRM . getCurrentKey (  )  . getKeyId (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["rollNMTokenMasterKey"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "TestContainerManagerSecurity . testRootDir . mkdirs (  )  ;", "TestContainerManagerSecurity . httpSpnegoKeytabFile . deleteOnExit (  )  ;", "getKdc (  )  . createPrincipal ( TestContainerManagerSecurity . httpSpnegoKeytabFile ,    TestContainerManagerSecurity . httpSpnegoPrincipal )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunchContext   context    =    Records . newRecord ( ContainerLaunchContext . class )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( context ,    containerToken )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "ContainerManagementProtocol   proxy    =    null ;", "try    {", "proxy    =    gementProtocolProxy ( rpc ,    nmToken ,    nodeId ,    user )  ;", "StartContainersResponse   response    =    proxy . startContainers ( allRequests )  ;", "for    ( SerializedException   ex    :    response . getFailedRequests (  )  . values (  )  )     {", "parseAndThrowException ( ex . deSerialize (  )  )  ;", "}", "}    finally    {", "if    ( proxy    !  =    null )     {", "rpc . stopProxy ( proxy ,    conf )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["startContainer"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "StopContainersRequest   request    =    StopContainersRequest . newInstance ( containerId )  ;", "ContainerManagementProtocol   proxy    =    null ;", "try    {", "proxy    =    gementProtocolProxy ( rpc ,    nmToken ,    nodeId ,    appAttemptId . toString (  )  )  ;", "StopContainersResponse   response    =    proxy . stopContainers ( request )  ;", "if    (  (  ( response . getFailedRequests (  )  )     !  =    null )     &  &     ( response . getFailedRequests (  )  . containsKey ( containerId )  )  )     {", "parseAndThrowException ( response . getFailedRequests (  )  . get ( containerId )  . deSerialize (  )  )  ;", "}", "}    catch    ( Exception   e )     {", "if    ( proxy    !  =    null )     {", "rpc . stopProxy ( proxy ,    conf )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["stopContainer"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "TestContainerManagerSecurity . testRootDir . delete (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "try    {", ". yarnCluster    =    new   MiniYARNCluster (  . class . getName (  )  ,     1  ,     1  ,     1  )  ;", ". yarnCluster . init ( conf )  ;", ". yarnCluster . start (  )  ;", "testNMTokens ( conf )  ;", "testContainerToken ( conf )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "throw   e ;", "}    finally    {", "if    (  (  . yarnCluster )     !  =    null )     {", ". yarnCluster . stop (  )  ;", ". yarnCluster    =    null ;", "}", "}", "}", "METHOD_END"], "methodName": ["testContainerManager"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "TestContainerManagerSecurity . LOG . info (  \" Running   test   for   malice   user \"  )  ;", "NMTokenSecretManagerInRM   nmTokenSecretManagerInRM    =    TestContainerManagerSecurity . yarnCluster . getResourceManager (  )  . getRMContext (  )  . getNMTokenSecretManager (  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     0  )  ;", "ContainerId   cId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "NodeManager   nm    =    TestContainerManagerSecurity . yarnCluster . getNodeManager (  0  )  ;", "NMTokenSecretManagerInNM   nmTokenSecretManagerInNM    =    nm . getNMContext (  )  . getNMTokenSecretManager (  )  ;", "String   user    =     \" test \"  ;", "waitForNMToReceiveNMTokenKey ( nmTokenSecretManagerInNM ,    nm )  ;", "NodeId   nodeId    =    nm . getNMContext (  )  . getNodeId (  )  ;", "Assert . assertEquals ( nmTokenSecretManagerInNM . getCurrentKey (  )  . getKeyId (  )  ,    nmTokenSecretManagerInRM . getCurrentKey (  )  . getKeyId (  )  )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    TestContainerManagerSecurity . yarnCluster . getResourceManager (  )  . getRMContext (  )  . getContainerTokenSecretManager (  )  ;", "RMContainerTokenSecretManager   tamperedContainerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "tamperedContainerTokenSecretManager . rollMasterKey (  )  ;", "do    {", "tamperedContainerTokenSecretManager . rollMasterKey (  )  ;", "tamperedContainerTokenSecretManager . activateNextMasterKey (  )  ;", "}    while    (  ( containerTokenSecretManager . getCurrentKey (  )  . getKeyId (  )  )     =  =     ( tamperedContainerTokenSecretManager . getCurrentKey (  )  . getKeyId (  )  )     )  ;", "Resource   r    =    Resource . newInstance (  1  2  3  0  ,     2  )  ;", "Token   containerToken    =    tamperedContainerTokenSecretManager . createContainerToken ( cId ,    nodeId ,    user ,    r ,    Priority . newInstance (  0  )  ,     0  )  ;", "Token   nmToken    =    nmTokenSecretManagerInRM . createNMToken ( appAttemptId ,    nodeId ,    user )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "StringBuilder   sb    =    new   StringBuilder (  \" Given   Container    \"  )  ;", "sb . append ( cId )  ;", "sb . append (  \"    seems   to   have   an   illegally   generated   token .  \"  )  ;", "Assert . assertTrue ( testStartContainer ( rpc ,    appAttemptId ,    nodeId ,    containerToken ,    nmToken ,    true )  . contains ( sb . toString (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerToken"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "try    {", "geStatus ( rpc ,    nmToken ,    containerId ,    appAttemptId ,    nodeId ,    isExceptionExpected )  ;", "if    ( isExceptionExpected )     {", "fail (  \" Exception   was   expected !  !  \"  )  ;", "}", "return    \"  \"  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "return   e . getMessage (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetContainer"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "NMTokenSecretManagerInRM   nmTokenSecretManagerRM    =    TestContainerManagerSecurity . yarnCluster . getResourceManager (  )  . getRMContext (  )  . getNMTokenSecretManager (  )  ;", "NMTokenSecretManagerInNM   nmTokenSecretManagerNM    =    TestContainerManagerSecurity . yarnCluster . getNodeManager (  0  )  . getNMContext (  )  . getNMTokenSecretManager (  )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    TestContainerManagerSecurity . yarnCluster . getResourceManager (  )  . getRMContext (  )  . getContainerTokenSecretManager (  )  ;", "NodeManager   nm    =    TestContainerManagerSecurity . yarnCluster . getNodeManager (  0  )  ;", "waitForNMToReceiveNMTokenKey ( nmTokenSecretManagerNM ,    nm )  ;", "Assert . assertEquals ( nmTokenSecretManagerNM . getCurrentKey (  )  . getKeyId (  )  ,    nmTokenSecretManagerRM . getCurrentKey (  )  . getKeyId (  )  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "String   user    =     \" test \"  ;", "Resource   r    =    Resource . newInstance (  1  0  2  4  ,     1  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "ApplicationAttemptId   validAppAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   validContainerId    =    ContainerId . newInstance ( validAppAttemptId ,     0  )  ;", "NodeId   validNode    =    TestContainerManagerSecurity . yarnCluster . getNodeManager (  0  )  . getNMContext (  )  . getNodeId (  )  ;", "NodeId   invalidNode    =    NodeId . newInstance (  \" InvalidHost \"  ,     1  2  3  4  )  ;", "Token   validNMToken    =    nmTokenSecretManagerRM . createNMToken ( validAppAttemptId ,    validNode ,    user )  ;", "Token   validContainerToken    =    containerTokenSecretManager . createContainerToken ( validContainerId ,    validNode ,    user ,    r ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "ContainerTokenIdentifier   identifier    =    BuilderUtils . newContainerTokenIdentifier ( validContainerToken )  ;", "Assert . assertEquals ( Priority . newInstance (  1  0  )  ,    identifier . getPriority (  )  )  ;", "Assert . assertEquals (  1  2  3  4  ,    identifier . getCreationTime (  )  )  ;", "StringBuilder   sb ;", "NMTokenSecretManagerInRM   tempManager    =    new   NMTokenSecretManagerInRM ( conf )  ;", "tempManager . rollMasterKey (  )  ;", "do    {", "tempManager . rollMasterKey (  )  ;", "tempManager . activateNextMasterKey (  )  ;", "}    while    (  ( tempManager . getCurrentKey (  )  . getKeyId (  )  )     =  =     ( nmTokenSecretManagerRM . getCurrentKey (  )  . getKeyId (  )  )     )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "sb    =    new   StringBuilder (  \" Client   cannot   authenticate   via :  [ TOKEN ]  \"  )  ;", "} else    {", "sb    =    new   StringBuilder (  \" SIMPLE   authentication   is   not   enabled .       Available :  [ TOKEN ]  \"  )  ;", "}", "String   errorMsg    =    testStartContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerToken ,    null ,    true )  ;", "Assert . assertTrue ( errorMsg . contains ( sb . toString (  )  )  )  ;", "Token   invalidNMToken    =    tempManager . createNMToken ( validAppAttemptId ,    validNode ,    user )  ;", "sb    =    new   StringBuilder (  \" Given   NMToken   for   application    :     \"  )  ;", "sb . append ( validAppAttemptId . toString (  )  )  . append (  \"    seems   to   have   been   generated   illegally .  \"  )  ;", "Assert . assertTrue ( sb . toString (  )  . contains ( testStartContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerToken ,    invalidNMToken ,    true )  )  )  ;", "invalidNMToken    =    nmTokenSecretManagerRM . createNMToken ( validAppAttemptId ,    invalidNode ,    user )  ;", "sb    =    new   StringBuilder (  \" Given   NMToken   for   application    :     \"  )  ;", "sb . append ( validAppAttemptId )  . append (  \"    is   not   valid   for   current   node   manager . expected    :     \"  )  . append ( validNode . toString (  )  )  . append (  \"    found    :     \"  )  . append ( invalidNode . toString (  )  )  ;", "Assert . assertTrue ( sb . toString (  )  . contains ( testStartContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerToken ,    invalidNMToken ,    true )  )  )  ;", "conf . setInt ( RM _ CONTAINER _ ALLOC _ EXPIRY _ INTERVAL _ MS ,     (  (  4     *     6  0  )     *     1  0  0  0  )  )  ;", "validContainerToken    =    containerTokenSecretManager . createContainerToken ( validContainerId ,    validNode ,    user ,    r ,    Priority . newInstance (  0  )  ,     0  )  ;", "Assert . assertTrue ( testStartContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerToken ,    validNMToken ,    false )  . isEmpty (  )  )  ;", "Assert . assertTrue ( nmTokenSecretManagerNM . isAppAttemptNMTokenKeyPresent ( validAppAttemptId )  )  ;", "waitForContainerToFinishOnNM ( validContainerId )  ;", "sb    =    new   StringBuilder (  \" Attempt   to   relaunch   the   same   container   with   id    \"  )  ;", "sb . append ( validContainerId )  ;", "Assert . assertTrue ( testStartContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerToken ,    validNMToken ,    true )  . contains ( sb . toString (  )  )  )  ;", "testStopContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerId ,    validNMToken ,    false )  ;", "rollNMTokenMasterKey ( nmTokenSecretManagerRM ,    nmTokenSecretManagerNM )  ;", "rollNMTokenMasterKey ( nmTokenSecretManagerRM ,    nmTokenSecretManagerNM )  ;", "sb    =    new   StringBuilder (  \" Container    \"  )  ;", "sb . append ( validContainerId )  ;", "sb . append (  \"    was   recently   stopped   on   node   manager \"  )  ;", "Assert . assertTrue ( testGetContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerId ,    validNMToken ,    true )  . contains ( sb . toString (  )  )  )  ;", "nm . getNodeStatusUpdater (  )  . clearFinishedContainersFromCache (  )  ;", "sb    =    new   StringBuilder (  \" Container    \"  )  ;", "sb . append ( validContainerId . toString (  )  )  ;", "sb . append (  \"    is   not   handled   by   this   NodeManager \"  )  ;", "Assert . assertTrue ( testGetContainer ( rpc ,    validAppAttemptId ,    validNode ,    validContainerId ,    validNMToken ,    false )  . contains ( sb . toString (  )  )  )  ;", "ApplicationAttemptId   attempt 2     =    ApplicationAttemptId . newInstance ( appId ,     2  )  ;", "Token   attempt 1 NMToken    =    nmTokenSecretManagerRM . createNMToken ( validAppAttemptId ,    validNode ,    user )  ;", "Token   newContainerToken    =    containerTokenSecretManager . createContainerToken ( ContainerId . newInstance ( attempt 2  ,     1  )  ,    validNode ,    user ,    r ,    Priority . newInstance (  0  )  ,     0  )  ;", "Assert . assertTrue ( testStartContainer ( rpc ,    attempt 2  ,    validNode ,    newContainerToken ,    attempt 1 NMToken ,    false )  . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNMTokens"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "try    {", "star ( rpc ,    nmToken ,    containerToken ,    nodeId ,    appAttemptId . toString (  )  )  ;", "if    ( isExceptionExpected )     {", "fail (  \" Exception   was   expected !  !  \"  )  ;", "}", "return    \"  \"  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "return   e . getMessage (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStartContainer"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "try    {", "stop ( rpc ,    nmToken ,    Arrays . asList ( new   Id [  ]  {    containerId    }  )  ,    appAttemptId ,    nodeId )  ;", "if    ( isExceptionExpected )     {", "fail (  \" Exception   was   expected !  !  \"  )  ;", "}", "return    \"  \"  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "return   e . getMessage (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStopContainer"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "Context   nmContet    =    TestContainerManagerSecurity . yarnCluster . getNodeManager (  0  )  . getNMContext (  )  ;", "int   interval    =     4     *     6  0  ;", "while    (  (  ( interval -  -  )     >     0  )     &  &     ( nmContet . getContainers (  )  . containsKey ( containerId )  )  )     {", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "Assert . assertFalse ( nmContet . getContainers (  )  . containsKey ( containerId )  )  ;", "}", "METHOD_END"], "methodName": ["waitForContainerToFinishOnNM"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "int   attempt    =     6  0  ;", "ContainerManagerImpl   cm    =     (  ( ContainerManagerImpl )     ( nm . getNMContext (  )  . ge (  )  )  )  ;", "while    (  (  ( cm . getBlockNewContainerRequestsStatus (  )  )     |  |     (  ( nmTokenSecretManagerNM . getNodeId (  )  )     =  =    null )  )     &  &     (  ( attempt -  -  )     >     0  )  )     {", "Thread . sleep (  2  0  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForNMToReceiveNMTokenKey"], "fileName": "org.apache.hadoop.yarn.server.TestContainerManagerSecurity"}, {"methodBody": ["METHOD_START", "{", "File   file    =    new   File ( dir )  ;", "FileUtil . fullyDelete ( file )  ;", "file . createNewFile (  )  ;", ". LOG . info (  (  (  \" Prepared    \"     +    dir )     +     \"    to   fail .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["prepareDirToFail"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "TestDiskFailures . localFS    =    FileContext . getLocalFSFileContext (  )  ;", "TestDiskFailures . localFS . delete ( new   Path ( TestDiskFailures . localFSDirBase . getAbsolutePath (  )  )  ,    true )  ;", "TestDiskFailures . localFSDirBase . mkdirs (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestDiskFailures . yarnCluster )     !  =    null )     {", "TestDiskFailures . yarnCluster . stop (  )  ;", "TestDiskFailures . yarnCluster    =    null ;", "}", "FileUtil . fullyDelete ( TestDiskFailures . localFSDirBase )  ;", "}", "METHOD_END"], "methodName": ["teardown"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "String   localDir 1     =    new   File (  . testDir ,     \" localDir 1  \"  )  . getPath (  )  ;", "String   localDir 2     =    new   File (  . testDir ,     \" localDir 2  \"  )  . getPath (  )  ;", "String   logDir 1     =    new   File (  . testDir ,     \" logDir 1  \"  )  . getPath (  )  ;", "String   logDir 2     =    new   File (  . testDir ,     \" logDir 2  \"  )  . getPath (  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,     (  ( localDir 1     +     \"  ,  \"  )     +    localDir 2  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,     (  ( logDir 1     +     \"  ,  \"  )     +    logDir 2  )  )  ;", "prepareDirToFail ( localDir 1  )  ;", "prepareDirToFail ( logDir 2  )  ;", "LocalDirsHandlerService   dirSvc    =    new   LocalDirsHandlerService (  )  ;", "dirSvc . init ( conf )  ;", "List < String >    localDirs    =    dirSvc . getLocalDirs (  )  ;", "Assert . assertEquals (  1  ,    localDirs . size (  )  )  ;", "Assert . assertEquals ( new   Path ( localDir 2  )  . toString (  )  ,    localDirs . get (  0  )  )  ;", "List < String >    logDirs    =    dirSvc . getLogDirs (  )  ;", "Assert . assertEquals (  1  ,    logDirs . size (  )  )  ;", "Assert . assertEquals ( new   Path ( logDir 1  )  . toString (  )  ,    logDirs . get (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testDirFailuresOnStartup"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "String   dirType    =     ( localORLogDirs )     ?     \" local \"     :     \" log \"  ;", "String   dirsProperty    =     ( localORLogDirs )     ?    YarnConfiguration . NM _ LOCAL _ DIRS    :    YarnConfiguration . NM _ LOG _ DIRS ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setLong ( NM _ DISK _ HEALTH _ CHECK _ INTERVAL _ MS ,     . DISK _ HEALTH _ CHECK _ INTERVAL )  ;", "conf . setFloat ( NM _ MIN _ HEALTHY _ DISKS _ FRACTION ,     0  .  6 F )  ;", "if    (  (  . yarnCluster )     !  =    null )     {", ". yarnCluster . stop (  )  ;", "FileUtil . fullyDelete (  . localFSDirBase )  ;", ". localFSDirBase . mkdirs (  )  ;", "}", ". LOG . info (  \" Starting   up   YARN   cluster \"  )  ;", ". yarnCluster    =    new   MiniYARNCluster (  . class . getName (  )  ,     1  ,     . numLocalDirs ,     . numLogDirs )  ;", ". yarnCluster . init ( conf )  ;", ". yarnCluster . start (  )  ;", "NodeManager   nm    =     . yarnCluster . getNodeManager (  0  )  ;", ". LOG . info (  (  (  (  \" Configured   nm -  \"     +    dirType )     +     \"  - dirs =  \"  )     +     ( nm . getConfig (  )  . get ( dirsProperty )  )  )  )  ;", "dirsHandler    =    nm . getNodeHealthChecker (  )  . getDiskHandler (  )  ;", "List < String >    list    =     ( localORLogDirs )     ?    dirsHandler . getLocalDirs (  )     :    dirsHandler . getLogDirs (  )  ;", "String [  ]    dirs    =    list . toArray ( new   String [ list . size (  )  ]  )  ;", "Assert . assertEquals (  (  (  \" Number   of   nm -  \"     +    dirType )     +     \"  - dirs   is   wrong .  \"  )  ,     . numLocalDirs ,    dirs . length )  ;", "String   expectedDirs    =    StringUtils . join (  \"  ,  \"  ,    list )  ;", "verifyDisksHealth ( localORLogDirs ,    expectedDirs ,    true )  ;", "prepareDirToFail ( dirs [  2  ]  )  ;", "expectedDirs    =     (  (  (  ( dirs [  0  ]  )     +     \"  ,  \"  )     +     ( dirs [  1  ]  )  )     +     \"  ,  \"  )     +     ( dirs [  3  ]  )  ;", "verifyDisksHealth ( localORLogDirs ,    expectedDirs ,    true )  ;", "prepareDirToFail ( dirs [  0  ]  )  ;", "expectedDirs    =     (  ( dirs [  1  ]  )     +     \"  ,  \"  )     +     ( dirs [  3  ]  )  ;", "verifyDisksHealth ( localORLogDirs ,    expectedDirs ,    false )  ;", "prepareDirToFail ( dirs [  1  ]  )  ;", "prepareDirToFail ( dirs [  3  ]  )  ;", "expectedDirs    =     \"  \"  ;", "verifyDisksHealth ( localORLogDirs ,    expectedDirs ,    false )  ;", "}", "METHOD_END"], "methodName": ["testDirsFailures"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "testDirsFailures ( true )  ;", "}", "METHOD_END"], "methodName": ["testLocalDirsFailures"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "testDirsFailures ( false )  ;", "}", "METHOD_END"], "methodName": ["testLogDirsFailures"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "waitForDiskHealthCheck (  )  ;", "List < String >    list    =     ( localORLogDirs )     ?    dirsHandler . getLocalDirs (  )     :    dirsHandler . getLogDirs (  )  ;", "String   seenDirs    =    StringUtils . join (  \"  ,  \"  ,    list )  ;", ". LOG . info (  (  \" ExpectedDirs =  \"     +    expectedDirs )  )  ;", ". LOG . info (  (  \" SeenDirs =  \"     +    seenDirs )  )  ;", "Assert . assertTrue (  \" NodeManager   could   not   identify   disk   failure .  \"  ,    expectedDirs . equals ( seenDirs )  )  ;", "Assert . assertEquals (  \" Node ' s   health   in   terms   of   disks   is   wrong \"  ,    isHealthy ,    dirsHandler . areDisksHealthy (  )  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "Iterator < RMNode >    iter    =     . yarnCluster . getResourceManager (  )  . getRMContext (  )  . getRMNodes (  )  . values (  )  . iterator (  )  ;", "if    (  (  ( iter . next (  )  . getState (  )  )     !  =     ( NodeState . UNHEALTHY )  )     =  =    isHealthy )     {", "break ;", "}", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . error (  \" Interrupted   while   waiting   for   NM -  > RM   heartbeat .  \"  )  ;", "}", "}", "Iterator < RMNode >    iter    =     . yarnCluster . getResourceManager (  )  . getRMContext (  )  . getRMNodes (  )  . values (  )  . iterator (  )  ;", "Assert . assertEquals (  \" RM   is   not   updated   with   the   health   status   of   a   node \"  ,    isHealthy ,     (  ( iter . next (  )  . getState (  )  )     !  =     ( NodeState . UNHEALTHY )  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyDisksHealth"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "long   lastDisksCheckTime    =    dirsHandler . getLastDisksCheckTime (  )  ;", "long   time    =    lastDisksCheckTime ;", "for    ( int   i    =     0  ;     ( i    <     1  0  )     &  &     ( time    <  =    lastDisksCheckTime )  ;    i +  +  )     {", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . error (  \" Interrupted   while   waiting   for   NodeManager ' s   disk   health   check .  \"  )  ;", "}", "time    =    dirsHandler . getLastDisksCheckTime (  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForDiskHealthCheck"], "fileName": "org.apache.hadoop.yarn.server.TestDiskFailures"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "conf . set ( RM _ WEBAPP _ ADDRESS ,     \" localhost :  0  \"  )  ;", "cluster    =    new   MiniYARNCluster (  . class . getName (  )  ,     2  ,     1  ,     1  ,     1  )  ;", "cluster . init ( conf )  ;", "cluster . start (  )  ;", "cluster . getResourceManager (  0  )  . getRMContext (  )  . getRMAdminService (  )  . transitionToActive ( new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER )  )  ;", "assertFalse (  \" RM   never   turned   active \"  ,     (  (  -  1  )     =  =     ( cluster . getActiveRMIndex (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.TestMiniYARNClusterForHA"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" NMs   fail   to   connect   to   the   RM \"  ,    cluster . waitForNodeManagersToConnect (  5  0  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testClusterWorks"], "fileName": "org.apache.hadoop.yarn.server.TestMiniYARNClusterForHA"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "validateKeyExchange ( conf )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "validateKeyExchange ( conf )  ;", "}", "METHOD_END"], "methodName": ["testNMUpdation"], "fileName": "org.apache.hadoop.yarn.server.TestRMNMSecretKeys"}, {"methodBody": ["METHOD_START", "{", "final   DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "ResourceManager   rm    =    new   ResourceManager (  )     {", "@ Override", "protected   void   doSecureLogin (  )    throws   IOException    {", "}", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "@ Override", "protected   void   startWepApp (  )     {", "}", "}  ;", "rm . init ( conf )  ;", "rm . start (  )  ;", "String   containerToken    =     \" Container   Token    :     \"  ;", "String   nmToken    =     \" NM   Token    :     \"  ;", "MockNM   nm    =    new   MockNM (  \" host :  1  2  3  4  \"  ,     3  0  7  2  ,    rm . getResourceTrackerService (  )  )  ;", "RegisterNodeManagerResponse   registrationResponse    =    nm . registerNode (  )  ;", "MasterKey   containerTokenMasterKey    =    registrationResponse . getContainerTokenMasterKey (  )  ;", "Assert . assertNotNull (  ( containerToken    +     \" Registration   should   cause   a   key - update !  \"  )  ,    containerTokenMasterKey )  ;", "MasterKey   nmTokenMasterKey    =    registrationResponse . getNMTokenMasterKey (  )  ;", "Assert . assertNotNull (  ( nmToken    +     \" Registration   should   cause   a   key - update !  \"  )  ,    nmTokenMasterKey )  ;", "dispatcher . await (  )  ;", "NodeHeartbeatResponse   response    =    nm . nodeHeartbeat ( true )  ;", "Assert . assertNull (  ( containerToken    +     \" First   heartbeat   after   registration   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "Assert . assertNull (  ( nmToken    +     \" First   heartbeat   after   registration   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getNMTokenMasterKey (  )  )  ;", "dispatcher . await (  )  ;", "response    =    nm . nodeHeartbeat ( true )  ;", "Assert . assertNull (  ( containerToken    +     \" Even   second   heartbeat   after   registration   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "Assert . assertNull (  ( nmToken    +     \" Even   second   heartbeat   after   registration   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "dispatcher . await (  )  ;", "rm . getRMContext (  )  . getContainerTokenManager (  )  . rollMasterKey (  )  ;", "rm . getRMContext (  )  . getNMTokenManager (  )  . rollMasterKey (  )  ;", "response    =    nm . nodeHeartbeat ( true )  ;", "Assert . assertNotNull (  ( containerToken    +     \" Heartbeats   after   roll - over   and   before   activation   should   not   err   out .  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "Assert . assertNotNull (  ( nmToken    +     \" Heartbeats   after   roll - over   and   before   activation   should   not   err   out .  \"  )  ,    response . getNMTokenMasterKey (  )  )  ;", "Assert . assertEquals (  ( containerToken    +     \" Roll - over   should   have   incremented   the   key - id   only   by   one !  \"  )  ,     (  ( containerTokenMasterKey . getKeyId (  )  )     +     1  )  ,    response . getContainerTokenMasterKey (  )  . getKeyId (  )  )  ;", "Assert . assertEquals (  ( nmToken    +     \" Roll - over   should   have   incremented   the   key - id   only   by   one !  \"  )  ,     (  ( nmTokenMasterKey . getKeyId (  )  )     +     1  )  ,    response . getNMTokenMasterKey (  )  . getKeyId (  )  )  ;", "dispatcher . await (  )  ;", "response    =    nm . nodeHeartbeat ( true )  ;", "Assert . assertNull (  ( containerToken    +     \" Second   heartbeat   after   roll - over   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "Assert . assertNull (  ( nmToken    +     \" Second   heartbeat   after   roll - over   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getNMTokenMasterKey (  )  )  ;", "dispatcher . await (  )  ;", "rm . getRMContext (  )  . getContainerTokenManager (  )  . activateNextMasterKey (  )  ;", "rm . getRMContext (  )  . getNMTokenManager (  )  . activateNextMasterKey (  )  ;", "response    =    nm . nodeHeartbeat ( true )  ;", "Assert . assertNull (  ( containerToken    +     \" Activation   shouldn ' t   cause   any   key   updates !  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "Assert . assertNull (  ( nmToken    +     \" Activation   shouldn ' t   cause   any   key   updates !  \"  )  ,    response . getNMTokenMasterKey (  )  )  ;", "dispatcher . await (  )  ;", "response    =    nm . nodeHeartbeat ( true )  ;", "Assert . assertNull (  ( containerToken    +     \" Even   second   heartbeat   after   activation   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getContainerTokenMasterKey (  )  )  ;", "Assert . assertNull (  ( nmToken    +     \" Even   second   heartbeat   after   activation   shouldn ' t   get   any   key   updates !  \"  )  ,    response . getNMTokenMasterKey (  )  )  ;", "dispatcher . await (  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["validateRMNMKeyExchange"], "fileName": "org.apache.hadoop.yarn.server.TestRMNMSecretKeys"}, {"methodBody": ["METHOD_START", "{", "return   ServerRMProxy . createRMProxy ( configuration ,    protocol ,    ServerRMProxy . INSTANCE )  ;", "}", "METHOD_END"], "methodName": ["createRMProxy"], "fileName": "org.apache.hadoop.yarn.server.api.ServerRMProxy"}, {"methodBody": ["METHOD_START", "{", "NMContainerStatus   status    =    Records . newRecord ( NMContainerStatus . class )  ;", "status . setContainerId ( containerId )  ;", "status . setContainerState ( containerState )  ;", "status . setAllocatedResource ( allocatedResource )  ;", "status . setDiagnostics ( diagnostics )  ;", "status . setContainerExitStatus ( containerExitStatus )  ;", "status . setPriority ( priority )  ;", "status . setCreationTime ( creationTime )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatRequest   nodeHeartbeatRequest    =    Records . newRecord ( NodeHeartbeatRequest . class )  ;", "nodeHeartbeatRequest . setNodeStatus ( nodeStatus )  ;", "nodeHeartbeatRequest . setLastKnownContainerTokenMasterKey ( lastKnownContainerTokenMasterKey )  ;", "nodeHeartbeatRequest . setLastKnownNMTokenMasterKey ( lastKnownNMTokenMasterKey )  ;", "return   nodeHeartbeatRequest ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerRequest   request    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "request . setHttpPort ( httpPort )  ;", "request . setResource ( resource )  ;", "request . setNodeId ( nodeId )  ;", "request . setNMVersion ( nodeManagerVersionId )  ;", "request . setContainerStatuses ( containerStatuses )  ;", "request . setRunningApplications ( runningApplications )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  1  2  3  4  5  6  7  8  9  ,     1  )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( attemptId ,     1  )  ;", "Resource   resource    =    Resource . newInstance (  1  0  0  0  ,     2  0  0  )  ;", "NMContainerStatus   report    =    NMContainerStatus . newInstance ( containerId ,    COMPLETE ,    resource ,     \" diagnostics \"  ,    ABORTED ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "NMContainerStatus   repor    =    new   NMContainerStatusPBImpl (  (  ( NMContainerStatusPBImpl )     ( report )  )  . ge (  )  )  ;", "Assert . assertEquals (  \" diagnostics \"  ,    repor . getDiagnostics (  )  )  ;", "Assert . assertEquals ( resource ,    repor . getAllocatedResource (  )  )  ;", "Assert . assertEquals ( ABORTED ,    repor . getContainerExitStatus (  )  )  ;", "Assert . assertEquals ( COMPLETE ,    repor . getContainerState (  )  )  ;", "Assert . assertEquals ( containerId ,    repor . getContainerId (  )  )  ;", "Assert . assertEquals ( Priority . newInstance (  1  0  )  ,    repor . getPriority (  )  )  ;", "Assert . assertEquals (  1  2  3  4  ,    repor . getCreationTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNMContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.TestProtocolRecords"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  1  2  3  4  5  6  7  8  9  ,     1  )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( attemptId ,     1  )  ;", "NMContainerStatus   containerReport    =    NMContainerStatus . newInstance ( containerId ,    RUNNING ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" diagnostics \"  ,     0  ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "List < NMContainerStatus >    reports    =    Arrays . asList ( containerReport )  ;", "RegisterNodeManagerRequest   request    =    RegisterNodeManagerRequest . newInstance ( NodeId . newInstance (  \"  1  .  1  .  1  .  1  \"  ,     1  0  0  0  )  ,     8  0  8  0  ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" NM - version - id \"  ,    reports ,    Arrays . asList ( appId )  )  ;", "RegisterNodeManagerRequest   requ    =    new   RegisterNodeManagerRequestPBImpl (  (  ( RegisterNodeManagerRequestPBImpl )     ( request )  )  . getProto (  )  )  ;", "Assert . assertEquals ( containerReport ,    requ . getNMContainerStatuses (  )  . get (  0  )  )  ;", "Assert . assertEquals (  8  0  8  0  ,    requ . getHttpPort (  )  )  ;", "Assert . assertEquals (  \" NM - version - id \"  ,    requ . getNMVersion (  )  )  ;", "Assert . assertEquals ( NodeId . newInstance (  \"  1  .  1  .  1  .  1  \"  ,     1  0  0  0  )  ,    requ . getNodeId (  )  )  ;", "Assert . assertEquals ( Resource . newInstance (  1  0  2  4  ,     1  )  ,    requ . getResource (  )  )  ;", "Assert . assertEquals (  1  ,    requ . getRunningApplications (  )  . size (  )  )  ;", "Assert . assertEquals ( appId ,    requ . getRunningApplications (  )  . get (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegisterNodeManagerRequest"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.TestProtocolRecords"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerRequest   request    =    RegisterNodeManagerRequest . newInstance ( NodeId . newInstance (  \" host \"  ,     1  2  3  4  )  ,     1  2  3  4  ,    Resource . newInstance (  0  ,     0  )  ,     \" version \"  ,    Arrays . asList ( NMContainerStatus . newInstance ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4 L ,     1  )  ,     1  )  ,     1  )  ,    RUNNING ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" good \"  ,     (  -  1  )  ,    Priority . newInstance (  0  )  ,     1  2  3  4  )  )  ,    Arrays . asList ( ApplicationId . newInstance (  1  2  3  4 L ,     1  )  ,    ApplicationId . newInstance (  1  2  3  4 L ,     2  )  )  )  ;", "RegisterNodeManagerRequest   request 1     =    new   RegisterNodeManagerRequestPBImpl (  (  ( RegisterNodeManagerRequestPBImpl )     ( request )  )  . getProto (  )  )  ;", "Assert . assertEquals ( request 1  . getNMContainerStatuses (  )  . size (  )  ,    request . getNMContainerStatuses (  )  . size (  )  )  ;", "Assert . assertEquals ( request 1  . getNMContainerStatuses (  )  . get (  0  )  . getContainerId (  )  ,    request . getNMContainerStatuses (  )  . get (  0  )  . getContainerId (  )  )  ;", "Assert . assertEquals ( request 1  . getRunningApplications (  )  . size (  )  ,    request . getRunningApplications (  )  . size (  )  )  ;", "Assert . assertEquals ( request 1  . getRunningApplications (  )  . get (  0  )  ,    request . getRunningApplications (  )  . get (  0  )  )  ;", "Assert . assertEquals ( request 1  . getRunningApplications (  )  . get (  1  )  ,    request . getRunningApplications (  )  . get (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegisterNodeManagerRequest"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.TestRegisterNodeManagerRequest"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerRequest   request    =    RegisterNodeManagerRequest . newInstance ( NodeId . newInstance (  \" host \"  ,     1  2  3  4  )  ,     1  2  3  4  ,    Resource . newInstance (  0  ,     0  )  ,     \" version \"  ,    null ,    null )  ;", "RegisterNodeManagerRequest   request 1     =    new   RegisterNodeManagerRequestPBImpl (  (  ( RegisterNodeManagerRequestPBImpl )     ( request )  )  . getProto (  )  )  ;", "Assert . assertEquals (  0  ,    request 1  . getNMContainerStatuses (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    request 1  . getRunningApplications (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRegisterNodeManagerRequestWithNullArrays"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.TestRegisterNodeManagerRequest"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerResponsePBImpl   asPB    =     (  ( RegisterNodeManagerResponsePBImpl )     ( orig )  )  ;", "RegisterNodeManagerResponseProto   proto    =    asPB . getProto (  )  ;", "ByteArrayOutputStream   out    =    new   ByteArrayOutputStream (  )  ;", "proto . writeTo ( out )  ;", "ByteArrayInputStream   in    =    new   ByteArrayInputStream ( out . toByteArray (  )  )  ;", "RegisterNodeManagerResponseProto . Builder   cp    =    RegisterNodeManagerResponseProto . newBuilder (  )  ;", "cp . mergeFrom ( in )  ;", "return   new   RegisterNodeManagerResponsePBImpl ( cp . build (  )  )  ;", "}", "METHOD_END"], "methodName": ["serDe"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.TestRegisterNodeManagerResponse"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerResponse   resp    =    TestRegisterNodeManagerResponse . recordFactory . newRecordInstance ( RegisterNodeManagerResponse . class )  ;", "byte [  ]    b    =    new   byte [  ]  {     0  ,     1  ,     2  ,     3  ,     4  ,     5     }  ;", "MasterKey   containerTokenMK    =    TestRegisterNodeManagerResponse . recordFactory . newRecordInstance ( MasterKey . class )  ;", "containerTokenMK . setKeyId (  5  4  3  2  1  )  ;", "containerTokenMK . setBytes ( ByteBuffer . wrap ( b )  )  ;", "resp . setContainerTokenMasterKey ( containerTokenMK )  ;", "MasterKey   nmTokenMK    =    TestRegisterNodeManagerResponse . recordFactory . newRecordInstance ( MasterKey . class )  ;", "nmTokenMK . setKeyId (  1  2  3  4  5  )  ;", "nmTokenMK . setBytes ( ByteBuffer . wrap ( b )  )  ;", "resp . setNMTokenMasterKey ( nmTokenMK )  ;", "resp . setNodeAction ( NodeAction . NORMAL )  ;", "assertEquals ( NodeAction . NORMAL ,    resp . getNodeAction (  )  )  ;", "assertNotNull ( resp . getContainerTokenMasterKey (  )  )  ;", "assertEquals (  5  4  3  2  1  ,    resp . getContainerTokenMasterKey (  )  . getKeyId (  )  )  ;", "assertArrayEquals ( b ,    resp . getContainerTokenMasterKey (  )  . getBytes (  )  . array (  )  )  ;", "RegisterNodeManagerResponse   respCopy    =    TestRegisterNodeManagerResponse . serDe ( resp )  ;", "assertEquals ( NodeAction . NORMAL ,    respCopy . getNodeAction (  )  )  ;", "assertNotNull ( respCopy . getContainerTokenMasterKey (  )  )  ;", "assertEquals (  5  4  3  2  1  ,    respCopy . getContainerTokenMasterKey (  )  . getKeyId (  )  )  ;", "assertArrayEquals ( b ,    respCopy . getContainerTokenMasterKey (  )  . getBytes (  )  . array (  )  )  ;", "assertNotNull ( resp . getNMTokenMasterKey (  )  )  ;", "assertEquals (  1  2  3  4  5  ,    resp . getNMTokenMasterKey (  )  . getKeyId (  )  )  ;", "assertArrayEquals ( b ,    resp . getNMTokenMasterKey (  )  . getBytes (  )  . array (  )  )  ;", "respCopy    =    TestRegisterNodeManagerResponse . serDe ( resp )  ;", "assertEquals ( NodeAction . NORMAL ,    respCopy . getNodeAction (  )  )  ;", "assertNotNull ( respCopy . getNMTokenMasterKey (  )  )  ;", "assertEquals (  1  2  3  4  5  ,    respCopy . getNMTokenMasterKey (  )  . getKeyId (  )  )  ;", "assertArrayEquals ( b ,    respCopy . getNMTokenMasterKey (  )  . getBytes (  )  . array (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRoundTrip"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.TestRegisterNodeManagerResponse"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( containerState )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PriorityPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( containerState )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PriorityPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . containerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( containerId )  )  . getProto (  )  . equals ( builder . getContainerId (  )  )  )  )  )     {", "builder . setContainerId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . resource )  )  . getProto (  )  . equals ( builder . getResource (  )  )  )  )  )     {", "builder . setResource ( convertToProtoFormat ( this . resource )  )  ;", "}", "if    (  ( this . priority )     !  =    null )     {", "builder . setPriority ( convertToProtoFormat ( this . priority )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "if    ( containerId    =  =    null )", "builder . clearId (  )  ;", "this . containerId    =    containerId ;", "}", "METHOD_END"], "methodName": ["setContainerId"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NMContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   MasterKeyPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeStatusPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( MasterKeyPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeStatusPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeStatus )     !  =    null )     {", "builder . setStatus ( convertToProtoFormat ( this . nodeStatus )  )  ;", "}", "if    (  ( this . lastKnownContainerTokenMasterKey )     !  =    null )     {", "builder . setLastKnownContainerTokenMasterKey ( convertToProtoFormat ( this . lastKnownContainerTokenMasterKey )  )  ;", "}", "if    (  ( this . lastKnownNMTokenMasterKey )     !  =    null )     {", "builder . setLastKnownNmTokenMasterKey ( convertToProtoFormat ( this . lastKnownNMTokenMasterKey )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearApplicationsToCleanup (  )  ;", "if    (  ( applicationsToCleanup )     =  =    null )", "return ;", "Iterable < ApplicationIdP >    iterable    =    new   Iterable < ApplicationIdP >  (  )     {", "@ Override", "public   Iterator < ApplicationIdP >    iterator (  )     {", "return   new   Iterator < ApplicationIdP >  (  )     {", "Iterator < ApplicationId >    iter    =    applicationsToCleanup . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ApplicationIdP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllApplicationsToCleanup ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addApplicationsToCleanupToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearContainersToCleanup (  )  ;", "if    (  ( containersToCleanup )     =  =    null )", "return ;", "Iterable < ContainerIdP >    iterable    =    new   Iterable < ContainerIdP >  (  )     {", "@ Override", "public   Iterator < ContainerIdP >    iterator (  )     {", "return   new   Iterator < ContainerIdP >  (  )     {", "Iterator < ContainerId >    iter    =    containersToCleanup . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ContainerIdP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllContainersToCleanup ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addContainersToCleanupToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   MasterKeyPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   NodeAction . valueOf ( p . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( MasterKeyPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   NodeActionProto . valueOf ( t . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationsToCleanup )     !  =    null )     {", "return ;", "}", "NodeHeartbeatResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationIdProto >    list    =    p . getApplicationsToCleanupList (  )  ;", "this . applicationsToCleanup    =    new   ArrayList < api . records . ApplicationId >  (  )  ;", "for    ( ApplicationIdProto   c    :    list )     {", "this . applicationsToCleanup . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initApplicationsToCleanup"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containersToCleanup )     !  =    null )     {", "return ;", "}", "NodeHeartbeatResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerIdProto >    list    =    p . getContainersToCleanupList (  )  ;", "this . containersToCleanup    =    new   ArrayList < api . records . ContainerId >  (  )  ;", "for    ( ContainerIdProto   c    :    list )     {", "this . containersToCleanup . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initContainersToCleanup"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containersToCleanup )     !  =    null )     {", "addContainersToCleanupToP (  )  ;", "}", "if    (  ( this . applicationsToCleanup )     !  =    null )     {", "addApplicationsToCleanupToP (  )  ;", "}", "if    (  ( this . containerTokenMasterKey )     !  =    null )     {", "builder . setContainerTokenMasterKey ( convertToPFormat ( this . containerTokenMasterKey )  )  ;", "}", "if    (  ( this . nmTokenMasterKey )     !  =    null )     {", "builder . setNmTokenMasterKey ( convertToPFormat ( this . nmTokenMasterKey )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.NodeHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearContainerStatuses (  )  ;", "List < NMContainerStatusP >    list    =    new   ArrayList < NMContainerStatusP >  (  )  ;", "for    ( NMContainerStatus   status    :    this . containerStatuses )     {", "list . add ( convertToPFormat ( status )  )  ;", "}", "builder . addAllContainerStatuses ( list )  ;", "}", "METHOD_END"], "methodName": ["addNMContainerStatusesToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearRunningApplications (  )  ;", "if    (  ( runningApplications )     =  =    null )     {", "return ;", "}", "Iterable < ApplicationIdP >    it    =    new   Iterable < ApplicationIdP >  (  )     {", "@ Override", "public   Iterator < ApplicationIdP >    iterator (  )     {", "return   new   Iterator < ApplicationIdP >  (  )     {", "Iterator < ApplicationId >    iter    =    runningApplications . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ApplicationIdP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllRunningApplications ( it )  ;", "}", "METHOD_END"], "methodName": ["addRunningApplicationsToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NMContainerStatusPBImpl ( c )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NMContainerStatusPBImpl )     ( c )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerStatuses )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NMContainerStatusProto >    list    =    p . getContainerStatusesList (  )  ;", "this . containerStatuses    =    new   ArrayList < NMContainerStatus >  (  )  ;", "for    ( NMContainerStatusProto   c    :    list )     {", "this . containerStatuses . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initContainerRecoveryReports"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . runningApplications )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationIdProto >    list    =    p . getRunningApplicationsList (  )  ;", "this . runningApplications    =    new   ArrayList < ApplicationId >  (  )  ;", "for    ( ApplicationIdProto   c    :    list )     {", "this . runningApplications . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initRunningApplications"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerStatuses )     !  =    null )     {", "addNMContainerStatusesToP (  )  ;", "}", "if    (  ( this . runningApplications )     !  =    null )     {", "addRunningApplicationsToP (  )  ;", "}", "if    (  ( this . resource )     !  =    null )     {", "builder . setResource ( convertToPFormat ( this . resource )  )  ;", "}", "if    (  ( this . nodeId )     !  =    null )     {", "builder . setNodeId ( convertToPFormat ( this . nodeId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   MasterKeyPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   NodeAction . valueOf ( p . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( MasterKeyPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   NodeActionProto . valueOf ( t . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( rebuild )", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerTokenMasterKey )     !  =    null )     {", "builder . setContainerTokenMasterKey ( convertToProtoFormat ( this . containerTokenMasterKey )  )  ;", "}", "if    (  ( this . nmTokenMasterKey )     !  =    null )     {", "builder . setNmTokenMasterKey ( convertToProtoFormat ( this . nmTokenMasterKey )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "rebuild    =    false ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RegisterNodeManagerResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "NodeHealthStatus   status    =    Records . newRecord ( NodeHealthStatus . class )  ;", "status . setIsNodeHealthy ( isNodeHealthy )  ;", "status . setHealthReport ( healthReport )  ;", "status . setLastHealthReportTime ( lastHealthReport )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.api.records.NodeHealthStatus"}, {"methodBody": ["METHOD_START", "{", "NodeStatus   nodeStatus    =    Records . newRecord ( NodeStatus . class )  ;", "nodeStatus . setResponseId ( responseId )  ;", "nodeStatus . setNodeId ( nodeId )  ;", "nodeStatus . setContainersStatuses ( containerStatuses )  ;", "nodeStatus . setKeepAliveApplications ( keepAliveApplications )  ;", "nodeStatus . setNodeHealthStatus ( nodeHealthStatus )  ;", "return   nodeStatus ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.api.records.NodeStatus"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "this . proto    =     ( this . viaProto )     ?    this . proto    :    this . buildbuild (  )  ;", "this . viaProto    =    true ;", "return   this . proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeHealthStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . viaProto )     |  |     (  ( this . builder )     =  =    null )  )     {", "this . builder    =    roto . newBuilder ( this . proto )  ;", "}", "this . viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeHealthStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( this . viaProto )", "maybeInitBuild )  ;", "this . proto    =    this . buildbuild (  )  ;", "this . viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeHealthStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearContainersStatuses (  )  ;", "if    (  ( containers )     =  =    null )", "return ;", "Iterable < Containerroto >    iterable    =    new   Iterable < Containerroto >  (  )     {", "@ Override", "public   Iterator < Containerroto >    iterator (  )     {", "return   new   Iterator < Containerroto >  (  )     {", "Iterator < ContainerStatus >    iter    =    containers . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   Containerroto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllContainersStatuses ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addContainersToProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearKeepAliveApplications (  )  ;", "if    (  ( keepAliveApplications )     =  =    null )", "return ;", "Iterable < ApplicationIdProto >    iterable    =    new   Iterable < ApplicationIdProto >  (  )     {", "@ Override", "public   Iterator < ApplicationIdProto >    iterator (  )     {", "return   new   Iterator < ApplicationIdProto >  (  )     {", "Iterator < ApplicationId >    iter    =    keepAliveApplications . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ApplicationIdProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "buildddAllKeepAliveApplications ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addKeepAliveApplicationsToProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( c )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerStatusPBImpl ( c )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( proto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeHealthStatusPBImpl ( proto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( c )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerStatusPBImpl )     ( c )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeHealthStatusPBImpl )     ( healthStatus )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containers )     !  =    null )     {", "return ;", "}", "NodeStatusProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerStatusProto >    list    =    p . getContainersStatusesList (  )  ;", "this . containers    =    new   ArrayList < api . records . ContainerStatus >  (  )  ;", "for    ( ContainerStatusProto   c    :    list )     {", "this . containers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initContainers"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . keepAliveApplications )     !  =    null )     {", "return ;", "}", "NodeStatusProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationIdProto >    list    =    p . getKeepAliveApplicationsList (  )  ;", "this . keepAliveApplications    =    new   ArrayList < api . records . ApplicationId >  (  )  ;", "for    ( ApplicationIdProto   c    :    list )     {", "this . keepAliveApplications . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initKeepAliveApplications"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeId )     !  =    null )     {", "builder . setNodeId ( convertToProtoFormat ( this . nodeId )  )  ;", "}", "if    (  ( this . containers )     !  =    null )     {", "addContainersToProto (  )  ;", "}", "if    (  ( this . nodeHealth )     !  =    null )     {", "builder . setNodeHealth ( convertToProtoFormat ( this . nodeHealth )  )  ;", "}", "if    (  ( this . keepAliveApplications )     !  =    null )     {", "addKeepAliveApplicationsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild )  ;", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . bindAddress ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "return   this . protocolHandler ;", "}", "METHOD_END"], "methodName": ["getClientHandler"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "InetSocketAddress   address    =    conf . getSocketAddr ( TIMELINE _ SERVICE _ BIND _ HOST ,    TIMELINE _ SERVICE _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ PORT )  ;", "server    =    rpc . getServer ( ApplicationHistoryProtocol . class ,    protocolHandler ,    address ,    conf ,    null ,    conf . getInt ( TIMELINE _ SERVICE _ HANDLER _ THREAD _ COUNT ,    DEFAULT _ TIMELINE _ SERVICE _ CLIENT _ THREAD _ COUNT )  )  ;", "server . start (  )  ;", "this . bindAddress    =    conf . updateConnectAddr ( TIMELINE _ SERVICE _ BIND _ HOST ,    TIMELINE _ SERVICE _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ ADDRESS ,    server . getListenerAddress (  )  )  ;", ". LOG . info (  (  \" Instantiated      at    \"     +     ( this . bindAddress )  )  )  ;", "super . serviceStart (  )  ;", "}", "METHOD_END"], "methodName": ["serviceStart"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationAttemptReport . newInstance ( appAttemptHistory . getApplicationAttemptId (  )  ,    appAttemptHistory . getHost (  )  ,    appAttemptHistory . getRPCPort (  )  ,    appAttemptHistory . getTrackingURL (  )  ,    appAttemptHistory . getDiagnosticsInfo (  )  ,    appAttemptHistory . getYarnApplicationAttemptState (  )  ,    appAttemptHistory . getMasterContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToApplicationAttemptReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   currentApplicationAttemptId    =    null ;", "String   trackingUrl    =     . UNAVAILABLE ;", "String   host    =     . UNAVAILABLE ;", "int   rpcPort    =     -  1  ;", "ApplicationAttemptHistoryData   lastAttempt    =    getLastAttempt ( appHistory . getApplicationId (  )  )  ;", "if    ( lastAttempt    !  =    null )     {", "currentApplicationAttemptId    =    lastAttempt . getApplicationAttemptId (  )  ;", "trackingUrl    =    lastAttempt . getTrackingURL (  )  ;", "host    =    lastAttempt . getHost (  )  ;", "rpcPort    =    lastAttempt . getRPCPort (  )  ;", "}", "return   ApplicationReport . newInstance ( appHistory . getApplicationId (  )  ,    currentApplicationAttemptId ,    appHistory . getUser (  )  ,    appHistory . getQueue (  )  ,    appHistory . getApplicationName (  )  ,    host ,    rpcPort ,    null ,    appHistory . getYarnApplicationState (  )  ,    appHistory . getDiagnosticsInfo (  )  ,    trackingUrl ,    appHistory . getStartTime (  )  ,    appHistory . getFinishTime (  )  ,    appHistory . getFinalApplicationStatus (  )  ,    null ,     \"  \"  ,     1  0  0  ,    appHistory . getApplicationType (  )  ,    null )  ;", "}", "METHOD_END"], "methodName": ["convertToApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "String   logUrl    =    WebAppUtils . getAggregatedLogURL ( serverHttpAddress ,    containerHistory . getAssignedNode (  )  . toString (  )  ,    containerHistory . getContainerId (  )  . toString (  )  ,    containerHistory . getContainerId (  )  . toString (  )  ,    user )  ;", "return   ContainerReport . newInstance ( containerHistory . getContainerId (  )  ,    containerHistory . getAllocatedResource (  )  ,    containerHistory . getAssignedNode (  )  ,    containerHistory . getPriority (  )  ,    containerHistory . getStartTime (  )  ,    containerHistory . getFinishTime (  )  ,    containerHistory . getDiagnosticsInfo (  )  ,    logUrl ,    containerHistory . getContainerExitStatus (  )  ,    containerHistory . getContainerState (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToContainerReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   ReflectionUtils . newInstance ( conf . getClass ( APPLICATION _ HISTORY _ STORE ,    FileSystemApplicationHistoryStore . class ,    ApplicationHistoryStore . class )  ,    conf )  ;", "}", "METHOD_END"], "methodName": ["createApplicationHistoryStore"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . historyStore ;", "}", "METHOD_END"], "methodName": ["getHistoryStore"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "Map < ApplicationAttemptId ,    ApplicationAttemptHistoryData >    attempts    =    historyStore . getApplicationAttempts ( appId )  ;", "ApplicationAttemptId   prevMaxAttemptId    =    null ;", "for    ( ApplicationAttemptId   attemptId    :    attempts . keySet (  )  )     {", "if    ( prevMaxAttemptId    =  =    null )     {", "prevMaxAttemptId    =    attemptId ;", "} else    {", "if    (  ( prevMaxAttemptId . getAttemptId (  )  )     <     ( attemptId . getAttemptId (  )  )  )     {", "prevMaxAttemptId    =    attemptId ;", "}", "}", "}", "return   attempts . get ( prevMaxAttemptId )  ;", "}", "METHOD_END"], "methodName": ["getLastAttempt"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationHistoryClientService ( historyManager )  ;", "}", "METHOD_END"], "methodName": ["createApplicationHistoryClientService"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationHistoryManagerImpl (  )  ;", "}", "METHOD_END"], "methodName": ["createApplicationHistoryManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   new   TimelineDataManager ( timelineStore ,    new   TimelineACLsManager ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["createTimelineDataManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   new   TimelineDelegationTokenSecretManagerService (  )  ;", "}", "METHOD_END"], "methodName": ["createTimelineDelegationTokenSecretManagerService"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   ReflectionUtils . newInstance ( conf . getClass ( TIMELINE _ SERVICE _ STORE ,    LeveldbTimelineStore . class ,    TimelineStore . class )  ,    conf )  ;", "}", "METHOD_END"], "methodName": ["createTimelineStore"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   socAddr    =    ApplicationHistoryServer . getBindAddress ( conf )  ;", "SecurityUtil . login ( conf ,    TIMELINE _ SERVICE _ KEYTAB ,    TIMELINE _ SERVICE _ PRINCIPAL ,    socAddr . getHostName (  )  )  ;", "}", "METHOD_END"], "methodName": ["doSecureLogin"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   this . historyManager ;", "}", "METHOD_END"], "methodName": ["getApplicationHistoryManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   conf . getSocketAddr ( TIMELINE _ SERVICE _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ PORT )  ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   this . ahsClientService ;", "}", "METHOD_END"], "methodName": ["getClientService"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "return   timelineStore ;", "}", "METHOD_END"], "methodName": ["getTimelineStore"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "Thread . setDefaultUncaughtExceptionHandler ( new   YarnUncaughtExceptionHandler (  )  )  ;", "StringUtils . startupShutdownMessage (  . class ,    args ,     . LOG )  ;", "appHistoryServer    =    null ;", "try    {", "appHistoryServer    =    new    (  )  ;", "ShutdownHookManager . get (  )  . addShutdownHook ( new   CompositeServiceShutdownHook ( appHistoryServer )  ,     . SHUTDOWN _ HOOK _ PRIORITY )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "appHistoryServer . init ( conf )  ;", "appHistoryServer . start (  )  ;", "}    catch    ( Throwable   t )     {", ". LOG . fatal (  \" Error   starting    \"  ,    t )  ;", "ExitUtil . terminate (  (  -  1  )  ,     \" Error   starting    \"  )  ;", "}", "return   appHistoryServer ;", "}", "METHOD_END"], "methodName": ["launchAppHistoryServer"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "ApplicationHistoryServer . launchAppHistoryServer ( args )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "String   initializers    =    conf . get (  \" hadoop . http . filter . initializers \"  )  ;", "initializers    =     (  ( initializers    =  =    null )     |  |     (  ( initializers . length (  )  )     =  =     0  )  )     ?     \"  \"     :    initializers ;", "if    (  !  ( initializers . contains ( TimelineAuthenticationFilterInitializer . class . getName (  )  )  )  )     {", "initializers    =     (  ( TimelineAuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,  \"  )     +    initializers ;", "}", "String [  ]    parts    =    initializers . split (  \"  ,  \"  )  ;", "ArrayList < String >    target    =    new   ArrayList < String >  (  )  ;", "for    ( String   filterInitializer    :    parts )     {", "filterInitializer    =    filterInitializer . trim (  )  ;", "if    ( filterInitializer . equals ( AuthenticationFilterInitializer . class . getName (  )  )  )     {", "continue ;", "}", "target . add ( filterInitializer )  ;", "}", "String   actualInitializers    =    StringUtils . join ( target ,     \"  ,  \"  )  ;", "if    (  !  ( actualInitializers . equals ( initializers )  )  )     {", "conf . set (  \" hadoop . http . filter . initializers \"  ,    actualInitializers )  ;", "}", "String   bindAddress    =    WebAppUtils . getWebAppBindURL ( conf ,    TIMELINE _ SERVICE _ BIND _ HOST ,    WebAppUtils . getAHSWebAppURLWithoutScheme ( conf )  )  ;", ". LOG . info (  (  \" Instantiating   AHSWebApp   at    \"     +    bindAddress )  )  ;", "try    {", "AHSWebApp   ahsWebApp    =    AHSWebApp . getInstance (  )  ;", "ahsWebApp . setApplicationHistoryManager ( historyManager )  ;", "ahsWebApp . setTimelineDelegationTokenSecretManagerService ( secretManagerService )  ;", "ahsWebApp . setTimelineDataManager ( timelineDataManager )  ;", "webApp    =    WebApps .  $ for (  \" applicationhistory \"  ,    ApplicationHistoryClientService . class ,    ahsClientService ,     \" ws \"  )  . with ( conf )  . at ( bindAddress )  . start ( ahsWebApp )  ;", "}    catch    ( Exception   e )     {", "String   msg    =     \" AHSWebApp   failed   to   start .  \"  ;", ". LOG . error ( msg ,    e )  ;", "throw   new   YarnRuntimeException ( msg ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["startWebApp"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "store . applicationAttemptFinished ( ApplicationAttemptFinishData . newInstance ( appAttemptId ,    appAttemptId . toString (  )  ,     \" test   tracking   url \"  ,    UNDEFINED ,    FINISHED )  )  ;", "}", "METHOD_END"], "methodName": ["writeApplicationAttemptFinishData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "store . applicationAttemptStarted ( ApplicationAttemptStartData . newInstance ( appAttemptId ,    appAttemptId . toString (  )  ,     0  ,    ContainerId . newInstance ( appAttemptId ,     1  )  )  )  ;", "}", "METHOD_END"], "methodName": ["writeApplicationAttemptStartData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "store . applicationFinished ( ApplicationFinishData . newInstance ( appId ,     0  ,    appId . toString (  )  ,    UNDEFINED ,    FINISHED )  )  ;", "}", "METHOD_END"], "methodName": ["writeApplicationFinishData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "store . applicationStarted ( ApplicationStartData . newInstance ( appId ,    appId . toString (  )  ,     \" test   type \"  ,     \" test   queue \"  ,     \" test   user \"  ,     0  ,     0  )  )  ;", "}", "METHOD_END"], "methodName": ["writeApplicationStartData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "store . containerFinished ( ContainerFinishData . newInstance ( containerId ,     0  ,    containerId . toString (  )  ,     0  ,    COMPLETE )  )  ;", "}", "METHOD_END"], "methodName": ["writeContainerFinishData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "store . containerStarted ( ContainerStartData . newInstance ( containerId ,    Resource . newInstance (  0  ,     0  )  ,    NodeId . newInstance (  \" localhost \"  ,     0  )  ,    Priority . newInstance ( containerId . getId (  )  )  ,     0  )  )  ;", "}", "METHOD_END"], "methodName": ["writeContainerStartData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "Path   applicationHistoryFile    =    new   Path ( rootDirPath ,    appId . toString (  )  )  ;", "if    (  !  ( fs . exists ( applicationHistoryFile )  )  )     {", "throw   new   IOException (  (  (  \" History   file   for   application    \"     +    appId )     +     \"    is   not   found \"  )  )  ;", "}", "if    ( outstandingWriters . containsKey ( appId )  )     {", "throw   new   IOException (  (  (  \" History   file   for   application    \"     +    appId )     +     \"    is   under   writing \"  )  )  ;", "}", "return   new    . HistoryFileReader ( applicationHistoryFile )  ;", "}", "METHOD_END"], "methodName": ["getHistoryFileReader"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "FileSystemApplicationHistoryStore . HistoryFileWriter   hfWriter    =    outstandingWriters . get ( appId )  ;", "if    ( hfWriter    =  =    null )     {", "throw   new   IOException (  (  (  \" History   file   of   application    \"     +    appId )     +     \"    is   not   opened \"  )  )  ;", "}", "return   hfWriter ;", "}", "METHOD_END"], "methodName": ["getHistoryFileWriter"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyData . setDiagnosticsInfo ( finishData . getDiagnosticsInfo (  )  )  ;", "historyData . setTrackingURL ( finishData . getTrackingURL (  )  )  ;", "historyData . setFinalStatus ( finishData . getFinalStatus (  )  )  ;", "historyData . setYarnAttemptState ( finishData . getYarnAttemptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["mergeApplicationAttemptHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyData . setHost ( startData . getHost (  )  )  ;", "historyData . setRPCPort ( startData . getRPCPort (  )  )  ;", "historyData . setMasterContainerId ( startData . getMasterContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["mergeApplicationAttemptHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyData . setFinishTime ( finishData . getFinishTime (  )  )  ;", "historyData . setDiagnosticsInfo ( finishData . getDiagnosticsInfo (  )  )  ;", "historyData . setFinalStatus ( finishData . getFinalStatus (  )  )  ;", "historyData . setYarnState ( finishData . getYarnState (  )  )  ;", "}", "METHOD_END"], "methodName": ["mergeApplicationHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyData . setApplicationName ( startData . getApplicationName (  )  )  ;", "historyData . setApplicationType ( startData . getApplicationType (  )  )  ;", "historyData . setQueue ( startData . getQueue (  )  )  ;", "historyData . setUser ( startData . getUser (  )  )  ;", "historyData . setSubmitTime ( startData . getSubmitTime (  )  )  ;", "historyData . setStartTime ( startData . getStartTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["mergeApplicationHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyData . setFinishTime ( finishData . getFinishTime (  )  )  ;", "historyData . setDiagnosticsInfo ( finishData . getDiagnosticsInfo (  )  )  ;", "historyData . setContainerExitStatus ( finishData . getContainerExitStatus (  )  )  ;", "historyData . setContainerState ( finishData . getContainerState (  )  )  ;", "}", "METHOD_END"], "methodName": ["mergeContainerHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyData . setAllocatedResource ( startData . getAllocatedResource (  )  )  ;", "historyData . setAssignedNode ( startData . getAssignedNode (  )  )  ;", "historyData . setPriority ( startData . getPriority (  )  )  ;", "historyData . setStartTime ( startData . getStartTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["mergeContainerHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptFinishDataPBImpl ( ApplicationAttemptFinishDataProto . parseFrom ( value )  )  ;", "}", "METHOD_END"], "methodName": ["parseApplicationAttemptFinishData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptStartDataPBImpl ( ApplicationAttemptStartDataProto . parseFrom ( value )  )  ;", "}", "METHOD_END"], "methodName": ["parseApplicationAttemptStartData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationFinishDataPBImpl ( ApplicationFinishDataProto . parseFrom ( value )  )  ;", "}", "METHOD_END"], "methodName": ["parseApplicationFinishData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationStartDataPBImpl ( ApplicationStartDataProto . parseFrom ( value )  )  ;", "}", "METHOD_END"], "methodName": ["parseApplicationStartData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerFinishDataPBImpl ( ContainerFinishDataProto . parseFrom ( value )  )  ;", "}", "METHOD_END"], "methodName": ["parseContainerFinishData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerStartDataPBImpl ( ContainerStartDataProto . parseFrom ( value )  )  ;", "}", "METHOD_END"], "methodName": ["parseContainerStartData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "containerData . putIfAbsent ( appAttemptId ,    new   ConcurrentHashMap < ContainerId ,    ContainerHistoryData >  (  )  )  ;", "return   containerData . get ( appAttemptId )  ;", "}", "METHOD_END"], "methodName": ["getSubMap"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "applicationAttemptData . putIfAbsent ( appId ,    new   ConcurrentHashMap < ApplicationAttemptId ,    ApplicationAttemptHistoryData >  (  )  )  ;", "return   applicationAttemptData . get ( appId )  ;", "}", "METHOD_END"], "methodName": ["getSubMap"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.MemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "historyServer    =    new   ApplicationHistoryServer (  )  ;", "Configuration   config    =    new   YarnConfiguration (  )  ;", "expectedLogUrl    =     (  (  ( WebAppUtils . getHttpSchemePrefix ( config )  )     +     ( WebAppUtils . getAHSWebAppURLWithoutScheme ( config )  )  )     +     \"  / applicationhistory / logs / localhost :  0  / container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  /  \"  )     +     \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  / test   user \"  ;", "config . setClass ( APPLICATION _ HISTORY _ STORE ,    MemoryApplicationHistoryStore . class ,    ApplicationHistoryStore . class )  ;", "historyServer . init ( config )  ;", "historyServer . start (  )  ;", "store    =     (  ( ApplicationHistoryManagerImpl )     ( historyServer . geManager (  )  )  )  . getHistoryStore (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "AHSWebApp . resetInstance (  )  ;", "Server . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "writeApplicationAttemptStartData ( appAttemptId )  ;", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "GetApplicationAttemptReportRequest   request    =    GetApplicationAttemptReportRequest . newInstance ( appAttemptId )  ;", "GetApplicationAttemptReportResponse   response    =    historyServer . get (  )  . getClientHandler (  )  . getApplicationAttemptReport ( request )  ;", "ApplicationAttemptReport   attemptReport    =    response . getApplicationAttemptReport (  )  ;", "Assert . assertNotNull ( attemptReport )  ;", "Assert . assertEquals (  \" appattempt _  0  _  0  0  0  1  _  0  0  0  0  0  1  \"  ,    attemptReport . getApplicationAttemptId (  )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationAttemptReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ApplicationAttemptId   appAttemptId 1     =    ApplicationAttemptId . newInstance ( appId ,     2  )  ;", "writeApplicationAttemptStartData ( appAttemptId )  ;", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "writeApplicationAttemptStartData ( appAttemptId 1  )  ;", "writeApplicationAttemptFinishData ( appAttemptId 1  )  ;", "GetApplicationAttemptsRequest   request    =    GetApplicationAttemptsRequest . newInstance ( appId )  ;", "GetApplicationAttemptsResponse   response    =    historyServer . get (  )  . getClientHandler (  )  . getApplicationAttempts ( request )  ;", "List < ApplicationAttemptReport >    attemptReports    =    response . getApplicationAttemptList (  )  ;", "Assert . assertNotNull ( attemptReports )  ;", "Assert . assertEquals ( appAttemptId ,    attemptReports . get (  0  )  . getApplicationAttemptId (  )  )  ;", "Assert . assertEquals ( appAttemptId 1  ,    attemptReports . get (  1  )  . getApplicationAttemptId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationAttempts"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    null ;", "appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "writeApplicationFinishData ( appId )  ;", "GetApplicationReportRequest   request    =    GetApplicationReportRequest . newInstance ( appId )  ;", "GetApplicationReportResponse   response    =    historyServer . get (  )  . getClientHandler (  )  . getApplicationReport ( request )  ;", "ApplicationReport   appReport    =    response . getApplicationReport (  )  ;", "Assert . assertNotNull ( appReport )  ;", "Assert . assertEquals (  \" application _  0  _  0  0  0  1  \"  ,    appReport . getApplicationId (  )  . toString (  )  )  ;", "Assert . assertEquals (  \" test   type \"  ,    appReport . getApplicationType (  )  . toString (  )  )  ;", "Assert . assertEquals (  \" test   queue \"  ,    appReport . getQueue (  )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    null ;", "appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "writeApplicationFinishData ( appId )  ;", "ApplicationId   appId 1     =    ApplicationId . newInstance (  0  ,     2  )  ;", "writeApplicationStartData ( appId 1  )  ;", "writeApplicationFinishData ( appId 1  )  ;", "GetApplicationsRequest   request    =    GetApplicationsRequest . newInstance (  )  ;", "GetApplicationsResponse   response    =    historyServer . get (  )  . getClientHandler (  )  . getApplications ( request )  ;", "List < ApplicationReport >    appReport    =    response . getApplicationList (  )  ;", "Assert . assertNotNull ( appReport )  ;", "Assert . assertEquals ( appId ,    appReport . get (  0  )  . getApplicationId (  )  )  ;", "Assert . assertEquals ( appId 1  ,    appReport . get (  1  )  . getApplicationId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplications"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "writeContainerStartData ( containerId )  ;", "writeContainerFinishData ( containerId )  ;", "writeApplicationFinishData ( appId )  ;", "GetContainerReportRequest   request    =    GetContainerReportRequest . newInstance ( containerId )  ;", "GetContainerReportResponse   response    =    historyServer . get (  )  . getClientHandler (  )  . getContainerReport ( request )  ;", "ContainerReport   container    =    response . getContainerReport (  )  ;", "Assert . assertNotNull ( container )  ;", "Assert . assertEquals ( containerId ,    container . getContainerId (  )  )  ;", "Assert . assertEquals ( expectedLogUrl ,    container . getLogUrl (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "ContainerId   containerId 1     =    ContainerId . newInstance ( appAttemptId ,     2  )  ;", "writeContainerStartData ( containerId )  ;", "writeContainerFinishData ( containerId )  ;", "writeContainerStartData ( containerId 1  )  ;", "writeContainerFinishData ( containerId 1  )  ;", "writeApplicationFinishData ( appId )  ;", "GetContainersRequest   request    =    GetContainersRequest . newInstance ( appAttemptId )  ;", "GetContainersResponse   response    =    historyServer . get (  )  . getClientHandler (  )  . getContainers ( request )  ;", "List < ContainerReport >    containers    =    response . getContainerList (  )  ;", "Assert . assertNotNull ( containers )  ;", "Assert . assertEquals ( containerId ,    containers . get (  1  )  . getContainerId (  )  )  ;", "Assert . assertEquals ( containerId 1  ,    containers . get (  0  )  . getContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainers"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryClientService"}, {"methodBody": ["METHOD_START", "{", "Configuration   config    =    new   Configuration (  )  ;", "config . setClass ( APPLICATION _ HISTORY _ STORE ,    MemoryApplicationHistoryStore . class ,    ApplicationHistoryStore . class )  ;", "applicationHistoryManagerImpl    =    new    (  )  ;", "applicationHistoryManagerImpl . init ( config )  ;", "applicationHistoryManagerImpl . start (  )  ;", "store    =    applicationHistoryManagerImpl . getHistoryStore (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "applicationHistoryManagerImpl . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    null ;", "appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "writeApplicationFinishData ( appId )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "writeApplicationAttemptStartData ( appAttemptId )  ;", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "ApplicationReport   appReport    =    a . getApplication ( appId )  ;", "Assert . assertNotNull ( appReport )  ;", "Assert . assertEquals ( appId ,    appReport . getApplicationId (  )  )  ;", "Assert . assertEquals ( appAttemptId ,    appReport . getCurrentApplicationAttemptId (  )  )  ;", "Assert . assertEquals ( appAttemptId . toString (  )  ,    appReport . getHost (  )  )  ;", "Assert . assertEquals (  \" test   type \"  ,    appReport . getApplicationType (  )  . toString (  )  )  ;", "Assert . assertEquals (  \" test   queue \"  ,    appReport . getQueue (  )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryManagerImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( historyServer )     !  =    null )     {", "historyServer . stop (  )  ;", "}", "AHSWebApp . resetInstance (  )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "String [  ]    filterInitializers    =    new   String [  ]  {    AuthenticationFilterInitializer . class . getName (  )  ,    TimelineAuthenticationFilterInitializer . class . getName (  )  ,     (  ( AuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,  \"  )     +     ( TimelineAuthenticationFilterInitializer . class . getName (  )  )  ,     (  ( AuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,     \"  )     +     ( TimelineAuthenticationFilterInitializer . class . getName (  )  )     }  ;", "for    ( String   filterInitializer    :    filterInitializers )     {", "historyServer    =    new    (  )  ;", "Configuration   config    =    new   YarnConfiguration (  )  ;", "config . set (  \" hadoop . http . filter . initializers \"  ,    filterInitializer )  ;", "historyServer . init ( config )  ;", "historyServer . start (  )  ;", "Configuration   tmp    =    historyServer . getConfig (  )  ;", "assertEquals ( TimelineAuthenticationFilterInitializer . class . getName (  )  ,    tmp . get (  \" hadoop . http . filter . initializers \"  )  )  ;", "historyServer . stop (  )  ;", "AHSWebApp . resetInstance (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFilteOverrides"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "ExitUtil . disableSystemExit (  )  ;", "try    {", "historyServer    =     . launchAppHistoryServer ( new   String [  0  ]  )  ;", "}    catch    ( ExitUtil   e )     {", "assertEquals (  0  ,    e . status )  ;", "ExitUtil . resetFirstExitException (  )  ;", "fail (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testLaunch"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "historyServer    =    new   ApplicationHistoryServer (  )  ;", "Configuration   config    =    new   YarnConfiguration (  )  ;", "historyServer . init ( config )  ;", "assertEquals ( INITED ,    historyServer . getServiceState (  )  )  ;", "assertEquals (  4  ,    historyServer . getServices (  )  . size (  )  )  ;", "ApplicationHistoryClientService   historyService    =    historyServer . getClientService (  )  ;", "assertNotNull ( historyServer . getClientService (  )  )  ;", "assertEquals ( INITED ,    historyService . getServiceState (  )  )  ;", "historyServer . start (  )  ;", "assertEquals ( STARTED ,    historyServer . getServiceState (  )  )  ;", "assertEquals ( STARTED ,    historyService . getServiceState (  )  )  ;", "historyServer . stop (  )  ;", "assertEquals ( STOPPED ,    historyServer . getServiceState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStartStopServer"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestApplicationHistoryServer"}, {"methodBody": ["METHOD_START", "{", "fs    =    new   RawLocalFileSystem (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "fs . initialize ( new   URI (  \"  /  \"  )  ,    conf )  ;", "fsWorkingPath    =    new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  ;", "fs . delete ( fsWorkingPath ,    true )  ;", "conf . set ( FS _ APPLICATION _ HISTORY _ STORE _ URI ,    fsWorkingPath . toString (  )  )  ;", "store    =    new   FileSystemApplicationHistoryStore (  )  ;", "store . init ( conf )  ;", "store . start (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "store . stop (  )  ;", "fs . delete ( fsWorkingPath ,    true )  ;", "fs . close (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "TestFileSystemApplicationHistoryStore . LOG . info (  \" Starting   testMassiveWriteContainerHistoryData \"  )  ;", "long   mb    =     1  0  2  4     *     1  0  2  4  ;", "long   usedDiskBefore    =     ( fs . getContentSummary ( fsWorkingPath )  . getLength (  )  )     /    mb ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "for    ( int   i    =     1  ;    i    <  =     1  0  0  0  0  0  ;     +  + i )     {", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    i )  ;", "writeContainerStartData ( containerId )  ;", "writeContainerFinishData ( containerId )  ;", "}", "writeApplicationFinishData ( appId )  ;", "long   usedDiskAfter    =     ( fs . getContentSummary ( fsWorkingPath )  . getLength (  )  )     /    mb ;", "Assert . assertTrue (  (  ( usedDiskAfter    -    usedDiskBefore )     <     2  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testMassiveWriteContainerHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "TestFileSystemApplicationHistoryStore . LOG . info (  \" Starting   testMissingApplicationAttemptHistoryData \"  )  ;", "testWriteHistoryData (  3  ,    false ,    true )  ;", "testReadHistoryData (  3  ,    false ,    true )  ;", "}", "METHOD_END"], "methodName": ["testMissingApplicationAttemptHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "TestFileSystemApplicationHistoryStore . LOG . info (  \" Starting   testMissingContainerHistoryData \"  )  ;", "testWriteHistoryData (  3  ,    true ,    false )  ;", "testReadHistoryData (  3  ,    true ,    false )  ;", "}", "METHOD_END"], "methodName": ["testMissingContainerHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "testReadHistoryData ( num ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testReadHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( num ,    store . getAllApplications (  )  . size (  )  )  ;", "for    ( int   i    =     1  ;    i    <  =    num ;     +  + i )     {", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,    i )  ;", "Data   appData    =    store . getApplication ( appId )  ;", "Assert . assertNotNull ( appData )  ;", "Assert . assertEquals ( appId . toString (  )  ,    appData . getApplicationName (  )  )  ;", "Assert . assertEquals ( appId . toString (  )  ,    appData . getDiagnosticsInfo (  )  )  ;", "Assert . assertEquals ( num ,    store . getApplicationAttempts ( appId )  . size (  )  )  ;", "for    ( int   j    =     1  ;    j    <  =    num ;     +  + j )     {", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    j )  ;", "ApplicationAttemptHistoryData   attemptData    =    store . getApplicationAttempt ( appAttemptId )  ;", "Assert . assertNotNull ( attemptData )  ;", "Assert . assertEquals ( appAttemptId . toString (  )  ,    attemptData . getHost (  )  )  ;", "if    ( missingApplicationAttempt    &  &     ( j    =  =    num )  )     {", "Assert . assertNull ( attemptData . getDiagnosticsInfo (  )  )  ;", "continue ;", "} else    {", "Assert . assertEquals ( appAttemptId . toString (  )  ,    attemptData . getDiagnosticsInfo (  )  )  ;", "}", "Assert . assertEquals ( num ,    store . getContainers ( appAttemptId )  . size (  )  )  ;", "for    ( int   k    =     1  ;    k    <  =    num ;     +  + k )     {", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    k )  ;", "ContainerHistoryData   containerData    =    store . getContainer ( containerId )  ;", "Assert . assertNotNull ( containerData )  ;", "Assert . assertEquals ( Priority . newInstance ( containerId . getId (  )  )  ,    containerData . getPriority (  )  )  ;", "if    ( missingContainer    &  &     ( k    =  =    num )  )     {", "Assert . assertNull ( containerData . getDiagnosticsInfo (  )  )  ;", "} else    {", "Assert . assertEquals ( containerId . toString (  )  ,    containerData . getDiagnosticsInfo (  )  )  ;", "}", "}", "ContainerHistoryData   masterContainer    =    store . getAMContainer ( appAttemptId )  ;", "Assert . assertNotNull ( masterContainer )  ;", "Assert . assertEquals ( ContainerId . newInstance ( appAttemptId ,     1  )  ,    masterContainer . getContainerId (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testReadHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "TestFileSystemApplicationHistoryStore . LOG . info (  \" Starting   testReadWriteHistoryData \"  )  ;", "testWriteHistoryData (  5  )  ;", "testReadHistoryData (  5  )  ;", "}", "METHOD_END"], "methodName": ["testReadWriteHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "TestFileSystemApplicationHistoryStore . LOG . info (  \" Starting   testWriteAfterApplicationFinish \"  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "writeApplicationStartData ( appId )  ;", "writeApplicationFinishData ( appId )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "try    {", "writeApplicationAttemptStartData ( appAttemptId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   not   opened \"  )  )  ;", "}", "try    {", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   not   opened \"  )  )  ;", "}", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "try    {", "writeContainerStartData ( containerId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   not   opened \"  )  )  ;", "}", "try    {", "writeContainerFinishData ( containerId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   not   opened \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testWriteAfterApplicationFinish"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "testWriteHistoryData ( num ,    false ,    false )  ;", "}", "METHOD_END"], "methodName": ["testWriteHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     1  ;    i    <  =    num ;     +  + i )     {", "Id   appId    =    Id . newInstance (  0  ,    i )  ;", "writeStartData ( appId )  ;", "for    ( int   j    =     1  ;    j    <  =    num ;     +  + j )     {", "AttemptId   appAttemptId    =    AttemptId . newInstance ( appId ,    j )  ;", "writeAttemptStartData ( appAttemptId )  ;", "if    ( missingAttempt    &  &     ( j    =  =    num )  )     {", "continue ;", "}", "for    ( int   k    =     1  ;    k    <  =    num ;     +  + k )     {", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    k )  ;", "writeContainerStartData ( containerId )  ;", "if    ( missingContainer    &  &     ( k    =  =    num )  )     {", "continue ;", "}", "writeContainerFinishData ( containerId )  ;", "}", "writeAttemptFinishData ( appAttemptId )  ;", "}", "writeFinishData ( appId )  ;", "}", "}", "METHOD_END"], "methodName": ["testWriteHistoryData"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestFileSystemApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "store    =    new   MemoryApplicationHistoryStore (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "long   mb    =     1  0  2  4     *     1  0  2  4  ;", "Runtime   runtime    =    Runtime . getRuntime (  )  ;", "long   usedMemoryBefore    =     (  ( runtime . totalMemory (  )  )     -     ( runtime . freeMemory (  )  )  )     /    mb ;", "int   numContainers    =     1  0  0  0  0  0  ;", "Id   appId    =    Id . newInstance (  0  ,     1  )  ;", "AttemptId   appAttemptId    =    AttemptId . newInstance ( appId ,     1  )  ;", "for    ( int   i    =     1  ;    i    <  =    numContainers ;     +  + i )     {", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    i )  ;", "writeContainerStartData ( containerId )  ;", "writeContainerFinishData ( containerId )  ;", "}", "long   usedMemoryAfter    =     (  ( runtime . totalMemory (  )  )     -     ( runtime . freeMemory (  )  )  )     /    mb ;", "Assert . assertTrue (  (  ( usedMemoryAfter    -    usedMemoryBefore )     <     4  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testMassiveWriteContainerHistory"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "try    {", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   stored   before   the   start   information \"  )  )  ;", "}", "int   numAppAttempts    =     5  ;", "writeApplicationStartData ( appId )  ;", "for    ( int   i    =     1  ;    i    <  =    numAppAttempts ;     +  + i )     {", "appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    i )  ;", "writeApplicationAttemptStartData ( appAttemptId )  ;", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "}", "Assert . assertEquals ( numAppAttempts ,    store . getApplicationAttempts ( appId )  . size (  )  )  ;", "for    ( int   i    =     1  ;    i    <  =    numAppAttempts ;     +  + i )     {", "appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    i )  ;", "ApplicationAttemptHistoryData   data    =    store . getApplicationAttempt ( appAttemptId )  ;", "Assert . assertNotNull ( data )  ;", "Assert . assertEquals ( appAttemptId . toString (  )  ,    data . getHost (  )  )  ;", "Assert . assertEquals ( appAttemptId . toString (  )  ,    data . getDiagnosticsInfo (  )  )  ;", "}", "writeApplicationFinishData ( appId )  ;", "appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "try    {", "writeApplicationAttemptStartData ( appAttemptId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   already   stored \"  )  )  ;", "}", "try    {", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   already   stored \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testReadWriteApplicationAttemptHistory"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "try    {", "writeApplicationFinishData ( appId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   stored   before   the   start   information \"  )  )  ;", "}", "int   numApps    =     5  ;", "for    ( int   i    =     1  ;    i    <  =    numApps ;     +  + i )     {", "appId    =    ApplicationId . newInstance (  0  ,    i )  ;", "writeApplicationStartData ( appId )  ;", "writeApplicationFinishData ( appId )  ;", "}", "Assert . assertEquals ( numApps ,    store . getAllApplications (  )  . size (  )  )  ;", "for    ( int   i    =     1  ;    i    <  =    numApps ;     +  + i )     {", "appId    =    ApplicationId . newInstance (  0  ,    i )  ;", "Data   data    =    store . getApplication ( appId )  ;", "Assert . assertNotNull ( data )  ;", "Assert . assertEquals ( appId . toString (  )  ,    data . getApplicationName (  )  )  ;", "Assert . assertEquals ( appId . toString (  )  ,    data . getDiagnosticsInfo (  )  )  ;", "}", "appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "try    {", "writeApplicationStartData ( appId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   already   stored \"  )  )  ;", "}", "try    {", "writeApplicationFinishData ( appId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   already   stored \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testReadWriteApplicationHistory"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "try    {", "writeContainerFinishData ( containerId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   stored   before   the   start   information \"  )  )  ;", "}", "writeApplicationAttemptStartData ( appAttemptId )  ;", "int   numContainers    =     5  ;", "for    ( int   i    =     1  ;    i    <  =    numContainers ;     +  + i )     {", "containerId    =    ContainerId . newInstance ( appAttemptId ,    i )  ;", "writeContainerStartData ( containerId )  ;", "writeContainerFinishData ( containerId )  ;", "}", "Assert . assertEquals ( numContainers ,    store . getContainers ( appAttemptId )  . size (  )  )  ;", "for    ( int   i    =     1  ;    i    <  =    numContainers ;     +  + i )     {", "containerId    =    ContainerId . newInstance ( appAttemptId ,    i )  ;", "ContainerHistoryData   data    =    store . getContainer ( containerId )  ;", "Assert . assertNotNull ( data )  ;", "Assert . assertEquals ( Priority . newInstance ( containerId . getId (  )  )  ,    data . getPriority (  )  )  ;", "Assert . assertEquals ( containerId . toString (  )  ,    data . getDiagnosticsInfo (  )  )  ;", "}", "ContainerHistoryData   masterContainer    =    store . getAMContainer ( appAttemptId )  ;", "Assert . assertNotNull ( masterContainer )  ;", "Assert . assertEquals ( ContainerId . newInstance ( appAttemptId ,     1  )  ,    masterContainer . getContainerId (  )  )  ;", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "containerId    =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "try    {", "writeContainerStartData ( containerId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   already   stored \"  )  )  ;", "}", "try    {", "writeContainerFinishData ( containerId )  ;", "Assert . fail (  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" is   already   stored \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testReadWriteContainerHistory"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.TestMemoryApplicationHistoryStore"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptFinishData   appAttemptFD    =    Records . newRecord ( ApplicationAttemptFinishData . class )  ;", "appAttemptFD . setApplicationAttemptId ( appAttemptId )  ;", "appAttemptFD . setDiagnosticsInfo ( diagnosticsInfo )  ;", "appAttemptFD . setTrackingURL ( trackingURL )  ;", "appAttemptFD . setFinalApplicationStatus ( finalApplicationStatus )  ;", "appAttemptFD . setYarnApplicationAttemptState ( yarnApplicationAttemptState )  ;", "return   appAttemptFD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptFinishData"}, {"methodBody": ["METHOD_START", "{", "return   applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   finalApplicationStatus ;", "}", "METHOD_END"], "methodName": ["getFinalApplicationStatus"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   masterContainerId ;", "}", "METHOD_END"], "methodName": ["getMasterContainerId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   rpcPort ;", "}", "METHOD_END"], "methodName": ["getRPCPort"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   trackingURL ;", "}", "METHOD_END"], "methodName": ["getTrackingURL"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   yarnApplicationAttemptState ;", "}", "METHOD_END"], "methodName": ["getYarnApplicationAttemptState"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptHistoryData   appAttemptHD    =    new   ApplicationAttemptHistoryData (  )  ;", "appAttemptHD . setApplicationAttemptId ( appAttemptId )  ;", "appAttemptHD . setHost ( host )  ;", "appAttemptHD . setRPCPort ( rpcPort )  ;", "appAttemptHD . setMasterContainerId ( masterContainerId )  ;", "appAttemptHD . setDiagnosticsInfo ( diagnosticsInfo )  ;", "appAttemptHD . setTrackingURL ( trackingURL )  ;", "appAttemptHD . setFinalApplicationStatus ( finalApplicationStatus )  ;", "appAttemptHD . setYarnApplicationAttemptState ( yarnApplicationAttemptState )  ;", "return   appAttemptHD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . applicationAttemptId    =    applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["setApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . diagnosticsInfo    =    diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["setDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . finalApplicationStatus    =    finalApplicationStatus ;", "}", "METHOD_END"], "methodName": ["setFinalApplicationStatus"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . host    =    host ;", "}", "METHOD_END"], "methodName": ["setHost"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . masterContainerId    =    masterContainerId ;", "}", "METHOD_END"], "methodName": ["setMasterContainerId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . rpcPort    =    rpcPort ;", "}", "METHOD_END"], "methodName": ["setRPCPort"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . trackingURL    =    trackingURL ;", "}", "METHOD_END"], "methodName": ["setTrackingURL"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . yarnApplicationAttemptState    =    yarnApplicationAttemptState ;", "}", "METHOD_END"], "methodName": ["setYarnApplicationAttemptState"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptHistoryData"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptStartData   appAttemptSD    =    Records . newRecord ( ApplicationAttemptStartData . class )  ;", "appAttemptSD . setApplicationAttemptId ( appAttemptId )  ;", "appAttemptSD . setHost ( host )  ;", "appAttemptSD . setRPCPort ( rpcPort )  ;", "appAttemptSD . setMasterContainerId ( masterContainerId )  ;", "return   appAttemptSD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationAttemptStartData"}, {"methodBody": ["METHOD_START", "{", "ApplicationFinishData   appFD    =    Records . newRecord ( ApplicationFinishData . class )  ;", "appFD . setApplicationId ( applicationId )  ;", "appFD . setFinishTime ( finishTime )  ;", "appFD . setDiagnosticsInfo ( diagnosticsInfo )  ;", "appFD . setFinalApplicationStatus ( finalApplicationStatus )  ;", "appFD . setYarnApplicationState ( yarnApplicationState )  ;", "return   appFD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationFinishData"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   applicationName ;", "}", "METHOD_END"], "methodName": ["getApplicationName"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   applicationType ;", "}", "METHOD_END"], "methodName": ["getApplicationType"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   finalApplicationStatus ;", "}", "METHOD_END"], "methodName": ["getFinalApplicationStatus"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   finishTime ;", "}", "METHOD_END"], "methodName": ["getFinishTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   startTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   submitTime ;", "}", "METHOD_END"], "methodName": ["getSubmitTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   this . yarnApplicationState ;", "}", "METHOD_END"], "methodName": ["getYarnApplicationState"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "ApplicationHistoryData   appHD    =    new   ApplicationHistoryData (  )  ;", "appHD . setApplicationId ( applicationId )  ;", "appHD . setApplicationName ( applicationName )  ;", "appHD . setApplicationType ( applicationType )  ;", "appHD . setQueue ( queue )  ;", "appHD . setUser ( user )  ;", "appHD . setSubmitTime ( submitTime )  ;", "appHD . setStartTime ( startTime )  ;", "appHD . setFinishTime ( finishTime )  ;", "appHD . setDiagnosticsInfo ( diagnosticsInfo )  ;", "appHD . setFinalApplicationStatus ( finalApplicationStatus )  ;", "appHD . setYarnApplicationState ( yarnApplicationState )  ;", "return   appHD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . applicationId    =    applicationId ;", "}", "METHOD_END"], "methodName": ["setApplicationId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . applicationName    =    applicationName ;", "}", "METHOD_END"], "methodName": ["setApplicationName"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . applicationType    =    applicationType ;", "}", "METHOD_END"], "methodName": ["setApplicationType"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . diagnosticsInfo    =    diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["setDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . finalApplicationStatus    =    finalApplicationStatus ;", "}", "METHOD_END"], "methodName": ["setFinalApplicationStatus"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . finishTime    =    finishTime ;", "}", "METHOD_END"], "methodName": ["setFinishTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . queue    =    queue ;", "}", "METHOD_END"], "methodName": ["setQueue"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . startTime    =    startTime ;", "}", "METHOD_END"], "methodName": ["setStartTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . submitTime    =    submitTime ;", "}", "METHOD_END"], "methodName": ["setSubmitTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . user    =    user ;", "}", "METHOD_END"], "methodName": ["setUser"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . yarnApplicationState    =    yarnApplicationState ;", "}", "METHOD_END"], "methodName": ["setYarnApplicationState"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationHistoryData"}, {"methodBody": ["METHOD_START", "{", "ApplicationStartData   appSD    =    Records . newRecord ( ApplicationStartData . class )  ;", "appSD . setApplicationId ( applicationId )  ;", "appSD . setApplicationName ( applicationName )  ;", "appSD . setApplicationType ( applicationType )  ;", "appSD . setQueue ( queue )  ;", "appSD . setUser ( user )  ;", "appSD . setSubmitTime ( submitTime )  ;", "appSD . setStartTime ( startTime )  ;", "return   appSD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ApplicationStartData"}, {"methodBody": ["METHOD_START", "{", "ContainerFinishData   containerFD    =    Records . newRecord ( ContainerFinishData . class )  ;", "containerFD . setContainerId ( containerId )  ;", "containerFD . setFinishTime ( finishTime )  ;", "containerFD . setDiagnosticsInfo ( diagnosticsInfo )  ;", "containerFD . setContainerExitStatus ( containerExitCode )  ;", "containerFD . setContainerState ( containerState )  ;", "return   containerFD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerFinishData"}, {"methodBody": ["METHOD_START", "{", "return   allocatedResource ;", "}", "METHOD_END"], "methodName": ["getAllocatedResource"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   assignedNode ;", "}", "METHOD_END"], "methodName": ["getAssignedNode"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   containerExitStatus ;", "}", "METHOD_END"], "methodName": ["getContainerExitStatus"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   containerState ;", "}", "METHOD_END"], "methodName": ["getContainerState"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   finishTime ;", "}", "METHOD_END"], "methodName": ["getFinishTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "return   startTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "ContainerHistoryData   containerHD    =    new   ContainerHistoryData (  )  ;", "containerHD . setContainerId ( containerId )  ;", "containerHD . setAllocatedResource ( allocatedResource )  ;", "containerHD . setAssignedNode ( assignedNode )  ;", "containerHD . setPriority ( priority )  ;", "containerHD . setStartTime ( startTime )  ;", "containerHD . setFinishTime ( finishTime )  ;", "containerHD . setDiagnosticsInfo ( diagnosticsInfo )  ;", "containerHD . setContainerExitStatus ( containerExitCode )  ;", "containerHD . setContainerState ( containerState )  ;", "return   containerHD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . allocatedResource    =    resource ;", "}", "METHOD_END"], "methodName": ["setAllocatedResource"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . assignedNode    =    nodeId ;", "}", "METHOD_END"], "methodName": ["setAssignedNode"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . containerExitStatus    =    containerExitStatus ;", "}", "METHOD_END"], "methodName": ["setContainerExitStatus"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . containerId    =    containerId ;", "}", "METHOD_END"], "methodName": ["setContainerId"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . containerState    =    containerState ;", "}", "METHOD_END"], "methodName": ["setContainerState"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . diagnosticsInfo    =    diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["setDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . finishTime    =    finishTime ;", "}", "METHOD_END"], "methodName": ["setFinishTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . priority    =    priority ;", "}", "METHOD_END"], "methodName": ["setPriority"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "this . startTime    =    startTime ;", "}", "METHOD_END"], "methodName": ["setStartTime"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerHistoryData"}, {"methodBody": ["METHOD_START", "{", "ContainerStartData   containerSD    =    Records . newRecord ( ContainerStartData . class )  ;", "containerSD . setContainerId ( containerId )  ;", "containerSD . setAllocatedResource ( allocatedResource )  ;", "containerSD . setAssignedNode ( assignedNode )  ;", "containerSD . setPriority ( priority )  ;", "containerSD . setStartTime ( startTime )  ;", "return   containerSD ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.ContainerStartData"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( applicationAttemptId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( finalApplicationStatus )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( yarnApplicationAttemptState )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( applicationAttemptId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( finalApplicationStatus )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( state )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . applicationAttemptId )     !  =    null )     &  &     (  !  (  (  ( ApplicationAttemptIdPBImpl )     ( this . applicationAttemptId )  )  . getProto (  )  . equals ( builder . getApplicationAttemptId (  )  )  )  )  )     {", "builder . setApplicationAttemptId ( convertToProtoFormat ( this . applicationAttemptId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild )  ;", "}", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( applicationAttemptId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( containerId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( applicationAttemptId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( masterContainerId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . applicationAttemptId )     !  =    null )     &  &     (  !  (  (  ( ApplicationAttemptIdPBImpl )     ( this . applicationAttemptId )  )  . getProto (  )  . equals ( builder . getApplicationAttemptId (  )  )  )  )  )     {", "builder . setApplicationAttemptId ( convertToProtoFormat ( this . applicationAttemptId )  )  ;", "}", "if    (  (  ( this . masterContainerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( this . masterContainerId )  )  . getProto (  )  . equals ( builder . getMasterContainerId (  )  )  )  )  )     {", "builder . setMasterContainerId ( convertToProtoFormat ( this . masterContainerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild )  ;", "}", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationAttemptStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( applicationId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( finalApplicationStatus )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( yarnApplicationState )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( applicationId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( finalApplicationStatus )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( state )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . applicationId )     !  =    null )     &  &     (  !  (  (  ( ApplicationIdPBImpl )     ( this . applicationId )  )  . getProto (  )  . equals ( builder . getApplicationId (  )  )  )  )  )     {", "builder . setApplicationId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild )  ;", "}", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( applicationId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( applicationId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . applicationId )     !  =    null )     &  &     (  !  (  (  ( ApplicationIdPBImpl )     ( this . applicationId )  )  . getProto (  )  . equals ( builder . getApplicationId (  )  )  )  )  )     {", "builder . setApplicationId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild )  ;", "}", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ApplicationStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( containerId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( containerState )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( containerId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( state )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . containerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( this . containerId )  )  . getProto (  )  . equals ( builder . getContainerId (  )  )  )  )  )     {", "builder . setContainerId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild )  ;", "}", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerFinishDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( containerId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( nodeId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PriorityPBImpl ( priority )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( resource )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( containerId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PriorityPBImpl )     ( priority )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( resource )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . containerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( this . containerId )  )  . getProto (  )  . equals ( builder . getContainerId (  )  )  )  )  )     {", "builder . setContainerId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . resource )  )  . getProto (  )  . equals ( builder . getAllocatedResource (  )  )  )  )  )     {", "builder . setAllocatedResource ( convertToProtoFormat ( this . resource )  )  ;", "}", "if    (  (  ( this . nodeId )     !  =    null )     &  &     (  !  (  (  ( NodeIdPBImpl )     ( this . nodeId )  )  . getProto (  )  . equals ( builder . getAssignedNodeId (  )  )  )  )  )     {", "builder . setAssignedNodeId ( convertToProtoFormat ( this . nodeId )  )  ;", "}", "if    (  (  ( this . priority )     !  =    null )     &  &     (  !  (  (  ( PriorityPBImpl )     ( this . priority )  )  . getProto (  )  . equals ( builder . getPriority (  )  )  )  )  )     {", "builder . setPriority ( convertToProtoFormat ( this . priority )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild )  ;", "}", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.records.impl.pb.ContainerStartDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "render ( AppPage . class )  ;", "}", "METHOD_END"], "methodName": ["app"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController"}, {"methodBody": ["METHOD_START", "{", "render ( AppAttemptPage . class )  ;", "}", "METHOD_END"], "methodName": ["appattempt"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController"}, {"methodBody": ["METHOD_START", "{", "render ( ContainerPage . class )  ;", "}", "METHOD_END"], "methodName": ["container"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController"}, {"methodBody": ["METHOD_START", "{", "render ( AHSLogsPage . class )  ;", "}", "METHOD_END"], "methodName": ["logs"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSController"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  ,     ' aaData '  :    appsTableData \"  )  . append (  \"  ,    bDeferRender :    true \"  )  . append (  \"  ,    bProcessing :    true \"  )  . append (  \"  \\ n ,    aoColumnDefs :     \"  )  . append ( getAppsTableColumnDefs (  )  )  . append (  \"  ,    aaSorting :     [  [  0  ,     ' desc '  ]  ]  }  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["appsTableInit"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSView"}, {"methodBody": ["METHOD_START", "{", "set ( ACCORDION _ ID ,     \" nav \"  )  ;", "set ( initID ( ACCORDION ,     \" nav \"  )  ,     \"  { autoHeight : false ,    active :  0  }  \"  )  ;", "}", "METHOD_END"], "methodName": ["commonPreHead"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSView"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "return   sb . append (  \"  [  \\ n \"  )  . append (  \"  {  ' sType '  :  ' numeric '  ,     ' aTargets '  :     [  0  ]  \"  )  . append (  \"  ,     ' mRender '  :    parseHID    }  \"  )  . append (  \"  \\ n ,     {  \\  ' sType \\  '  :  \\  ' numeric \\  '  ,     \\  ' aTargets \\  '  :     [  5  ,     6  ]  \"  )  . append (  \"  ,     ' mRender '  :    renderHDate    }  \"  )  . append (  \"  \\ n ,     {  \\  ' sType \\  '  :  \\  ' numeric \\  '  ,    bSearchable : false ,     \\  ' aTargets \\  '  :     [  9  ]  \"  )  . append (  \"  ,     ' mRender '  :    parseHProgress    }  ]  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsTableColumnDefs"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSView"}, {"methodBody": ["METHOD_START", "{", "return   applicationHistoryManager ;", "}", "METHOD_END"], "methodName": ["getApplicationHistoryManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "if    (  ( AHSWebApp . instance )     =  =    null )     {", "AHSWebApp . instance    =    new   AHSWebApp (  )  ;", "}", "return   AHSWebApp . instance ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "return   timelineDataManager ;", "}", "METHOD_END"], "methodName": ["getTimelineDataManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "return   secretManagerService ;", "}", "METHOD_END"], "methodName": ["getTimelineDelegationTokenSecretManagerService"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "AHSWebApp . instance    =    null ;", "}", "METHOD_END"], "methodName": ["resetInstance"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "this . applicationHistoryManager    =    applicationHistoryManager ;", "}", "METHOD_END"], "methodName": ["setApplicationHistoryManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "this . timelineDataManager    =    timelineDataManager ;", "}", "METHOD_END"], "methodName": ["setTimelineDataManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "this . secretManagerService    =    secretManagerService ;", "}", "METHOD_END"], "methodName": ["setTimelineDelegationTokenSecretManagerService"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebApp"}, {"methodBody": ["METHOD_START", "{", "return   getApps ( req ,    res ,    null ,    Collections .  < String > emptySet (  )  ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    Collections .  < String > emptySet (  )  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( stateQuery    !  =    null )     &  &     (  !  ( stateQuery . isEmpty (  )  )  )  )     {", "statesQuery . add ( stateQuery )  ;", "}", "Set < String >    appStates    =     . parseQueries ( statesQuery ,    true )  ;", "for    ( String   appState    :    appStates )     {", "switch    ( YarnApplicationState . valueOf ( appState . toUpperCase (  )  )  )     {", "case   FINISHED    :", "case   FAILED    :", "case   KILLED    :", "continue ;", "default    :", "throw   new   BadRequestException (  (  (  \" Invalid   application - state    \"     +    appState )     +     \"    specified .    It   should   be   a   final   state \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateStates"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  ,     ' aaData '  :    containersTableData \"  )  . append (  \"  ,    bDeferRender :    true \"  )  . append (  \"  ,    bProcessing :    true \"  )  . append (  \"  \\ n ,    aoColumnDefs :     \"  )  . append ( getContainersTableColumnDefs (  )  )  . append (  \"  ,    aaSorting :     [  [  0  ,     ' desc '  ]  ]  }  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["containersTableInit"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AppAttemptPage"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "return   sb . append (  \"  [  \\ n \"  )  . append (  \"  {  ' sType '  :  ' numeric '  ,     ' aTargets '  :     [  0  ]  \"  )  . append (  \"  ,     ' mRender '  :    parseHID    }  ]  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getContainersTableColumnDefs"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AppAttemptPage"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  ,     ' aaData '  :    attemptsTableData \"  )  . append (  \"  ,    bDeferRender :    true \"  )  . append (  \"  ,    bProcessing :    true \"  )  . append (  \"  \\ n ,    aoColumnDefs :     \"  )  . append ( getAttemptsTableColumnDefs (  )  )  . append (  \"  ,    aaSorting :     [  [  0  ,     ' desc '  ]  ]  }  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["attemptsTableInit"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AppPage"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "return   sb . append (  \"  [  \\ n \"  )  . append (  \"  {  ' sType '  :  ' numeric '  ,     ' aTargets '  :     [  0  ]  \"  )  . append (  \"  ,     ' mRender '  :    parseHID    }  \"  )  . append (  \"  \\ n ,     {  \\  ' sType \\  '  :  \\  ' numeric \\  '  ,     \\  ' aTargets \\  '  :     [  1  ]  \"  )  . append (  \"  ,     ' mRender '  :    renderHDate    }  ]  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getAttemptsTableColumnDefs"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AppPage"}, {"methodBody": ["METHOD_START", "{", "ApplicationHistoryManager   ahManager    =    new   TestAHSWebApp . MockApplicationHistoryManagerImpl ( store )  ;", "for    ( int   i    =     1  ;    i    <  =    numApps ;     +  + i )     {", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,    i )  ;", "writeApplicationStartData ( appId )  ;", "for    ( int   j    =     1  ;    j    <  =    numAppAttempts ;     +  + j )     {", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    j )  ;", "writeApplicationAttemptStartData ( appAttemptId )  ;", "for    ( int   k    =     1  ;    k    <  =    numContainers ;     +  + k )     {", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    k )  ;", "writeContainerStartData ( containerId )  ;", "writeContainerFinishData ( containerId )  ;", "}", "writeApplicationAttemptFinishData ( appAttemptId )  ;", "}", "writeApplicationFinishData ( appId )  ;", "}", "return   ahManager ;", "}", "METHOD_END"], "methodName": ["mockApplicationHistoryManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "this . store    =    store ;", "}", "METHOD_END"], "methodName": ["setApplicationHistoryStore"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "store    =    new   MemoryApplicationHistoryStore (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( ApplicationContext . class ,    mockApplicationHistoryManager (  1  ,     1  ,     5  )  )  ;", "AppAttemptPage   appAttemptPageInstance    =    injector . getInstance ( AppAttemptPage . class )  ;", "appAttemptPageInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "appAttemptPageInstance . set ( APPLICATION _ ATTEMPT _ ID ,    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  . toString (  )  )  ;", "appAttemptPageInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptPage"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "ApplicationHistoryManager   ahManager    =    mock ( ApplicationHistoryManager . class )  ;", "Injector   injector    =    WebAppTests . createMockInjector ( ApplicationHistoryManager . class ,    ahManager )  ;", "AHSController   controller    =    injector . getInstance ( AHSController . class )  ;", "controller . index (  )  ;", "Assert . assertEquals (  \" Application   History \"  ,    controller . get ( TITLE ,     \" unknown \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppControllerIndex"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( ApplicationContext . class ,    mockApplicationHistoryManager (  1  ,     5  ,     1  )  )  ;", "AppPage   appPageInstance    =    injector . getInstance ( AppPage . class )  ;", "appPageInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "appPageInstance . set ( APPLICATION _ ID ,    ApplicationId . newInstance (  0  ,     1  )  . toString (  )  )  ;", "appPageInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testAppPage"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( ApplicationContext . class ,    mockApplicationHistoryManager (  1  ,     1  ,     1  )  )  ;", "ContainerPage   containerPageInstance    =    injector . getInstance ( ContainerPage . class )  ;", "containerPageInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "containerPageInstance . set ( CONTAINER _ ID ,    ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  ,     1  )  . toString (  )  )  ;", "containerPageInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testContainerPage"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( ApplicationContext . class ,    mockApplicationHistoryManager (  5  ,     1  ,     1  )  )  ;", "AHSView   ahsViewInstance    =    injector . getInstance ( AHSView . class )  ;", "ahsViewInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "ahsViewInstance . set ( APP _ STATE ,    FAILED . toString (  )  )  ;", "ahsViewInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "ahsViewInstance . set ( APP _ STATE ,    StringHelper . cjoin ( FAILED . toString (  )  ,    KILLED )  )  ;", "ahsViewInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testView"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebApp"}, {"methodBody": ["METHOD_START", "{", "ApplicationHistoryStore   store    =    new   MemoryApplicationHistoryStore (  )  ;", "TestAHSWebApp   testAHSWebApp    =    new   TestAHSWebApp (  )  ;", "testAHSWebApp . setApplicationHistoryStore ( store )  ;", "ApplicationHistoryManager   ahManager    =    testAHSWebApp . mockApplicationHistoryManager (  5  ,     5  ,     5  )  ;", "return   ahManager ;", "}", "METHOD_END"], "methodName": ["mockApplicationHistoryManager"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,    FINISHED . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     5  ,    array . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQuery"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . accept ( TEXT _ PLAIN )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( INTERNAL _ SERVER _ ERROR ,    response . getClientResponseStatus (  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidAccept"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" bogus \"  )  . accept ( APPLICATION _ JSON )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidUri"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . accept ( APPLICATION _ JSON )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidUri2"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . path ( appId . toString (  )  )  . path (  \" appattempts \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   appAttempts    =    json . getJSONObject (  \" appAttempts \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appAttempts . length (  )  )  ;", "JSONArray   array    =    appAttempts . getJSONArray (  \" appAttempt \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     5  ,    array . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleAttempts"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . path ( appId . toString (  )  )  . path (  \" appattempts \"  )  . path ( appAttemptId . toString (  )  )  . path (  \" containers \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   containers    =    json . getJSONObject (  \" containers \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    containers . length (  )  )  ;", "JSONArray   array    =    containers . getJSONArray (  \" container \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     5  ,    array . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleContainers"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . path ( appId . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   app    =    json . getJSONObject (  \" app \"  )  ;", "assertEquals ( appId . toString (  )  ,    app . getString (  \" appId \"  )  )  ;", "assertEquals ( appId . toString (  )  ,    app . get (  \" name \"  )  )  ;", "assertEquals ( appId . toString (  )  ,    app . get (  \" diagnosticsInfo \"  )  )  ;", "assertEquals (  \" test   queue \"  ,    app . get (  \" queue \"  )  )  ;", "assertEquals (  \" test   user \"  ,    app . get (  \" user \"  )  )  ;", "assertEquals (  \" test   type \"  ,    app . get (  \" type \"  )  )  ;", "assertEquals ( UNDEFINED . toString (  )  ,    app . get (  \" finalAppStatus \"  )  )  ;", "assertEquals ( FINISHED . toString (  )  ,    app . get (  \" appState \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleApp"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . path ( appId . toString (  )  )  . path (  \" appattempts \"  )  . path ( appAttemptId . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   appAttempt    =    json . getJSONObject (  \" appAttempt \"  )  ;", "assertEquals ( appAttemptId . toString (  )  ,    appAttempt . getString (  \" appAttemptId \"  )  )  ;", "assertEquals ( appAttemptId . toString (  )  ,    appAttempt . getString (  \" host \"  )  )  ;", "assertEquals ( appAttemptId . toString (  )  ,    appAttempt . getString (  \" diagnosticsInfo \"  )  )  ;", "assertEquals (  \" test   tracking   url \"  ,    appAttempt . getString (  \" trackingUrl \"  )  )  ;", "assertEquals ( FINISHED . toString (  )  ,    appAttempt . get (  \" appAttemptState \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleAttempt"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . path ( appId . toString (  )  )  . path (  \" appattempts \"  )  . path ( appAttemptId . toString (  )  )  . path (  \" containers \"  )  . path ( containerId . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   container    =    json . getJSONObject (  \" container \"  )  ;", "assertEquals ( containerId . toString (  )  ,    container . getString (  \" containerId \"  )  )  ;", "assertEquals ( containerId . toString (  )  ,    container . getString (  \" diagnosticsInfo \"  )  )  ;", "assertEquals (  \"  0  \"  ,    container . getString (  \" allocatedMB \"  )  )  ;", "assertEquals (  \"  0  \"  ,    container . getString (  \" allocatedVCores \"  )  )  ;", "assertEquals ( NodeId . newInstance (  \" localhost \"  ,     0  )  . toString (  )  ,    container . getString (  \" assignedNodeId \"  )  )  ;", "assertEquals ( Priority . newInstance ( containerId . getId (  )  )  . toString (  )  ,    container . getString (  \" priority \"  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "assertEquals (  (  (  (  ( WebAppUtils . getHttpSchemePrefix ( conf )  )     +     ( WebAppUtils . getAHSWebAppURLWithoutScheme ( conf )  )  )     +     \"  /  / logs / localhost :  0  / container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  /  \"  )     +     \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  / test   user \"  )  ,    container . getString (  \" logUrl \"  )  )  ;", "assertEquals ( COMPLETE . toString (  )  ,    container . getString (  \" containerState \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleContainer"], "fileName": "org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.TestAHSWebServices"}, {"methodBody": ["METHOD_START", "{", "return   this . appsToCleanup ;", "}", "METHOD_END"], "methodName": ["getAppsToCleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.CMgrCompletedAppsEvent"}, {"methodBody": ["METHOD_START", "{", "return   reason ;", "}", "METHOD_END"], "methodName": ["getReason"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.CMgrCompletedAppsEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . containerToCleanup ;", "}", "METHOD_END"], "methodName": ["getContainersToCleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.CMgrCompletedContainersEvent"}, {"methodBody": ["METHOD_START", "{", "return   reason ;", "}", "METHOD_END"], "methodName": ["getReason"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.CMgrCompletedContainersEvent"}, {"methodBody": ["METHOD_START", "{", "try    {", "writeLock . lock (  )  ;", "this . pidFiles . put ( cId ,    pidFilePath )  ;", "}    finally    {", "writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["activateContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "try    {", "writeLock . lock (  )  ;", "this . pidFiles . remove ( cId )  ;", "}    finally    {", "writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["deactivateContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "try    {", "readLock . lock (  )  ;", "return   this . pidFiles . get ( cId )  ;", "}    finally    {", "readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getPidFilePath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "String   pid    =    null ;", "Path   pidFile    =    pidFiles . get ( containerID )  ;", "if    ( pidFile    =  =    null )     {", "return   pid ;", "}", "try    {", "pid    =    ProcessIdFileReader . getProcessId ( pidFile )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Got   exception   reading   pid   from   pid - file    \"     +    pidFile )  ,    e )  ;", "}", "return   pid ;", "}", "METHOD_END"], "methodName": ["getProcessId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "boolean   containerSchedPriorityIsSet    =    false ;", "int   containerSchedPriorityAdjustment    =    YarnConfiguration . DEFAULT _ NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY ;", "if    (  ( conf . get ( NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY )  )     !  =    null )     {", "containerSchedPriorityIsSet    =    true ;", "containerSchedPriorityAdjustment    =    conf . getInt ( NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY ,    DEFAULT _ NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY )  ;", "}", "if    ( Shell . WINDOWS )     {", "return   new   String [  ]  {    Shell . WINUTILS ,     \" task \"  ,     \" create \"  ,    groupId ,     \" cmd    / c    \"     +    command    }  ;", "} else    {", "List < String >    retCommand    =    new   ArrayList < String >  (  )  ;", "if    ( containerSchedPriorityIsSet )     {", "retCommand . addAll ( Arrays . asList (  \" nice \"  ,     \"  - n \"  ,    Integer . toString ( containerSchedPriorityAdjustment )  )  )  ;", "}", "retCommand . addAll ( Arrays . asList (  \" bash \"  ,    command )  )  ;", "return   retCommand . toArray ( new   String [ retCommand . size (  )  ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["getRunCommand"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "try    {", "readLock . lock (  )  ;", "return   this . pidFiles . containsKey ( cId )  ;", "}    finally    {", "readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["isContainerActive"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "String   shExecOutput    =    output ;", "if    ( shExecOutput    !  =    null )     {", "for    ( String   str    :    shExecOutput . split (  \"  \\ n \"  )  )     {", ". LOG . info ( str )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["logOutput"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "Path   pidPath    =    getPidFilePath ( containerId )  ;", "if    ( pidPath    =  =    null )     {", ". LOG . warn (  ( containerId    +     \"    is   not   active ,    returning   terminated   error \"  )  )  ;", "return    . ExitCode . TERMINATED . getExitCode (  )  ;", "}", "String   pid    =    null ;", "pid    =    ProcessIdFileReader . getProcessId ( pidPath )  ;", "if    ( pid    =  =    null )     {", "throw   new   IOException (  (  \" Unable   to   determine   pid   for    \"     +    containerId )  )  ;", "}", ". LOG . info (  (  (  (  \" Reacquiring    \"     +    containerId )     +     \"    with   pid    \"  )     +    pid )  )  ;", "try    {", "while    ( isContainerProcessAlive ( user ,    pid )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "}    catch    ( InterruptedException   e )     {", "throw   new   IOException (  (  (  \" Interrupted   while   waiting   for   process    \"     +    pid )     +     \"    to   exit \"  )  ,    e )  ;", "}", "String   exitCodeFile    =    ContainerLaunch . getExitCodeFile ( pidPath . toString (  )  )  ;", "File   file    =    new   File ( exitCodeFile )  ;", "final   int   sleepMsec    =     1  0  0  ;", "int   msecLeft    =     2  0  0  0  ;", "while    (  (  !  ( file . exists (  )  )  )     &  &     ( msecLeft    >  =     0  )  )     {", "if    (  !  ( isContainerActive ( containerId )  )  )     {", ". LOG . info (  ( containerId    +     \"    was   deactivated \"  )  )  ;", "return    . ExitCode . TERMINATED . getExitCode (  )  ;", "}", "try    {", "Thread . sleep ( sleepMsec )  ;", "}    catch    ( InterruptedException   e )     {", "throw   new   IOException (  (  \" Interrupted   while   waiting   for   exit   code   from    \"     +    containerId )  ,    e )  ;", "}", "msecLeft    -  =    sleepMsec ;", "}", "if    ( msecLeft    <     0  )     {", "throw   new   IOException (  (  \" Timeout   while   waiting   for   exit   code   from    \"     +    containerId )  )  ;", "}", "try    {", "return   Integer . parseInt ( FileUtils . readFileToString ( file )  . trim (  )  )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   IOException (  (  \" Error   parsing   exit   code   from   pid    \"     +    pid )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["reacquireContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "try    {", "new   util . Shell . ShellCommandExecutor ( Shell . getCheckProcessIsAliveCommand ( pid )  )  . execute (  )  ;", "return   true ;", "}    catch    ( ExitCodeException   e )     {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["containerIsAlive"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "boolean   initAppDirStatus    =    false ;", "FsPermission   appperms    =    new   FsPermission (  . APPDIR _ PERM )  ;", "for    ( String   localDir    :    localDirs )     {", "Path   fullAppDir    =    getApplicationDir ( new   Path ( localDir )  ,    user ,    appId )  ;", "try    {", "createDir ( fullAppDir ,    appperms ,    true )  ;", "initAppDirStatus    =    true ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Unable   to   create   app   directory    \"     +     ( fullAppDir . toString (  )  )  )  ,    e )  ;", "}", "}", "if    (  ! initAppDirStatus )     {", "throw   new   IOException (  (  (  \" Not   able   to   initialize   app   directories    \"     +     \" in   any   of   the   configured   local   directories   for   app    \"  )     +     ( appId . toString (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createAppDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "boolean   appLogDirStatus    =    false ;", "FsPermission   appLogDirPerms    =    new   FsPermission (  . LOGDIR _ PERM )  ;", "for    ( String   rootLogDir    :    logDirs )     {", "Path   appLogDir    =    new   Path ( rootLogDir ,    appId )  ;", "try    {", "createDir ( appLogDir ,    appLogDirPerms ,    true )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Unable   to   create   the   app - log   directory    :     \"     +    appLogDir )  ,    e )  ;", "continue ;", "}", "appLogDirStatus    =    true ;", "}", "if    (  ! appLogDirStatus )     {", "throw   new   IOException (  (  (  \" Not   able   to   initialize   app - log   directories    \"     +     \" in   any   of   the   configured   local   directories   for   app    \"  )     +    appId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createAppLogDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "boolean   containerLogDirStatus    =    false ;", "FsPermission   containerLogDirPerms    =    new   FsPermission (  . LOGDIR _ PERM )  ;", "for    ( String   rootLogDir    :    logDirs )     {", "Path   appLogDir    =    new   Path ( rootLogDir ,    appId )  ;", "Path   containerLogDir    =    new   Path ( appLogDir ,    containerId )  ;", "try    {", "createDir ( containerLogDir ,    containerLogDirPerms ,    true )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Unable   to   create   the   container - log   directory    :     \"     +    appLogDir )  ,    e )  ;", "continue ;", "}", "containerLogDirStatus    =    true ;", "}", "if    (  ! containerLogDirStatus )     {", "throw   new   IOException (  (  (  \" Not   able   to   initialize   container - log   directories    \"     +     \" in   any   of   the   configured   local   directories   for   container    \"  )     +    containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createContainerLogDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "lfs . mkdir ( dirPath ,    perms ,    createParent )  ;", "if    (  !  ( perms . equals ( permsplyUMask ( lfs . getUMask (  )  )  )  )  )     {", "lfs . setPermission ( dirPath ,    perms )  ;", "}", "}", "METHOD_END"], "methodName": ["createDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "DefaultContainerExecutor . LOG . info (  (  \" Initializing   user    \"     +    user )  )  ;", "boolean   appcacheDirStatus    =    false ;", "boolean   distributedCacheDirStatus    =    false ;", "FsPermission   appCachePerms    =    new   FsPermission ( DefaultContainerExecutor . APPCACHE _ PERM )  ;", "FsPermission   fileperms    =    new   FsPermission ( DefaultContainerExecutor . FILECACHE _ PERM )  ;", "for    ( String   localDir    :    localDirs )     {", "Path   localDirPath    =    new   Path ( localDir )  ;", "final   Path   appDir    =    getAppcacheDir ( localDirPath ,    user )  ;", "try    {", "createDir ( appDir ,    appCachePerms ,    true )  ;", "appcacheDirStatus    =    true ;", "}    catch    ( IOException   e )     {", "DefaultContainerExecutor . LOG . warn (  (  \" Unable   to   create   app   cache   directory    :     \"     +    appDir )  ,    e )  ;", "}", "final   Path   distDir    =    getFileCacheDir ( localDirPath ,    user )  ;", "try    {", "createDir ( distDir ,    fileperms ,    true )  ;", "distributedCacheDirStatus    =    true ;", "}    catch    ( IOException   e )     {", "DefaultContainerExecutor . LOG . warn (  (  \" Unable   to   create   file   cache   directory    :     \"     +    distDir )  ,    e )  ;", "}", "}", "if    (  ! appcacheDirStatus )     {", "throw   new   IOException (  (  (  \" Not   able   to   initialize   app - cache   directories    \"     +     \" in   any   of   the   configured   local   directories   for   user    \"  )     +    user )  )  ;", "}", "if    (  ! distributedCacheDirStatus )     {", "throw   new   IOException (  (  (  \" Not   able   to   initialize   distributed - cache   directories    \"     +     \" in   any   of   the   configured   local   directories   for   user    \"  )     +    user )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createUserCacheDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "boolean   userDirStatus    =    false ;", "FsPermission   userperms    =    new   FsPermission (  . USER _ PERM )  ;", "for    ( String   localDir    :    localDirs )     {", "try    {", "createDir ( getUserCacheDir ( new   Path ( localDir )  ,    user )  ,    userperms ,    true )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Unable   to   create   the   user   directory    :     \"     +    localDir )  ,    e )  ;", "continue ;", "}", "userDirStatus    =    true ;", "}", "if    (  ! userDirStatus )     {", "throw   new   IOException (  (  (  \" Not   able   to   initialize   user   directories    \"     +     \" in   any   of   the   configured   local   directories   for   user    \"  )     +    user )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createUserLocalDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( getUserCacheDir ( base ,    user )  ,    ContainerLocalizer . APPCACHE )  ;", "}", "METHOD_END"], "methodName": ["getAppcacheDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( getAppcacheDir ( base ,    user )  ,    appId )  ;", "}", "METHOD_END"], "methodName": ["getApplicationDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( getUserCacheDir ( base ,    user )  ,    ContainerLocalizer . FILECACHE )  ;", "}", "METHOD_END"], "methodName": ["getFileCacheDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   getApplicationDir ( new   Path ( localDirs . get (  0  )  )  ,    user ,    appId )  ;", "}", "METHOD_END"], "methodName": ["getFirstApplicationDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "List < Path >    paths    =    new   ArrayList < Path >  ( dirs . size (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( dirs . size (  )  )  ;    i +  +  )     {", "paths . add ( new   Path ( dirs . get ( i )  )  )  ;", "}", "return   paths ;", "}", "METHOD_END"], "methodName": ["getPaths"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( new   Path ( base ,    ContainerLocalizer . USERCACHE )  ,    user )  ;", "}", "METHOD_END"], "methodName": ["getUserCacheDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "new   ShellCommandExecutor ( Shell . getSignalKillCommand ( signal . getValue (  )  ,    pid )  )  . execute (  )  ;", "}", "METHOD_END"], "methodName": ["killContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   new   DeletionService . FileDeletionTask ( this ,    user ,    subDir ,    Arrays . asList ( baseDirs )  )  ;", "}", "METHOD_END"], "methodName": ["createFileDeletionTask"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( debugDelay )     !  =     (  -  1  )  )     {", "List < Path >    baseDirList    =    null ;", "if    (  ( baseDirs    !  =    null )     &  &     (  ( baseDirs . length )     !  =     0  )  )     {", "baseDirList    =    Arrays . asList ( baseDirs )  ;", "}", ". FileDeletionTask   task    =    new    . FileDeletionTask ( this ,    user ,    subDir ,    baseDirList )  ;", "recordDeletionTaskInStateStore ( task )  ;", "sched . schedule ( task ,    debugDelay ,    TimeUnit . SECONDS )  ;", "}", "}", "METHOD_END"], "methodName": ["delete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "int   taskId    =    nextTaskId . incrementAndGet (  )  ;", "while    ( taskId    =  =     (  . FileDeletionTask . INVALID _ TASK _ ID )  )     {", "taskId    =    nextTaskId . incrementAndGet (  )  ;", "}", "return   taskId ;", "}", "METHOD_END"], "methodName": ["generateTaskId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   FileContext . getLocalFSFileContext (  )  ;", "}    catch    ( UnsupportedFileSystemExcep   e )     {", "throw   new   RuntimeExcep ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getLfs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "return    (  ( getServiceState (  )  )     =  =     ( STATE . STOPPED )  )     &  &     ( sched . isTerminated (  )  )  ;", "}", "METHOD_END"], "methodName": ["isTerminated"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "int   taskId    =    proto . getId (  )  ;", "String   user    =     ( proto . hasUser (  )  )     ?    proto . getUser (  )     :    null ;", "Path   subdir    =    null ;", "List < Path >    basePaths    =    null ;", "if    ( proto . hasSubdir (  )  )     {", "subdir    =    new   Path ( proto . getSubdir (  )  )  ;", "}", "List < String >    basedirs    =    proto . getBasedirsList (  )  ;", "if    (  ( basedirs    !  =    null )     &  &     (  ( basedirs . size (  )  )     >     0  )  )     {", "basePaths    =    new   ArrayList < Path >  ( basedirs . size (  )  )  ;", "for    ( String   basedir    :    basedirs )     {", "basePaths . add ( new   Path ( basedir )  )  ;", "}", "}", ". FileDeletionTask   task    =    new    . FileDeletionTask ( taskId ,    this ,    user ,    subdir ,    basePaths )  ;", "return   new    . DeletionTaskRecoveryInfo ( task ,    proto . getSuccessorIdsList (  )  ,    proto . getDeletionTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["parseTaskProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( stateStore . canRecover (  )  )  )     {", "return ;", "}", "if    (  ( task . taskId )     !  =     (  . FileDeletionTask . INVALID _ TASK _ ID )  )     {", "return ;", "}", "task . taskId    =    generateTaskId (  )  ;", ". FileDeletionTask [  ]    successors    =    task . getSuccessorTasks (  )  ;", "for    (  . FileDeletionTask   successor    :    successors )     {", "recordDeletionTaskInStateStore ( successor )  ;", "}", "DeleteTaskProto . Builder   builder    =    DeleteTaskProto . newBuilder (  )  ;", "builder . setId ( task . taskId )  ;", "if    (  ( task . getUser (  )  )     !  =    null )     {", "builder . setUser ( task . getUser (  )  )  ;", "}", "if    (  ( task . getSubDir (  )  )     !  =    null )     {", "builder . setSubdir ( task . getSubDir (  )  . toString (  )  )  ;", "}", "builder . setDeletionTime (  (  ( System . currentTimeMillis (  )  )     +     ( TimeUnit . MILLISECONDS . convert ( debugDelay ,    TimeUnit . SECONDS )  )  )  )  ;", "if    (  ( task . getBaseDirs (  )  )     !  =    null )     {", "for    ( Path   dir    :    task . getBaseDirs (  )  )     {", "builder . addBasedirs ( dir . toString (  )  )  ;", "}", "}", "for    (  . FileDeletionTask   successor    :    successors )     {", "builder . addSuccessorIds ( successor . taskId )  ;", "}", "try    {", "stateStore . storeDeletionTask ( task . taskId ,    builder . build (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  (  \" Unable   to   store   deletion   task    \"     +     ( task . taskId )  )     +     \"    for    \"  )     +     ( task . getSubDir (  )  )  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["recordDeletionTaskInStateStore"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "List < DeletionServiceDeleteTaskProto >    taskProtos    =    state . getTasks (  )  ;", "Map < Integer ,    DeletionService . DeletionTaskRecoveryInfo >    idToInfoMap    =    new   HashMap < Integer ,    DeletionService . DeletionTaskRecoveryInfo >  ( taskProtos . size (  )  )  ;", "Set < Integer >    successorTasks    =    new   HashSet < Integer >  (  )  ;", "for    ( DeletionServiceDeleteTaskProto   proto    :    taskProtos )     {", "DeletionService . DeletionTaskRecoveryInfo   info    =    parseTaskProto ( proto )  ;", "idToInfoMap . put ( info . task . taskId ,    info )  ;", "nextTaskId . set ( Math . max ( nextTaskId . get (  )  ,    info . task . taskId )  )  ;", "successorTasks . addAll ( info . successorTaskIds )  ;", "}", "final   long   now    =    System . currentTimeMillis (  )  ;", "for    ( DeletionService . DeletionTaskRecoveryInfo   info    :    idToInfoMap . values (  )  )     {", "for    ( Integer   successorId    :    info . successorTaskIds )     {", "DeletionService . DeletionTaskRecoveryInfo   successor    =    idToInfoMap . get ( successorId )  ;", "if    ( successor    !  =    null )     {", "info . task . addFileDeletionTaskDependency ( successor . task )  ;", "} else    {", "DeletionService . LOG . error (  (  (  (  \" Unable   to   locate   dependency   task   for   deletion   task    \"     +     ( info . task . taskId )  )     +     \"    at    \"  )     +     ( info . task . getSubDir (  )  )  )  )  ;", "}", "}", "if    (  !  ( successorTasks . contains ( info . task . taskId )  )  )     {", "long   msecTilDeletion    =     ( info . deletionTimestamp )     -    now ;", "sched . schedule ( info . task ,    msecTilDeletion ,    TimeUnit . MILLISECONDS )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["recover"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( debugDelay )     !  =     (  -  1  )  )     {", "recordTaskInStateStore ( fileTask )  ;", "sched . schedule ( fileTask ,    debugDelay ,    TimeUnit . SECONDS )  ;", "}", "}", "METHOD_END"], "methodName": ["scheduleFileDeletionTask"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DeletionService"}, {"methodBody": ["METHOD_START", "{", "int   oldNumFailures    =    numFailures ;", "HashSet < String >    checkFailedDirs    =    new   HashSet < String >  (  )  ;", "for    ( final   String   dir    :    localDirs )     {", "try    {", "File   testDir    =    new   File ( dir )  ;", "DiskChecker . checkDir ( testDir )  ;", "if    ( isDiskUsageUnderPercentageLimit ( testDir )  )     {", ". LOG . warn (  (  (  (  (  \" Directory    \"     +    dir )     +     \"    error ,    used   space   above   threshold   of    \"  )     +     ( diskUtilizationPercentageCutoff )  )     +     \"  %  ,    removing   from   the   list   of   valid   directories .  \"  )  )  ;", "checkFailedDirs . add ( dir )  ;", "} else", "if    ( isDiskFreeSpaceWithinLimit ( testDir )  )     {", ". LOG . warn (  (  (  (  (  \" Directory    \"     +    dir )     +     \"    error ,    free   space   below   limit   of    \"  )     +     ( diskUtilizationSpaceCutoff )  )     +     \" MB ,    removing   from   the   list   of   valid   directories .  \"  )  )  ;", "checkFailedDirs . add ( dir )  ;", "}", "}    catch    ( DiskErrorException   de )     {", ". LOG . warn (  (  (  (  (  \" Directory    \"     +    dir )     +     \"    error    \"  )     +     ( de . getMessage (  )  )  )     +     \"  ,    removing   from   the   list   of   valid   directories .  \"  )  )  ;", "checkFailedDirs . add ( dir )  ;", "}", "}", "for    ( String   dir    :    checkFailedDirs )     {", "localDirs . remove ( dir )  ;", "failedDirs . add ( dir )  ;", "( numFailures )  +  +  ;", "}", "return    ( numFailures )     >    oldNumFailures ;", "}", "METHOD_END"], "methodName": ["checkDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "if    ( dir    =  =    null )     {", "return ;", "}", "try    {", "localFs . getFileStatus ( dir )  ;", "}    catch    ( FileNotFoundExcep   e )     {", "createDir ( localFs ,    dir . getParent (  )  ,    perm )  ;", "localFs . mkdir ( dir ,    perm ,    false )  ;", "if    (  !  ( perm . equals ( perm . applyUMask ( localFs . getUMask (  )  )  )  )  )     {", "localFs . setPermission ( dir ,    perm )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["createDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "boolean   failed    =    false ;", "for    ( final   String   dir    :    localDirs )     {", "try    {", "createDir ( localFs ,    new   Path ( dir )  ,    perm )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  (  (  (  \" Unable   to   create   directory    \"     +    dir )     +     \"    error    \"  )     +     ( e . getMessage (  )  )  )     +     \"  ,    removing   from   the   list   of   valid   directories .  \"  )  )  ;", "localDirs . remove ( dir )  ;", "failedDirs . add ( dir )  ;", "( numFailures )  +  +  ;", "failed    =    true ;", "}", "}", "return    ! failed ;", "}", "METHOD_END"], "methodName": ["createNonExistentDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "return   diskUtilizationPercentageCutoff ;", "}", "METHOD_END"], "methodName": ["getDiskUtilizationPercentageCutoff"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "return   diskUtilizationSpaceCutoff ;", "}", "METHOD_END"], "methodName": ["getDiskUtilizationSpaceCutoff"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "return   Collections . unmodifiableList ( failedDirs )  ;", "}", "METHOD_END"], "methodName": ["getFailedDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "return   Collections . unmodifiableList ( localDirs )  ;", "}", "METHOD_END"], "methodName": ["getGoodDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "return   numFailures ;", "}", "METHOD_END"], "methodName": ["getNumFailures"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "long   freeSpace    =     ( dir . getUsableSpace (  )  )     /     (  1  0  2  4     *     1  0  2  4  )  ;", "if    ( freeSpace    <     ( this . diskUtilizaSpaceCutoff )  )     {", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isDiskFreeSpaceWithinLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "float   freePercentage    =     1  0  0     *     (  ( dir . getUsableSpace (  )  )     /     (  ( float )     ( dir . getTotalSpace (  )  )  )  )  ;", "float   usedPercentage    =     1  0  0  .  0 F    -    freePercentage ;", "if    (  ( usedPercentage    >     ( diskUtilizaPercentageCutoff )  )     |  |     ( usedPercentage    >  =     1  0  0  .  0 F )  )     {", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isDiskUsageUnderPercentageLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "this . diskUtilizationPercentageCutoff    =     ( diskUtilizationPercentageCutoff    <     0  .  0 F )     ?     0  .  0 F    :    diskUtilizationPercentageCutoff    >     1  0  0  .  0 F    ?     1  0  0  .  0 F    :    diskUtilizationPercentageCutoff ;", "}", "METHOD_END"], "methodName": ["setDiskUtilizationPercentageCutoff"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "diskUtilizationSpaceCutoff    =     ( diskUtilizationSpaceCutoff    <     0  )     ?     0     :    diskUtilizationSpaceCutoff ;", "this . diskUtilizationSpaceCutoff    =    diskUtilizationSpaceCutoff ;", "}", "METHOD_END"], "methodName": ["setDiskUtilizationSpaceCutoff"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.DirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "if    ( containerSchedPriorityIsSet )     {", "command . addAll ( Arrays . asList (  \" nice \"  ,     \"  - n \"  ,    Integer . toString ( containerSchedPriorityAdjustment )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addSchedPriorityCommand"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "String   yarnHomeEnvVar    =    System . getenv ( HADOOP _ YARN _ HOME . key (  )  )  ;", "File   hadoopBin    =    new   File ( yarnHomeEnvVar ,     \" bin \"  )  ;", "String   defaultPath    =    new   File ( hadoopBin ,     \" c - executor \"  )  . getAbsolutePath (  )  ;", "return   null    =  =    conf    ?    defaultPath    :    conf . get ( NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH ,    defaultPath )  ;", "}", "METHOD_END"], "methodName": ["getContainerExecutorExecutablePath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    (  ( UserGroupInformation . isSecurityEnabled (  )  )     |  |     (  !  ( containerLimitUsers )  )  )     {", "return   user ;", "} else    {", "return   nonsecureLocalUser ;", "}", "}", "METHOD_END"], "methodName": ["getRunAsUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "List < String >    command    =    new   ArrayList < String >  ( Arrays . asList ( containerExecutorExe ,     \"  -  - mount - cgroups \"  ,    hierarchy )  )  ;", "command . addAll ( cgroupKVs )  ;", "String [  ]    commandArray    =    command . toArray ( new   String [ command . size (  )  ]  )  ;", "ShellCommandExecutor   shExec    =    new   ShellCommandExecutor ( commandArray )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" mountCgroups :     \"     +     ( Arrays . toString ( commandArray )  )  )  )  ;", "}", "try    {", "shExec . execute (  )  ;", "}    catch    ( IOException   e )     {", "int   ret _ code    =    shExec . getExitCode (  )  ;", ". LOG . warn (  \" Exception   in      mountCgroups    \"  ,    e )  ;", "logOutput ( shExec . getOutput (  )  )  ;", "throw   new   IOException (  (  (  (  (  (  \" Problem   mounting   cgroups    \"     +    cgroupKVs )     +     \"  ;    exit   code    =     \"  )     +    ret _ code )     +     \"    and   output :     \"  )     +     ( shExec . getOutput (  )  )  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["mountCgroups"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    (  (  !  ( UserGroupInformation . isSecurityEnabled (  )  )  )     &  &     (  !  ( nonsecureLocalUserPattern . matcher ( user )  . matches (  )  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  (  \" Invalid   user   name    '  \"     +    user )     +     \"  '  ,  \"  )     +     \"    it   must   match    '  \"  )     +     ( nonsecureLocalUserPattern . pattern (  )  )  )     +     \"  '  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyUsernamePattern"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isDiskHealthCheckerEnabled )  )     {", "return   true ;", "}", "int   goodDirs    =    get (  )  . size (  )  ;", "int   failedDirs    =    localDirs . getFailedDirs (  )  . size (  )  ;", "int   totalConfiguredDirs    =    goodDirs    +    failedDirs ;", "if    (  ( goodDirs    /     (  ( float )     ( totalConfiguredDirs )  )  )     <     ( minNeededHealthyDisksFactor )  )     {", "return   false ;", "}", "goodDirs    =    getLogDirs (  )  . size (  )  ;", "failedDirs    =    logDirs . getFailedDirs (  )  . size (  )  ;", "totalConfiguredDirs    =    goodDirs    +    failedDirs ;", "if    (  ( goodDirs    /     (  ( float )     ( totalConfiguredDirs )  )  )     <     ( minNeededHealthyDisksFactor )  )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["areDisksHealthy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "boolean   newFailure    =    false ;", "if    ( l . checkDirs (  )  )     {", "newFailure    =    true ;", "}", "if    ( logDirs . checkDirs (  )  )     {", "newFailure    =    true ;", "}", "if    ( newFailure )     {", "updateDirsAfterFailure (  )  ;", "}", "lastDisksCheckTime    =    System . currentTimeMillis (  )  ;", "}", "METHOD_END"], "methodName": ["checkDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isDiskHealthCheckerEnabled )  )     {", "return    \"  \"  ;", "}", "StringBuilder   report    =    new   StringBuilder (  )  ;", "List < String >    failedList    =    localDirs . getFailedDirs (  )  ;", "List < String >    failedLogDirsList    =    logDirs . getFailedDirs (  )  ;", "int   num    =     ( localDirs . getGoodDirs (  )  . size (  )  )     +     ( failedList . size (  )  )  ;", "int   numLogDirs    =     ( logDirs . getGoodDirs (  )  . size (  )  )     +     ( failedLogDirsList . size (  )  )  ;", "if    (  !  ( failedList . isEmpty (  )  )  )     {", "report . append (  (  (  (  (  (  ( failedList . size (  )  )     +     \"  /  \"  )     +    num )     +     \"    local - dirs   turned   bad :     \"  )     +     ( StringUtils . join (  \"  ,  \"  ,    failedList )  )  )     +     \"  ;  \"  )  )  ;", "}", "if    (  !  ( failedLogDirsList . isEmpty (  )  )  )     {", "report . append (  (  (  (  (  ( failedLogDirsList . size (  )  )     +     \"  /  \"  )     +    numLogDirs )     +     \"    log - dirs   turned   bad :     \"  )     +     ( StringUtils . join (  \"  ,  \"  ,    failedLogDirsList )  )  )  )  ;", "}", "return   report . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getDisksHealthReport"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   lastDisksCheckTime ;", "}", "METHOD_END"], "methodName": ["getLastDisksCheckTime"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   localDirs . getGoodDirs (  )  ;", "}", "METHOD_END"], "methodName": ["getLocalDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   localDirsAllocator . getLocalPathForWrite ( pathStr ,    getConfig (  )  )  ;", "}", "METHOD_END"], "methodName": ["getLocalPathForWrite"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   localDirsAllocator . getLocalPathForWrite ( pathStr ,    size ,    getConfig (  )  ,    checkWrite )  ;", "}", "METHOD_END"], "methodName": ["getLocalPathForWrite"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   logDirs . getGoodDirs (  )  ;", "}", "METHOD_END"], "methodName": ["getLogDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   logDirsAllocator . getLocalPathForWrite ( pathStr ,    SIZE _ UNKNOWN ,    getConfig (  )  ,    checkWrite )  ;", "}", "METHOD_END"], "methodName": ["getLogPathForWrite"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   logDirsAllocator . getLocalPathToRead ( pathStr ,    getConfig (  )  )  ;", "}", "METHOD_END"], "methodName": ["getLogPathToRead"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "LocalDirsHandlerService . LOG . info (  (  \" Disk ( s )    failed .     \"     +     ( getDisksHealthReport (  )  )  )  )  ;", "Configuration   conf    =    getConfig (  )  ;", "List < String >    localDirs    =    getLocalDirs (  )  ;", "conf . setStrings ( NM _ LOCAL _ DIRS ,    localDirs . toArray ( new   String [ localDirs . size (  )  ]  )  )  ;", "List < String >    logDirs    =    getLogDirs (  )  ;", "conf . setStrings ( NM _ LOG _ DIRS ,    logDirs . toArray ( new   String [ logDirs . size (  )  ]  )  )  ;", "if    (  !  ( areDisksHealthy (  )  )  )     {", "LocalDirsHandlerService . LOG . error (  (  \" Most   of   the   disks   failed .     \"     +     ( getDisksHealthReport (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateDirsAfterFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "ArrayList < String >    validPaths    =    new   ArrayList < String >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( paths . length )  ;     +  + i )     {", "try    {", "URI   uriPath    =    new   Path ( paths [ i ]  )  . toUri (  )  ;", "if    (  (  ( uriPath . getScheme (  )  )     =  =    null )     |  |     ( uriPath . getScheme (  )  . equals (  . FILE _ SCHEME )  )  )     {", "validPaths . add ( new   Path ( uriPath . getPath (  )  )  . toString (  )  )  ;", "} else    {", ". LOG . warn (  (  (  (  ( paths [ i ]  )     +     \"    is   not   a   valid   path .    Path   should   be   with    \"  )     +     (  . FILE _ SCHEME )  )     +     \"    scheme   or   without   scheme \"  )  )  ;", "throw   new   YarnRuntimeException (  (  (  (  ( paths [ i ]  )     +     \"    is   not   a   valid   path .    Path   should   be   with    \"  )     +     (  . FILE _ SCHEME )  )     +     \"    scheme   or   without   scheme \"  )  )  ;", "}", "}    catch    ( IllegalArgumentException   e )     {", ". LOG . warn ( e . getMessage (  )  )  ;", "throw   new   YarnRuntimeException (  (  (  (  ( paths [ i ]  )     +     \"    is   not   a   valid   path .    Path   should   be   with    \"  )     +     (  . FILE _ SCHEME )  )     +     \"    scheme   or   without   scheme \"  )  )  ;", "}", "}", "String [  ]    arrValidPaths    =    new   String [ validPaths . size (  )  ]  ;", "validPaths . toArray ( arrValidPaths )  ;", "return   arrValidPaths ;", "}", "METHOD_END"], "methodName": ["validatePaths"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "return   new   MockNodeStatusUpdater . MockResourceTracker (  )  ;", "}", "METHOD_END"], "methodName": ["createResourceTracker"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.MockNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "b . append ( NMAuditLogger . AuditConstants . PAIR _ SEPARATOR )  . append ( key . name (  )  )  . append ( NMAuditLogger . AuditConstants . KEY _ VAL _ SEPARATOR )  . append ( value )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "if    ( ip    !  =    null )     {", ". add (  . Keys . IP ,    ip . getHostAddress (  )  ,    b )  ;", "}", "}", "METHOD_END"], "methodName": ["addRemoteIP"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   b    =    new   StringBuilder (  )  ;", ". start (  . Keys . USER ,    user ,    b )  ;", ". addRemoteIP ( b )  ;", ". add (  . Keys . OPERATION ,    operation ,    b )  ;", ". add (  . Keys . TARGET ,    target ,    b )  ;", ". add (  . Keys . RESULT ,     . AuditConstants . FAILURE ,    b )  ;", ". add (  . Keys . DESCRIPTION ,    description ,    b )  ;", "if    ( appId    !  =    null )     {", ". add (  . Keys . APPID ,    appId . toString (  )  ,    b )  ;", "}", "if    ( containerId    !  =    null )     {", ". add (  . Keys . CONTAINERID ,    containerId . toString (  )  ,    b )  ;", "}", "return   b . toString (  )  ;", "}", "METHOD_END"], "methodName": ["createFailureLog"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   b    =    new   StringBuilder (  )  ;", ". start (  . Keys . USER ,    user ,    b )  ;", ". addRemoteIP ( b )  ;", ". add (  . Keys . OPERATION ,    operation ,    b )  ;", ". add (  . Keys . TARGET ,    target ,    b )  ;", ". add (  . Keys . RESULT ,     . AuditConstants . SUCCESS ,    b )  ;", "if    ( appId    !  =    null )     {", ". add (  . Keys . APPID ,    appId . toString (  )  ,    b )  ;", "}", "if    ( containerId    !  =    null )     {", ". add (  . Keys . CONTAINERID ,    containerId . toString (  )  ,    b )  ;", "}", "return   b . toString (  )  ;", "}", "METHOD_END"], "methodName": ["createSuccessLog"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( NMAuditLogger . LOG . isWarnEnabled (  )  )     {", "NMAuditLogger . LOG . warn ( NMAuditLogger . createFailureLog ( user ,    operation ,    target ,    description ,    null ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( NMAuditLogger . LOG . isWarnEnabled (  )  )     {", "NMAuditLogger . LOG . warn ( NMAuditLogger . createFailureLog ( user ,    operation ,    target ,    description ,    appId ,    containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( NMAuditLogger . LOG . isInfoEnabled (  )  )     {", "NMAuditLogger . LOG . info ( NMAuditLogger . createSuccessLog ( user ,    operation ,    target ,    null ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logSuccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( NMAuditLogger . LOG . isInfoEnabled (  )  )     {", "NMAuditLogger . LOG . info ( NMAuditLogger . createSuccessLog ( user ,    operation ,    target ,    appId ,    containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logSuccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "b . append ( key . name (  )  )  . append ( NMAuditLogger . AuditConstants . KEY _ VAL _ SEPARATOR )  . append ( value )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "return   dirsHandler ;", "}", "METHOD_END"], "methodName": ["getDiskHandler"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService"}, {"methodBody": ["METHOD_START", "{", "String   scriptReport    =     (  ( nodeHealthScriptRunner )     =  =    null )     ?     \"  \"     :    nodeHealthScriptRunner . getHealthReport (  )  ;", "if    ( scriptReport . equals (  \"  \"  )  )     {", "return   dirsHandler . getDisksHealthReport (  )  ;", "} else    {", "return   scriptReport . concat (  (  (  . SEPARATOR )     +     ( dirsHandler . getDisksHealthReport (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getHealthReport"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService"}, {"methodBody": ["METHOD_START", "{", "long   diskCheckTime    =    dirsHandler . getLastDisksCheckTime (  )  ;", "long   lastReportTime    =     (  ( nScriptRunner )     =  =    null )     ?    diskCheckTime    :    Math . max ( nScriptRunner . getLastReportedTime (  )  ,    diskCheckTime )  ;", "return   lastReportTime ;", "}", "METHOD_END"], "methodName": ["getLastHealthReportTime"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService"}, {"methodBody": ["METHOD_START", "{", "return   nodeHealthScriptRunner ;", "}", "METHOD_END"], "methodName": ["getNodeHealthScriptRunner"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService"}, {"methodBody": ["METHOD_START", "{", "boolean   scriptHealthStatus    =     (  ( nodeHealthScriptRunner )     =  =    null )     ?    true    :    nodeHealthScriptRunner . isHealthy (  )  ;", "return   scriptHealthStatus    &  &     ( dirsHandler . areDisksHealthy (  )  )  ;", "}", "METHOD_END"], "methodName": ["isHealthy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService"}, {"methodBody": ["METHOD_START", "{", "return   healthReport ;", "}", "METHOD_END"], "methodName": ["getHealthReport"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "return   lastReportedTime ;", "}", "METHOD_END"], "methodName": ["getLastReportedTime"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "return   timer ;", "}", "METHOD_END"], "methodName": ["getTimerTask"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "return   isHealthy ;", "}", "METHOD_END"], "methodName": ["isHealthy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "this . healthReport    =    healthReport ;", "}", "METHOD_END"], "methodName": ["setHealthReport"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "this . setHealthy ( isHealthy )  ;", "this . setHealthReport ( output )  ;", "}", "METHOD_END"], "methodName": ["setHealthStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "this . setHealthStatus ( isHealthy ,    output )  ;", "this . setLastReportedTime ( time )  ;", "}", "METHOD_END"], "methodName": ["setHealthStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "this . isHealthy    =    isHealthy ;", "}", "METHOD_END"], "methodName": ["setHealthy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "this . lastReportedTime    =    lastReportedTime ;", "}", "METHOD_END"], "methodName": ["setLastReportedTime"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "String   nodeHealthScript    =    conf . get ( NM _ HEALTH _ CHECK _ SCRIPT _ PATH )  ;", "if    (  ( nodeHealthScript    =  =    null )     |  |     ( nodeHealthScript . trim (  )  . isEmpty (  )  )  )     {", "return   false ;", "}", "File   f    =    new   File ( nodeHealthScript )  ;", "return    ( f . exists (  )  )     &  &     ( FileUtil . canExecute ( f )  )  ;", "}", "METHOD_END"], "methodName": ["shouldRun"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeHealthScriptRunner"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerManagerImpl ( context ,    exec ,    del ,    nodeStatusUpdater ,    metrics ,    aclsManager ,    dirsHandler )  ;", "}", "METHOD_END"], "methodName": ["createContainerManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   new   DeletionService ( exec ,    nmStore )  ;", "}", "METHOD_END"], "methodName": ["createDeletionService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeManager . NMContext ( containerTokenSecretManager ,    nmTokenSecretManager ,    dirsHandler ,    aclsManager ,    stateStore )  ;", "}", "METHOD_END"], "methodName": ["createNMContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeManager (  )  ;", "}", "METHOD_END"], "methodName": ["createNewNodeManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeResourceMonitorImpl (  )  ;", "}", "METHOD_END"], "methodName": ["createNodeResourceMonitor"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeStatusUpdaterImpl ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", "}", "METHOD_END"], "methodName": ["createNodeStatusUpdater"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   new   WebServer ( nmContext ,    resourceView ,    aclsManager ,    dirsHandler )  ;", "}", "METHOD_END"], "methodName": ["createWebServer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "SecurityUtil . login ( getConfig (  )  ,    NM _ KEYTAB ,    NM _ PRINCIPAL )  ;", "}", "METHOD_END"], "methodName": ["doSecureLogin"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   containerManager ;", "}", "METHOD_END"], "methodName": ["getContainerManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   this . context ;", "}", "METHOD_END"], "methodName": ["getNMContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   dispatcher ;", "}", "METHOD_END"], "methodName": ["getNMDispatcher"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return    \" NodeManager \"  ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   nodeHealthChecker ;", "}", "METHOD_END"], "methodName": ["getNodeHealthChecker"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   nodeStatusUpdater ;", "}", "METHOD_END"], "methodName": ["getNodeStatusUpdater"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    ( hasToReboot    &  &     ( null    !  =     (  . nodeManagerShutdownHook )  )  )     {", "ShutdownHookManager . get (  )  . removeShutdownHook (  . nodeManagerShutdownHook )  ;", "}", ". nodeManagerShutdownHook    =    new   CompositeServiceShutdownHook ( this )  ;", "ShutdownHookManager . get (  )  . addShutdownHook (  . nodeManagerShutdownHook ,     . SHUTDOWN _ HOOK _ PRIORITY )  ;", "this . init ( conf )  ;", "this . start (  )  ;", "}    catch    ( Throwable   t )     {", ". LOG . fatal (  \" Error   starting    \"  ,    t )  ;", "System . exit (  (  -  1  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initAndStartNodeManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "boolean   recoveryEnabled    =    conf . getBoolean ( NM _ RECOVERY _ ENABLED ,    DEFAULT _ NM _ RECOVERY _ ENABLED )  ;", "if    ( recoveryEnabled )     {", "FileSystem   recoveryFs    =    FileSystem . getLocal ( conf )  ;", "String   recoveryDirName    =    conf . get ( NM _ RECOVERY _ DIR )  ;", "if    ( recoveryDirName    =  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" Recovery   is   enabled   but    \"     +     ( YarnConfiguration . NM _ RECOVERY _ DIR )  )     +     \"    is   not   set .  \"  )  )  ;", "}", "Path   recoveryRoot    =    new   Path ( recoveryDirName )  ;", "recoveryFs . mkdirs ( recoveryRoot ,    new   FsPermission (  (  ( short )     (  4  4  8  )  )  )  )  ;", "nmStore    =    new   NMLeveldbStateStoreService (  )  ;", "} else    {", "nmStore    =    new   NMNullStateStoreService (  )  ;", "}", "nmStore . init ( conf )  ;", "nmStore . start (  )  ;", "}", "METHOD_END"], "methodName": ["initAndStartRecoveryStore"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "Thread . setDefaultUncaughtExceptionHandler ( new   YarnUncaughtExceptionHandler (  )  )  ;", "StringUtils . startupShutdownMessage (  . class ,    args ,     . LOG )  ;", "nodeManager    =    new    (  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "nodeManager . initAndStart ( conf ,    false )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "if    ( nmStore . canRecover (  )  )     {", "nmTokenSecret . recover (  )  ;", "containerTokenSecret . recover (  )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverTokens"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "try    {", ". LOG . info (  \" Notifying   ContainerManager   to   block   new   container - requests \"  )  ;", "containerManager . setBlockNewContainerRequests ( true )  ;", "if    (  !  ( rmWorkPreservingRestartEnabled )  )     {", ". LOG . info (  \" Cleaning   up   running   containers   on   resync \"  )  ;", "containerManager . cleanupContainersOnNMResync (  )  ;", "} else    {", ". LOG . info (  \" Preserving   containers   on   resync \"  )  ;", "}", "(  ( NodeStatusUpdaterImpl )     ( nodeStatusUpdater )  )  . rebootNodeStatusUpdaterAndRegisterWithRM (  )  ;", "}    catch    ( YarnRuntimeException   e )     {", ". LOG . fatal (  \" Error   while   rebooting   NodeStatusUpdater .  \"  ,    e )  ;", "shutDown (  )  ;", "}", "}", "}  . start (  )  ;", "}", "METHOD_END"], "methodName": ["resyncWithRM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "new   Thread (  )     {", "@ Override", "public   void   run (  )     {", ". this . stop (  )  ;", "}", "}  . start (  )  ;", "}", "METHOD_END"], "methodName": ["shutDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "nmStore . stop (  )  ;", "if    (  ( context . getDecommissioned (  )  )     &  &     ( nmStore . canRecover (  )  )  )     {", ". LOG . info (  \" Removing   state   store   due   to   decommission \"  )  ;", "Configuration   conf    =    getConfig (  )  ;", "Path   recoveryRoot    =    new   Path ( conf . get ( NM _ RECOVERY _ DIR )  )  ;", ". LOG . info (  (  (  \" Removing   state   store   at    \"     +    recoveryRoot )     +     \"    due   to   decommission \"  )  )  ;", "FileSystem   recoveryFs    =    FileSystem . getLocal ( conf )  ;", "if    (  !  ( recoveryFs . delete ( recoveryRoot ,    true )  )  )     {", ". LOG . warn (  (  \" Unable   to   delete    \"     +    recoveryRoot )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["stopRecoveryStore"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( tokenKeepAliveEnabled )  )     {", "return   Collections . emptyList (  )  ;", "}", "List < ApplicationId > pList    =    new   ArrayList < ApplicationId >  (  )  ;", "for    ( Iterator <  . Entry < ApplicationId ,    Long >  >    i    =    thispTokenKeepAlive . entrySet (  )  . iterator (  )  ;    i . hasNext (  )  ;  )     {", ". Entry < ApplicationId ,    Long >    e    =    i . next (  )  ;", "ApplicationIdpId    =    e . getKey (  )  ;", "Long   nextKeepAlive    =    e . getValue (  )  ;", "if    (  !  ( this . context . getApplications (  )  . containsKeypId )  )  )     {", "i . remove (  )  ;", "} else", "if    (  ( System . currentTimeMillis (  )  )     >    nextKeepAlive )     {", "pList . addpId )  ;", "trackAppForKeepAlivepId )  ;", "}", "}", "returnpList ;", "}", "METHOD_END"], "methodName": ["createKeepAliveApplicationList"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "List < ContainerStatus >    containerStatuses    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( Container   container    :    this . context . getContainers (  )  . values (  )  )     {", "ContainerStatus   containerStatus    =    container . cloneAndGetContainerStatus (  )  ;", "containerStatuses . add ( containerStatus )  ;", "if    ( containerStatus . getState (  )  . equals ( COMPLETE )  )     {", "addCompletedContainer ( container . getContainerId (  )  )  ;", "}", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Sending   out    \"     +     ( containerStatuses . size (  )  )  )     +     \"    container   statuses :     \"  )     +    containerStatuses )  )  ;", "}", "return   containerStatuses ;", "}", "METHOD_END"], "methodName": ["getContainerStatuses"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "List < NMContainerStatus >    containerStatuses    =    new   ArrayList < NMContainerStatus >  (  )  ;", "for    ( Container   container    :    this . context . getContainers (  )  . values (  )  )     {", "NMContainerStatus   status    =    container . getNMContainerStatus (  )  ;", "containerStatuses . add ( status )  ;", "if    ( status . getContainerState (  )  . equals ( COMPLETE )  )     {", "addCompletedContainer ( container . getContainerId (  )  )  ;", "}", "}", ". LOG . info (  (  (  (  \" Sending   out    \"     +     ( containerStatuses . size (  )  )  )     +     \"    NM   container   statuses :     \"  )     +    containerStatuses )  )  ;", "return   containerStatuses ;", "}", "METHOD_END"], "methodName": ["getNMContainerStatuses"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "NodeHealthStatus   nodeHealthStatus    =    this . context . getNodeHealthStatus (  )  ;", "nodeHealthStatus . setHealthReport ( healthChecker . getHealthReport (  )  )  ;", "nodeHealthStatus . setIsNodeHealthy ( healthChecker . isHealthy (  )  )  ;", "nodeHealthStatus . setLastHealthReportTime ( healthChecker . getLastHealthReportTime (  )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Node ' s   health - status    :     \"     +     ( nodeHealthStatus . getIsNodeHealthy (  )  )  )     +     \"  ,     \"  )     +     ( nodeHealthStatus . getHealthReport (  )  )  )  )  ;", "}", "List < ContainerStatus >    containersStatuses    =    getContainerStatuses (  )  ;", "NodeStatus   nodeStatus    =    NodeStatus . newInstance ( nodeId ,    responseId ,    containersStatuses ,    createKeepAliveApplicationList (  )  ,    nodeHealthStatus )  ;", "return   nodeStatus ;", "}", "METHOD_END"], "methodName": ["getNodeStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "return   SRMProxy . createRMProxy ( conf ,    ResourceTracker . class )  ;", "}", "METHOD_END"], "methodName": ["getRMClient"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "List < ApplicationId >    runningApplications    =    new   ArrayList < ApplicationId >  (  )  ;", "runningApplications . addAll ( this . context . getApplications (  )  . keySet (  )  )  ;", "return   runningApplications ;", "}", "METHOD_END"], "methodName": ["getRunningApplications"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "synchronized ( recentlyStoppedContainers )     {", "return   recentlyStoppedContainers . containsKey ( containd )  ;", "}", "}", "METHOD_END"], "methodName": ["isContainerRecentlyStopped"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "return    ( conf . getBoolean ( LOG _ AGGREGATION _ ENABLED ,    DEFAULT _ LOG _ AGGREGATION _ ENABLED )  )     &  &     ( UserGroupInformation . isSecurityEnabled (  )  )  ;", "}", "METHOD_END"], "methodName": ["isTokenKeepAliveEnabled"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "this . isStopped    =    true ;", "try    {", "statusUpdater . join (  )  ;", "registerWithRM (  )  ;", "statusUpdater    =    new   Thread ( statusUpdaterRunnable ,     \" Node   Status   Updater \"  )  ;", "this . isStopped    =    false ;", "statusUpdater . start (  )  ;", ". LOG . info (  \" NodeStatusUpdater   thread   is   reRegistered   and   restarted \"  )  ;", "}    catch    ( Exception   e )     {", "String   errorMessage    =     \" Unexpected   error   rebooting   NodeStatusUpdater \"  ;", ". LOG . error ( errorMessage ,    e )  ;", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["rebootNodeStatusUpdaterAndRegisterWithRM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "List < NMContainerStatus >    containerReports    =    getNMContainerStatuses (  )  ;", "RegisterNodeManagerRequest   request    =    RegisterNodeManagerRequest . newInstance ( nodeId ,    httpPort ,    totalResource ,    nodeManagerVersionId ,    containerReports ,    getRunningApplications (  )  )  ;", "if    ( containerReports    !  =    null )     {", ". LOG . info (  (  \" Registering   with   RM   using   containers    :  \"     +    containerReports )  )  ;", "}", "RegisterNodeManagerResponse   regNMResponse    =    resourceTracker . registerNodeManager ( request )  ;", "this . rmIdentifier    =    regNMResponse . getRMIdentifier (  )  ;", "if    ( NodeAction . SHUTDOWN . equals ( regNMResponse . getNodeAction (  )  )  )     {", "String   message    =     \" Message   from   ResourceManager :     \"     +     ( regNMResponse . getDiagnosticsMessage (  )  )  ;", "throw   new   YarnRuntimeException (  (  \" Recieved   SHUTDOWN   signal   from   Resourcemanager    , Registration   of   NodeManager   failed ,     \"     +    message )  )  ;", "}", "if    (  !  ( minimumResourceManagerVersion . equals (  \" NONE \"  )  )  )     {", "if    ( minimumResourceManagerVersion . equals (  \" EqualToNM \"  )  )     {", "minimumResourceManagerVersion    =    nodeManagerVersionId ;", "}", "String   rmVersion    =    regNMResponse . getRMVersion (  )  ;", "if    ( rmVersion    =  =    null )     {", "String   message    =     \" The   Resource   Manager ' s   did   not   return   a   version .     \"     +     \" Valid   version   cannot   be   checked .  \"  ;", "throw   new   YarnRuntimeException (  (  \" Shutting   down   the   Node   Manager .     \"     +    message )  )  ;", "}", "if    (  ( VersionUtil . compareVersions ( rmVersion ,    minimumResourceManagerVersion )  )     <     0  )     {", "String   message    =     (  (  (  \" The   Resource   Manager ' s   version    (  \"     +    rmVersion )     +     \"  )    is   less   than   the   minimum    \"  )     +     \" allowed   version    \"  )     +     ( minimumResourceManagerVersion )  ;", "throw   new   YarnRuntimeException (  (  (  \" Shutting   down   the   Node   Manager   on   RM    \"     +     \" version   error ,     \"  )     +    message )  )  ;", "}", "}", "MasterKey   masterKey    =    regNMResponse . getContainerTokenMasterKey (  )  ;", "if    ( masterKey    !  =    null )     {", "this . context . getContainerTokenSecretManager (  )  . setMasterKey ( masterKey )  ;", "}", "masterKey    =    regNMResponse . getNMTokenMasterKey (  )  ;", "if    ( masterKey    !  =    null )     {", "this . context . getNMTokenSecretManager (  )  . setMasterKey ( masterKey )  ;", "}", ". LOG . info (  (  (  (  \" Registered   with   ResourceManager   as    \"     +     ( this . nodeId )  )     +     \"    with   total   resource   of    \"  )     +     ( this . totalResource )  )  )  ;", ". LOG . info (  \" Notifying   ContainerManager   to   unblock   new   container - requests \"  )  ;", "(  ( ContainerManagerImpl )     ( this . context . getContainerManager (  )  )  )  . setBlockNewContainerRequests ( false )  ;", "}", "METHOD_END"], "methodName": ["registerWithRM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "synchronized ( previousCompletedContainers )     {", "if    (  !  ( previousCompletedContainers . isEmpty (  )  )  )     {", "for    ( ContainerId   containerId    :    previousCompletedContainers )     {", "this . context . getContainers (  )  . remove ( containerId )  ;", "}", ". LOG . info (  (  \" Removed   completed   containers   from   NM   context :     \"     +     ( previousCompletedContainers )  )  )  ;", "previousCompletedContainers . clear (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["removeCompletedContainersFromContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "synchronized ( recentlyStoppedContainers )     {", "long   currentTime    =    System . currentTimeMillis (  )  ;", "Iterator < ContainerId >    i    =    recentlyStoppedContainers . keySet (  )  . iterator (  )  ;", "while    ( i . hasNext (  )  )     {", "ContainerId   cid    =    i . next (  )  ;", "if    (  ( recentlyStoppedContainers . get ( cid )  )     <    currentTime )     {", "i . remove (  )  ;", "try    {", "context . getNMStateStore (  )  . removeContainer ( cid )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  \" Unable   to   remove   container    \"     +    cid )     +     \"    in   store \"  )  ,    e )  ;", "}", "} else    {", "break ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["removeVeryOldStoppedContainersFromCache"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "statusUpdaterRunnable    =    new   Runnable (  )     {", "@ Override", "@ SuppressWarnings (  \" unchecked \"  )", "public   void   run (  )     {", "int   lastHeartBeatID    =     0  ;", "while    (  !  ( isStopped )  )     {", "try    {", "NodeHeartbeatResponse   response    =    null ;", "NodeStatus   nodeStatus    =    getNodeStatus ( lastHeartBeatID )  ;", "NodeHeartbeatRequest   request    =    NodeHeartbeatRequest . newInstance ( nodeStatus ,     . this . context . getContainerTokenSecretManager (  )  . getCurrentKey (  )  ,     . this . context . getNMTokenSecretManager (  )  . getCurrentKey (  )  )  ;", "response    =    resourceTracker . nodeHeartbeat ( request )  ;", "nextHeartBeatInterval    =    response . getNextHeartBeatInterval (  )  ;", "updateMasterKeys ( response )  ;", "if    (  ( response . getNodeAction (  )  )     =  =     ( NodeAction . SHUTDOWN )  )     {", ". LOG . warn (  (  \" Recieved   SHUTDOWN   signal   from   Resourcemanager   as   part   of   heartbeat ,  \"     +     \"    hence   shutting   down .  \"  )  )  ;", ". LOG . warn (  (  \" Message   from   ResourceManager :     \"     +     ( response . getDiagnosticsMessage (  )  )  )  )  ;", "context . setDecommissioned ( true )  ;", "dispatcher . getEventHandler (  )  . handle ( new   NodeManagerEvent ( NodeManagerEventType . SHUTDOWN )  )  ;", "break ;", "}", "if    (  ( response . getNodeAction (  )  )     =  =     ( NodeAction . RESYNC )  )     {", ". LOG . warn (  (  \" Node   is   out   of   sync   with   ResourceManager ,  \"     +     \"    hence   resyncing .  \"  )  )  ;", ". LOG . warn (  (  \" Message   from   ResourceManager :     \"     +     ( response . getDiagnosticsMessage (  )  )  )  )  ;", ". this . rmIdentifier    =    ResourceManagerConstants . RM _ INVALID _ IDENTIFIER ;", "dispatcher . getEventHandler (  )  . handle ( new   NodeManagerEvent ( NodeManagerEventType . RESYNC )  )  ;", "break ;", "}", "removeCompletedContainersFromContext (  )  ;", "lastHeartBeatID    =    response . getResponseId (  )  ;", "List < ContainerId >    containersToCleanup    =    response . getContainersToCleanup (  )  ;", "if    (  !  ( containersToCleanup . isEmpty (  )  )  )     {", "dispatcher . getEventHandler (  )  . handle ( new   CMgrCompletedContainersEvent ( containersToCleanup ,    CMgrCompletedContainersEvent . Reason . BY _ RESOURCEMANAGER )  )  ;", "}", "List < ApplicationId >    appsToCleanup    =    response . getApplicationsToCleanup (  )  ;", "trackAppsForKeepAlive ( appsToCleanup )  ;", "if    (  !  ( appsToCleanup . isEmpty (  )  )  )     {", "dispatcher . getEventHandler (  )  . handle ( new   CMgrCompletedAppsEvent ( appsToCleanup ,    CMgrCompletedAppsEvent . Reason . BY _ RESOURCEMANAGER )  )  ;", "}", "}    catch    ( ConnectException   e )     {", "dispatcher . getEventHandler (  )  . handle ( new   NodeManagerEvent ( NodeManagerEventType . SHUTDOWN )  )  ;", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( Throwable   e )     {", ". LOG . error (  \" Caught   exception   in   status - updater \"  ,    e )  ;", "}    finally    {", "synchronized ( heartbeatMonitor )     {", "nextHeartBeatInterval    =     (  ( nextHeartBeatInterval )     <  =     0  )     ?    YarnConfiguration . DEFAULT _ RM _ NM _ HEARTBEAT _ INTERVAL _ MS    :    nextHeartBeatInterval ;", "try    {", "heartbeatMonitor . wait ( nextHeartBeatInterval )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "}", "}", "}", "private   void   updateMasterKeys ( NodeHeartbeatResponse   response )     {", "MasterKey   updatedMasterKey    =    response . getContainerTokenMasterKey (  )  ;", "if    ( updatedMasterKey    !  =    null )     {", "context . getContainerTokenSecretManager (  )  . setMasterKey ( updatedMasterKey )  ;", "}", "updatedMasterKey    =    response . getNMTokenMasterKey (  )  ;", "if    ( updatedMasterKey    !  =    null )     {", "context . getNMTokenSecretManager (  )  . setMasterKey ( updatedMasterKey )  ;", "}", "}", "}  ;", "statusUpdater    =    new   Thread ( statusUpdaterRunnable ,     \" Node   Status   Updater \"  )  ;", "statusUpdater . start (  )  ;", "}", "METHOD_END"], "methodName": ["startStatusUpdater"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . resourceTracker )     !  =    null )     {", "RPC . stopProxy ( this . resourceTracker )  ;", "}", "}", "METHOD_END"], "methodName": ["stopRMProxy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "long   nextTime    =     ( System . currentTimeMillis (  )  )     +     (  ( long )     (  (  0  .  7     *     ( tokenRemovalDelayMs )  )     +     (  (  (  0  .  2     *     ( tokenRemovalDelayMs )  )     *     ( keepAliveDelayRandom . nextInt (  1  0  0  )  )  )     /     1  0  0  )  )  )  ;", "appTokenKeepAliveMap . put ( appId ,    nextTime )  ;", "}", "METHOD_END"], "methodName": ["trackAppForKeepAlive"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( tokenKeepAliveEnabled )     &  &     ( appIds    !  =    null )  )     &  &     (  ( appIds . size (  )  )     >     0  )  )     {", "for    ( ApplicationId   appId    :    appIds )     {", "trackAppForKeepAlive ( appId )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["trackAppsForKeepAlive"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "String [  ]    command    =     . getRunCommand (  \" echo \"  ,     \" group 1  \"  ,    conf )  ;", "assertTrue (  \" first   command   should   be   the   run   command   for   the   platform \"  ,     (  ( command [  0  ]  . equals ( WINUTILS )  )     |  |     ( command [  0  ]  . equals (  \" bash \"  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRunCommandNoPriority"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY ,     2  )  ;", "String [  ]    command    =     . getRunCommand (  \" echo \"  ,     \" group 1  \"  ,    conf )  ;", "if    ( Shell . WINDOWS )     {", "assertEquals (  \" first   command   should   be   the   run   command   for   the   platform \"  ,    WINUTILS ,    command [  0  ]  )  ;", "} else    {", "assertEquals (  \" first   command   should   be   nice \"  ,     \" nice \"  ,    command [  0  ]  )  ;", "assertEquals (  \" second   command   should   be    - n \"  ,     \"  - n \"  ,    command [  1  ]  )  ;", "assertEquals (  \" third   command   should   be   the   priority \"  ,    Integer . toString (  2  )  ,    command [  2  ]  )  ;", "}", "conf . setInt ( NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY ,     (  -  5  )  )  ;", "command    =     . getRunCommand (  \" echo \"  ,     \" group 1  \"  ,    conf )  ;", "if    ( Shell . WINDOWS )     {", "assertEquals (  \" first   command   should   be   the   run   command   for   the   platform \"  ,    WINUTILS ,    command [  0  ]  )  ;", "} else    {", "assertEquals (  \" first   command   should   be   nice \"  ,     \" nice \"  ,    command [  0  ]  )  ;", "assertEquals (  \" second   command   should   be    - n \"  ,     \"  - n \"  ,    command [  1  ]  )  ;", "assertEquals (  \" third   command   should   be   the   priority \"  ,    Integer . toString (  (  -  5  )  )  ,    command [  2  ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRunCommandwithPriority"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return    ( System . getProperty ( NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH )  )     !  =    null ;", "}", "METHOD_END"], "methodName": ["shouldRunTest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE"}, {"methodBody": ["METHOD_START", "{", "FileContext   lfs    =    FileContext . getLocalFSFileContext (  )  ;", "dst    =    lfs . makeQualified ( dst )  ;", "lfs . mkdir ( dst . getParent (  )  ,    null ,    true )  ;", "byte [  ]    bytes    =    new   byte [ len ]  ;", "FSDataOutputStream   out    =    null ;", "try    {", "out    =    lfs . create ( dst ,    EnumSet . of ( CREATE ,    OVERWRITE )  )  ;", "r . nextBytes ( bytes )  ;", "out . write ( bytes )  ;", "}    finally    {", "if    ( out    !  =    null )", "out . close (  )  ;", "}", "return   bytes ;", "}", "METHOD_END"], "methodName": ["createTmpFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "FileContext   lfs    =    FileContext . getLocalFSFileContext (  )  ;", "try    {", "lfs . delete (  . BASE _ TMP _ PATH ,    true )  ;", "}    catch    ( FileNotFoundException   e )     {", "}", "}", "METHOD_END"], "methodName": ["deleteTmpFiles"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "Path   localDir    =    new   Path ( TestDefaultContainerExecutor . BASE _ TMP _ PATH ,     \" localDir \"  )  ;", "List < String >    localDirs    =    new   ArrayList < String >  (  )  ;", "localDirs . add ( localDir . toString (  )  )  ;", "List < String >    logDirs    =    new   ArrayList < String >  (  )  ;", "Path   logDir    =    new   Path ( TestDefaultContainerExecutor . BASE _ TMP _ PATH ,     \" logDir \"  )  ;", "logDirs . add ( logDir . toString (  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,    localDir . toString (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    logDir . toString (  )  )  ;", "FileContext   lfs    =    FileContext . getLocalFSFileContext ( conf )  ;", "DefaultContainerExecutor   mockExec    =    spy ( new   DefaultContainerExecutor ( lfs )  )  ;", "mockExec . setConf ( conf )  ;", "doAnswer ( new   Answer (  )     {", "@ Override", "public   Object   answer ( InvocationOnMock   invocationOnMock )    throws   Throwable    {", "String   diagnostics    =     (  ( String )     ( invocationOnMock . getArguments (  )  [  0  ]  )  )  ;", "assertTrue (  (  \" Invalid   Diagnostics   message :     \"     +    diagnostics )  ,    diagnostics . contains (  \" No   such   file   or   directory \"  )  )  ;", "return   null ;", "}", "}  )  . when ( mockExec )  . logOutput ( any ( String . class )  )  ;", "String   appSubmitter    =     \" nobody \"  ;", "String   appId    =     \" APP _ ID \"  ;", "String   containerId    =     \" CONTAINER _ ID \"  ;", "Container   container    =    mock ( Container . class )  ;", "ContainerId   cId    =    mock ( ContainerId . class )  ;", "ContainerLaunchContext   context    =    mock ( ContainerLaunchContext . class )  ;", "HashMap < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( cId )  ;", "when ( container . getLaunchContext (  )  )  . thenReturn ( context )  ;", "try    {", "doAnswer ( new   Answer (  )     {", "@ Override", "public   Object   answer ( InvocationOnMock   invocationOnMock )    throws   Throwable    {", "ContainerDiagnosticsUpdateEvent   event    =     (  ( ContainerDiagnosticsUpdateEvent )     ( invocationOnMock . getArguments (  )  [  0  ]  )  )  ;", "assertTrue (  (  \" Invalid   Diagnostics   message :     \"     +     ( event . getDiagnosticsUpdate (  )  )  )  ,    event . getDiagnosticsUpdate (  )  . contains (  \" No   such   file   or   directory \"  )  )  ;", "return   null ;", "}", "}  )  . when ( container )  . handle ( any ( ContainerDiagnosticsUpdateEvent . class )  )  ;", "when ( cId . toString (  )  )  . thenReturn ( containerId )  ;", "when ( cId . getApplicationAttemptId (  )  )  . thenReturn ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     0  )  )  ;", "when ( context . getEnvironment (  )  )  . thenReturn ( env )  ;", "mockExec . createUserLocalDirs ( localDirs ,    appSubmitter )  ;", "mockExec . createUserCacheDirs ( localDirs ,    appSubmitter )  ;", "mockExec . createAppDirs ( localDirs ,    appSubmitter ,    appId )  ;", "mockExec . createAppLogDirs ( appId ,    logDirs )  ;", "Path   scriptPath    =    new   Path (  \" file :  /  /  / bin / echo \"  )  ;", "Path   tokensPath    =    new   Path (  \" file :  /  /  / dev / null \"  )  ;", "Path   workDir    =    localDir ;", "Path   pidFile    =    new   Path ( workDir ,     \" pid . txt \"  )  ;", "mockExec . init (  )  ;", "mockExec . activateContainer ( cId ,    pidFile )  ;", "int   ret    =    mockExec . launchContainer ( container ,    scriptPath ,    tokensPath ,    appSubmitter ,    appId ,    workDir ,    localDirs ,    localDirs )  ;", "Assert . assertNotSame (  0  ,    ret )  ;", "}    finally    {", "mockExec . deleteAsUser ( appSubmitter ,    localDir )  ;", "mockExec . deleteAsUser ( appSubmitter ,    logDir )  ;", "}", "}", "METHOD_END"], "methodName": ["testContainerLaunchError"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "TestDefaultContainerExecutor . deleteTmpFiles (  )  ;", "final   String   user    =     \" somebody \"  ;", "final   String   appId    =     \" app _  1  2  3  4  5  _  1  2  3  \"  ;", "final   FsPermission   userCachePerm    =    new   FsPermission ( DefaultContainerExecutor . USER _ PERM )  ;", "final   FsPermission   appCachePerm    =    new   FsPermission ( DefaultContainerExecutor . APPCACHE _ PERM )  ;", "final   FsPermission   fileCachePerm    =    new   FsPermission ( DefaultContainerExecutor . FILECACHE _ PERM )  ;", "final   FsPermission   appDirPerm    =    new   FsPermission ( DefaultContainerExecutor . APPDIR _ PERM )  ;", "final   FsPermission   logDirPerm    =    new   FsPermission ( DefaultContainerExecutor . LOGDIR _ PERM )  ;", "List < String >    localDirs    =    new   ArrayList < String >  (  )  ;", "localDirs . add ( new   Path ( TestDefaultContainerExecutor . BASE _ TMP _ PATH ,     \" localDirA \"  )  . toString (  )  )  ;", "localDirs . add ( new   Path ( TestDefaultContainerExecutor . BASE _ TMP _ PATH ,     \" localDirB \"  )  . toString (  )  )  ;", "List < String >    logDirs    =    new   ArrayList < String >  (  )  ;", "logDirs . add ( new   Path ( TestDefaultContainerExecutor . BASE _ TMP _ PATH ,     \" logDirA \"  )  . toString (  )  )  ;", "logDirs . add ( new   Path ( TestDefaultContainerExecutor . BASE _ TMP _ PATH ,     \" logDirB \"  )  . toString (  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "FileContext   lfs    =    FileContext . getLocalFSFileContext ( conf )  ;", "DefaultContainerExecutor   executor    =    new   DefaultContainerExecutor ( lfs )  ;", "executor . init (  )  ;", "try    {", "executor . createUserLocalDirs ( localDirs ,    user )  ;", "executor . createUserCacheDirs ( localDirs ,    user )  ;", "executor . createAppDirs ( localDirs ,    user ,    appId )  ;", "for    ( String   dir    :    localDirs )     {", "FileStatus   stats    =    lfs . getFileStatus ( new   Path ( new   Path ( dir ,    ContainerLocalizer . USERCACHE )  ,    user )  )  ;", "Assert . assertEquals ( userCachePerm ,    stats . getPermission (  )  )  ;", "}", "for    ( String   dir    :    localDirs )     {", "Path   userCachePath    =    new   Path ( new   Path ( dir ,    ContainerLocalizer . USERCACHE )  ,    user )  ;", "Path   appCachePath    =    new   Path ( userCachePath ,    ContainerLocalizer . APPCACHE )  ;", "FileStatus   stats    =    lfs . getFileStatus ( appCachePath )  ;", "Assert . assertEquals ( appCachePerm ,    stats . getPermission (  )  )  ;", "stats    =    lfs . getFileStatus ( new   Path ( userCachePath ,    ContainerLocalizer . FILECACHE )  )  ;", "Assert . assertEquals ( fileCachePerm ,    stats . getPermission (  )  )  ;", "stats    =    lfs . getFileStatus ( new   Path ( appCachePath ,    appId )  )  ;", "Assert . assertEquals ( appDirPerm ,    stats . getPermission (  )  )  ;", "}", "executor . createAppLogDirs ( appId ,    logDirs )  ;", "for    ( String   dir    :    logDirs )     {", "FileStatus   stats    =    lfs . getFileStatus ( new   Path ( dir ,    appId )  )  ;", "Assert . assertEquals ( logDirPerm ,    stats . getPermission (  )  )  ;", "}", "}    finally    {", "TestDefaultContainerExecutor . deleteTmpFiles (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDirPermissions"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDefaultContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "ArrayList < Path >    ret    =    new   ArrayList < Path >  (  )  ;", "for    ( int   i    =     0  ;    i    <    numpaths ;     +  + i )     {", "Path   p    =    root ;", "long   name    =    extLong (  )  ;", "do    {", "p    =    new   Path ( p ,     (  \"  \"     +    name )  )  ;", "name    =    extLong (  )  ;", "}    while    (  0     =  =     ( name    %     2  )     )  ;", "ret . add ( p )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["buildDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "for    ( Path   dir    :    dirs )     {", ". lfs . mkdir ( new   Path ( base ,    dir )  ,    null ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["createDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   FileContext . getLocalFSFileContext (  )  ;", "}    catch    ( UnsupportedFileSystemExcep   e )     {", "throw   new   RuntimeExcep ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getLfs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "TestDeletionService . lfs . delete ( TestDeletionService . base ,    true )  ;", "}", "METHOD_END"], "methodName": ["removeBase"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "List < Path >    dirs    =    buildDirs ( r ,     . base ,     2  0  )  ;", "createDirs ( new   Path (  \"  .  \"  )  ,    dirs )  ;", ". FakeDefaultContainerExecutor   exec    =    new    . FakeDefaultContainerExecutor (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "exec . setConf ( conf )  ;", "DeletionService   del    =    new   DeletionService ( exec )  ;", "del . init ( conf )  ;", "del . start (  )  ;", "try    {", "for    ( Path   p    :    dirs )     {", "del . delete (  (  (  ( Long . parseLong ( p . getName (  )  )  )     %     2  )     =  =     0     ?    null    :     \" dingo \"  )  ,    p ,    null )  ;", "}", "int   msecToWait    =     2  0     *     1  0  0  0  ;", "for    ( Path   p    :    dirs )     {", "while    (  ( msecToWait    >     0  )     &  &     (  . lfs . util (  )  . exists ( p )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "msecToWait    -  =     1  0  0  ;", "}", "assertFalse (  . lfs . util (  )  . exists ( p )  )  ;", "}", "}    finally    {", "del . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAbsDelete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "TestDeletionService . FakeDefaultContainerExecutor   exec    =    new   TestDeletionService . FakeDefaultContainerExecutor (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "exec . setConf ( conf )  ;", "DeletionService   del    =    new   DeletionService ( exec )  ;", "del . init ( conf )  ;", "del . start (  )  ;", "try    {", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "List < Path >    dirs    =    buildDirs ( r ,    TestDeletionService . base ,     2  )  ;", "createDirs ( new   Path (  \"  .  \"  )  ,    dirs )  ;", "List < Path >    subDirs    =    buildDirs ( r ,    dirs . get (  0  )  ,     2  )  ;", "DeletionService . FileDeletionTask   dependentDeletionTask    =    del . createFileDeletionTask ( null ,    dirs . get (  0  )  ,    new   Path [  ]  {        }  )  ;", "List < DeletionService . FileDeletionTask >    deletionTasks    =    new   ArrayList < DeletionService . FileDeletionTask >  (  )  ;", "for    ( Path   subDir    :    subDirs )     {", "DeletionService . FileDeletionTask   deletionTask    =    del . createFileDeletionTask ( null ,    null ,    new   Path [  ]  {    subDir    }  )  ;", "deletionTask . addFileDeletionTaskDependency ( dependentDeletionTask )  ;", "deletionTasks . add ( deletionTask )  ;", "}", "for    ( DeletionService . FileDeletionTask   task    :    deletionTasks )     {", "del . scheduleFileDeletionTask ( task )  ;", "}", "int   msecToWait    =     2  0     *     1  0  0  0  ;", "while    (  ( msecToWait    >     0  )     &  &     ( TestDeletionService . lfs . util (  )  . exists ( dirs . get (  0  )  )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "msecToWait    -  =     1  0  0  ;", "}", "assertFalse ( TestDeletionService . lfs . util (  )  . exists ( dirs . get (  0  )  )  )  ;", "subDirs    =    buildDirs ( r ,    dirs . get (  1  )  ,     2  )  ;", "subDirs . add ( new   Path ( dirs . get (  1  )  ,     \" absentFile \"  )  )  ;", "dependentDeletionTask    =    del . createFileDeletionTask ( null ,    dirs . get (  1  )  ,    new   Path [  ]  {        }  )  ;", "deletionTasks    =    new   ArrayList < DeletionService . FileDeletionTask >  (  )  ;", "for    ( Path   subDir    :    subDirs )     {", "DeletionService . FileDeletionTask   deletionTask    =    del . createFileDeletionTask ( null ,    null ,    new   Path [  ]  {    subDir    }  )  ;", "deletionTask . addFileDeletionTaskDependency ( dependentDeletionTask )  ;", "deletionTasks . add ( deletionTask )  ;", "}", "deletionTasks . get (  2  )  . setSuccess ( false )  ;", "for    ( DeletionService . FileDeletionTask   task    :    deletionTasks )     {", "del . scheduleFileDeletionTask ( task )  ;", "}", "msecToWait    =     2  0     *     1  0  0  0  ;", "while    (  ( msecToWait    >     0  )     &  &     (  ( TestDeletionService . lfs . util (  )  . exists ( subDirs . get (  0  )  )  )     |  |     ( TestDeletionService . lfs . util (  )  . exists ( subDirs . get (  1  )  )  )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "msecToWait    -  =     1  0  0  ;", "}", "assertTrue ( TestDeletionService . lfs . util (  )  . exists ( dirs . get (  1  )  )  )  ;", "}    finally    {", "del . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFileDeletionTaskDependency"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "List < Path >    dirs    =    buildDirs ( r ,     . base ,     2  0  )  ;", "createDirs ( new   Path (  \"  .  \"  )  ,    dirs )  ;", ". FakeDefaultContainerExecutor   exec    =    new    . FakeDefaultContainerExecutor (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( DEBUG _ NM _ DELETE _ DELAY _ SEC ,     (  -  1  )  )  ;", "exec . setConf ( conf )  ;", "DeletionService   del    =    new   DeletionService ( exec )  ;", "try    {", "del . init ( conf )  ;", "del . start (  )  ;", "for    ( Path   p    :    dirs )     {", "del . delete (  (  (  ( Long . parseLong ( p . getName (  )  )  )     %     2  )     =  =     0     ?    null    :     \" dingo \"  )  ,    p ,    null )  ;", "}", "int   msecToWait    =     2  0     *     1  0  0  0  ;", "for    ( Path   p    :    dirs )     {", "while    (  ( msecToWait    >     0  )     &  &     (  . lfs . util (  )  . exists ( p )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "msecToWait    -  =     1  0  0  ;", "}", "assertTrue (  . lfs . util (  )  . exists ( p )  )  ;", "}", "}    finally    {", "del . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNoDelete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "List < Path >    baseDirs    =    buildDirs ( r ,     . base ,     4  )  ;", "createDirs ( new   Path (  \"  .  \"  )  ,    baseDirs )  ;", "List < Path >    content    =    buildDirs ( r ,    new   Path (  \"  .  \"  )  ,     1  0  )  ;", "for    ( Path   b    :    baseDirs )     {", "createDirs ( b ,    content )  ;", "}", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "conf . setInt ( DEBUG _ NM _ DELETE _ DELAY _ SEC ,     1  )  ;", "NMMemoryStateStoreService   stateStore    =    new   NMMemoryStateStoreService (  )  ;", "stateStore . init ( conf )  ;", "stateStore . start (  )  ;", "DeletionService   del    =    new   DeletionService ( new    . FakeDefaultContainerExecutor (  )  ,    stateStore )  ;", "try    {", "del . init ( conf )  ;", "del . start (  )  ;", "for    ( Path   p    :    content )     {", "assertTrue (  . lfs . util (  )  . exists ( new   Path ( baseDirs . get (  0  )  ,    p )  )  )  ;", "del . delete (  (  (  ( Long . parseLong ( p . getName (  )  )  )     %     2  )     =  =     0     ?    null    :     \" dingo \"  )  ,    p ,    baseDirs . toArray ( new   Path [  4  ]  )  )  ;", "}", "del . stop (  )  ;", "del    =    new   DeletionService ( new    . FakeDefaultContainerExecutor (  )  ,    stateStore )  ;", "del . init ( conf )  ;", "del . start (  )  ;", "int   msecToWait    =     1  0     *     1  0  0  0  ;", "for    ( Path   p    :    baseDirs )     {", "for    ( Path   q    :    content )     {", "Path   fp    =    new   Path ( p ,    q )  ;", "while    (  ( msecToWait    >     0  )     &  &     (  . lfs . util (  )  . exists ( fp )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "msecToWait    -  =     1  0  0  ;", "}", "assertFalse (  . lfs . util (  )  . exists ( fp )  )  ;", "}", "}", "}    finally    {", "del . close (  )  ;", "stateStore . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRecovery"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "List < Path >    baseDirs    =    buildDirs ( r ,     . base ,     4  )  ;", "createDirs ( new   Path (  \"  .  \"  )  ,    baseDirs )  ;", "List < Path >    content    =    buildDirs ( r ,    new   Path (  \"  .  \"  )  ,     1  0  )  ;", "for    ( Path   b    :    baseDirs )     {", "createDirs ( b ,    content )  ;", "}", "DeletionService   del    =    new   DeletionService ( new    . FakeDefaultContainerExecutor (  )  )  ;", "try    {", "del . init ( new   Configuration (  )  )  ;", "del . start (  )  ;", "for    ( Path   p    :    content )     {", "assertTrue (  . lfs . util (  )  . exists ( new   Path ( baseDirs . get (  0  )  ,    p )  )  )  ;", "del . delete (  (  (  ( Long . parseLong ( p . getName (  )  )  )     %     2  )     =  =     0     ?    null    :     \" dingo \"  )  ,    p ,    baseDirs . toArray ( new   Path [  4  ]  )  )  ;", "}", "int   msecToWait    =     2  0     *     1  0  0  0  ;", "for    ( Path   p    :    baseDirs )     {", "for    ( Path   q    :    content )     {", "Path   fp    =    new   Path ( p ,    q )  ;", "while    (  ( msecToWait    >     0  )     &  &     (  . lfs . util (  )  . exists ( fp )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "msecToWait    -  =     1  0  0  ;", "}", "assertFalse (  . lfs . util (  )  . exists ( fp )  )  ;", "}", "}", "}    finally    {", "del . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRelativeDelete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "DeletionService   del    =    new   DeletionService ( Mockito . mock ( ContainerExecutor . class )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( DEBUG _ NM _ DELETE _ DELAY _ SEC ,     6  0  )  ;", "try    {", "del . init ( conf )  ;", "del . start (  )  ;", "del . delete (  \" dingo \"  ,    new   Path (  \"  / does / not / exist \"  )  )  ;", "}    finally    {", "del . stop (  )  ;", "}", "assertTrue ( del . isTerminated (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStopWithDelayedTasks"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDeletionService"}, {"methodBody": ["METHOD_START", "{", "TestDirectoryCollection . testDir . mkdirs (  )  ;", "TestDirectoryCollection . testFile . createNewFile (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestDirectoryCollection . testDir )  ;", "}", "METHOD_END"], "methodName": ["teardown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "String [  ]    dirs    =    new   String [  ]  {     . testFile . getPath (  )     }  ;", "DirectoryCollection   dc    =    new   DirectoryCollection ( dirs ,    conf . getFloat ( NM _ MAX _ PER _ DISK _ UTILIZATION _ PERCENTAGE ,    DEFAULT _ NM _ MAX _ PER _ DISK _ UTILIZATION _ PERCENTAGE )  )  ;", "List < String >    list    =    dc . getGoodDirs (  )  ;", "ListIterator < String >    li    =    list . listIterator (  )  ;", "Assert . assertTrue (  \" checkDirs   did   not   remove   test   file   from   directory   list \"  ,    dc . checkDirs (  )  )  ;", "li . next (  )  ;", "}", "METHOD_END"], "methodName": ["testConcurrentAccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "String [  ]    dirs    =    new   String [  ]  {     \" dir \"     }  ;", "float   delta    =     0  .  1 F ;", "dc    =    new    ( dirs )  ;", "Assert . assertEquals (  1  0  0  .  0 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "Assert . assertEquals (  0  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "dc    =    new    ( dirs ,     5  7  .  5 F )  ;", "Assert . assertEquals (  5  7  .  5 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "Assert . assertEquals (  0  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "dc    =    new    ( dirs ,     5  7  )  ;", "Assert . assertEquals (  1  0  0  .  0 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "Assert . assertEquals (  5  7  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "dc    =    new    ( dirs ,     5  7  .  5 F ,     6  7  )  ;", "Assert . assertEquals (  5  7  .  5 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "Assert . assertEquals (  6  7  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "dc    =    new    ( dirs ,     (  -  5  7  .  5 F )  ,     (  -  6  7  )  )  ;", "Assert . assertEquals (  0  .  0 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "Assert . assertEquals (  0  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "dc    =    new    ( dirs ,     1  5  7  .  5 F ,     (  -  6  7  )  )  ;", "Assert . assertEquals (  1  0  0  .  0 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "Assert . assertEquals (  0  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "}", "METHOD_END"], "methodName": ["testConstructors"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "FileContext   localFs    =    FileContext . getLocalFSFileContext ( conf )  ;", "String   dirA    =    new   File (  . testDir ,     \" dirA \"  )  . getPath (  )  ;", "String   dirB    =    new   File ( dirA ,     \" dirB \"  )  . getPath (  )  ;", "String   dirC    =    new   File (  . testDir ,     \" dirC \"  )  . getPath (  )  ;", "Path   pathC    =    new   Path ( dirC )  ;", "FsPermission   permDirC    =    new   FsPermission (  (  ( short )     (  4  5  6  )  )  )  ;", "localFs . mkdir ( pathC ,    null ,    true )  ;", "localFs . setPermission ( pathC ,    permDirC )  ;", "String [  ]    dirs    =    new   String [  ]  {    dirA ,    dirB ,    dirC    }  ;", "DirectoryCollection   dc    =    new   DirectoryCollection ( dirs ,    conf . getFloat ( NM _ MAX _ PER _ DISK _ UTILIZATION _ PERCENTAGE ,    DEFAULT _ NM _ MAX _ PER _ DISK _ UTILIZATION _ PERCENTAGE )  )  ;", "FsPermission   defaultPerm    =    FsPermission . getDefault (  )  . applyUMask ( new   FsPermission (  (  ( short )     ( FsPermission . DEFAULT _ UMASK )  )  )  )  ;", "boolean   createResult    =    dc . createNonExistentDirs ( localFs ,    defaultPerm )  ;", "Assert . assertTrue ( createResult )  ;", "FileStatus   status    =    localFs . getFileStatus ( new   Path ( dirA )  )  ;", "Assert . assertEquals (  \" local   dir   parent   not   created   with   proper   permissions \"  ,    defaultPerm ,    status . getPermission (  )  )  ;", "status    =    localFs . getFileStatus ( new   Path ( dirB )  )  ;", "Assert . assertEquals (  \" local   dir   not   created   with   proper   permissions \"  ,    defaultPerm ,    status . getPermission (  )  )  ;", "status    =    localFs . getFileStatus ( pathC )  ;", "Assert . assertEquals (  \" existing   local   directory   permissions   modified \"  ,    permDirC ,    status . getPermission (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateDirectories"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "String [  ]    dirs    =    new   String [  ]  {     \" dir \"     }  ;", "dc    =    new    ( dirs ,     0  .  0 F ,     1  0  0  )  ;", "float   testValue    =     5  7  .  5 F ;", "float   delta    =     0  .  1 F ;", "dc . setDiskUtilizationPercentageCutoff ( testValue )  ;", "Assert . assertEquals ( testValue ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "testValue    =     -  5  7  .  5 F ;", "dc . setDiskUtilizationPercentageCutoff ( testValue )  ;", "Assert . assertEquals (  0  .  0 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "testValue    =     1  5  7  .  5 F ;", "dc . setDiskUtilizationPercentageCutoff ( testValue )  ;", "Assert . assertEquals (  1  0  0  .  0 F ,    dc . getDiskUtilizationPercentageCutoff (  )  ,    delta )  ;", "long   spaceValue    =     5  7  ;", "dc . setDiskUtilizationSpaceCutoff ( spaceValue )  ;", "Assert . assertEquals ( spaceValue ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "spaceValue    =     -  5  7  ;", "dc . setDiskUtilizationSpaceCutoff ( spaceValue )  ;", "Assert . assertEquals (  0  ,    dc . getDiskUtilizationSpaceCutoff (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDiskLimitsCutoffSetters"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "String   dirA    =    new   File ( TestDirectoryCollection . testDir ,     \" dirA \"  )  . getPath (  )  ;", "String [  ]    dirs    =    new   String [  ]  {    dirA    }  ;", "DirectoryCollection   dc    =    new   DirectoryCollection ( dirs ,     0  .  0 F )  ;", "dc . checkDirs (  )  ;", "Assert . assertEquals (  0  ,    dc . getGoodDirs (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    dc . getFailedDirs (  )  . size (  )  )  ;", "dc    =    new   DirectoryCollection ( dirs ,     1  0  0  .  0 F )  ;", "dc . checkDirs (  )  ;", "Assert . assertEquals (  1  ,    dc . getGoodDirs (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    dc . getFailedDirs (  )  . size (  )  )  ;", "dc    =    new   DirectoryCollection ( dirs ,     (  ( TestDirectoryCollection . testDir . getTotalSpace (  )  )     /     (  1  0  2  4     *     1  0  2  4  )  )  )  ;", "dc . checkDirs (  )  ;", "Assert . assertEquals (  0  ,    dc . getGoodDirs (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    dc . getFailedDirs (  )  . size (  )  )  ;", "dc    =    new   DirectoryCollection ( dirs ,     1  0  0  .  0 F ,     0  )  ;", "dc . checkDirs (  )  ;", "Assert . assertEquals (  1  ,    dc . getGoodDirs (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    dc . getFailedDirs (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDiskSpaceUtilizationLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestDirectoryCollection"}, {"methodBody": ["METHOD_START", "{", "FileContext   localFS    =    FileContext . getLocalFSFileContext (  )  ;", "localFS . delete ( new   Path ( TestEventFlow . localDir . getAbsolutePath (  )  )  ,    true )  ;", "localFS . delete ( new   Path ( TestEventFlow . localLogDir . getAbsolutePath (  )  )  ,    true )  ;", "localFS . delete ( new   Path ( TestEventFlow . remoteLogDir . getAbsolutePath (  )  )  ,    true )  ;", "TestEventFlow . localDir . mkdir (  )  ;", "TestEventFlow . localLogDir . mkdir (  )  ;", "TestEventFlow . remoteLogDir . mkdir (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "Context   context    =    new   NodeManager . NMContext ( new   NMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInNM (  )  ,    null ,    null ,    new   NMNullStateStoreService (  )  )     {", "@ Override", "public   int   getHttpPort (  )     {", "return    1  2  3  4  ;", "}", "}  ;", "conf . set ( NM _ LOCAL _ DIRS ,    TestEventFlow . localDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    TestEventFlow . localLogDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    TestEventFlow . remoteLogDir . getAbsolutePath (  )  )  ;", "ContainerExecutor   exec    =    new   DefaultContainerExecutor (  )  ;", "exec . setConf ( conf )  ;", "DeletionService   del    =    new   DeletionService ( exec )  ;", "Dispatcher   dispatcher    =    new   AsyncDispatcher (  )  ;", "NodeHealthCheckerService   healthChecker    =    new   NodeHealthCheckerService (  )  ;", "healthChecker . init ( conf )  ;", "LocalDirsHandlerService   dirsHandler    =    healthChecker . getDiskHandler (  )  ;", "NodeManagerMetrics   metrics    =    NodeManagerMetrics . create (  )  ;", "NodeStatusUpdater   nodeStatusUpdater    =    new   NodeStatusUpdaterImpl ( context ,    dispatcher ,    healthChecker ,    metrics )     {", "@ Override", "protected   ResourceTracker   getRMClient (  )     {", "return   new   LocalRMInterface (  )  ;", "}", "@ Override", "protected   void   stopRMProxy (  )     {", "return ;", "}", "@ Override", "protected   void   startStatusUpdater (  )     {", "return ;", "}", "@ Override", "public   long   getRMIdentifier (  )     {", "return   TestEventFlow . SIMULATED _ RM _ IDENTIFIER ;", "}", "}  ;", "DummyContainerManager   containerManager    =    new   DummyContainerManager ( context ,    exec ,    del ,    nodeStatusUpdater ,    metrics ,    new   security . ApplicationACLsManager ( conf )  ,    dirsHandler )  ;", "nodeStatusUpdater . init ( conf )  ;", "(  ( NodeManager . NMContext )     ( context )  )  . setContainerManager ( containerManager )  ;", "nodeStatusUpdater . start (  )  ;", "containerManager . init ( conf )  ;", "containerManager . start (  )  ;", "ContainerLaunchContext   launchContext    =    TestEventFlow . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "ApplicationId   applicationId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     0  )  ;", "ContainerId   cID    =    ContainerId . newInstance ( applicationAttemptId ,     0  )  ;", "String   user    =     \" testing \"  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( launchContext ,    TestContainerManager . createContainerToken ( cID ,    TestEventFlow . SIMULATED _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cID ,    RUNNING )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cID )  ;", "StopContainersRequest   stopRequest    =    StopContainersRequest . newInstance ( containerIds )  ;", "containerManager . stopContainers ( stopRequest )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cID ,    COMPLETE )  ;", "containerManager . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSuccessfulContainerLaunch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestEventFlow"}, {"methodBody": ["METHOD_START", "{", "ContainerId   cId    =    mock ( ContainerId . class )  ;", "String   id    =     \" CONTAINER _  \"     +     ( getNextId (  )  )  ;", "when ( cId . toString (  )  )  . thenReturn ( id )  ;", "return   cId ;", "}", "METHOD_END"], "methodName": ["getNextContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "id    +  =     1  ;", "retu   id ;", "}", "METHOD_END"], "methodName": ["getNextId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "return   runAndBlock ( getNextContainerId (  )  ,    cmd )  ;", "}", "METHOD_END"], "methodName": ["runAndBlock"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "String   appId    =     \" APP _  \"     +     ( getNextId (  )  )  ;", "Container   container    =    mock ( Container . class )  ;", "ContainerLaunchContext   context    =    mock ( ContainerLaunchContext . class )  ;", "HashMap < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( cId )  ;", "when ( container . getLaunchContext (  )  )  . thenReturn ( context )  ;", "when ( context . getEnvironment (  )  )  . thenReturn ( env )  ;", "String   script    =    writeScriptFile ( cmd )  ;", "Path   scriptPath    =    new   Path ( script )  ;", "Path   tokensPath    =    new   Path (  \"  / dev / null \"  )  ;", "Path   workDir    =    new   Path (  . workSpace . getAbsolutePath (  )  )  ;", "Path   pidFile    =    new   Path ( workDir ,     \" pid . txt \"  )  ;", "exec . activateContainer ( cId ,    pidFile )  ;", "return   exec . launchContainer ( container ,    scriptPath ,    tokensPath ,    appSubmitter ,    appId ,    workDir ,    dirsHandler . getLocalDirs (  )  ,    dirsHandler . getLogDirs (  )  )  ;", "}", "METHOD_END"], "methodName": ["runAndBlock"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "FileContext   files    =    FileContext . getLocalFSFileContext (  )  ;", "Path   workSpacePath    =    new   Path (  . workSpace . getAbsolutePath (  )  )  ;", "files . mkdir ( workSpacePath ,    null ,    true )  ;", "FileUtil . chmod (  . workSpace . getAbsolutePath (  )  ,     \"  7  7  7  \"  )  ;", "File   localDir    =    new   File (  . workSpace . getAbsoluteFile (  )  ,     \" localDir \"  )  ;", "files . mkdir ( new   Path ( localDir . getAbsolutePath (  )  )  ,    new   FsPermission (  \"  7  7  7  \"  )  ,    false )  ;", "File   logDir    =    new   File (  . workSpace . getAbsoluteFile (  )  ,     \" logDir \"  )  ;", "files . mkdir ( new   Path ( logDir . getAbsolutePath (  )  )  ,    new   FsPermission (  \"  7  7  7  \"  )  ,    false )  ;", "String   exec _ path    =    System . getProperty (  \" container - executor . path \"  )  ;", "if    (  ( exec _ path    !  =    null )     &  &     (  !  ( exec _ path . isEmpty (  )  )  )  )     {", "Configuration   conf    =    new   Configuration ( false )  ;", ". LOG . info (  (  (  (  \" Setting    \"     +     ( YarnConfiguration . NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH )  )     +     \"  =  \"  )     +    exec _ path )  )  ;", "conf . set ( NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH ,    exec _ path )  ;", "exec    =    new   LinuxContainerExecutor (  )  ;", "exec . setConf ( conf )  ;", "conf . set ( NM _ LOCAL _ DIRS ,    localDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    logDir . getAbsolutePath (  )  )  ;", "dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "}", "appSubmitter    =    System . getProperty (  \" application . submitter \"  )  ;", "if    (  (  ( appSubmitter )     =  =    null )     |  |     ( appSubmitter . isEmpty (  )  )  )     {", "appSubmitter    =     \" nobody \"  ;", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    (  ( exec )     =  =    null )     {", ". LOG . warn (  \" Not   running   test   because   container - executor . path   is   not   set \"  )  ;", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["shouldRun"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "FileContext . getLocalFSFileContext (  )  . delete ( new   Path ( TestLinuxContainerExecutor . workSpace . getAbsolutePath (  )  )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( shouldRun (  )  )  )     {", "return ;", "}", "final   ContainerId   sleepId    =    getNextContainerId (  )  ;", "Thread   t    =    new   Thread (  )     {", "public   void   run (  )     {", "try    {", "runAndBlock ( sleepId ,     \" sleep \"  ,     \"  1  0  0  \"  )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  \" Caught   exception   while   running   sleep \"  ,    e )  ;", "}", "}", "}  ;", "t . setDaemon ( true )  ;", "t . start (  )  ;", "assertTrue ( t . isAlive (  )  )  ;", "String   pid    =    null ;", "int   count    =     1  0  ;", "while    (  (  ( pid    =    exec . getProcessId ( sleepId )  )     =  =    null )     &  &     ( count    >     0  )  )     {", ". LOG . info (  \" Sleeping   for    2  0  0    ms   before   checking   for   pid    \"  )  ;", "Thread . sleep (  2  0  0  )  ;", "count -  -  ;", "}", "assertNotNull ( pid )  ;", ". LOG . info (  \" Going   to   killing   the   process .  \"  )  ;", "exec . signalContainer ( appSubmitter ,    pid ,    ContainerExecutor . Signal . TERM )  ;", ". LOG . info (  \" sleeping   for    1  0  0 ms   to   let   the   sleep   be   killed \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "assertFalse ( t . isAlive (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerKill"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( shouldRun (  )  )  )     {", "return ;", "}", "File   touchFile    =    new   File (  . workSpace ,     \" touch - file \"  )  ;", "int   ret    =    runAndBlock (  \" touch \"  ,    touchFile . getAbsolutePath (  )  )  ;", "assertEquals (  0  ,    ret )  ;", "FileStatus   fileStatus    =    FileContext . getLocalFSFileContext (  )  . getFileStatus ( new   Path ( touchFile . getAbsolutePath (  )  )  )  ;", "assertEquals ( appSubmitter ,    fileStatus . getOwner (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "try    {", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" simple \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "Assert . assertEquals ( DEFAULT _ NM _ NONSECURE _ MODE _ LOCAL _ USER ,    lce . getRunAsUser (  \" foo \"  )  )  ;", "conf . set ( NM _ NONSECURE _ MODE _ LOCAL _ USER _ KEY ,     \" bar \"  )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "Assert . assertEquals (  \" bar \"  ,    lce . getRunAsUser (  \" foo \"  )  )  ;", "conf . set ( NM _ NONSECURE _ MODE _ LOCAL _ USER _ KEY ,     \" bar \"  )  ;", "conf . setBoolean ( NM _ NONSECURE _ MODE _ LIMIT _ USERS ,    false )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "Assert . assertEquals (  \" foo \"  ,    lce . getRunAsUser (  \" foo \"  )  )  ;", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "Assert . assertEquals (  \" foo \"  ,    lce . getRunAsUser (  \" foo \"  )  )  ;", "}    finally    {", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" simple \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "}", "}", "METHOD_END"], "methodName": ["testLocalUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "try    {", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" simple \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "lce . verifyUsernamePattern (  \" foo \"  )  ;", "try    {", "lce . verifyUsernamePattern (  \" foo / x \"  )  ;", "Assert . fail (  )  ;", "}    catch    ( IllegalArgumentException   ex )     {", "}    catch    ( Throwable   ex )     {", "Assert . fail ( ex . toString (  )  )  ;", "}", "conf . set ( NM _ NONSECURE _ MODE _ USER _ PATTERN _ KEY ,     \" foo \"  )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "lce . verifyUsernamePattern (  \" foo \"  )  ;", "try    {", "lce . verifyUsernamePattern (  \" bar \"  )  ;", "Assert . fail (  )  ;", "}    catch    ( IllegalArgumentException   ex )     {", "}    catch    ( Throwable   ex )     {", "Assert . fail ( ex . toString (  )  )  ;", "}", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "lce    =    new    (  )  ;", "lce . setConf ( conf )  ;", "lce . verifyUsernamePattern (  \" foo \"  )  ;", "lce . verifyUsernamePattern (  \" foo / w \"  )  ;", "}    finally    {", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" simple \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonsecureUsernamePattern"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "File   f    =    File . createTempFile (  \" TestLinuxContainerExecutor \"  ,     \"  . sh \"  )  ;", "f . deleteOnExit (  )  ;", "PrintWriter   p    =    new   PrintWriter ( new   FileOutputStream ( f )  )  ;", "p . println (  \"  #  !  / bin / sh \"  )  ;", "p . print (  \" exec \"  )  ;", "for    ( String   part    :    cmd )     {", "p . print (  \"     '  \"  )  ;", "p . print ( part . replace (  \"  \\  \\  \"  ,     \"  \\  \\  \\  \\  \"  )  . replace (  \"  '  \"  ,     \"  \\  \\  \\  '  \"  )  )  ;", "p . print (  \"  '  \"  )  ;", "}", "p . println (  )  ;", "p . close (  )  ;", "return   f . getAbsolutePath (  )  ;", "}", "METHOD_END"], "methodName": ["writeScriptFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutor"}, {"methodBody": ["METHOD_START", "{", "if    ( mockParamFile . exists (  )  )     {", "mockParamFile . delete (  )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteMockParamFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "LinkedList < String >    ret    =    new   LinkedList < String >  (  )  ;", "LineNumberReader   reader    =    new   LineNumberReader ( new   FileReader ( mockParamFile )  )  ;", "String   line ;", "while    (  ( line    =    reader . readLine (  )  )     !  =    null )     {", "ret . add ( line )  ;", "}", "reader . close (  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["readMockParams"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "assumeTrue (  (  !  ( Path . WINDOWS )  )  )  ;", "File   f    =    new   File (  \"  .  / src / test / resources / mock - container - executor \"  )  ;", "if    (  !  ( FileUtil . canExecute ( f )  )  )     {", "FileUtil . setExecutable ( f ,    true )  ;", "}", "String   executorPath    =    f . getAbsolutePath (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH ,    executorPath )  ;", "mockExec    =    new    (  )  ;", "dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "mockExec . setConf ( conf )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "deleteMockParamFile (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "String   appSubmitter    =     \" nobody \"  ;", "String   cmd    =    String . valueOf (  . Commands . SIGNAL _ CONTAINER . getValue (  )  )  ;", "ContainerExecutor . Signal   signal    =    ContainerExecutor . Signal . QUIT ;", "String   sigVal    =    String . valueOf ( signal . getValue (  )  )  ;", "mockExec . signalContainer ( appSubmitter ,     \"  1  0  0  0  \"  ,    signal )  ;", "assertEquals ( Arrays . asList ( DEFAULT _ NM _ NONSECURE _ MODE _ LOCAL _ USER ,    appSubmitter ,    cmd ,     \"  1  0  0  0  \"  ,    sigVal )  ,    readMockParams (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerKill"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "String   appSubmitter    =     \" nobody \"  ;", "String   cmd    =    String . valueOf (  . Commands . LAUNCH _ CONTAINER . getValue (  )  )  ;", "String   appId    =     \" APP _ ID \"  ;", "String   containerId    =     \" CONTAINER _ ID \"  ;", "Container   container    =    mock ( Container . class )  ;", "ContainerId   cId    =    mock ( ContainerId . class )  ;", "ContainerLaunchContext   context    =    mock ( ContainerLaunchContext . class )  ;", "HashMap < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( cId )  ;", "when ( container . getLaunchContext (  )  )  . thenReturn ( context )  ;", "when ( cId . toString (  )  )  . thenReturn ( containerId )  ;", "when ( context . getEnvironment (  )  )  . thenReturn ( env )  ;", "Path   scriptPath    =    new   Path (  \" file :  /  /  / bin / echo \"  )  ;", "Path   tokensPath    =    new   Path (  \" file :  /  /  / dev / null \"  )  ;", "Path   workDir    =    new   Path (  \"  / tmp \"  )  ;", "Path   pidFile    =    new   Path ( workDir ,     \" pid . txt \"  )  ;", "mockExec . activateContainer ( cId ,    pidFile )  ;", "int   ret    =    mockExec . launchContainer ( container ,    scriptPath ,    tokensPath ,    appSubmitter ,    appId ,    workDir ,    dirsHandler . getLocalDirs (  )  ,    dirsHandler . getLogDirs (  )  )  ;", "assertEquals (  0  ,    ret )  ;", "assertEquals ( Arrays . asList ( DEFAULT _ NM _ NONSECURE _ MODE _ LOCAL _ USER ,    appSubmitter ,    cmd ,    appId ,    containerId ,    workDir . toString (  )  ,     \"  / bin / echo \"  ,     \"  / dev / null \"  ,    pidFile . toString (  )  ,    StringUtils . join (  \"  ,  \"  ,    dirsHandler . getLocalDirs (  )  )  ,    StringUtils . join (  \"  ,  \"  ,    dirsHandler . getLogDirs (  )  )  ,     \" cgroups = none \"  )  ,    readMockParams (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "File   f    =    new   File (  \"  .  / src / test / resources / mock - container - executer - with - error \"  )  ;", "if    (  !  ( FileUtil . canExecute ( f )  )  )     {", "FileUtil . setExecutable ( f ,    true )  ;", "}", "String   executorPath    =    f . getAbsolutePath (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH ,    executorPath )  ;", "conf . set ( NM _ LOCAL _ DIRS ,     \" file :  /  /  / bin / echo \"  )  ;", "conf . set ( NM _ LOG _ DIRS ,     \" file :  /  /  / dev / null \"  )  ;", "mockExec    =    spy ( new    (  )  )  ;", "doAnswer ( new   Answer (  )     {", "@ Override", "public   Object   answer ( InvocationOnMock   invocationOnMock )    throws   Throwable    {", "String   diagnostics    =     (  ( String )     ( invocationOnMock . getArguments (  )  [  0  ]  )  )  ;", "assertTrue (  (  \" Invalid   Diagnostics   message :     \"     +    diagnostics )  ,    diagnostics . contains (  \" badcommand \"  )  )  ;", "return   null ;", "}", "}  )  . when ( mockExec )  . logOutput ( any ( String . class )  )  ;", "dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "mockExec . setConf ( conf )  ;", "String   appSubmitter    =     \" nobody \"  ;", "String   cmd    =    String . valueOf (  . Commands . LAUNCH _ CONTAINER . getValue (  )  )  ;", "String   appId    =     \" APP _ ID \"  ;", "String   containerId    =     \" CONTAINER _ ID \"  ;", "Container   container    =    mock ( Container . class )  ;", "ContainerId   cId    =    mock ( ContainerId . class )  ;", "ContainerLaunchContext   context    =    mock ( ContainerLaunchContext . class )  ;", "HashMap < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( cId )  ;", "when ( container . getLaunchContext (  )  )  . thenReturn ( context )  ;", "doAnswer ( new   Answer (  )     {", "@ Override", "public   Object   answer ( InvocationOnMock   invocationOnMock )    throws   Throwable    {", "ContainerDiagnosticsUpdateEvent   event    =     (  ( ContainerDiagnosticsUpdateEvent )     ( invocationOnMock . getArguments (  )  [  0  ]  )  )  ;", "assertTrue (  (  \" Invalid   Diagnostics   message :     \"     +     ( event . getDiagnosticsUpdate (  )  )  )  ,    event . getDiagnosticsUpdate (  )  . contains (  \" badcommand \"  )  )  ;", "return   null ;", "}", "}  )  . when ( container )  . handle ( any ( ContainerDiagnosticsUpdateEvent . class )  )  ;", "when ( cId . toString (  )  )  . thenReturn ( containerId )  ;", "when ( context . getEnvironment (  )  )  . thenReturn ( env )  ;", "Path   scriptPath    =    new   Path (  \" file :  /  /  / bin / echo \"  )  ;", "Path   tokensPath    =    new   Path (  \" file :  /  /  / dev / null \"  )  ;", "Path   workDir    =    new   Path (  \"  / tmp \"  )  ;", "Path   pidFile    =    new   Path ( workDir ,     \" pid . txt \"  )  ;", "mockExec . activateContainer ( cId ,    pidFile )  ;", "int   ret    =    mockExec . launchContainer ( container ,    scriptPath ,    tokensPath ,    appSubmitter ,    appId ,    workDir ,    dirsHandler . getLocalDirs (  )  ,    dirsHandler . getLogDirs (  )  )  ;", "Assert . assertNotSame (  0  ,    ret )  ;", "assertEquals ( Arrays . asList ( DEFAULT _ NM _ NONSECURE _ MODE _ LOCAL _ USER ,    appSubmitter ,    cmd ,    appId ,    containerId ,    workDir . toString (  )  ,     \"  / bin / echo \"  ,     \"  / dev / null \"  ,    pidFile . toString (  )  ,    StringUtils . join (  \"  ,  \"  ,    dirsHandler . getLocalDirs (  )  )  ,    StringUtils . join (  \"  ,  \"  ,    dirsHandler . getLogDirs (  )  )  ,     \" cgroups = none \"  )  ,    readMockParams (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchError"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "File   f    =    new   File (  \"  .  / src / test / resources / mock - container - executor \"  )  ;", "if    (  !  ( FileUtil . canExecute ( f )  )  )     {", "FileUtil . setExecutable ( f ,    true )  ;", "}", "String   executorPath    =    f . getAbsolutePath (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LINUX _ CONTAINER _ EXECUTOR _ PATH ,    executorPath )  ;", "conf . setInt ( NM _ CONTAINER _ EXECUTOR _ SCHED _ PRIORITY ,     2  )  ;", "mockExec . setConf ( conf )  ;", "List < String >    command    =    new   ArrayList < String >  (  )  ;", "mockExec . addSchedPriorityCommand ( command )  ;", "assertEquals (  \" first   should   be   nice \"  ,     \" nice \"  ,    command . get (  0  )  )  ;", "assertEquals (  \" second   should   be    - n \"  ,     \"  - n \"  ,    command . get (  1  )  )  ;", "assertEquals (  \" third   should   be   the   priority \"  ,    Integer . toString (  2  )  ,    command . get (  2  )  )  ;", "testLaunch (  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchWithPriority"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "String   appSubmitter    =     \" nobody \"  ;", "String   cmd    =    String . valueOf (  . Commands . DELETE _ AS _ USER . getValue (  )  )  ;", "Path   dir    =    new   Path (  \"  / tmp / testdir \"  )  ;", "mockExec . deleteAsUser ( appSubmitter ,    dir )  ;", "assertEquals ( Arrays . asList ( DEFAULT _ NM _ NONSECURE _ MODE _ LOCAL _ USER ,    appSubmitter ,    cmd ,     \"  / tmp / testdir \"  )  ,    readMockParams (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteAsUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "mockExec . init (  )  ;", "assertEquals ( Arrays . asList (  \"  -  - checksetup \"  )  ,    readMockParams (  )  )  ;", "}", "METHOD_END"], "methodName": ["testInit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "List < String >    command    =    new   ArrayList < String >  (  )  ;", "mock . addSchedPriorityCommand ( command )  ;", "assertEquals (  \" addSchedPriority   should   be   empty \"  ,     0  ,    command . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLaunchCommandWithoutPriority"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   address    =    InetSocketAddress . createUnresolved (  \" localhost \"  ,     8  0  4  0  )  ;", "Path   nmPrivateCTokensPath    =    new   Path (  \" file :  /  /  / bin / nmPrivateCTokensPath \"  )  ;", "try    {", "mockExec . startLocalizer ( nmPrivateCTokensPath ,    address ,     \" test \"  ,     \" application _  0  \"  ,     \"  1  2  3  4  5  \"  ,    dirsHandler . getLocalDirs (  )  ,    dirsHandler . getLogDirs (  )  )  ;", "List < String >    result    =    readMockParams (  )  ;", "Assert . assertEquals ( result . size (  )  ,     1  7  )  ;", "Assert . assertEquals ( result . get (  0  )  ,    DEFAULT _ NM _ NONSECURE _ MODE _ LOCAL _ USER )  ;", "Assert . assertEquals ( result . get (  1  )  ,     \" test \"  )  ;", "Assert . assertEquals ( result . get (  2  )  ,     \"  0  \"  )  ;", "Assert . assertEquals ( result . get (  3  )  ,     \" application _  0  \"  )  ;", "Assert . assertEquals ( result . get (  4  )  ,     \"  / bin / nmPrivateCTokensPath \"  )  ;", "Assert . assertEquals ( result . get (  8  )  ,     \"  - classpath \"  )  ;", "Assert . assertEquals ( result . get (  1  1  )  ,     \" containermanager . localizer . ContainerLocalizer \"  )  ;", "Assert . assertEquals ( result . get (  1  2  )  ,     \" test \"  )  ;", "Assert . assertEquals ( result . get (  1  3  )  ,     \" application _  0  \"  )  ;", "Assert . assertEquals ( result . get (  1  4  )  ,     \"  1  2  3  4  5  \"  )  ;", "Assert . assertEquals ( result . get (  1  5  )  ,     \" localhost \"  )  ;", "Assert . assertEquals ( result . get (  1  6  )  ,     \"  8  0  4  0  \"  )  ;", "}    catch    ( InterruptedException   e )     {", "TestLinuxContainerExecutorWithMocks . LOG . error (  (  \" Error :  \"     +     ( e . getMessage (  )  )  )  ,    e )  ;", "Assert . fail (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStartLocalizer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks"}, {"methodBody": ["METHOD_START", "{", "TestLocalDirsHandlerService . testDir . mkdirs (  )  ;", "TestLocalDirsHandlerService . testFile . createNewFile (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestLocalDirsHandlerService . testDir )  ;", "}", "METHOD_END"], "methodName": ["teardown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "String   localDir 1     =    new   File (  (  \" file :  /  /  /  \"     +     (  . testDir )  )  ,     \" localDir 1  \"  )  . getPath (  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,    localDir 1  )  ;", "String   logDir 1     =    new   File (  (  \" file :  /  /  /  \"     +     (  . testDir )  )  ,     \" logDir 1  \"  )  . getPath (  )  ;", "conf . set ( NM _ LOG _ DIRS ,    logDir 1  )  ;", "LocalDirsHandlerService   dirSvc    =    new   LocalDirsHandlerService (  )  ;", "dirSvc . init ( conf )  ;", "Assert . assertEquals (  1  ,    dirSvc . getLocalDirs (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDirStructure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "String   localDir 1     =    new   File (  (  \" file :  /  /  /  \"     +     (  . testDir )  )  ,     \" localDir 1  \"  )  . getPath (  )  ;", "String   localDir 2     =    new   File (  (  \" hdfs :  /  /  /  \"     +     (  . testDir )  )  ,     \" localDir 2  \"  )  . getPath (  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,     (  ( localDir 1     +     \"  ,  \"  )     +    localDir 2  )  )  ;", "String   logDir 1     =    new   File (  (  \" file :  /  /  /  \"     +     (  . testDir )  )  ,     \" logDir 1  \"  )  . getPath (  )  ;", "conf . set ( NM _ LOG _ DIRS ,    logDir 1  )  ;", "LocalDirsHandlerService   dirSvc    =    new   LocalDirsHandlerService (  )  ;", "try    {", "dirSvc . init ( conf )  ;", "Assert . fail (  \" Service   should   have   thrown   an   exception   due   to   wrong   URI \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "}", "Assert . assertEquals (  \" Service   should   not   be   inited \"  ,    STOPPED ,    dirSvc . getServiceState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testValidPathsDirHandlerService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestLocalDirsHandlerService"}, {"methodBody": ["METHOD_START", "{", "when ( TestNMAuditLogger . APPID . toString (  )  )  . thenReturn (  \" app _  1  \"  )  ;", "when ( TestNMAuditLogger . CONTAINERID . toString (  )  )  . thenReturn (  \" container _  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "testFailureLogFormatHelper ( checkIP ,    null ,    null )  ;", "testFailureLogFormatHelper ( checkIP ,     . APPID ,    null )  ;", "testFailureLogFormatHelper ( checkIP ,    null ,     . CONTAINERID )  ;", "testFailureLogFormatHelper ( checkIP ,     . APPID ,     . CONTAINERID )  ;", "}", "METHOD_END"], "methodName": ["testFailureLogFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "String   fLog    =    NMAuditLogger . createFailureLog ( TestNMAuditLogger . USER ,    TestNMAuditLogger . OPERATION ,    TestNMAuditLogger . TARGET ,    TestNMAuditLogger . DESC ,    appId ,    containerId )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "expLog . append (  \" USER = test \\ t \"  )  ;", "if    ( checkIP )     {", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "expLog . append (  (  (  (  ( NMAuditLogger . Keys . IP . name (  )  )     +     \"  =  \"  )     +     ( ip . getHostAddress (  )  )  )     +     \"  \\ t \"  )  )  ;", "}", "expLog . append (  \" OPERATION = oper \\ tTARGET = tgt \\ tRESULT = FAILURE \\ t \"  )  ;", "expLog . append (  \" DESCRIPTION = description   of   an   audit   log \"  )  ;", "if    ( appId    !  =    null )     {", "expLog . append (  \"  \\ tAPPID = app _  1  \"  )  ;", "}", "if    ( containerId    !  =    null )     {", "expLog . append (  \"  \\ tCONTAINERID = container _  1  \"  )  ;", "}", "assertEquals ( expLog . toString (  )  ,    fLog )  ;", "}", "METHOD_END"], "methodName": ["testFailureLogFormatHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   actLog    =    new   StringBuilder (  )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "NMAuditLogger . start ( NMAuditLogger . Keys . USER ,     . USER ,    actLog )  ;", "expLog . append (  \" USER = test \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "NMAuditLogger . add ( NMAuditLogger . Keys . OPERATION ,     . OPERATION ,    actLog )  ;", "expLog . append (  \"  \\ tOPERATION = oper \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "NMAuditLogger . add ( NMAuditLogger . Keys . APPID ,     (  ( String )     ( null )  )  ,    actLog )  ;", "expLog . append (  \"  \\ tAPPID = null \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "NMAuditLogger . add ( NMAuditLogger . Keys . TARGET ,     . TARGET ,    actLog )  ;", "expLog . append (  \"  \\ tTARGET = tgt \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testKeyValLogFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Server   server    =    new   RPC . Builder ( conf )  . setProtocol ( TestProtocol . class )  . setInstance ( new    . MyTestRPCServer (  )  )  . setBindAddress (  \"  0  .  0  .  0  .  0  \"  )  . setPort (  0  )  . setNumHandlers (  5  )  . setVerbose ( true )  . build (  )  ;", "server . start (  )  ;", "InetSocketAddress   addr    =    NetUtils . getConnectAddress ( server )  ;", "TestProtocol   proxy    =     (  ( TestProtocol )     ( RPC . getProxy ( TestProtocol . class ,    versionID ,    addr ,    conf )  )  )  ;", "proxy . ping (  )  ;", "server . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNMAuditLoggerWithIP"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "testSuccessLogFormat ( false )  ;", "testFailureLogFormat ( false )  ;", "}", "METHOD_END"], "methodName": ["testNMAuditLoggerWithoutIP"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "testSuccessLogFormatHelper ( checkIP ,    null ,    null )  ;", "testSuccessLogFormatHelper ( checkIP ,     . APPID ,    null )  ;", "testSuccessLogFormatHelper ( checkIP ,    null ,     . CONTAINERID )  ;", "testSuccessLogFormatHelper ( checkIP ,     . APPID ,     . CONTAINERID )  ;", "testSuccessLogNulls ( checkIP )  ;", "}", "METHOD_END"], "methodName": ["testSuccessLogFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "String   sLog    =    NMAuditLogger . createSuccessLog ( TestNMAuditLogger . USER ,    TestNMAuditLogger . OPERATION ,    TestNMAuditLogger . TARGET ,    appId ,    containerId )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "expLog . append (  \" USER = test \\ t \"  )  ;", "if    ( checkIP )     {", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "expLog . append (  (  (  (  ( NMAuditLogger . Keys . IP . name (  )  )     +     \"  =  \"  )     +     ( ip . getHostAddress (  )  )  )     +     \"  \\ t \"  )  )  ;", "}", "expLog . append (  \" OPERATION = oper \\ tTARGET = tgt \\ tRESULT = SUCCESS \"  )  ;", "if    ( appId    !  =    null )     {", "expLog . append (  \"  \\ tAPPID = app _  1  \"  )  ;", "}", "if    ( containerId    !  =    null )     {", "expLog . append (  \"  \\ tCONTAINERID = container _  1  \"  )  ;", "}", "assertEquals ( expLog . toString (  )  ,    sLog )  ;", "}", "METHOD_END"], "methodName": ["testSuccessLogFormatHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "String   sLog    =    NMAuditLogger . createSuccessLog ( null ,    null ,    null ,    null ,    null )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "expLog . append (  \" USER = null \\ t \"  )  ;", "if    ( checkIP )     {", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "expLog . append (  (  (  (  ( NMAuditLogger . Keys . IP . name (  )  )     +     \"  =  \"  )     +     ( ip . getHostAddress (  )  )  )     +     \"  \\ t \"  )  )  ;", "}", "expLog . append (  \" OPERATION = null \\ tTARGET = null \\ tRESULT = SUCCESS \"  )  ;", "assertEquals ( expLog . toString (  )  ,    sLog )  ;", "}", "METHOD_END"], "methodName": ["testSuccessLogNulls"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ HEALTH _ CHECK _ SCRIPT _ PATH ,    nscriptFile . getAbsolutePath (  )  )  ;", "conf . setLong ( NM _ HEALTH _ CHECK _ INTERVAL _ MS ,     5  0  0  )  ;", "conf . setLong ( NM _ HEALTH _ CHECK _ SCRIPT _ TIMEOUT _ MS ,     1  0  0  0  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConfForNodeHealthScript"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "healthStatus . setHealthReport ( healthReport )  ;", "healthStatus . setIsy ( isHealthy )  ;", "healthStatus . setLastHealthReportTime ( lastHealthReportTime )  ;", "}", "METHOD_END"], "methodName": ["setHealthStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "TestNodeHealthService . testRootDir . mkdirs (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "if    ( TestNodeHealthService . testRootDir . exists (  )  )     {", "FileContext . getLocalFSFileContext (  )  . delete ( new   Path ( TestNodeHealthService . testRootDir . getAbsolutePath (  )  )  ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   factory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "NodeHealthStatus   healthStatus    =    factory . newRecordInstance ( NodeHealthStatus . class )  ;", "String   errorScript    =     \" echo   ERROR \\ n   echo    \\  \" Tracker   not   healthy \\  \"  \"  ;", "String   normalScript    =     \" echo    \\  \" I   am   all   fine \\  \"  \"  ;", "String   timeOutScript    =     ( Shell . WINDOWS )     ?     \"  @ echo   off \\ nping    - n    4     1  2  7  .  0  .  0  .  1     > nul \\ necho    \\  \" I   am   fine \\  \"  \"     :     \" sleep    4  \\ necho    \\  \" I   am   fine \\  \"  \"  ;", "Configuration   conf    =    getConfForNodeHealthScript (  )  ;", "conf . writeXml ( new   FileOutputStream (  . nodeHealthConfigFile )  )  ;", "conf . addResource (  . nodeHealthConfigFile . getName (  )  )  ;", "writeNodeHealthScriptFile ( normalScript ,    true )  ;", "NodeHealthCheckerService   nodeHealthChecker    =    new   NodeHealthCheckerService (  )  ;", "nodeHealthChecker . init ( conf )  ;", "NodeHealthScriptRunner   nodeHealthScriptRunner    =    nodeHealthChecker . getNodeHealthScriptRunner (  )  ;", "TimerTask   timerTask    =    nodeHealthScriptRunner . getTimerTask (  )  ;", "timerTask . run (  )  ;", "setHealthStatus ( healthStatus ,    nodeHealthChecker . isHealthy (  )  ,    nodeHealthChecker . getHealthReport (  )  ,    nodeHealthChecker . getLastHealthReportTime (  )  )  ;", ". LOG . info (  \" Checking   initial   healthy   condition \"  )  ;", "Assert . assertTrue (  \" Node   health   status   reported   unhealthy \"  ,    healthStatus . getIsNodeHealthy (  )  )  ;", "Assert . assertTrue (  \" Node   health   status   reported   unhealthy \"  ,    healthStatus . getHealthReport (  )  . equals ( nodeHealthChecker . getHealthReport (  )  )  )  ;", "writeNodeHealthScriptFile ( errorScript ,    true )  ;", "timerTask . run (  )  ;", "setHealthStatus ( healthStatus ,    nodeHealthChecker . isHealthy (  )  ,    nodeHealthChecker . getHealthReport (  )  ,    nodeHealthChecker . getLastHealthReportTime (  )  )  ;", ". LOG . info (  \" Checking   Healthy -  -  -  > Unhealthy \"  )  ;", "Assert . assertFalse (  \" Node   health   status   reported   healthy \"  ,    healthStatus . getIsNodeHealthy (  )  )  ;", "Assert . assertTrue (  \" Node   health   status   reported   healthy \"  ,    healthStatus . getHealthReport (  )  . equals ( nodeHealthChecker . getHealthReport (  )  )  )  ;", "writeNodeHealthScriptFile ( normalScript ,    true )  ;", "timerTask . run (  )  ;", "setHealthStatus ( healthStatus ,    nodeHealthChecker . isHealthy (  )  ,    nodeHealthChecker . getHealthReport (  )  ,    nodeHealthChecker . getLastHealthReportTime (  )  )  ;", ". LOG . info (  \" Checking   UnHealthy -  -  -  > healthy \"  )  ;", "Assert . assertTrue (  \" Node   health   status   reported   unhealthy \"  ,    healthStatus . getIsNodeHealthy (  )  )  ;", "Assert . assertTrue (  \" Node   health   status   reported   unhealthy \"  ,    healthStatus . getHealthReport (  )  . equals ( nodeHealthChecker . getHealthReport (  )  )  )  ;", "writeNodeHealthScriptFile ( timeOutScript ,    true )  ;", "timerTask . run (  )  ;", "setHealthStatus ( healthStatus ,    nodeHealthChecker . isHealthy (  )  ,    nodeHealthChecker . getHealthReport (  )  ,    nodeHealthChecker . getLastHealthReportTime (  )  )  ;", ". LOG . info (  \" Checking   Healthy -  -  -  > timeout \"  )  ;", "Assert . assertFalse (  \" Node   health   status   reported   healthy   even   after   timeout \"  ,    healthStatus . getIsNodeHealthy (  )  )  ;", "Assert . assertTrue (  \" Node   script   time   out   message   not   propogated \"  ,    healthStatus . getHealthReport (  )  . equals (  (  (  ( NodeHealthScriptRunner . NODE _ HEALTH _ SCRIPT _ TIMED _ OUT _ MSG )     +     ( NodeHealthCheckerService . SEPARATOR )  )     +     ( nodeHealthChecker . getDiskHandler (  )  . getDisksHealthReport (  )  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeHealthScript"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "Assert . assertFalse (  \" By   default   Health   script   should   not   have   started \"  ,    NodeHealthScriptRunner . shouldRun ( new   Configuration (  )  )  )  ;", "Configuration   conf    =    getConfForNodeHealthScript (  )  ;", "Assert . assertFalse (  \" Node   health   script   should   start \"  ,    NodeHealthScriptRunner . shouldRun ( conf )  )  ;", "conf . writeXml ( new   FileOutputStream (  . nodeHealthConfigFile )  )  ;", "conf . addResource (  . nodeHealthConfigFile . getName (  )  )  ;", "writeNodeHealthScriptFile (  \"  \"  ,    false )  ;", "Assert . assertFalse (  \" Node   health   script   should   start \"  ,    NodeHealthScriptRunner . shouldRun ( conf )  )  ;", "writeNodeHealthScriptFile (  \"  \"  ,    true )  ;", "Assert . assertTrue (  \" Node   health   script   should   start \"  ,    NodeHealthScriptRunner . shouldRun ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeHealthScriptShouldRun"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   pw    =    null ;", "try    {", "FileUtil . setWritable ( nscriptFile ,    true )  ;", "FileUtil . setReadable ( nscriptFile ,    true )  ;", "pw    =    new   PrintWriter ( new   FileOutputStream ( nscriptFile )  )  ;", "pw . println ( scriptStr )  ;", "pw . flush (  )  ;", "}    finally    {", "pw . close (  )  ;", "}", "FileUtil . setExecutable ( nscriptFile ,    setExecutable )  ;", "}", "METHOD_END"], "methodName": ["writeNodeHealthScriptFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeHealthService"}, {"methodBody": ["METHOD_START", "{", "NodeManager   nm    =    new   NodeManager (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( NM _ CONTAINER _ EXECUTOR ,     . InvalidContainerExecutor . class ,    ContainerExecutor . class )  ;", "try    {", "nm . init ( conf )  ;", "fail (  \" Init   should   fail \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assert   e . getCause (  )  . getMessage (  )  . contains (  \" dummy   executor   init   called \"  )  ;", "}    finally    {", "nm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testContainerExecutorInitCall"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManager"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "return   containerId ;", "}", "METHOD_END"], "methodName": ["createContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <    numOfFiles ;    i +  +  )     {", "File   newFile    =    new   File (  (  ( dir    +     \"  /  \"  )     +    subDir )  ,     (  \" file _  \"     +     ( i    +     1  )  )  )  ;", "try    {", "newFile . createNewFile (  )  ;", "}    catch    ( IOException   e )     {", "}", "}", "}", "METHOD_END"], "methodName": ["createFiles"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot"}, {"methodBody": ["METHOD_START", "{", "File [  ]    listOfFiles    =    new   File ( localDir ,    localSubDir )  . listFiles (  )  ;", "if    ( listOfFiles    =  =    null )     {", "return    0  ;", "} else    {", "return   listOfFiles . length ;", "}", "}", "METHOD_END"], "methodName": ["numOfLocalDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot"}, {"methodBody": ["METHOD_START", "{", "localFS    =    FileContext . getLocalFSFileContext (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot"}, {"methodBody": ["METHOD_START", "{", "localFS . delete ( new   Path ( TestNodeManagerReboot . basedir . getPath (  )  )  ,    true )  ;", "if    (  ( nm )     !  =    null )     {", "nm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot"}, {"methodBody": ["METHOD_START", "{", "nm    =    new   TestNodeManagerReboot . MyNodeManager (  )  ;", "nm . start (  )  ;", "final   ContainerManagementProtocol   containerManager    =    nm . getContainerManager (  )  ;", "createFiles ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ContainerLocalizer . FILECACHE ,     1  0  0  )  ;", "TestNodeManagerReboot . localResourceDir . mkdirs (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    Records . newRecord ( ContainerLaunchContext . class )  ;", "ContainerId   cId    =    createContainerId (  )  ;", "URL   localResourceUri    =    ConverterUtils . getYarnUrlFromPath ( localFS . makeQualified ( new   Path ( TestNodeManagerReboot . localResourceDir . getAbsolutePath (  )  )  )  )  ;", "LocalResource   localResource    =    LocalResource . newInstance ( localResourceUri ,    FILE ,    APPLICATION ,     (  -  1  )  ,    TestNodeManagerReboot . localResourceDir . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    localResource )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "NodeId   nodeId    =    nm . getNMContext (  )  . getNodeId (  )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    TestContainerManager . createContainerToken ( cId ,     0  ,    nodeId ,    destinationFile ,    nm . getNMContext (  )  . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "final   StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "final   UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( cId . getApplicationAttemptId (  )  . toString (  )  )  ;", "NMTokenIdentifier   nmIdentifier    =    new   NMTokenIdentifier ( cId . getApplicationAttemptId (  )  ,    nodeId ,    TestNodeManagerReboot . user ,     1  2  3  )  ;", "currentUser . addTokenIdentifier ( nmIdentifier )  ;", "currentUser . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   IOException ,    YarnException    {", "nm . getContainerManager (  )  . startContainers ( allRequests )  ;", "return   null ;", "}", "}  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "GetContainerStatusesRequest   request    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "Container   container    =    nm . getNMContext (  )  . getContainers (  )  . get ( request . getContainerIds (  )  . get (  0  )  )  ;", "final   int   MAX _ TRIES    =     2  0  ;", "int   numTries    =     0  ;", "while    (  (  !  ( container . getContainerState (  )  . equals ( ContainerState . DONE )  )  )     &  &     ( numTries    <  =    MAX _ TRIES )  )     {", "try    {", "Thread . sleep (  5  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "}", "numTries +  +  ;", "}", "Assert . assertEquals ( ContainerState . DONE ,    container . getContainerState (  )  )  ;", "Assert . assertTrue (  (  (  \" The   container   should   create   a   subDir   named   currentUser :     \"     +     ( TestNodeManagerReboot . user )  )     +     \" under   localDir / usercache \"  )  ,     (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ContainerLocalizer . USERCACHE )  )     >     0  )  )  ;", "Assert . assertTrue (  (  \" There   should   be   files   or   Dirs   under   nm _ private   when    \"     +     \" container   is   launched \"  )  ,     (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ResourceLocalizationService . NM _ PRIVATE _ DIR )  )     >     0  )  )  ;", "nm . stop (  )  ;", "nm    =    new   TestNodeManagerReboot . MyNodeManager (  )  ;", "nm . start (  )  ;", "numTries    =     0  ;", "while    (  (  (  (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ContainerLocalizer . USERCACHE )  )     >     0  )     |  |     (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ContainerLocalizer . FILECACHE )  )     >     0  )  )     |  |     (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ResourceLocalizationService . NM _ PRIVATE _ DIR )  )     >     0  )  )     &  &     ( numTries    <    MAX _ TRIES )  )     {", "try    {", "Thread . sleep (  5  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "}", "numTries +  +  ;", "}", "Assert . assertTrue (  \" After   NM   reboots ,    all   local   files   should   be   deleted \"  ,     (  (  (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ContainerLocalizer . USERCACHE )  )     =  =     0  )     &  &     (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ContainerLocalizer . FILECACHE )  )     =  =     0  )  )     &  &     (  ( numOfLocalDirs ( TestNodeManagerReboot . nmLocalDir . getAbsolutePath (  )  ,    ResourceLocalizationService . NM _ PRIVATE _ DIR )  )     =  =     0  )  )  )  ;", "verify ( delService ,    times (  1  )  )  . delete (  (  ( String )     ( isNull (  )  )  )  ,    argThat ( new   TestNodeManagerReboot . PathInclude (  (  ( ResourceLocalizationService . NM _ PRIVATE _ DIR )     +     \"  _ DEL _  \"  )  )  )  )  ;", "verify ( delService ,    times (  1  )  )  . delete (  (  ( String )     ( isNull (  )  )  )  ,    argThat ( new   TestNodeManagerReboot . PathInclude (  (  ( ContainerLocalizer . FILECACHE )     +     \"  _ DEL _  \"  )  )  )  )  ;", "verify ( delService ,    times (  1  )  )  . scheduleFileDeletionTask ( argThat ( new   TestNodeManagerReboot . FileDeletionInclude ( TestNodeManagerReboot . user ,    null ,    new   String [  ]  {    destinationFile    }  )  )  )  ;", "verify ( delService ,    times (  1  )  )  . scheduleFileDeletionTask ( argThat ( new   TestNodeManagerReboot . FileDeletionInclude ( null ,     (  ( ContainerLocalizer . USERCACHE )     +     \"  _ DEL _  \"  )  ,    new   String [  ]  {        }  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testClearLocalDirWhenNodeReboot"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerReboot"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( NM _ PMEM _ MB ,     (  5     *     1  0  2  4  )  )  ;", "conf . set ( NM _ ADDRESS ,     \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  5  \"  )  ;", "conf . set ( NM _ LOCALIZER _ ADDRESS ,     \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  6  \"  )  ;", "conf . set ( NM _ LOG _ DIRS ,     . logsDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,     . remoteLogsDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,     . nmLocalDir . getAbsolutePath (  )  )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,     1  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createNMConfig"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( applicationAttemptId ,    id )  ;", "NMContainerStatus   containport    =    NMContainerStatus . newInstance ( containerId ,    containerState ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" recover   container \"  ,     0  ,    Priority . newInstance (  1  0  )  ,     0  )  ;", "return   containport ;", "}", "METHOD_END"], "methodName": ["createNMContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "localFS    =    FileContext . getLocalFSFileContext (  )  ;", ". tmpDir . mkdirs (  )  ;", ". logsDir . mkdirs (  )  ;", ". remoteLogsDir . mkdirs (  )  ;", ". nmLocalDir . mkdirs (  )  ;", "syncBarrier    =    new   CyclicBarrier (  2  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "localFS . delete ( new   Path ( TestNodeManagerResync . basedir . getPath (  )  )  ,    true )  ;", "assertionFailedInThread . set ( false )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "NodeManager   nm    =    new   TestNodeManagerResync . TestNodeManager 2  (  )  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "ContainerId   cId    =    TestNodeManagerShutdown . createContainerId (  )  ;", "TestNodeManagerShutdown . startContainer ( nm ,    cId ,    localFS ,    TestNodeManagerResync . tmpDir ,    TestNodeManagerResync . processStartFile )  ;", "nm . getNMDispatcher (  )  . getEventHandler (  )  . handle ( new   NodeManagerEvent ( NodeManagerEventType . RESYNC )  )  ;", "try    {", "syncBarrier . await (  )  ;", "}    catch    ( BrokenBarrierException   e )     {", "}", "Assert . assertFalse ( assertionFailedInThread . get (  )  )  ;", "nm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testBlockNewContainerRequestsOnStartAndResync"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "conf . setBoolean ( RM _ WORK _ PRESERVING _ RECOVERY _ ENABLED ,    isWorkPreservingRestartEnabled )  ;", "try    {", "nm . init ( conf )  ;", "nm . start (  )  ;", "ContainerId   cId    =    TestNodeManagerShutdown . createContainerId (  )  ;", "TestNodeManagerShutdown . startContainer ( nm ,    cId ,    localFS ,     . tmpDir ,     . processStartFile )  ;", "nm . setExistingContainerId ( cId )  ;", "Assert . assertEquals (  1  ,     (  (  . TestNodeManager 1  )     ( nm )  )  . getNMRegistrationCount (  )  )  ;", "nm . getNMDispatcher (  )  . getEventHandler (  )  . handle ( resyncEvent )  ;", "try    {", "syncBarrier . await (  )  ;", "}    catch    ( BrokenBarrierException   e )     {", "}", "Assert . assertEquals (  2  ,     (  (  . TestNodeManager 1  )     ( nm )  )  . getNMRegistrationCount (  )  )  ;", "Assert . assertTrue ( nm . getNMContext (  )  . getApplications (  )  . containsKey ( cId . getApplicationAttemptId (  )  . getApplicationId (  )  )  )  ;", "Assert . assertFalse ( assertionFailedInThread . get (  )  )  ;", "}    finally    {", "nm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testContainerPreservationOnResyncImpl"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "TestNodeManagerResync . TestNodeManager 1    nm    =    new   TestNodeManagerResync . TestNodeManager 1  ( false )  ;", "testContainerPreservationOnResyncImpl ( nm ,    false )  ;", "}", "METHOD_END"], "methodName": ["testKillContainersOnResync"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "final   ContainerStatus   testCompleteContainer    =    TestNodeStatusUpdater . createContainerStatus (  2  ,    COMPLETE )  ;", "final   Container   container    =    TestNodeStatusUpdater . getMockContainer ( testCompleteContainer )  ;", "NMContainerStatus   report    =     . createNMContainerStatus (  2  ,    COMPLETE )  ;", "when ( container . getNMContainerStatus (  )  )  . thenReturn ( report )  ;", "NodeManager   nm    =    new   NodeManager (  )     {", "int   registerCount    =     0  ;", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", "return   new    . TestNodeStatusUpdaterResync ( context ,    dispatcher ,    healthChecker ,    metrics )     {", "@ Override", "protected   ResourceTracker   createResourceTracker (  )     {", "return   new   MockNodeStatusUpdater . MockResourceTracker (  )     {", "@ Override", "public   RegisterNodeManagerResponse   registerNodeManager ( RegisterNodeManagerRequest   request )    throws   IOException ,    YarnException    {", "if    (  ( registerCount )     =  =     0  )     {", "try    {", "Assert . assertEquals (  0  ,    request . getNMContainerStatuses (  )  . size (  )  )  ;", "}    catch    ( AssertionError   error )     {", "error . printStackTrace (  )  ;", "assertionFailedInThread . set ( true )  ;", "}", "getNMContext (  )  . getContainers (  )  . put ( testCompleteContainer . getContainerId (  )  ,    container )  ;", "} else    {", "List < NMContainerStatus >    statuses    =    request . getNMContainerStatuses (  )  ;", "try    {", "Assert . assertEquals (  1  ,    statuses . size (  )  )  ;", "Assert . assertEquals ( testCompleteContainer . getContainerId (  )  ,    statuses . get (  0  )  . getContainerId (  )  )  ;", "}    catch    ( AssertionError   error )     {", "error . printStackTrace (  )  ;", "assertionFailedInThread . set ( true )  ;", "}", "}", "( registerCount )  +  +  ;", "return   super . registerNodeManager ( request )  ;", "}", "@ Override", "public   NodeHeartbeatResponse   nodeHeartbeat ( NodeHeartbeatRequest   request )     {", "List < ContainerStatus >    statuses    =    request . getNodeStatus (  )  . getContainersStatuses (  )  ;", "try    {", "Assert . assertEquals (  1  ,    statuses . size (  )  )  ;", "Assert . assertEquals ( testCompleteContainer . getContainerId (  )  ,    statuses . get (  0  )  . getContainerId (  )  )  ;", "}    catch    ( AssertionError   error )     {", "error . printStackTrace (  )  ;", "assertionFailedInThread . set ( true )  ;", "}", "return   YarnServerBuilderUtils . newNodeHeartbeatResponse (  1  ,    NodeAction . RESYNC ,    null ,    null ,    null ,    null ,     1  0  0  0 L )  ;", "}", "}  ;", "}", "}  ;", "}", "}  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "try    {", "syncBarrier . await (  )  ;", "}    catch    ( BrokenBarrierException   e )     {", "}", "Assert . assertFalse ( assertionFailedInThread . get (  )  )  ;", "nm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNMSentContainerStatusOnResync"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "NodeManager   nm    =    new   TestNodeManagerResync . TestNodeManager 3  (  )  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "Assert . assertEquals (  1  ,     (  ( TestNodeManagerResync . TestNodeManager 3  )     ( nm )  )  . getNMRegistrationCount (  )  )  ;", "nm . getNMDispatcher (  )  . getEventHandler (  )  . handle ( new   NodeManagerEvent ( NodeManagerEventType . RESYNC )  )  ;", "synchronized ( isNMShutdownCalled )     {", "while    (  ( isNMShutdownCalled . get (  )  )     =  =    false )     {", "try    {", "isNMShutdownCalled . wait (  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "}", "Assert . assertTrue (  \" NM   shutdown   not   called .  \"  ,    isNMShutdownCalled . get (  )  )  ;", "nm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNMshutdownWhenResyncThrowException"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "TestNodeManagerResync . TestNodeManager 1    nm    =    new   TestNodeManagerResync . TestNodeManager 1  ( true )  ;", "testContainerPreservationOnResyncImpl ( nm ,    true )  ;", "}", "METHOD_END"], "methodName": ["testPreserveContainersOnResyncKeepingContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerResync"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "return   containerId ;", "}", "METHOD_END"], "methodName": ["createContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( NM _ PMEM _ MB ,     (  5     *     1  0  2  4  )  )  ;", "conf . set ( NM _ ADDRESS ,     \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  5  \"  )  ;", "conf . set ( NM _ LOCALIZER _ ADDRESS ,     \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  6  \"  )  ;", "conf . set ( NM _ LOG _ DIRS ,     . logsDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,     . remoteLogsDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,     . nmLocalDir . getAbsolutePath (  )  )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,     1  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createNMConfig"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "File   scriptFile    =    Shell . appendScriptExtension ( scriptFileDir ,     \" scriptFile \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( scriptFile )  ;", "if    ( Shell . WINDOWS )     {", "fileWriter . println (  \"  @ echo    \\  \" Running   testscript   for   delayed   kill \\  \"  \"  )  ;", "fileWriter . println (  \"  @ echo    \\  \" Writing   pid   to   start   file \\  \"  \"  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +    cId )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  \"  @ pause \"  )  ;", "} else    {", "fileWriter . write (  \"  #  !  / bin / bash \\ n \\ n \"  )  ;", "fileWriter . write (  \" echo    \\  \" Running   testscript   for   delayed   kill \\  \"  \\ n \"  )  ;", "fileWriter . write (  \" hello =  \\  \" Got   SIGTERM \\  \"  \\ n \"  )  ;", "fileWriter . write (  \" umask    0  \\ n \"  )  ;", "fileWriter . write (  (  (  \" trap    \\  \" echo    $ hello    >  >     \"     +    processStartFile )     +     \"  \\  \"    SIGTERM \\ n \"  )  )  ;", "fileWriter . write (  \" echo    \\  \" Writing   pid   to   start   file \\  \"  \\ n \"  )  ;", "fileWriter . write (  (  (  \" echo    $  $     >  >     \"     +    processStartFile )     +     \"  \\ n \"  )  )  ;", "fileWriter . write (  \" while   true ;    do \\ ndate    >  >     / dev / null ;  \\ n   done \\ n \"  )  ;", "}", "fileWriter . close (  )  ;", "return   scriptFile ;", "}", "METHOD_END"], "methodName": ["createUnhaltingScriptFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "localFS    =    FileContext . getLocalFSFileContext (  )  ;", ". tmpDir . mkdirs (  )  ;", ". logsDir . mkdirs (  )  ;", ". remoteLogsDir . mkdirs (  )  ;", ". nmLocalDir . mkdirs (  )  ;", "cId    =     . createContainerId (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "File   scriptFile    =    TestNodeManagerShutdown . createUnhaltingScriptFile ( cId ,    scriptFileDir ,    processStartFile )  ;", "ContainerLaunchContext   containerLaunchContext    =    TestNodeManagerShutdown . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "NodeId   nodeId    =    BuilderUtils . newNodeId ( InetAddress . getByName (  \" localhost \"  )  . getCanonicalHostName (  )  ,     1  2  3  4  5  )  ;", "URL   localResourceUri    =    ConverterUtils . getYarnUrlFromPath ( localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   localResource    =    TestNodeManagerShutdown . recordFactory . newRecordInstance ( LocalResource . class )  ;", "localResource . setResource ( localResourceUri )  ;", "localResource . setSize (  (  -  1  )  )  ;", "localResource . setVisibility ( APPLICATION )  ;", "localResource . setType ( FILE )  ;", "localResource . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    localResource )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    Arrays . asList ( Shell . getRunScriptCommand ( scriptFile )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "final   InetSocketAddress   containerManagerBindAddress    =    NetUtils . createSocketAddrForHost (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  2  3  4  5  )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( cId . toString (  )  )  ;", "Token < NMTokenIdentifier >    nmToken    =    ConverterUtils . convertFromYarn ( nm . getNMContext (  )  . getNMTokenSecretManager (  )  . createNMToken ( cId . getApplicationAttemptId (  )  ,    nodeId ,    TestNodeManagerShutdown . user )  ,    containerManagerBindAddress )  ;", "currentUser . addToken ( nmToken )  ;", "ContainerManagementProtocol   containerManager    =    currentUser . doAs ( new   PrivilegedAction < ContainerManagementProtocol >  (  )     {", "@ Override", "public   ContainerManagementProtocol   run (  )     {", "Configuration   conf    =    new   Configuration (  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "InetSocketAddress   containerManagerBindAddress    =    NetUtils . createSocketAddrForHost (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  2  3  4  5  )  ;", "return    (  ( ContainerManagementProtocol )     ( rpc . getProxy ( ContainerManagementProtocol . class ,    containerManagerBindAddress ,    conf )  )  )  ;", "}", "}  )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    TestContainerManager . createContainerToken ( cId ,     0  ,    nodeId ,    TestNodeManagerShutdown . user ,    nm . getNMContext (  )  . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "GetContainerStatusesRequest   request    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( request )  . getContainerStatuses (  )  . get (  0  )  ;", "Assert . assertEquals ( RUNNING ,    containerStatus . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["startContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "if    (  ( nm )     !  =    null )     {", "nm . stop (  )  ;", "}", "localFS . delete ( new   Path (  . basedir . getPath (  )  )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "nm    =    new   TestNodeManagerShutdown . TestNodeManager (  )  ;", "nm . init ( createNMConfig (  )  )  ;", "nm . start (  )  ;", "TestNodeManagerShutdown . startContainer ( nm ,    cId ,    localFS ,    TestNodeManagerShutdown . tmpDir ,    TestNodeManagerShutdown . processStartFile )  ;", "final   int   MAX _ TRIES    =     2  0  ;", "int   numTries    =     0  ;", "while    (  (  !  ( TestNodeManagerShutdown . processStartFile . exists (  )  )  )     &  &     ( numTries    <    MAX _ TRIES )  )     {", "try    {", "Thread . sleep (  5  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "ex . printStackTrace (  )  ;", "}", "numTries +  +  ;", "}", "nm . stop (  )  ;", "if    ( Shell . WINDOWS )     {", "Assert . assertFalse (  \" Process   is   still   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( cId . toString (  )  )  )  ;", "} else    {", "BufferedReader   reader    =    new   BufferedReader ( new   FileReader ( TestNodeManagerShutdown . processStartFile )  )  ;", "boolean   foundSigTermMessage    =    false ;", "while    ( true )     {", "String   line    =    reader . readLine (  )  ;", "if    ( line    =  =    null )     {", "break ;", "}", "if    ( line . contains (  \" SIGTERM \"  )  )     {", "foundSigTermMessage    =    true ;", "break ;", "}", "}", "Assert . assertTrue (  \" Did   not   find   sigterm   message \"  ,    foundSigTermMessage )  ;", "reader . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testKillContainersOnShutdown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "final   File   recoveryDir    =    new   File ( TestNodeManagerShutdown . basedir ,     \" nm - recovery \"  )  ;", "nm    =    new   TestNodeManagerShutdown . TestNodeManager (  )  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "conf . set ( NM _ RECOVERY _ DIR ,    recoveryDir . getAbsolutePath (  )  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "Assert . assertTrue ( recoveryDir . exists (  )  )  ;", "Assert . assertTrue ( recoveryDir . isDirectory (  )  )  ;", "nm . stop (  )  ;", "nm    =    null ;", "Assert . assertTrue ( recoveryDir . exists (  )  )  ;", "Assert . assertTrue ( recoveryDir . isDirectory (  )  )  ;", "nm    =    new   TestNodeManagerShutdown . TestNodeManager (  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "Assert . assertTrue ( recoveryDir . exists (  )  )  ;", "Assert . assertTrue ( recoveryDir . isDirectory (  )  )  ;", "nm . getNMContext (  )  . setDecommissioned ( true )  ;", "nm . stop (  )  ;", "nm    =    null ;", "Assert . assertFalse ( recoveryDir . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStateStoreRemovalOnDecommission"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown"}, {"methodBody": ["METHOD_START", "{", "nmStartError    =    null ;", "}", "METHOD_END"], "methodName": ["clearError"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     1  )  ;", "ContainerId   contaierId    =    ContainerId . newInstance ( applicationAttemptId ,    id )  ;", "Container   container    =    BuilderUtils . newContainer ( contaierId ,    containerState ,     (  (  (  \" test _ container :    id =  \"     +    id )     +     \"  ,    containerState :     \"  )     +    containerState )  ,     0  )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "MasterKey   masterKey    =    new   MasterKeyPBImpl (  )  ;", "masterKey . setKeyId (  1  2  3  )  ;", "masterKey . setBytes ( ByteBuffer . wrap ( new   byte [  ]  {    new   Integer (  1  2  3  )  . byteValue (  )     }  )  )  ;", "return   masterKey ;", "}", "METHOD_END"], "methodName": ["createMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "String   localhostAddress    =    null ;", "try    {", "localhostAddress    =    InetAddress . getByName (  \" localhost \"  )  . getCanonicalHostName (  )  ;", "}    catch    ( UnknownHostException   e )     {", "Assert . fail (  (  \" Unable   to   get   localhost   address :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "conf . setInt ( NM _ PMEM _ MB ,     (  5     *     1  0  2  4  )  )  ;", "conf . set ( NM _ ADDRESS ,     ( localhostAddress    +     \"  :  1  2  3  4  5  \"  )  )  ;", "conf . set ( NM _ LOCALIZER _ ADDRESS ,     ( localhostAddress    +     \"  :  1  2  3  4  6  \"  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,     . logsDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,     . remoteLogsDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,     . nmLocalDir . getAbsolutePath (  )  )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,     1  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createNMConfig"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "FileContext   lfs    =    FileContext . getLocalFSFileContext (  )  ;", "lfs . delete ( new   Path (  . basedir . getPath (  )  )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["deleteBaseDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "ContainerImpl   container    =    mock ( ContainerImpl . class )  ;", "when ( container . cloneAndGetContainer (  )  )  . thenReturn ( container )  ;", "when ( container . getCurrentState (  )  )  . thenReturn ( container . getState (  )  )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( container . getContainerId (  )  )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["getMockContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeManager (  )     {", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", ". MyNodeStatusUpdater   myNodeStatusUpdater    =    new    . MyNodeStatusUpdater ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", ". MyResourceTracker 2    myResourceTracker 2     =    new    . MyResourceTracker 2  (  )  ;", "myResourceTracker 2  . heartBeatNodeAction    =    nodeHeartBeatAction ;", "myNodeStatusUpdater . resourceTracker    =    myResourceTracker 2  ;", "return   myNodeStatusUpdater ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getNodeManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "TestNodeStatusUpdater . nmLocalDir . mkdirs (  )  ;", "TestNodeStatusUpdater . tmpDir . mkdirs (  )  ;", "TestNodeStatusUpdater . logsDir . mkdirs (  )  ;", "TestNodeStatusUpdater . remoteLogsDir . mkdirs (  )  ;", "conf    =    createNMConfig (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "this . registeredNodes . clear (  )  ;", "heartBeatID    =     0  ;", "ServiceOperations . stop ( nm )  ;", "assertionFailedInThread . set ( false )  ;", "DefaultMetricsSystem . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "TestNodeStatusUpdater . MyNodeManager   nm    =    new   TestNodeStatusUpdater . MyNodeManager (  )  ;", "try    {", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "conf . setBoolean ( LOG _ AGGREGATION _ ENABLED ,    true )  ;", "conf . setLong ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,     4  0  0  0 L )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "while    (  ( heartBeatID )     <     1  2  )     {", "Thread . sleep (  1  0  0  0 L )  ;", "}", "TestNodeStatusUpdater . MyResourceTracker 3    rt    =     (  ( TestNodeStatusUpdater . MyResourceTracker 3  )     ( nm . getNodeStatusUpdater (  )  . getRMClient (  )  )  )  ;", "rt . context . getApplications (  )  . remove ( rt . appId )  ;", "Assert . assertEquals (  1  ,    rt . keepAliveRequests . size (  )  )  ;", "int   numKeepAliveRequests    =    rt . keepAliveRequests . get ( rt . appId )  . size (  )  ;", "TestNodeStatusUpdater . LOG . info (  (  (  \" Number   of   Keep   Alive   Requests :     [  \"     +    numKeepAliveRequests )     +     \"  ]  \"  )  )  ;", "Assert . assertTrue (  (  ( numKeepAliveRequests    =  =     2  )     |  |     ( numKeepAliveRequests    =  =     3  )  )  )  ;", "while    (  ( heartBeatID )     <     2  0  )     {", "Thread . sleep (  1  0  0  0 L )  ;", "}", "int   numKeepAliveRequests 2     =    rt . keepAliveRequests . get ( rt . appId )  . size (  )  ;", "Assert . assertEquals ( numKeepAliveRequests ,    numKeepAliveRequests 2  )  ;", "}    finally    {", "if    (  ( nm . getServiceState (  )  )     =  =     ( STATE . STARTED )  )", "nm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testApplicationKeepAlive"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "nm    =    new   NodeManager (  )     {", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", ". MyNodeStatusUpdater 2    myNodeStatusUpdater    =    new    . MyNodeStatusUpdater 2  ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", "return   myNodeStatusUpdater ;", "}", "@ Override", "protected   NodeManager . NMContext   createNMContext ( NMContainerTokenSecretManager   containerTokenSecretManager ,    NMTokenSecretManagerInNM   nmTokenSecretManager ,    NMStateStoreService   store )     {", "return   new    . MyNMContext ( containerTokenSecretManager ,    nmTokenSecretManager )  ;", "}", "}  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( heartBeatID )     <  =     3  )     &  &     (  ( waitCount +  +  )     !  =     2  0  )  )     {", "Thread . sleep (  5  0  0  )  ;", "}", "if    ( assertionFailedInThread . get (  )  )     {", "Assert . fail (  \" ContainerStatus   Backup   failed \"  )  ;", "}", "nm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testCompletedContainerStatusBackup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "final   long   delta    =     5  0  0  0  0  ;", "final   long   connectionWaitMs    =     5  0  0  0  ;", "final   long   connectionRetryIntervalMs    =     1  0  0  0  ;", "final   long   rmStartIntervalMS    =     2     *     1  0  0  0  ;", "conf . setLong ( RESOURCEMANAGER _ CONNECT _ MAX _ WAIT _ MS ,    connectionWaitMs )  ;", "conf . setLong ( RESOURCEMANAGER _ CONNECT _ RETRY _ INTERVAL _ MS ,    connectionRetryIntervalMs )  ;", ". NodeManagerWithCustomNodeStatusUpdater   nmWithUpdater ;", "nm    =    nmWithUpdater    =    new    . NodeManagerWithCustomNodeStatusUpdater (  )     {", "@ Override", "protected   NodeStatusUpdater   createUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", "NodeStatusUpdater   nodeStatusUpdater    =    new    . MyNodeStatusUpdater 4  ( context ,    dispatcher ,    healthChecker ,    metrics ,    rmStartIntervalMS ,    true )  ;", "return   nodeStatusUpdater ;", "}", "}  ;", "nm . init ( conf )  ;", "long   waitStartTime    =    System . currentTimeMillis (  )  ;", "try    {", "nm . start (  )  ;", "Assert . fail (  \" NM   should   have   failed   to   start   due   to   RM   connect   failure \"  )  ;", "}    catch    ( Exception   e )     {", "long   t    =    System . currentTimeMillis (  )  ;", "long   duration    =    t    -    waitStartTime ;", "boolean   waitTimeValid    =     ( duration    >  =    connectionWaitMs )     &  &     ( duration    <     ( connectionWaitMs    +    delta )  )  ;", "if    (  ! waitTimeValid )     {", "throw   new   Exception (  (  (  (  (  (  (  (  \" NM   should   have   tried   re - connecting   to   RM   during    \"     +     \" period   of   at   least    \"  )     +    connectionWaitMs )     +     \"    ms ,    but    \"  )     +     \" stopped   retrying   within    \"  )     +     ( connectionWaitMs    +    delta )  )     +     \"    ms :     \"  )     +    e )  ,    e )  ;", "}", "}", "nm    =    nmWithUpdater    =    new    . NodeManagerWithCustomNodeStatusUpdater (  )     {", "@ Override", "protected   NodeStatusUpdater   createUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", "NodeStatusUpdater   nodeStatusUpdater    =    new    . MyNodeStatusUpdater 4  ( context ,    dispatcher ,    healthChecker ,    metrics ,    rmStartIntervalMS ,    false )  ;", "return   nodeStatusUpdater ;", "}", "}  ;", "nm . init ( conf )  ;", "NodeStatusUpdater   updater    =    nmWithUpdater . getUpdater (  )  ;", "Assert . assertNotNull (  \" Updater   not   yet   created    \"  ,    updater )  ;", "waitStartTime    =    System . currentTimeMillis (  )  ;", "try    {", "nm . start (  )  ;", "}    catch    ( Exception   ex )     {", ". LOG . error (  (  \" NM   should   have   started   successfully    \"     +     \" after   connecting   to   RM .  \"  )  ,    ex )  ;", "throw   ex ;", "}", "long   duration    =     ( System . currentTimeMillis (  )  )     -    waitStartTime ;", ". MyNodeStatusUpdater 4    myUpdater    =     (  (  . MyNodeStatusUpdater 4  )     ( updater )  )  ;", "Assert . assertTrue (  \" NM   started   before   updater   triggered \"  ,    myUpdater . isTriggered (  )  )  ;", "Assert . assertTrue (  (  (  (  (  (  (  \" NM   should   have   connected   to   RM   after    \"     +     \" the   start   interval   of    \"  )     +    rmStartIntervalMS )     +     \"  :    actual    \"  )     +    duration )     +     \"     \"  )     +    myUpdater )  ,     ( duration    >  =    rmStartIntervalMS )  )  ;", "Assert . assertTrue (  (  (  (  (  (  \" NM   should   have   connected   to   RM   less   than    \"     +     ( rmStartIntervalMS    +    delta )  )     +     \"    milliseconds   of   RM   starting   up :    actual    \"  )     +    duration )     +     \"     \"  )     +    myUpdater )  ,     ( duration    <     ( rmStartIntervalMS    +    delta )  )  )  ;", "}", "METHOD_END"], "methodName": ["testNMConnectionToRM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "nm    =    new   NodeManager (  )     {", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", "return   new    . MyNodeStatusUpdater ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", "}", "}  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "Object [  ]    services    =    nm . getServices (  )  . toArray (  )  ;", "Object   lastService    =    services [  (  ( services . length )     -     1  )  ]  ;", "Assert . assertTrue (  \" last   service   is   NOT   the   node   status   updater \"  ,     ( lastService   instanceof   NodeStatusUpdater )  )  ;", "new   Thread (  )     {", "public   void   run (  )     {", "try    {", "nm . start (  )  ;", "}    catch    ( Throwable   e )     {", ". this . nmStartError    =    e ;", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "}  . start (  )  ;", "System . out . println (  (  \"     -  -  -  -  -    thread   already   started .  .  \"     +     ( nm . getServiceState (  )  )  )  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( nm . getServiceState (  )  )     =  =     ( STATE . INITED )  )     &  &     (  ( waitCount +  +  )     !  =     5  0  )  )     {", ". LOG . info (  \" Waiting   for   NM   to   start .  .  \"  )  ;", "if    (  ( nmStartError )     !  =    null )     {", ". LOG . error (  \" Error   during   startup .     \"  ,    nmStartError )  ;", "Assert . fail ( nmStartError . getCause (  )  . getMessage (  )  )  ;", "}", "Thread . sleep (  2  0  0  0  )  ;", "}", "if    (  ( nm . getServiceState (  )  )     !  =     ( STATE . STARTED )  )     {", "Assert . fail (  \" NodeManager   failed   to   start \"  )  ;", "}", "waitCount    =     0  ;", "while    (  (  ( heartBeatID )     <  =     3  )     &  &     (  ( waitCount +  +  )     !  =     2  0  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertFalse (  (  ( heartBeatID )     <  =     3  )  )  ;", "Assert . assertEquals (  \" Number   of   registered   NMs   is   wrong !  !  \"  ,     1  ,    this . registeredNodes . size (  )  )  ;", "nm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNMRegistration"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "nm    =    new   TestNodeStatusUpdater . NodeManagerWithCustomNodeStatusUpdater (  )     {", "@ Override", "protected   NodeStatusUpdater   createUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", "TestNodeStatusUpdater . MyNodeStatusUpdater   nodeStatusUpdater    =    new   TestNodeStatusUpdater . MyNodeStatusUpdater ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", "TestNodeStatusUpdater . MyResourceTracker 2    myResourceTracker 2     =    new   TestNodeStatusUpdater . MyResourceTracker 2  (  )  ;", "myResourceTracker 2  . registerNodeAction    =    NodeAction . SHUTDOWN ;", "myResourceTracker 2  . shutDownMessage    =     \" RM   Shutting   Down   Node \"  ;", "nodeStatusUpdater . resourceTracker    =    myResourceTracker 2  ;", "return   nodeStatusUpdater ;", "}", "}  ;", "verifyNodeStartFailure (  (  \" Recieved   SHUTDOWN   signal   from   Resourcemanager    ,  \"     +     (  \" Registration   of   NodeManager   failed ,     \"     +     \" Message   from   ResourceManager :    RM   Shutting   Down   Node \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testNMShutdownForRegistrationFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "nm    =    new   NodeManager (  )     {", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", "return   new    . MyNodeStatusUpdater ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", "}", "@ Override", "protected   ContainerManagerImpl   createContainerManager ( Context   context ,    ContainerExecutor   exec ,    DeletionService   del ,    NodeStatusUpdater   nodeStatusUpdater ,    ApplicationACLsManager   aclsManager ,    LocalDirsHandlerService   diskhandler )     {", "return   new   ContainerManagerImpl ( context ,    exec ,    del ,    nodeStatusUpdater ,    metrics ,    aclsManager ,    diskhandler )     {", "@ Override", "protected   void   serviceStart (  )     {", "throw   new   YarnRuntimeException (  \" Starting   of   RPC   Server   failed \"  )  ;", "}", "}  ;", "}", "}  ;", "verifyNodeStartFailure (  \" Starting   of   RPC   Server   failed \"  )  ;", "}", "METHOD_END"], "methodName": ["testNoRegistrationWhenNMServicesFail"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "nm    =    getNodeManager ( NodeAction . SHUTDOWN )  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "Assert . assertEquals ( INITED ,    nm . getServiceState (  )  )  ;", "nm . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( heartBeatID )     <     1  )     &  &     (  ( waitCount +  +  )     !  =     2  0  0  )  )     {", "Thread . sleep (  5  0  0  )  ;", "}", "Assert . assertFalse (  (  ( heartBeatID )     <     1  )  )  ;", "Assert . assertTrue ( nm . getNMContext (  )  . getDecommissioned (  )  )  ;", "waitCount    =     0  ;", "while    (  (  ( nm . getServiceState (  )  )     !  =     ( STATE . STOPPED )  )     &  &     (  ( waitCount +  +  )     !  =     2  0  )  )     {", ". LOG . info (  \" Waiting   for   NM   to   stop .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertEquals ( STOPPED ,    nm . getServiceState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeDecommision"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "final   long   connectionWaitSecs    =     1  0  0  0  ;", "final   long   connectionRetryIntervalMs    =     1  0  0  0  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "conf . setLong ( RESOURCEMANAGER _ CONNECT _ MAX _ WAIT _ MS ,    connectionWaitSecs )  ;", "conf . setLong ( RESOURCEMANAGER _ CONNECT _ RETRY _ INTERVAL _ MS ,    connectionRetryIntervalMs )  ;", "conf . setLong ( NM _ SLEEP _ DELAY _ BEFORE _ SIGKILL _ MS ,     5  0  0  0  )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,     1  )  ;", "CyclicBarrier   syncBarrier    =    new   CyclicBarrier (  2  )  ;", "nm    =    new    . MyNodeManager 2  ( syncBarrier ,    conf )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "ContainerId   cId    =    TestNodeManagerShutdown . createContainerId (  )  ;", "FileContext   localFS    =    FileContext . getLocalFSFileContext (  )  ;", "TestNodeManagerShutdown . startContainer ( nm ,    cId ,    localFS ,     . nmLocalDir ,    new   File (  \" start _ file . txt \"  )  )  ;", "try    {", "syncBarrier . await (  1  0  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "}    catch    ( Exception   e )     {", "}", "Assert . assertFalse (  \" Containers   not   cleaned   up   when   NM   stopped \"  ,    assertionFailedInThread . get (  )  )  ;", "Assert . assertTrue (  (  (  . MyNodeManager 2  )     ( nm )  )  . isStopped )  ;", "Assert . assertTrue (  (  \" calculate   heartBeatCount   based   on \"     +     \"    connectionWaitSecs   and   RetryIntervalSecs \"  )  ,     (  ( heartBeatID )     =  =     2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeStatusUpdaterRetryAndNMShutdown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "final   AtomicInteger   numCleanups    =    new   AtomicInteger (  0  )  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "conf . set ( NM _ RESOURCEMANAGER _ MINIMUM _ VERSION ,     \"  3  .  0  .  0  \"  )  ;", "nm    =    new   NodeManager (  )     {", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", ". MyNodeStatusUpdater   myNodeStatusUpdater    =    new    . MyNodeStatusUpdater ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", ". MyResourceTracker 2    myResourceTracker 2     =    new    . MyResourceTracker 2  (  )  ;", "myResourceTracker 2  . heartBeatNodeAction    =    NodeAction . NORMAL ;", "myResourceTracker 2  . rmVersion    =     \"  3  .  0  .  0  \"  ;", "myNodeStatusUpdater . resourceTracker    =    myResourceTracker 2  ;", "return   myNodeStatusUpdater ;", "}", "@ Override", "protected   ContainerManagerImpl   createContainerManager ( Context   context ,    ContainerExecutor   exec ,    DeletionService   del ,    NodeStatusUpdater   nodeStatusUpdater ,    ApplicationACLsManager   aclsManager ,    LocalDirsHandlerService   dirsHandler )     {", "return   new   ContainerManagerImpl ( context ,    exec ,    del ,    nodeStatusUpdater ,    metrics ,    aclsManager ,    dirsHandler )     {", "@ Override", "public   void   cleanUpApplicationsOnNMShutDown (  )     {", "super . cleanUpApplicationsOnNMShutDown (  )  ;", "numCleanups . incrementAndGet (  )  ;", "}", "}  ;", "}", "}  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( nm . getServiceState (  )  )     !  =     ( STATE . STARTED )  )     &  &     (  ( waitCount +  +  )     !  =     2  0  )  )     {", ". LOG . info (  \" Waiting   for   NM   to   stop .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertTrue (  (  ( nm . getServiceState (  )  )     =  =     ( STATE . STARTED )  )  )  ;", "nm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMVersionLessThanMinimum"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "NodeManager   nm    =    new   NodeManager (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( NodeStatusUpdaterImpl . YARN _ NODEMANAGER _ DURATION _ TO _ TRACK _ STOPPED _ CONTAINERS ,     \"  1  0  0  0  0  \"  )  ;", "nm . init ( conf )  ;", "NodeStatusUpdaterImpl   nodeStatusUpdater    =     (  ( NodeStatusUpdaterImpl )     ( nm . ge (  )  )  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     0  )  ;", "ContainerId   cId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "nodeStatusUpdater . addCompletedContainer ( cId )  ;", "Assert . assertTrue ( nodeStatusUpdater . isContainerRecentlyStopped ( cId )  )  ;", "long   time 1     =    System . currentTimeMillis (  )  ;", "int   waitInterval    =     1  5  ;", "while    (  (  ( waitInterval -  -  )     >     0  )     &  &     ( nodeStatusUpdater . isContainerRecentlyStopped ( cId )  )  )     {", "nodeStatusUpdater . removeVeryOldStoppedContainersFromCache (  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "long   time 2     =    System . currentTimeMillis (  )  ;", "Assert . assertFalse ( nodeStatusUpdater . isContainerRecentlyStopped ( cId )  )  ;", "Assert . assertTrue (  (  (  ( time 2     -    time 1  )     >  =     1  0  0  0  0  )     &  &     (  ( time 2     -    time 1  )     <  =     2  5  0  0  0  0  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRecentlyFinishedContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "final   AtomicInteger   numCleanups    =    new   AtomicInteger (  0  )  ;", "nm    =    new   NodeManager (  )     {", "@ Override", "protected   NodeStatusUpdater   createNodeStatusUpdater ( Context   context ,    Dispatcher   dispatcher ,    NodeHealthCheckerService   healthChecker )     {", ". MyNodeStatusUpdater   myNodeStatusUpdater    =    new    . MyNodeStatusUpdater ( context ,    dispatcher ,    healthChecker ,    metrics )  ;", ". MyResourceTracker 2    myResourceTracker 2     =    new    . MyResourceTracker 2  (  )  ;", "myResourceTracker 2  . heartBeatNodeAction    =    NodeAction . SHUTDOWN ;", "myNodeStatusUpdater . resourceTracker    =    myResourceTracker 2  ;", "return   myNodeStatusUpdater ;", "}", "@ Override", "protected   ContainerManagerImpl   createContainerManager ( Context   context ,    ContainerExecutor   exec ,    DeletionService   del ,    NodeStatusUpdater   nodeStatusUpdater ,    ApplicationACLsManager   aclsManager ,    LocalDirsHandlerService   dirsHandler )     {", "return   new   ContainerManagerImpl ( context ,    exec ,    del ,    nodeStatusUpdater ,    metrics ,    aclsManager ,    dirsHandler )     {", "@ Override", "public   void   cleanUpApplicationsOnNMShutDown (  )     {", "super . cleanUpApplicationsOnNMShutDown (  )  ;", "numCleanups . incrementAndGet (  )  ;", "}", "}  ;", "}", "}  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "nm . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( heartBeatID )     <     1  )     &  &     (  ( waitCount +  +  )     !  =     2  0  0  )  )     {", "Thread . sleep (  5  0  0  )  ;", "}", "Assert . assertFalse (  (  ( heartBeatID )     <     1  )  )  ;", "nm . stop (  )  ;", "waitCount    =     0  ;", "while    (  (  ( nm . getServiceState (  )  )     !  =     ( STATE . STOPPED )  )     &  &     (  ( waitCount +  +  )     !  =     2  0  )  )     {", ". LOG . info (  \" Waiting   for   NM   to   stop .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertEquals ( STOPPED ,    nm . getServiceState (  )  )  ;", "Assert . assertEquals ( numCleanups . get (  )  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["testStopReentrant"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "Assert . assertNotNull (  \" nm   is   null \"  ,    nm )  ;", "YarnConfiguration   conf    =    createNMConfig (  )  ;", "nm . init ( conf )  ;", "try    {", "nm . start (  )  ;", "Assert . fail (  \" NM   should   have   failed   to   start .    Didn ' t   get   exception !  !  \"  )  ;", "}    catch    ( Exception   e )     {", "if    (  !  ( e . getMessage (  )  . contains ( errMessage )  )  )     {", "throw   e ;", "}", "}", "Assert . assertEquals (  \" NM   state   is   wrong !  \"  ,    STOPPED ,    nm . getService (  )  )  ;", "Assert . assertEquals (  \" Number   of   registered   nodes   is   wrong !  \"  ,     0  ,    this . registeredNodes . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeStartFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestNodeStatusUpdater"}, {"methodBody": ["METHOD_START", "{", "testPbServerFactory (  )  ;", "testPbClientFactory (  )  ;", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestRPCFactories"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    new   InetSocketAddress (  0  )  ;", "System . err . println (  (  ( addr . getHostName (  )  )     +     ( addr . getPort (  )  )  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "LocalizationProtocol   instance    =    new    . LocalizationProtocolTestImpl (  )  ;", "Server   server    =    null ;", "try    {", "server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( LocalizationProtocol . class ,    instance ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "System . err . println ( server . getListenerAddress (  )  )  ;", "System . err . println ( NetUtils . getConnectAddress ( server )  )  ;", "try    {", "LocalizationProtocol   client    =     (  ( LocalizationProtocol )     ( RpcClientFactoryPBImpl . get (  )  . getClient ( LocalizationProtocol . class ,     1  ,    NetUtils . getConnectAddress ( server )  ,    conf )  )  )  ;", "Assert . assertNotNull ( client )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   client \"  )  ;", "}", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   server \"  )  ;", "}    finally    {", "server . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPbClientFactory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestRPCFactories"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    new   InetSocketAddress (  0  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "LocalizationProtocol   instance    =    new    . LocalizationProtocolTestImpl (  )  ;", "Server   server    =    null ;", "try    {", "server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( LocalizationProtocol . class ,    instance ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   server \"  )  ;", "}    finally    {", "if    ( server    !  =    null )     {", "server . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testPbServerFactory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestRPCFactories"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   pbRecordFactory    =    RecordFactoryPBImpl . get (  )  ;", "try    {", "LocalizerHeartbeatResponse   response    =    pbRecordFactory . newRecordInstance ( LocalizerHeartbeatResponse . class )  ;", "Assert . assertEquals ( LocalizerHeartbeatResponsePBImpl . class ,    response . getClass (  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   crete   record \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPbRecordFactory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.TestRecordFactory"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( builder )     =  =    null )     |  |     ( viaProto )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.ResourceLocalizationSpecPBImpl"}, {"methodBody": ["METHOD_START", "{", "ResourceLocalizationSpecProtoOrBuilder   l    =     ( viaProto )     ?    proto    :    builder ;", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  ( l . getResource (  )  . equals (  (  ( LocalResourcePBImpl )     ( resource )  )  . getProto (  )  )  )  )  )     {", "maybeInitBuilder (  )  ;", "builder . setResource (  (  ( LocalResourcePBImpl )     ( resource )  )  . getProto (  )  )  ;", "}", "if    (  (  ( this . destinationDirectory )     !  =    null )     &  &     (  !  ( l . getDestinationDirectory (  )  . equals (  (  ( URLPBImpl )     ( destinationDirectory )  )  . getProto (  )  )  )  )  )     {", "maybeInitBuilder (  )  ;", "builder . setDestinationDirectory (  (  ( URLPBImpl )     ( destinationDirectory )  )  . getProto (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.ResourceLocalizationSpecPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   LocalResourcePBImpl ( rsrc )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   SerializedExceptionPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   URLPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ResourceStatusType . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( LocalResourcePBImpl )     ( rsrc )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( SerializedExceptionPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( URLPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ResourceStatusTypeProto . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  (  (  ( LocalResourcePBImpl )     ( this . resource )  )  . getProto (  )  . equals ( builder . getResource (  )  )  )  )  )     {", "builder . setResource ( convertToProtoFormat ( this . resource )  )  ;", "}", "if    (  (  ( this . localPath )     !  =    null )     &  &     (  !  (  (  ( URLPBImpl )     ( this . localPath )  )  . getProto (  )  . equals ( builder . getLocalPath (  )  )  )  )  )     {", "builder . setLocalPath ( convertToProtoFormat ( this . localPath )  )  ;", "}", "if    (  (  ( this . exception )     !  =    null )     &  &     (  !  (  (  ( SerializedExceptionPBImpl )     ( this . exception )  )  . getProto (  )  . equals ( builder . getException (  )  )  )  )  )     {", "builder . setException ( convertToProtoFormat ( this . exception )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearResources (  )  ;", "if    (  ( this . resourceSpecs )     =  =    null )", "return ;", "Iterable < ResourceationSpecProto >    iterable    =    new   Iterable < ResourceationSpecProto >  (  )     {", "@ Override", "public   Iterator < ResourceationSpecProto >    iterator (  )     {", "return   new   Iterator < ResourceationSpecProto >  (  )     {", "Iterator < ResourceationSpec >    iter    =    resourceSpecs . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ResourceationSpecProto   next (  )     {", "ResourceationSpec   resource    =    iter . next (  )  ;", "return    (  ( ResourceationSpecPBImpl )     ( resource )  )  . getProto (  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllResources ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addResourcesToProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   LocalizerAction . valueOf ( a . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceLocalizationSpecPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   LocalizerActionProto . valueOf ( a . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . resourceSpecs )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ResourceLocalizationSpecProto >    list    =    p . getResourcesList (  )  ;", "this . resourceSpecs    =    new   ArrayList < ResourceLocalizationSpec >  (  )  ;", "for    ( ResourceLocalizationSpecProto   c    :    list )     {", "this . resourceSpecs . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceSpecs )     !  =    null )     {", "addResourcesToP (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "if    ( action    =  =    null )     {", "builder . clearAction (  )  ;", "return ;", "}", "builder . setAction ( convertToPFormat ( action )  )  ;", "}", "METHOD_END"], "methodName": ["setLocalizerAction"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuild )  ;", "if    ( rsrcs    =  =    null )     {", "buildclearResources (  )  ;", "return ;", "}", "this . resourceSpecs    =    rsrcs ;", "}", "METHOD_END"], "methodName": ["setResourceSpecs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerHeartbeatResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearResources (  )  ;", "if    (  ( this . resources )     =  =    null )", "return ;", "Iterable < LocalResourceroto >    iterable    =    new   Iterable < LocalResourceroto >  (  )     {", "@ Override", "public   Iterator < LocalResourceroto >    iterator (  )     {", "return   new   Iterator < LocalResourceroto >  (  )     {", "Iterator < LocalResourceStatus >    iter    =    resources . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   LocalResourceroto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllResources ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addResourcesToProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   LocalResourceStatusPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( LocalResourceStatusPBImpl )     ( s )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . resources )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < LocalResourceStatusProto >    list    =    p . getResourcesList (  )  ;", "this . resources    =    new   ArrayList < LocalResourceStatus >  (  )  ;", "for    ( LocalResourceStatusProto   c    :    list )     {", "this . resources . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . resources )     !  =    null )     {", "addResourcesToP (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalizerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "return   RFactoryProvider . getRFactory ( conf )  ;", "}", "METHOD_END"], "methodName": ["createPBRecordFactory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC"}, {"methodBody": ["METHOD_START", "{", "LocalizerHeartbeatResponse   response    =    TestPBLocalizerRPC . recordFactory . newRecordInstance ( LocalizerHeartbeatResponse . class )  ;", "response . setLocalizerAction ( LocalizerAction . DIE )  ;", "return   response ;", "}", "METHOD_END"], "methodName": ["dieHBResponse"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   locAddr    =    new   InetSocketAddress (  \"  0  .  0  .  0  .  0  \"  ,     8  0  4  0  )  ;", ". LocalizerService   server    =    new    . LocalizerService ( locAddr )  ;", "try    {", "server . start (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "LocalizationProtocol   client    =     (  ( LocalizationProtocol )     ( rpc . getProxy ( LocalizationProtocol . class ,    locAddr ,    conf )  )  )  ;", "LocalizerStatus   status    =     . recordFactory . newRecordInstance ( LocalizerStatus . class )  ;", "status . setLocalizerId (  \" localizer 0  \"  )  ;", "LocalizerHeartbeatResponse   response    =    client . heartbeat ( status )  ;", "assertEquals (  . dieHBResponse (  )  ,    response )  ;", "}    finally    {", "server . stop (  )  ;", "}", "assertTrue ( true )  ;", "}", "METHOD_END"], "methodName": ["testLocalizerRPC"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBLocalizerRPC"}, {"methodBody": ["METHOD_START", "{", "LocalResourceStatus   ret    =    TestPBRecordImpl . recordFactory . newRecordInstance ( LocalResourceStatus . class )  ;", "assertTrue (  ( ret   instanceof   LocalResourceStatusPBImpl )  )  ;", "ret . setResource ( TestPBRecordImpl . createResource (  )  )  ;", "ret . setLocalPath ( ConverterUtils . getYarnUrlFromPath ( new   Path (  \" file :  /  /  / local / foo / bar \"  )  )  )  ;", "ret . setStatus ( ResourceStatusType . FETCH _ SUCCESS )  ;", "ret . setLocalSize (  4  4  4  3 L )  ;", "Exception   e    =    new   Exception (  \" Dingos .  \"  )  ;", "e . setStackTrace ( new   StackTraceElement [  ]  {    new   StackTraceElement (  \" foo \"  ,     \" bar \"  ,     \" baz \"  ,     1  0  )  ,    new   StackTraceElement (  \" sbb \"  ,     \" one \"  ,     \" onm \"  ,     1  0  )     }  )  ;", "ret . setException ( SerializedException . newInstance ( e )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createLocalResourceStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "LocalizerHeartbeatResponse   ret    =    TestPBRecordImpl . recordFactory . newRecordInstance ( LocalizerHeartbeatResponse . class )  ;", "assertTrue (  ( ret   instanceof   LocalizerHeartbeatResponsePBImpl )  )  ;", "ret . setLocalizerAction ( LocalizerAction . LIVE )  ;", "LocalResource   rsrc    =    TestPBRecordImpl . createResource (  )  ;", "ArrayList < ResourceLocalizationSpec >    rsrcs    =    new   ArrayList < ResourceLocalizationSpec >  (  )  ;", "ResourceLocalizationSpec   resource    =    TestPBRecordImpl . recordFactory . newRecordInstance ( ResourceLocalizationSpec . class )  ;", "resource . setResource ( rsrc )  ;", "resource . setDestinationDirectory ( ConverterUtils . getYarnUrlFromPath ( new   Path (  (  \"  / tmp \"     +     ( System . currentTimeMillis (  )  )  )  )  )  )  ;", "rsrcs . add ( resource )  ;", "ret . setResourceSpecs ( rsrcs )  ;", "System . out . println ( resource )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createLocalizerHeartbeatResponse"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "LocalizerStatus   ret    =    TestPBRecordImpl . recordFactory . newRecordInstance ( LocalizerStatus . class )  ;", "assertTrue (  ( ret   instanceof   LocalizerStatusPBImpl )  )  ;", "ret . setLocalizerId (  \" localizer 0  \"  )  ;", "ret . addResourceStatus ( TestPBRecordImpl . createLocalResourceStatus (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createLocalizerStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "return   FactoryProvider . getFactory ( conf )  ;", "}", "METHOD_END"], "methodName": ["createPBRecordFactory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "LocalResource   ret    =    TestPBRecordImpl . recordFactory . newRecordInstance ( LocalResource . class )  ;", "assertTrue (  ( ret   instanceof   LocalResourcePBImpl )  )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( new   Path (  \" hdfs :  /  / y . ak :  8  0  2  0  / foo / bar \"  )  )  )  ;", "ret . setSize (  4  3  4  4 L )  ;", "ret . setTimestamp (  3  1  4  1  5  9  2  6  5  3  5  8  9  7  9  3 L )  ;", "ret . setVisibility ( PUBLIC )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "LocalResourceStatus   rsrcS    =    TestPBRecordImpl . createLocalResourceStatus (  )  ;", "assertTrue (  ( rsrcS   instanceof   LocalResourceStatusPBImpl )  )  ;", "LocalResourceStatusPBImpl   rsrcPb    =     (  ( LocalResourceStatusPBImpl )     ( rsrcS )  )  ;", "DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "rsrcPb . getProto (  )  . writeDelimitedTo ( out )  ;", "DataInputBuffer   in    =    new   DataInputBuffer (  )  ;", "in . reset ( out . getData (  )  ,     0  ,    out . getLength (  )  )  ;", "LocalResourceStatusProto   rsrcPbD    =    LocalResourceStatusProto . parseDelimitedFrom ( in )  ;", "assertNotNull ( rsrcPbD )  ;", "LocalResourceStatus   rsrcD    =    new   LocalResourceStatusPBImpl ( rsrcPbD )  ;", "assertEquals ( rsrcS ,    rsrcD )  ;", "assertEquals ( TestPBRecordImpl . createResource (  )  ,    rsrcS . getResource (  )  )  ;", "assertEquals ( TestPBRecordImpl . createResource (  )  ,    rsrcD . getResource (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalResourceStatusSerDe"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "LocalizerHeartbeatResponse   rsrcS    =    TestPBRecordImpl . createLocalizerHeartbeatResponse (  )  ;", "assertTrue (  ( rsrcS   instanceof   LocalizerHeartbeatResponsePBImpl )  )  ;", "LocalizerHeartbeatResponsePBImpl   rsrcPb    =     (  ( LocalizerHeartbeatResponsePBImpl )     ( rsrcS )  )  ;", "DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "rsrcPb . getProto (  )  . writeDelimitedTo ( out )  ;", "DataInputBuffer   in    =    new   DataInputBuffer (  )  ;", "in . reset ( out . getData (  )  ,     0  ,    out . getLength (  )  )  ;", "LocalizerHeartbeatResponseProto   rsrcPbD    =    LocalizerHeartbeatResponseProto . parseDelimitedFrom ( in )  ;", "assertNotNull ( rsrcPbD )  ;", "LocalizerHeartbeatResponse   rsrcD    =    new   LocalizerHeartbeatResponsePBImpl ( rsrcPbD )  ;", "assertEquals ( rsrcS ,    rsrcD )  ;", "assertEquals ( TestPBRecordImpl . createResource (  )  ,    rsrcS . getResourceSpecs (  )  . get (  0  )  . getResource (  )  )  ;", "assertEquals ( TestPBRecordImpl . createResource (  )  ,    rsrcD . getResourceSpecs (  )  . get (  0  )  . getResource (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalizerHeartbeatResponseSerDe"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "LocalizerStatus   rsrcS    =    TestPBRecordImpl . createLocalizerStatus (  )  ;", "assertTrue (  ( rsrcS   instanceof   LocalizerStatusPBImpl )  )  ;", "LocalizerStatusPBImpl   rsrcPb    =     (  ( LocalizerStatusPBImpl )     ( rsrcS )  )  ;", "DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "rsrcPb . getProto (  )  . writeDelimitedTo ( out )  ;", "DataInputBuffer   in    =    new   DataInputBuffer (  )  ;", "in . reset ( out . getData (  )  ,     0  ,    out . getLength (  )  )  ;", "LocalizerStatusProto   rsrcPbD    =    LocalizerStatusProto . parseDelimitedFrom ( in )  ;", "assertNotNull ( rsrcPbD )  ;", "LocalizerStatus   rsrcD    =    new   LocalizerStatusPBImpl ( rsrcPbD )  ;", "assertEquals ( rsrcS ,    rsrcD )  ;", "assertEquals (  \" localizer 0  \"  ,    rsrcS . getLocalizerId (  )  )  ;", "assertEquals (  \" localizer 0  \"  ,    rsrcD . getLocalizerId (  )  )  ;", "assertEquals ( TestPBRecordImpl . createLocalResourceStatus (  )  ,    rsrcS . getResourceStatus (  0  )  )  ;", "assertEquals ( TestPBRecordImpl . createLocalResourceStatus (  )  ,    rsrcD . getResourceStatus (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalizerStatusSerDe"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "YarnException   yarnEx    =    new   YarnException (  \" Yarn _ Exception \"  )  ;", "SerializedException   serEx    =    SerializedException . newInstance ( yarnEx )  ;", "Throwable   throwable    =    serEx . deSerialize (  )  ;", "Assert . assertEquals ( yarnEx . getClass (  )  ,    throwable . getClass (  )  )  ;", "Assert . assertEquals ( yarnEx . getMessage (  )  ,    throwable . getMessage (  )  )  ;", "IOException   ioe    =    new   IOException (  \" Test _ IOException \"  )  ;", "RuntimeException   runtimeException    =    new   RuntimeException (  \" Test _ RuntimeException \"  ,    ioe )  ;", "YarnException   yarnEx 2     =    new   YarnException (  \" Test _ YarnException \"  ,    runtimeException )  ;", "SerializedException   serEx 2     =    SerializedException . newInstance ( yarnEx 2  )  ;", "Throwable   throwable 2     =    serEx 2  . deSerialize (  )  ;", "throwable 2  . printStackTrace (  )  ;", "Assert . assertEquals ( yarnEx 2  . getClass (  )  ,    throwable 2  . getClass (  )  )  ;", "Assert . assertEquals ( yarnEx 2  . getMessage (  )  ,    throwable 2  . getMessage (  )  )  ;", "Assert . assertEquals ( runtimeException . getClass (  )  ,    throwable 2  . getCause (  )  . getClass (  )  )  ;", "Assert . assertEquals ( runtimeException . getMessage (  )  ,    throwable 2  . getCause (  )  . getMessage (  )  )  ;", "Assert . assertEquals ( ioe . getClass (  )  ,    throwable 2  . getCause (  )  . getCause (  )  . getClass (  )  )  ;", "Assert . assertEquals ( ioe . getMessage (  )  ,    throwable 2  . getCause (  )  . getCause (  )  . getMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSerializedExceptionDeSer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.TestPBRecordImpl"}, {"methodBody": ["METHOD_START", "{", "AuxServices . LOG . info (  (  (  (  (  \" Adding   auxiliary   service    \"     +     ( service . getName (  )  )  )     +     \"  ,     \\  \"  \"  )     +    name )     +     \"  \\  \"  \"  )  )  ;", "serviceMap . put ( name ,    service )  ;", "}", "METHOD_END"], "methodName": ["addService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    ByteBuffer >    metaClone    =    new   HashMap < String ,    ByteBuffer >  ( serviceMetaData . size (  )  )  ;", "synchronized ( serviceMetaData )     {", "for    ( Map . Entry < String ,    ByteBuffer >    entry    :    serviceMetaData . entrySet (  )  )     {", "metaClone . put ( entry . getKey (  )  ,    entry . getValue (  )  . duplicate (  )  )  ;", "}", "}", "return   metaClone ;", "}", "METHOD_END"], "methodName": ["getMetaData"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices"}, {"methodBody": ["METHOD_START", "{", "return   Collections . unmodifiableCollection ( serviceMap . values (  )  )  ;", "}", "METHOD_END"], "methodName": ["getServices"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices"}, {"methodBody": ["METHOD_START", "{", "AuxServices . LOG . warn (  (  (  ( null    =  =    service    ?     \" The   auxService   is   null \"     :     \" The   auxService   name   is    \"     +     ( service . getName (  )  )  )     +     \"    and   it   got   an   error   at   event :     \"  )     +    eventType )  ,    th )  ;", "}", "METHOD_END"], "methodName": ["logWarningWhenAuxServiceThrowExceptions"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( name    =  =    null )     |  |     ( name . trim (  )  . isEmpty (  )  )  )     {", "return   false ;", "}", "return   p . matr ( name )  . mats (  )  ;", "}", "METHOD_END"], "methodName": ["validateAuxServiceName"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices"}, {"methodBody": ["METHOD_START", "{", "return   appId ;", "}", "METHOD_END"], "methodName": ["getApplicationID"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent"}, {"methodBody": ["METHOD_START", "{", "return   serviceData ;", "}", "METHOD_END"], "methodName": ["getServiceData"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent"}, {"methodBody": ["METHOD_START", "{", "return   serviceId ;", "}", "METHOD_END"], "methodName": ["getServiceID"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEvent"}, {"methodBody": ["METHOD_START", "{", "DefaultContainerExecutor   exec    =    new   DefaultContainerExecutor (  )  ;", "exec . setConf ( conf )  ;", "return   exec ;", "}", "METHOD_END"], "methodName": ["createContainerExecutor"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerManagerImpl ( context ,    exec ,    delSrvc ,    nodeStatusUpdater ,    metrics ,    new   ApplicationACLsManager ( conf )  ,    dirsHandler )     {", "@ Override", "public   void   setBlockNewContainerRequests ( boolean   blockNewContainerRequests )     {", "}", "@ Override", "protected   void   authorizeGetAndStopContainerRequest ( ContainerId   containerId ,    Container   container ,    boolean   stopRequest ,    NMTokenIdentifier   identifier )    throws   YarnException    {", "}", "@ Override", "protected   void   authorizeUser ( UserGroupInformation   remoteUgi ,    NMTokenIdentifier   nmTokenIdentifier )     {", "}", "@ Override", "protected   void   authorizeStartRequest ( NMTokenIdentifier   nmTokenIdentifier ,    ContainerTokenIdentifier   containerTokenIdentifier )    throws   YarnException    {", "}", "@ Override", "protected   void   updateNMTokenIdentifier ( NMTokenIdentifier   nmTokenIdentifier )    throws   InvalidToken    {", "}", "@ Override", "public   Map < String ,    ByteBuffer >    getAuxServiceMetaData (  )     {", "Map < String ,    ByteBuffer >    serviceData    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "serviceData . put (  \" AuxService 1  \"  ,    ByteBuffer . wrap (  \" AuxServiceMetaData 1  \"  . getBytes (  )  )  )  ;", "serviceData . put (  \" AuxService 2  \"  ,    ByteBuffer . wrap (  \" AuxServiceMetaData 2  \"  . getBytes (  )  )  )  ;", "return   serviceData ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createContainerManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "return   new   DeletionService ( exec )     {", "@ Override", "public   void   delete ( String   user ,    Path   subDir ,    Path .  .  .    baseDirs )     {", ". LOG . info (  (  (  (  (  (  \" Psuedo   delete :    user    -     \"     +    user )     +     \"  ,    subDir    -     \"  )     +    subDir )     +     \"  ,    baseDirs    -     \"  )     +    baseDirs )  )  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createDeletionService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "BaseContainerManagerTest . localFS . delete ( new   Path ( BaseContainerManagerTest . localDir . getAbsolutePath (  )  )  ,    true )  ;", "BaseContainerManagerTest . localFS . delete ( new   Path ( BaseContainerManagerTest . tmpDir . getAbsolutePath (  )  )  ,    true )  ;", "BaseContainerManagerTest . localFS . delete ( new   Path ( BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ,    true )  ;", "BaseContainerManagerTest . localFS . delete ( new   Path ( BaseContainerManagerTest . remoteLogDir . getAbsolutePath (  )  )  ,    true )  ;", "BaseContainerManagerTest . localDir . mkdir (  )  ;", "BaseContainerManagerTest . tmpDir . mkdir (  )  ;", "BaseContainerManagerTest . localLogDir . mkdir (  )  ;", "BaseContainerManagerTest . remoteLogDir . mkdir (  )  ;", "BaseContainerManagerTest . LOG . info (  (  \" Created   localDir   in    \"     +     ( BaseContainerManagerTest . localDir . getAbsolutePath (  )  )  )  )  ;", "BaseContainerManagerTest . LOG . info (  (  \" Created   tmpDir   in    \"     +     ( BaseContainerManagerTest . tmpDir . getAbsolutePath (  )  )  )  )  ;", "String   bindAddress    =     \"  0  .  0  .  0  .  0  :  1  2  3  4  5  \"  ;", "conf . set ( NM _ ADDRESS ,    bindAddress )  ;", "conf . set ( NM _ LOCAL _ DIRS ,    BaseContainerManagerTest . localDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    BaseContainerManagerTest . remoteLogDir . getAbsolutePath (  )  )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,     1  )  ;", "delSrvc    =    createDeletionService (  )  ;", "delSrvc . init ( conf )  ;", "exec    =    createContainerExecutor (  )  ;", "nodeHealthChecker    =    new   NodeHealthCheckerService (  )  ;", "nodeHealthChecker . init ( conf )  ;", "dirsHandler    =    nodeHealthChecker . getDiskHandler (  )  ;", "containerManager    =    createContainerManager ( delSrvc )  ;", "(  ( NodeManager . NMContext )     ( context )  )  . setContainerManager ( containerManager )  ;", "nodeStatusUpdater . init ( conf )  ;", "containerManager . init ( conf )  ;", "nodeStatusUpdater . start (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "if    (  ( containerManager )     !  =    null )     {", "containerManager . stop (  )  ;", "}", "createContainerExecutor (  )  . deleteAsUser ( user ,    new   Path (  . localDir . getAbsolutePath (  )  )  ,    new   Path [  ]  {        }  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "Application   app    =    containerManager . getContext (  )  . getApplications (  )  . get ( appID )  ;", "int   timeout    =     0  ;", "while    (  (  !  ( app . getApplicationState (  )  . equals ( finalState )  )  )     &  &     (  ( timeout +  +  )     <     1  5  )  )     {", ". LOG . info (  (  (  (  \" Waiting   for   app   to   reach    \"     +    finalState )     +     \"  .  .    Current   state   is    \"  )     +     ( app . getApplicationState (  )  )  )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertTrue (  (  (  \" App   is   not   in    \"     +    finalState )     +     \"    yet !  !    Timedout !  !  \"  )  ,    app . getApplicationState (  )  . equals ( finalState )  )  ;", "}", "METHOD_END"], "methodName": ["waitForApplicationState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    containerID ,    finalState ,     2  0  )  ;", "}", "METHOD_END"], "methodName": ["waitForContainerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "List < ContainerId >    list    =    new   ArrayList < ContainerId >  (  )  ;", "list . add ( containerID )  ;", "GetContainerStatusesRequest   request    =    GetContainerStatusesRequest . newInstance ( list )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( request )  . getContainerStatuses (  )  . get (  0  )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( containerStatus . getState (  )  . equals ( finalState )  )  )     &  &     (  ( timeoutSecs +  +  )     <    timeOutMax )  )     {", "Thread . sleep (  1  0  0  0  )  ;", ". LOG . info (  (  (  (  \" Waiting   for   container   to   get   into   state    \"     +    finalState )     +     \"  .    Current   state   is    \"  )     +     ( containerStatus . getState (  )  )  )  )  ;", "containerStatus    =    containerManager . getContainerStatuses ( request )  . getContainerStatuses (  )  . get (  0  )  ;", "}", ". LOG . info (  (  \" Container   state   is    \"     +     ( containerStatus . getState (  )  )  )  )  ;", "Assert . assertEquals (  \" ContainerState   is   not   correct    ( timedout )  \"  ,    finalState ,    containerStatus . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForContainerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.BaseContainerManagerTest"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   nmTokenAppId    =    identifier . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "if    (  (  !  ( nmTokenAppId . equals ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  )  )     |  |     (  ( container    !  =    null )     &  &     (  !  ( nmTokenAppId . equals ( container . getContainerId (  )  . getApplicationAttemptId (  )  . getApplicationId (  )  )  )  )  )  )     {", "if    ( stopRequest )     {", ". LOG . warn (  (  (  ( identifier . getApplicationAttemptId (  )  )     +     \"    attempted   to   stop   non - application   container    :     \"  )     +     ( container . getContainerId (  )  )  )  )  ;", "NMAuditLogger . logFailure (  \" UnknownUser \"  ,    NMAuditLogger . AuditConstants . STOP _ CONTAINER ,     \"  \"  ,     \" Trying   to   stop   unknown   container !  \"  ,    nmTokenAppId ,    container . getContainerId (  )  )  ;", "} else    {", ". LOG . warn (  (  (  ( identifier . getApplicationAttemptId (  )  )     +     \"    attempted   to   get   status   for   non - application   container    :     \"  )     +     ( container . getContainerId (  )  )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["authorizeGetAndStopContainerRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    containerTokenIdentifier . getContainerID (  )  ;", "String   containerIDStr    =    containerId . toString (  )  ;", "boolean   unauthorized    =    false ;", "StringBuilder   messageBuilder    =    new   StringBuilder (  \" Unauthorized   request   to   start   container .     \"  )  ;", "if    (  !  ( nmTokenIdentifier . getApplicationAttemptId (  )  . getApplicationId (  )  . equals ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  )  )     {", "unauthorized    =    true ;", "messageBuilder . append (  \"  \\ nNMToken   for   application   attempt    :     \"  )  . append ( nmTokenIdentifier . getApplicationAttemptId (  )  )  . append (  \"    was   used   for   starting   container   with   container   token \"  )  . append (  \"    issued   for   application   attempt    :     \"  )  . append ( containerId . getApplicationAttemptId (  )  )  ;", "} else", "if    (  !  ( this . context . getContainerTokenSecretManager (  )  . isValidStartContainerRequest ( containerTokenIdentifier )  )  )     {", "unauthorized    =    true ;", "messageBuilder . append (  \"  \\ n   Attempt   to   relaunch   the   same    \"  )  . append (  \" container   with   id    \"  )  . append ( containerIDStr )  . append (  \"  .  \"  )  ;", "} else", "if    (  ( containerTokenIdentifier . getExpiryTimeStamp (  )  )     <     ( System . currentTimeMillis (  )  )  )     {", "unauthorized    =    true ;", "messageBuilder . append (  \"  \\ nThis   token   is   expired .    current   time   is    \"  )  . append ( System . currentTimeMillis (  )  )  . append (  \"    found    \"  )  . append ( containerTokenIdentifier . getExpiryTimeStamp (  )  )  ;", "messageBuilder . append (  \"  \\ nNote :    System   times   on   machines   may   be   out   of   sync .  \"  )  . append (  \"    Check   system   time   and   time   zones .  \"  )  ;", "}", "if    ( unauthorized )     {", "String   msg    =    messageBuilder . toString (  )  ;", ". LOG . error ( msg )  ;", "throw   RPCUtil . getRemoteException ( msg )  ;", "}", "}", "METHOD_END"], "methodName": ["authorizeStartRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( remoteUgi . getUserName (  )  . equals ( nmTokenIdentifier . getApplicationAttemptId (  )  . toString (  )  )  )  )     {", "throw   RPCUtil . getRemoteException (  (  (  (  \" Expected   applicationAttemptId :     \"     +     ( remoteUgi . getUserName (  )  )  )     +     \" Found :     \"  )     +     ( nmTokenIdentifier . getApplicationAttemptId (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["authorizeUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "ContainerManagerApplicationProto . Builder   builder    =    ContainerManagerApplicationProto . newBuilder (  )  ;", "builder . setId (  (  ( ApplicationIdPBImpl )     ( appId )  )  . getProto (  )  )  ;", "builder . setUser ( user )  ;", "builder . clearCredentials (  )  ;", "if    ( credentials    !  =    null )     {", "DataOutputBuffer   dob    =    new   DataOutputBuffer (  )  ;", "try    {", "credentials . writeTokenStorageToStream ( dob )  ;", "builder . setCredentials ( ByteString . copyFrom ( dob . getData (  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Cannot   serialize   credentials \"  ,    e )  ;", "}", "}", "builder . clearAcls (  )  ;", "if    ( appAcls    !  =    null )     {", "for    ( Map . Entry < ApplicationAccessType ,    String >    acl    :    appAcls . entrySet (  )  )     {", "ApplicationACLMapProto   p    =    ApplicationACLMapProto . newBuilder (  )  . setAccessType ( ProtoUtils . convertToProtoFormat ( acl . getKey (  )  )  )  . setAcl ( acl . getValue (  )  )  . build (  )  ;", "builder . addAcls ( p )  ;", "}", "}", "return   builder . build (  )  ;", "}", "METHOD_END"], "methodName": ["buildAppProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( hostOverride    !  =    null )     {", "connectAddress    =    NetUtils . getConnectAddress ( new   InetSocketAddress ( hostOverride ,    connectAddress . getPort (  )  )  )  ;", "}", "return   NodeId . newInstance ( connectAddress . getAddress (  )  . getCanonicalHostName (  )  ,    connectAddress . getPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["buildNodeId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "Map < ApplicationId ,    Application >    applications    =    this . context . getApplications (  )  ;", "if    ( applications . isEmpty (  )  )     {", "return ;", "}", ". LOG . info (  (  \" Applications   still   running    :     \"     +     ( applications . keySet (  )  )  )  )  ;", "if    (  ( this . context . getNMStateStore (  )  . canRecover (  )  )     &  &     (  !  ( this . context . getDecommissioned (  )  )  )  )     {", "return ;", "}", "List < ApplicationId >    appIds    =    new   ArrayList < ApplicationId >  ( applications . keySet (  )  )  ;", "this . handle ( new   CMgrCompletedAppsEvent ( appIds ,    CMgrCompletedAppsEvent . Reason . ON _ SHUTDOWN )  )  ;", ". LOG . info (  \" Waiting   for   Applications   to   be   Finished \"  )  ;", "long   waitStartTime    =    System . currentTimeMillis (  )  ;", "while    (  (  !  ( applications . isEmpty (  )  )  )     &  &     (  (  ( System . currentTimeMillis (  )  )     -    waitStartTime )     <     ( waitForContainersOnShutdownMillis )  )  )     {", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", ". LOG . warn (  \" Interrupted   while   sleeping   on   applications   finish   on   shutdown \"  ,    ex )  ;", "}", "}", "if    ( applications . isEmpty (  )  )     {", ". LOG . info (  \" All   applications   in   FINISHED   state \"  )  ;", "} else    {", ". LOG . info (  (  \" Done   waiting   for   Applications   to   be   Finished .    Still   alive :     \"     +     ( applications . keySet (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanUpApplicationsOnNMShutDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "Map < ContainerId ,    Container >    containers    =    context . getContainers (  )  ;", "if    ( containers . isEmpty (  )  )     {", "return ;", "}", ". LOG . info (  (  (  (  \" Containers   still   running   on    \"     +     ( CMgrCompletedContainersEvent . Reason . ON _ NODEMANAGER _ RESYNC )  )     +     \"     :     \"  )     +     ( containers . keySet (  )  )  )  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  ( containers . keySet (  )  )  ;", ". LOG . info (  \" Waiting   for   containers   to   be   killed \"  )  ;", "this . handle ( new   CMgrCompletedContainersEvent ( containerIds ,    CMgrCompletedContainersEvent . Reason . ON _ NODEMANAGER _ RESYNC )  )  ;", "boolean   allContainersCompleted    =    false ;", "while    (  (  !  ( containers . isEmpty (  )  )  )     &  &     (  ! allContainersCompleted )  )     {", "allContainersCompleted    =    true ;", "for    ( Map . Entry < ContainerId ,    Container >    container    :    containers . entrySet (  )  )     {", "if    (  (  (  ( ContainerImpl )     ( container . getValue (  )  )  )  . getCurrentState (  )  )     !  =     ( ContainerState . COMPLETE )  )     {", "allContainersCompleted    =    false ;", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", ". LOG . warn (  \" Interrupted   while   sleeping   on   container   kill   on   resync \"  ,    ex )  ;", "}", "break ;", "}", "}", "}", "if    ( allContainersCompleted )     {", ". LOG . info (  \" All   containers   in   DONE   state \"  )  ;", "} else    {", ". LOG . info (  (  \" Done   waiting   for   containers   to   be   killed .    Still   alive :     \"     +     ( containers . keySet (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanupContainersOnNMResync"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainersLauncher ( context ,    this . dispatcher ,    exec ,    dirsHandler ,    this )  ;", "}", "METHOD_END"], "methodName": ["createContainersLauncher"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( conf . getBoolean ( LOG _ AGGREGATION _ ENABLED ,    DEFAULT _ LOG _ AGGREGATION _ ENABLED )  )     {", "return   new   LogAggregationService ( this . dispatcher ,    ext ,    deletionService ,    dirsHandler )  ;", "} else    {", "return   new   NonAggregatingLogHandler ( this . dispatcher ,    deletionService ,    dirsHandler )  ;", "}", "}", "METHOD_END"], "methodName": ["createLogHandler"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceLocalizationService ( this . dispatcher ,    exec ,    deletionContext ,    dirsHandler ,    context . getNMStateStore (  )  )  ;", "}", "METHOD_END"], "methodName": ["createResourceLocalizationService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . auxiliaryServices . getMetaData (  )  ;", "}", "METHOD_END"], "methodName": ["getAuxServiceMetaData"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . blockNewContainerRequests . get (  )  ;", "}", "METHOD_END"], "methodName": ["getBlockNewContainerRequestsStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "String   containerIDStr    =    containerID . toString (  )  ;", "Container   container    =    this . context . getContainers (  )  . get ( containerID )  ;", ". LOG . info (  (  \" Getting   container - status   for    \"     +    containerIDStr )  )  ;", "authorizeGetAndStopContainerRequest ( containerID ,    container ,    false ,    nmTokenIdentifier )  ;", "if    ( container    =  =    null )     {", "if    ( nodeStatusUpdater . isContainerRecentlyStopped ( containerID )  )     {", "throw   RPCUtil . getRemoteException (  (  (  \" Container    \"     +    containerIDStr )     +     \"    was   recently   stopped   on   node   manager .  \"  )  )  ;", "} else    {", "throw   RPCUtil . getRemoteException (  (  (  \" Container    \"     +    containerIDStr )     +     \"    is   not   handled   by   this   NodeManager \"  )  )  ;", "}", "}", "ContainerStatus   containerStatus    =    container . cloneAndGetContainerStatus (  )  ;", ". LOG . info (  (  \" Returning    \"     +    containerStatus )  )  ;", "return   containerStatus ;", "}", "METHOD_END"], "methodName": ["getContainerStatusInternal"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . containersMonitor ;", "}", "METHOD_END"], "methodName": ["getContainersMonitor"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . context ;", "}", "METHOD_END"], "methodName": ["getContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   remoteUgi ;", "try    {", "remoteUgi    =    UserGroupInformation . getCurrentUser (  )  ;", "}    catch    ( IOException   e )     {", "String   msg    =     \" Cannot   obtain   the   user - name .    Got   exception :     \"     +     ( StringUtils . stringifyException ( e )  )  ;", ". LOG . warn ( msg )  ;", "throw   RPCUtil . getRemoteException ( msg )  ;", "}", "return   remoteUgi ;", "}", "METHOD_END"], "methodName": ["getRemoteUgi"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "Credentials   credentials    =    new   Credentials (  )  ;", "ByteBuffer   tokens    =    launchContext . getTokens (  )  ;", "if    ( tokens    !  =    null )     {", "DataInputByteBuffer   buf    =    new   DataInputByteBuffer (  )  ;", "tokens . rewind (  )  ;", "buf . reset ( tokens )  ;", "credentials . readTokenStorageStream ( buf )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", "for    ( Token <  ?    extends   TokenIdentifier >    tk    :    credentials . getAllTokens (  )  )     {", ". LOG . debug (  (  (  ( tk . getService (  )  )     +     \"     =     \"  )     +     ( tk . toString (  )  )  )  )  ;", "}", "}", "}", "return   credentials ;", "}", "METHOD_END"], "methodName": ["parseCredentials"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService   stateStore    =    context . getNMStateStore (  )  ;", "if    ( stateStore . canRecover (  )  )     {", "rsrcLocalizationSrvc . recoverLocalizedResources ( stateStore . loadLocalizationState (  )  )  ;", "NMStateStoreService . RecoveredApplicationsState   appsState    =    stateStore . loadApplicationsState (  )  ;", "for    ( ApplicationProto   proto    :    appsState . getApplications (  )  )     {", "recoverApplication ( proto )  ;", "}", "for    ( NMStateStoreService . RecoveredContainerState   rcs    :    stateStore . loadContainersState (  )  )     {", "recoverContainer ( rcs )  ;", "}", "String   diagnostic    =     \" Application   marked   finished   during   recovery \"  ;", "for    ( ApplicationId   appId    :    appsState . getFinishedApplications (  )  )     {", "dispatcher . getEventHandler (  )  . handle ( new   ApplicationFinishEvent ( appId ,    diagnostic )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["recover"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    new   ApplicationIdPBImpl ( p . getId (  )  )  ;", "Credentials   creds    =    new   Credentials (  )  ;", "creds . readTokenStorageStream ( new   DataInputStream ( p . getCredentials (  )  . newInput (  )  )  )  ;", "List < ApplicationACLMapProto >    aclProtoList    =    p . getAclsList (  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  ( aclProtoList . size (  )  )  ;", "for    ( ApplicationACLMapProto   aclProto    :    aclProtoList )     {", "acls . put ( ProtoUtils . convertFromProtoFormat ( aclProto . getAccessType (  )  )  ,    aclProto . getAcl (  )  )  ;", "}", "ContainerManagerImpl . LOG . info (  (  \" Recovering   application    \"     +    appId )  )  ;", "ApplicationImpl   app    =    new   ApplicationImpl ( dispatcher ,    p . getUser (  )  ,    appId ,    creds ,    context )  ;", "context . getApplications (  )  . put ( appId ,    app )  ;", "app . handle ( new   ApplicationInitEvent ( appId ,    acls )  )  ;", "}", "METHOD_END"], "methodName": ["recoverApplication"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "StartContainerRequest   req    =    rcs . getStartRequest (  )  ;", "ContainerLaunchContext   launchContext    =    req . getContainerLaunchContext (  )  ;", "ContainerTokenIdentifier   token    =    BuilderUtils . newContainerTokenIdentifier ( req . getContainerToken (  )  )  ;", "ContainerId   containerId    =    token . getContainerID (  )  ;", "ApplicationId   appId    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", ". LOG . info (  (  (  (  (  (  \" Recovering    \"     +    containerId )     +     \"    in   state    \"  )     +     ( rcs . getStatus (  )  )  )     +     \"    with   exit   code    \"  )     +     ( rcs . getExitCode (  )  )  )  )  ;", "if    ( context . getApplications (  )  . containsKey ( appId )  )     {", "Credentials   credentials    =    parseCredentials ( launchContext )  ;", "Container   container    =    new   ContainerImpl ( getConfig (  )  ,    dispatcher ,    context . getNMStateStore (  )  ,    req . getContainerLaunchContext (  )  ,    credentials ,    metrics ,    token ,    rcs . getStatus (  )  ,    rcs . getExitCode (  )  ,    rcs . getDiagnostics (  )  ,    rcs . getKilled (  )  )  ;", "context . getContainers (  )  . put ( containerId ,    container )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ApplicationContainerInitEvent ( container )  )  ;", "} else    {", "if    (  ( rcs . getStatus (  )  )     !  =     ( NMStateStoreService . RecoveredContainerStatus . COMPLETED )  )     {", ". LOG . warn (  ( containerId    +     \"    has   no   corresponding   application !  \"  )  )  ;", "}", ". LOG . info (  (  (  \" Adding    \"     +    containerId )     +     \"    to   recently   stopped   containers \"  )  )  ;", "nodeStatusUpdater . addCompletedContainer ( containerId )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "this . server . refreshServiceAcl ( configuration ,    policyProvider )  ;", "}", "METHOD_END"], "methodName": ["refreshServiceAcls"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "Set < TokenIdentifier >    tokenIdentifiers    =    remoteUgi . getTokenIdentifiers (  )  ;", "NMTokenIdentifier   resultId    =    null ;", "for    ( TokenIdentifier   id    :    tokenIdentifiers )     {", "if    ( id   instanceof   NMTokenIdentifier )     {", "resultId    =     (  ( NMTokenIdentifier )     ( id )  )  ;", "break ;", "}", "}", "return   resultId ;", "}", "METHOD_END"], "methodName": ["selectNMTokenIdentifier"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "this . blockNewContainerRequests . set ( blockNewContainerRequests )  ;", "}", "METHOD_END"], "methodName": ["setBlockNewContainerRequests"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "authorizeStartRequest ( nmTokenIdentifier ,    containerTokenIdentifier )  ;", "if    (  ( containerTokenIdentifier . getRMIdentifer (  )  )     !  =     ( nodeStatusUpdater . getRMIdentifier (  )  )  )     {", "StringBuilder   sb    =    new   StringBuilder (  \"  \\ nContainer    \"  )  ;", "sb . append ( containerTokenIdentifier . getContainerID (  )  . toString (  )  )  . append (  \"    rejected   as   it   is   allocated   by   a   previous   RM \"  )  ;", "throw   new   InvalidContainerException ( sb . toString (  )  )  ;", "}", "updateNMTokenIdentifier ( nmTokenIdentifier )  ;", "ContainerId   containerId    =    containerTokenIdentifier . getContainerID (  )  ;", "String   containerIdStr    =    containerId . toString (  )  ;", "String   user    =    containerTokenIdentifier . getApplicationSubmitter (  )  ;", ". LOG . info (  (  (  (  \" Start   request   for    \"     +    containerIdStr )     +     \"    by   user    \"  )     +    user )  )  ;", "ContainerLaunchContext   launchContext    =    request . getContainerLaunchContext (  )  ;", "Map < String ,    ByteBuffer >    serviceData    =    getAuxServiceMetaData (  )  ;", "if    (  (  ( launchContext . getServiceData (  )  )     !  =    null )     &  &     (  !  ( launchContext . getServiceData (  )  . isEmpty (  )  )  )  )     {", "for    ( Map . Entry < String ,    ByteBuffer >    meta    :    launchContext . getServiceData (  )  . entrySet (  )  )     {", "if    ( null    =  =     ( serviceData . get ( meta . getKey (  )  )  )  )     {", "throw   new   InvalidAuxServiceException (  (  (  \" The   auxService :  \"     +     ( meta . getKey (  )  )  )     +     \"    does   not   exist \"  )  )  ;", "}", "}", "}", "Credentials   credentials    =    parseCredentials ( launchContext )  ;", "Container   container    =    new   ContainerImpl ( getConfig (  )  ,    this . dispatcher ,    context . getNMStateStore (  )  ,    launchContext ,    credentials ,    metrics ,    containerTokenIdentifier )  ;", "ApplicationId   applicationID    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "if    (  ( context . getContainers (  )  . putIfAbsent ( containerId ,    container )  )     !  =    null )     {", "NMAuditLogger . logFailure ( user ,    NMAuditLogger . AuditConstants . START _ CONTAINER ,     \"  \"  ,     \" Container   already   running   on   this   node !  \"  ,    applicationID ,    containerId )  ;", "throw   RPCUtil . getRemoteException (  (  (  \" Container    \"     +    containerIdStr )     +     \"    already   is   running   on   this   node !  !  \"  )  )  ;", "}", "this . readLock . lock (  )  ;", "try    {", "if    (  !  ( serviceStopped )  )     {", "Application   application    =    new   ApplicationImpl ( dispatcher ,    user ,    applicationID ,    credentials ,    context )  ;", "if    ( null    =  =     ( context . getApplications (  )  . putIfAbsent ( applicationID ,    application )  )  )     {", ". LOG . info (  (  \" Creating   a   new   application   reference   for   app    \"     +    applicationID )  )  ;", "Map < ApplicationAccessType ,    String >    appAcls    =    container . getLaunchContext (  )  . getApplicationACLs (  )  ;", "context . getNMStateStore (  )  . storeApplication ( applicationID ,    buildAppProto ( applicationID ,    user ,    credentials ,    appAcls )  )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ApplicationInitEvent ( applicationID ,    appAcls )  )  ;", "}", "this . context . getNMStateStore (  )  . storeContainer ( containerId ,    request )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ApplicationContainerInitEvent ( container )  )  ;", "this . context . getContainerTokenSecretManager (  )  . startContainerSuccessful ( containerTokenIdentifier )  ;", "NMAuditLogger . logSuccess ( user ,    NMAuditLogger . AuditConstants . START _ CONTAINER ,     \" ContainerManageImpl \"  ,    applicationID ,    containerId )  ;", "metrics . launchedContainer (  )  ;", "metrics . allocateContainer ( containerTokenIdentifier . getResource (  )  )  ;", "} else    {", "throw   new   YarnException (  (  \" Container   start   failed   as   the   NodeManager   is    \"     +     \" in   the   process   of   shutting   down \"  )  )  ;", "}", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["startContainerInternal"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "String   containerIDStr    =    containerID . toString (  )  ;", "Container   container    =    this . context . getContainers (  )  . get ( containerID )  ;", ". LOG . info (  (  \" Stopping   container   with   container   Id :     \"     +    containerIDStr )  )  ;", "authorizeGetAndStopContainerRequest ( containerID ,    container ,    true ,    nmTokenIdentifier )  ;", "if    ( container    =  =    null )     {", "if    (  !  ( nodeStatusUpdater . isContainerRecentlyStopped ( containerID )  )  )     {", "throw   RPCUtil . getRemoteException (  (  (  \" Container    \"     +    containerIDStr )     +     \"    is   not   handled   by   this   NodeManager \"  )  )  ;", "}", "} else    {", "context . getNMStateStore (  )  . storeContainerKilled ( containerID )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ContainerKillEvent ( containerID ,    ContainerExitStatus . KILLED _ BY _ APPMASTER ,     \" Container   killed   by   the   ApplicationMaster .  \"  )  )  ;", "NMAuditLogger . logSuccess ( container . getUser (  )  ,    NMAuditLogger . AuditConstants . STOP _ CONTAINER ,     \" ContainerManageImpl \"  ,    containerID . getApplicationAttemptId (  )  . getApplicationId (  )  ,    containerID )  ;", "nodeStatusUpdater . sendOutofBandHeartBeat (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stopContainerInternal"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "context . getNMTokenSecretManager (  )  . appAttemptStartContainer ( nmTokenIdentifier )  ;", "}", "METHOD_END"], "methodName": ["updateNMTokenIdentifier"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    password    =    context . getContainerTokenSecretManager (  )  . retrievePassword ( containerTokenIdentifier )  ;", "byte [  ]    tokenPass    =    token . getPassword (  )  . array (  )  ;", "if    (  (  ( password    =  =    null )     |  |     ( tokenPass    =  =    null )  )     |  |     (  !  ( Arrays . equals ( password ,    tokenPass )  )  )  )     {", "throw   new   security . token . SecretManager . InvalidToken (  (  \" Invalid   container   token   used   for   starting   container   on    :     \"     +     ( context . getNodeId (  )  . toString (  )  )  )  )  ;", "}", "return   containerTokenIdentifier ;", "}", "METHOD_END"], "methodName": ["verifyAndGetContainerTokenIdentifier"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "final   int   sleepMsec    =     1  0  0  ;", "int   waitIterations    =     1  0  0  ;", "List < ContainerId >    newContainers    =    new   ArrayList < ContainerId >  (  )  ;", "while    (  (  -  - waitIterations )     >  =     0  )     {", "newContainers . clear (  )  ;", "for    ( Container   container    :    context . getContainers (  )  . values (  )  )     {", "if    (  ( container . getContainerState (  )  )     =  =     ( ContainerState . NEW )  )     {", "newContainers . add ( container . getContainerId (  )  )  ;", "}", "}", "if    ( newContainers . isEmpty (  )  )     {", "break ;", "}", ". LOG . info (  (  \" Waiting   for   containers :     \"     +    newContainers )  )  ;", "Thread . sleep ( sleepMsec )  ;", "}", "if    ( waitIterations    <     0  )     {", ". LOG . warn (  \" Timeout   waiting   for   recovered   containers \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForRecoveredContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" Asrv \"  ,     \" Bsrv \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Asrv \"  )  ,     . ServiceA . class ,    Service . class )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Bsrv \"  )  ,     . ServiceB . class ,    Service . class )  ;", "conf . setInt (  \" A . expected . init \"  ,     1  )  ;", "conf . setInt (  \" B . expected . stop \"  ,     1  )  ;", "final   AuxServices   aux    =    new   AuxServices (  )  ;", "aux . init ( conf )  ;", "aux . start (  )  ;", "ApplicationId   appId 1     =    ApplicationId . newInstance (  0  ,     6  5  )  ;", "ByteBuffer   buf    =    ByteBuffer . allocate (  6  )  ;", "buf . putChar (  ' A '  )  ;", "buf . putInt (  6  5  )  ;", "buf . flip (  )  ;", "AuxServicesEvent   event    =    new   AuxServicesEvent ( AuxServicesEventType . APPLICATION _ INIT ,     \" user 0  \"  ,    appId 1  ,     \" Asrv \"  ,    buf )  ;", "aux . handle ( event )  ;", "ApplicationId   appId 2     =    ApplicationId . newInstance (  0  ,     6  6  )  ;", "event    =    new   AuxServicesEvent ( AuxServicesEventType . APPLICATION _ STOP ,     \" user 0  \"  ,    appId 2  ,     \" Bsrv \"  ,    null )  ;", "aux . handle ( event )  ;", "Collection < AuxiliaryService >    servs    =    aux . getServices (  )  ;", "for    ( AuxiliaryService   serv    :    servs )     {", "ArrayList < Integer >    appIds    =     (  (  . LightService )     ( serv )  )  . getAppIdsStopped (  )  ;", "assertEquals (  \" app   not   properly   stopped \"  ,     1  ,    appIds . size (  )  )  ;", "assertTrue (  \" wrong   app   stopped \"  ,    appIds . contains (  (  ( Integer )     (  6  6  )  )  )  )  ;", "}", "for    ( AuxiliaryService   serv    :    servs )     {", "assertNull (  (  (  . LightService )     ( serv )  )  . containerId )  ;", "assertNull (  (  (  . LightService )     ( serv )  )  . resource )  ;", "}", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( appId 1  ,     1  )  ;", "ContainerTokenIdentifier   cti    =    new   ContainerTokenIdentifier ( ContainerId . newInstance ( attemptId ,     1  )  ,     \"  \"  ,     \"  \"  ,    Resource . newInstance (  1  ,     1  )  ,     0  ,     0  ,     0  ,    Priority . newInstance (  0  )  ,     0  )  ;", "Container   container    =    new   ContainerImpl ( null ,    null ,    null ,    null ,    null ,    null ,    cti )  ;", "ContainerId   containerId    =    container . getContainerId (  )  ;", "Resource   resource    =    container . getResource (  )  ;", "event    =    new   AuxServicesEvent ( AuxServicesEventType . CONTAINER _ INIT ,    container )  ;", "aux . handle ( event )  ;", "for    ( AuxiliaryService   serv    :    servs )     {", "assertEquals ( containerId ,     (  (  . LightService )     ( serv )  )  . containerId )  ;", "assertEquals ( resource ,     (  (  . LightService )     ( serv )  )  . resource )  ;", "(  (  . LightService )     ( serv )  )  . containerId    =    null ;", "(  (  . LightService )     ( serv )  )  . resource    =    null ;", "}", "event    =    new   AuxServicesEvent ( AuxServicesEventType . CONTAINER _ STOP ,    container )  ;", "aux . handle ( event )  ;", "for    ( AuxiliaryService   serv    :    servs )     {", "assertEquals ( containerId ,     (  (  . LightService )     ( serv )  )  . containerId )  ;", "assertEquals ( resource ,     (  (  . LightService )     ( serv )  )  . resource )  ;", "}", "}", "METHOD_END"], "methodName": ["testAuxEventDispatch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "conf . set ( NM _ RECOVERY _ DIR ,     . TEST _ DIR . toString (  )  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" Asrv \"  ,     \" Bsrv \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Asrv \"  )  ,     . RecoverableServiceA . class ,    Service . class )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Bsrv \"  )  ,     . RecoverableServiceB . class ,    Service . class )  ;", "try    {", "final   AuxServices   aux    =    new   AuxServices (  )  ;", "aux . init ( conf )  ;", "Assert . assertEquals (  2  ,    aux . getServices (  )  . size (  )  )  ;", "File   auxStorageDir    =    new   File (  . TEST _ DIR ,    AuxServices . STATE _ STORE _ ROOT _ NAME )  ;", "Assert . assertEquals (  2  ,    auxStorageDir . listFiles (  )  . length )  ;", "aux . close (  )  ;", "}    finally    {", "FileUtil . fullyDelete (  . TEST _ DIR )  ;", "}", "}", "METHOD_END"], "methodName": ["testAuxServiceRecoverySetup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" Asrv \"  ,     \" Bsrv \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Asrv \"  )  ,     . ServiceA . class ,    Service . class )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Bsrv \"  )  ,     . ServiceB . class ,    Service . class )  ;", "final   AuxServices   aux    =    new   AuxServices (  )  ;", "aux . init ( conf )  ;", "int   latch    =     1  ;", "for    ( Service   s    :    aux . getServices (  )  )     {", "assertEquals ( INITED ,    s . getServiceState (  )  )  ;", "if    ( s   instanceof    . ServiceA )     {", "latch    *  =     2  ;", "} else", "if    ( s   instanceof    . ServiceB )     {", "latch    *  =     3  ;", "} else", "fail (  (  \" Unexpected   service   type    \"     +     ( s . getClass (  )  )  )  )  ;", "}", "assertEquals (  \" Invalid   mix   of   services \"  ,     6  ,    latch )  ;", "aux . start (  )  ;", "for    ( Service   s    :    aux . getServices (  )  )     {", "assertEquals ( STARTED ,    s . getServiceState (  )  )  ;", "}", "aux . stop (  )  ;", "for    ( Service   s    :    aux . getServices (  )  )     {", "assertEquals ( STOPPED ,    s . getServiceState (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAuxServices"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" Asrv \"  ,     \" Bsrv \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Asrv \"  )  ,     . ServiceA . class ,    Service . class )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Bsrv \"  )  ,     . ServiceB . class ,    Service . class )  ;", "final   AuxServices   aux    =    new   AuxServices (  )  ;", "aux . init ( conf )  ;", "int   latch    =     1  ;", "for    ( Service   s    :    aux . getServices (  )  )     {", "assertEquals ( INITED ,    s . getServiceState (  )  )  ;", "if    ( s   instanceof    . ServiceA )     {", "latch    *  =     2  ;", "} else", "if    ( s   instanceof    . ServiceB )     {", "latch    *  =     3  ;", "} else", "fail (  (  \" Unexpected   service   type    \"     +     ( s . getClass (  )  )  )  )  ;", "}", "assertEquals (  \" Invalid   mix   of   services \"  ,     6  ,    latch )  ;", "aux . start (  )  ;", "for    ( Service   s    :    aux . getServices (  )  )     {", "assertEquals ( STARTED ,    s . getServiceState (  )  )  ;", "}", "Map < String ,    ByteBuffer >    meta    =    aux . getMetaData (  )  ;", "assertEquals (  2  ,    meta . size (  )  )  ;", "assertEquals (  \" A \"  ,    new   String ( meta . get (  \" Asrv \"  )  . array (  )  )  )  ;", "assertEquals (  \" B \"  ,    new   String ( meta . get (  \" Bsrv \"  )  . array (  )  )  )  ;", "aux . stop (  )  ;", "for    ( Service   s    :    aux . getServices (  )  )     {", "assertEquals ( STOPPED ,    s . getServiceState (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAuxServicesMeta"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" Asrv \"  ,     \" Bsrv \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Asrv \"  )  ,     . ServiceA . class ,    Service . class )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Bsrv \"  )  ,     . ServiceB . class ,    Service . class )  ;", "final   AuxServices   aux    =    new   AuxServices (  )  ;", "aux . init ( conf )  ;", "aux . start (  )  ;", "Service   s    =    aux . getServices (  )  . iterator (  )  . next (  )  ;", "s . stop (  )  ;", "assertEquals (  \" Auxiliary   service   stopped ,    but   AuxService   unaffected .  \"  ,    STOPPED ,    aux . getServiceState (  )  )  ;", "assertTrue ( aux . getServices (  )  . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAuxUnexpectedStop"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices"}, {"methodBody": ["METHOD_START", "{", "final   AuxServices   aux    =    new   AuxServices (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" Asrv 1  \"  ,     \" Bsrv _  2  \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Asrv 1  \"  )  ,     . ServiceA . class ,    Service . class )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" Bsrv _  2  \"  )  ,     . ServiceB . class ,    Service . class )  ;", "try    {", "aux . init ( conf )  ;", "}    catch    ( Exception   ex )     {", "Assert . fail (  \" Should   not   receive   the   exception .  \"  )  ;", "}", "final   AuxServices   aux 1     =    new   AuxServices (  )  ;", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \"  1 Asrv 1  \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \"  1 Asrv 1  \"  )  ,     . ServiceA . class ,    Service . class )  ;", "try    {", "aux 1  . init ( conf )  ;", "Assert . fail (  \" Should   receive   the   exception .  \"  )  ;", "}    catch    ( Exception   ex )     {", "assertTrue ( ex . getMessage (  )  . contains (  (  \" The   ServiceName :     1 Asrv 1    set   in    \"     +     (  \" yarn . nodemanager . aux - services   is   invalid . The   valid   service   name    \"     +     \" should   only   contain   a - zA - Z 0  -  9  _    and   can   not   start   with   numbers \"  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testValidAuxServiceName"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestAuxServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "Id   containerId    =    Id . newInstance ( appAttemptId ,    id )  ;", "return   containerId ;", "}", "METHOD_END"], "methodName": ["createContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "Resource   r    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "ContainerTokenIdentifier   TokenIdentifier    =    new   ContainerTokenIdentifier ( cId ,    nodeId . toString (  )  ,    user ,    r ,     (  ( System . currentTimeMillis (  )  )     +     1  0  0  0  0  0 L )  ,     1  2  3  ,    rmIdentifier ,    Priority . newInstance (  0  )  ,     0  )  ;", "Token   Token    =    BuilderUtils . newContainerToken ( nodeId ,    TokenSecretManager . retrievePassword ( TokenIdentifier )  ,    TokenIdentifier )  ;", "return   Token ;", "}", "METHOD_END"], "methodName": ["createContainerToken"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "File   scriptFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" scriptFile \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( scriptFile )  ;", "File   processStartFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" start _ file . txt \"  )  . getAbsoluteFile (  )  ;", "ContainerId   cId    =    createContainerId (  0  )  ;", "if    ( Shell . WINDOWS )     {", "fileWriter . println (  (  \"  @ echo   Hello   World !  >     \"     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +    cId )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "if    ( exitCode    !  =     0  )     {", "fileWriter . println (  (  \"  @ exit    \"     +    exitCode )  )  ;", "}", "} else    {", "fileWriter . write (  \"  \\ numask    0  \"  )  ;", "fileWriter . write (  (  \"  \\ necho   Hello   World !     >     \"     +    processStartFile )  )  ;", "fileWriter . write (  (  \"  \\ necho    $  $     >  >     \"     +    processStartFile )  )  ;", "if    ( exitCode    !  =     0  )     {", "fileWriter . write (  (  \"  \\ nexit    \"     +    exitCode )  )  ;", "}", "}", "fileWriter . close (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    Arrays . asList ( Shell . getRunScriptCommand ( scriptFile )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,     . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "GetContainerStatusesRequest   gcsRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( gcsRequest )  . getContainerStatuses (  )  . get (  0  )  ;", "Assert . assertEquals ( exitCode ,    containerStatus . getExitStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchAndExit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "int   exitCode    =     5  0  ;", "testContainerLaunchAndExit ( exitCode )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchAndExitFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "int   exitCode    =     0  ;", "testContainerLaunchAndExit ( exitCode )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchAndExitSuccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "File   scriptFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" scriptFile \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( scriptFile )  ;", "File   processStartFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" start _ file . txt \"  )  . getAbsoluteFile (  )  ;", "ContainerId   cId    =    createContainerId (  0  )  ;", "if    ( Shell . WINDOWS )     {", "fileWriter . println (  (  \"  @ echo   Hello   World !  >     \"     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +    cId )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  \"  @ ping    - n    1  0  0     1  2  7  .  0  .  0  .  1     > nul \"  )  ;", "} else    {", "fileWriter . write (  \"  \\ numask    0  \"  )  ;", "fileWriter . write (  (  \"  \\ necho   Hello   World !     >     \"     +    processStartFile )  )  ;", "fileWriter . write (  (  \"  \\ necho    $  $     >  >     \"     +    processStartFile )  )  ;", "fileWriter . write (  \"  \\ nexec   sleep    1  0  0  \"  )  ;", "}", "fileWriter . close (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    Arrays . asList ( Shell . getRunScriptCommand ( scriptFile )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,     . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( processStartFile . exists (  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "BaseContainerManagerTest . LOG . info (  \" Waiting   for   process   start - file   to   be   created \"  )  ;", "}", "Assert . assertTrue (  \" ProcessStartFile   doesn ' t   exist !  \"  ,    processStartFile . exists (  )  )  ;", "BufferedReader   reader    =    new   BufferedReader ( new   FileReader ( processStartFile )  )  ;", "Assert . assertEquals (  \" Hello   World !  \"  ,    reader . readLine (  )  )  ;", "String   pid    =    reader . readLine (  )  . trim (  )  ;", "Assert . assertEquals ( null ,    reader . readLine (  )  )  ;", "Assert . assertTrue (  \" Process   is   not   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( pid )  )  ;", "Assert . assertTrue (  \" Process   is   not   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( pid )  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "StopContainersRequest   stopRequest    =    StopContainersRequest . newInstance ( containerIds )  ;", "containerManager . stopContainers ( stopRequest )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE )  ;", "GetContainerStatusesRequest   gcsRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( gcsRequest )  . getContainerStatuses (  )  . get (  0  )  ;", "int   expectedExitCode    =    ContainerExitStatus . KILLED _ BY _ APPMASTER ;", "Assert . assertEquals ( expectedExitCode ,    containerStatus . getExitStatus (  )  )  ;", "Assert . assertFalse (  \" Process   is   still   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( pid )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchAndStop"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "ContainerId   cId 1     =    createContainerId (  0  )  ;", "ContainerId   cId 2     =    createContainerId (  0  )  ;", "containerLaunchContext . setLocalResources ( new   HashMap < String ,    api . records . LocalResource >  (  )  )  ;", "StartContainerRequest   startRequest 1     =    StartContainerRequest . newInstance ( containerLaunchContext ,    TestContainerManager . createContainerToken ( cId 1  ,    ResourceManagerConstants . RM _ INVALID _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( startRequest 1  )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "boolean   catchException    =    false ;", "try    {", "StartContainersResponse   response    =    containerManager . startContainers ( allRequests )  ;", "if    ( response . getFailedRequests (  )  . containsKey ( cId 1  )  )     {", "throw   response . getFailedRequests (  )  . get ( cId 1  )  . deSerialize (  )  ;", "}", "}    catch    ( Throwable   e )     {", "e . printStackTrace (  )  ;", "catchException    =    true ;", "Assert . assertTrue ( e . getMessage (  )  . contains (  (  (  \" Container    \"     +    cId 1  )     +     \"    rejected   as   it   is   allocated   by   a   previous   RM \"  )  )  )  ;", "Assert . assertTrue ( e . getClass (  )  . getName (  )  . equalsIgnoreCase ( InvalidContainerException . class . getName (  )  )  )  ;", "}", "Assert . assertTrue ( catchException )  ;", "StartContainerRequest   startRequest 2     =    StartContainerRequest . newInstance ( containerLaunchContext ,    TestContainerManager . createContainerToken ( cId 2  ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list 2     =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( startRequest 2  )  ;", "StartContainersRequest   allRequests 2     =    StartContainersRequest . newInstance ( list 2  )  ;", "containerManager . startContainers ( allRequests 2  )  ;", "boolean   noException    =    true ;", "try    {", "containerManager . startContainers ( allRequests 2  )  ;", "}    catch    ( YarnException   e )     {", "noException    =    false ;", "}", "Assert . assertTrue ( noException )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchFromPreviousRM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "InetAddress   localAddr    =    InetAddress . getLocalHost (  )  ;", "String   fqdn    =    localAddr . getCanonicalHostName (  )  ;", "if    (  !  ( localAddr . getHostAddress (  )  . equals ( fqdn )  )  )     {", "Assert . assertEquals ( fqdn ,    context . getNodeId (  )  . getHost (  )  )  ;", "}", "boolean   throwsException    =    false ;", "try    {", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "ContainerId   id    =    createContainerId (  0  )  ;", "containerIds . add ( id )  ;", "GetContainerStatusesRequest   request    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "GetContainerStatusesResponse   response    =    containerManager . getContainerStatuses ( request )  ;", "if    ( response . getFailedRequests (  )  . containsKey ( id )  )     {", "throw   response . getFailedRequests (  )  . get ( id )  . deSerialize (  )  ;", "}", "}    catch    ( Throwable   e )     {", "throwsException    =    true ;", "}", "Assert . assertTrue ( throwsException )  ;", "}", "METHOD_END"], "methodName": ["testContainerManagerInitialization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "File   dir    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" dir \"  )  ;", "dir . mkdirs (  )  ;", "File   file    =    new   File ( dir ,     \" file \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( file )  ;", "fileWriter . write (  \" Hello   World !  \"  )  ;", "fileWriter . close (  )  ;", "ContainerId   cId    =    createContainerId (  0  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( file . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( file . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,     . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE )  ;", "ApplicationId   appId    =    cId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "String   appIDStr    =    ConverterUtils . toString ( appId )  ;", "String   containerIDStr    =    ConverterUtils . toString ( cId )  ;", "File   userCacheDir    =    new   File ( BaseContainerManagerTest . localDir ,    ContainerLocalizer . USERCACHE )  ;", "File   userDir    =    new   File ( userCacheDir ,    user )  ;", "File   appCache    =    new   File ( userDir ,    ContainerLocalizer . APPCACHE )  ;", "File   appDir    =    new   File ( appCache ,    appIDStr )  ;", "File   containerDir    =    new   File ( appDir ,    containerIDStr )  ;", "File   targetFile    =    new   File ( containerDir ,    destinationFile )  ;", "File   sysDir    =    new   File ( BaseContainerManagerTest . localDir ,    ResourceLocalizationService . NM _ PRIVATE _ DIR )  ;", "File   appSysDir    =    new   File ( sysDir ,    appIDStr )  ;", "File   containerSysDir    =    new   File ( appSysDir ,    containerIDStr )  ;", "for    ( File   f    :    new   File [  ]  {    BaseContainerManagerTest . localDir ,    sysDir ,    userCacheDir ,    appDir ,    appSysDir ,    containerDir ,    containerSysDir    }  )     {", "Assert . assertTrue (  (  ( f . getAbsolutePath (  )  )     +     \"    doesn ' t   exist !  !  \"  )  ,    f . exists (  )  )  ;", "Assert . assertTrue (  (  ( f . getAbsolutePath (  )  )     +     \"    is   not   a   directory !  !  \"  )  ,    f . isDirectory (  )  )  ;", "}", "Assert . assertTrue (  (  ( targetFile . getAbsolutePath (  )  )     +     \"    doesn ' t   exist !  !  \"  )  ,    targetFile . exists (  )  )  ;", "BufferedReader   reader    =    new   BufferedReader ( new   FileReader ( targetFile )  )  ;", "Assert . assertEquals (  \" Hello   World !  \"  ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( null ,    reader . readLine (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerSetup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "delSrvc    =    new   DeletionService ( exec )  ;", "delSrvc . init ( conf )  ;", "containerManager    =    createContainerManager ( delSrvc )  ;", "containerManager . init ( conf )  ;", "containerManager . start (  )  ;", "File   dir    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" dir \"  )  ;", "dir . mkdirs (  )  ;", "File   file    =    new   File ( dir ,     \" file \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( file )  ;", "fileWriter . write (  \" Hello   World !  \"  )  ;", "fileWriter . close (  )  ;", "ContainerId   cId    =    createContainerId (  0  )  ;", "ApplicationId   appId    =    cId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( FileContext . getLocalFSFileContext (  )  . makeQualified ( new   Path ( file . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( file . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,     . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE )  ;", "BaseContainerManagerTest . waitForApplicationState ( containerManager ,    cId . getApplicationAttemptId (  )  . getApplicationId (  )  ,    ApplicationState . RUNNING )  ;", "String   appIDStr    =    ConverterUtils . toString ( appId )  ;", "String   containerIDStr    =    ConverterUtils . toString ( cId )  ;", "File   userCacheDir    =    new   File ( BaseContainerManagerTest . localDir ,    ContainerLocalizer . USERCACHE )  ;", "File   userDir    =    new   File ( userCacheDir ,    user )  ;", "File   appCache    =    new   File ( userDir ,    ContainerLocalizer . APPCACHE )  ;", "File   appDir    =    new   File ( appCache ,    appIDStr )  ;", "File   containerDir    =    new   File ( appDir ,    containerIDStr )  ;", "File   targetFile    =    new   File ( containerDir ,    destinationFile )  ;", "File   sysDir    =    new   File ( BaseContainerManagerTest . localDir ,    ResourceLocalizationService . NM _ PRIVATE _ DIR )  ;", "File   appSysDir    =    new   File ( sysDir ,    appIDStr )  ;", "File   containerSysDir    =    new   File ( appSysDir ,    containerIDStr )  ;", "Assert . assertTrue (  (  (  \" AppDir    \"     +     ( appDir . getAbsolutePath (  )  )  )     +     \"    doesn ' t   exist !  !  \"  )  ,    appDir . exists (  )  )  ;", "Assert . assertTrue (  (  (  \" AppSysDir    \"     +     ( appSysDir . getAbsolutePath (  )  )  )     +     \"    doesn ' t   exist !  !  \"  )  ,    appSysDir . exists (  )  )  ;", "for    ( File   f    :    new   File [  ]  {    containerDir ,    containerSysDir    }  )     {", "Assert . assertFalse (  (  ( f . getAbsolutePath (  )  )     +     \"    exists !  !  \"  )  ,    f . exists (  )  )  ;", "}", "Assert . assertFalse (  (  ( targetFile . getAbsolutePath (  )  )     +     \"    exists !  !  \"  )  ,    targetFile . exists (  )  )  ;", "containerManager . handle ( new   CMgrCompletedAppsEvent ( Arrays . asList ( new   ApplicationId [  ]  {    appId    }  )  ,    CMgrCompletedAppsEvent . Reason . ON _ SHUTDOWN )  )  ;", "BaseContainerManagerTest . waitForApplicationState ( containerManager ,    cId . getApplicationAttemptId (  )  . getApplicationId (  )  ,    ApplicationState . FINISHED )  ;", "for    ( File   f    :    new   File [  ]  {    appDir ,    containerDir ,    appSysDir ,    containerSysDir    }  )     {", "int   timeout    =     0  ;", "while    (  ( f . exists (  )  )     &  &     (  ( timeout +  +  )     <     1  5  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertFalse (  (  ( f . getAbsolutePath (  )  )     +     \"    exists !  !  \"  )  ,    f . exists (  )  )  ;", "}", "int   timeout    =     0  ;", "while    (  ( targetFile . exists (  )  )     &  &     (  ( timeout +  +  )     <     1  5  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertFalse (  (  ( targetFile . getAbsolutePath (  )  )     +     \"    exists !  !  \"  )  ,    targetFile . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalFilesCleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "ContainerId   cId    =    createContainerId ( i )  ;", "long   identifier    =     0  ;", "if    (  ( i    &     1  )     =  =     0  )", "identifier    =    api . ResourceManagerConstants . RM _ INVALID _ IDENTIFIER ;", "else", "identifier    =    DUMMY _ RM _ IDENTIFIER ;", "Token   containerToken    =    TestContainerManager . createContainerToken ( cId ,    identifier ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  ;", "StartContainerRequest   request    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "list . add ( request )  ;", "}", "StartContainersRequest   requestList    =    StartContainersRequest . newInstance ( list )  ;", "StartContainersResponse   response    =    containerManager . startContainers ( requestList )  ;", "Assert . assertEquals (  5  ,    response . getSuccessfullyStartedContainers (  )  . size (  )  )  ;", "for    ( ContainerId   id    :    response . getSuccessfullyStartedContainers (  )  )     {", "Assert . assertEquals (  1  ,     (  ( id . getId (  )  )     &     1  )  )  ;", "}", "Assert . assertEquals (  5  ,    response . getFailedRequests (  )  . size (  )  )  ;", "for    ( Map . Entry < ContainerId ,    SerializedException >    entry    :    response . getFailedRequests (  )  . entrySet (  )  )     {", "Assert . assertEquals (  0  ,     (  ( entry . getKey (  )  . getId (  )  )     &     1  )  )  ;", "Assert . assertTrue ( entry . getValue (  )  . getMessage (  )  . contains (  (  (  \" Container    \"     +     ( entry . getKey (  )  )  )     +     \"    rejected   as   it   is   allocated   by   a   previous   RM \"  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultipleContainersLaunch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "List < StartContainerRequest >    startRequest    =    new   ArrayList < StartContainerRequest >  (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "ContainerId   cId    =    createContainerId ( i )  ;", "String   user    =    null ;", "if    (  ( i    &     1  )     =  =     0  )     {", "user    =     \" Fail \"  ;", "} else    {", "user    =     \" Pass \"  ;", "}", "Token   containerToken    =     . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  ;", "StartContainerRequest   request    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "startRequest . add ( request )  ;", "containerIds . add ( cId )  ;", "}", "StartContainersRequest   requestList    =    StartContainersRequest . newInstance ( startRequest )  ;", "containerManager . startContainers ( requestList )  ;", "GetContainerStatusesRequest   statusRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "GetContainerStatusesResponse   statusResponse    =    containerManager . getContainerStatuses ( statusRequest )  ;", "Assert . assertEquals (  5  ,    statusResponse . getContainerStatuses (  )  . size (  )  )  ;", "for    ( ContainerStatus   status    :    statusResponse . getContainerStatuses (  )  )     {", "Assert . assertEquals (  1  ,     (  ( status . getContainerId (  )  . getId (  )  )     &     1  )  )  ;", "}", "Assert . assertEquals (  5  ,    statusResponse . getFailedRequests (  )  . size (  )  )  ;", "for    ( Map . Entry < ContainerId ,    SerializedException >    entry    :    statusResponse . getFailedRequests (  )  . entrySet (  )  )     {", "Assert . assertEquals (  0  ,     (  ( entry . getKey (  )  . getId (  )  )     &     1  )  )  ;", "Assert . assertTrue ( entry . getValue (  )  . getMessage (  )  . contains (  \" Reject   this   container \"  )  )  ;", "}", "StopContainersRequest   stopRequest    =    StopContainersRequest . newInstance ( containerIds )  ;", "StopContainersResponse   stopResponse    =    containerManager . stopContainers ( stopRequest )  ;", "Assert . assertEquals (  5  ,    stopResponse . getSuccessfullyStoppedContainers (  )  . size (  )  )  ;", "for    ( ContainerId   id    :    stopResponse . getSuccessfullyStoppedContainers (  )  )     {", "Assert . assertEquals (  1  ,     (  ( id . getId (  )  )     &     1  )  )  ;", "}", "Assert . assertEquals (  5  ,    stopResponse . getFailedRequests (  )  . size (  )  )  ;", "for    ( Map . Entry < ContainerId ,    SerializedException >    entry    :    stopResponse . getFailedRequests (  )  . entrySet (  )  )     {", "Assert . assertEquals (  0  ,     (  ( entry . getKey (  )  . getId (  )  )     &     1  )  )  ;", "Assert . assertTrue ( entry . getValue (  )  . getMessage (  )  . contains (  \" Reject   this   container \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMultipleContainersStopAndGetStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "conf . setStrings ( NM _ AUX _ SERVICES ,    new   String [  ]  {     \" existService \"     }  )  ;", "conf . setClass ( String . format ( NM _ AUX _ SERVICE _ FMT ,     \" existService \"  )  ,    TestAuxServices . ServiceA . class ,    Service . class )  ;", "containerManager . start (  )  ;", "List < StartContainerRequest >    startRequest    =    new   ArrayList < StartContainerRequest >  (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "Map < String ,    ByteBuffer >    serviceData    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "String   serviceName    =     \" non _ exist _ auxService \"  ;", "serviceData . put ( serviceName ,    ByteBuffer . wrap ( serviceName . getBytes (  )  )  )  ;", "containerLaunchContext . setServiceData ( serviceData )  ;", "ContainerId   cId    =    createContainerId (  0  )  ;", "String   user    =     \" start _ container _ fail \"  ;", "Token   containerToken    =     . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  ;", "StartContainerRequest   request    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "startRequest . add ( request )  ;", "StartContainersRequest   requestList    =    StartContainersRequest . newInstance ( startRequest )  ;", "StartContainersResponse   response    =    containerManager . startContainers ( requestList )  ;", "Assert . assertTrue (  (  ( response . getFailedRequests (  )  . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue (  (  ( response . getSuccessfullyStartedContainers (  )  . size (  )  )     =  =     0  )  )  ;", "Assert . assertTrue ( response . getFailedRequests (  )  . containsKey ( cId )  )  ;", "Assert . assertTrue ( response . getFailedRequests (  )  . get ( cId )  . getMessage (  )  . contains (  (  (  \" The   auxService :  \"     +    serviceName )     +     \"    does   not   exist \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testStartContainerFailureWithUnknownAuxService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager"}, {"methodBody": ["METHOD_START", "{", "final   LogHandler   logHandler    =    mock ( LogHandler . class )  ;", "final   ResourceLocalizationService   rsrcSrv    =    new   ResourceLocalizationService ( null ,    null ,    null ,    null ,    context . getNMStateStore (  )  )     {", "@ Override", "public   void   serviceInit ( Configuration   conf )    throws   Exception    {", "}", "@ Override", "public   void   serviceStart (  )    throws   Exception    {", "}", "@ Override", "public   void   serviceStop (  )    throws   Exception    {", "}", "@ Override", "public   void   handle ( LocalizationEvent   event )     {", "}", "}  ;", "final   ContainersLauncher   launcher    =    new   ContainersLauncher ( context ,    null ,    null ,    null ,    null )     {", "@ Override", "public   void   handle ( ContainersLauncherEvent   event )     {", "}", "}  ;", "return   new   Impl ( context ,    mock ( ContainerExecutor . class )  ,    mock ( DeletionService . class )  ,    mock ( NodeStatusUpdater . class )  ,    metrics ,    context . getApplicationACLsManager (  )  ,    null )     {", "@ Override", "protected   LogHandler   createLogHandler ( Configuration   conf ,    Context   context ,    DeletionService   deletionService )     {", "return   logHandler ;", "}", "@ Override", "protected   ResourceLocalizationService   createResourceLocalizationService ( ContainerExecutor   exec ,    DeletionService   deletionContext )     {", "return   rsrcSrv ;", "}", "@ Override", "protected   ContainersLauncher   createContainersLauncher ( Context   context ,    ContainerExecutor   exec )     {", "return   launcher ;", "}", "@ Override", "public   void   setBlockNewContainerRequests ( boolean   blockNewContainerRequests )     {", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createContainerManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   user    =    UserGroupInformation . createRemoteUser ( cid . getApplicationAttemptId (  )  . toString (  )  )  ;", "StartContainerRequest   scReq    =    StartContainerRequest . newInstance ( clc ,     . createContainerToken ( cid ,     0  ,    context . getNodeId (  )  ,    user . getShortUserName (  )  ,    context . getContainerTokenSecretManager (  )  )  )  ;", "final   List < StartContainerRequest >    scReqList    =    new   ArrayList < StartContainerRequest >  (  )  ;", "scReqList . add ( scReq )  ;", "NMTokenIdentifier   nmToken    =    new   NMTokenIdentifier ( cid . getApplicationAttemptId (  )  ,    context . getNodeId (  )  ,    user . getShortUserName (  )  ,    context . getNMTokenSecretManager (  )  . getCurrentKey (  )  . getKeyId (  )  )  ;", "user . addTokenIdentifier ( nmToken )  ;", "return   user . doAs ( new   PrivilegedExceptionAction < StartContainersResponse >  (  )     {", "@ Override", "public   StartContainersResponse   run (  )    throws   Exception    {", "return   cm . startContainers ( StartContainersRequest . newInstance ( scReqList )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["startContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "conf . set ( NM _ ADDRESS ,     \" localhost :  1  2  3  4  \"  )  ;", "conf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "conf . set ( YARN _ ADMIN _ ACL ,     \" yarn _ admin _ user \"  )  ;", "NMStateStoreService   stateStore    =    new   NMMemoryStateStoreService (  )  ;", "stateStore . init ( conf )  ;", "stateStore . start (  )  ;", "Context   context    =    new   NodeManager . NMContext ( new   NMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInNM (  )  ,    null ,    new   ApplicationACLsManager ( conf )  ,    stateStore )  ;", "ContainerManagerImpl   cm    =    createContainerManager ( context )  ;", "cm . init ( conf )  ;", "cm . start (  )  ;", "MasterKey   masterKey    =    new   MasterKeyPBImpl (  )  ;", "masterKey . setKeyId (  1  2  3  )  ;", "masterKey . setBytes ( ByteBuffer . wrap ( new   byte [  ]  {    new   Integer (  1  2  3  )  . byteValue (  )     }  )  )  ;", "context . getContainerTokenSecretManager (  )  . setMasterKey ( masterKey )  ;", "context . getNMTokenSecretManager (  )  . setMasterKey ( masterKey )  ;", "String   appUser    =     \" app _ user 1  \"  ;", "String   modUser    =     \" modify _ user 1  \"  ;", "String   viewUser    =     \" view _ user 1  \"  ;", "String   enemyUser    =     \" enemy _ user \"  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   cid    =    ContainerId . newInstance ( attemptId ,     1  )  ;", "Map < String ,    LocalResource >    localResources    =    Collections . emptyMap (  )  ;", "Map < String ,    String >    containerEnv    =    Collections . emptyMap (  )  ;", "List < String >    containerCmds    =    Collections . emptyList (  )  ;", "Map < String ,    ByteBuffer >    serviceData    =    Collections . emptyMap (  )  ;", "Credentials   containerCreds    =    new   Credentials (  )  ;", "DataOutputBuffer   dob    =    new   DataOutputBuffer (  )  ;", "containerCreds . writeTokenStorageToStream ( dob )  ;", "ByteBuffer   containerTokens    =    ByteBuffer . wrap ( dob . getData (  )  ,     0  ,    dob . getLength (  )  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "acls . put ( MODIFY _ APP ,    modUser )  ;", "acls . put ( VIEW _ APP ,    viewUser )  ;", "ContainerLaunchContext   clc    =    ContainerLaunchContext . newInstance ( localResources ,    containerEnv ,    containerCmds ,    serviceData ,    containerTokens ,    acls )  ;", "StartContainersResponse   startResponse    =    startContainer ( context ,    cm ,    cid ,    clc )  ;", "assertTrue ( startResponse . getFailedRequests (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    context . getApplications (  )  . size (  )  )  ;", "Application   app    =    context . getApplications (  )  . get ( appId )  ;", "assertNotNull ( app )  ;", "waitForAppState ( app ,    ApplicationState . INITING )  ;", "assertTrue ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( modUser )  ,    MODIFY _ APP ,    appUser ,    appId )  )  ;", "assertFalse ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( viewUser )  ,    MODIFY _ APP ,    appUser ,    appId )  )  ;", "assertTrue ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( viewUser )  ,    VIEW _ APP ,    appUser ,    appId )  )  ;", "assertFalse ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( enemyUser )  ,    VIEW _ APP ,    appUser ,    appId )  )  ;", "cm . stop (  )  ;", "context    =    new   NodeManager . NMContext ( new   NMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInNM (  )  ,    null ,    new   ApplicationACLsManager ( conf )  ,    stateStore )  ;", "cm    =    createContainerManager ( context )  ;", "cm . init ( conf )  ;", "cm . start (  )  ;", "assertEquals (  1  ,    context . getApplications (  )  . size (  )  )  ;", "app    =    context . getApplications (  )  . get ( appId )  ;", "assertNotNull ( app )  ;", "waitForAppState ( app ,    ApplicationState . INITING )  ;", "assertTrue ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( modUser )  ,    MODIFY _ APP ,    appUser ,    appId )  )  ;", "assertFalse ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( viewUser )  ,    MODIFY _ APP ,    appUser ,    appId )  )  ;", "assertTrue ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( viewUser )  ,    VIEW _ APP ,    appUser ,    appId )  )  ;", "assertFalse ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( enemyUser )  ,    VIEW _ APP ,    appUser ,    appId )  )  ;", "List < ApplicationId >    finishedApps    =    new   ArrayList < ApplicationId >  (  )  ;", "finishedApps . add ( appId )  ;", "cm . handle ( new   CMgrCompletedAppsEvent ( finishedApps ,    CMgrCompletedAppsEvent . Reason . BY _ RESOURCEMANAGER )  )  ;", "waitForAppState ( app ,    ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP )  ;", "cm . stop (  )  ;", "context    =    new   NodeManager . NMContext ( new   NMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInNM (  )  ,    null ,    new   ApplicationACLsManager ( conf )  ,    stateStore )  ;", "cm    =    createContainerManager ( context )  ;", "cm . init ( conf )  ;", "cm . start (  )  ;", "assertEquals (  1  ,    context . getApplications (  )  . size (  )  )  ;", "app    =    context . getApplications (  )  . get ( appId )  ;", "assertNotNull ( app )  ;", "waitForAppState ( app ,    ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP )  ;", "assertTrue ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( modUser )  ,    MODIFY _ APP ,    appUser ,    appId )  )  ;", "assertFalse ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( viewUser )  ,    MODIFY _ APP ,    appUser ,    appId )  )  ;", "assertTrue ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( viewUser )  ,    VIEW _ APP ,    appUser ,    appId )  )  ;", "assertFalse ( context . getApplicationACLsManager (  )  . checkAccess ( UserGroupInformation . createRemoteUser ( enemyUser )  ,    VIEW _ APP ,    appUser ,    appId )  )  ;", "app . handle ( new   ApplicationEvent ( app . getAppId (  )  ,    ApplicationEventType . APPLICATION _ RESOURCES _ CLEANEDUP )  )  ;", "assertEquals ( app . getApplicationState (  )  ,    ApplicationState . FINISHED )  ;", "app . handle ( new   ApplicationEvent ( app . getAppId (  )  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )  )  ;", "cm . stop (  )  ;", "context    =    new   NodeManager . NMContext ( new   NMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInNM (  )  ,    null ,    new   ApplicationACLsManager ( conf )  ,    stateStore )  ;", "cm    =    createContainerManager ( context )  ;", "cm . init ( conf )  ;", "cm . start (  )  ;", "assertTrue ( context . getApplications (  )  . isEmpty (  )  )  ;", "cm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationRecovery"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery"}, {"methodBody": ["METHOD_START", "{", "final   int   msecPerSleep    =     1  0  ;", "int   msecLeft    =     5  0  0  0  ;", "while    (  (  ( app . getApplicationState (  )  )     !  =    state )     &  &     ( msecLeft    >     0  )  )     {", "Thread . sleep ( msecPerSleep )  ;", "msecLeft    -  =    msecPerSleep ;", "}", "astEquals ( state ,    app . getApplicationState (  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForAppState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManagerRecovery"}, {"methodBody": ["METHOD_START", "{", "return   this . containerID ;", "}", "METHOD_END"], "methodName": ["getContainerID"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationContainerFinishedEvent"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationContainerInitEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationID ;", "}", "METHOD_END"], "methodName": ["getApplicationID"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent"}, {"methodBody": ["METHOD_START", "{", "return   diagnostic ;", "}", "METHOD_END"], "methodName": ["getDiagnostic"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationFinishEvent"}, {"methodBody": ["METHOD_START", "{", "this . dispatcher . getEventHandler (  )  . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . DESTROY _ APPLICATION _ RESOURCES ,    this )  )  ;", "this . dispatcher . getEventHandler (  )  . handle ( new   AuxServicesEvent ( AuxServicesEventType . APPLICATION _ STOP ,    appId )  )  ;", "}", "METHOD_END"], "methodName": ["handleAppFinishWithContainersCleanedup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationACLs ;", "}", "METHOD_END"], "methodName": ["getApplicationACLs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationInitEvent"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   cId    =    BuilderUtils . newContainerId ( appAttemptId ,    containerId )  ;", "Container   c    =    mock ( Container . class )  ;", "when ( c . getContainerId (  )  )  . thenReturn ( cId )  ;", "ContainerLaunchContext   launchContext    =    mock ( ContainerLaunchContext . class )  ;", "when ( c . getLaunchContext (  )  )  . thenReturn ( launchContext )  ;", "when ( launchContext . getApplicationACLs (  )  )  . thenReturn ( new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  )  ;", "return   c ;", "}", "METHOD_END"], "methodName": ["createMockedContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  5  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  (  -  1  )  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "reset ( wa . localizerBus )  ;", "wa . containerFinished (  0  )  ;", "wa . containerFinished (  1  )  ;", "wa . containerFinished (  2  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . appFinished (  )  ;", "assertEquals ( ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP ,    wa . app . getApplicationState (  )  )  ;", "verify ( wa . localizerBus )  . handle ( refEq ( new   ApplicationLocalizationEvent ( LocalizationEventType . DESTROY _ APPLICATION _ RESOURCES ,    wa . app )  )  )  ;", "wa . appResourcesCleanedup (  )  ;", "for    ( Container   container    :    wa . containers )     {", "ContainerTokenIdentifier   identifier    =    wa . getContainerTokenIdentifier ( container . getContainerId (  )  )  ;", "waitForContainerTokenToExpire ( identifier )  ;", "Assert . assertTrue ( wa . context . getContainerTokenSecretManager (  )  . isValidStartContainerRequest ( identifier )  )  ;", "}", "assertEquals ( ApplicationState . FINISHED ,    wa . app . getApplicationState (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppFinishedOnCompletedContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  1  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  0  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  1  ,    wa . app . getContainers (  )  . size (  )  )  ;", "reset ( wa . localizerBus )  ;", "wa . appFinished (  )  ;", "verify ( wa . containerBus )  . handle ( argThat ( new   TestApplication . ContainerKillMatcher ( wa . containers . get (  0  )  . getContainerId (  )  )  )  )  ;", "assertEquals ( ApplicationState . FINISHING _ CONTAINERS _ WAIT ,    wa . app . getApplicationState (  )  )  ;", "wa . containerFinished (  0  )  ;", "assertEquals ( ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP ,    wa . app . getApplicationState (  )  )  ;", "verify ( wa . localizerBus )  . handle ( refEq ( new   ApplicationLocalizationEvent ( LocalizationEventType . DESTROY _ APPLICATION _ RESOURCES ,    wa . app )  )  )  ;", "wa . initContainer (  1  )  ;", "assertEquals ( ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . appResourcesCleanedup (  )  ;", "assertEquals ( ApplicationState . FINISHED ,    wa . app . getApplicationState (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppFinishedOnIniting"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  4  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  (  -  1  )  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "wa . containerFinished (  0  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  2  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . appFinished (  )  ;", "assertEquals ( ApplicationState . FINISHING _ CONTAINERS _ WAIT ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  2  ,    wa . app . getContainers (  )  . size (  )  )  ;", "for    ( int   i    =     1  ;    i    <     ( wa . containers . size (  )  )  ;    i +  +  )     {", "verify ( wa . containerBus )  . handle ( argThat ( new   TestApplication . ContainerKillMatcher ( wa . containers . get ( i )  . getContainerId (  )  )  )  )  ;", "}", "wa . containerFinished (  1  )  ;", "assertEquals ( ApplicationState . FINISHING _ CONTAINERS _ WAIT ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  1  ,    wa . app . getContainers (  )  . size (  )  )  ;", "reset ( wa . localizerBus )  ;", "wa . containerFinished (  2  )  ;", "assertEquals ( ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "verify ( wa . localizerBus )  . handle ( refEq ( new   ApplicationLocalizationEvent ( LocalizationEventType . DESTROY _ APPLICATION _ RESOURCES ,    wa . app )  )  )  ;", "verify ( wa . auxBus )  . handle ( refEq ( new   AuxServicesEvent ( AuxServicesEventType . APPLICATION _ STOP ,    wa . appId )  )  )  ;", "wa . appResourcesCleanedup (  )  ;", "for    ( Container   container    :    wa . containers )     {", "ContainerTokenIdentifier   identifier    =    wa . getContainerTokenIdentifier ( container . getContainerId (  )  )  ;", "waitForContainerTokenToExpire ( identifier )  ;", "Assert . assertTrue ( wa . context . getContainerTokenSecretManager (  )  . isValidStartContainerRequest ( identifier )  )  ;", "}", "assertEquals ( ApplicationState . FINISHED ,    wa . app . getApplicationState (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppFinishedOnRunningContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  3  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  (  -  1  )  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "wa . containerFinished (  0  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  2  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . containerFinished (  1  )  ;", "wa . containerFinished (  2  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppRunningAfterContainersComplete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  1  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  1  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  1  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . initContainer (  0  )  ;", "wa . initContainer (  2  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  3  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( wa . containers . size (  )  )  ;    i +  +  )     {", "verify ( wa . containerBus )  . handle ( argThat ( new   TestApplication . ContainerInitMatcher ( wa . containers . get ( i )  . getContainerId (  )  )  )  )  ;", "}", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testApplicationInit1"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  2  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  0  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  1  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "verify ( wa . containerBus )  . handle ( argThat ( new   TestApplication . ContainerInitMatcher ( wa . containers . get (  0  )  . getContainerId (  )  )  )  )  ;", "wa . initContainer (  1  )  ;", "wa . initContainer (  2  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  3  ,    wa . app . getContainers (  )  . size (  )  )  ;", "for    ( int   i    =     1  ;    i    <     ( wa . containers . size (  )  )  ;    i +  +  )     {", "verify ( wa . containerBus )  . handle ( argThat ( new   TestApplication . ContainerInitMatcher ( wa . containers . get ( i )  . getContainerId (  )  )  )  )  ;", "}", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testApplicationInit2"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  3  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     1  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  (  -  1  )  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . containerFinished (  0  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testContainersCompleteDuringAppInit1"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  3  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  (  -  1  )  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . containerFinished (  0  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  2  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . containerFinished (  1  )  ;", "wa . containerFinished (  2  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testContainersCompleteDuringAppInit2"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  1  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     1  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  0  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  1  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . appFinished (  )  ;", "wa . containerFinished (  0  )  ;", "wa . appResourcesCleanedup (  )  ;", "assertEquals ( ApplicationState . FINISHED ,    wa . app . getApplicationState (  )  )  ;", "verify ( wa . nmTokenSecretMgr )  . appFinished ( eq ( wa . appId )  )  ;", "}    finally    {", "if    ( wa    !  =    null )     {", "wa . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testNMTokenSecretManagerCleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "TestApplication . WrappedApplication   wa    =    null ;", "try    {", "wa    =    new   TestApplication . WrappedApplication (  5  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     \" yak \"  ,     3  )  ;", "wa . initApplication (  )  ;", "wa . initContainer (  (  -  1  )  )  ;", "assertEquals ( ApplicationState . INITING ,    wa . app . getApplicationState (  )  )  ;", "wa . applicationInited (  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "reset ( wa . localizerBus )  ;", "wa . containerFinished (  0  )  ;", "wa . containerFinished (  1  )  ;", "wa . containerFinished (  2  )  ;", "assertEquals ( ApplicationState . RUNNING ,    wa . app . getApplicationState (  )  )  ;", "assertEquals (  0  ,    wa . app . getContainers (  )  . size (  )  )  ;", "wa . appFinished (  )  ;", "assertEquals ( ApplicationState . APPLICATION _ RESOURCES _ CLEANINGUP ,    wa . app . getApplicationState (  )  )  ;", "verify ( wa . localizerBus )  . handle ( refEq ( new   ApplicationLocalizationEvent ( LocalizationEventType . DESTROY _ APPLICATION _ RESOURCES ,    wa . app )  )  )  ;", "wa . appResourcesCleanedup (  )  ;", "assertEquals ( ApplicationState . FINISHED ,    wa . app . getApplicationState (  )  )  ;", "}    finally    {", "if    ( wa    !  =    null )", "wa . finished (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testStartContainerAfterAppFinished"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "int   attempts    =     5  ;", "while    (  (  ( System . currentTimeMillis (  )  )     <     ( identifier . getExpiryTimeStamp (  )  )  )     &  &     (  ( attempts -  -  )     >     0  )  )     {", "try    {", "Thread . sleep (  1  0  0  0  )  ;", "}    catch    ( Excep   e )     {", "}", "}", "return   identifier ;", "}", "METHOD_END"], "methodName": ["waitForContainerTokenToExpire"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.TestApplication"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnosticsUpdate ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsUpdate"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerID ;", "}", "METHOD_END"], "methodName": ["getContainerID"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticInfo"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerExitEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . exitCode ;", "}", "METHOD_END"], "methodName": ["getExitCode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerExitEvent"}, {"methodBody": ["METHOD_START", "{", "for    ( String   s    :    diags )     {", "this . diagnostics . append ( s )  ;", "}", "try    {", "stateStore . storeContainerDiagnostics ( containerId ,    diagnostics )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Unable   to   update   diagnostics   in   state   store   for    \"     +     ( containerId )  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["addDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    rsrc    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "if    (  !  ( publicRsrcs . isEmpty (  )  )  )     {", "rsrc . put ( PUBLIC ,    publicRsrcs )  ;", "}", "if    (  !  ( privateRsrcs . isEmpty (  )  )  )     {", "rsrc . put ( PRIVATE ,    privateRsrcs )  ;", "}", "if    (  !  ( appRsrcs . isEmpty (  )  )  )     {", "rsrc . put ( APPLICATION ,    appRsrcs )  ;", "}", "dispatcher . getEventHandler (  )  . handle ( new   LocalizationCleanupEvent ( this ,    rsrc )  )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "switch    ( getContainerState (  )  )     {", "case   EXITED _ WITH _ SUCCESS    :", "metrics . endRunningContainer (  )  ;", "metrics . completedContainer (  )  ;", "NMAuditLogger . logSuccess ( user ,    NMAuditLogger . AuditConstants . FINISH _ SUCCESS _ CONTAINER ,     \"  \"  ,    applicationId ,    containerId )  ;", "break ;", "case   EXITED _ WITH _ FAILURE    :", "if    ( wasLaunched )     {", "metrics . endRunningContainer (  )  ;", "}", "case   LOCALIZATION _ FAILED    :", "metrics . failedContainer (  )  ;", "NMAuditLogger . logFailure ( user ,    NMAuditLogger . AuditConstants . FINISH _ FAILED _ CONTAINER ,     \"  \"  ,     (  \" Container   failed   with   state :     \"     +     ( getContainerState (  )  )  )  ,    applicationId ,    containerId )  ;", "break ;", "case   CONTAINER _ CLEANEDUP _ AFTER _ KILL    :", "if    ( wasLaunched )     {", "metrics . endRunningContainer (  )  ;", "}", "case   NEW    :", "metrics . killedContainer (  )  ;", "NMAuditLogger . logSuccess ( user ,    NMAuditLogger . AuditConstants . FINISH _ KILLED _ CONTAINER ,     \"  \"  ,    applicationId ,    containerId )  ;", "}", "metrics . releaseContainer ( this . resource )  ;", "sendFinishedEvents (  )  ;", "}", "METHOD_END"], "methodName": ["finished"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "switch    ( stateMachine . getCurrentState (  )  )     {", "case   NEW    :", "case   LOCALIZING    :", "case   LOCALIZATION _ FAILED    :", "case   LOCALIZED    :", "case   RUNNING    :", "case   EXITED _ WITH _ SUCCESS    :", "case   EXITED _ WITH _ FAILURE    :", "case   KILLING    :", "case   CONTAINER _ CLEANEDUP _ AFTER _ KILL    :", "case   CONTAINER _ RESOURCES _ CLEANINGUP    :", "return   api . records . ContainerState ;", "case   DONE    :", "default    :", "return   api . records . ContainerState ;", "}", "}", "METHOD_END"], "methodName": ["getCurrentState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "return    ( this . exitCode )     =  =     ( ContainerExitStatus . INVALID )  ;", "}", "METHOD_END"], "methodName": ["hasDefaultExitCode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "long   pmemBytes    =     (  ( getResource (  )  . getMemory (  )  )     *     1  0  2  4  )     *     1  0  2  4 L ;", "float   pmemRatio    =    daemonConf . getFloat ( NM _ VMEM _ PMEM _ RATIO ,    DEFAULT _ NM _ VMEM _ PMEM _ RATIO )  ;", "long   vmemBytes    =     (  ( long )     ( pmemRatio    *    pmemBytes )  )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ContainerStartMonitoringEvent ( Id ,    vmemBytes ,    pmemBytes )  )  ;", "}", "METHOD_END"], "methodName": ["sendContainerMonitorStartEvent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "@ SuppressWarnings (  \" rawtypes \"  )", "EventHandler   eventHandler    =    dispatcher . getEventHandler (  )  ;", "eventHandler . handle ( new   ApplicationFinishedEvent ( containerId )  )  ;", "eventHandler . handle ( new   StopMonitoringEvent ( containerId )  )  ;", "eventHandler . handle ( new   LogHandlerFinishedEvent ( containerId ,    exitCode )  )  ;", "}", "METHOD_END"], "methodName": ["sendFinishedEvents"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "ContainersLauncherEventType   launcherEvent    =    ContainersLauncherEventType . LAUNCH _ CONTAINER ;", "if    (  ( recoveredStatus )     =  =     ( NMStateStoreService . RecoveredContainerStatus . LAUNCHED )  )     {", "launcherEvent    =    ContainersLauncherEventType . RECOVER _ CONTAINER ;", "}", "dispatcher . getEventHandler (  )  . handle ( new   ContainersLauncherEvent ( this ,    launcherEvent )  )  ;", "}", "METHOD_END"], "methodName": ["sendLaunchEvent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . exitStatus ;", "}", "METHOD_END"], "methodName": ["getContainerExitStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerKillEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnostic ;", "}", "METHOD_END"], "methodName": ["getDiagnostic"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerKillEvent"}, {"methodBody": ["METHOD_START", "{", "return   rsrc ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerResourceEvent"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticMesage ;", "}", "METHOD_END"], "methodName": ["getDiagnosticMessage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerResourceFailedEvent"}, {"methodBody": ["METHOD_START", "{", "return   loc ;", "}", "METHOD_END"], "methodName": ["getLocation"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerResourceLocalizedEvent"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "for    ( int   i    =     ( r . nextInt (  5  )  )     +     5  ;    i    >  =     0  ;     -  - i )     {", "Map . Entry < String ,    LocalResource >    rsrc    =     . getMockRsrc ( r ,    PUBLIC )  ;", "localResources . put ( rsrc . getKey (  )  ,    rsrc . getValue (  )  )  ;", "}", "for    ( int   i    =     ( r . nextInt (  5  )  )     +     5  ;    i    >  =     0  ;     -  - i )     {", "Map . Entry < String ,    LocalResource >    rsrc    =     . getMockRsrc ( r ,    PRIVATE )  ;", "localResources . put ( rsrc . getKey (  )  ,    rsrc . getValue (  )  )  ;", "}", "for    ( int   i    =     ( r . nextInt (  2  )  )     +     2  ;    i    >  =     0  ;     -  - i )     {", "Map . Entry < String ,    LocalResource >    rsrc    =     . getMockRsrc ( r ,    APPLICATION )  ;", "localResources . put ( rsrc . getKey (  )  ,    rsrc . getValue (  )  )  ;", "}", "return   localResources ;", "}", "METHOD_END"], "methodName": ["createLocalResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    ByteBuffer >    serviceData    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "for    ( int   i    =     ( r . nextInt (  5  )  )     +     5  ;    i    >  =     0  ;     -  - i )     {", "String   service    =    Long . toHexString ( r . nextLong (  )  )  ;", "byte [  ]    b    =    new   byte [  ( r . nextInt (  1  0  2  4  )  )     +     1  0  2  4  ]  ;", "r . nextBytes ( b )  ;", "serviceData . put ( service ,    ByteBuffer . wrap ( b )  )  ;", "}", "return   serviceData ;", "}", "METHOD_END"], "methodName": ["createServiceData"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "String   name    =    Long . toHexString ( r . nextLong (  )  )  ;", "URL   url    =    BuilderUtils . newURL (  \" file \"  ,    null ,     0  ,     (  (  (  \"  / local \"     +    vis )     +     \"  /  \"  )     +    name )  )  ;", "LocalResource   rsrc    =    BuilderUtils . newLocalResource ( url ,    FILE ,    vis ,     (  ( r . nextInt (  1  0  2  4  )  )     +     1  0  2  4 L )  ,     (  ( r . nextInt (  1  0  2  4  )  )     +     2  0  4  8 L )  )  ;", "return   new   AbstractMap . SimpleEntry < String ,    LocalResource >  ( name ,    rsrc )  ;", "}", "METHOD_END"], "methodName": ["getMockRsrc"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  0  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . launchContainer (  )  ;", "reset ( wc . localizerBus )  ;", "wc . containerFailed ( ContainerExecutor . ExitCode . FORCE _ KILLED . getExitCode (  )  )  ;", "assertEquals ( ContainerState . EXITED _ WITH _ FAILURE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCleanupOnFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  2  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . launchContainer (  )  ;", "reset ( wc . localizerBus )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . containerKilledOnRequest (  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCleanupOnKillRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  1  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . launchContainer (  )  ;", "reset ( wc . localizerBus )  ;", "wc . containerSuccessful (  )  ;", "assertEquals ( ContainerState . EXITED _ WITH _ SUCCESS ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testCleanupOnSuccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  3  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . launchContainer (  )  ;", "reset ( wc . localizerBus )  ;", "wc . containerKilledOnRequest (  )  ;", "assertEquals ( ContainerState . EXITED _ WITH _ FAILURE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testExternalKill"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  6  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . launchContainer (  )  ;", "reset ( wc . localizerBus )  ;", "wc . containerSuccessful (  )  ;", "wc . containerResourcesCleanup (  )  ;", "assertEquals ( ContainerState . DONE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . initContainer (  )  ;", "assertEquals ( ContainerState . DONE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testInitWhileDone"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  5  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . failLocalizeResources ( wc . getLocalResourceCount (  )  )  ;", "assertEquals ( ContainerState . LOCALIZATION _ FAILED ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . LOCALIZATION _ FAILED ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testKillOnLocalizationFailed"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  7  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "assertEquals ( ContainerState . LOCALIZED ,    wc . c . getContainerState (  )  )  ;", "ContainerLaunch   launcher    =    wc . launcher . running . get ( wc . c . getContainerId (  )  )  ;", "launcher . call (  )  ;", "wc . drainDispatcherEvents (  )  ;", "assertEquals ( ContainerState . EXITED _ WITH _ FAILURE ,    wc . c . getContainerState (  )  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . EXITED _ WITH _ FAILURE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testKillOnLocalizedWhenContainerLaunched"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  7  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "assertEquals ( ContainerState . LOCALIZED ,    wc . c . getContainerState (  )  )  ;", "ContainerLaunch   launcher    =    wc . launcher . running . get ( wc . c . getContainerId (  )  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "launcher . call (  )  ;", "wc . drainDispatcherEvents (  )  ;", "assertEquals ( ContainerState . CONTAINER _ CLEANEDUP _ AFTER _ KILL ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "wc . c . handle ( new   ContainerEvent ( wc . c . getContainerId (  )  ,    ContainerEventType . CONTAINER _ RESOURCES _ CLEANEDUP )  )  ;", "assertEquals (  0  ,    metrics . getRunningContainers (  )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testKillOnLocalizedWhenContainerNotLaunched"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  4  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "assertEquals ( ContainerState . LOCALIZING ,    wc . c . getContainerState (  )  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "assertEquals ( KILLED _ BY _ RESOURCEMANAGER ,    wc . c . cloneAndGetContainerStatus (  )  . getExitStatus (  )  )  ;", "assertTrue ( wc . c . cloneAndGetContainerStatus (  )  . getDiagnostics (  )  . contains (  \" KillRequest \"  )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testKillOnLocalizing"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  3  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "assertEquals ( ContainerState . NEW ,    wc . c . getContainerState (  )  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . DONE ,    wc . c . getContainerState (  )  )  ;", "assertEquals ( KILLED _ BY _ RESOURCEMANAGER ,    wc . c . cloneAndGetContainerStatus (  )  . getExitStatus (  )  )  ;", "assertTrue ( wc . c . cloneAndGetContainerStatus (  )  . getDiagnostics (  )  . contains (  \" KillRequest \"  )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testKillOnNew"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  4  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . launchContainer (  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . containerKilledOnRequest (  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLaunchAfterKillRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  6  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "wc . localizeResources (  )  ;", "wc . launchContainer (  )  ;", "reset ( wc . localizerBus )  ;", "wc . containerSuccessful (  )  ;", "wc . containerResourcesCleanup (  )  ;", "assertEquals ( ContainerState . DONE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . resourceFailedContainer (  )  ;", "assertEquals ( ContainerState . DONE ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLocalizationFailureAtDone"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  8  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "assertEquals ( ContainerState . NEW ,    wc . c . getContainerState (  )  )  ;", "wc . initContainer (  )  ;", "Map < Path ,    List < String >  >    localPaths    =    wc . localizeResources (  )  ;", "assertEquals ( ContainerState . LOCALIZED ,    wc . c . getContainerState (  )  )  ;", "assertNotNull ( wc . c . getLocalizedResources (  )  )  ;", "for    ( Map . Entry < Path ,    List < String >  >    loc    :    wc . c . getLocalizedResources (  )  . entrySet (  )  )     {", "assertEquals ( localPaths . remove ( loc . getKey (  )  )  ,    loc . getValue (  )  )  ;", "}", "assertTrue ( localPaths . isEmpty (  )  )  ;", "final   TestContainer . WrappedContainer   wcf    =    wc ;", "ArgumentMatcher < ContainersLauncherEvent >    matchesContainerLaunch    =    new   ArgumentMatcher < ContainersLauncherEvent >  (  )     {", "@ Override", "public   boolean   matches ( Object   o )     {", "ContainersLauncherEvent   launchEvent    =     (  ( ContainersLauncherEvent )     ( o )  )  ;", "return    ( wcf . c )     =  =     ( launchEvent . getContainer (  )  )  ;", "}", "}  ;", "verify ( wc . launcherBus )  . handle ( argThat ( matchesContainerLaunch )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLocalizationLaunch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  7  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "assertEquals ( ContainerState . NEW ,    wc . c . getContainerState (  )  )  ;", "wc . initContainer (  )  ;", "TestContainer . ResourcesRequestedMatcher   matchesReq    =    new   TestContainer . ResourcesRequestedMatcher ( wc . localResources ,    EnumSet . of ( PUBLIC ,    PRIVATE ,    APPLICATION )  )  ;", "verify ( wc . localizerBus )  . handle ( argThat ( matchesReq )  )  ;", "assertEquals ( ContainerState . LOCALIZING ,    wc . c . getContainerState (  )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLocalizationRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  6  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "Iterator < String >    lRsrcKeys    =    wc . localResources . keySet (  )  . iterator (  )  ;", "String   key 1     =    lRsrcKeys . next (  )  ;", "wc . killContainer (  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . failLocalizeSpecificResource ( key 1  )  ;", "assertEquals ( ContainerState . KILLING ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testResourceFailedOnKilling"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  6  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "Iterator < String >    lRsrcKeys    =    wc . localResources . keySet (  )  . iterator (  )  ;", "String   key 1     =    lRsrcKeys . next (  )  ;", "String   key 2     =    lRsrcKeys . next (  )  ;", "wc . failLocalizeSpecificResource ( key 1  )  ;", "assertEquals ( ContainerState . LOCALIZATION _ FAILED ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . failLocalizeSpecificResource ( key 2  )  ;", "assertEquals ( ContainerState . LOCALIZATION _ FAILED ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testResourceFailedOnLocalizationFailed"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  1  6  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  )  ;", "wc . initContainer (  )  ;", "int   failCount    =     ( wc . getLocalResourceCount (  )  )     /     2  ;", "if    ( failCount    =  =     0  )     {", "failCount    =     1  ;", "}", "wc . failLocalizeResources ( failCount )  ;", "assertEquals ( ContainerState . LOCALIZATION _ FAILED ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "wc . localizeResourcesFromInvalidState ( failCount )  ;", "assertEquals ( ContainerState . LOCALIZATION _ FAILED ,    wc . c . getContainerState (  )  )  ;", "assertNull ( wc . c . getLocalizedResources (  )  )  ;", "verifyCleanupCall ( wc )  ;", "Assert . assertTrue ( wc . getDiagnostics (  )  . contains ( FAKE _ LOCALIZATION _ ERROR )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testResourceLocalizedOnLocalizationFailed"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . WrappedContainer   wc    =    null ;", "try    {", "wc    =    new   TestContainer . WrappedContainer (  9  ,     3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     4  3  4  4  ,     \" yak \"  ,    false ,    true )  ;", "assertEquals ( ContainerState . NEW ,    wc . c . getContainerState (  )  )  ;", "wc . initContainer (  )  ;", "for    ( final   Map . Entry < String ,    ByteBuffer >    e    :    wc . serviceData . entrySet (  )  )     {", "ArgumentMatcher < AuxServicesEvent >    matchesServiceReq    =    new   ArgumentMatcher < AuxServicesEvent >  (  )     {", "@ Override", "public   boolean   matches ( Object   o )     {", "AuxServicesEvent   evt    =     (  ( AuxServicesEvent )     ( o )  )  ;", "return    ( e . getKey (  )  . equals ( evt . getServiceID (  )  )  )     &  &     (  0     =  =     ( e . getValue (  )  . compareTo ( evt . getServiceData (  )  )  )  )  ;", "}", "}  ;", "verify ( wc . auxBus )  . handle ( argThat ( matchesServiceReq )  )  ;", "}", "final   TestContainer . WrappedContainer   wcf    =    wc ;", "ArgumentMatcher < ContainersLauncherEvent >    matchesLaunchReq    =    new   ArgumentMatcher < ContainersLauncherEvent >  (  )     {", "@ Override", "public   boolean   matches ( Object   o )     {", "ContainersLauncherEvent   evt    =     (  ( ContainersLauncherEvent )     ( o )  )  ;", "return    (  ( evt . getType (  )  )     =  =     ( ContainersLauncherEventType . LAUNCH _ CONTAINER )  )     &  &     (  ( wcf . cId )     =  =     ( evt . getContainer (  )  . getContainerId (  )  )  )  ;", "}", "}  ;", "verify ( wc . launcherBus )  . handle ( argThat ( matchesLaunchReq )  )  ;", "}    finally    {", "if    ( wc    !  =    null )     {", "wc . finished (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testServiceData"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "TestContainer . ResourcesReleasedMatcher   matchesReq    =    new   TestContainer . ResourcesReleasedMatcher ( wc . localResources ,    EnumSet . of ( PUBLIC ,    PRIVATE ,    APPLICATION )  )  ;", "verify ( wc . localizerBus )  . handle ( argThat ( matchesReq )  )  ;", "}", "METHOD_END"], "methodName": ["verifyCleanupCall"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.TestContainer"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    container . getContainerId (  )  ;", "String   containerIdStr    =    ConverterUtils . toString ( containerId )  ;", ". LOG . info (  (  \" Cleaning   up   container    \"     +    containerIdStr )  )  ;", "try    {", "context . getNMStateStore (  )  . storeContainerKilled ( containerId )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  \" Unable   to   mark   container    \"     +    containerId )     +     \"    killed   in   store \"  )  ,    e )  ;", "}", "boolean   alreadyLaunched    =     !  ( shouldLaunchContainer . compareAndSet ( false ,    true )  )  ;", "if    (  ! alreadyLaunched )     {", ". LOG . info (  (  (  (  \" Container    \"     +    containerIdStr )     +     \"    not   launched .  \"  )     +     \"    No   cleanup   needed   to   be   done \"  )  )  ;", "return ;", "}", ". LOG . debug (  (  (  \" Marking   container    \"     +    containerIdStr )     +     \"    as   inactive \"  )  )  ;", "exec . deactivateContainer ( containerId )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  \" Getting   pid   for   container    \"     +    containerIdStr )     +     \"    to   kill \"  )     +     \"    from   pid   file    \"  )     +     (  ( pidFilePath )     !  =    null    ?    pidFilePath . toString (  )     :     \" null \"  )  )  )  ;", "}", "try    {", "String   processId    =    null ;", "if    (  ( pidFilePath )     !  =    null )     {", "processId    =    getContainerPid ( pidFilePath )  ;", "}", "if    ( processId    !  =    null )     {", "String   user    =    container . getUser (  )  ;", ". LOG . debug (  (  (  (  (  (  \" Sending   signal   to   pid    \"     +    processId )     +     \"    as   user    \"  )     +    user )     +     \"    for   container    \"  )     +    containerIdStr )  )  ;", "final   ContainerExecutor . Signal   signal    =     (  ( sleepDelayBeforeSigKill )     >     0  )     ?    ContainerExecutor . Signal . TERM    :    ContainerExecutor . Signal . KILL ;", "boolean   result    =    exec . signalContainer ( user ,    processId ,    signal )  ;", ". LOG . debug (  (  (  (  (  (  (  (  (  (  \" Sent   signal    \"     +    signal )     +     \"    to   pid    \"  )     +    processId )     +     \"    as   user    \"  )     +    user )     +     \"    for   container    \"  )     +    containerIdStr )     +     \"  ,    result =  \"  )     +     ( result    ?     \" success \"     :     \" failed \"  )  )  )  ;", "if    (  ( sleepDelayBeforeSigKill )     >     0  )     {", "new   ContainerExecutor . DelayedProcessKiller ( container ,    user ,    processId ,    sleepDelayBeforeSigKill ,    ContainerExecutor . Signal . KILL ,    exec )  . start (  )  ;", "}", "}", "}    catch    ( Exception   e )     {", "String   message    =     (  (  \" Exception   when   trying   to   cleanup   container    \"     +    containerIdStr )     +     \"  :     \"  )     +     ( StringUtils . stringifyException ( e )  )  ;", ". LOG . warn ( message )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ContainerDiagnosticsUpdateEvent ( containerId ,    message )  )  ;", "}    finally    {", "if    (  ( pidFilePath )     !  =    null )     {", "FileContext   lfs    =    FileContext . getLocalFSFileContext (  )  ;", "lfs . delete ( pidFilePath ,    false )  ;", "lfs . delete ( pidFilePath . suffix (  . EXIT _ CODE _ FILE _ SUFFIX )  ,    false )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["cleanupContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "var    =    var . replace ( LOG _ DIR _ EXPANSION _ VAR ,    containerLogDir . toString (  )  )  ;", "var    =    var . replace ( CLASS _ PATH _ SEPARATOR ,    File . pathSeparator )  ;", "if    ( Shell . WINDOWS )     {", "var    =    var . replaceAll (  \"  (  \\  \\  {  \\  \\  {  )  |  (  \\  \\  }  \\  \\  }  )  \"  ,     \"  %  \"  )  ;", "} else    {", "var    =    var . replace ( PARAMETER _ EXPANSION _ LEFT ,     \"  $  \"  )  ;", "var    =    var . replace ( PARAMETER _ EXPANSION _ RIGHT ,     \"  \"  )  ;", "}", "return   var ;", "}", "METHOD_END"], "methodName": ["expandEnvironment"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourceLocalizationService . NM _ PRIVATE _ DIR )     +     ( Path . SEPARATOR )  )     +    appIdStr ;", "}", "METHOD_END"], "methodName": ["getAppPrivateDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "String   containerIdStr    =    ConverterUtils . toString ( container . getContainerId (  )  )  ;", "String   processId    =    null ;", ". LOG . debug (  (  (  (  \" Accessing   pid   for   container    \"     +    containerIdStr )     +     \"    from   pid   file    \"  )     +    pidFilePath )  )  ;", "int   sleepCounter    =     0  ;", "final   int   sleepInterval    =     1  0  0  ;", "while    (  !  ( completed . get (  )  )  )     {", "processId    =    ProcessIdFileReader . getProcessId ( pidFilePath )  ;", "if    ( processId    !  =    null )     {", ". LOG . debug (  (  (  (  \" Got   pid    \"     +    processId )     +     \"    for   container    \"  )     +    containerIdStr )  )  ;", "break ;", "} else", "if    (  ( sleepCounter    *    sleepInterval )     >     ( maxKillWaitTime )  )     {", ". LOG . info (  (  (  (  (  \" Could   not   get   pid   for    \"     +    containerIdStr )     +     \"  .    Waited   for    \"  )     +     ( maxKillWaitTime )  )     +     \"    ms .  \"  )  )  ;", "break ;", "} else    {", "+  + sleepCounter ;", "Thread . sleep ( sleepInterval )  ;", "}", "}", "return   processId ;", "}", "METHOD_END"], "methodName": ["getContainerPid"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return    (  (  ( getAppPrivateDir ( appIdStr )  )     +     ( Path . SEPARATOR )  )     +    containerIdStr )     +     ( Path . SEPARATOR )  ;", "}", "METHOD_END"], "methodName": ["getContainerPrivateDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return   context ;", "}", "METHOD_END"], "methodName": ["getContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return   pidFile    +     ( ContainerLaunch . EXIT _ CODE _ FILE _ SUFFIX )  ;", "}", "METHOD_END"], "methodName": ["getExitCodeFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return    (  ( getContainerPrivateDir ( appIdStr ,    containerIdStr )  )     +     ( Path . SEPARATOR )  )     +     ( String . format ( ContainerLaunch . PID _ FILE _ NAME _ FMT ,    containerIdStr )  )  ;", "}", "METHOD_END"], "methodName": ["getPidFileSubpath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return    ( appIdStr    +     ( Path . SEPARATOR )  )     +    containerIdStr ;", "}", "METHOD_END"], "methodName": ["getRelativeContainerLogDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "if    (  ( environment . get ( variable )  )     =  =    null )     {", ". putEnvIfNotNull ( environment ,    variable ,    System . getenv ( variable )  )  ;", "}", "}", "METHOD_END"], "methodName": ["putEnvIfAbsent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "if    ( value    !  =    null )     {", "envirment . put ( variable ,    value )  ;", "}", "}", "METHOD_END"], "methodName": ["putEnvIfNotNull"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "environment . put ( CONTAINER _ ID . name (  )  ,    container . getContainerId (  )  . toString (  )  )  ;", "environment . put ( NM _ PORT . name (  )  ,    String . valueOf ( this . context . getNodeId (  )  . getPort (  )  )  )  ;", "environment . put ( NM _ HOST . name (  )  ,    this . context . getNodeId (  )  . getHost (  )  )  ;", "environment . put ( NM _ HTTP _ PORT . name (  )  ,    String . valueOf ( this . context . getHttpPort (  )  )  )  ;", "environment . put ( LOCAL _ DIRS . name (  )  ,    StringUtils . join (  \"  ,  \"  ,    appDirs )  )  ;", "environment . put ( LOG _ DIRS . name (  )  ,    StringUtils . join (  \"  ,  \"  ,    containerLogDirs )  )  ;", "environment . put ( USER . name (  )  ,    container . getUser (  )  )  ;", "environment . put ( LOGNAME . name (  )  ,    container . getUser (  )  )  ;", "environment . put ( HOME . name (  )  ,    conf . get ( NM _ USER _ HOME _ DIR ,    DEFAULT _ NM _ USER _ HOME _ DIR )  )  ;", "environment . put ( PWD . name (  )  ,    pwd . toString (  )  )  ;", ". putEnvIfNotNull ( environment ,    HADOOP _ CONF _ DIR . name (  )  ,    System . getenv ( HADOOP _ CONF _ DIR . name (  )  )  )  ;", "if    (  !  ( Shell . WINDOWS )  )     {", "environment . put (  \" JVM _ PID \"  ,     \"  $  $  \"  )  ;", "}", "String [  ]    whitelist    =    conf . get ( NM _ ENV _ WHITELIST ,    DEFAULT _ NM _ ENV _ WHITELIST )  . split (  \"  ,  \"  )  ;", "for    ( String   whitelistEnvVariable    :    whitelist )     {", ". putEnvIfAbsent ( environment ,    whitelistEnvVariable . trim (  )  )  ;", "}", "Apps . setEnvFromInputString ( environment ,    conf . get ( NM _ ADMIN _ USER _ ENV ,    DEFAULT _ NM _ ADMIN _ USER _ ENV )  ,    File . pathSeparator )  ;", "if    ( Shell . WINDOWS )     {", "String   inputClassPath    =    environment . get ( CLASSPATH . name (  )  )  ;", "if    (  ( inputClassPath    !  =    null )     &  &     (  !  ( inputClassPath . isEmpty (  )  )  )  )     {", "StringBuilder   newClassPath    =    new   StringBuilder ( inputClassPath )  ;", "for    ( Map . Entry < Path ,    List < String >  >    entry    :    resources . entrySet (  )  )     {", "boolean   targetIsDirectory    =    new   File ( entry . getKey (  )  . toUri (  )  . getPath (  )  )  . isDirectory (  )  ;", "for    ( String   linkName    :    entry . getValue (  )  )     {", "newClassPath . append ( File . pathSeparator )  . append ( pwd . toString (  )  )  . append ( SEPARATOR )  . append ( linkName )  ;", "if    ( targetIsDirectory )     {", "newClassPath . append ( SEPARATOR )  ;", "}", "}", "}", "Map < String ,    String >    mergedEnv    =    new   HashMap < String ,    String >  ( System . getenv (  )  )  ;", "mergedEnv . putAll ( environment )  ;", "String   classPathJar    =    FileUtil . createJarWithClassPath ( newClassPath . toString (  )  ,    pwd ,    mergedEnv )  ;", "environment . put ( CLASSPATH . name (  )  ,    classPathJar )  ;", "}", "}", "for    ( Map . Entry < String ,    ByteBuffer >    meta    :    containerManager . getAuxServiceMetaData (  )  . entrySet (  )  )     {", "AuxiliaryServiceHelper . setServiceDataIntoEnv ( meta . getKey (  )  ,    meta . getValue (  )  ,    environment )  ;", "}", "}", "METHOD_END"], "methodName": ["sanitizeEnv"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunch . ShellScriptBuilder   sb    =    ContainerLaunch . ShellScriptBuilder . create (  )  ;", "if    ( environment    !  =    null )     {", "for    ( Map . Entry < String ,    String >    env    :    environment . entrySet (  )  )     {", "sb . env ( env . getKey (  )  . toString (  )  ,    env . getValue (  )  . toString (  )  )  ;", "}", "}", "if    ( resources    !  =    null )     {", "for    ( Map . Entry < Path ,    List < String >  >    entry    :    resources . entrySet (  )  )     {", "for    ( String   linkName    :    entry . getValue (  )  )     {", "sb . symlink ( entry . getKey (  )  ,    new   Path ( linkName )  )  ;", "}", "}", "}", "sb . command ( command )  ;", "PrintStream   pout    =    null ;", "try    {", "pout    =    new   PrintStream ( out )  ;", "sb . write ( pout )  ;", "}    finally    {", "if    ( out    !  =    null )     {", "out . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["writeLaunchEnv"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEvent"}, {"methodBody": ["METHOD_START", "{", "String   pidSubpath    =    getPidFileSubpath ( appIdStr ,    containerIdStr )  ;", "for    ( String   dir    :    getContext (  )  . getLocalDirsHandler (  )  . getLocalDirs (  )  )     {", "File   pidFile    =    new   File ( dir ,    pidSubpath )  ;", "if    ( pidFile . exists (  )  )     {", "return   pidFile ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["locatePidFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "Resource   r    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "ContainerTokenIdentifier   containerTokenIdentifier    =    new   ContainerTokenIdentifier ( cId ,    context . getNodeId (  )  . toString (  )  ,    user ,    r ,     (  ( System . currentTimeMillis (  )  )     +     1  0  0  0  0 L )  ,     1  2  3  ,    DUMMY _ RM _ IDENTIFIER ,    priority ,    createTime )  ;", "Token   containerToken    =    BuilderUtils . newContainerToken ( context . getNodeId (  )  ,    context . geTokenSecretManager (  )  . retrievePassword ( containerTokenIdentifier )  ,    containerTokenIdentifier )  ;", "return   containerToken ;", "}", "METHOD_END"], "methodName": ["createContainerToken"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "conf . setLong ( NM _ SLEEP _ DELAY _ BEFORE _ SIGKILL _ MS ,     ( delayed    ?     1  0  0  0     :     0  )  )  ;", "containerManager . start (  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   cId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "File   processStartFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" pid . txt \"  )  . getAbsoluteFile (  )  ;", "File   scriptFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" testscript \"  )  ;", "PrintWriter   writer    =    new   PrintWriter ( new   FileOutputStream ( scriptFile )  )  ;", "if    ( Shell . WINDOWS )     {", "writer . println (  \"  @ echo    \\  \" Running   testscript   for   delayed   kill \\  \"  \"  )  ;", "writer . println (  \"  @ echo    \\  \" Writing   pid   to   start   file \\  \"  \"  )  ;", "writer . println (  (  (  (  \"  @ echo    \"     +    cId )     +     \"  >     \"  )     +    processStartFile )  )  ;", "writer . println (  \"  @ ping    - n    1  0  0     1  2  7  .  0  .  0  .  1     > nul \"  )  ;", "} else    {", "writer . println (  \"  #  !  / bin / bash \\ n \\ n \"  )  ;", "writer . println (  \" echo    \\  \" Running   testscript   for   delayed   kill \\  \"  \"  )  ;", "writer . println (  \" hello =  \\  \" Got   SIGTERM \\  \"  \"  )  ;", "writer . println (  \" umask    0  \"  )  ;", "writer . println (  (  (  \" trap    \\  \" echo    $ hello    >  >     \"     +    processStartFile )     +     \"  \\  \"    SIGTERM \"  )  )  ;", "writer . println (  \" echo    \\  \" Writing   pid   to   start   file \\  \"  \"  )  ;", "writer . println (  (  \" echo    $  $     >  >     \"     +    processStartFile )  )  ;", "writer . println (  \" while   true ;    do \\ nsleep    1 s ;  \\ ndone \"  )  ;", "}", "writer . close (  )  ;", "FileUtil . setExecutable ( scriptFile ,    true )  ;", "Context   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( Context . class )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file . sh \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    Arrays . asList ( Shell . getRunScriptCommand ( scriptFile )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "Priority   priority    =    Priority . newInstance (  1  0  )  ;", "long   createTime    =     1  2  3  4  ;", "Token   containerToken    =    createContainerToken ( cId ,    priority ,    createTime )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( processStartFile . exists (  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "BaseContainerManagerTest . LOG . info (  \" Waiting   for   process   start - file   to   be   created \"  )  ;", "}", "Assert . assertTrue (  \" ProcessStartFile   doesn ' t   exist !  \"  ,    processStartFile . exists (  )  )  ;", "NMContainerStatus   nmContainerStatus    =    containerManager . getContext (  )  . getContainers (  )  . get ( cId )  . getNMContainerStatus (  )  ;", "Assert . assertEquals ( priority ,    nmContainerStatus . getPriority (  )  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "StopContainersRequest   stopRequest    =    StopContainersRequest . newInstance ( containerIds )  ;", "containerManager . stopContainers ( stopRequest )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE )  ;", "GetContainerStatusesRequest   gcsRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( gcsRequest )  . getContainerStatuses (  )  . get (  0  )  ;", "Assert . assertEquals ( KILLED _ BY _ APPMASTER ,    containerStatus . getExitStatus (  )  )  ;", "if    (  ( Shell . WINDOWS )     |  |     (  ! delayed )  )     {", "Assert . assertFalse (  \" Process   is   still   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( cId . toString (  )  )  )  ;", "} else    {", "BufferedReader   reader    =    new   BufferedReader ( new   FileReader ( processStartFile )  )  ;", "boolean   foundSigTermMessage    =    false ;", "while    ( true )     {", "String   line    =    reader . readLine (  )  ;", "if    ( line    =  =    null )     {", "break ;", "}", "if    ( line . contains (  \" SIGTERM \"  )  )     {", "foundSigTermMessage    =    true ;", "break ;", "}", "}", "Assert . assertTrue (  \" Did   not   find   sigterm   message \"  ,    foundSigTermMessage )  ;", "reader . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["internalKillTest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "conf . setClass ( NM _ CONTAINER _ MON _ RESOURCE _ CALCULATOR ,    LinuxResourceCalculatorPlugin . class ,    ResourceCalculatorPlugin . class )  ;", "super . setup (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "String   iceName    =     \" testAuxiliaryService \"  ;", "ByteBuffer   bb    =    ByteBuffer . wrap (  \" testAuxiliaryService \"  . getBytes (  )  )  ;", "AuxiliaryServiceHelper . setServiceDataIntoEnv ( iceName ,    bb ,    env )  ;", "Assert . assertEquals ( bb ,    AuxiliaryServiceHelper . getServiceDataFromEnv ( iceName ,    env )  )  ;", "}", "METHOD_END"], "methodName": ["testAuxiliaryServiceHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    mock ( Container . class )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance ( System . currentTimeMillis (  )  ,     1  )  ,     1  )  ,     1  )  )  ;", "Context   clc    =    mock ( Context . class )  ;", "when ( clc . getCommands (  )  )  . thenReturn ( Collections .  < String > emptyList (  )  )  ;", "when ( container . getLaunchContext (  )  )  . thenReturn ( clc )  ;", "when ( container . getLocalizedResources (  )  )  . thenReturn ( null )  ;", "Dispatcher   dispatcher    =    mock ( Dispatcher . class )  ;", "EventHandler   eventHandler    =    new   EventHandler (  )     {", "public   void   handle ( Event   event )     {", "Assert . assertTrue (  ( event   instanceof   ContainerExitEvent )  )  ;", "ContainerExitEvent   exitEvent    =     (  ( ContainerExitEvent )     ( event )  )  ;", "Assert . assertEquals ( ContainerEventType . CONTAINER _ EXITED _ WITH _ FAILURE ,    exitEvent . getType (  )  )  ;", "}", "}  ;", "when ( dispatcher . getEventHandler (  )  )  . thenReturn ( eventHandler )  ;", "launch    =    new    ( context ,    new   Configuration (  )  ,    dispatcher ,    exec ,    null ,    container ,    dirsHandler ,    containerManager )  ;", "launch . call (  )  ;", "}", "METHOD_END"], "methodName": ["testCallFailureWithNullLocalizedResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "containerManager . start (  )  ;", "Context   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( Context . class )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   cId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "Map < String ,    String >    userSetEnv    =    new   HashMap < String ,    String >  (  )  ;", "userSetEnv . put ( CONTAINER _ ID . name (  )  ,     \" user _ set _ container _ id \"  )  ;", "userSetEnv . put ( NM _ HOST . name (  )  ,     \" user _ set _ NM _ HOST \"  )  ;", "userSetEnv . put ( NM _ PORT . name (  )  ,     \" user _ set _ NM _ PORT \"  )  ;", "userSetEnv . put ( NM _ HTTP _ PORT . name (  )  ,     \" user _ set _ NM _ HTTP _ PORT \"  )  ;", "userSetEnv . put ( LOCAL _ DIRS . name (  )  ,     \" user _ set _ LOCAL _ DIR \"  )  ;", "userSetEnv . put ( USER . key (  )  ,     (  \" user _ set _  \"     +     ( USER . key (  )  )  )  )  ;", "userSetEnv . put ( LOGNAME . name (  )  ,     \" user _ set _ LOGNAME \"  )  ;", "userSetEnv . put ( PWD . name (  )  ,     \" user _ set _ PWD \"  )  ;", "userSetEnv . put ( HOME . name (  )  ,     \" user _ set _ HOME \"  )  ;", "containerLaunchContext . setEnvironment ( userSetEnv )  ;", "File   scriptFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" scriptFile \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( scriptFile )  ;", "File   processStartFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" env _ vars . txt \"  )  . getAbsoluteFile (  )  ;", "if    ( Shell . WINDOWS )     {", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( CONTAINER _ ID .  $  (  )  )  )     +     \"  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( NM _ HOST .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( NM _ PORT .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( NM _ HTTP _ PORT .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( LOCAL _ DIRS .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( USER .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( LOGNAME .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( PWD .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  (  (  (  \"  @ echo    \"     +     ( HOME .  $  (  )  )  )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "for    ( String   serviceName    :    containerManager . getAuxServiceMetaData (  )  . keySet (  )  )     {", "fileWriter . println (  (  (  (  (  \"  @ echo    %  \"     +     ( AuxiliaryServiceHelper . NM _ AUX _ SERVICE )  )     +    serviceName )     +     \"  %  >  >     \"  )     +    processStartFile )  )  ;", "}", "fileWriter . println (  (  (  (  \"  @ echo    \"     +    cId )     +     \"  >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . println (  \"  @ ping    - n    1  0  0     1  2  7  .  0  .  0  .  1     > nul \"  )  ;", "} else    {", "fileWriter . write (  \"  \\ numask    0  \"  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( CONTAINER _ ID . name (  )  )  )     +     \"     >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( NM _ HOST . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( NM _ PORT . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( NM _ HTTP _ PORT . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( LOCAL _ DIRS . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( USER . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( LOGNAME . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( PWD . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "fileWriter . write (  (  (  (  \"  \\ necho    $  \"     +     ( HOME . name (  )  )  )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "for    ( String   serviceName    :    containerManager . getAuxServiceMetaData (  )  . keySet (  )  )     {", "fileWriter . write (  (  (  (  (  \"  \\ necho    $  \"     +     ( AuxiliaryServiceHelper . NM _ AUX _ SERVICE )  )     +    serviceName )     +     \"     >  >     \"  )     +    processStartFile )  )  ;", "}", "fileWriter . write (  (  \"  \\ necho    $  $     >  >     \"     +    processStartFile )  )  ;", "fileWriter . write (  \"  \\ nexec   sleep    1  0  0  \"  )  ;", "}", "fileWriter . close (  )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    Arrays . asList ( Shell . getRunScriptCommand ( scriptFile )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    createContainerToken ( cId ,    Priority . newInstance (  0  )  ,     0  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "containerManager . startContainers ( allRequests )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( processStartFile . exists (  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "BaseContainerManagerTest . LOG . info (  \" Waiting   for   process   start - file   to   be   created \"  )  ;", "}", "Assert . assertTrue (  \" ProcessStartFile   doesn ' t   exist !  \"  ,    processStartFile . exists (  )  )  ;", "List < String >    localDirs    =    dirsHandler . getLocalDirs (  )  ;", "List < String >    logDirs    =    dirsHandler . getLogDirs (  )  ;", "List < Path >    appDirs    =    new   ArrayList < Path >  ( localDirs . size (  )  )  ;", "for    ( String   localDir    :    localDirs )     {", "Path   usersdir    =    new   Path ( localDir ,    ContainerLocalizer . USERCACHE )  ;", "Path   userdir    =    new   Path ( usersdir ,    user )  ;", "Path   appsdir    =    new   Path ( userdir ,    ContainerLocalizer . APPCACHE )  ;", "appDirs . add ( new   Path ( appsdir ,    appId . toString (  )  )  )  ;", "}", "List < String >    containerLogDirs    =    new   ArrayList < String >  (  )  ;", "String   relativeContainerLogDir    =     . getRelativeContainerLogDir ( appId . toString (  )  ,    cId . toString (  )  )  ;", "for    ( String   logDir    :    logDirs )     {", "containerLogDirs . add (  (  ( logDir    +     ( Path . SEPARATOR )  )     +    relativeContainerLogDir )  )  ;", "}", "BufferedReader   reader    =    new   BufferedReader ( new   FileReader ( processStartFile )  )  ;", "Assert . assertEquals ( cId . toString (  )  ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( context . getNodeId (  )  . getHost (  )  ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( String . valueOf ( context . getNodeId (  )  . getPort (  )  )  ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( String . valueOf ( BaseContainerManagerTest . HTTP _ PORT )  ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( StringUtils . join (  \"  ,  \"  ,    appDirs )  ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( user ,    reader . readLine (  )  )  ;", "Assert . assertEquals ( user ,    reader . readLine (  )  )  ;", "String   obtainedPWD    =    reader . readLine (  )  ;", "boolean   found    =    false ;", "for    ( Path   localDir    :    appDirs )     {", "if    ( new   Path ( localDir ,    cId . toString (  )  )  . toString (  )  . equals ( obtainedPWD )  )     {", "found    =    true ;", "break ;", "}", "}", "Assert . assertTrue (  (  \" Wrong   local - dir   found    :     \"     +    obtainedPWD )  ,    found )  ;", "Assert . assertEquals ( conf . get ( NM _ USER _ HOME _ DIR ,    DEFAULT _ NM _ USER _ HOME _ DIR )  ,    reader . readLine (  )  )  ;", "for    ( String   serviceName    :    containerManager . getAuxServiceMetaData (  )  . keySet (  )  )     {", "Assert . assertEquals ( containerManager . getAuxServiceMetaData (  )  . get ( serviceName )  ,    ByteBuffer . wrap ( Base 6  4  . decodeBase 6  4  ( reader . readLine (  )  . getBytes (  )  )  )  )  ;", "}", "Assert . assertEquals ( cId . toString (  )  ,    containerLaunchContext . getEnvironment (  )  . get ( CONTAINER _ ID . name (  )  )  )  ;", "Assert . assertEquals ( context . getNodeId (  )  . getHost (  )  ,    containerLaunchContext . getEnvironment (  )  . get ( NM _ HOST . name (  )  )  )  ;", "Assert . assertEquals ( String . valueOf ( context . getNodeId (  )  . getPort (  )  )  ,    containerLaunchContext . getEnvironment (  )  . get ( NM _ PORT . name (  )  )  )  ;", "Assert . assertEquals ( String . valueOf ( BaseContainerManagerTest . HTTP _ PORT )  ,    containerLaunchContext . getEnvironment (  )  . get ( NM _ HTTP _ PORT . name (  )  )  )  ;", "Assert . assertEquals ( StringUtils . join (  \"  ,  \"  ,    appDirs )  ,    containerLaunchContext . getEnvironment (  )  . get ( LOCAL _ DIRS . name (  )  )  )  ;", "Assert . assertEquals ( StringUtils . join (  \"  ,  \"  ,    containerLogDirs )  ,    containerLaunchContext . getEnvironment (  )  . get ( LOG _ DIRS . name (  )  )  )  ;", "Assert . assertEquals ( user ,    containerLaunchContext . getEnvironment (  )  . get ( USER . name (  )  )  )  ;", "Assert . assertEquals ( user ,    containerLaunchContext . getEnvironment (  )  . get ( LOGNAME . name (  )  )  )  ;", "found    =    false ;", "obtainedPWD    =    containerLaunchContext . getEnvironment (  )  . get ( PWD . name (  )  )  ;", "for    ( Path   localDir    :    appDirs )     {", "if    ( new   Path ( localDir ,    cId . toString (  )  )  . toString (  )  . equals ( obtainedPWD )  )     {", "found    =    true ;", "break ;", "}", "}", "Assert . assertTrue (  (  \" Wrong   local - dir   found    :     \"     +    obtainedPWD )  ,    found )  ;", "Assert . assertEquals ( conf . get ( NM _ USER _ HOME _ DIR ,    DEFAULT _ NM _ USER _ HOME _ DIR )  ,    containerLaunchContext . getEnvironment (  )  . get ( HOME . name (  )  )  )  ;", "String   pid    =    reader . readLine (  )  . trim (  )  ;", "Assert . assertEquals ( null ,    reader . readLine (  )  )  ;", "Assert . assertTrue (  \" Process   is   not   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( pid )  )  ;", "Assert . assertTrue (  \" Process   is   not   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( pid )  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "StopContainersRequest   stopRequest    =    StopContainersRequest . newInstance ( containerIds )  ;", "containerManager . stopContainers ( stopRequest )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE )  ;", "GetContainerStatusesRequest   gcsRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( gcsRequest )  . getContainerStatuses (  )  . get (  0  )  ;", "int   expectedExitCode    =    ContainerExitStatus . KILLED _ BY _ APPMASTER ;", "Assert . assertEquals ( expectedExitCode ,    containerStatus . getExitStatus (  )  )  ;", "Assert . assertFalse (  \" Process   is   still   alive !  \"  ,    DefaultContainerExecutor . containerIsAlive ( pid )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerEnvVariables"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "File   shellFile    =    null ;", "try    {", "shellFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" hello \"  )  ;", "String   command    =     ( Shell . WINDOWS )     ?     \"  @ echo    \\  \" hello \\  \"     &     @ echo    \\  \" error \\  \"     1  >  &  2     &    exit    / b    2  \"     :     \" echo    \\  \" hello \\  \"  ;    echo    \\  \" error \\  \"     1  >  &  2  ;    exit    2  ;  \"  ;", "PrintWriter   writer    =    new   PrintWriter ( new   FileOutputStream ( shellFile )  )  ;", "FileUtil . setExecutable ( shellFile ,    true )  ;", "writer . println ( command )  ;", "writer . close (  )  ;", "Map < Path ,    List < String >  >    resources    =    new   HashMap < Path ,    List < String >  >  (  )  ;", "FileOutputStream   fos    =    new   FileOutputStream ( shellFile ,    true )  ;", "Map < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "commands . add ( command )  ;", ". writeLaunchEnv ( fos ,    env ,    resources ,    commands )  ;", "fos . flush (  )  ;", "fos . close (  )  ;", "Shell . ShellCommandExecutor   shexc    =    new   Shell . ShellCommandExecutor ( new   String [  ]  {    shellFile . getAbsolutePath (  )     }  ,    BaseContainerManagerTest . tmpDir )  ;", "String   diagnostics    =    null ;", "try    {", "shexc . execute (  )  ;", "Assert . fail (  \" Should   catch   exception \"  )  ;", "}    catch    ( ExitCodeException   e )     {", "diagnostics    =    e . getMessage (  )  ;", "}", "Assert . assertTrue ( diagnostics . contains (  \" error \"  )  )  ;", "Assert . assertTrue ( shexc . getOutput (  )  . contains (  \" hello \"  )  )  ;", "Assert . assertTrue (  (  ( shexc . getExitCode (  )  )     =  =     2  )  )  ;", "}    finally    {", "if    (  ( shellFile    !  =    null )     &  &     ( shellFile . exists (  )  )  )     {", "shellFile . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testContainerLaunchStdoutAndStderrDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "internalKillTest ( true )  ;", "}", "METHOD_END"], "methodName": ["testDelayedKill"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "Path   logPath    =    new   Path (  \"  / nm / container / logs \"  )  ;", "String   input    =     (  (  (  (  (  (  ( Apps . crossPlatformify (  \" HADOOP _ HOME \"  )  )     +     \"  / share / hadoop / common /  *  \"  )     +     ( ApplicationConstants . CLASS _ PATH _ SEPARATOR )  )     +     ( Apps . crossPlatformify (  \" HADOOP _ HOME \"  )  )  )     +     \"  / share / hadoop / common / lib /  *  \"  )     +     ( ApplicationConstants . CLASS _ PATH _ SEPARATOR )  )     +     ( Apps . crossPlatformify (  \" HADOOP _ LOG _ HOME \"  )  )  )     +     ( ApplicationConstants . LOG _ DIR _ EXPANSION _ VAR )  ;", "String   res    =     . expandEnvironment ( input ,    logPath )  ;", "if    ( Shell . WINDOWS )     {", "Assert . assertEquals (  (  \"  % HADOOP _ HOME %  / share / hadoop / common /  *  ;  \"     +     (  \"  % HADOOP _ HOME %  / share / hadoop / common / lib /  *  ;  \"     +     \"  % HADOOP _ LOG _ HOME %  / nm / container / logs \"  )  )  ,    res )  ;", "} else    {", "Assert . assertEquals (  (  \"  $ HADOOP _ HOME / share / hadoop / common /  *  :  \"     +     (  \"  $ HADOOP _ HOME / share / hadoop / common / lib /  *  :  \"     +     \"  $ HADOOP _ LOG _ HOME / nm / container / logs \"  )  )  ,    res )  ;", "}", "System . out . println ( res )  ;", "}", "METHOD_END"], "methodName": ["testEnvExpansion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "internalKillTest ( false )  ;", "}", "METHOD_END"], "methodName": ["testImmediateKill"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "File   shellFile    =    null ;", "try    {", "shellFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" hello \"  )  ;", "Map < Path ,    List < String >  >    resources    =    new   HashMap < Path ,    List < String >  >  (  )  ;", "FileOutputStream   fos    =    new   FileOutputStream ( shellFile )  ;", "FileUtil . setExecutable ( shellFile ,    true )  ;", "Map < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "env . put (  \" APPLICATION _ WORKFLOW _ CONTEXT \"  ,     (  \"  {  \\  \" workflowId \\  \"  :  \\  \"  6  0  9 f 9  1 c 5 cd 8  3  \\  \"  ,  \"     +     (  \"  \\  \" workflowName \\  \"  :  \\  \"  \\ n \\ ninsert   table    \"     +     \"  \\ npartition    ( cd _ education _ status )  \\ nselect   cd _ demo _ sk ,    cd _ gender ,     \"  )  )  )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", ". writeLaunchEnv ( fos ,    env ,    resources ,    commands )  ;", "fos . flush (  )  ;", "fos . close (  )  ;", "Map < String ,    String >    cmdEnv    =    new   HashMap < String ,    String >  (  )  ;", "cmdEnv . put (  \" LANG \"  ,     \" C \"  )  ;", "Shell . ShellCommandExecutor   shexc    =    new   Shell . ShellCommandExecutor ( new   String [  ]  {    shellFile . getAbsolutePath (  )     }  ,    BaseContainerManagerTest . tmpDir ,    cmdEnv )  ;", "String   diagnostics    =    null ;", "try    {", "shexc . execute (  )  ;", "Assert . fail (  \" Should   catch   exception \"  )  ;", "}    catch    ( ExitCodeException   e )     {", "diagnostics    =    e . getMessage (  )  ;", "}", "Assert . assertTrue ( diagnostics . contains (  ( Shell . WINDOWS    ?     \" is   not   recognized   as   an   internal   or   external   command \"     :     \" command   not   found \"  )  )  )  ;", "Assert . assertTrue (  (  ( shexc . getExitCode (  )  )     !  =     0  )  )  ;", "}    finally    {", "if    (  ( shellFile    !  =    null )     &  &     ( shellFile . exists (  )  )  )     {", "shellFile . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testInvalidEnvSyntaxDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "File   shellFile    =    null ;", "File   tempFile    =    null ;", "String   symLink    =     ( Shell . WINDOWS )     ?     \" test . cmd \"     :     \" test \"  ;", "File   symLinkFile    =    null ;", "try    {", "shellFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" hello \"  )  ;", "tempFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" temp \"  )  ;", "String   timeoutCommand    =     ( Shell . WINDOWS )     ?     \"  @ echo    \\  \" hello \\  \"  \"     :     \" echo    \\  \" hello \\  \"  \"  ;", "PrintWriter   writer    =    new   PrintWriter ( new   FileOutputStream ( shellFile )  )  ;", "FileUtil . setExecutable ( shellFile ,    true )  ;", "writer . println ( timeoutCommand )  ;", "writer . close (  )  ;", "Map < Path ,    List < String >  >    resources    =    new   HashMap < Path ,    List < String >  >  (  )  ;", "Path   invalidPath    =    new   Path (  (  ( shellFile . getAbsolutePath (  )  )     +     \" randomPath \"  )  )  ;", "resources . put ( invalidPath ,    Arrays . asList ( symLink )  )  ;", "FileOutputStream   fos    =    new   FileOutputStream ( tempFile )  ;", "Map < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "if    ( Shell . WINDOWS )     {", "commands . add (  \" cmd \"  )  ;", "commands . add (  \"  / c \"  )  ;", "commands . add (  (  (  \"  \\  \"  \"     +    symLink )     +     \"  \\  \"  \"  )  )  ;", "} else    {", "commands . add (  (  (  \"  / bin / sh    .  /  \\  \\  \\  \"  \"     +    symLink )     +     \"  \\  \\  \\  \"  \"  )  )  ;", "}", ". writeLaunchEnv ( fos ,    env ,    resources ,    commands )  ;", "fos . flush (  )  ;", "fos . close (  )  ;", "FileUtil . setExecutable ( tempFile ,    true )  ;", "Shell . ShellCommandExecutor   shexc    =    new   Shell . ShellCommandExecutor ( new   String [  ]  {    tempFile . getAbsolutePath (  )     }  ,    BaseContainerManagerTest . tmpDir )  ;", "String   diagnostics    =    null ;", "try    {", "shexc . execute (  )  ;", "Assert . fail (  \" Should   catch   exception \"  )  ;", "}    catch    ( ExitCodeException   e )     {", "diagnostics    =    e . getMessage (  )  ;", "}", "Assert . assertNotNull ( diagnostics )  ;", "Assert . assertTrue (  (  ( shexc . getExitCode (  )  )     !  =     0  )  )  ;", "symLinkFile    =    new   File ( BaseContainerManagerTest . tmpDir ,    symLink )  ;", "}    finally    {", "if    (  ( shellFile    !  =    null )     &  &     ( shellFile . exists (  )  )  )     {", "shellFile . delete (  )  ;", "}", "if    (  ( tempFile    !  =    null )     &  &     ( tempFile . exists (  )  )  )     {", "tempFile . delete (  )  ;", "}", "if    (  ( symLinkFile    !  =    null )     &  &     ( symLinkFile . exists (  )  )  )     {", "symLinkFile . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testInvalidSymlinkDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunch . ShellScriptBuilder   builder    =    ContainerLaunch . ShellScriptBuilder . create (  )  ;", "builder . command ( Arrays . asList ( new   String [  ]  {     \" unknownCommand \"     }  )  )  ;", "File   shellFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" testShellScriptBuilderError \"  )  ;", "PrintStream   writer    =    new   PrintStream ( new   FileOutputStream ( shellFile )  )  ;", "builder . write ( writer )  ;", "writer . close (  )  ;", "try    {", "FileUtil . setExecutable ( shellFile ,    true )  ;", "Shell . ShellCommandExecutor   shexc    =    new   Shell . ShellCommandExecutor ( new   String [  ]  {    shellFile . getAbsolutePath (  )     }  ,    BaseContainerManagerTest . tmpDir )  ;", "try    {", "shexc . execute (  )  ;", "fail (  \" builder   shell   command   was   expected   to   throw \"  )  ;", "}    catch    ( IOException   e )     {", "System . out . println (  (  \" Received   an   expected   exception :     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}    finally    {", "FileUtil . fullyDelete ( shellFile )  ;", "}", "}", "METHOD_END"], "methodName": ["testShellScriptBuilderNonZeroExitCode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "File   shellFile    =    null ;", "File   tempFile    =    null ;", "String   badSymlink    =     ( Shell . WINDOWS )     ?     \" foo @ zz _  #  !  -  + bar . cmd \"     :     \" foo @ zz %  _  #  *  &  !  -  +  =    bar (  )  \"  ;", "File   symLinkFile    =    null ;", "try    {", "shellFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" hello \"  )  ;", "tempFile    =    Shell . appendScriptExtension ( BaseContainerManagerTest . tmpDir ,     \" temp \"  )  ;", "String   timeoutCommand    =     ( Shell . WINDOWS )     ?     \"  @ echo    \\  \" hello \\  \"  \"     :     \" echo    \\  \" hello \\  \"  \"  ;", "PrintWriter   writer    =    new   PrintWriter ( new   FileOutputStream ( shellFile )  )  ;", "FileUtil . setExecutable ( shellFile ,    true )  ;", "writer . println ( timeoutCommand )  ;", "writer . close (  )  ;", "Map < Path ,    List < String >  >    resources    =    new   HashMap < Path ,    List < String >  >  (  )  ;", "Path   path    =    new   Path ( shellFile . getAbsolutePath (  )  )  ;", "resources . put ( path ,    Arrays . asList ( badSymlink )  )  ;", "FileOutputStream   fos    =    new   FileOutputStream ( tempFile )  ;", "Map < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "if    ( Shell . WINDOWS )     {", "commands . add (  \" cmd \"  )  ;", "commands . add (  \"  / c \"  )  ;", "commands . add (  (  (  \"  \\  \"  \"     +    badSymlink )     +     \"  \\  \"  \"  )  )  ;", "} else    {", "commands . add (  (  (  \"  / bin / sh    .  /  \\  \\  \\  \"  \"     +    badSymlink )     +     \"  \\  \\  \\  \"  \"  )  )  ;", "}", ". writeLaunchEnv ( fos ,    env ,    resources ,    commands )  ;", "fos . flush (  )  ;", "fos . close (  )  ;", "FileUtil . setExecutable ( tempFile ,    true )  ;", "Shell . ShellCommandExecutor   shexc    =    new   Shell . ShellCommandExecutor ( new   String [  ]  {    tempFile . getAbsolutePath (  )     }  ,    BaseContainerManagerTest . tmpDir )  ;", "shexc . execute (  )  ;", "assertEquals ( shexc . getExitCode (  )  ,     0  )  ;", "assert   shexc . getOutput (  )  . contains (  \" hello \"  )  ;", "symLinkFile    =    new   File ( BaseContainerManagerTest . tmpDir ,    badSymlink )  ;", "}    finally    {", "if    (  ( shellFile    !  =    null )     &  &     ( shellFile . exists (  )  )  )     {", "shellFile . delete (  )  ;", "}", "if    (  ( tempFile    !  =    null )     &  &     ( tempFile . exists (  )  )  )     {", "tempFile . delete (  )  ;", "}", "if    (  ( symLinkFile    !  =    null )     &  &     ( symLinkFile . exists (  )  )  )     {", "symLinkFile . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testSpecialCharSymlinks"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "String   callCmd    =     \"  @ call    \"  ;", "Assume . assumeTrue ( WINDOWS )  ;", "assertEquals (  8  1  9  1  ,    WINDOWS _ MAX _ SHELL _ LENGHT )  ;", "ContainerLaunch . ShellScriptBuilder   builder    =    ContainerLaunch . ShellScriptBuilder . create (  )  ;", "builder . command ( Arrays . asList ( StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  )  )  ;", "builder . command ( Arrays . asList ( StringUtils . repeat (  \" E \"  ,     (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( callCmd . length (  )  )  )  )  )  )  ;", "try    {", "builder . command ( Arrays . asList ( StringUtils . repeat (  \" X \"  ,     (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( callCmd . length (  )  )  )     +     1  )  )  )  )  ;", "fail (  \" longCommand   was   expected   to   throw \"  )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  . expectedMessage )  )  ;", "}", "builder . command ( Arrays . asList ( StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  ,    StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  ,    StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  )  )  ;", "builder . command ( Arrays . asList ( StringUtils . repeat (  \" E \"  ,     4  0  9  5  )  ,    StringUtils . repeat (  \" E \"  ,     2  0  4  7  )  ,    StringUtils . repeat (  \" E \"  ,     (  2  0  4  7     -     ( callCmd . length (  )  )  )  )  )  )  ;", "try    {", "builder . command ( Arrays . asList ( StringUtils . repeat (  \" X \"  ,     4  0  9  5  )  ,    StringUtils . repeat (  \" X \"  ,     2  0  4  7  )  ,    StringUtils . repeat (  \" X \"  ,     (  2  0  4  8     -     ( callCmd . length (  )  )  )  )  )  )  ;", "fail (  \" long   commands   was   expected   to   throw \"  )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  . expectedMessage )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testWindowsShellScriptBuilderCommand"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeTrue ( WINDOWS )  ;", "assertEquals (  8  1  9  1  ,    WINDOWS _ MAX _ SHELL _ LENGHT )  ;", "ContainerLaunch . ShellScriptBuilder   builder    =    ContainerLaunch . ShellScriptBuilder . create (  )  ;", "builder . env (  \" somekey \"  ,    StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  )  ;", "builder . env (  \" somekey \"  ,    StringUtils . repeat (  \" A \"  ,     (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     (  \"  @ set   somekey =  \"  . length (  )  )  )  )  )  ;", "try    {", "builder . env (  \" somekey \"  ,     (  ( StringUtils . repeat (  \" A \"  ,     (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     (  \"  @ set   somekey =  \"  . length (  )  )  )  )  )     +     1  )  )  ;", "fail (  \" long   env   was   expected   to   throw \"  )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  . expectedMessage )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testWindowsShellScriptBuilderEnv"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeTrue ( WINDOWS )  ;", "String   linkCmd    =     (  \"  @  \"     +     ( Shell . WINUTILS )  )     +     \"    symlink    \\  \"  \\  \"     \\  \"  \\  \"  \"  ;", "assertEquals (  8  1  9  1  ,    WINDOWS _ MAX _ SHELL _ LENGHT )  ;", "ContainerLaunch . ShellScriptBuilder   builder    =    ContainerLaunch . ShellScriptBuilder . create (  )  ;", "builder . link ( new   Path ( StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  )  ,    new   Path ( StringUtils . repeat (  \" B \"  ,     1  0  2  4  )  )  )  ;", "builder . link ( new   Path ( StringUtils . repeat (  \" E \"  ,     (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( linkCmd . length (  )  )  )     /     2  )  )  )  ,    new   Path ( StringUtils . repeat (  \" F \"  ,     (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( linkCmd . length (  )  )  )     /     2  )  )  )  )  ;", "try    {", "builder . link ( new   Path ( StringUtils . repeat (  \" X \"  ,     (  (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( linkCmd . length (  )  )  )     /     2  )     +     1  )  )  )  ,    new   Path (  (  ( StringUtils . repeat (  \" Y \"  ,     (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( linkCmd . length (  )  )  )     /     2  )  )  )     +     1  )  )  )  ;", "fail (  \" long   link   was   expected   to   throw \"  )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  . expectedMessage )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testWindowsShellScriptBuilderLink"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "String   mkDirCmd    =     \"  @ if   not   exist    \\  \"  \\  \"    mkdir    \\  \"  \\  \"  \"  ;", "Assume . assumeTrue ( WINDOWS )  ;", "assertEquals (  8  1  9  1  ,    WINDOWS _ MAX _ SHELL _ LENGHT )  ;", "ContainerLaunch . ShellScriptBuilder   builder    =    ContainerLaunch . ShellScriptBuilder . create (  )  ;", "builder . mkdir ( new   Path ( StringUtils . repeat (  \" A \"  ,     1  0  2  4  )  )  )  ;", "builder . mkdir ( new   Path ( StringUtils . repeat (  \" E \"  ,     (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( mkDirCmd . length (  )  )  )     /     2  )  )  )  )  ;", "try    {", "builder . mkdir ( new   Path ( StringUtils . repeat (  \" X \"  ,     (  (  (  ( Shell . WINDOWS _ MAX _ SHELL _ LENGHT )     -     ( mkDirCmd . length (  )  )  )     /     2  )     +     1  )  )  )  )  ;", "fail (  \" long   mkdir   was   expected   to   throw \"  )  ;", "}    catch    ( IOException   e )     {", "assertThat ( e . getMessage (  )  ,    containsString (  . expectedMessage )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testWindowsShellScriptBuilderMkdir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.TestContainerLaunch"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileSystem . closeAllForUGI ( ugi )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  \" Failed   to   close   filesystems :     \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["closeFileSystems"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "return   new   ExecutorCompletionService < Path >  ( exec )  ;", "}", "METHOD_END"], "methodName": ["createCompletionService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "lfs . mkdir ( dirPath ,    perms ,    createParent )  ;", "if    (  !  ( perms . equals ( perms . applyUMask ( lfs . getUMask (  )  )  )  )  )     {", "lfs . setPermission ( dirPath ,    perms )  ;", "}", "}", "METHOD_END"], "methodName": ["createDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "return   Executors . newSingleThreadExecutor ( new   ThreadFactoryBuilder (  )  . setNameFormat (  \" ContainerLocalizer   Downloader \"  )  . build (  )  )  ;", "}", "METHOD_END"], "methodName": ["createDownloadThreadPool"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "final   List < LocalResourceStatus >    currentResources    =    new   ArrayList < LocalResourceStatus >  (  )  ;", "for    ( Iterator < LocalResource >    i    =    pendingResources . keySet (  )  . iterator (  )  ;    i . hasNext (  )  ;  )     {", "LocalResource   rsrc    =    i . next (  )  ;", "LocalResourceStatus   stat    =    recordFactory . newRecordInstance ( LocalResourceStatus . class )  ;", "stat . setResource ( rsrc )  ;", "Future < Path >    fPath    =    pendingResources . get ( rsrc )  ;", "if    ( fPath . isDone (  )  )     {", "try    {", "Path   localPath    =    fPath . get (  )  ;", "stat . setLocalPath ( ConverterUtils . getYarnUrlFromPath ( localPath )  )  ;", "stat . setLocalSize ( FileUtil . getDU ( new   File ( localPath . getParent (  )  . toUri (  )  )  )  )  ;", "stat . setStatus ( ResourceStatusType . FETCH _ SUCCESS )  ;", "}    catch    ( ExecutionException   e )     {", "stat . setStatus ( ResourceStatusType . FETCH _ FAILURE )  ;", "stat . setException ( SerializedException . newInstance ( e . getCause (  )  )  )  ;", "}    catch    ( CancellationException   e )     {", "stat . setStatus ( ResourceStatusType . FETCH _ FAILURE )  ;", "stat . setException ( SerializedException . newInstance ( e )  )  ;", "}", "i . remove (  )  ;", "} else    {", "stat . setStatus ( ResourceStatusType . FETCH _ PENDING )  ;", "}", "currentResources . add ( stat )  ;", "}", "Status   status    =    recordFactory . newRecordInstance ( Status . class )  ;", "status . setId ( localizerId )  ;", "status . addAllResources ( currentResources )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["createStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "DiskChecker . checkDir ( new   File ( path . toUri (  )  . getRawPath (  )  )  )  ;", "return   new   util . FSDownload ( lfs ,    ugi ,    conf ,    path ,    rsrc )  ;", "}", "METHOD_END"], "methodName": ["download"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rsrc . getSize (  )  )     <     0  )     {", "return    -  1  ;", "}", "switch    ( rsrc . getType (  )  )     {", "case   ARCHIVE    :", "case   PATTERN    :", "return    5     *     ( rsrc . getSize (  )  )  ;", "case   FILE    :", "default    :", "return   rsrc . getSize (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getEstimatedSize"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "return    (  ( ationProtocol )     ( rpc . getProxy ( ationProtocol . class ,    nmAddr ,    conf )  )  )  ;", "}", "METHOD_END"], "methodName": ["getProxy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( null    =  =    localDirs )     |  |     (  0     =  =     ( localDirs . size (  )  )  )  )     {", "throw   new   IOException (  \" Cannot   initialize   without   local   dirs \"  )  ;", "}", "String [  ]    appsFileCacheDirs    =    new   String [ localDirs . size (  )  ]  ;", "String [  ]    usersFileCacheDirs    =    new   String [ localDirs . size (  )  ]  ;", "for    ( int   i    =     0  ,    n    =    localDirs . size (  )  ;    i    <    n ;     +  + i )     {", "Path   base    =    lfs . makeQualified ( new   Path ( new   Path ( localDirs . get ( i )  ,     . USERCACHE )  ,    user )  )  ;", "Path   userFileCacheDir    =    new   Path ( base ,     . FILECACHE )  ;", "usersFileCacheDirs [ i ]     =    userFileCacheDir . toString (  )  ;", ". createDir ( lfs ,    userFileCacheDir ,     . FILECACHE _ PERMS ,    false )  ;", "Path   appBase    =    new   Path ( base ,    new   Path (  . APPCACHE ,    appId )  )  ;", "Path   appFileCacheDir    =    new   Path ( appBase ,     . FILECACHE )  ;", "appsFileCacheDirs [ i ]     =    appFileCacheDir . toString (  )  ;", ". createDir ( lfs ,    appFileCacheDir ,     . FILECACHE _ PERMS ,    false )  ;", "}", "conf . setStrings ( String . format (  . APPCACHE _ CTXT _ FMT ,    appId )  ,    appsFileCacheDirs )  ;", "conf . setStrings ( String . format (  . USERCACHE _ CTXT _ FMT ,    user )  ,    usersFileCacheDirs )  ;", "}", "METHOD_END"], "methodName": ["initDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "while    ( true )     {", "try    {", "LocalizerStatus   status    =    createStatus (  )  ;", "LocalizerHeartbeatResponse   response    =    heartbeat ( status )  ;", "switch    ( response . getLocalizerAction (  )  )     {", "case   LIVE    :", "List < ResourceLocalizationSpec >    newRsrcs    =    response . getResourceSpecs (  )  ;", "for    ( ResourceLocalizationSpec   newRsrc    :    newRsrcs )     {", "if    (  !  ( pendingResources . containsKey ( newRsrc . getResource (  )  )  )  )     {", "pendingResources . put ( newRsrc . getResource (  )  ,    cs . submit ( download ( new   Path ( newRsrc . getDestinationDirectory (  )  . getFile (  )  )  ,    newRsrc . getResource (  )  ,    ugi )  )  )  ;", "}", "}", "break ;", "case   DIE    :", "for    ( Future < Path >    pending    :    pendingResources . values (  )  )     {", "pending . cancel ( true )  ;", "}", "status    =    createStatus (  )  ;", "try    {", "heartbeat ( status )  ;", "}    catch    ( YarnException   e )     {", "}", "return ;", "}", "cs . poll (  1  0  0  0  ,    TimeUnit . MILLISECONDS )  ;", "}    catch    ( InterruptedException   e )     {", "return ;", "}    catch    ( YarnException   e )     {", "return ;", "}", "}", "}", "METHOD_END"], "methodName": ["localizeFiles"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "Thread . setDefaultUncaughtExceptionHandler ( new   YarnUncaughtExceptionHandler (  )  )  ;", "try    {", "String   user    =    argv [  0  ]  ;", "String   appId    =    argv [  1  ]  ;", "String   locId    =    argv [  2  ]  ;", "InetSocketAddress   nmAddr    =    new   InetSocketAddress ( argv [  3  ]  ,    Integer . parseInt ( argv [  4  ]  )  )  ;", "String [  ]    sLocaldirs    =    Arrays . copyOfRange ( argv ,     5  ,    argv . length )  ;", "ArrayList < Path >    localDirs    =    new   ArrayList < Path >  ( sLocaldirs . length )  ;", "for    ( String   sLocaldir    :    sLocaldirs )     {", "localDirs . add ( new   Path ( sLocaldir )  )  ;", "}", "final   String   uid    =    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ;", "if    (  !  ( user . equals ( uid )  )  )     {", ". LOG . warn (  (  (  (  \" Localization   running   as    \"     +    uid )     +     \"    not    \"  )     +    user )  )  ;", "}", "localizer    =    new    ( FileContext . getLocalFSFileContext (  )  ,    user ,    appId ,    locId ,    localDirs ,    RecordFactoryProvider . getRecordFactory ( null )  )  ;", "System . exit ( localizer . runLocalization ( nmAddr )  )  ;", "}    catch    ( Throwable   e )     {", "e . printStackTrace ( System . out )  ;", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "ContainerLocalizer . initDirs ( conf ,    user ,    appId ,    lfs ,    localDirs )  ;", "final   Credentials   creds    =    new   Credentials (  )  ;", "DataInputStream   credFile    =    null ;", "try    {", "Path   tokenPath    =    new   Path ( String . format ( ContainerLocalizer . TOKEN _ FILE _ NAME _ FMT ,    localizerId )  )  ;", "credFile    =    lfs . open ( tokenPath )  ;", "creds . readTokenStorageStream ( credFile )  ;", "lfs . delete ( tokenPath ,    false )  ;", "}    finally    {", "if    ( credFile    !  =    null )     {", "credFile . close (  )  ;", "}", "}", "UserGroupInformation   remoteUser    =    UserGroupInformation . createRemoteUser ( user )  ;", "remoteUser . addToken ( creds . getToken ( LocalizerTokenIdentifier . KIND )  )  ;", "final   LocalizationProtocol   nodeManager    =    remoteUser . doAs ( new   PrivilegedAction < LocalizationProtocol >  (  )     {", "@ Override", "public   LocalizationProtocol   run (  )     {", "return   getProxy ( nmAddr )  ;", "}", "}  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( user )  ;", "for    ( Token <  ?    extends   TokenIdentifier >    token    :    creds . getAllTokens (  )  )     {", "ugi . addToken ( token )  ;", "}", "ExecutorService   exec    =    null ;", "try    {", "exec    =    createDownloadThreadPool (  )  ;", "CompletionService < Path >    ecs    =    createCompletionService ( exec )  ;", "localizeFiles ( nodeManager ,    ecs ,    ugi )  ;", "return    0  ;", "}    catch    ( Throwable   e )     {", "e . printStackTrace ( System . out )  ;", "return    -  1  ;", "}    finally    {", "try    {", "if    ( exec    !  =    null )     {", "exec . shutdownNow (  )  ;", "}", "LocalDirAllocator . removeContext ( appCacheDirContextName )  ;", "}    finally    {", "closeFileSystems ( ugi )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["runLocalization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "TimeUnit . SECONDS . sleep ( duration )  ;", "}", "METHOD_END"], "methodName": ["sleep"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "return    -  1  ;", "}", "METHOD_END"], "methodName": ["getPos"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.FakeFSDataInputStream"}, {"methodBody": ["METHOD_START", "{", "return    -  1  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.FakeFSDataInputStream"}, {"methodBody": ["METHOD_START", "{", "return   false ;", "}", "METHOD_END"], "methodName": ["seekToNewSource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.FakeFSDataInputStream"}, {"methodBody": ["METHOD_START", "{", "relPath    =     ( relPath    =  =    null )     ?     \"  \"     :    relPath . trim (  )  ;", ". Directory   subDir    =    knownDirectories . get ( relPath )  ;", "int   oldCount    =    subDir . getCount (  )  ;", "if    (  (  ( subDir . decrementAndGetCount (  )  )     <     ( perDirectoryFileLimit )  )     &  &     ( oldCount    >  =     ( perDirectoryFileLimit )  )  )     {", "nonFullDirectories . add ( subDir )  ;", "}", "}", "METHOD_END"], "methodName": ["decrementFileCountForPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "while    ( path    !  =    null )     {", "String   name    =    path . getName (  )  ;", "if    (  ( name . length (  )  )     !  =     1  )     {", "return   path ;", "}", "int   dirnum    =     . DIRECTORIES _ PER _ LEVEL ;", "try    {", "dirnum    =    Integer . parseInt ( name ,     . DIRECTORIES _ PER _ LEVEL )  ;", "}    catch    ( NumberFormatException   e )     {", "}", "if    ( dirnum    >  =     (  . DIRECTORIES _ PER _ LEVEL )  )     {", "return   path ;", "}", "path    =    path . getParent (  )  ;", "}", "return   path ;", "}", "METHOD_END"], "methodName": ["getCacheDirectoryRoot"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "return   knownDirectories . get ( relPath )  ;", "}", "METHOD_END"], "methodName": ["getDirectory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "if    ( nonFullDirectories . isEmpty (  )  )     {", "( totalSubDirectories )  +  +  ;", ". Directory   newDir    =    new    . Directory ( totalSubDirectories )  ;", "nonFullDirectories . add ( newDir )  ;", "knownDirectories . put ( newDir . getRelativePath (  )  ,    newDir )  ;", "}", ". Directory   subDir    =    nonFullDirectories . peek (  )  ;", "if    (  ( subDir . incrementAndGetCount (  )  )     >  =     ( perDirectoryFileLimit )  )     {", "nonFullDirectories . remove (  )  ;", "}", "return   subDir . getRelativePath (  )  ;", "}", "METHOD_END"], "methodName": ["getRelativePathForLocalization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "relPath    =     ( relPath    =  =    null )     ?     \"  \"     :    relPath . trim (  )  ;", ". Directory   subDir    =    knownDirectories . get ( relPath )  ;", "if    ( subDir    =  =    null )     {", "int   dirnum    =     . Directory . getDirectoryNumber ( relPath )  ;", "totalSubDirectories    =    Math . max ( dirnum ,    totalSubDirectories )  ;", "subDir    =    new    . Directory ( dirnum )  ;", "nonFullDirectories . add ( subDir )  ;", "knownDirectories . put ( subDir . getRelativePath (  )  ,    subDir )  ;", "}", "if    (  ( subDir . incrementAndGetCount (  )  )     >  =     ( perDirectoryFileLimit )  )     {", "nonFullDirectories . remove ( subDir )  ;", "}", "}", "METHOD_END"], "methodName": ["incrementFileCountForPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "return   loc ;", "}", "METHOD_END"], "methodName": ["getPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourceRequest"}, {"methodBody": ["METHOD_START", "{", "LocalResourcePBImpl   lrpb ;", "if    (  !  ( lr   instanceof   LocalResourcePBImpl )  )     {", "lr    =    LocalResource . newInstance ( lr . getResource (  )  ,    lr . getType (  )  ,    lr . getVisibility (  )  ,    lr . getSize (  )  ,    lr . getTimestamp (  )  ,    lr . getPattern (  )  )  ;", "}", "lrpb    =     (  ( LocalResourcePBImpl )     ( lr )  )  ;", "return   lrpb . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["buildLocalResourceProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "return   LocalizedResourceProto . newBuilder (  )  . setResource ( buildLocalResourceProto ( rsrc . getRequest (  )  )  )  . setLocalPath ( rsrc . getLocalPath (  )  . toString (  )  )  . setSize ( rsrc . getSize (  )  )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["buildLocalizedResourceProto"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( useLocalCacheDirectoryManager )     {", "Path   rsrcPath    =    null ;", "if    ( inProgressMap . containsKey ( req )  )     {", "rsrcPath    =    inProgressMap . remove ( req )  ;", "} else", "if    (  ( rsrc    !  =    null )     &  &     (  ( rsrc . getLocalPath (  )  )     !  =    null )  )     {", "rsrcPath    =    rsrc . getLocalPath (  )  . getParent (  )  . getParent (  )  ;", "}", "if    ( rsrcPath    !  =    null )     {", "Path   parentPath    =    new   Path ( rsrcPath . toUri (  )  . getRawPath (  )  )  ;", "while    (  !  ( directoryManagers . containsKey ( parentPath )  )  )     {", "parentPath    =    parentPath . getParent (  )  ;", "if    ( parentPath    =  =    null )     {", "return ;", "}", "}", "if    ( parentPath    !  =    null )     {", "String   parentDir    =    parentPath . toUri (  )  . getRawPath (  )  . toString (  )  ;", "LocalCacheDirectoryManager   dir    =    directoryManagers . get ( parentPath )  ;", "String   rsrcDir    =    rsrcPath . toUri (  )  . getRawPath (  )  ;", "if    ( rsrcDir . equals ( parentDir )  )     {", "dir . decrementFileCountForPath (  \"  \"  )  ;", "} else    {", "dir . decrementFileCountForPath ( rsrcDir . substring (  (  ( parentDir . length (  )  )     +     1  )  )  )  ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["decrementFileCountForLocalCacheDirectory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "LocalCacheDirectoryManager   mgr    =    null ;", "if    ( useLocalCacheDirectoryManager )     {", "mgr    =    directoryManagers . get ( localDirPath )  ;", "}", "return   mgr ;", "}", "METHOD_END"], "methodName": ["getDirectoryManager"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "Path   delPath    =    localPath . getParent (  )  ;", "String   name    =    delPath . getName (  )  ;", "Matcher   matcher    =     . RANDOM _ DIR _ PATTERN . matcher ( name )  ;", "if    ( matcher . matches (  )  )     {", "return   delPath ;", "} else    {", ". LOG . warn (  (  \" Random   directory   component   did   not   match .     \"     +     \" Deleting   localized   path   only \"  )  )  ;", "return   localPath ;", "}", "}", "METHOD_END"], "methodName": ["getPathToDelete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( useLocalCacheDirectoryM    {", "Path   cacheRoot    =    LocalCacheDirectoryMgetCacheDirectoryRoot ( cacheDir )  ;", "if    ( cacheRoot    !  =    null )     {", "LocalCacheDirectoryMdir    =    directoryM . get ( cacheRoot )  ;", "if    ( dir    =  =    null )     {", "dir    =    new   LocalCacheDirectoryMconf )  ;", "LocalCacheDirectoryMotherDir    =    directoryM . putIfAbsent ( cacheRoot ,    dir )  ;", "if    ( otherDir    !  =    null )     {", "dir    =    otherDir ;", "}", "}", "if    ( cacheDir . equals ( cacheRoot )  )     {", "dir . incrementFileCountForPath (  \"  \"  )  ;", "} else    {", "String   dirStr    =    cacheDir . toUri (  )  . getRawPath (  )  ;", "String   rootStr    =    cacheRoot . toUri (  )  . getRawPath (  )  ;", "dir . incrementFileCountForPath ( dirStr . substring (  (  ( rootStr . length (  )  )     +     1  )  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["incrementFileCountForLocalCacheDirectory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "boolean   ret    =    true ;", "if    (  ( rsrc . getState (  )  )     =  =     ( State . LOCALIZED )  )     {", "File   file    =    new   File ( rsrc . getLocalPath (  )  . toUri (  )  . getRawPath (  )  . toString (  )  )  ;", "if    (  !  ( file . exists (  )  )  )     {", "ret    =    false ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["isResourcePresent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "Path   localDir    =    event . getLocalPath (  )  . getParent (  )  ;", "long   rsrcId    =    Long . parseLong ( localDir . getName (  )  )  ;", "while    ( true )     {", "long   currentRsrcId    =    uniqueNumberGenerator . get (  )  ;", "long   nextRsrcId    =    Math . max ( currentRsrcId ,    rsrcId )  ;", "if    ( uniqueNumberGenerator . compareAndSet ( currentRsrcId ,    nextRsrcId )  )     {", "break ;", "}", "}", "incrementFileCountForLocalCacheDirectory ( localDir . getParent (  )  )  ;", "return   new   Localized ( req ,    dispatcher )  ;", "}", "METHOD_END"], "methodName": ["recoverResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "LocalizedResource   rsrc    =    localrsrc . remove ( req )  ;", "decrementFileCountForLocalCacheDirectory ( req ,    rsrc )  ;", "if    ( rsrc    !  =    null )     {", "Path   localPath    =    rsrc . getLocalPath (  )  ;", "if    ( localPath    !  =    null )     {", "try    {", "stateStore . removeLocalizedResource ( user ,    appId ,    localPath )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  \" Unable   to   remove   resource    \"     +    rsrc )     +     \"    from   state   store \"  )  ,    e )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["removeResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "return   System . nanoTime (  )  ;", "}", "METHOD_END"], "methodName": ["currentTime"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   localPath ;", "}", "METHOD_END"], "methodName": ["getLocalPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   ref . size (  )  ;", "}", "METHOD_END"], "methodName": ["getRefCount"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   rsrc ;", "}", "METHOD_END"], "methodName": ["getRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return   stateMachine . getCurrentState (  )  ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   timestamp . get (  )  ;", "}", "METHOD_END"], "methodName": ["getTimestamp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "if    ( ref . remove ( container )  )     {", "timestamp . set ( currentTime (  )  )  ;", "} else    {", ". LOG . info (  (  (  (  (  \" Container    \"     +    container )     +     \"    doesn ' t   exist   in   the   container   list   of   the   Resource    \"  )     +     ( this )  )     +     \"    to   which   it   sent   RELEASE   event \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["release"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "this . localPath    =    Path . getPathWithoutSchemeAndAuthority ( localPath )  ;", "}", "METHOD_END"], "methodName": ["setLocalPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append (  \"  {     \"  )  . append ( rsrc . toString (  )  )  . append (  \"  ,  \"  )  . append (  (  ( getState (  )  )     =  =     ( State . LOCALIZED )     ?     (  ( getLocalPath (  )  )     +     \"  ,  \"  )     +     ( getSize (  )  )     :     \" pending \"  )  )  . append (  \"  ,  [  \"  )  ;", "try    {", "this . readLock . lock (  )  ;", "for    ( ContainerId   c    :    ref )     {", "sb . append (  \"  (  \"  )  . append ( c . toString (  )  )  . append (  \"  )  \"  )  ;", "}", "sb . append (  \"  ]  ,  \"  )  . append ( getTimestamp (  )  )  . append (  \"  ,  \"  )  . append ( getState (  )  )  . append (  \"  }  \"  )  ;", "return   sb . toString (  )  ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   sem . tryAcquire (  )  ;", "}", "METHOD_END"], "methodName": ["tryAcquire"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "sem . release (  )  ;", "}", "METHOD_END"], "methodName": ["unlock"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext"}, {"methodBody": ["METHOD_START", "{", "return   credentials ;", "}", "METHOD_END"], "methodName": ["getCredentials"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext"}, {"methodBody": ["METHOD_START", "{", "return   statCache ;", "}", "METHOD_END"], "methodName": ["getStatCache"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizerContext"}, {"methodBody": ["METHOD_START", "{", "return   action ;", "}", "METHOD_END"], "methodName": ["getLocalizerAction"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.MockLocalizerHeartbeatResponse"}, {"methodBody": ["METHOD_START", "{", "this . action    =    action ;", "}", "METHOD_END"], "methodName": ["setLocalizerAction"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.MockLocalizerHeartbeatResponse"}, {"methodBody": ["METHOD_START", "{", "stats . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clearResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.MockLocalizerStatus"}, {"methodBody": ["METHOD_START", "{", "RemoteIterator < FileStatus >    userDirStatus    =    lfs . listStatus ( userDirPath )  ;", "Dele . FileDeletionTask   dependentDeletionTask    =    del . createFileDeletionTask ( null ,    userDirPath ,    new   Path [  ]  {        }  )  ;", "if    ( userDirStatus    !  =    null )     {", "List < Dele . FileDeletionTask >    deletionTasks    =    new   ArrayList < Dele . FileDeletionTask >  (  )  ;", "while    ( userDirStatus . hasNext (  )  )     {", "FileStatus   status    =    userDirStatus . next (  )  ;", "String   owner    =    status . getOwner (  )  ;", "Dele . FileDeletionTask   deletionTask    =    del . createFileDeletionTask ( owner ,    null ,    new   Path [  ]  {    status . getPath (  )     }  )  ;", "deletionTask . addFileDeletionTaskDependency ( dependentDeletionTask )  ;", "deletionTasks . add ( deletionTask )  ;", "}", "for    ( Dele . FileDeletionTask   task    :    deletionTasks )     {", "del . scheduleFileDeletionTask ( task )  ;", "}", "} else    {", "del . scheduleFileDeletionTask ( dependentDeletionTask )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanUpFilesPerUserDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "long   currentTimeStamp    =    System . currentTimeMillis (  )  ;", "for    ( String   localDir    :    dirsHandler . getLocalDirs (  )  )     {", "renameLocalDir ( lfs ,    localDir ,    ContainerLocalizer . USERCACHE ,    currentTimeStamp )  ;", "renameLocalDir ( lfs ,    localDir ,    ContainerLocalizer . FILECACHE ,    currentTimeStamp )  ;", "renameLocalDir ( lfs ,    localDir ,     . NM _ PRIVATE _ DIR ,    currentTimeStamp )  ;", "try    {", "deleteLocalDir ( lfs ,    del ,    localDir )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Failed   to   delete   localDir :     \"     +    localDir )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["cleanUpLocalDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "int   nThreads    =    conf . getInt ( NM _ LOCALIZER _ FETCH _ THREAD _ COUNT ,    DEFAULT _ NM _ LOCALIZER _ FETCH _ THREAD _ COUNT )  ;", "ThreadFactory   tf    =    new   ThreadFactoryBuilder (  )  . setNameFormat (  \" PublicL    #  % d \"  )  . build (  )  ;", "return   Executors . newFixedThreadPool ( nThreads ,    tf )  ;", "}", "METHOD_END"], "methodName": ["createLocalizerExecutor"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceLocalizationService . LocalizerTracker ( conf )  ;", "}", "METHOD_END"], "methodName": ["createLocalizerTracker"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "secretManager    =    new   LocalizerTokenSecretManager (  )  ;", "}", "Server   server    =    rpc . getServer ( LocalizationProtocol . class ,    this ,    lerAddress ,    conf ,    secretManager ,    conf . getInt ( NM _ LOCALIZER _ CLIENT _ THREAD _ COUNT ,    DEFAULT _ NM _ LOCALIZER _ CLIENT _ THREAD _ COUNT )  )  ;", "if    ( conf . getBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    false )  )     {", "server . refreshServiceAcl ( conf ,    new   NMPolicyProvider (  )  )  ;", "}", "return   server ;", "}", "METHOD_END"], "methodName": ["createServer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "RemoteIterator < FileStatus >    fileStatus    =    lfs . listStatus ( new   Path ( localDir )  )  ;", "if    ( fileStatus    !  =    null )     {", "while    ( fileStatus . hasNext (  )  )     {", "FileStatus   status    =    fileStatus . next (  )  ;", "try    {", "if    ( status . getPath (  )  . getName (  )  . matches (  (  (  \"  .  *  \"     +     ( ContainerLocalizer . USERCACHE )  )     +     \"  _ DEL _  .  *  \"  )  )  )     {", ". LOG . info (  (  \" usercache   path    :     \"     +     ( status . getPath (  )  . toString (  )  )  )  )  ;", "cleanUpFilesPerUserDir ( lfs ,    del ,    status . getPath (  )  )  ;", "} else", "if    (  ( status . getPath (  )  . getName (  )  . matches (  (  (  \"  .  *  \"     +     (  . NM _ PRIVATE _ DIR )  )     +     \"  _ DEL _  .  *  \"  )  )  )     |  |     ( status . getPath (  )  . getName (  )  . matches (  (  (  \"  .  *  \"     +     ( ContainerLocalizer . FILECACHE )  )     +     \"  _ DEL _  .  *  \"  )  )  )  )     {", "del . delete ( null ,    status . getPath (  )  ,    new   Path [  ]  {        }  )  ;", "}", "}    catch    ( IOException   ex )     {", ". LOG . warn (  (  \" Failed   to   delete   this   local   Directory :     \"     +     ( status . getPath (  )  . getName (  )  )  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["deleteLocalDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   StringUtils . join ( SEPARATOR ,    Arrays . asList (  \"  .  \"  ,    ContainerLocalizer . USERCACHE ,    user ,    ContainerLocalizer . APPCACHE ,    appId ,    ContainerLocalizer . FILECACHE )  )  ;", "}", "METHOD_END"], "methodName": ["getAppFileCachePath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   FileContext . getFSFileContext ( conf )  ;", "}    catch    ( IOException   e )     {", "throw   new   YarnRuntimeException (  \" Failed   to   access   local   fs \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["getLocalFileContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "switch    ( visibility )     {", "default    :", "case   PUBLIC    :", "return   publicRsrc ;", "case   PRIVATE    :", "return   privateRsrc . get ( u )  ;", "case   APPLICATION    :", "return   appRsrc . get ( ConverterUtils . toString ( appId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["getLocalResourcesTracker"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   localizerTracker . privLocalizers . get ( locId )  ;", "}", "METHOD_END"], "methodName": ["getLocalizerRunner"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   localizerTracker . privLocalizers ;", "}", "METHOD_END"], "methodName": ["getPrivateLocalizers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   localizerTracker . publicLocalizer ;", "}", "METHOD_END"], "methodName": ["getPublicLocalizer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   StringUtils . join ( SEPARATOR ,    Arrays . asList (  \"  .  \"  ,    ContainerLocalizer . USERCACHE ,    user ,    ContainerLocalizer . FILECACHE )  )  ;", "}", "METHOD_END"], "methodName": ["getUserFileCachePath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "ResourceRetentionSet   retain    =    new   ResourceRetentionSet ( delService ,    cacheTargetSize )  ;", "retain . addResources ( publicRsrc )  ;", ". LOG . debug (  (  \" Resource   cleanup    ( public )     \"     +    retain )  )  ;", "for    ( LocalResourcesTracker   t    :    privateRsrc . values (  )  )     {", "retain . addResources ( t )  ;", ". LOG . debug (  (  (  (  \" Resource   cleanup    \"     +     ( t . getUser (  )  )  )     +     \"  :  \"  )     +    retain )  )  ;", "}", "}", "METHOD_END"], "methodName": ["handleCacheCleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Container   c    =    rsrcCleanup . getContainer (  )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    rsrcs    =    rsrcCleanup . getResources (  )  ;", "for    ( Map . Entry < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    e    :    rsrcs . entrySet (  )  )     {", "LocalResourcesTracker   tracker    =    getLocalResourcesTracker ( e . getKey (  )  ,    c . getUser (  )  ,    c . getContainerId (  )  . getApplicationAttemptId (  )  . getApplicationId (  )  )  ;", "for    ( LocalResourceRequest   req    :    e . getValue (  )  )     {", "tracker . handle ( new   ResourceReleaseEvent ( req ,    c . getContainerId (  )  )  )  ;", "}", "}", "String   locId    =    ConverterUtils . toString ( c . getContainerId (  )  )  ;", "localizerTracker . cleanupPrivLocalizers ( locId )  ;", "String   userName    =    c . getUser (  )  ;", "String   containerIDStr    =    c . toString (  )  ;", "String   appIDStr    =    ConverterUtils . toString ( c . getContainerId (  )  . getApplicationAttemptId (  )  . getApplicationId (  )  )  ;", "for    ( String   localDir    :    dirsHandler . getLocalDirs (  )  )     {", "Path   usersdir    =    new   Path ( localDir ,    ContainerLocalizer . USERCACHE )  ;", "Path   userdir    =    new   Path ( usersdir ,    userName )  ;", "Path   allAppsdir    =    new   Path ( userdir ,    ContainerLocalizer . APPCACHE )  ;", "Path   appDir    =    new   Path ( allAppsdir ,    appIDStr )  ;", "Path   containerDir    =    new   Path ( appDir ,    containerIDStr )  ;", "delService . delete ( userName ,    containerDir ,    new   Path [  ]  {        }  )  ;", "Path   sysDir    =    new   Path ( localDir ,     . NM _ PRIVATE _ DIR )  ;", "Path   appSysDir    =    new   Path ( sysDir ,    appIDStr )  ;", "Path   containerSysDir    =    new   Path ( appSysDir ,    containerIDStr )  ;", "delService . delete ( null ,    containerSysDir ,    new   Path [  ]  {        }  )  ;", "}", "dispatcher . getEventHandler (  )  . handle ( new   ContainerEvent ( c . getContainerId (  )  ,    ContainerEventType . CONTAINER _ RESOURCES _ CLEANEDUP )  )  ;", "}", "METHOD_END"], "methodName": ["handleCleanupContainerResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "String   userName    =    application . getUser (  )  ;", "ApplicationId   appId    =    application . getAppId (  )  ;", "String   appIDStr    =    application . toString (  )  ;", "LocalResourcesTracker   appLocalRsrcsTracker    =    appRsrc . remove ( ConverterUtils . toString ( appId )  )  ;", "if    ( appLocalRsrcsTracker    !  =    null )     {", "for    ( LocalizedResource   rsrc    :    appLocalRsrcsTracker )     {", "Path   localPath    =    rsrc . getLocalPath (  )  ;", "if    ( localPath    !  =    null )     {", "try    {", "stateStore . removeLocalizedResource ( userName ,    appId ,    localPath )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  (  (  \" Unable   to   remove   resource    \"     +    rsrc )     +     \"    for    \"  )     +    appIDStr )     +     \"    from   state   store \"  )  ,    e )  ;", "}", "}", "}", "} else    {", ". LOG . warn (  (  \" Removing   uninitialized   application    \"     +    application )  )  ;", "}", "for    ( String   localDir    :    dirsHandler . getLocalDirs (  )  )     {", "Path   usersdir    =    new   Path ( localDir ,    ContainerLocalizer . USERCACHE )  ;", "Path   userdir    =    new   Path ( usersdir ,    userName )  ;", "Path   allAppsdir    =    new   Path ( userdir ,    ContainerLocalizer . APPCACHE )  ;", "Path   appDir    =    new   Path ( allAppsdir ,    appIDStr )  ;", "delService . delete ( userName ,    appDir ,    new   Path [  ]  {        }  )  ;", "Path   sysDir    =    new   Path ( localDir ,     . NM _ PRIVATE _ DIR )  ;", "Path   appSysDir    =    new   Path ( sysDir ,    appIDStr )  ;", "delService . delete ( null ,    appSysDir ,    new   Path [  ]  {        }  )  ;", "}", "dispatcher . getEventHandler (  )  . handle ( new   ApplicationEvent ( application . getAppId (  )  ,    ApplicationEventType . APPLICATION _ RESOURCES _ CLEANEDUP )  )  ;", "}", "METHOD_END"], "methodName": ["handleDestroyApplicationResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "String   userName    =    app . getUser (  )  ;", "privateRsrc . putIfAbsent ( userName ,    new   LocalsTrackerImpl ( userName ,    null ,    dispatcher ,    true ,    super . getConfig (  )  ,    stateStore )  )  ;", "String   appIdStr    =    ConverterUtils . toString ( app . getAppId (  )  )  ;", "appRsrc . putIfAbsent ( appIdStr ,    new   LocalsTrackerImpl ( app . getUser (  )  ,    app . getAppId (  )  ,    dispatcher ,    false ,    super . getConfig (  )  ,    stateStore )  )  ;", "dispatcher . getEventHandler (  )  . handle ( new   ApplicationInitedEvent ( app . getAppId (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["handleInitApplicationResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Container   c    =    rsrcReqs . getContainer (  )  ;", "LoadingCache < Path ,    Future < FileStatus >  >    statCache    =    CacheBuilder . newBuilder (  )  . build ( FSDownload . createStatusCacheLoader ( getConfig (  )  )  )  ;", "LocalizerContext   ctxt    =    new   LocalizerContext ( c . getUser (  )  ,    c . getContainerId (  )  ,    c . getCredentials (  )  ,    statCache )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    rsrcs    =    rsrcReqs . getRequestedResources (  )  ;", "for    ( Map . Entry < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    e    :    rsrcs . entrySet (  )  )     {", "LocalResourcesTracker   tracker    =    getLocalResourcesTracker ( e . getKey (  )  ,    c . getUser (  )  ,    c . getContainerId (  )  . getApplicationAttemptId (  )  . getApplicationId (  )  )  ;", "for    ( LocalResourceRequest   req    :    e . getValue (  )  )     {", "tracker . handle ( new   ResourceRequestEvent ( req ,    e . getKey (  )  ,    ctxt )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["handleInitContainerResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . LocalResourceTrackerState   trackerState    =    state . getPublicTrackerState (  )  ;", "recoverTrackerResources ( publicRsrc ,    trackerState )  ;", "for    ( Map . Entry < String ,    NMStateStoreService . RecoveredUserResources >    userEntry    :    state . getUserResources (  )  . entrySet (  )  )     {", "String   user    =    userEntry . getKey (  )  ;", "NMStateStoreService . RecoveredUserResources   userResources    =    userEntry . getValue (  )  ;", "trackerState    =    userResources . getPrivateTrackerState (  )  ;", "if    (  !  ( trackerState . isEmpty (  )  )  )     {", "LocalResourcesTracker   tracker    =    new   LocalResourcesTrackerImpl ( user ,    null ,    dispatcher ,    true ,    super . getConfig (  )  ,    stateStore )  ;", "LocalResourcesTracker   oldTracker    =    privateRsrc . putIfAbsent ( user ,    tracker )  ;", "if    ( oldTracker    !  =    null )     {", "tracker    =    oldTracker ;", "}", "recoverTrackerResources ( tracker ,    trackerState )  ;", "}", "for    ( Map . Entry < ApplicationId ,    NMStateStoreService . LocalResourceTrackerState >    appEntry    :    userResources . getAppTrackerStates (  )  . entrySet (  )  )     {", "trackerState    =    appEntry . getValue (  )  ;", "if    (  !  ( trackerState . isEmpty (  )  )  )     {", "ApplicationId   appId    =    appEntry . getKey (  )  ;", "String   appIdStr    =    ConverterUtils . toString ( appId )  ;", "LocalResourcesTracker   tracker    =    new   LocalResourcesTrackerImpl ( user ,    appId ,    dispatcher ,    false ,    super . getConfig (  )  ,    stateStore )  ;", "LocalResourcesTracker   oldTracker    =    appRsrc . putIfAbsent ( appIdStr ,    tracker )  ;", "if    ( oldTracker    !  =    null )     {", "tracker    =    oldTracker ;", "}", "recoverTrackerResources ( tracker ,    trackerState )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["recoverLocalizedResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "for    ( LocalizedResourceProto   proto    :    state . getLocalizedResources (  )  )     {", "LocalResource   rsrc    =    new   LocalResourcePBImpl ( proto . getResource (  )  )  ;", "LocalResourceRequest   req    =    new   LocalResourceRequest ( rsrc )  ;", ". LOG . info (  (  (  (  \" Recovering   localized   resource    \"     +    req )     +     \"    at    \"  )     +     ( proto . getLocalPath (  )  )  )  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( req ,    new   Path ( proto . getLocalPath (  )  )  ,    proto . getSize (  )  )  )  ;", "}", "for    ( Map . Entry < LocalResourceProto ,    Path >    entry    :    state . getInProgressResources (  )  . entrySet (  )  )     {", "LocalResource   rsrc    =    new   LocalResourcePBImpl ( entry . getKey (  )  )  ;", "LocalResourceRequest   req    =    new   LocalResourceRequest ( rsrc )  ;", "Path   localPath    =    entry . getValue (  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( req ,    localPath ,     0  )  )  ;", ". LOG . info (  (  (  (  \" Deleting   in - progress   localization   for    \"     +    req )     +     \"    at    \"  )     +    localPath )  )  ;", "tracker . remove ( tracker . getLocalizedResource ( req )  ,    delService )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverTrackerResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "try    {", "lfs . rename ( new   Path ( localDir ,    localSubDir )  ,    new   Path ( localDir ,     (  ( localSubDir    +     \"  _ DEL _  \"  )     +    currentTimeStamp )  )  )  ;", "}    catch    ( FileNotFoundException   ex )     {", "}    catch    ( Exception   ex )     {", ". LOG . warn (  (  (  (  \" Failed   to   rename   the   local   file   under    \"     +    localDir )     +     \"  /  \"  )     +    localSubDir )  )  ;", "}", "}", "METHOD_END"], "methodName": ["renameLocalDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "int   perDirFileLimit    =    conf . getInt ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,    DEFAULT _ NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY )  ;", "if    ( perDirFileLimit    <  =     3  6  )     {", ". LOG . error (  (  ( YarnConfiguration . NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY )     +     \"    parameter   is   configured   with   very   low   value .  \"  )  )  ;", "throw   new   YarnRuntimeException (  (  ( YarnConfiguration . NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY )     +     \"    parameter   is   configured   with   a   value   less   than    3  7  .  \"  )  )  ;", "} else    {", ". LOG . info (  (  \" per   directory   file   limit    =     \"     +    perDirFileLimit )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateConf"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "for    ( LocalizedResource   resource    :    newTracker )     {", "currentSize    +  =    resource . getSize (  )  ;", "if    (  ( resource . getRefCount (  )  )     >     0  )     {", "continue ;", "}", "retain . put ( resource ,    newTracker )  ;", "}", "for    ( Iterator < Map . Entry < LocalizedResource ,    LocalResourcesTracker >  >    i    =    retain . entrySet (  )  . iterator (  )  ;     (  (  ( currentSize )     -     ( delSize )  )     >     ( targetSize )  )     &  &     ( i . hasNext (  )  )  ;  )     {", "Map . Entry < LocalizedResource ,    LocalResourcesTracker >    rsrc    =    i . next (  )  ;", "LocalizedResource   resource    =    rsrc . getKey (  )  ;", "LocalResourcesTracker   tracker    =    rsrc . getValue (  )  ;", "if    ( tracker . remove ( resource ,    delService )  )     {", "delSize    +  =    resource . getSize (  )  ;", "i . remove (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceRetentionSet"}, {"methodBody": ["METHOD_START", "{", "Credentials   creds    =    new   Credentials (  )  ;", "byte [  ]    password    =    new   byte [  2  0  ]  ;", "Text   kind    =    new   Text (  )  ;", "Text   ice    =    new   Text (  )  ;", "Text   alias    =    new   Text (  )  ;", "for    ( int   i    =     0  ;    i    <    nTok ;     +  + i )     {", "byte [  ]    identifier    =     (  \" idef \"     +    i )  . getBytes (  )  ;", "r . nextBytes ( password )  ;", "kind . set (  (  \" kind \"     +    i )  )  ;", "ice . set (  (  \" ice \"     +    i )  )  ;", "alias . set (  (  \" token \"     +    i )  )  ;", "Token   token    =    new   Token ( identifier ,    password ,    kind ,    ice )  ;", "creds . addToken ( alias ,    token )  ;", "}", "DataOutputBuffer   buf    =    new   DataOutputBuffer (  )  ;", "creds . writeTokenStorageToStream ( buf )  ;", "DataInputBuffer   ret    =    new   DataInputBuffer (  )  ;", "ret . reset ( buf . getData (  )  ,     0  ,    buf . getLength (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createFakeCredentials"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   mockRF    =    mock ( RecordFactory . class )  ;", "when ( mockRF . newRecordInstance ( same ( LocalResourceStatus . class )  )  )  . thenAnswer ( new   Answer < LocalResourceStatus >  (  )     {", "@ Override", "public   LocalResourceStatus   answer ( InvocationOnMock   invoc )    throws   Throwable    {", "return   new   MockLocalResourceStatus (  )  ;", "}", "}  )  ;", "when ( mockRF . newRecordInstance ( same ( Status . class )  )  )  . thenAnswer ( new   Answer < Status >  (  )     {", "@ Override", "public   Status   answer ( InvocationOnMock   invoc )    throws   Throwable    {", "return   new   MockStatus (  )  ;", "}", "}  )  ;", "return   mockRF ;", "}", "METHOD_END"], "methodName": ["getMockLocalizerRecordFactory"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "ResourceLocalizationSpec   resourceLocalizationSpec    =    mock ( ResourceLocalizationSpec . class )  ;", "LocalResource   rsrc    =    mock ( LocalResource . class )  ;", "String   name    =    Long . toHexString ( r . nextLong (  )  )  ;", "URL   uri    =    mock ( URL . class )  ;", "when ( uri . getScheme (  )  )  . thenReturn (  \" file \"  )  ;", "when ( uri . getHost (  )  )  . thenReturn ( null )  ;", "when ( uri . getFile (  )  )  . thenReturn (  (  (  (  \"  / local /  \"     +    vis )     +     \"  /  \"  )     +    name )  )  ;", "when ( rsrc . getResource (  )  )  . thenReturn ( uri )  ;", "when ( rsrc . getSize (  )  )  . thenReturn (  (  ( r . nextInt (  1  0  2  4  )  )     +     1  0  2  4 L )  )  ;", "when ( rsrc . getTimestamp (  )  )  . thenReturn (  (  ( r . nextInt (  1  0  2  4  )  )     +     2  0  4  8 L )  )  ;", "when ( rsrc . getType (  )  )  . thenReturn ( FILE )  ;", "when ( rsrc . getVisibility (  )  )  . thenReturn ( vis )  ;", "when ( resourceLocalizationSpec . getResource (  )  )  . thenReturn ( rsrc )  ;", "when ( resourceLocalizationSpec . getDestinationDirectory (  )  )  . thenReturn ( ConverterUtils . getYarnUrlFromPath ( p )  )  ;", "return   resourceLocalizationSpec ;", "}", "METHOD_END"], "methodName": ["getMockRsrc"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "doNothing (  )  . when ( spylfs )  . mkdir ( isA ( Path . class )  ,    isA ( FsPermission . class )  ,    anyBoolean (  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "FileContext   lfs    =    FileContext . getFileContext ( spylfs ,    conf )  ;", "localDirs    =    new   ArrayList < Path >  (  )  ;", "for    ( int   i    =     0  ;    i    <     4  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "}", "RecordFactory   mockRF    =     . getMockLocalizerRecordFactory (  )  ;", "ContainerLocalizer   concreteLoc    =    new   ContainerLocalizer ( lfs ,     . appUser ,     . appId ,     . containerId ,    localDirs ,    mockRF )  ;", "ContainerLocalizer   localizer    =    spy ( concreteLoc )  ;", "random    =    new   Random (  )  ;", "long   seed    =    random . nextLong (  )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "random . setSeed ( seed )  ;", "DataInputBuffer   appTokens    =     . createFakeCredentials ( random ,     1  0  )  ;", "tokenPath    =    lfs . makeQualified ( new   Path ( String . format ( ContainerLocalizer . TOKEN _ FILE _ NAME _ FMT ,     . containerId )  )  )  ;", "doReturn ( new   FSDataInputStream ( new   FakeFSDataInputStream ( appTokens )  )  )  . when ( spylfs )  . open ( tokenPath )  ;", "nmProxy    =    mock ( LocalizationProtocol . class )  ;", "doReturn ( nmProxy )  . when ( localizer )  . getProxy (  . nmAddr )  ;", "doNothing (  )  . when ( localizer )  . sleep ( anyInt (  )  )  ;", "ExecutorService   syncExec    =    mock ( ExecutorService . class )  ;", "CompletionService < Path >    cs    =    mock ( CompletionService . class )  ;", "when ( cs . submit ( isA ( Callable . class )  )  )  . thenAnswer ( new   Answer < Future < Path >  >  (  )     {", "@ Override", "public   Future < Path >    answer ( InvocationOnMock   invoc )    throws   Throwable    {", "Future < Path >    done    =    mock ( class )  ;", "when ( done . isDone (  )  )  . thenReturn ( true )  ;", ". FakeDownload   d    =     (  (  . FakeDownload )     ( invoc . getArguments (  )  [  0  ]  )  )  ;", "when ( done . get (  )  )  . thenReturn ( d . call (  )  )  ;", "return   done ;", "}", "}  )  ;", "doReturn ( syncExec )  . when ( localizer )  . createDownloadThreadPool (  )  ;", "doReturn ( cs )  . when ( localizer )  . createCompletionService ( syncExec )  ;", "return   localizer ;", "}", "METHOD_END"], "methodName": ["setupContainerLocalizerForTest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "FileContext   fs    =    FileContext . getLocalFSFileContext (  )  ;", "spylfs    =    spy ( fs . getDefaultFileSystem (  )  )  ;", "ContainerLocalizer   localizer    =    setupContainerLocalizerForTest (  )  ;", "doNothing (  )  . when ( localizer )  . localizeFiles ( any ( LocalizationProtocol . class )  ,    any ( CompletionService . class )  ,    any ( UserGroupInformation . class )  )  ;", "verify ( localizer ,    never (  )  )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "localizer . runLocalization (  . nmAddr )  ;", "verify ( localizer )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "spylfs    =    spy ( fs . getDefaultFileSystem (  )  )  ;", "localizer    =    setupContainerLocalizerForTest (  )  ;", "doThrow ( new   YarnRuntimeException (  \" Forced   Failure \"  )  )  . when ( localizer )  . localizeFiles ( any ( LocalizationProtocol . class )  ,    any ( CompletionService . class )  ,    any ( UserGroupInformation . class )  )  ;", "verify ( localizer ,    never (  )  )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "localizer . runLocalization (  . nmAddr )  ;", "verify ( localizer )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLocalizerClosesFilesystems"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "FileContext   fs    =    FileContext . getLocalFSFileContext (  )  ;", "spylfs    =    spy ( fs . getDefaultFileSystem (  )  )  ;", "ContainerLocalizer   localizer    =    setupContainerLocalizerForTest (  )  ;", "List < Path >    privCacheList    =    new   ArrayList < Path >  (  )  ;", "List < Path >    appCacheList    =    new   ArrayList < Path >  (  )  ;", "for    ( Path   p    :    localDirs )     {", "Path   base    =    new   Path ( new   Path ( p ,    ContainerLocalizer . USERCACHE )  ,     . appUser )  ;", "Path   privcache    =    new   Path ( base ,    ContainerLocalizer . FILECACHE )  ;", "privCacheList . add ( privcache )  ;", "Path   appDir    =    new   Path ( base ,    new   Path ( ContainerLocalizer . APPCACHE ,     . appId )  )  ;", "Path   appcache    =    new   Path ( appDir ,    ContainerLocalizer . FILECACHE )  ;", "appCacheList . add ( appcache )  ;", "}", "ResourceLocalizationSpec   rsrcA    =     . getMockRsrc ( random ,    PRIVATE ,    privCacheList . get (  0  )  )  ;", "ResourceLocalizationSpec   rsrcB    =     . getMockRsrc ( random ,    PRIVATE ,    privCacheList . get (  0  )  )  ;", "ResourceLocalizationSpec   rsrcC    =     . getMockRsrc ( random ,    APPLICATION ,    appCacheList . get (  0  )  )  ;", "ResourceLocalizationSpec   rsrcD    =     . getMockRsrc ( random ,    PRIVATE ,    privCacheList . get (  0  )  )  ;", "when ( nmProxy . heartbeat ( isA ( LocalizerStatus . class )  )  )  . thenReturn ( new   MockLocalizerHeartbeatResponse ( LocalizerAction . LIVE ,    Collections . singletonList ( rsrcA )  )  )  . thenReturn ( new   MockLocalizerHeartbeatResponse ( LocalizerAction . LIVE ,    Collections . singletonList ( rsrcB )  )  )  . thenReturn ( new   MockLocalizerHeartbeatResponse ( LocalizerAction . LIVE ,    Collections . singletonList ( rsrcC )  )  )  . thenReturn ( new   MockLocalizerHeartbeatResponse ( LocalizerAction . LIVE ,    Collections . singletonList ( rsrcD )  )  )  . thenReturn ( new   MockLocalizerHeartbeatResponse ( LocalizerAction . LIVE ,    Collections .  < ResourceLocalizationSpec > emptyList (  )  )  )  . thenReturn ( new   MockLocalizerHeartbeatResponse ( LocalizerAction . DIE ,    null )  )  ;", "LocalResource   tRsrcA    =    rsrcA . getResource (  )  ;", "LocalResource   tRsrcB    =    rsrcB . getResource (  )  ;", "LocalResource   tRsrcC    =    rsrcC . getResource (  )  ;", "LocalResource   tRsrcD    =    rsrcD . getResource (  )  ;", "doReturn ( new    . FakeDownload ( rsrcA . getResource (  )  . getResource (  )  . getFile (  )  ,    true )  )  . when ( localizer )  . download ( isA ( Path . class )  ,    eq ( tRsrcA )  ,    isA ( UserGroupInformation . class )  )  ;", "doReturn ( new    . FakeDownload ( rsrcB . getResource (  )  . getResource (  )  . getFile (  )  ,    true )  )  . when ( localizer )  . download ( isA ( Path . class )  ,    eq ( tRsrcB )  ,    isA ( UserGroupInformation . class )  )  ;", "doReturn ( new    . FakeDownload ( rsrcC . getResource (  )  . getResource (  )  . getFile (  )  ,    true )  )  . when ( localizer )  . download ( isA ( Path . class )  ,    eq ( tRsrcC )  ,    isA ( UserGroupInformation . class )  )  ;", "doReturn ( new    . FakeDownload ( rsrcD . getResource (  )  . getResource (  )  . getFile (  )  ,    true )  )  . when ( localizer )  . download ( isA ( Path . class )  ,    eq ( tRsrcD )  ,    isA ( UserGroupInformation . class )  )  ;", "assertEquals (  0  ,    localizer . runLocalization (  . nmAddr )  )  ;", "for    ( Path   p    :    localDirs )     {", "Path   base    =    new   Path ( new   Path ( p ,    ContainerLocalizer . USERCACHE )  ,     . appUser )  ;", "Path   privcache    =    new   Path ( base ,    ContainerLocalizer . FILECACHE )  ;", "verify ( spylfs )  . mkdir ( eq ( privcache )  ,    eq (  . CACHE _ DIR _ PERM )  ,    eq ( false )  )  ;", "Path   appDir    =    new   Path ( base ,    new   Path ( ContainerLocalizer . APPCACHE ,     . appId )  )  ;", "Path   appcache    =    new   Path ( appDir ,    ContainerLocalizer . FILECACHE )  ;", "verify ( spylfs )  . mkdir ( eq ( appcache )  ,    eq (  . CACHE _ DIR _ PERM )  ,    eq ( false )  )  ;", "}", "verify ( spylfs )  . open ( tokenPath )  ;", "verify ( nmProxy )  . heartbeat ( argThat ( new    . HBMatches ( rsrcA . getResource (  )  )  )  )  ;", "verify ( nmProxy )  . heartbeat ( argThat ( new    . HBMatches ( rsrcB . getResource (  )  )  )  )  ;", "verify ( nmProxy )  . heartbeat ( argThat ( new    . HBMatches ( rsrcC . getResource (  )  )  )  )  ;", "verify ( nmProxy )  . heartbeat ( argThat ( new    . HBMatches ( rsrcD . getResource (  )  )  )  )  ;", "verify ( nmProxy ,    never (  )  )  . heartbeat ( argThat ( new   ArgumentMatcher < LocalizerStatus >  (  )     {", "@ Override", "public   boolean   matches ( Object   o )     {", "LocalizerStatus   status    =     (  ( LocalizerStatus )     ( o )  )  ;", "return    !  (  . containerId . equals ( status . getLocalizerId (  )  )  )  ;", "}", "}  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLocalizerMain"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "FileContext   fs    =    FileContext . getLocalFSFileContext (  )  ;", "spylfs    =    spy ( fs . getDefaultFileSystem (  )  )  ;", "ContainerLocalizer   localizer    =    setupContainerLocalizerForTest (  )  ;", "doNothing (  )  . when ( localizer )  . localizeFiles ( any ( LocalizationProtocol . class )  ,    any ( CompletionService . class )  ,    any ( UserGroupInformation . class )  )  ;", "localizer . runLocalization (  . nmAddr )  ;", "verify ( spylfs ,    times (  1  )  )  . delete ( tokenPath ,    false )  ;", "}", "METHOD_END"], "methodName": ["testLocalizerTokenIsGettingRemoved"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestContainerLocalizer"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     1  0  0  0  0  ;     +  + i )     {", "String   path    =     . Directory . getRelativePath ( i )  ;", "Assert . assertEquals (  (  \" Incorrect   conversion   for    \"     +    i )  ,    i ,     . Directory . getDirectoryNumber ( path )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDirectoryConversion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,     \"  4  0  \"  )  ;", "dir    =    new    ( conf )  ;", "String   rootPath    =     \"  \"  ;", "String   firstSubDir    =     \"  0  \"  ;", "for    ( int   i    =     0  ;    i    <     4  ;    i +  +  )     {", "Assert . assertEquals ( rootPath ,    dir . getRelativePathForLocalization (  )  )  ;", "}", "dir . decrementFileCountForPath ( rootPath )  ;", "dir . decrementFileCountForPath ( rootPath )  ;", "Assert . assertEquals ( rootPath ,    dir . getRelativePathForLocalization (  )  )  ;", "Assert . assertEquals ( rootPath ,    dir . getRelativePathForLocalization (  )  )  ;", "Assert . assertEquals ( firstSubDir ,    dir . getRelativePathForLocalization (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDirectoryStateChangeFromFullToNonFull"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,     \"  3  7  \"  )  ;", "hDir    =    new    ( conf )  ;", "Assert . assertTrue ( hDir . getRelativePathForLocalization (  )  . isEmpty (  )  )  ;", "for    ( int   i    =     1  ;    i    <  =     (  (  3  7     *     3  6  )     *     3  6  )  ;    i +  +  )     {", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "String   num    =    Integer . toString (  ( i    -     1  )  ,     3  6  )  ;", "if    (  ( num . length (  )  )     =  =     1  )     {", "sb . append ( num . charAt (  0  )  )  ;", "} else    {", "sb . append ( Integer . toString (  (  ( Integer . parseInt ( num . substring (  0  ,     1  )  ,     3  6  )  )     -     1  )  ,     3  6  )  )  ;", "}", "for    ( int   j    =     1  ;    j    <     ( num . length (  )  )  ;    j +  +  )     {", "sb . append ( SEPARATOR )  . append ( num . charAt ( j )  )  ;", "}", "Assert . assertEquals ( sb . toString (  )  ,    hDir . getRelativePathForLocalization (  )  )  ;", "}", "String   testPath 1     =     \"  4  \"  ;", "String   testPath 2     =     \"  2  \"  ;", "hDir . decrementFileCountForPath ( testPath 1  )  ;", "hDir . decrementFileCountForPath ( testPath 2  )  ;", "Assert . assertEquals ( testPath 1  ,    hDir . getRelativePathForLocalization (  )  )  ;", "Assert . assertEquals ( testPath 2  ,    hDir . getRelativePathForLocalization (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHierarchicalSubDirectoryCreation"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,     (  (  . DIRECTORIES _ PER _ LEVEL )     +     2  )  )  ;", "mgr    =    new    ( conf )  ;", "final   String   rootPath    =     \"  \"  ;", "mgr . incrementFileCountForPath ( rootPath )  ;", "Assert . assertEquals ( rootPath ,    mgr . getRelativePathForLocalization (  )  )  ;", "Assert . assertFalse (  \" root   dir   should   be   full \"  ,    rootPath . equals ( mgr . getRelativePathForLocalization (  )  )  )  ;", "mgr . getRelativePathForLocalization (  )  ;", "mgr . decrementFileCountForPath ( rootPath )  ;", "mgr . decrementFileCountForPath ( rootPath )  ;", "Assert . assertEquals ( rootPath ,    mgr . getRelativePathForLocalization (  )  )  ;", "Assert . assertEquals ( rootPath ,    mgr . getRelativePathForLocalization (  )  )  ;", "String   otherDir    =    mgr . getRelativePathForLocalization (  )  ;", "Assert . assertFalse (  \" root   dir   should   be   full \"  ,    otherDir . equals ( rootPath )  )  ;", "final   String   deepDir 0     =     \" d / e / e / p /  0  \"  ;", "final   String   deepDir 1     =     \" d / e / e / p /  1  \"  ;", "final   String   deepDir 2     =     \" d / e / e / p /  2  \"  ;", "final   String   deepDir 3     =     \" d / e / e / p /  3  \"  ;", "mgr . incrementFileCountForPath ( deepDir 0  )  ;", "Assert . assertEquals ( otherDir ,    mgr . getRelativePathForLocalization (  )  )  ;", "Assert . assertEquals ( deepDir 0  ,    mgr . getRelativePathForLocalization (  )  )  ;", "Assert . assertEquals (  \" total   dir   count   incorrect   after   increment \"  ,    deepDir 1  ,    mgr . getRelativePathForLocalization (  )  )  ;", "mgr . incrementFileCountForPath ( deepDir 2  )  ;", "mgr . incrementFileCountForPath ( deepDir 1  )  ;", "mgr . incrementFileCountForPath ( deepDir 2  )  ;", "Assert . assertEquals ( deepDir 3  ,    mgr . getRelativePathForLocalization (  )  )  ;", "}", "METHOD_END"], "methodName": ["testIncrementFileCountForPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,     \"  1  \"  )  ;", "Exception   e    =    null ;", "ResourceLationService   service    =    new   ResourceLationService ( null ,    null ,    null ,    null ,    null )  ;", "try    {", "service . init ( conf )  ;", "}    catch    ( Exception   e 1  )     {", "e    =    e 1  ;", "}", "Assert . assertNotNull ( e )  ;", "Assert . assertEquals ( YarnRuntimeException . class ,    e . getClass (  )  )  ;", "Assert . assertEquals ( e . getMessage (  )  ,     (  ( YarnConfiguration . NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY )     +     \"    parameter   is   configured   with   a   value   less   than    3  7  .  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testMinimumPerDirectoryFileLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalCacheDirectoryManager"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( a ,    b )  ;", "assertEquals ( a . hashCode (  )  ,    b . hashCode (  )  )  ;", "assertEquals (  0  ,    a . compareTo ( b )  )  ;", "assertEquals (  0  ,    b . compareTo ( a )  )  ;", "}", "METHOD_END"], "methodName": ["checkEqual"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource"}, {"methodBody": ["METHOD_START", "{", "assertFalse ( a . equals ( b )  )  ;", "assertFalse ( b . equals ( a )  )  ;", "assertFalse (  (  ( a . hashCode (  )  )     =  =     ( b . hashCode (  )  )  )  )  ;", "assertFalse (  (  0     =  =     ( a . compareTo ( b )  )  )  )  ;", "assertFalse (  (  0     =  =     ( b . compareTo ( a )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["checkNotEqual"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource"}, {"methodBody": ["METHOD_START", "{", "LocalResource   ret    =    RecordFactoryProvider . getRecordFactory ( null )  . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromURI ( p . toUri (  )  )  )  ;", "ret . setSize ( size )  ;", "ret . setTimestamp ( timestamp )  ;", "ret . setType ( type )  ;", "ret . setVisibility ( state )  ;", "ret . setPattern ( pattern )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["getYarnResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource"}, {"methodBody": ["METHOD_START", "{", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "long   basetime    =     ( r . nextLong (  )  )     >  >  >     2  ;", "LocalResource   yA    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    FILE ,    PUBLIC ,    null )  ;", "LocalResource   yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    FILE ,    PUBLIC ,    null )  ;", "final   LocalResourceRequest   a    =    new   LocalResourceRequest ( yA )  ;", "LocalResourceRequest   b    =    new   LocalResourceRequest ( yA )  ;", ". checkEqual ( a ,    b )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkEqual ( a ,    b )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    FILE ,    PRIVATE ,    null )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkEqual ( a ,    b )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     0  ,    basetime ,    FILE ,    PRIVATE ,    null )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkEqual ( a ,    b )  ;", "yB    =     . getYarnResource ( new   Path (  \" hdfs :  /  / dingo . org :  8  0  / foobar \"  )  ,     0  ,    basetime ,    ARCHIVE ,    PUBLIC ,    null )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkNotEqual ( a ,    b )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     0  ,    basetime ,    ARCHIVE ,    PUBLIC ,    null )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkNotEqual ( a ,    b )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     0  ,     ( basetime    +     1  )  ,    FILE ,    PUBLIC ,    null )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkNotEqual ( a ,    b )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     0  ,     ( basetime    +     1  )  ,    FILE ,    PUBLIC ,     \"  ^  / foo /  .  *  \"  )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", ". checkNotEqual ( a ,    b )  ;", "}", "METHOD_END"], "methodName": ["testResourceEquality"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource"}, {"methodBody": ["METHOD_START", "{", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "r . setSeed ( seed )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "long   basetime    =     ( r . nextLong (  )  )     >  >  >     2  ;", "LocalResource   yA    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    FILE ,    PUBLIC ,     \"  ^  / foo /  .  *  \"  )  ;", "final   LocalResourceRequest   a    =    new   LocalResourceRequest ( yA )  ;", "LocalResource   yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobaz \"  )  ,     (  -  1  )  ,    basetime ,    FILE ,    PUBLIC ,     \"  ^  / foo /  .  *  \"  )  ;", "LocalResourceRequest   b    =    new   LocalResourceRequest ( yB )  ;", "assertTrue (  (  0     >     ( a . compareTo ( b )  )  )  )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,     ( basetime    +     1  )  ,    FILE ,    PUBLIC ,     \"  ^  / foo /  .  *  \"  )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", "assertTrue (  (  0     >     ( a . compareTo ( b )  )  )  )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    ARCHIVE ,    PUBLIC ,     \"  ^  / foo /  .  *  \"  )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", "assertTrue (  (  0     !  =     ( a . compareTo ( b )  )  )  )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    ARCHIVE ,    PUBLIC ,     \"  ^  / food /  .  *  \"  )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", "assertTrue (  (  0     !  =     ( a . compareTo ( b )  )  )  )  ;", "yB    =     . getYarnResource ( new   Path (  \" http :  /  / yak . org :  8  0  / foobar \"  )  ,     (  -  1  )  ,    basetime ,    ARCHIVE ,    PUBLIC ,    null )  ;", "b    =    new   LocalResourceRequest ( yB )  ;", "assertTrue (  (  0     !  =     ( a . compareTo ( b )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceOrder"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResource"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "return   dispatcher ;", "}", "METHOD_END"], "methodName": ["createDispatcher"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "final   LocalResourceRequest   req    =    new   LocalResourceRequest ( new   Path (  (  (  (  \" file :  /  /  / tmp /  \"     +    user )     +     \"  / rsrc \"  )     +    i )  )  ,     ( ts    +     ( i    *     2  0  0  0  )  )  ,    LocalResourceType . FILE ,    vis ,    null )  ;", "return   req ;", "}", "METHOD_END"], "methodName": ["createLocalResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "LocalizedResource   lr    =    new   LocalizedResource ( req ,    dispatcher )  ;", "return   lr ;", "}", "METHOD_END"], "methodName": ["createLocalizedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "boolean   ret    =    false ;", "File   file    =    new   File ( path . toUri (  )  . getRawPath (  )  . toString (  )  )  ;", "try    {", "ret    =    file . createNewFile (  )  ;", "}    catch    ( IOException   e )     {", "e . printStacke (  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["createdummylocalizefile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" testuser \"  ;", "DrainDispatcher   dispatcher    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "DeletionService   mockDelService    =    mock ( DeletionService . class )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalizerContext   lc 1     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "ContainerId   cId 2     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     2  )  ;", "LocalizerContext   lc 2     =    new   LocalizerContext ( user ,    cId 2  ,    null )  ;", "LocalResourceRequest   req 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    PUBLIC )  ;", "LocalResourceRequest   req 2     =    createLocalResourceRequest ( user ,     2  ,     1  ,    PUBLIC )  ;", "LocalizedResource   lr 1     =    createLocalizedResource ( req 1  ,    dispatcher )  ;", "LocalizedResource   lr 2     =    createLocalizedResource ( req 2  ,    dispatcher )  ;", "ConcurrentMap < LocalResourceRequest ,    LocalizedResource >    localrsrc    =    new   ConcurrentHashMap < LocalResourceRequest ,    LocalizedResource >  (  )  ;", "localrsrc . put ( req 1  ,    lr 1  )  ;", "localrsrc . put ( req 2  ,    lr 2  )  ;", "LocalResourcesTracker   tracker    =    new    ( user ,    null ,    dispatcher ,    localrsrc ,    false ,    conf ,    new   NMNullStateStoreService (  )  )  ;", "ResourceEvent   req 1  1 Event    =    new   ResourceRequestEvent ( req 1  ,    LocalResourceVisibility . PUBLIC ,    lc 1  )  ;", "ResourceEvent   req 1  2 Event    =    new   ResourceRequestEvent ( req 1  ,    LocalResourceVisibility . PUBLIC ,    lc 2  )  ;", "ResourceEvent   req 2  1 Event    =    new   ResourceRequestEvent ( req 2  ,    LocalResourceVisibility . PUBLIC ,    lc 1  )  ;", "ResourceEvent   rel 1  1 Event    =    new   ResourceReleaseEvent ( req 1  ,    cId 1  )  ;", "ResourceEvent   rel 1  2 Event    =    new   ResourceReleaseEvent ( req 1  ,    cId 2  )  ;", "ResourceEvent   rel 2  1 Event    =    new   ResourceReleaseEvent ( req 2  ,    cId 1  )  ;", "tracker . handle ( req 1  1 Event )  ;", "tracker . handle ( req 1  2 Event )  ;", "tracker . handle ( req 2  1 Event )  ;", "dispatcher . await (  )  ;", "verify ( localizerEventHandler ,    times (  3  )  )  . handle ( any ( LocalizerResourceRequestEvent . class )  )  ;", "Assert . assertEquals (  2  ,    lr 1  . getRefCount (  )  )  ;", "Assert . assertEquals (  1  ,    lr 2  . getRefCount (  )  )  ;", "tracker . handle ( rel 2  1 Event )  ;", "dispatcher . await (  )  ;", "verifyTrackedResourceCount ( tracker ,     2  )  ;", "Assert . assertEquals (  2  ,    lr 1  . getRefCount (  )  )  ;", "Assert . assertFalse ( tracker . remove ( lr 1  ,    mockDelService )  )  ;", "verifyTrackedResourceCount ( tracker ,     2  )  ;", "ResourceLocalizedEvent   rle    =    new   ResourceLocalizedEvent ( req 1  ,    new   Path (  \" file :  /  /  / tmp / r 1  \"  )  ,     1  )  ;", "lr 1  . handle ( rle )  ;", "Assert . assertTrue ( lr 1  . getState (  )  . equals ( ResourceState . LOCALIZED )  )  ;", "tracker . handle ( rel 1  1 Event )  ;", "tracker . handle ( rel 1  2 Event )  ;", "Assert . assertEquals (  0  ,    lr 1  . getRefCount (  )  )  ;", "Assert . assertTrue ( tracker . remove ( lr 1  ,    mockDelService )  )  ;", "verifyTrackedResourceCount ( tracker ,     1  )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" testuser \"  ;", "DrainDispatcher   dispatcher    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalizerContext   lc 1     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "LocalResourceRequest   req 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    PUBLIC )  ;", "LocalizedResource   lr 1     =    createLocalizedResource ( req 1  ,    dispatcher )  ;", "ConcurrentMap < LocalResourceRequest ,    LocalizedResource >    localrsrc    =    new   ConcurrentHashMap < LocalResourceRequest ,    LocalizedResource >  (  )  ;", "localrsrc . put ( req 1  ,    lr 1  )  ;", "LocalResourcesTracker   tracker    =    new    ( user ,    null ,    dispatcher ,    localrsrc ,    false ,    conf ,    new   NMNullStateStoreService (  )  )  ;", "ResourceEvent   req 1  1 Event    =    new   ResourceRequestEvent ( req 1  ,    LocalResourceVisibility . PUBLIC ,    lc 1  )  ;", "ResourceEvent   rel 1  1 Event    =    new   ResourceReleaseEvent ( req 1  ,    cId 1  )  ;", "tracker . handle ( req 1  1 Event )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  1  ,    lr 1  . getRefCount (  )  )  ;", "dispatcher . await (  )  ;", "verifyTrackedResourceCount ( tracker ,     1  )  ;", "ResourceLocalizedEvent   rle    =    new   ResourceLocalizedEvent ( req 1  ,    new   Path (  \" file :  /  /  / tmp / r 1  \"  )  ,     1  )  ;", "lr 1  . handle ( rle )  ;", "Assert . assertTrue ( lr 1  . getState (  )  . equals ( ResourceState . LOCALIZED )  )  ;", "Assert . assertTrue ( createdummylocalizefile ( new   Path (  \" file :  /  /  / tmp / r 1  \"  )  )  )  ;", "LocalizedResource   rsrcbefore    =    tracker . iterator (  )  . next (  )  ;", "File   resFile    =    new   File ( lr 1  . getLocalPath (  )  . toUri (  )  . getRawPath (  )  . toString (  )  )  ;", "Assert . assertTrue ( resFile . exists (  )  )  ;", "Assert . assertTrue ( resFile . delete (  )  )  ;", "tracker . handle ( req 1  1 Event )  ;", "dispatcher . await (  )  ;", "lr 1  . handle ( rle )  ;", "Assert . assertTrue ( lr 1  . getState (  )  . equals ( ResourceState . LOCALIZED )  )  ;", "LocalizedResource   rsrcafter    =    tracker . iterator (  )  . next (  )  ;", "if    ( rsrcbefore    =  =    rsrcafter )     {", "Assert . fail (  \" Localized   resource   should   not   be   equal \"  )  ;", "}", "tracker . handle ( rel 1  1 Event )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testConsistency"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" testuser \"  ;", "DrainDispatcher   dispatcher    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,     \"  3  7  \"  )  ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "DeletionService   mockDelService    =    mock ( DeletionService . class )  ;", "ConcurrentMap < LocalResourceRequest ,    LocalizedResource >    localrsrc    =    new   ConcurrentHashMap < LocalResourceRequest ,    LocalizedResource >  (  )  ;", "LocalResourcesTracker   tracker    =    new    ( user ,    null ,    dispatcher ,    localrsrc ,    true ,    conf ,    new   NMNullStateStoreService (  )  )  ;", "Path   localDir    =    new   Path (  \"  / tmp \"  )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalResourceRequest   lr 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    PUBLIC )  ;", "LocalizerContext   lc 1     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "ResourceEvent   reqEvent 1     =    new   ResourceRequestEvent ( lr 1  ,    LocalResourceVisibility . PUBLIC ,    lc 1  )  ;", "tracker . handle ( reqEvent 1  )  ;", "Path   hierarchicalPath 1     =    tracker . getPathForLocalization ( lr 1  ,    localDir )  . getParent (  )  ;", "ResourceLocalizedEvent   rle 1     =    new   ResourceLocalizedEvent ( lr 1  ,    new   Path (  (  (  ( hierarchicalPath 1  . toUri (  )  . toString (  )  )     +     ( Path . SEPARATOR )  )     +     \" file 1  \"  )  )  ,     1  2  0  )  ;", "tracker . handle ( rle 1  )  ;", "LocalResourceRequest   lr 2     =    createLocalResourceRequest ( user ,     3  ,     3  ,    PUBLIC )  ;", "ResourceEvent   reqEvent 2     =    new   ResourceRequestEvent ( lr 2  ,    LocalResourceVisibility . PUBLIC ,    lc 1  )  ;", "tracker . handle ( reqEvent 2  )  ;", "Path   hierarchicalPath 2     =    tracker . getPathForLocalization ( lr 2  ,    localDir )  . getParent (  )  ;", "ResourceFailedLocalizationEvent   rfe 2     =    new   ResourceFailedLocalizationEvent ( lr 2  ,    new   Exception (  \" Test \"  )  . toString (  )  )  ;", "tracker . handle ( rfe 2  )  ;", "Assert . assertNotSame ( hierarchicalPath 1  ,    hierarchicalPath 2  )  ;", "LocalResourceRequest   lr 3     =    createLocalResourceRequest ( user ,     2  ,     2  ,    PUBLIC )  ;", "ResourceEvent   reqEvent 3     =    new   ResourceRequestEvent ( lr 3  ,    LocalResourceVisibility . PUBLIC ,    lc 1  )  ;", "tracker . handle ( reqEvent 3  )  ;", "Path   hierarchicalPath 3     =    tracker . getPathForLocalization ( lr 3  ,    localDir )  . getParent (  )  ;", "ResourceLocalizedEvent   rle 3     =    new   ResourceLocalizedEvent ( lr 3  ,    new   Path (  (  (  ( hierarchicalPath 3  . toUri (  )  . toString (  )  )     +     ( Path . SEPARATOR )  )     +     \" file 3  \"  )  )  ,     1  2  0  )  ;", "tracker . handle ( rle 3  )  ;", "Assert . assertEquals ( hierarchicalPath 3  . toUri (  )  . toString (  )  ,     (  (  ( hierarchicalPath 1  . toUri (  )  . toString (  )  )     +     ( Path . SEPARATOR )  )     +     \"  0  \"  )  )  ;", "ResourceEvent   relEvent 1     =    new   ResourceReleaseEvent ( lr 1  ,    cId 1  )  ;", "tracker . handle ( relEvent 1  )  ;", "int   resources    =     0  ;", "Iterator < LocalizedResource >    iter    =    tracker . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "iter . next (  )  ;", "resources +  +  ;", "}", "Assert . assertEquals (  2  ,    resources )  ;", "iter    =    tracker . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "LocalizedResource   rsrc    =    iter . next (  )  ;", "if    (  ( rsrc . getRefCount (  )  )     =  =     0  )     {", "Assert . assertTrue ( tracker . remove ( rsrc ,    mockDelService )  )  ;", "resources -  -  ;", "}", "}", "Assert . assertEquals (  1  ,    resources )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testHierarchicalLocalCacheDirectories"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" testuser \"  ;", "DrainDispatcher   dispatcher    =    null ;", "try    {", "Configuration   conf    =    new   Configuration (  )  ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < ContainerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "ConcurrentMap < LocalResourceRequest ,    LocalizedResource >    localrsrc    =    new   ConcurrentHashMap < LocalResourceRequest ,    LocalizedResource >  (  )  ;", "LocalResourcesTracker   tracker    =    new    ( user ,    null ,    dispatcher ,    localrsrc ,    true ,    conf ,    new   NMNullStateStoreService (  )  )  ;", "LocalResourceRequest   lr    =    createLocalResourceRequest ( user ,     1  ,     1  ,    PUBLIC )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalizerContext   lc 1     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "ResourceEvent   reqEvent 1     =    new   ResourceRequestEvent ( lr ,    LocalResourceVisibility . PRIVATE ,    lc 1  )  ;", "Assert . assertEquals (  0  ,    localrsrc . size (  )  )  ;", "tracker . handle ( reqEvent 1  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  1  ,    localrsrc . size (  )  )  ;", "Assert . assertTrue ( localrsrc . containsKey ( lr )  )  ;", "Assert . assertEquals (  1  ,    localrsrc . get ( lr )  . getRefCount (  )  )  ;", "Assert . assertTrue ( localrsrc . get ( lr )  . ref . contains ( cId 1  )  )  ;", "Assert . assertEquals ( ResourceState . DOWNLOADING ,    localrsrc . get ( lr )  . getState (  )  )  ;", "ContainerId   cId 2     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     2  )  ;", "LocalizerContext   lc 2     =    new   LocalizerContext ( user ,    cId 2  ,    null )  ;", "ResourceEvent   reqEvent 2     =    new   ResourceRequestEvent ( lr ,    LocalResourceVisibility . PRIVATE ,    lc 2  )  ;", "tracker . handle ( reqEvent 2  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  2  ,    localrsrc . get ( lr )  . getRefCount (  )  )  ;", "Assert . assertTrue ( localrsrc . get ( lr )  . ref . contains ( cId 2  )  )  ;", "ResourceEvent   resourceFailedEvent    =    new   ResourceFailedLocalizationEvent ( lr ,    new   Exception (  \" test \"  )  . getMessage (  )  )  ;", "LocalizedResource   localizedResource    =    localrsrc . get ( lr )  ;", "tracker . handle ( resourceFailedEvent )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  0  ,    localrsrc . size (  )  )  ;", "verify ( containerEventHandler ,    times (  2  )  )  . handle ( isA ( ContainerResourceFailedEvent . class )  )  ;", "Assert . assertEquals ( ResourceState . FAILED ,    localizedResource . getState (  )  )  ;", "ResourceReleaseEvent   relEvent 1     =    new   ResourceReleaseEvent ( lr ,    cId 1  )  ;", "tracker . handle ( relEvent 1  )  ;", "dispatcher . await (  )  ;", "ContainerId   cId 3     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     3  )  ;", "LocalizerContext   lc 3     =    new   LocalizerContext ( user ,    cId 3  ,    null )  ;", "ResourceEvent   reqEvent 3     =    new   ResourceRequestEvent ( lr ,    LocalResourceVisibility . PRIVATE ,    lc 3  )  ;", "tracker . handle ( reqEvent 3  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  1  ,    localrsrc . size (  )  )  ;", "Assert . assertTrue ( localrsrc . containsKey ( lr )  )  ;", "Assert . assertEquals (  1  ,    localrsrc . get ( lr )  . getRefCount (  )  )  ;", "Assert . assertTrue ( localrsrc . get ( lr )  . ref . contains ( cId 3  )  )  ;", "ResourceReleaseEvent   relEvent 2     =    new   ResourceReleaseEvent ( lr ,    cId 2  )  ;", "tracker . handle ( relEvent 2  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  1  ,    localrsrc . size (  )  )  ;", "Assert . assertTrue ( localrsrc . containsKey ( lr )  )  ;", "Assert . assertEquals (  1  ,    localrsrc . get ( lr )  . getRefCount (  )  )  ;", "Assert . assertTrue ( localrsrc . get ( lr )  . ref . contains ( cId 3  )  )  ;", "Path   localizedPath    =    new   Path (  \"  / tmp / file 1  \"  )  ;", "ResourceLocalizedEvent   localizedEvent    =    new   ResourceLocalizedEvent ( lr ,    localizedPath ,     1  2  3 L )  ;", "tracker . handle ( localizedEvent )  ;", "dispatcher . await (  )  ;", "verify ( containerEventHandler ,    times (  1  )  )  . handle ( isA ( ContainerResourceLocalizedEvent . class )  )  ;", "Assert . assertEquals ( ResourceState . LOCALIZED ,    localrsrc . get ( lr )  . getState (  )  )  ;", "Assert . assertEquals (  1  ,    localrsrc . get ( lr )  . getRefCount (  )  )  ;", "ResourceReleaseEvent   relEvent 3     =    new   ResourceReleaseEvent ( lr ,    cId 3  )  ;", "tracker . handle ( relEvent 3  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  0  ,    localrsrc . get ( lr )  . getRefCount (  )  )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLocalResourceCache"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "final   String   user    =     \" someuser \"  ;", "final   ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "final   Path   localDir    =    new   Path (  \"  / tmp / localdir \"  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "DrainDispatcher   dispatcher    =    null ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "NMStateStoreService   stateStore    =    mock ( NMStateStoreService . class )  ;", "try    {", "LocalResourcesTracker   tracker    =    new    ( user ,    appId ,    dispatcher ,    false ,    conf ,    stateStore )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalResourceRequest   lr 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    APPLICATION )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( lr 1  )  )  ;", "final   long   localizedId 1     =     5  2  ;", "Path   hierarchicalPath 1     =    new   Path ( localDir ,    Long . toString ( localizedId 1  )  )  ;", "Path   localizedPath 1     =    new   Path ( hierarchicalPath 1  ,     \" resource . jar \"  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( lr 1  ,    localizedPath 1  ,     1  2  0  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertNotNull ( tracker . getLocalizedResource ( lr 1  )  )  ;", "LocalResourceRequest   lr 2     =    createLocalResourceRequest ( user ,     2  ,     2  ,    APPLICATION )  ;", "LocalizerContext   lc 2     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "ResourceEvent   reqEvent 2     =    new   ResourceRequestEvent ( lr 2  ,    LocalResourceVisibility . APPLICATION ,    lc 2  )  ;", "tracker . handle ( reqEvent 2  )  ;", "dispatcher . await (  )  ;", "Path   hierarchicalPath 2     =    tracker . getPathForLocalization ( lr 2  ,    localDir )  ;", "long   localizedId 2     =    Long . parseLong ( hierarchicalPath 2  . getName (  )  )  ;", "Assert . assertEquals (  ( localizedId 1     +     1  )  ,    localizedId 2  )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testRecoveredResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "final   String   user    =     \" someuser \"  ;", "final   ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "final   Path   localDirRoot    =    new   Path (  \"  / tmp / localdir \"  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "DrainDispatcher   dispatcher    =    null ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "NMStateStoreService   stateStore    =    mock ( NMStateStoreService . class )  ;", "try    {", "tracker    =    new    ( user ,    appId ,    dispatcher ,    true ,    conf ,    stateStore )  ;", "LocalResourceRequest   lr 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    PUBLIC )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( lr 1  )  )  ;", "final   long   localizedId 1     =     5  2  ;", "Path   hierarchicalPath 1     =    new   Path (  ( localDirRoot    +     \"  /  4  /  2  \"  )  ,    Long . toString ( localizedId 1  )  )  ;", "Path   localizedPath 1     =    new   Path ( hierarchicalPath 1  ,     \" resource . jar \"  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( lr 1  ,    localizedPath 1  ,     1  2  0  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertNotNull ( tracker . getLocalizedResource ( lr 1  )  )  ;", "LocalCacheDirectoryManager   dirMgrRoot    =    tracker . getDirectoryManager ( localDirRoot )  ;", "Assert . assertEquals (  0  ,    dirMgrRoot . getDirectory (  \"  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  1  ,    dirMgrRoot . getDirectory (  \"  4  /  2  \"  )  . getCount (  )  )  ;", "LocalResourceRequest   lr 2     =    createLocalResourceRequest ( user ,     2  ,     2  ,    PUBLIC )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( lr 2  )  )  ;", "final   long   localizedId 2     =    localizedId 1     +     1  ;", "Path   hierarchicalPath 2     =    new   Path (  ( localDirRoot    +     \"  /  4  /  2  \"  )  ,    Long . toString ( localizedId 2  )  )  ;", "Path   localizedPath 2     =    new   Path ( hierarchicalPath 2  ,     \" resource . jar \"  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( lr 2  ,    localizedPath 2  ,     1  2  0  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertNotNull ( tracker . getLocalizedResource ( lr 2  )  )  ;", "Assert . assertEquals (  0  ,    dirMgrRoot . getDirectory (  \"  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  2  ,    dirMgrRoot . getDirectory (  \"  4  /  2  \"  )  . getCount (  )  )  ;", "LocalResourceRequest   lr 3     =    createLocalResourceRequest ( user ,     3  ,     3  ,    PUBLIC )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( lr 3  )  )  ;", "final   long   localizedId 3     =     1  2  8  ;", "Path   hierarchicalPath 3     =    new   Path (  ( localDirRoot    +     \"  /  4  /  3  \"  )  ,    Long . toString ( localizedId 3  )  )  ;", "Path   localizedPath 3     =    new   Path ( hierarchicalPath 3  ,     \" resource . jar \"  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( lr 3  ,    localizedPath 3  ,     1  2  0  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertNotNull ( tracker . getLocalizedResource ( lr 3  )  )  ;", "Assert . assertEquals (  0  ,    dirMgrRoot . getDirectory (  \"  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  2  ,    dirMgrRoot . getDirectory (  \"  4  /  2  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  1  ,    dirMgrRoot . getDirectory (  \"  4  /  3  \"  )  . getCount (  )  )  ;", "LocalResourceRequest   lr 4     =    createLocalResourceRequest ( user ,     4  ,     4  ,    PUBLIC )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( lr 4  )  )  ;", "final   long   localizedId 4     =     2  5  6  ;", "Path   hierarchicalPath 4     =    new   Path (  ( localDirRoot    +     \"  /  4  \"  )  ,    Long . toString ( localizedId 4  )  )  ;", "Path   localizedPath 4     =    new   Path ( hierarchicalPath 4  ,     \" resource . jar \"  )  ;", "tracker . handle ( new   ResourceRecoveredEvent ( lr 4  ,    localizedPath 4  ,     1  2  0  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertNotNull ( tracker . getLocalizedResource ( lr 4  )  )  ;", "Assert . assertEquals (  0  ,    dirMgrRoot . getDirectory (  \"  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  1  ,    dirMgrRoot . getDirectory (  \"  4  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  2  ,    dirMgrRoot . getDirectory (  \"  4  /  2  \"  )  . getCount (  )  )  ;", "Assert . assertEquals (  1  ,    dirMgrRoot . getDirectory (  \"  4  /  3  \"  )  . getCount (  )  )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testRecoveredResourceWithDirCacheMgr"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "final   String   user    =     \" someuser \"  ;", "final   ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "final   Path   localDir    =    new   Path (  \"  / tmp \"  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "DrainDispatcher   dispatcher    =    null ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "NMStateStoreService   stateStore    =    mock ( NMStateStoreService . class )  ;", "try    {", "LocalResourcesTracker   tracker    =    new    ( user ,    appId ,    dispatcher ,    false ,    conf ,    stateStore )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalResourceRequest   lr 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    APPLICATION )  ;", "LocalizerContext   lc 1     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "ResourceEvent   reqEvent 1     =    new   ResourceRequestEvent ( lr 1  ,    LocalResourceVisibility . APPLICATION ,    lc 1  )  ;", "tracker . handle ( reqEvent 1  )  ;", "dispatcher . await (  )  ;", "Path   hierarchicalPath 1     =    tracker . getPathForLocalization ( lr 1  ,    localDir )  ;", "ArgumentCaptor < LocalResourceProto >    localResourceCaptor    =    ArgumentCaptor . forClass ( LocalResourceProto . class )  ;", "ArgumentCaptor < Path >    pathCaptor    =    ArgumentCaptor . forClass ( Path . class )  ;", "verify ( stateStore )  . startResourceLocalization ( eq ( user )  ,    eq ( appId )  ,    localResourceCaptor . capture (  )  ,    pathCaptor . capture (  )  )  ;", "LocalResourceProto   lrProto    =    localResourceCaptor . getValue (  )  ;", "Path   localizedPath 1     =    pathCaptor . getValue (  )  ;", "Assert . assertEquals ( lr 1  ,    new   LocalResourceRequest ( new   LocalResourcePBImpl ( lrProto )  )  )  ;", "Assert . assertEquals ( hierarchicalPath 1  ,    localizedPath 1  . getParent (  )  )  ;", "ResourceFailedLocalizationEvent   rfe 1     =    new   ResourceFailedLocalizationEvent ( lr 1  ,    new   Exception (  \" Test \"  )  . toString (  )  )  ;", "tracker . handle ( rfe 1  )  ;", "dispatcher . await (  )  ;", "verify ( stateStore )  . removeLocalizedResource ( eq ( user )  ,    eq ( appId )  ,    eq ( localizedPath 1  )  )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testStateStoreFailedLocalization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "final   String   user    =     \" someuser \"  ;", "final   ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "final   Path   localDir    =    new   Path (  \"  / tmp \"  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "DrainDispatcher   dispatcher    =    null ;", "dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < LocalizerEvent >    localizerEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    containerEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerEventHandler )  ;", "dispatcher . register ( ContainerEventType . class ,    containerEventHandler )  ;", "DeletionService   mockDelService    =    mock ( DeletionService . class )  ;", "NMStateStoreService   stateStore    =    mock ( NMStateStoreService . class )  ;", "try    {", "LocalResourcesTracker   tracker    =    new    ( user ,    appId ,    dispatcher ,    false ,    conf ,    stateStore )  ;", "ContainerId   cId 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "LocalResourceRequest   lr 1     =    createLocalResourceRequest ( user ,     1  ,     1  ,    APPLICATION )  ;", "LocalizerContext   lc 1     =    new   LocalizerContext ( user ,    cId 1  ,    null )  ;", "ResourceEvent   reqEvent 1     =    new   ResourceRequestEvent ( lr 1  ,    LocalResourceVisibility . APPLICATION ,    lc 1  )  ;", "tracker . handle ( reqEvent 1  )  ;", "dispatcher . await (  )  ;", "Path   hierarchicalPath 1     =    tracker . getPathForLocalization ( lr 1  ,    localDir )  ;", "ArgumentCaptor < LocalResourceProto >    localResourceCaptor    =    ArgumentCaptor . forClass ( LocalResourceProto . class )  ;", "ArgumentCaptor < Path >    pathCaptor    =    ArgumentCaptor . forClass ( Path . class )  ;", "verify ( stateStore )  . startResourceLocalization ( eq ( user )  ,    eq ( appId )  ,    localResourceCaptor . capture (  )  ,    pathCaptor . capture (  )  )  ;", "LocalResourceProto   lrProto    =    localResourceCaptor . getValue (  )  ;", "Path   localizedPath 1     =    pathCaptor . getValue (  )  ;", "Assert . assertEquals ( lr 1  ,    new   LocalResourceRequest ( new   LocalResourcePBImpl ( lrProto )  )  )  ;", "Assert . assertEquals ( hierarchicalPath 1  ,    localizedPath 1  . getParent (  )  )  ;", "ResourceLocalizedEvent   rle 1     =    new   ResourceLocalizedEvent ( lr 1  ,    pathCaptor . getValue (  )  ,     1  2  0  )  ;", "tracker . handle ( rle 1  )  ;", "dispatcher . await (  )  ;", "ArgumentCaptor < LocalizedResourceProto >    localizedProtoCaptor    =    ArgumentCaptor . forClass ( LocalizedResourceProto . class )  ;", "verify ( stateStore )  . finishResourceLocalization ( eq ( user )  ,    eq ( appId )  ,    localizedProtoCaptor . capture (  )  )  ;", "LocalizedResourceProto   localizedProto    =    localizedProtoCaptor . getValue (  )  ;", "Assert . assertEquals ( lr 1  ,    new   LocalResourceRequest ( new   LocalResourcePBImpl ( localizedProto . getResource (  )  )  )  )  ;", "Assert . assertEquals ( localizedPath 1  . toString (  )  ,    localizedProto . getLocalPath (  )  )  ;", "LocalizedResource   localizedRsrc 1     =    tracker . getLocalizedResource ( lr 1  )  ;", "Assert . assertNotNull ( localizedRsrc 1  )  ;", "tracker . handle ( new   ResourceReleaseEvent ( lr 1  ,    cId 1  )  )  ;", "dispatcher . await (  )  ;", "boolean   removeResult    =    tracker . remove ( localizedRsrc 1  ,    mockDelService )  ;", "Assert . assertTrue ( removeResult )  ;", "verify ( stateStore )  . removeLocalizedResource ( eq ( user )  ,    eq ( appId )  ,    eq ( localizedPath 1  )  )  ;", "}    finally    {", "if    ( dispatcher    !  =    null )     {", "dispatcher . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testStateStoreSuccessfulLocalization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "int   count    =     0  ;", "Iterator < Localized >    iter    =    tracker . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "iter . next (  )  ;", "count +  +  ;", "}", "Assert . assertEquals (  \" Tracker   resource   count   does   not   match \"  ,    expected ,    count )  ;", "}", "METHOD_END"], "methodName": ["verifyTrackedResourceCount"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalResourcesTrackerImpl"}, {"methodBody": ["METHOD_START", "{", "URL   uriA    =    mock ( URL . class )  ;", "when ( uriA . getScheme (  )  )  . thenReturn (  \" file \"  )  ;", "when ( uriA . getHost (  )  )  . thenReturn ( null )  ;", "when ( uriA . getFile (  )  )  . thenReturn (  \"  / localA / rsrc \"  )  ;", "Local   apiRsrc    =    mock ( Local . class )  ;", "when ( apiRsrc . get (  )  )  . thenReturn ( uriA )  ;", "when ( apiRsrc . getTimestamp (  )  )  . thenReturn (  4  3  4  4 L )  ;", "when ( apiRsrc . getType (  )  )  . thenReturn ( FILE )  ;", "return   apiRsrc ;", "}", "METHOD_END"], "methodName": ["createMockResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalizedResource"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    mock ( ApplicationId . class )  ;", "when ( appId . getClusterTimestamp (  )  )  . thenReturn (  3  1  4  1  5  9  2  6  5 L )  ;", "when ( appId . getId (  )  )  . thenReturn (  3  )  ;", "ApplicationAttemptId   appAttemptId    =    mock ( ApplicationAttemptId . class )  ;", "when ( appAttemptId . getApplicationId (  )  )  . thenReturn ( appId )  ;", "when ( appAttemptId . getAttemptId (  )  )  . thenReturn (  0  )  ;", "ContainerId       =    mock ( ContainerId . class )  ;", "when (  . getId (  )  )  . thenReturn ( id )  ;", "when (  . getApplicationAttemptId (  )  )  . thenReturn ( appAttemptId )  ;", "return    ;", "}", "METHOD_END"], "methodName": ["getMockContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalizedResource"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( new   Configuration (  )  )  ;", "try    {", "dispatcher . start (  )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "EventHandler < LocalizerEvent >    localizerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ContainerEventType . class ,    containerBus )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerBus )  ;", "LocalResource   apiRsrc    =     . createMockResource (  )  ;", "final   ContainerId   container 0     =     . getMockContainer (  0  )  ;", "final   Credentials   creds 0     =    new   Credentials (  )  ;", "final   LocalResourceVisibility   vis 0     =    LocalResourceVisibility . PRIVATE ;", "final   LocalizerContext   ctxt 0     =    new   LocalizerContext (  \" yak \"  ,    container 0  ,    creds 0  )  ;", "LocalResourceRequest   rsrcA    =    new   LocalResourceRequest ( apiRsrc )  ;", "LocalizedResource   local    =    new   LocalizedResource ( rsrcA ,    dispatcher )  ;", "local . handle ( new   ResourceRequestEvent ( rsrcA ,    vis 0  ,    ctxt 0  )  )  ;", "dispatcher . await (  )  ;", ". LocalizerEventMatcher   matchesL 0 Req    =    new    . LocalizerEventMatcher ( container 0  ,    creds 0  ,    vis 0  ,    LocalizerEventType . REQUEST _ RESOURCE _ LOCALIZATION )  ;", "verify ( localizerBus )  . handle ( argThat ( matchesL 0 Req )  )  ;", "assertEquals ( ResourceState . DOWNLOADING ,    local . getState (  )  )  ;", "final   Credentials   creds 1     =    new   Credentials (  )  ;", "final   ContainerId   container 1     =     . getMockContainer (  1  )  ;", "final   LocalizerContext   ctxt 1     =    new   LocalizerContext (  \" yak \"  ,    container 1  ,    creds 1  )  ;", "final   LocalResourceVisibility   vis 1     =    LocalResourceVisibility . PUBLIC ;", "local . handle ( new   ResourceRequestEvent ( rsrcA ,    vis 1  ,    ctxt 1  )  )  ;", "dispatcher . await (  )  ;", ". LocalizerEventMatcher   matchesL 1 Req    =    new    . LocalizerEventMatcher ( container 1  ,    creds 1  ,    vis 1  ,    LocalizerEventType . REQUEST _ RESOURCE _ LOCALIZATION )  ;", "verify ( localizerBus )  . handle ( argThat ( matchesL 1 Req )  )  ;", "local . handle ( new   ResourceReleaseEvent ( rsrcA ,    container 0  )  )  ;", "dispatcher . await (  )  ;", "verify ( containerBus ,    never (  )  )  . handle ( isA ( ContainerEvent . class )  )  ;", "assertEquals ( ResourceState . DOWNLOADING ,    local . getState (  )  )  ;", "local . handle ( new   ResourceReleaseEvent ( rsrcA ,    container 1  )  )  ;", "dispatcher . await (  )  ;", "verify ( containerBus ,    never (  )  )  . handle ( isA ( ContainerEvent . class )  )  ;", "assertEquals ( ResourceState . DOWNLOADING ,    local . getState (  )  )  ;", "final   ContainerId   container 2     =     . getMockContainer (  2  )  ;", "final   LocalResourceVisibility   vis 2     =    LocalResourceVisibility . PRIVATE ;", "final   Credentials   creds 2     =    new   Credentials (  )  ;", "final   LocalizerContext   ctxt 2     =    new   LocalizerContext (  \" yak \"  ,    container 2  ,    creds 2  )  ;", "final   ContainerId   container 3     =     . getMockContainer (  3  )  ;", "final   LocalResourceVisibility   vis 3     =    LocalResourceVisibility . PRIVATE ;", "final   Credentials   creds 3     =    new   Credentials (  )  ;", "final   LocalizerContext   ctxt 3     =    new   LocalizerContext (  \" yak \"  ,    container 3  ,    creds 3  )  ;", "local . handle ( new   ResourceRequestEvent ( rsrcA ,    vis 2  ,    ctxt 2  )  )  ;", "local . handle ( new   ResourceRequestEvent ( rsrcA ,    vis 3  ,    ctxt 3  )  )  ;", "dispatcher . await (  )  ;", ". LocalizerEventMatcher   matchesL 2 Req    =    new    . LocalizerEventMatcher ( container 2  ,    creds 2  ,    vis 2  ,    LocalizerEventType . REQUEST _ RESOURCE _ LOCALIZATION )  ;", "verify ( localizerBus )  . handle ( argThat ( matchesL 2 Req )  )  ;", ". LocalizerEventMatcher   matchesL 3 Req    =    new    . LocalizerEventMatcher ( container 3  ,    creds 3  ,    vis 3  ,    LocalizerEventType . REQUEST _ RESOURCE _ LOCALIZATION )  ;", "verify ( localizerBus )  . handle ( argThat ( matchesL 3 Req )  )  ;", "Path   locA    =    new   Path (  \" file :  /  /  / cache / rsrcA \"  )  ;", "local . handle ( new   ResourceLocalizedEvent ( rsrcA ,    locA ,     1  0  )  )  ;", "dispatcher . await (  )  ;", ". ContainerEventMatcher   matchesC 2 Localized    =    new    . ContainerEventMatcher ( container 2  ,    ContainerEventType . RESOURCE _ LOCALIZED )  ;", ". ContainerEventMatcher   matchesC 3 Localized    =    new    . ContainerEventMatcher ( container 3  ,    ContainerEventType . RESOURCE _ LOCALIZED )  ;", "verify ( containerBus )  . handle ( argThat ( matchesC 2 Localized )  )  ;", "verify ( containerBus )  . handle ( argThat ( matchesC 3 Localized )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    local . getState (  )  )  ;", "final   ContainerId   container 4     =     . getMockContainer (  4  )  ;", "final   Credentials   creds 4     =    new   Credentials (  )  ;", "final   LocalizerContext   ctxt 4     =    new   LocalizerContext (  \" yak \"  ,    container 4  ,    creds 4  )  ;", "final   LocalResourceVisibility   vis 4     =    LocalResourceVisibility . PRIVATE ;", "local . handle ( new   ResourceRequestEvent ( rsrcA ,    vis 4  ,    ctxt 4  )  )  ;", "dispatcher . await (  )  ;", ". ContainerEventMatcher   matchesC 4 Localized    =    new    . ContainerEventMatcher ( container 4  ,    ContainerEventType . RESOURCE _ LOCALIZED )  ;", "verify ( containerBus )  . handle ( argThat ( matchesC 4 Localized )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    local . getState (  )  )  ;", "}    finally    {", "dispatcher . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNotification"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestLocalizedResource"}, {"methodBody": ["METHOD_START", "{", "conf    =    null ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Application   app    =    mock ( Application . class )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "return   new   ApplicationEvent ( EventType . INIT _ APPLICATION _ RESOURCES ,    app )  ;", "}", "METHOD_END"], "methodName": ["createApplicationLocalizationEvent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    reqs    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "List < LocalResourceRequest >    resourceList    =    new   ArrayList < LocalResourceRequest >  (  )  ;", "resourceList . add ( req )  ;", "reqs . put ( vis ,    resourceList )  ;", "return   new   ContainerRequestEvent ( container ,    reqs )  ;", "}", "METHOD_END"], "methodName": ["createContainerLocalizationEvent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "LocalizerStatus   status    =    new   LocalizerStatusPBImpl (  )  ;", "status . setLocalizerId ( Id 1  )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["createLocalizerStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "LocalizerStatus   status    =    createLocalizerStatus ( localizerId )  ;", "LocalResourceStatus   resourceStatus    =    new   LocalResourceStatusPBImpl (  )  ;", "resourceStatus . setException ( SerializedException . newInstance ( new   YarnException (  \" test \"  )  )  )  ;", "resourceStatus . setStatus ( ResourceStatusType . FETCH _ FAILURE )  ;", "resourceStatus . setResource ( req )  ;", "status . addResourceStatus ( resourceStatus )  ;", "return   status ;", "}", "METHOD_END"], "methodName": ["createLocalizerStatusForFailedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "ContainerImpl   container    =    mock ( ContainerImpl . class )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( BuilderUtils . newContainerId (  1  ,     1  ,     1  ,    containerId )  )  ;", "when ( container . getUser (  )  )  . thenReturn ( user )  ;", "Credentials   mockCredentials    =    mock ( Credentials . class )  ;", "when ( container . getCredentials (  )  )  . thenReturn ( mockCredentials )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createMockContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "ResourceLocalizationService . LocalizerTracker   mockLocalizerTracker    =    mock ( ResourceLocalizationService . LocalizerTracker . class )  ;", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "ResourceLocalizationService   rawService    =    new   ResourceLocalizationService ( dispatcher ,    exec ,    delService ,    dirsHandler ,    stateStore )  ;", "ResourceLocalizationService   spyService    =    spy ( rawService )  ;", "doReturn (  . mockServer )  . when ( spyService )  . createServer (  )  ;", "doReturn ( mockLocalizerTracker )  . when ( spyService )  . createLocalizerTracker ( isA ( Configuration . class )  )  ;", "doReturn ( lfs )  . when ( spyService )  . getLocalFileContext ( isA ( Configuration . class )  )  ;", "return   spyService ;", "}", "METHOD_END"], "methodName": ["createSpyService"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   TestResourceLocalizationService . getMockedResource ( r ,    APPLICATION )  ;", "}", "METHOD_END"], "methodName": ["getAppMockedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   service . getLocalResourcesTracker ( vis ,    user ,    appId )  . getLocalizedResource ( req )  ;", "}", "METHOD_END"], "methodName": ["getLocalizedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Container   c    =    mock ( Container . class )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   cId    =    BuilderUtils . newContainerId ( appAttemptId ,    id )  ;", "when ( c . getUser (  )  )  . thenReturn ( user )  ;", "when ( c . getContainerId (  )  )  . thenReturn ( cId )  ;", "Credentials   creds    =    new   Credentials (  )  ;", "creds . addToken ( new   Text (  (  \" tok \"     +    id )  )  ,     . getToken ( id )  )  ;", "when ( c . getCredentials (  )  )  . thenReturn ( creds )  ;", "when ( c . toString (  )  )  . thenReturn ( cId . toString (  )  )  ;", "return   c ;", "}", "METHOD_END"], "methodName": ["getMockContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "String   name    =    Long . toHexString ( r . nextLong (  )  )  ;", "URL   url    =     . getPath (  (  \"  / local / PRIVATE /  \"     +    name )  )  ;", "LocalResource   rsrc    =    BuilderUtils . newLocalResource ( url ,    FILE ,    vis ,     (  ( r . nextInt (  1  0  2  4  )  )     +     1  0  2  4 L )  ,     (  ( r . nextInt (  1  0  2  4  )  )     +     2  0  4  8 L )  )  ;", "return   rsrc ;", "}", "METHOD_END"], "methodName": ["getMockedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "URL   url    =    BuilderUtils . newURL (  \" file \"  ,    null ,     0  ,    path )  ;", "return   url ;", "}", "METHOD_END"], "methodName": ["getPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   TestResourceLocalizationService . getMockedResource ( r ,    PRIVATE )  ;", "}", "METHOD_END"], "methodName": ["getPrivateMockedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   TestResourceLocalizationService . getMockedResource ( r ,    PUBLIC )  ;", "}", "METHOD_END"], "methodName": ["getPublicMockedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "return   new   Token (  (  \" ident \"     +    id )  . getBytes (  )  ,     (  \" passwd \"     +    id )  . getBytes (  )  ,    new   Text (  (  \" kind \"     +    id )  )  ,    new   Text (  (  \" service \"     +    id )  )  )  ;", "}", "METHOD_END"], "methodName": ["getToken"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   Configuration (  )  ;", "spylfs    =    spy ( FileContext . getLocalFSFileContext (  )  . getDefaultFileSystem (  )  )  ;", "lfs    =    FileContext . getFileContext ( spylfs ,    conf )  ;", "doNothing (  )  . when ( spylfs )  . mkdir ( isA ( Path . class )  ,    isA ( FsPermission . class )  ,    anyBoolean (  )  )  ;", "String   logDir    =    lfs . makeQualified ( new   Path (  . basedir ,     \" logdir    \"  )  )  . toString (  )  ;", "conf . set ( NM _ LOG _ DIRS ,    logDir )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "TestResourceLocalizationService . mockServer    =    mock ( Server . class )  ;", "doReturn ( new   InetSocketAddress (  1  2  3  )  )  . when ( TestResourceLocalizationService . mockServer )  . getListenerAddress (  )  ;", "}", "METHOD_END"], "methodName": ["setupClass"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  4  ]  ;", "for    ( int   i    =     0  ;    i    <     4  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ContainerEventType . class ,    containerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "try    {", "ResourceLocalizationService   rawService    =    new   ResourceLocalizationService ( dispatcher ,    exec ,    delService ,    dirsHandler ,    new   NMNullStateStoreService (  )  )  ;", "ResourceLocalizationService   spyService    =    spy ( rawService )  ;", "doReturn (  . mockServer )  . when ( spyService )  . createServer (  )  ;", "doReturn ( lfs )  . when ( spyService )  . getLocalFileContext ( isA ( Configuration . class )  )  ;", "spyService . init ( conf )  ;", "spyService . start (  )  ;", "final   String   user    =     \" user 0  \"  ;", "final   Application   app    =    mock ( Application . class )  ;", "final   ApplicationId   appId    =    BuilderUtils . newApplicationId (  3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     3  )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "spyService . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app )  )  ;", "dispatcher . await (  )  ;", "final   Container   c    =     . getMockContainer ( appId ,     4  2  ,    user )  ;", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "r . setSeed ( seed )  ;", "final   CyclicBarrier   barrier    =    new   CyclicBarrier (  2  )  ;", "doAnswer ( new   Answer < Void >  (  )     {", "public   Void   answer ( InvocationOnMock   invocation )    throws   IOException    {", "try    {", "barrier . await (  )  ;", "}    catch    ( InterruptedException   e )     {", "}    catch    ( BrokenBarrierException   e )     {", "}", "throw   new   IOException (  \" forced   failure \"  )  ;", "}", "}  )  . when ( spylfs )  . setPermission ( isA ( Path . class )  ,    isA ( FsPermission . class )  )  ;", "final   LocalResource   pubResource    =     . getPublicMockedResource ( r )  ;", "final   LocalResourceRequest   pubReq    =    new   LocalResourceRequest ( pubResource )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    req    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "req . put ( PUBLIC ,    Collections . singletonList ( pubReq )  )  ;", "Set < LocalResourceRequest >    pubRsrcs    =    new   HashSet < LocalResourceRequest >  (  )  ;", "pubRsrcs . add ( pubReq )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    req )  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    req )  )  ;", "dispatcher . await (  )  ;", "barrier . await (  )  ;", "verify ( containerBus ,    timeout (  5  0  0  0  )  . times (  2  )  )  . handle ( isA ( ContainerResourceFailedEvent . class )  )  ;", "}    finally    {", "dispatcher . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFailedPublicResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher 1     =    null ;", "try    {", "dispatcher 1     =    new   DrainDispatcher (  )  ;", "String   user    =     \" testuser \"  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  1  ]  ;", "for    ( int   i    =     0  ;    i    <     1  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "LocalDirsHandlerService   localDirHandler    =    new   LocalDirsHandlerService (  )  ;", "localDirHandler . init ( conf )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher 1  . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher 1  . register ( ContainerEventType . class ,    containerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "dispatcher 1  . init ( conf )  ;", "dispatcher 1  . start (  )  ;", "ResourceLocalizationService   rls    =    new   ResourceLocalizationService ( dispatcher 1  ,    exec ,    delService ,    localDirHandler ,    new   NMNullStateStoreService (  )  )  ;", "dispatcher 1  . register ( LocalizationEventType . class ,    rls )  ;", "rls . init ( conf )  ;", "rls . handle ( createApplicationLocalizationEvent ( user ,    appId )  )  ;", "Container   container 1     =    createMockContainer ( user ,     1  )  ;", "String   localizerId 1     =    container 1  . getContainerId (  )  . toString (  )  ;", "rls . getPrivateLocalizers (  )  . put ( localizerId 1  ,    rls . new   LocalizerRunner ( new   LocalizerContext ( user ,    container 1  . getContainerId (  )  ,    null )  ,    localizerId 1  )  )  ;", "LocalResourceRequest   reqPriv    =    new   LocalResourceRequest ( new   Path (  \" file :  /  /  / tmp 1  \"  )  ,     1  2  3 L ,    LocalResourceType . FILE ,    LocalResourceVisibility . PRIVATE ,     \"  \"  )  ;", "List < LocalResourceRequest >    privList    =    new   ArrayList < LocalResourceRequest >  (  )  ;", "privList . add ( reqPriv )  ;", "LocalResourceRequest   reqApp    =    new   LocalResourceRequest ( new   Path (  \" file :  /  /  / tmp 2  \"  )  ,     1  2  3 L ,    LocalResourceType . FILE ,    LocalResourceVisibility . APPLICATION ,     \"  \"  )  ;", "List < LocalResourceRequest >    appList    =    new   ArrayList < LocalResourceRequest >  (  )  ;", "appList . add ( reqApp )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    rsrcs    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "rsrcs . put ( APPLICATION ,    appList )  ;", "rsrcs . put ( PRIVATE ,    privList )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( new   ContainerLocalizationRequestEvent ( container 1  ,    rsrcs )  )  ;", "Assert . assertTrue ( waitForPrivateDownloadToStart ( rls ,    localizerId 1  ,     2  ,     5  0  0  )  )  ;", "String   userCachePath    =    StringUtils . join ( SEPARATOR ,    Arrays . asList ( localDirs . get (  0  )  . toUri (  )  . getRawPath (  )  ,    ContainerLocalizer . USERCACHE ,    user ,    ContainerLocalizer . FILECACHE )  )  ;", "String   userAppCachePath    =    StringUtils . join ( SEPARATOR ,    Arrays . asList ( localDirs . get (  0  )  . toUri (  )  . getRawPath (  )  ,    ContainerLocalizer . USERCACHE ,    user ,    ContainerLocalizer . APPCACHE ,    appId . toString (  )  ,    ContainerLocalizer . FILECACHE )  )  ;", "int   returnedResources    =     0  ;", "boolean   appRsrc    =    false ;", "boolean   privRsrc    =    false ;", "while    ( returnedResources    <     2  )     {", "LocalizerHeartbeatResponse   response    =    rls . heartbeat ( createLocalizerStatus ( localizerId 1  )  )  ;", "for    ( ResourceLocalizationSpec   resourceSpec    :    response . getResourceSpecs (  )  )     {", "returnedResources +  +  ;", "Path   destinationDirectory    =    new   Path ( resourceSpec . getDestinationDirectory (  )  . getFile (  )  )  ;", "if    (  ( resourceSpec . getResource (  )  . getVisibility (  )  )     =  =     ( LocalResourceVisibility . APPLICATION )  )     {", "appRsrc    =    true ;", "Assert . assertEquals ( userAppCachePath ,    destinationDirectory . getParent (  )  . toUri (  )  . toString (  )  )  ;", "} else", "if    (  ( resourceSpec . getResource (  )  . getVisibility (  )  )     =  =     ( LocalResourceVisibility . PRIVATE )  )     {", "privRsrc    =    true ;", "Assert . assertEquals ( userCachePath ,    destinationDirectory . getParent (  )  . toUri (  )  . toString (  )  )  ;", "} else    {", "throw   new   Exception (  \" Unexpected   resource   recevied .  \"  )  ;", "}", "}", "}", "Assert . assertTrue (  ( appRsrc    &  &    privRsrc )  )  ;", "}    finally    {", "if    ( dispatcher 1     !  =    null )     {", "dispatcher 1  . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testLocalResourcePath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  1  ]  ;", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     (  0     +     \"  \"  )  )  )  )  ;", "sDirs [  0  ]     =    localDirs . get (  0  )  . toString (  )  ;", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "conf . set ( NM _ LOCAL _ CACHE _ MAX _ FILES _ PER _ DIRECTORY ,     \"  3  7  \"  )  ;", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ContainerEventType . class ,    containerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "DeletionService   delServiceReal    =    new   DeletionService ( exec )  ;", "DeletionService   delService    =    spy ( delServiceReal )  ;", "delService . init ( new   Configuration (  )  )  ;", "delService . start (  )  ;", "ResourceLocalizationService   rawService    =    new   ResourceLocalizationService ( dispatcher ,    exec ,    delService ,    dirsHandler ,    new   NMNullStateStoreService (  )  )  ;", "ResourceLocalizationService   spyService    =    spy ( rawService )  ;", "doReturn (  . mockServer )  . when ( spyService )  . createServer (  )  ;", "doReturn ( lfs )  . when ( spyService )  . getLocalFileContext ( isA ( Configuration . class )  )  ;", "try    {", "spyService . init ( conf )  ;", "spyService . start (  )  ;", "final   Application   app    =    mock ( Application . class )  ;", "final   ApplicationId   appId    =    BuilderUtils . newApplicationId (  3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     3  )  ;", "when ( app . getUser (  )  )  . thenReturn (  \" user 0  \"  )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "spyService . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app )  )  ;", "ArgumentMatcher < ApplicationEvent >    matchesAppInit    =    new   ArgumentMatcher < ApplicationEvent >  (  )     {", "@ Override", "public   boolean   matches ( Object   o )     {", "ApplicationEvent   evt    =     (  ( ApplicationEvent )     ( o )  )  ;", "return    (  ( evt . getType (  )  )     =  =     ( ApplicationEventType . APPLICATION _ INITED )  )     &  &     ( appId    =  =     ( evt . getApplicationID (  )  )  )  ;", "}", "}  ;", "dispatcher . await (  )  ;", "verify ( applicationBus )  . handle ( argThat ( matchesAppInit )  )  ;", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "r . setSeed ( seed )  ;", "final   Container   c    =     . getMockContainer ( appId ,     4  2  ,     \" user 0  \"  )  ;", "FSDataOutputStream   out    =    new   FSDataOutputStream ( new   DataOutputBuffer (  )  ,    null )  ;", "doReturn ( out )  . when ( spylfs )  . createInternal ( isA ( Path . class )  ,    isA ( EnumSet . class )  ,    isA ( FsPermission . class )  ,    anyInt (  )  ,    anyShort (  )  ,    anyLong (  )  ,    isA ( Progressable . class )  ,    isA ( ChecksumOpt . class )  ,    anyBoolean (  )  )  ;", "final   LocalResource   resource 1     =     . getPrivateMockedResource ( r )  ;", "LocalResource   resource 2     =    null ;", "do    {", "resource 2     =     . getPrivateMockedResource ( r )  ;", "}    while    (  ( resource 2     =  =    null )     |  |     ( resource 2  . equals ( resource 1  )  )     )  ;", "final   LocalResourceRequest   req 1     =    new   LocalResourceRequest ( resource 1  )  ;", "final   LocalResourceRequest   req 2     =    new   LocalResourceRequest ( resource 2  )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    rsrcs    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "List < LocalResourceRequest >    privateResourceList    =    new   ArrayList < LocalResourceRequest >  (  )  ;", "privateResourceList . add ( req 1  )  ;", "privateResourceList . add ( req 2  )  ;", "rsrcs . put ( PRIVATE ,    privateResourceList )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    rsrcs )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "dispatcher . await (  )  ;", "String   appStr    =    ConverterUtils . toString ( appId )  ;", "String   ctnrStr    =    c . getContainerId (  )  . toString (  )  ;", "ArgumentCaptor < Path >    tokenPathCaptor    =    ArgumentCaptor . forClass ( Path . class )  ;", "verify ( exec )  . startLocalizer ( tokenPathCaptor . capture (  )  ,    isA ( InetSocketAddress . class )  ,    eq (  \" user 0  \"  )  ,    eq ( appStr )  ,    eq ( ctnrStr )  ,    isA ( List . class )  ,    isA ( List . class )  )  ;", "Path   localizationTokenPath    =    tokenPathCaptor . getValue (  )  ;", "LocalResourceStatus   rsrcStat 1     =    mock ( LocalResourceStatus . class )  ;", "LocalResourceStatus   rsrcStat 2     =    mock ( LocalResourceStatus . class )  ;", "LocalizerStatus   stat    =    mock ( LocalizerStatus . class )  ;", "when ( stat . getLocalizerId (  )  )  . thenReturn ( ctnrStr )  ;", "when ( rsrcStat 1  . getResource (  )  )  . thenReturn ( resource 1  )  ;", "when ( rsrcStat 2  . getResource (  )  )  . thenReturn ( resource 2  )  ;", "when ( rsrcStat 1  . getLocalSize (  )  )  . thenReturn (  4  3  4  4 L )  ;", "when ( rsrcStat 2  . getLocalSize (  )  )  . thenReturn (  2  3  4  2 L )  ;", "URL   locPath    =     . getPath (  \"  / cache / private / blah \"  )  ;", "when ( rsrcStat 1  . getLocalPath (  )  )  . thenReturn ( locPath )  ;", "when ( rsrcStat 2  . getLocalPath (  )  )  . thenReturn ( locPath )  ;", "when ( rsrcStat 1  . getStatus (  )  )  . thenReturn ( ResourceStatusType . FETCH _ SUCCESS )  ;", "when ( rsrcStat 2  . getStatus (  )  )  . thenReturn ( ResourceStatusType . FETCH _ SUCCESS )  ;", "when ( stat . getResources (  )  )  . thenReturn ( Collections .  < LocalResourceStatus > emptyList (  )  )  . thenReturn ( Collections . singletonList ( rsrcStat 1  )  )  . thenReturn ( Collections . singletonList ( rsrcStat 2  )  )  . thenReturn ( Collections .  < LocalResourceStatus > emptyList (  )  )  ;", "String   localPath    =     (  (  (  (  ( Path . SEPARATOR )     +     ( ContainerLocalizer . USERCACHE )  )     +     ( Path . SEPARATOR )  )     +     \" user 0  \"  )     +     ( Path . SEPARATOR )  )     +     ( ContainerLocalizer . FILECACHE )  ;", "LocalizerHeartbeatResponse   response    =    spyService . heartbeat ( stat )  ;", "assertEquals ( LocalizerAction . LIVE ,    response . getLocalizerAction (  )  )  ;", "assertEquals (  1  ,    response . getResourceSpecs (  )  . size (  )  )  ;", "assertEquals ( req 1  ,    new   LocalResourceRequest ( response . getResourceSpecs (  )  . get (  0  )  . getResource (  )  )  )  ;", "URL   localizedPath    =    response . getResourceSpecs (  )  . get (  0  )  . getDestinationDirectory (  )  ;", "assertTrue ( localizedPath . getFile (  )  . endsWith (  (  ( localPath    +     ( Path . SEPARATOR )  )     +     \"  1  0  \"  )  )  )  ;", "response    =    spyService . heartbeat ( stat )  ;", "assertEquals ( LocalizerAction . LIVE ,    response . getLocalizerAction (  )  )  ;", "assertEquals (  1  ,    response . getResourceSpecs (  )  . size (  )  )  ;", "assertEquals ( req 2  ,    new   LocalResourceRequest ( response . getResourceSpecs (  )  . get (  0  )  . getResource (  )  )  )  ;", "localizedPath    =    response . getResourceSpecs (  )  . get (  0  )  . getDestinationDirectory (  )  ;", "assertTrue ( localizedPath . getFile (  )  . endsWith (  (  (  (  ( localPath    +     ( Path . SEPARATOR )  )     +     \"  0  \"  )     +     ( Path . SEPARATOR )  )     +     \"  1  1  \"  )  )  )  ;", "response    =    spyService . heartbeat ( stat )  ;", "assertEquals ( LocalizerAction . LIVE ,    response . getLocalizerAction (  )  )  ;", "assertEquals (  0  ,    response . getResourceSpecs (  )  . size (  )  )  ;", "response    =    spyService . heartbeat ( stat )  ;", "assertEquals ( LocalizerAction . DIE ,    response . getLocalizerAction (  )  )  ;", "dispatcher . await (  )  ;", "ArgumentMatcher < ContainerEvent >    matchesContainerLoc    =    new   ArgumentMatcher < ContainerEvent >  (  )     {", "@ Override", "public   boolean   matches ( Object   o )     {", "ContainerEvent   evt    =     (  ( ContainerEvent )     ( o )  )  ;", "return    (  ( evt . getType (  )  )     =  =     ( ContainerEventType . RESOURCE _ LOCALIZED )  )     &  &     (  ( c . getContainerId (  )  )     =  =     ( evt . getContainerID (  )  )  )  ;", "}", "}  ;", "verify ( containerBus ,    times (  2  )  )  . handle ( argThat ( matchesContainerLoc )  )  ;", "verify ( delService )  . delete (  (  ( String )     ( isNull (  )  )  )  ,    eq ( localizationTokenPath )  )  ;", "}    finally    {", "spyService . stop (  )  ;", "dispatcher . stop (  )  ;", "delService . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testLocalizationHeartbeat"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "AsyncDispatcher   dispatcher    =    new   AsyncDispatcher (  )  ;", "dispatcher . init ( new   Configuration (  )  )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "DeletionService   delService    =    spy ( new   DeletionService ( exec )  )  ;", "delService . init ( conf )  ;", "delService . start (  )  ;", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  4  ]  ;", "for    ( int   i    =     0  ;    i    <     4  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "LocalDirsHandlerService   diskhandler    =    new   LocalDirsHandlerService (  )  ;", "diskhandler . init ( conf )  ;", "ResourceLocalizationService   locService    =    spy ( new   ResourceLocalizationService ( dispatcher ,    exec ,    delService ,    diskhandler ,    new   NMNullStateStoreService (  )  )  )  ;", "doReturn ( lfs )  . when ( locService )  . getLocalFileContext ( isA ( Configuration . class )  )  ;", "try    {", "dispatcher . start (  )  ;", "locService . init ( conf )  ;", "final   FsPermission   defaultPerm    =    new   FsPermission (  (  ( short )     (  4  9  3  )  )  )  ;", "for    ( Path   p    :    localDirs )     {", "p    =    new   Path ( new   URI ( p . toString (  )  )  . getPath (  )  )  ;", "Path   usercache    =    new   Path ( p ,    ContainerLocalizer . USERCACHE )  ;", "verify ( spylfs )  . mkdir ( eq ( usercache )  ,    eq ( defaultPerm )  ,    eq ( true )  )  ;", "Path   publicCache    =    new   Path ( p ,    ContainerLocalizer . FILECACHE )  ;", "verify ( spylfs )  . mkdir ( eq ( publicCache )  ,    eq ( defaultPerm )  ,    eq ( true )  )  ;", "Path   nmPriv    =    new   Path ( p ,    ResourceLocalizationService . NM _ PRIVATE _ DIR )  ;", "verify ( spylfs )  . mkdir ( eq ( nmPriv )  ,    eq ( ResourceLocalizationService . NM _ PRIVATE _ PERM )  ,    eq ( true )  )  ;", "}", "}    finally    {", "dispatcher . stop (  )  ;", "delService . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testLocalizationInit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher 1     =    null ;", "try    {", "dispatcher 1     =    new   DrainDispatcher (  )  ;", "String   user    =     \" testuser \"  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  1  ]  ;", "for    ( int   i    =     0  ;    i    <     1  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "LocalDirsHandlerService   localDirHandler    =    new   LocalDirsHandlerService (  )  ;", "localDirHandler . init ( conf )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher 1  . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher 1  . register ( ContainerEventType . class ,    containerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "dispatcher 1  . init ( conf )  ;", "dispatcher 1  . start (  )  ;", "ResourceLocalizationService   rls    =    new   ResourceLocalizationService ( dispatcher 1  ,    exec ,    delService ,    localDirHandler ,    new   NMNullStateStoreService (  )  )  ;", "dispatcher 1  . register ( LocalizationEventType . class ,    rls )  ;", "rls . init ( conf )  ;", "rls . handle ( createApplicationLocalizationEvent ( user ,    appId )  )  ;", "LocalResourceRequest   req    =    new   LocalResourceRequest ( new   Path (  \" file :  /  /  / tmp \"  )  ,     1  2  3 L ,    LocalResourceType . FILE ,    LocalResourceVisibility . PRIVATE ,     \"  \"  )  ;", "ContainerImpl   container 1     =    createMockContainer ( user ,     1  )  ;", "String   localizerId 1     =    container 1  . getContainerId (  )  . toString (  )  ;", "rls . getPrivateLocalizers (  )  . put ( localizerId 1  ,    rls . new   LocalizerRunner ( new   LocalizerContext ( user ,    container 1  . getContainerId (  )  ,    null )  ,    localizerId 1  )  )  ;", "ResourceLocalizationService . LocalizerRunner   localizerRunner 1     =    rls . getLocalizerRunner ( localizerId 1  )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( createContainerLocalizationEvent ( container 1  ,    PRIVATE ,    req )  )  ;", "Assert . assertTrue ( waitForPrivateDownloadToStart ( rls ,    localizerId 1  ,     1  ,     2  0  0  )  )  ;", "ContainerImpl   container 2     =    createMockContainer ( user ,     2  )  ;", "String   localizerId 2     =    container 2  . getContainerId (  )  . toString (  )  ;", "rls . getPrivateLocalizers (  )  . put ( localizerId 2  ,    rls . new   LocalizerRunner ( new   LocalizerContext ( user ,    container 2  . getContainerId (  )  ,    null )  ,    localizerId 2  )  )  ;", "ResourceLocalizationService . LocalizerRunner   localizerRunner 2     =    rls . getLocalizerRunner ( localizerId 2  )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( createContainerLocalizationEvent ( container 2  ,    PRIVATE ,    req )  )  ;", "Assert . assertTrue ( waitForPrivateDownloadToStart ( rls ,    localizerId 2  ,     1  ,     2  0  0  )  )  ;", "LocalResourcesTracker   tracker    =    rls . getLocalResourcesTracker ( PRIVATE ,    user ,    appId )  ;", "LocalizedResource   lr    =    tracker . getLocalizedResource ( req )  ;", "Assert . assertEquals ( ResourceState . DOWNLOADING ,    lr . getState (  )  )  ;", "Assert . assertEquals (  1  ,    lr . sem . availablePermits (  )  )  ;", "LocalizerHeartbeatResponse   response 1     =    rls . heartbeat ( createLocalizerStatus ( localizerId 1  )  )  ;", "Assert . assertEquals (  1  ,    localizerRunner 1  . scheduled . size (  )  )  ;", "Assert . assertEquals ( req . getResource (  )  ,    response 1  . getResourceSpecs (  )  . get (  0  )  . getResource (  )  . getResource (  )  )  ;", "Assert . assertEquals (  0  ,    lr . sem . availablePermits (  )  )  ;", "LocalizerHeartbeatResponse   response 2     =    rls . heartbeat ( createLocalizerStatus ( localizerId 2  )  )  ;", "Assert . assertEquals (  0  ,    localizerRunner 2  . scheduled . size (  )  )  ;", "Assert . assertEquals (  0  ,    response 2  . getResourceSpecs (  )  . size (  )  )  ;", "rls . heartbeat ( createLocalizerStatusForFailedResource ( localizerId 1  ,    req )  )  ;", "Assert . assertTrue ( waitForResourceState ( lr ,    rls ,    req ,    PRIVATE ,    user ,    appId ,    ResourceState . FAILED ,     2  0  0  )  )  ;", "Assert . assertTrue ( lr . getState (  )  . equals ( ResourceState . FAILED )  )  ;", "Assert . assertEquals (  0  ,    localizerRunner 1  . scheduled . size (  )  )  ;", "response 2     =    rls . heartbeat ( createLocalizerStatus ( localizerId 2  )  )  ;", "Assert . assertEquals (  0  ,    localizerRunner 2  . scheduled . size (  )  )  ;", "Assert . assertEquals (  0  ,    localizerRunner 2  . pending . size (  )  )  ;", "Assert . assertEquals (  0  ,    response 2  . getResourceSpecs (  )  . size (  )  )  ;", "}    finally    {", "if    ( dispatcher 1     !  =    null )     {", "dispatcher 1  . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testParallelDownloadAttemptsForPrivateResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher 1     =    null ;", "String   user    =     \" testuser \"  ;", "try    {", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  1  ]  ;", "for    ( int   i    =     0  ;    i    <     1  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher 1     =    new   DrainDispatcher (  )  ;", "dispatcher 1  . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher 1  . register ( ContainerEventType . class ,    containerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "dispatcher 1  . init ( conf )  ;", "dispatcher 1  . start (  )  ;", "ResourceLocalizationService   rawService    =    new   ResourceLocalizationService ( dispatcher 1  ,    exec ,    delService ,    dirsHandler ,    new   NMNullStateStoreService (  )  )  ;", "ResourceLocalizationService   spyService    =    spy ( rawService )  ;", "dispatcher 1  . register ( LocalizationEventType . class ,    spyService )  ;", "spyService . init ( conf )  ;", "Assert . assertEquals (  0  ,    spyService . getPublicLocalizer (  )  . pending . size (  )  )  ;", "LocalResourceRequest   req    =    new   LocalResourceRequest ( new   Path (  \"  / tmp \"  )  ,     1  2  3 L ,    LocalResourceType . FILE ,    LocalResourceVisibility . PUBLIC ,     \"  \"  )  ;", "ApplicationImpl   app    =    mock ( ApplicationImpl . class )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app )  )  ;", "ContainerImpl   container 1     =    createMockContainer ( user ,     1  )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( createContainerLocalizationEvent ( container 1  ,    PUBLIC ,    req )  )  ;", "Assert . assertTrue ( waitForResourceState ( null ,    spyService ,    req ,    PUBLIC ,    user ,    null ,    ResourceState . DOWNLOADING ,     2  0  0  )  )  ;", "Assert . assertTrue ( waitForPublicDownloadToStart ( spyService ,     1  ,     2  0  0  )  )  ;", "LocalizedResource   lr    =    getLocalizedResource ( spyService ,    req ,    PUBLIC ,    user ,    null )  ;", "Assert . assertEquals ( ResourceState . DOWNLOADING ,    lr . getState (  )  )  ;", "Assert . assertEquals (  1  ,    spyService . getPublicLocalizer (  )  . pending . size (  )  )  ;", "Assert . assertEquals (  0  ,    lr . sem . availablePermits (  )  )  ;", "ContainerImpl   container 2     =    createMockContainer ( user ,     2  )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( createContainerLocalizationEvent ( container 2  ,    PUBLIC ,    req )  )  ;", "Assert . assertFalse ( waitForPublicDownloadToStart ( spyService ,     2  ,     1  0  0  )  )  ;", "ResourceFailedLocalizationEvent   locFailedEvent    =    new   ResourceFailedLocalizationEvent ( req ,    new   Exception (  \" test \"  )  . toString (  )  )  ;", "spyService . getLocalResourcesTracker ( PUBLIC ,    user ,    null )  . handle ( locFailedEvent )  ;", "Assert . assertTrue ( waitForResourceState ( lr ,    spyService ,    req ,    PUBLIC ,    user ,    null ,    ResourceState . FAILED ,     2  0  0  )  )  ;", "lr . unlock (  )  ;", "spyService . getPublicLocalizer (  )  . pending . clear (  )  ;", "LocalizerResourceRequestEvent   localizerEvent    =    new   LocalizerResourceRequestEvent ( lr ,    null ,    mock ( LocalizerContext . class )  ,    null )  ;", "dispatcher 1  . getEventHandler (  )  . handle ( localizerEvent )  ;", "Assert . assertFalse ( waitForPublicDownloadToStart ( spyService ,     1  ,     1  0  0  )  )  ;", "Assert . assertEquals (  1  ,    lr . sem . availablePermits (  )  )  ;", "}    finally    {", "if    ( dispatcher 1     !  =    null )     {", "dispatcher 1  . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testParallelDownloadAttemptsForPublicResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  4  ]  ;", "for    ( int   i    =     0  ;    i    <     4  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "conf . setBoolean ( DISPATCHER _ EXIT _ ON _ ERROR _ KEY ,    true )  ;", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ContainerEventType . class ,    containerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "LocalDirsHandlerService   dirsHandlerSpy    =    spy ( dirsHandler )  ;", "dirsHandlerSpy . init ( conf )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "try    {", "ResourceLocalizationService   rawService    =    new   ResourceLocalizationService ( dispatcher ,    exec ,    delService ,    dirsHandlerSpy ,    new   NMNullStateStoreService (  )  )  ;", "ResourceLocalizationService   spyService    =    spy ( rawService )  ;", "doReturn (  . mockServer )  . when ( spyService )  . createServer (  )  ;", "doReturn ( lfs )  . when ( spyService )  . getLocalFileContext ( isA ( Configuration . class )  )  ;", "spyService . init ( conf )  ;", "spyService . start (  )  ;", "final   String   user    =     \" user 0  \"  ;", "final   Application   app    =    mock ( Application . class )  ;", "final   ApplicationId   appId    =    BuilderUtils . newApplicationId (  3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     3  )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "spyService . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app )  )  ;", "dispatcher . await (  )  ;", "Random   r    =    new   Random (  )  ;", "r . setSeed ( r . nextLong (  )  )  ;", "final   LocalResource   pubResource    =     . getPublicMockedResource ( r )  ;", "final   LocalResourceRequest   pubReq    =    new   LocalResourceRequest ( pubResource )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    req    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "req . put ( PUBLIC ,    Collections . singletonList ( pubReq )  )  ;", "final   Container   c    =     . getMockContainer ( appId ,     4  2  ,    user )  ;", "Mockito . doThrow ( new   IOException (  )  )  . when ( dirsHandlerSpy )  . getLocalPathForWrite ( isA ( String . class )  ,    Mockito . anyLong (  )  ,    Mockito . anyBoolean (  )  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    req )  )  ;", "dispatcher . await (  )  ;", "LocalResourcesTracker   tracker    =    spyService . getLocalResourcesTracker ( PUBLIC ,    user ,    appId )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( pubReq )  )  ;", "Mockito . doCallRealMethod (  )  . when ( dirsHandlerSpy )  . getLocalPathForWrite ( isA ( String . class )  ,    Mockito . anyLong (  )  ,    Mockito . anyBoolean (  )  )  ;", "ResourceLocalizationService . PublicLocalizer   publicLocalizer    =    spyService . getPublicLocalizer (  )  ;", "publicLocalizer . threadPool . shutdown (  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    req )  )  ;", "dispatcher . await (  )  ;", "tracker    =    spyService . getLocalResourcesTracker ( PUBLIC ,    user ,    appId )  ;", "Assert . assertNull ( tracker . getLocalizedResource ( pubReq )  )  ;", "}    finally    {", "dispatcher . await (  )  ;", "dispatcher . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPublicResourceAddResourceExceptions"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "final   String   user 1     =     \" user 1  \"  ;", "final   String   user 2     =     \" user 2  \"  ;", "final   ApplicationId   appId 1     =    ApplicationId . newInstance (  1  ,     1  )  ;", "final   ApplicationId   appId 2     =    ApplicationId . newInstance (  1  ,     2  )  ;", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  4  ]  ;", "for    ( int   i    =     0  ;    i    <     4  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "NMMemoryStateStoreService   stateStore    =    new   NMMemoryStateStoreService (  )  ;", "stateStore . init ( conf )  ;", "stateStore . start (  )  ;", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ContainerEventType . class ,    containerBus )  ;", "EventHandler < LocalizerEvent >    localizerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerBus )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "ResourceLocalizationService   spyService    =    createSpyService ( dispatcher ,    dirsHandler ,    stateStore )  ;", "try    {", "spyService . init ( conf )  ;", "spyService . start (  )  ;", "final   Application   app 1     =    mock ( Application . class )  ;", "when ( app 1  . getUser (  )  )  . thenReturn ( user 1  )  ;", "when ( app 1  . getAppId (  )  )  . thenReturn ( appId 1  )  ;", "final   Application   app 2     =    mock ( Application . class )  ;", "when ( app 2  . getUser (  )  )  . thenReturn ( user 2  )  ;", "when ( app 2  . getAppId (  )  )  . thenReturn ( appId 2  )  ;", "spyService . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app 1  )  )  ;", "spyService . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app 2  )  )  ;", "dispatcher . await (  )  ;", "LocalResourcesTracker   appTracker 1     =    spyService . getLocalResourcesTracker ( APPLICATION ,    user 1  ,    appId 1  )  ;", "LocalResourcesTracker   privTracker 1     =    spyService . getLocalResourcesTracker ( PRIVATE ,    user 1  ,    null )  ;", "LocalResourcesTracker   appTracker 2     =    spyService . getLocalResourcesTracker ( APPLICATION ,    user 2  ,    appId 2  )  ;", "LocalResourcesTracker   pubTracker    =    spyService . getLocalResourcesTracker ( PUBLIC ,    null ,    null )  ;", "final   Container   c 1     =     . getMockContainer ( appId 1  ,     1  ,    user 1  )  ;", "final   Container   c 2     =     . getMockContainer ( appId 2  ,     2  ,    user 2  )  ;", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "r . setSeed ( seed )  ;", "final   LocalResource   privResource 1     =     . getPrivateMockedResource ( r )  ;", "final   LocalResourceRequest   privReq 1     =    new   LocalResourceRequest ( privResource 1  )  ;", "final   LocalResource   privResource 2     =     . getPrivateMockedResource ( r )  ;", "final   LocalResourceRequest   privReq 2     =    new   LocalResourceRequest ( privResource 2  )  ;", "final   LocalResource   pubResource 1     =     . getPublicMockedResource ( r )  ;", "final   LocalResourceRequest   pubReq 1     =    new   LocalResourceRequest ( pubResource 1  )  ;", "final   LocalResource   pubResource 2     =     . getPublicMockedResource ( r )  ;", "final   LocalResourceRequest   pubReq 2     =    new   LocalResourceRequest ( pubResource 2  )  ;", "final   LocalResource   appResource 1     =     . getAppMockedResource ( r )  ;", "final   LocalResourceRequest   appReq 1     =    new   LocalResourceRequest ( appResource 1  )  ;", "final   LocalResource   appResource 2     =     . getAppMockedResource ( r )  ;", "final   LocalResourceRequest   appReq 2     =    new   LocalResourceRequest ( appResource 2  )  ;", "final   LocalResource   appResource 3     =     . getAppMockedResource ( r )  ;", "final   LocalResourceRequest   appReq 3     =    new   LocalResourceRequest ( appResource 3  )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    req 1     =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "req 1  . put ( PRIVATE ,    Arrays . asList ( new   LocalResourceRequest [  ]  {    privReq 1  ,    privReq 2     }  )  )  ;", "req 1  . put ( PUBLIC ,    Collections . singletonList ( pubReq 1  )  )  ;", "req 1  . put ( APPLICATION ,    Collections . singletonList ( appReq 1  )  )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    req 2     =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "req 2  . put ( APPLICATION ,    Arrays . asList ( new   LocalResourceRequest [  ]  {    appReq 2  ,    appReq 3     }  )  )  ;", "req 2  . put ( PUBLIC ,    Collections . singletonList ( pubReq 2  )  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c 1  ,    req 1  )  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c 2  ,    req 2  )  )  ;", "dispatcher . await (  )  ;", "privTracker 1  . getPathForLocalization ( privReq 1  ,    dirsHandler . getLocalPathForWrite (  (  ( ContainerLocalizer . USERCACHE )     +    user 1  )  )  )  ;", "privTracker 1  . getPathForLocalization ( privReq 2  ,    dirsHandler . getLocalPathForWrite (  (  ( ContainerLocalizer . USERCACHE )     +    user 1  )  )  )  ;", "LocalizedResource   privLr 1     =    privTracker 1  . getLocalizedResource ( privReq 1  )  ;", "LocalizedResource   privLr 2     =    privTracker 1  . getLocalizedResource ( privReq 2  )  ;", "appTracker 1  . getPathForLocalization ( appReq 1  ,    dirsHandler . getLocalPathForWrite (  (  ( ContainerLocalizer . APPCACHE )     +    appId 1  )  )  )  ;", "LocalizedResource   appLr 1     =    appTracker 1  . getLocalizedResource ( appReq 1  )  ;", "appTracker 2  . getPathForLocalization ( appReq 2  ,    dirsHandler . getLocalPathForWrite (  (  ( ContainerLocalizer . APPCACHE )     +    appId 2  )  )  )  ;", "LocalizedResource   appLr 2     =    appTracker 2  . getLocalizedResource ( appReq 2  )  ;", "appTracker 2  . getPathForLocalization ( appReq 3  ,    dirsHandler . getLocalPathForWrite (  (  ( ContainerLocalizer . APPCACHE )     +    appId 2  )  )  )  ;", "LocalizedResource   appLr 3     =    appTracker 2  . getLocalizedResource ( appReq 3  )  ;", "pubTracker . getPathForLocalization ( pubReq 1  ,    dirsHandler . getLocalPathForWrite ( ContainerLocalizer . FILECACHE )  )  ;", "LocalizedResource   pubLr 1     =    pubTracker . getLocalizedResource ( pubReq 1  )  ;", "pubTracker . getPathForLocalization ( pubReq 2  ,    dirsHandler . getLocalPathForWrite ( ContainerLocalizer . FILECACHE )  )  ;", "LocalizedResource   pubLr 2     =    pubTracker . getLocalizedResource ( pubReq 2  )  ;", "assertNotNull (  \" Localization   not   started \"  ,    privLr 1  . getLocalPath (  )  )  ;", "privTracker 1  . handle ( new   ResourceLocalizedEvent ( privReq 1  ,    privLr 1  . getLocalPath (  )  ,     (  ( privLr 1  . getSize (  )  )     +     5  )  )  )  ;", "assertNotNull (  \" Localization   not   started \"  ,    privLr 2  . getLocalPath (  )  )  ;", "privTracker 1  . handle ( new   ResourceLocalizedEvent ( privReq 2  ,    privLr 2  . getLocalPath (  )  ,     (  ( privLr 2  . getSize (  )  )     +     1  0  )  )  )  ;", "assertNotNull (  \" Localization   not   started \"  ,    appLr 1  . getLocalPath (  )  )  ;", "appTracker 1  . handle ( new   ResourceLocalizedEvent ( appReq 1  ,    appLr 1  . getLocalPath (  )  ,    appLr 1  . getSize (  )  )  )  ;", "assertNotNull (  \" Localization   not   started \"  ,    appLr 3  . getLocalPath (  )  )  ;", "appTracker 2  . handle ( new   ResourceLocalizedEvent ( appReq 3  ,    appLr 3  . getLocalPath (  )  ,     (  ( appLr 3  . getSize (  )  )     +     7  )  )  )  ;", "assertNotNull (  \" Localization   not   started \"  ,    pubLr 1  . getLocalPath (  )  )  ;", "pubTracker . handle ( new   ResourceLocalizedEvent ( pubReq 1  ,    pubLr 1  . getLocalPath (  )  ,     (  ( pubLr 1  . getSize (  )  )     +     1  0  0  0  )  )  )  ;", "assertNotNull (  \" Localization   not   started \"  ,    pubLr 2  . getLocalPath (  )  )  ;", "pubTracker . handle ( new   ResourceLocalizedEvent ( pubReq 2  ,    pubLr 2  . getLocalPath (  )  ,     (  ( pubLr 2  . getSize (  )  )     +     9  9  9  9  9  )  )  )  ;", "dispatcher . await (  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    privLr 1  . getState (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    privLr 2  . getState (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    appLr 1  . getState (  )  )  ;", "assertEquals ( ResourceState . DOWNLOADING ,    appLr 2  . getState (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    appLr 3  . getState (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    pubLr 1  . getState (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    pubLr 2  . getState (  )  )  ;", "spyService    =    createSpyService ( dispatcher ,    dirsHandler ,    stateStore )  ;", "spyService . init ( conf )  ;", "spyService . recoverLocalizedResources ( stateStore . loadLocalizationState (  )  )  ;", "dispatcher . await (  )  ;", "appTracker 1     =    spyService . getLocalResourcesTracker ( APPLICATION ,    user 1  ,    appId 1  )  ;", "privTracker 1     =    spyService . getLocalResourcesTracker ( PRIVATE ,    user 1  ,    null )  ;", "appTracker 2     =    spyService . getLocalResourcesTracker ( APPLICATION ,    user 2  ,    appId 2  )  ;", "pubTracker    =    spyService . getLocalResourcesTracker ( PUBLIC ,    null ,    null )  ;", "LocalizedResource   recoveredRsrc    =    privTracker 1  . getLocalizedResource ( privReq 1  )  ;", "assertEquals ( privReq 1  ,    recoveredRsrc . getRequest (  )  )  ;", "assertEquals ( privLr 1  . getLocalPath (  )  ,    recoveredRsrc . getLocalPath (  )  )  ;", "assertEquals ( privLr 1  . getSize (  )  ,    recoveredRsrc . getSize (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    recoveredRsrc . getState (  )  )  ;", "recoveredRsrc    =    privTracker 1  . getLocalizedResource ( privReq 2  )  ;", "assertEquals ( privReq 2  ,    recoveredRsrc . getRequest (  )  )  ;", "assertEquals ( privLr 2  . getLocalPath (  )  ,    recoveredRsrc . getLocalPath (  )  )  ;", "assertEquals ( privLr 2  . getSize (  )  ,    recoveredRsrc . getSize (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    recoveredRsrc . getState (  )  )  ;", "recoveredRsrc    =    appTracker 1  . getLocalizedResource ( appReq 1  )  ;", "assertEquals ( appReq 1  ,    recoveredRsrc . getRequest (  )  )  ;", "assertEquals ( appLr 1  . getLocalPath (  )  ,    recoveredRsrc . getLocalPath (  )  )  ;", "assertEquals ( appLr 1  . getSize (  )  ,    recoveredRsrc . getSize (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    recoveredRsrc . getState (  )  )  ;", "recoveredRsrc    =    appTracker 2  . getLocalizedResource ( appReq 2  )  ;", "assertNull (  \" in - progress   resource   should   not   be   present \"  ,    recoveredRsrc )  ;", "recoveredRsrc    =    appTracker 2  . getLocalizedResource ( appReq 3  )  ;", "assertEquals ( appReq 3  ,    recoveredRsrc . getRequest (  )  )  ;", "assertEquals ( appLr 3  . getLocalPath (  )  ,    recoveredRsrc . getLocalPath (  )  )  ;", "assertEquals ( appLr 3  . getSize (  )  ,    recoveredRsrc . getSize (  )  )  ;", "assertEquals ( ResourceState . LOCALIZED ,    recoveredRsrc . getState (  )  )  ;", "}    finally    {", "dispatcher . stop (  )  ;", "stateStore . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRecovery"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "List < Path >    localDirs    =    new   ArrayList < Path >  (  )  ;", "String [  ]    sDirs    =    new   String [  4  ]  ;", "for    ( int   i    =     0  ;    i    <     4  ;     +  + i )     {", "localDirs . add ( lfs . makeQualified ( new   Path (  . basedir ,     ( i    +     \"  \"  )  )  )  )  ;", "sDirs [ i ]     =    localDirs . get ( i )  . toString (  )  ;", "}", "conf . setStrings ( NM _ LOCAL _ DIRS ,    sDirs )  ;", "ResourceLocalizationService . LocalizerTracker   mockLocallilzerTracker    =    mock ( ResourceLocalizationService . LocalizerTracker . class )  ;", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "EventHandler < ApplicationEvent >    applicationBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    applicationBus )  ;", "EventHandler < ContainerEvent >    containerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ContainerEventType . class ,    containerBus )  ;", "EventHandler < LocalizerEvent >    localizerBus    =    mock ( EventHandler . class )  ;", "dispatcher . register ( LocalizerEventType . class ,    localizerBus )  ;", "ContainerExecutor   exec    =    mock ( ContainerExecutor . class )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "DeletionService   delService    =    new   DeletionService ( exec )  ;", "delService . init ( new   Configuration (  )  )  ;", "delService . start (  )  ;", "ResourceLocalizationService   rawService    =    new   ResourceLocalizationService ( dispatcher ,    exec ,    delService ,    dirsHandler ,    new   NMNullStateStoreService (  )  )  ;", "ResourceLocalizationService   spyService    =    spy ( rawService )  ;", "doReturn (  . mockServer )  . when ( spyService )  . createServer (  )  ;", "doReturn ( mockLocallilzerTracker )  . when ( spyService )  . createLocalizerTracker ( isA ( Configuration . class )  )  ;", "doReturn ( lfs )  . when ( spyService )  . getLocalFileContext ( isA ( Configuration . class )  )  ;", "try    {", "spyService . init ( conf )  ;", "spyService . start (  )  ;", "final   String   user    =     \" user 0  \"  ;", "final   Application   app    =    mock ( Application . class )  ;", "final   ApplicationId   appId    =    BuilderUtils . newApplicationId (  3  1  4  1  5  9  2  6  5  3  5  8  9  7  9 L ,     3  )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "spyService . handle ( new   ApplicationLocalizationEvent ( LocalizationEventType . INIT _ APPLICATION _ RESOURCES ,    app )  )  ;", "dispatcher . await (  )  ;", "LocalResourcesTracker   appTracker    =    spyService . getLocalResourcesTracker ( APPLICATION ,    user ,    appId )  ;", "LocalResourcesTracker   privTracker    =    spyService . getLocalResourcesTracker ( PRIVATE ,    user ,    appId )  ;", "LocalResourcesTracker   pubTracker    =    spyService . getLocalResourcesTracker ( PUBLIC ,    user ,    appId )  ;", "final   Container   c    =     . getMockContainer ( appId ,     4  2  ,    user )  ;", "Random   r    =    new   Random (  )  ;", "long   seed    =    r . nextLong (  )  ;", "System . out . println (  (  \" SEED :     \"     +    seed )  )  ;", "r . setSeed ( seed )  ;", "final   LocalResource   privResource    =     . getPrivateMockedResource ( r )  ;", "final   LocalResourceRequest   privReq    =    new   LocalResourceRequest ( privResource )  ;", "final   LocalResource   pubResource    =     . getPublicMockedResource ( r )  ;", "final   LocalResourceRequest   pubReq    =    new   LocalResourceRequest ( pubResource )  ;", "final   LocalResource   pubResource 2     =     . getPublicMockedResource ( r )  ;", "final   LocalResourceRequest   pubReq 2     =    new   LocalResourceRequest ( pubResource 2  )  ;", "final   LocalResource   appResource    =     . getAppMockedResource ( r )  ;", "final   LocalResourceRequest   appReq    =    new   LocalResourceRequest ( appResource )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    req    =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "req . put ( PRIVATE ,    Collections . singletonList ( privReq )  )  ;", "req . put ( PUBLIC ,    Collections . singletonList ( pubReq )  )  ;", "req . put ( APPLICATION ,    Collections . singletonList ( appReq )  )  ;", "Map < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >    req 2     =    new   HashMap < LocalResourceVisibility ,    Collection < LocalResourceRequest >  >  (  )  ;", "req 2  . put ( PRIVATE ,    Collections . singletonList ( privReq )  )  ;", "req 2  . put ( PUBLIC ,    Collections . singletonList ( pubReq 2  )  )  ;", "Set < LocalResourceRequest >    pubRsrcs    =    new   HashSet < LocalResourceRequest >  (  )  ;", "pubRsrcs . add ( pubReq )  ;", "pubRsrcs . add ( pubReq 2  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    req )  )  ;", "spyService . handle ( new   ContainerLocalizationRequestEvent ( c ,    req 2  )  )  ;", "dispatcher . await (  )  ;", "int   privRsrcCount    =     0  ;", "for    ( LocalizedResource   lr    :    privTracker )     {", "privRsrcCount +  +  ;", "Assert . assertEquals (  \" Incorrect   reference   count \"  ,     2  ,    lr . getRefCount (  )  )  ;", "Assert . assertEquals ( privReq ,    lr . getRequest (  )  )  ;", "}", "Assert . assertEquals (  1  ,    privRsrcCount )  ;", "int   pubRsrcCount    =     0  ;", "for    ( LocalizedResource   lr    :    pubTracker )     {", "pubRsrcCount +  +  ;", "Assert . assertEquals (  \" Incorrect   reference   count \"  ,     1  ,    lr . getRefCount (  )  )  ;", "pubRsrcs . remove ( lr . getRequest (  )  )  ;", "}", "Assert . assertEquals (  0  ,    pubRsrcs . size (  )  )  ;", "Assert . assertEquals (  2  ,    pubRsrcCount )  ;", "int   appRsrcCount    =     0  ;", "for    ( LocalizedResource   lr    :    appTracker )     {", "appRsrcCount +  +  ;", "Assert . assertEquals (  \" Incorrect   reference   count \"  ,     1  ,    lr . getRefCount (  )  )  ;", "Assert . assertEquals ( appReq ,    lr . getRequest (  )  )  ;", "}", "Assert . assertEquals (  1  ,    appRsrcCount )  ;", "spyService . handle ( new   ContainerLocalizationCleanupEvent ( c ,    req )  )  ;", "verify ( mockLocallilzerTracker )  . cleanupPrivLocalizers (  \" container _  3  1  4  1  5  9  2  6  5  3  5  8  9  7  9  _  0  0  0  3  _  0  1  _  0  0  0  0  4  2  \"  )  ;", "req 2  . remove ( PRIVATE )  ;", "spyService . handle ( new   ContainerLocalizationCleanupEvent ( c ,    req 2  )  )  ;", "dispatcher . await (  )  ;", "pubRsrcs . add ( pubReq )  ;", "pubRsrcs . add ( pubReq 2  )  ;", "privRsrcCount    =     0  ;", "for    ( LocalizedResource   lr    :    privTracker )     {", "privRsrcCount +  +  ;", "Assert . assertEquals (  \" Incorrect   reference   count \"  ,     1  ,    lr . getRefCount (  )  )  ;", "Assert . assertEquals ( privReq ,    lr . getRequest (  )  )  ;", "}", "Assert . assertEquals (  1  ,    privRsrcCount )  ;", "pubRsrcCount    =     0  ;", "for    ( LocalizedResource   lr    :    pubTracker )     {", "pubRsrcCount +  +  ;", "Assert . assertEquals (  \" Incorrect   reference   count \"  ,     0  ,    lr . getRefCount (  )  )  ;", "pubRsrcs . remove ( lr . getRequest (  )  )  ;", "}", "Assert . assertEquals (  0  ,    pubRsrcs . size (  )  )  ;", "Assert . assertEquals (  2  ,    pubRsrcCount )  ;", "appRsrcCount    =     0  ;", "for    ( LocalizedResource   lr    :    appTracker )     {", "appRsrcCount +  +  ;", "Assert . assertEquals (  \" Incorrect   reference   count \"  ,     0  ,    lr . getRefCount (  )  )  ;", "Assert . assertEquals ( appReq ,    lr . getRequest (  )  )  ;", "}", "Assert . assertEquals (  1  ,    appRsrcCount )  ;", "}    finally    {", "dispatcher . stop (  )  ;", "delService . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testResourceRelease"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "List < LocalizerResourceRequestEvent >    pending    =    null ;", "do    {", "if    (  ( service . getPrivateLocalizers (  )  . get ( Id )  )     !  =    null )     {", "pending    =    service . getPrivateLocalizers (  )  . get ( Id )  . pending ;", "}", "if    ( pending    =  =    null )     {", "try    {", "maxWaitTime    -  =     2  0  ;", "Thread . sleep (  2  0  )  ;", "}    catch    ( Exception   e )     {", "}", "} else    {", "break ;", "}", "}    while    ( maxWaitTime    >     0     )  ;", "if    ( pending    =  =    null )     {", "return   false ;", "}", "do    {", "if    (  ( pending . size (  )  )     =  =    size )     {", "return   true ;", "} else    {", "try    {", "maxWaitTime    -  =     2  0  ;", "Thread . sleep (  2  0  )  ;", "}    catch    ( Exception   e )     {", "}", "}", "}    while    ( maxWaitTime    >     0     )  ;", "return    ( pending . size (  )  )     =  =    size ;", "}", "METHOD_END"], "methodName": ["waitForPrivateDownloadToStart"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Map < Future < Path >  ,    LocalizerResourceRequestEvent >    pending    =    null ;", "do    {", "if    (  ( service . getPublicLocalizer (  )  )     !  =    null )     {", "pending    =    service . getPublicLocalizer (  )  . pending ;", "}", "if    ( pending    =  =    null )     {", "try    {", "maxWaitTime    -  =     2  0  ;", "Thread . sleep (  2  0  )  ;", "}    catch    ( Exception   e )     {", "}", "} else    {", "break ;", "}", "}    while    ( maxWaitTime    >     0     )  ;", "if    ( pending    =  =    null )     {", "return   false ;", "}", "do    {", "if    (  ( pending . size (  )  )     =  =    size )     {", "return   true ;", "} else    {", "try    {", "maxWaitTime    -  =     2  0  ;", "Thread . sleep (  2  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "}    while    ( maxWaitTime    >     0     )  ;", "return    ( pending . size (  )  )     =  =    size ;", "}", "METHOD_END"], "methodName": ["waitForPublicDownloadToStart"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "LocalResourcesTracker   tracker    =    null ;", "do    {", "if    ( tracker    =  =    null )     {", "tracker    =    service . getLocalResourcesTracker ( vis ,    user ,    appId )  ;", "}", "if    (  ( tracker    !  =    null )     &  &     ( lr    =  =    null )  )     {", "lr    =    tracker . getLocalizedResource ( req )  ;", "}", "if    ( lr    !  =    null )     {", "break ;", "} else    {", "try    {", "maxWaitTime    -  =     2  0  ;", "Thread . sleep (  2  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "}    while    ( maxWaitTime    >     0     )  ;", "if    ( lr    =  =    null )     {", "return   false ;", "}", "do    {", "if    (  !  ( lr . getState (  )  . equals ( resourceState )  )  )     {", "try    {", "maxWaitTime    -  =     5  0  ;", "Thread . sleep (  5  0  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "} else    {", "break ;", "}", "}    while    ( maxWaitTime    >     0     )  ;", "return   lr . getState (  )  . equals ( resourceState )  ;", "}", "METHOD_END"], "methodName": ["waitForResourceState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceLocalizationService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "ConcurrentMap < Localquest ,    LocalizedResource >    trackerResources    =    new   ConcurrentHashMap < Localquest ,    LocalizedResource >  (  )  ;", "LocalResourcesTracker   ret    =    spy ( new   LocalResourcesTrackerImpl ( user ,    null ,    null ,    trackerResources ,    false ,    conf ,    new   NMNullStateStoreService (  )  )  )  ;", "for    ( int   i    =     0  ;    i    <    nRsrcs ;     +  + i )     {", "final   Localquest   req    =    new   Localquest ( new   Path (  (  (  (  \" file :  /  /  /  \"     +    user )     +     \"  / rsrc \"  )     +    i )  )  ,     ( timestamp    +     ( i    *    tsstep )  )  ,    LocalResourceType . FILE ,    LocalResourceVisibility . PUBLIC ,    null )  ;", "final   long   ts    =    timestamp    +     ( i    *    tsstep )  ;", "final   Path   p    =    new   Path (  (  (  (  \" file :  /  /  / local /  \"     +    user )     +     \"  / rsrc \"  )     +    i )  )  ;", "LocalizedResource   rsrc    =    new   LocalizedResource ( req ,    null )     {", "@ Override", "public   int   getRefCount (  )     {", "return    0  ;", "}", "@ Override", "public   long   getSize (  )     {", "return   rsrcSize ;", "}", "@ Override", "public   Path   getLocalPath (  )     {", "return   p ;", "}", "@ Override", "public   long   getTimestamp (  )     {", "return   ts ;", "}", "@ Override", "public   ResourceState   getState (  )     {", "return   ResourceState . LOCALIZED ;", "}", "}  ;", "trackerResources . put ( req ,    rsrc )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["createMockTracker"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceRetention"}, {"methodBody": ["METHOD_START", "{", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "long   TARGET _ MB    =     1  0     <  <     2  0  ;", "Set   rss    =    new   Set ( delService ,    TARGET _ MB )  ;", "LocalResourcesTracker   pubTracker    =    createMockTracker ( null ,     (  (  3     *     1  0  2  4  )     *     1  0  2  4  )  ,     2  ,     1  0  ,     5  )  ;", "LocalResourcesTracker   trackerA    =    createMockTracker (  \" A \"  ,     (  (  1     *     1  0  2  4  )     *     1  0  2  4  )  ,     4  ,     3  ,     3  )  ;", "LocalResourcesTracker   trackerB    =    createMockTracker (  \" B \"  ,     (  (  4     *     1  0  2  4  )     *     1  0  2  4  )  ,     1  ,     1  0  ,     5  )  ;", "LocalResourcesTracker   trackerC    =    createMockTracker (  \" C \"  ,     (  (  2     *     1  0  2  4  )     *     1  0  2  4  )  ,     3  ,     7  ,     2  )  ;", "rss . addResources ( pubTracker )  ;", "rss . addResources ( trackerA )  ;", "rss . addResources ( trackerB )  ;", "rss . addResources ( trackerC )  ;", "long   deleted    =     0 L ;", "ArgumentCaptor < LocalizedResource >    captor    =    ArgumentCaptor . forClass ( LocalizedResource . class )  ;", "verify ( pubTracker ,    atMost (  2  )  )  . remove ( captor . capture (  )  ,    isA ( DeletionService . class )  )  ;", "verify ( trackerA ,    atMost (  4  )  )  . remove ( captor . capture (  )  ,    isA ( DeletionService . class )  )  ;", "verify ( trackerB ,    atMost (  1  )  )  . remove ( captor . capture (  )  ,    isA ( DeletionService . class )  )  ;", "verify ( trackerC ,    atMost (  3  )  )  . remove ( captor . capture (  )  ,    isA ( DeletionService . class )  )  ;", "for    ( LocalizedResource   rem    :    captor . getAllValues (  )  )     {", "deleted    +  =    rem . getSize (  )  ;", "}", "assertTrue (  ( deleted    >  =     (  (  1  0     *     1  0  2  4  )     *     1  0  2  4  )  )  )  ;", "assertTrue (  ( deleted    <     (  (  1  5     *     1  0  2  4  )     *     1  0  2  4  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRsrcUnused"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.TestResourceRetention"}, {"methodBody": ["METHOD_START", "{", "return   app ;", "}", "METHOD_END"], "methodName": ["getApplication"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ApplicationLocalizationEvent"}, {"methodBody": ["METHOD_START", "{", "return   rsrc ;", "}", "METHOD_END"], "methodName": ["getResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationCleanupEvent"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent"}, {"methodBody": ["METHOD_START", "{", "return   rsrc ;", "}", "METHOD_END"], "methodName": ["getRequestedResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationRequestEvent"}, {"methodBody": ["METHOD_START", "{", "return   localizerId ;", "}", "METHOD_END"], "methodName": ["getLocalizerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEvent"}, {"methodBody": ["METHOD_START", "{", "return   context ;", "}", "METHOD_END"], "methodName": ["getContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent"}, {"methodBody": ["METHOD_START", "{", "return   pattern ;", "}", "METHOD_END"], "methodName": ["getPattern"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent"}, {"methodBody": ["METHOD_START", "{", "return   resource ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent"}, {"methodBody": ["METHOD_START", "{", "return   vis ;", "}", "METHOD_END"], "methodName": ["getVisibility"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerResourceRequestEvent"}, {"methodBody": ["METHOD_START", "{", "return   rsrc ;", "}", "METHOD_END"], "methodName": ["getLocalResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceEvent"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticMesage ;", "}", "METHOD_END"], "methodName": ["getDiagnosticMessage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceFailedLocalizationEvent"}, {"methodBody": ["METHOD_START", "{", "return   location ;", "}", "METHOD_END"], "methodName": ["getLocation"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceLocalizedEvent"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceLocalizedEvent"}, {"methodBody": ["METHOD_START", "{", "return   localPath ;", "}", "METHOD_END"], "methodName": ["getLocalPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceRecoveredEvent"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceRecoveredEvent"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceReleaseEvent"}, {"methodBody": ["METHOD_START", "{", "return   context ;", "}", "METHOD_END"], "methodName": ["getContext"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceRequestEvent"}, {"methodBody": ["METHOD_START", "{", "return   vis ;", "}", "METHOD_END"], "methodName": ["getVisibility"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ResourceRequestEvent"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId ;", "while    (  (  !  ( this . appFinishing . get (  )  )  )     &  &     (  !  ( this . aborted . get (  )  )  )  )     {", "synchronized ( this )     {", "try    {", "wait (  . THREAD _ SLEEP _ TIME )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . warn (  \" PendingContainers   queue   is   interrupted \"  )  ;", "this . appFinishing . set ( true )  ;", "}", "}", "}", "if    ( this . aborted . get (  )  )     {", "return ;", "}", "while    (  ( containerId    =    this . pendingContainers . poll (  )  )     !  =    null )     {", "uploadLogsForContainer ( containerId )  ;", "}", "List < String >    rootLogDirs    =    dirsHandler . getLogDirs (  )  ;", "Path [  ]    localAppLogDirs    =    new   Path [ rootLogDirs . size (  )  ]  ;", "int   index    =     0  ;", "for    ( String   rootLogDir    :    rootLogDirs )     {", "localAppLogDirs [ index ]     =    new   Path ( rootLogDir ,    this . applicationId )  ;", "index +  +  ;", "}", "this . delService . delete ( this . userUgi . getShortUserName (  )  ,    null ,    localAppLogDirs )  ;", "if    (  ( this . writer )     !  =    null )     {", "this . writer . close (  )  ;", ". LOG . info (  (  \" Finished   aggregate   log - file   for   app    \"     +     ( this . applicationId )  )  )  ;", "}", "try    {", "userUgi . doAs ( new   PrivilegedExceptionAction < Object >  (  )     {", "@ Override", "public   Object   run (  )    throws   Exception    {", "FileSystem   remoteFS    =    FileSystem . get ( conf )  ;", "remoteFS . rename ( remoteNodeTmpLogFileForApp ,    remoteNodeLogFileForApp )  ;", "return   null ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  (  (  (  (  \" Failed   to   move   temporary   log   file   to   final   location :     [  \"     +     ( remoteNodeTmpLogFileForApp )  )     +     \"  ]    to    [  \"  )     +     ( remoteNodeLogFileForApp )  )     +     \"  ]  \"  )  ,    e )  ;", "}", "this . dispatcher . getEventHandler (  )  . handle ( new   ApplicationEvent ( this . appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )  )  ;", "this . appAggregationFinished . set ( true )  ;", "}", "METHOD_END"], "methodName": ["doAppLogAggregation"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( remoteNodeLogFileForApp . getParent (  )  ,     (  ( remoteNodeLogFileForApp . getName (  )  )     +     ( AppLogAggregatorImpl . TMP _ FILE _ SUFFIX )  )  )  ;", "}", "METHOD_END"], "methodName": ["getRemoteNodeTmpLogFileForApp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( this . retentionPolicy . equals ( ALL _ CONTAINERS )  )     {", "return   true ;", "}", "if    ( this . retentionPolicy . equals ( APPLICATION _ MASTER _ ONLY )  )     {", "if    (  ( Id . getId (  )  )     =  =     1  )     {", "return   true ;", "}", "return   false ;", "}", "if    ( this . retentionPolicy . equals ( AM _ AND _ FAILED _ CONTAINERS _ ONLY )  )     {", "if    (  ( Id . getId (  )  )     =  =     1  )     {", "return   true ;", "} else", "if    (  ! wasContainerSuccessful )     {", "return   true ;", "}", "return   false ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["shouldUploadLogs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( this . logAggregationDisabled )     {", "return ;", "}", "if    (  ( this . writer )     =  =    null )     {", "AppLogAggregatorImpl . LOG . info (  (  (  (  \" Starting   aggregate   log - file   for   app    \"     +     ( this . applicationId )  )     +     \"    at    \"  )     +     ( this . remoteNodeTmpLogFileForApp )  )  )  ;", "try    {", "this . writer    =    new   logaggregation . AggregatedLogFormat . LogWriter ( this . conf ,    this . remoteNodeTmpLogFileForApp ,    this . userUgi )  ;", "this . writer . writeApplicationACLs ( appAcls )  ;", "this . writer . writeApplicationOwner ( this . userUgi . getShortUserName (  )  )  ;", "}    catch    ( IOException   e )     {", "AppLogAggregatorImpl . LOG . error (  (  (  \" Cannot   create   writer   for   app    \"     +     ( this . applicationId )  )     +     \"  .    Disabling   log - aggregation   for   this   app .  \"  )  ,    e )  ;", "this . logAggregationDisabled    =    true ;", "return ;", "}", "}", "AppLogAggregatorImpl . LOG . info (  (  (  (  \" Uploading   logs   for   container    \"     +    containerId )     +     \"  .    Current   good   log   dirs   are    \"  )     +     ( StringUtils . join (  \"  ,  \"  ,    dirsHandler . getLogDirs (  )  )  )  )  )  ;", "LogKey   logKey    =    new   LogKey ( containerId )  ;", "LogValue   logValue    =    new   LogValue ( dirsHandler . getLogDirs (  )  ,    containerId ,    userUgi . getShortUserName (  )  )  ;", "try    {", "this . writer . append ( logKey ,    logValue )  ;", "}    catch    ( IOException   e )     {", "AppLogAggregatorImpl . LOG . error (  (  (  \" Couldn ' t   upload   logs   for    \"     +    containerId )     +     \"  .    Skipping   this   container .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["uploadLogsForContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl"}, {"methodBody": ["METHOD_START", "{", "boolean   exists    =    true ;", "try    {", "FileStatus   appDirStatus    =    fs . getFileStatus ( path )  ;", "if    (  !  (  . APP _ DIR _ PERMISSIONS . equals ( appDirStatus . getPermission (  )  )  )  )     {", "fs . setPermission ( path ,     . APP _ DIR _ PERMISSIONS )  ;", "}", "}    catch    ( FileNotFoundException   fnfe )     {", "exists    =    false ;", "}", "return   exists ;", "}", "METHOD_END"], "methodName": ["checkExists"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "try    {", "FileSystem . closeAllForUGI ( userUgi )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  \" Failed   to   close   filesystems :     \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["closeFileSystems"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "try    {", "userUgi . doAs ( new   PrivilegedExceptionAction < Object >  (  )     {", "@ Override", "public   Object   run (  )    throws   Exception    {", "try    {", "FileSystem   remoteFS    =    getFileSystem ( getConfig (  )  )  ;", "Path   appDir    =    LogAggregationUtils . getRemoteAppLogDir (  . this . remoteRootLogDir ,    appId ,    user ,     . this . remoteRootLogDirSuffix )  ;", "appDir    =    appDir . makeQualified ( remoteFS . getUri (  )  ,    remoteFS . getWorkingDirectory (  )  )  ;", "if    (  !  ( checkExists ( remoteFS ,    appDir ,     . APP _ DIR _ PERMISSIONS )  )  )     {", "Path   suffixDir    =    LogAggregationUtils . getRemoteLogSuffixedDir (  . this . remoteRootLogDir ,    user ,     . this . remoteRootLogDirSuffix )  ;", "suffixDir    =    suffixDir . makeQualified ( remoteFS . getUri (  )  ,    remoteFS . getWorkingDirectory (  )  )  ;", "if    (  !  ( checkExists ( remoteFS ,    suffixDir ,     . APP _ DIR _ PERMISSIONS )  )  )     {", "Path   userDir    =    LogAggregationUtils . getRemoteLogUserDir (  . this . remoteRootLogDir ,    user )  ;", "userDir    =    userDir . makeQualified ( remoteFS . getUri (  )  ,    remoteFS . getWorkingDirectory (  )  )  ;", "if    (  !  ( checkExists ( remoteFS ,    userDir ,     . APP _ DIR _ PERMISSIONS )  )  )     {", "createDir ( remoteFS ,    userDir ,     . APP _ DIR _ PERMISSIONS )  ;", "}", "createDir ( remoteFS ,    suffixDir ,     . APP _ DIR _ PERMISSIONS )  ;", "}", "createDir ( remoteFS ,    appDir ,     . APP _ DIR _ PERMISSIONS )  ;", "}", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Failed   to   setup   application   log   directory   for    \"     +    appId )  ,    e )  ;", "throw   e ;", "}", "return   null ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["createAppDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "FsPermissdirPerm    =    new   FsPermissfsPerm )  ;", "fs . mkdirs ( path ,    dirPerm )  ;", "FsPermissumask    =    FsPermissgetUMask ( fs . getConf (  )  )  ;", "if    (  !  ( dirPerm . equals ( dirPerm . applyUMask ( umask )  )  )  )     {", "fs . setPermisspath ,    new   FsPermissfsPerm )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "return   FileSystem . get ( conf )  ;", "}", "METHOD_END"], "methodName": ["getFileSystem"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "return   this . appLogAggregators . size (  )  ;", "}", "METHOD_END"], "methodName": ["getNumAggregators"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "return   LogAggregationUtils . getRemoteNodeLogFileForApp ( this . remoteRootLogDir ,    appId ,    user ,    this . nodeId ,    this . remoteRootLogDirSuffix )  ;", "}", "METHOD_END"], "methodName": ["getRemoteNodeLogFileForApp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "ApplicationEvent   eventResponse ;", "try    {", "verifyAndCreateRemoteLogDir ( getConfig (  )  )  ;", "initAppAggregator ( appId ,    user ,    credentials ,    logRetentionPolicy ,    appAcls )  ;", "eventResponse    =    new   ApplicationEvent ( appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )  ;", "}    catch    ( YarnRuntimeException   e )     {", ". LOG . warn (  \" Application   failed   to   init   aggregation \"  ,    e )  ;", "eventResponse    =    new   ApplicationEvent ( appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FAILED )  ;", "}", "this . dispatcher . getEventHandler (  )  . handle ( eventResponse )  ;", "}", "METHOD_END"], "methodName": ["initApp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "final   UserGroupInformation   userUgi    =    UserGroupInformation . createRemoteUser ( user )  ;", "if    ( credentials    !  =    null )     {", "userUgi . addCredentials ( credentials )  ;", "}", "final   Appor   appor    =    new   ApporImpl ( this . dispatcher ,    this . deletionService ,    getConfig (  )  ,    appId ,    userUgi ,    dirsHandler ,    getRemoteNodeLogFileForApp ( appId ,    user )  ,    logRetentionPolicy ,    appAcls )  ;", "if    (  ( this . appors . putIfAbsent ( appId ,    appor )  )     !  =    null )     {", "throw   new   YarnRuntimeException (  (  \" Duplicate   initApp   for    \"     +    appId )  )  ;", "}", "try    {", "createAppDir ( user ,    appId ,    userUgi )  ;", "}    catch    ( Exception   e )     {", "appors . remove ( appId )  ;", "closeFileSystems ( userUgi )  ;", "if    (  !  ( e   instanceof   YarnRuntimeException )  )     {", "e    =    new   YarnRuntimeException ( e )  ;", "}", "throw    (  ( YarnRuntimeException )     ( e )  )  ;", "}", "Runnable   aggregatorWrapper    =    new   Runnable (  )     {", "public   void   run (  )     {", "try    {", "appor . run (  )  ;", "}    finally    {", "appors . remove ( appId )  ;", "closeFileSystems ( userUgi )  ;", "}", "}", "}  ;", "this . threadPool . execute ( aggregatorWrapper )  ;", "}", "METHOD_END"], "methodName": ["initAppAggregator"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . remoteRootLogDir    =    new   Path ( conf . get ( NM _ REMOTE _ APP _ LOG _ DIR ,    DEFAULT _ NM _ REMOTE _ APP _ LOG _ DIR )  )  ;", "this . remoteRootLogDirSuffix    =    conf . get ( NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX ,    DEFAULT _ NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX )  ;", "super . serviceInit ( conf )  ;", "}", "METHOD_END"], "methodName": ["serviceInit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "threadPool . shutdown (  )  ;", "boolean   shouldAbort    =     ( context . getNMStateStore (  )  . canRecover (  )  )     &  &     (  !  ( context . getDecommissioned (  )  )  )  ;", "for    ( AppLogAggregator   aggregator    :    appLogAggregators . values (  )  )     {", "if    ( shouldAbort )     {", "aggregator . abortLogAggregation (  )  ;", "} else    {", "aggregator . finishLogAggregation (  )  ;", "}", "}", "while    (  !  ( threadPool . isTerminated (  )  )  )     {", "for    ( ApplicationId   appId    :    appLogAggregators . keySet (  )  )     {", ". LOG . info (  (  \" Waiting   for   aggregation   to   complete   for    \"     +    appId )  )  ;", "}", "try    {", "if    (  !  ( threadPool . awaitTermination (  3  0  ,    TimeUnit . SECONDS )  )  )     {", "threadPool . shutdownNow (  )  ;", "}", "}    catch    ( InterruptedException   e )     {", ". LOG . warn (  \" Aggregation   stop   interrupted !  \"  )  ;", "break ;", "}", "}", "for    ( ApplicationId   appId    :    appLogAggregators . keySet (  )  )     {", ". LOG . warn (  (  \" Some   logs   may   not   have   been   aggregated   for    \"     +    appId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["stopAggregators"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "AppLogAggregator   aggregator    =    this . appLogAggregators . get ( appId )  ;", "if    ( aggregator    =  =    null )     {", ". LOG . warn (  (  (  \" Log   aggregation   is   not   initialized   for    \"     +    appId )     +     \"  ,    did   it   fail   to   start ?  \"  )  )  ;", "return ;", "}", "aggregator . finishLogAggregation (  )  ;", "}", "METHOD_END"], "methodName": ["stopApp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "AppLogAggregator   aggregator    =    this . appLogAggregators . get ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  ;", "if    ( aggregator    =  =    null )     {", ". LOG . warn (  (  (  \" Log   aggregation   is   not   initialized   for    \"     +    containerId )     +     \"  ,    did   it   fail   to   start ?  \"  )  )  ;", "return ;", "}", "aggregator . startContainerLogAggregation ( containerId ,     ( exitCode    =  =     0  )  )  ;", "}", "METHOD_END"], "methodName": ["stopContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "FileSystem   remoteFS    =    null ;", "try    {", "remoteFS    =    getFileSystem ( conf )  ;", "}    catch    ( IOException   e )     {", "throw   new   YarnRuntimeException (  \" Unable   to   get   Remote   FileSystem   instance \"  ,    e )  ;", "}", "boolean   remoteExists    =    true ;", "try    {", "FsPermission   perms    =    remoteFS . getFileStatus ( this . remoteRootLogDir )  . getPermission (  )  ;", "if    (  !  ( perms . equals (  . TLDIR _ PERMISSIONS )  )  )     {", ". LOG . warn (  (  (  (  (  (  (  (  (  \" Remote   Root   Log   Dir    [  \"     +     ( this . remoteRootLogDir )  )     +     \"  ]    already   exist ,    but   with   incorrect   permissions .     \"  )     +     \" Expected :     [  \"  )     +     (  . TLDIR _ PERMISSIONS )  )     +     \"  ]  ,    Found :     [  \"  )     +    perms )     +     \"  ]  .  \"  )     +     \"    The   cluster   may   have   problems   with   multiple   users .  \"  )  )  ;", "}", "}    catch    ( FileNotFoundException   e )     {", "remoteExists    =    false ;", "}    catch    ( IOException   e )     {", "throw   new   YarnRuntimeException (  (  (  \" Failed   to   check   permissions   for   dir    [  \"     +     ( this . remoteRootLogDir )  )     +     \"  ]  \"  )  ,    e )  ;", "}", "if    (  ! remoteExists )     {", ". LOG . warn (  (  (  \" Remote   Root   Log   Dir    [  \"     +     ( this . remoteRootLogDir )  )     +     \"  ]    does   not   exist .    Attempting   to   create   it .  \"  )  )  ;", "try    {", "Path   qualified    =    this . remoteRootLogDir . makeQualified ( remoteFS . getUri (  )  ,    remoteFS . getWorkingDirectory (  )  )  ;", "remoteFS . mkdirs ( qualified ,    new   FsPermission (  . TLDIR _ PERMISSIONS )  )  ;", "remoteFS . setPermission ( qualified ,    new   FsPermission (  . TLDIR _ PERMISSIONS )  )  ;", "}    catch    ( IOException   e )     {", "throw   new   YarnRuntimeException (  (  (  \" Failed   to   create   remoteLogDir    [  \"     +     ( this . remoteRootLogDir )  )     +     \"  ]  \"  )  ,    e )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["verifyAndCreateRemoteLogDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"}, {"methodBody": ["METHOD_START", "{", "Class < T >    genericClass    =     (  ( Class < T >  )     ( expectedEvents . getClass (  )  . getComponentType (  )  )  )  ;", "ArgumentCaptor < T >    eventCaptor    =    ArgumentCaptor . forClass ( genericClass )  ;", "verify ( eventHandler ,    atLeast (  0  )  )  . handle ( eventCaptor . capture (  )  )  ;", "List < T >    actualEvents    =    eventCaptor . getAllValues (  )  ;", "MultiException   failures    =    new   MultiException (  )  ;", "try    {", "assertEquals (  \" expected   events \"  ,    expectedEvents . length ,    actualEvents . size (  )  )  ;", "}    catch    ( Throwable   e )     {", "failures . add ( e )  ;", "}", "if    ( inOrder )     {", "int   len    =    Math . max ( expectedEvents . length ,    actualEvents . size (  )  )  ;", "for    ( int   n    =     0  ;    n    <    len ;    n +  +  )     {", "try    {", "String   expect    =     ( n    <     ( expectedEvents . length )  )     ?     . eventToString ( expectedEvents [ n ]  ,    methods )     :    null ;", "String   actual    =     ( n    <     ( actualEvents . size (  )  )  )     ?     . eventToString ( actualEvents . get ( n )  ,    methods )     :    null ;", "assertEquals (  (  \" event #  \"     +    n )  ,    expect ,    actual )  ;", "}    catch    ( Throwable   e )     {", "failures . add ( e )  ;", "}", "}", "} else    {", "Set < String >    expectedSet    =    new   HashSet < String >  (  )  ;", "for    ( T   expectedEvent    :    expectedEvents )     {", "expectedSet . add (  . eventToString ( expectedEvent ,    methods )  )  ;", "}", "for    ( T   actualEvent    :    actualEvents )     {", "try    {", "String   actual    =     . eventToString ( actualEvent ,    methods )  ;", "assertTrue (  (  \" unexpected   event :     \"     +    actual )  ,    expectedSet . remove ( actual )  )  ;", "}    catch    ( Throwable   e )     {", "failures . add ( e )  ;", "}", "}", "for    ( String   expected    :    expectedSet )     {", "try    {", "Assert . fail (  (  \" missing   event :     \"     +    expected )  )  ;", "}    catch    ( Throwable   e )     {", "failures . add ( e )  ;", "}", "}", "}", "failures . ifExceptionThrow (  )  ;", "}", "METHOD_END"], "methodName": ["checkEvents"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "Map < ApplicationAccessType ,    String >    appAcls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "appAcls . put ( MODIFY _ APP ,     \" user   group \"  )  ;", "appAcls . put ( VIEW _ APP ,     \"  *  \"  )  ;", "return   appAcls ;", "}", "METHOD_END"], "methodName": ["createAppAcls"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( thisf )  ;", "dispatcher . start (  )  ;", "return   dispatcher ;", "}", "METHOD_END"], "methodName": ["createDispatcher"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  \"  [     \"  )  ;", "for    ( String   m    :    methods )     {", "try    {", "Method   method    =    event . getClass (  )  . getMethod ( m )  ;", "String   value    =    method . invoke ( event )  . toString (  )  ;", "sb . append ( method . getName (  )  )  . append (  \"  =  \"  )  . append ( value )  . append (  \"     \"  )  ;", "}    catch    ( Excep   e )     {", "}", "}", "sb . append (  \"  ]  \"  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["eventToString"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "final   String   logSuffix    =     \" logs \"  ;", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX ,    logSuffix )  ;", "InlineDispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "dispatcher . init ( this . conf )  ;", "dispatcher . start (  )  ;", "FileSystem   fs    =    FileSystem . get ( this . conf )  ;", "final   FileSystem   spyFs    =    spy ( FileSystem . get ( this . conf )  )  ;", "aggSvc    =    new    ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )     {", "@ Override", "protected   FileSystem   getFileSystem ( Configuration   conf )     {", "return   spyFs ;", "}", "}  ;", "aggSvc . init ( this . conf )  ;", "aggSvc . start (  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "Path   userDir    =    fs . makeQualified ( new   Path ( remoteRootLogDir . getAbsolutePath (  )  ,    this . user )  )  ;", "Path   suffixDir    =    new   Path ( userDir ,    logSuffix )  ;", "Path   appDir    =    new   Path ( suffixDir ,    appId . toString (  )  )  ;", "aggSvc . handle ( new   LogHandlerAppStartedEvent ( appId ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "verify ( spyFs )  . mkdirs ( eq ( userDir )  ,    isA ( FsPermission . class )  )  ;", "verify ( spyFs )  . mkdirs ( eq ( suffixDir )  ,    isA ( FsPermission . class )  )  ;", "verify ( spyFs )  . mkdirs ( eq ( appDir )  ,    isA ( FsPermission . class )  )  ;", "ApplicationId   appId 2     =    BuilderUtils . newApplicationId (  1  ,     2  )  ;", "Path   appDir 2     =    new   Path ( suffixDir ,    appId 2  . toString (  )  )  ;", "aggSvc . handle ( new   LogHandlerAppStartedEvent ( appId 2  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "verify ( spyFs )  . mkdirs ( eq ( appDir 2  )  ,    isA ( FsPermission . class )  )  ;", "ApplicationId   appId 3     =    BuilderUtils . newApplicationId (  1  ,     3  )  ;", "Path   appDir 3     =    new   Path ( suffixDir ,    appId 3  . toString (  )  )  ;", "new   File ( appDir 3  . toUri (  )  . getPath (  )  )  . mkdir (  )  ;", "aggSvc . handle ( new   LogHandlerAppStartedEvent ( appId 3  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "verify ( spyFs ,    never (  )  )  . mkdirs ( eq ( appDir 3  )  ,    isA ( FsPermission . class )  )  ;", "}", "METHOD_END"], "methodName": ["testAppLogDirCreation"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . delSrvc    =    new   DeletionService ( createContainerExecutor (  )  )  ;", "delSrvc    =    spy ( delSrvc )  ;", "this . delSrvc . init ( conf )  ;", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LogAggregationService   logAggregationService    =    spy ( new   LogAggregationService ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   application 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "File   app 1 LogDir    =    new   File ( BaseContainerManagerTest . localLogDir ,    ConverterUtils . toString ( application 1  )  )  ;", "app 1 LogDir . mkdir (  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 1  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( application 1  ,     1  )  ;", "ContainerId   container 1  1     =    BuilderUtils . newContainerId ( appAttemptId ,     1  )  ;", "writeContainerLogs ( app 1 LogDir ,    container 1  1  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 1  1  ,     0  )  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( application 1  )  )  ;", "logAggregationService . stop (  )  ;", "assertEquals (  0  ,    logAggregationService . getNumAggregators (  )  )  ;", "verify ( logAggregationService )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "verify ( delSrvc )  . delete ( eq ( user )  ,    eq (  (  ( Path )     ( null )  )  )  ,    eq ( new   Path ( app 1 LogDir . getAbsolutePath (  )  )  )  )  ;", "delSrvc . stop (  )  ;", "String   containerIdStr    =    ConverterUtils . toString ( container 1  1  )  ;", "File   containerLogDir    =    new   File ( app 1 LogDir ,    containerIdStr )  ;", "for    ( String   fileType    :    new   String [  ]  {     \" stdout \"  ,     \" stderr \"  ,     \" syslog \"     }  )     {", "File   f    =    new   File ( containerLogDir ,    fileType )  ;", "Assert . assertFalse (  (  \" check    \"     +    f )  ,    f . exists (  )  )  ;", "}", "Assert . assertFalse ( app 1 LogDir . exists (  )  )  ;", "Path   logFilePath    =    logAggregationService . getRemoteNodeLogFileForApp ( application 1  ,    this . user )  ;", "Assert . assertTrue (  (  (  \" Log   file    [  \"     +    logFilePath )     +     \"  ]    not   found \"  )  ,    new   File ( logFilePath . toUri (  )  . getPath (  )  )  . exists (  )  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( appAttemptId . getApplicationId (  )  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )  ,    new   ApplicationEvent ( appAttemptId . getApplicationId (  )  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )     }  ;", ". checkEvents ( appEventHandler ,    expectedEvents ,    true ,     \" getType \"  ,     \" getApplicationID \"  )  ;", "dispatcher . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testLocalFileDeletionAfterUpload"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LogAggregationService   logAggregationService    =    spy ( new   LogAggregationService ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,     (  ( int )     ( Math . random (  )  )  )  )  ;", "Exception   e    =    new   RuntimeException (  \" KABOOM !  \"  )  ;", "doThrow ( e )  . when ( logAggregationService )  . createAppDir ( any ( String . class )  ,    any ( ApplicationId . class )  ,    any ( UserGroupInformation . class )  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( appId ,    this . user ,    null ,    ContainerLogsRetentionPolicy . AM _ AND _ FAILED _ CONTAINERS _ ONLY ,    this . acls )  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FAILED )     }  ;", ". checkEvents ( appEventHandler ,    expectedEvents ,    false ,     \" getType \"  ,     \" getApplicationID \"  ,     \" getDiagnostic \"  )  ;", "verify ( logAggregationService )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( BuilderUtils . newContainerId (  4  ,     1  ,     1  ,     1  )  ,     0  )  )  ;", "dispatcher . await (  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( BuilderUtils . newApplicationId (  1  ,     5  )  )  )  ;", "dispatcher . await (  )  ;", "logAggregationService . stop (  )  ;", "assertEquals (  0  ,    logAggregationService . getNumAggregators (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLogAggregationCreateDirsFailsWithoutKillingNM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . containerManager . start (  )  ;", "File   scriptFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" scriptFile . sh \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( scriptFile )  ;", "fileWriter . write (  (  \"  \\ necho   Hello   World !    Stdout !     >     \"     +     ( new   File ( BaseContainerManagerTest . localLogDir ,     \" stdout \"  )  )  )  )  ;", "fileWriter . write (  (  \"  \\ necho   Hello   World !    Stderr !     >     \"     +     ( new   File ( BaseContainerManagerTest . localLogDir ,     \" stderr \"  )  )  )  )  ;", "fileWriter . write (  (  \"  \\ necho   Hello   World !    Syslog !     >     \"     +     ( new   File ( BaseContainerManagerTest . localLogDir ,     \" syslog \"  )  )  )  )  ;", "fileWriter . close (  )  ;", "ContainerLaunchContext   containerLaunchContext    =     . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   cId    =    BuilderUtils . newContainerId ( appAttemptId ,     0  )  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =     . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "commands . add (  \"  / bin / bash \"  )  ;", "commands . add ( scriptFile . getAbsolutePath (  )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    TestContainerManager . createContainerToken ( cId ,    DUMMY _ RM _ IDENTIFIER ,    context . getNodeId (  )  ,    user ,    context . getContainerTokenSecretManager (  )  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "this . containerManager . startContainers ( allRequests )  ;", "BaseContainerManagerTest . waitForContainerState ( this . containerManager ,    cId ,    COMPLETE )  ;", "this . containerManager . handle ( new   CMgrCompletedAppsEvent ( Arrays . asList ( appId )  ,    CMgrCompletedAppsEvent . Reason . ON _ SHUTDOWN )  )  ;", "this . containerManager . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testLogAggregationForRealContainerLaunch"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LogAggregationService   logAggregationService    =    spy ( new   LogAggregationService ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,     (  ( int )     ( Math . random (  )  )  )  )  ;", "doThrow ( new   YarnRuntimeException (  \" KABOOM !  \"  )  )  . when ( logAggregationService )  . initAppAggregator ( eq ( appId )  ,    eq ( user )  ,    any ( Credentials . class )  ,    any ( ContainerLogsRetentionPolicy . class )  ,    anyMap (  )  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( appId ,    this . user ,    null ,    ContainerLogsRetentionPolicy . AM _ AND _ FAILED _ CONTAINERS _ ONLY ,    this . acls )  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FAILED )     }  ;", ". checkEvents ( appEventHandler ,    expectedEvents ,    false ,     \" getType \"  ,     \" getApplicationID \"  ,     \" getDiagnostic \"  )  ;", "verify ( logAggregationService ,    never (  )  )  . closeFileSystems ( any ( UserGroupInformation . class )  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( BuilderUtils . newContainerId (  4  ,     1  ,     1  ,     1  )  ,     0  )  )  ;", "dispatcher . await (  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( BuilderUtils . newApplicationId (  1  ,     5  )  )  )  ;", "dispatcher . await (  )  ;", "}", "METHOD_END"], "methodName": ["testLogAggregationInitAppFailsWithoutKillingNM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "DeletionService   delSrvc    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   mockedDirSvc    =    mock ( LocalDirsHandlerService . class )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "logAggregationService    =    new    ( dispatcher ,    this . context ,    delSrvc ,    mockedDirSvc )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   application 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 1  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( application 1  )  )  ;", "dispatcher . await (  )  ;", "int   timeToWait    =     2  0     *     1  0  0  0  ;", "while    (  ( timeToWait    >     0  )     &  &     (  ( logAggregationService . getNumAggregators (  )  )     >     0  )  )     {", "Thread . sleep (  1  0  0  )  ;", "timeToWait    -  =     1  0  0  ;", "}", "Assert . assertEquals (  \" Log   aggregator   failed   to   cleanup !  \"  ,     0  ,    logAggregationService . getNumAggregators (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLogAggregatorCleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LogAggregationService   logAggregationService    =    new   LogAggregationService ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   application 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "File   app 1 LogDir    =    new   File ( BaseContainerManagerTest . localLogDir ,    ConverterUtils . toString ( application 1  )  )  ;", "app 1 LogDir . mkdir (  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 1  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "ApplicationAttemptId   appAttemptId 1     =    BuilderUtils . newApplicationAttemptId ( application 1  ,     1  )  ;", "ContainerId   container 1  1     =    BuilderUtils . newContainerId ( appAttemptId 1  ,     1  )  ;", "writeContainerLogs ( app 1 LogDir ,    container 1  1  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 1  1  ,     0  )  )  ;", "ApplicationId   application 2     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     2  )  ;", "ApplicationAttemptId   appAttemptId 2     =    BuilderUtils . newApplicationAttemptId ( application 2  ,     1  )  ;", "File   app 2 LogDir    =    new   File ( BaseContainerManagerTest . localLogDir ,    ConverterUtils . toString ( application 2  )  )  ;", "app 2 LogDir . mkdir (  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 2  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . APPLICATION _ MASTER _ ONLY ,    this . acls )  )  ;", "ContainerId   container 2  1     =    BuilderUtils . newContainerId ( appAttemptId 2  ,     1  )  ;", "writeContainerLogs ( app 2 LogDir ,    container 2  1  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 2  1  ,     0  )  )  ;", "ContainerId   container 1  2     =    BuilderUtils . newContainerId ( appAttemptId 1  ,     2  )  ;", "writeContainerLogs ( app 1 LogDir ,    container 1  2  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 1  2  ,     0  )  )  ;", "ApplicationId   application 3     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     3  )  ;", "ApplicationAttemptId   appAttemptId 3     =    BuilderUtils . newApplicationAttemptId ( application 3  ,     1  )  ;", "File   app 3 LogDir    =    new   File ( BaseContainerManagerTest . localLogDir ,    ConverterUtils . toString ( application 3  )  )  ;", "app 3 LogDir . mkdir (  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 3  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . AM _ AND _ FAILED _ CONTAINERS _ ONLY ,    this . acls )  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedInitEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( application 1  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )  ,    new   ApplicationEvent ( application 2  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )  ,    new   ApplicationEvent ( application 3  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )     }  ;", ". checkEvents ( appEventHandler ,    expectedInitEvents ,    false ,     \" getType \"  ,     \" getApplicationID \"  )  ;", "reset ( appEventHandler )  ;", "ContainerId   container 3  1     =    BuilderUtils . newContainerId ( appAttemptId 3  ,     1  )  ;", "writeContainerLogs ( app 3 LogDir ,    container 3  1  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 3  1  ,     0  )  )  ;", "ContainerId   container 3  2     =    BuilderUtils . newContainerId ( appAttemptId 3  ,     2  )  ;", "writeContainerLogs ( app 3 LogDir ,    container 3  2  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 3  2  ,     1  )  )  ;", "ContainerId   container 2  2     =    BuilderUtils . newContainerId ( appAttemptId 2  ,     2  )  ;", "writeContainerLogs ( app 2 LogDir ,    container 2  2  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 2  2  ,     0  )  )  ;", "ContainerId   container 3  3     =    BuilderUtils . newContainerId ( appAttemptId 3  ,     3  )  ;", "writeContainerLogs ( app 3 LogDir ,    container 3  3  )  ;", "logAggregationService . handle ( new   LogHandlerContainerFinishedEvent ( container 3  3  ,     0  )  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( application 2  )  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( application 3  )  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( application 1  )  )  ;", "logAggregationService . stop (  )  ;", "assertEquals (  0  ,    logAggregationService . getNumAggregators (  )  )  ;", "verifyContainerLogs ( logAggregationService ,    application 1  ,    new   ContainerId [  ]  {    container 1  1  ,    container 1  2     }  )  ;", "verifyContainerLogs ( logAggregationService ,    application 2  ,    new   ContainerId [  ]  {    container 2  1     }  )  ;", "verifyContainerLogs ( logAggregationService ,    application 3  ,    new   ContainerId [  ]  {    container 3  1  ,    container 3  2     }  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedFinishedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( application 1  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )  ,    new   ApplicationEvent ( application 2  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )  ,    new   ApplicationEvent ( application 3  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )     }  ;", ". checkEvents ( appEventHandler ,    expectedFinishedEvents ,    false ,     \" getType \"  ,     \" getApplicationID \"  )  ;", "dispatcher . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleAppsLogAggregation"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LogAggregationService   logAggregationService    =    new   LogAggregationService ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   application 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "File   app 1 LogDir    =    new   File ( BaseContainerManagerTest . localLogDir ,    ConverterUtils . toString ( application 1  )  )  ;", "app 1 LogDir . mkdir (  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 1  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "logAggregationService . handle ( new   LogHandlerAppFinishedEvent ( application 1  )  )  ;", "logAggregationService . stop (  )  ;", "assertEquals (  0  ,    logAggregationService . getNumAggregators (  )  )  ;", "Assert . assertFalse ( new   File ( logAggregationService . getRemoteNodeLogFileForApp ( application 1  ,    this . user )  . toUri (  )  . getPath (  )  )  . exists (  )  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( application 1  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )  ,    new   ApplicationEvent ( application 1  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FINISHED )     }  ;", ". checkEvents ( appEventHandler ,    expectedEvents ,    true ,     \" getType \"  ,     \" getApplicationID \"  )  ;", "dispatcher . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNoContainerOnNode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "DeletionService   delSrvc    =    mock ( DeletionService . class )  ;", "LocalDirsHandlerService   mockedDirSvc    =    mock ( LocalDirsHandlerService . class )  ;", "when ( mockedDirSvc . getLogDirs (  )  )  . thenThrow ( new   RuntimeException (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "logAggregationService    =    new    ( dispatcher ,    this . context ,    delSrvc ,    mockedDirSvc )  ;", "logAggregationService . init ( this . conf )  ;", "logAggregationService . start (  )  ;", "ApplicationId   application 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( application 1  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    this . acls )  )  ;", "logAggregationService . stop (  )  ;", "assertEquals (  0  ,    logAggregationService . getNumAggregators (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStopAfterError"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "File   aNewFile    =    new   File ( String . valueOf (  (  \" tmp \"     +     ( System . currentTimeMillis (  )  )  )  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    aNewFile . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "logAggregationService    =    spy ( new    ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  )  ;", "logAggregationService . init ( this . conf )  ;", "boolean   existsBefore    =    aNewFile . exists (  )  ;", "assertTrue (  \" The   new   file   already   exists !  \"  ,     (  ! existsBefore )  )  ;", "logAggregationService . verifyAndCreateRemoteLogDir ( this . conf )  ;", "boolean   existsAfter    =    aNewFile . exists (  )  ;", "assertTrue (  \" The   new   aggregate   file   is   not   successfully   created \"  ,    existsAfter )  ;", "aNewFile . delete (  )  ;", "}", "METHOD_END"], "methodName": ["testVerifyAndCreateRemoteDirNonExistence"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "this . conf . set ( NM _ LOG _ DIRS ,    BaseContainerManagerTest . localLogDir . getAbsolutePath (  )  )  ;", "this . conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    this . remoteRootLogDir . getAbsolutePath (  )  )  ;", "DrainDispatcher   dispatcher    =    createDispatcher (  )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LogAggregationService   logAggregationService    =    spy ( new   LogAggregationService ( dispatcher ,    this . context ,    this . delSrvc ,    super . dirsHandler )  )  ;", "logAggregationService . init ( this . conf )  ;", "YarnRuntimeException   e    =    new   YarnRuntimeException (  \" KABOOM !  \"  )  ;", "doThrow ( e )  . when ( logAggregationService )  . verifyAndCreateRemoteLogDir ( any ( Configuration . class )  )  ;", "logAggregationService . start (  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,     (  ( int )     ( Math . random (  )  )  )  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( appId ,    this . user ,    null ,    ContainerLogsRetentionPolicy . AM _ AND _ FAILED _ CONTAINERS _ ONLY ,    this . acls )  )  ;", "dispatcher . await (  )  ;", "ApplicationEvent [  ]    expectedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FAILED )     }  ;", ". checkEvents ( appEventHandler ,    expectedEvents ,    false ,     \" getType \"  ,     \" getApplicationID \"  ,     \" getDiagnostic \"  )  ;", "Mockito . reset ( logAggregationService )  ;", "ApplicationId   appId 2     =    BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,     (  ( int )     ( Math . random (  )  )  )  )  ;", "File   appLogDir    =    new   File ( BaseContainerManagerTest . localLogDir ,    ConverterUtils . toString ( appId 2  )  )  ;", "appLogDir . mkdir (  )  ;", "logAggregationService . handle ( new   LogHandlerAppStartedEvent ( appId 2  ,    this . user ,    null ,    ContainerLogsRetentionPolicy . AM _ AND _ FAILED _ CONTAINERS _ ONLY ,    this . acls )  )  ;", "dispatcher . await (  )  ;", "expectedEvents    =    new   ApplicationEvent [  ]  {    new   ApplicationEvent ( appId ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ FAILED )  ,    new   ApplicationEvent ( appId 2  ,    ApplicationEventType . APPLICATION _ LOG _ HANDLING _ INITED )     }  ;", ". checkEvents ( appEventHandler ,    expectedEvents ,    false ,     \" getType \"  ,     \" getApplicationID \"  ,     \" getDiagnostic \"  )  ;", "logAggregationService . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testVerifyAndCreateRemoteDirsFailure"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( this . acls . size (  )  ,    logAcls . size (  )  )  ;", "for    ( ApplicAccessType   appAccessType    :    this . acls . keySet (  )  )     {", "Assert . assertEquals ( this . acls . get ( appAccessType )  ,    logAcls . get ( appAccessType )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyAcls"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "AggregatedLogFormat . LogReader   reader    =    new   AggregatedLogFormat . LogReader ( this . conf ,    logAggregationService . getRemoteNodeLogFileForApp ( appId ,    this . user )  )  ;", "Assert . assertEquals ( this . user ,    reader . getApplicationOwner (  )  )  ;", "verifyAcls ( reader . getApplicationAcls (  )  )  ;", "try    {", "Map < String ,    Map < String ,    String >  >    logMap    =    new   HashMap < String ,    Map < String ,    String >  >  (  )  ;", "DataInputStream   valueStream ;", "LogKey   key    =    new   LogKey (  )  ;", "valueStream    =    reader . next ( key )  ;", "while    ( valueStream    !  =    null )     {", "BaseContainerManagerTest . LOG . info (  (  \" Found   container    \"     +     ( key . toString (  )  )  )  )  ;", "Map < String ,    String >    perContainerMap    =    new   HashMap < String ,    String >  (  )  ;", "logMap . put ( key . toString (  )  ,    perContainerMap )  ;", "while    ( true )     {", "try    {", "ByteArrayOutputStream   baos    =    new   ByteArrayOutputStream (  )  ;", "PrintStream   ps    =    new   PrintStream ( baos )  ;", "LogReader . readAContainerLogsForALogType ( valueStream ,    ps )  ;", "String [  ]    writtenLines    =    baos . toString (  )  . split ( System . getProperty (  \" line . separator \"  )  )  ;", "Assert . assertEquals (  \" LogType :  \"  ,    writtenLines [  0  ]  . substring (  0  ,     8  )  )  ;", "String   fileType    =    writtenLines [  0  ]  . substring (  9  )  ;", "Assert . assertEquals (  \" LogLength :  \"  ,    writtenLines [  1  ]  . substring (  0  ,     1  0  )  )  ;", "String   fileLengthStr    =    writtenLines [  1  ]  . substring (  1  1  )  ;", "long   fileLength    =    Long . parseLong ( fileLengthStr )  ;", "Assert . assertEquals (  \" Log   Contents :  \"  ,    writtenLines [  2  ]  . substring (  0  ,     1  3  )  )  ;", "String   logContents    =    StringUtils . join ( Arrays . copyOfRange ( writtenLines ,     3  ,    writtenLines . length )  ,     \"  \\ n \"  )  ;", "perContainerMap . put ( fileType ,    logContents )  ;", "BaseContainerManagerTest . LOG . info (  (  \" LogType :  \"     +    fileType )  )  ;", "BaseContainerManagerTest . LOG . info (  (  \" LogType :  \"     +    fileLength )  )  ;", "BaseContainerManagerTest . LOG . info (  (  \" Log   Contents :  \\ n \"     +     ( perContainerMap . get ( fileType )  )  )  )  ;", "}    catch    ( EOFException   eof )     {", "break ;", "}", "}", "key    =    new   LogKey (  )  ;", "valueStream    =    reader . next ( key )  ;", "}", "Assert . assertEquals ( expectedContainerIds . length ,    logMap . size (  )  )  ;", "for    ( ContainerId   cId    :    expectedContainerIds )     {", "String   containerStr    =    ConverterUtils . toString ( cId )  ;", "Map < String ,    String >    thisContainerMap    =    logMap . remove ( containerStr )  ;", "Assert . assertEquals (  3  ,    thisContainerMap . size (  )  )  ;", "for    ( String   fileType    :    new   String [  ]  {     \" stdout \"  ,     \" stderr \"  ,     \" syslog \"     }  )     {", "String   expectedValue    =     (  ( containerStr    +     \"    Hello    \"  )     +    fileType )     +     \"  !  \"  ;", "BaseContainerManagerTest . LOG . info (  (  \" Expected   log - content    :     \"     +     ( new   String ( expectedValue )  )  )  )  ;", "String   foundValue    =    thisContainerMap . remove ( fileType )  ;", "Assert . assertNotNull (  (  (  ( cId    +     \"     \"  )     +    fileType )     +     \"    not   present   in   aggregated   log - file !  \"  )  ,    foundValue )  ;", "Assert . assertEquals ( expectedValue ,    foundValue )  ;", "}", "Assert . assertEquals (  0  ,    thisContainerMap . size (  )  )  ;", "}", "Assert . assertEquals (  0  ,    logMap . size (  )  )  ;", "}    finally    {", "reader . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyContainerLogs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "String   containerStr    =    ConverterUtils . toString ( containerId )  ;", "File   containerLogDir    =    new   File ( appLogDir ,    containerStr )  ;", "containerLogDir . mkdir (  )  ;", "for    ( String   fileType    :    new   String [  ]  {     \" stdout \"  ,     \" stderr \"  ,     \" syslog \"     }  )     {", "Writer   writer 1  1     =    new   FileWriter ( new   File ( containerLogDir ,    fileType )  )  ;", "writer 1  1  . write (  (  (  ( containerStr    +     \"    Hello    \"  )     +    fileType )     +     \"  !  \"  )  )  ;", "writer 1  1  . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["writeContainerLogs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.TestLogAggregationService"}, {"methodBody": ["METHOD_START", "{", "ThreadFactory   tf    =    new   ThreadFactoryBuilder (  )  . setNameFormat (  \" LogDeleter    #  % d \"  )  . build (  )  ;", "sched    =    new   ScheduledThreadPoolExecutor ( conf . getInt ( NM _ LOG _ DELETION _ THREADS _ COUNT ,    DEFAULT _ NM _ LOG _ DELETE _ THREAD _ COUNT )  ,    tf )  ;", "return   sched ;", "}", "METHOD_END"], "methodName": ["createScheduledThreadPoolExecutor"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "dispatcher . init ( conf )  ;", "dispatcher . start (  )  ;", "return   dispatcher ;", "}", "METHOD_END"], "methodName": ["createDispatcher"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler"}, {"methodBody": ["METHOD_START", "{", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "String   user    =     \" testuser \"  ;", "File [  ]    localLogDirs    =    new   File [  2  ]  ;", "localLogDirs [  0  ]     =    new   File (  \" target \"  ,     (  ( this . getClass (  )  . getName (  )  )     +     \"  - localLogDir 0  \"  )  )  . getAbsoluteFile (  )  ;", "localLogDirs [  1  ]     =    new   File (  \" target \"  ,     (  ( this . getClass (  )  . getName (  )  )     +     \"  - localLogDir 1  \"  )  )  . getAbsoluteFile (  )  ;", "String   localLogDirsString    =     (  ( localLogDirs [  0  ]  . getAbsolutePath (  )  )     +     \"  ,  \"  )     +     ( localLogDirs [  1  ]  . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    localLogDirsString )  ;", "conf . setBoolean ( LOG _ AGGREGATION _ ENABLED ,    false )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,    DEFAULT _ NM _ LOG _ RETAIN _ SECONDS )  ;", "DrainDispatcher   dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "ApplicationId   appId 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "ApplicationAttemptId   appAttemptId 1     =    BuilderUtils . newApplicationAttemptId ( appId 1  ,     1  )  ;", "ContainerId   container 1  1     =    BuilderUtils . newContainerId ( appAttemptId 1  ,     1  )  ;", "NonAggregatingLogHandler   logHandler    =    new    . NonAggregatingLogHandlerWithMockExecutor ( dispatcher ,    delService ,    dirsHandler )  ;", "logHandler . init ( conf )  ;", "logHandler . start (  )  ;", "logHandler . handle ( new   LogHandlerAppStartedEvent ( appId 1  ,    user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    null )  )  ;", "logHandler . handle ( new   LogHandlerContainerFinishedEvent ( container 1  1  ,     0  )  )  ;", "logHandler . handle ( new   LogHandlerAppFinishedEvent ( appId 1  )  )  ;", "Path [  ]    localAppLogDirs    =    new   Path [  2  ]  ;", "localAppLogDirs [  0  ]     =    new   Path ( localLogDirs [  0  ]  . getAbsolutePath (  )  ,    appId 1  . toString (  )  )  ;", "localAppLogDirs [  1  ]     =    new   Path ( localLogDirs [  1  ]  . getAbsolutePath (  )  ,    appId 1  . toString (  )  )  ;", "ScheduledThreadPoolExecutor   mockSched    =     (  (  . NonAggregatingLogHandlerWithMockExecutor )     ( logHandler )  )  . mockSched ;", "verify ( mockSched )  . schedule ( any ( Runnable . class )  ,    eq (  1  0  8  0  0 L )  ,    eq ( TimeUnit . SECONDS )  )  ;", "}", "METHOD_END"], "methodName": ["testDelayedDelete"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "LocalDirsHandlerService   dirsService    =    new   LocalDirsHandlerService (  )  ;", "DeletionService   delService    =    new   DeletionService ( null )  ;", "aggregatingLogHandler    =    new    ( new   InlineDispatcher (  )  ,    delService ,    dirsService )  ;", "dirsService . init ( conf )  ;", "dirsService . start (  )  ;", "delService . init ( conf )  ;", "delService . start (  )  ;", "aggregatingLogHandler . init ( conf )  ;", "aggregatingLogHandler . start (  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "aggregatingLogHandler . handle ( new   LogHandlerAppFinishedEvent ( appId )  )  ;", "aggregatingLogHandler . stop (  )  ;", "aggregatingLogHandler . handle ( new   LogHandlerAppFinishedEvent ( appId )  )  ;", "}", "METHOD_END"], "methodName": ["testHandlingApplicationFinishedEvent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler"}, {"methodBody": ["METHOD_START", "{", "DeletionService   delService    =    mock ( DeletionService . class )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "String   user    =     \" testuser \"  ;", "File [  ]    localLogDirs    =    new   File [  2  ]  ;", "localLogDirs [  0  ]     =    new   File (  \" target \"  ,     (  ( this . getClass (  )  . getName (  )  )     +     \"  - localLogDir 0  \"  )  )  . getAbsoluteFile (  )  ;", "localLogDirs [  1  ]     =    new   File (  \" target \"  ,     (  ( this . getClass (  )  . getName (  )  )     +     \"  - localLogDir 1  \"  )  )  . getAbsoluteFile (  )  ;", "String   localLogDirsString    =     (  ( localLogDirs [  0  ]  . getAbsolutePath (  )  )     +     \"  ,  \"  )     +     ( localLogDirs [  1  ]  . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    localLogDirsString )  ;", "conf . setBoolean ( LOG _ AGGREGATION _ ENABLED ,    false )  ;", "conf . setLong ( NM _ LOG _ RETAIN _ SECONDS ,     0 L )  ;", "DrainDispatcher   dispatcher    =    createDispatcher ( conf )  ;", "EventHandler < ApplicationEvent >    appEventHandler    =    mock ( EventHandler . class )  ;", "dispatcher . register ( ApplicationEventType . class ,    appEventHandler )  ;", "LocalDirsHandlerService   dirsHandler    =    new   LocalDirsHandlerService (  )  ;", "dirsHandler . init ( conf )  ;", "ApplicationId   appId 1     =    BuilderUtils . newApplicationId (  1  2  3  4  ,     1  )  ;", "ApplicationAttemptId   appAttemptId 1     =    BuilderUtils . newApplicationAttemptId ( appId 1  ,     1  )  ;", "ContainerId   container 1  1     =    BuilderUtils . newContainerId ( appAttemptId 1  ,     1  )  ;", "logHandler    =    new    ( dispatcher ,    delService ,    dirsHandler )  ;", "logHandler . init ( conf )  ;", "logHandler . start (  )  ;", "logHandler . handle ( new   LogHandlerAppStartedEvent ( appId 1  ,    user ,    null ,    ContainerLogsRetentionPolicy . ALL _ CONTAINERS ,    null )  )  ;", "logHandler . handle ( new   LogHandlerContainerFinishedEvent ( container 1  1  ,     0  )  )  ;", "logHandler . handle ( new   LogHandlerAppFinishedEvent ( appId 1  )  )  ;", "Path [  ]    localAppLogDirs    =    new   Path [  2  ]  ;", "localAppLogDirs [  0  ]     =    new   Path ( localLogDirs [  0  ]  . getAbsolutePath (  )  ,    appId 1  . toString (  )  )  ;", "localAppLogDirs [  1  ]     =    new   Path ( localLogDirs [  1  ]  . getAbsolutePath (  )  ,    appId 1  . toString (  )  )  ;", "long   verifyStartTime    =    System . currentTimeMillis (  )  ;", "WantedButNotInvoked   notInvokedException    =    null ;", "boolean   matched    =    false ;", "while    (  (  ! matched )     &  &     (  ( System . currentTimeMillis (  )  )     <     ( verifyStartTime    +     5  0  0  0 L )  )  )     {", "try    {", "verify ( delService )  . delete ( eq ( user )  ,     (  ( Path )     ( eq ( null )  )  )  ,    eq ( localAppLogDirs [  0  ]  )  ,    eq ( localAppLogDirs [  1  ]  )  )  ;", "matched    =    true ;", "}    catch    ( WantedButNotInvoked   e )     {", "notInvokedException    =    e ;", "try    {", "Thread . sleep (  5  0 L )  ;", "}    catch    ( InterruptedException   i )     {", "}", "}", "}", "if    (  ! matched )     {", "throw   notInvokedException ;", "}", "}", "METHOD_END"], "methodName": ["testLogDeletion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler"}, {"methodBody": ["METHOD_START", "{", "NonAggregatingLogHandler   aggregatingLogHandler    =    new   NonAggregatingLogHandler ( null ,    null ,    null )  ;", "aggregatingLogHandler . stop (  )  ;", ". NonAggregatingLogHandlerWithMockExecutor   logHandler    =    new    . NonAggregatingLogHandlerWithMockExecutor ( null ,    null ,    null )  ;", "logHandler . init ( new   Configuration (  )  )  ;", "logHandler . stop (  )  ;", "verify ( logHandler . mockSched )  . shutdown (  )  ;", "verify ( logHandler . mockSched )  . awaitTermination ( eq (  1  0 L )  ,    eq ( TimeUnit . SECONDS )  )  ;", "verify ( logHandler . mockSched )  . shutdownNow (  )  ;", "}", "METHOD_END"], "methodName": ["testStop"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.TestNonAggregatingLogHandler"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppFinishedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . appAcls ;", "}", "METHOD_END"], "methodName": ["getApplicationAcls"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . credentials ;", "}", "METHOD_END"], "methodName": ["getCredentials"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . retentionPolicy ;", "}", "METHOD_END"], "methodName": ["getLogRetentionPolicy"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerContainerFinishedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . exitCode ;", "}", "METHOD_END"], "methodName": ["getExitCode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerContainerFinishedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . pmemLimit ;", "}", "METHOD_END"], "methodName": ["getPmemLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerStartMonitoringEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . vmemLimit ;", "}", "METHOD_END"], "methodName": ["getVmemLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerStartMonitoringEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEvent"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceCalculatorPlugin )     =  =    null )     {", ". LOG . info (  (  (  \" ResourceCalculatorPlugin   is   unavailable   on   this   system .     \"     +     ( this . getClass (  )  . getName (  )  )  )     +     \"    is   disabled .  \"  )  )  ;", "return   false ;", "}", "if    (  ( ResourceCalculatorProcessTree . getResourceCalculatorProcessTree (  \"  0  \"  ,    processTreeClass ,    conf )  )     =  =    null )     {", ". LOG . info (  (  (  \" ResourceCalculatorProcessTree   is   unavailable   on   this   system .     \"     +     ( this . getClass (  )  . getName (  )  )  )     +     \"    is   disabled .  \"  )  )  ;", "return   false ;", "}", "if    (  !  (  ( isPmemCheckEnabled (  )  )     |  |     ( isVmemCheckEnabled (  )  )  )  )     {", ". LOG . info (  (  \" Neither   virutal - memory   nor   physical - memory   monitoring   is    \"     +     \" needed .    Not   running   the   monitor - thread \"  )  )  ;", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["isEnabled"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl"}, {"methodBody": ["METHOD_START", "{", "boolean   isOverLimit    =    false ;", "if    ( currentMemUsage    >     (  2     *    vmemLimit )  )     {", ". LOG . warn (  (  (  (  (  (  (  \" Process   tree   for   container :     \"     +    containerId )     +     \"    running   over   twice    \"  )     +     \" the   configured   limit .    Limit =  \"  )     +    vmemLimit )     +     \"  ,    current   usage    =     \"  )     +    currentMemUsage )  )  ;", "isOverLimit    =    true ;", "} else", "if    ( curMemUsageOfAgedProcesses    >    vmemLimit )     {", ". LOG . warn (  (  (  (  (  (  (  \" Process   tree   for   container :     \"     +    containerId )     +     \"    has   processes   older   than    1     \"  )     +     \" iteration   running   over   the   configured   limit .    Limit =  \"  )     +    vmemLimit )     +     \"  ,    current   usage    =     \"  )     +    curMemUsageOfAgedProcesses )  )  ;", "isOverLimit    =    true ;", "}", "return   isOverLimit ;", "}", "METHOD_END"], "methodName": ["isProcessTreeOverLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl"}, {"methodBody": ["METHOD_START", "{", "long   currentMemUsage    =    pTree . getCumulativeVmem (  )  ;", "long   curMemUsageOfAgedProcesses    =    pTree . getCumulativeVmem (  1  )  ;", "return   isProcessTreeOverLimit ( Id ,    currentMemUsage ,    curMemUsageOfAgedProcesses ,    limit )  ;", "}", "METHOD_END"], "methodName": ["isProcessTreeOverLimit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( NM _ PMEM _ MB ,    nmPmem )  ;", "conf . setBoolean ( NM _ PMEM _ CHECK _ ENABLED ,    pMemEnabled )  ;", "conf . setBoolean ( NM _ VMEM _ CHECK _ ENABLED ,    vMemEnabled )  ;", "conf . setFloat ( NM _ VMEM _ PMEM _ RATIO ,    vMemToPMemRatio )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConfForCM"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor"}, {"methodBody": ["METHOD_START", "{", "conf . setClass ( NM _ CONTAINER _ MON _ RESOURCE _ CALCULATOR ,    LinuxResourceCalculatorPlugin . class ,    ResourceCalculatorPlugin . class )  ;", "conf . setBoolean ( NM _ VMEM _ CHECK _ ENABLED ,    true )  ;", "super . setup (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( ProcfsBasedProcessTree . isAvailable (  )  )  )     {", "return ;", "}", "containerManager . start (  )  ;", "File   scriptFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" scriptFile . sh \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( scriptFile )  ;", "File   processStartFile    =    new   File ( BaseContainerManagerTest . tmpDir ,     \" start _ file . txt \"  )  . getAbsoluteFile (  )  ;", "fileWriter . write (  \"  \\ numask    0  \"  )  ;", "fileWriter . write (  (  \"  \\ necho   Hello   World !     >     \"     +    processStartFile )  )  ;", "fileWriter . write (  (  \"  \\ necho    $  $     >  >     \"     +    processStartFile )  )  ;", "fileWriter . write (  \"  \\ nsleep    1  5  \"  )  ;", "fileWriter . close (  )  ;", "ContainerLaunchContext   containerLaunchContext    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "ContainerId   cId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "int   port    =     1  2  3  4  5  ;", "URL   resource _ alpha    =    ConverterUtils . getYarnUrlFromPath ( BaseContainerManagerTest . localFS . makeQualified ( new   Path ( scriptFile . getAbsolutePath (  )  )  )  )  ;", "LocalResource   rsrc _ alpha    =    BaseContainerManagerTest . recordFactory . newRecordInstance ( LocalResource . class )  ;", "rsrc _ alpha . setResource ( resource _ alpha )  ;", "rsrc _ alpha . setSize (  (  -  1  )  )  ;", "rsrc _ alpha . setVisibility ( APPLICATION )  ;", "rsrc _ alpha . setType ( FILE )  ;", "rsrc _ alpha . setTimestamp ( scriptFile . lastModified (  )  )  ;", "String   destinationFile    =     \" dest _ file \"  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put ( destinationFile ,    rsrc _ alpha )  ;", "containerLaunchContext . setLocalResources ( localResources )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "commands . add (  \"  / bin / bash \"  )  ;", "commands . add ( scriptFile . getAbsolutePath (  )  )  ;", "containerLaunchContext . setCommands ( commands )  ;", "Resource   r    =    BuilderUtils . newResource (  (  (  8     *     1  0  2  4  )     *     1  0  2  4  )  ,     1  )  ;", "ContainerTokenIdentifier   containerIdentifier    =    new   ContainerTokenIdentifier ( cId ,    context . getNodeId (  )  . toString (  )  ,    user ,    r ,     (  ( System . currentTimeMillis (  )  )     +     1  2  0  0  0  0  )  ,     1  2  3  ,    DUMMY _ RM _ IDENTIFIER ,    Priority . newInstance (  0  )  ,     0  )  ;", "Token   containerToken    =    BuilderUtils . newContainerToken ( context . getNodeId (  )  ,    containerManager . getContext (  )  . getContainerTokenSecretManager (  )  . createPassword ( containerIdentifier )  ,    containerIdentifier )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StarRequest   allRequests    =    StarRequest . newInstance ( list )  ;", "containerManager . star ( allRequests )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( processStartFile . exists (  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "BaseContainerManagerTest . LOG . info (  \" Waiting   for   process   start - file   to   be   created \"  )  ;", "}", "Assert . assertTrue (  \" ProcessStartFile   doesn ' t   exist !  \"  ,    processStartFile . exists (  )  )  ;", "BufferedReader   reader    =    new   BufferedReader ( new   FileReader ( processStartFile )  )  ;", "Assert . assertEquals (  \" Hello   World !  \"  ,    reader . readLine (  )  )  ;", "String   pid    =    reader . readLine (  )  . trim (  )  ;", "Assert . assertEquals ( null ,    reader . readLine (  )  )  ;", "BaseContainerManagerTest . waitForContainerState ( containerManager ,    cId ,    COMPLETE ,     6  0  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( cId )  ;", "GetContainerStatusesRequest   gcsRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "ContainerStatus   containerStatus    =    containerManager . getContainerStatuses ( gcsRequest )  . getContainerStatuses (  )  . get (  0  )  ;", "Assert . assertEquals ( KILLED _ EXCEEDED _ VMEM ,    containerStatus . getExitStatus (  )  )  ;", "String   expectedMsgPattern    =     (  (  (  (  (  (  (  (  \" Container    \\  \\  [ pid =  \"     +    pid )     +     \"  , containerID =  \"  )     +    cId )     +     \"  \\  \\  ]    is   running   beyond   virtual   memory   limits .    Current   usage :     \"  )     +     \"  [  0  -  9  .  ]  +     ?  [ KMGTPE ]  ? B   of    [  0  -  9  .  ]  +     ?  [ KMGTPE ]  ? B   physical   memory   used ;     \"  )     +     \"  [  0  -  9  .  ]  +     ?  [ KMGTPE ]  ? B   of    [  0  -  9  .  ]  +     ?  [ KMGTPE ]  ? B   virtual   memory   used .     \"  )     +     \" Killing   container .  \\ nDump   of   the   process - tree   for    \"  )     +    cId )     +     \"     :  \\ n \"  ;", "Pattern   pat    =    Pattern . compile ( expectedMsgPattern )  ;", "Assert . assertEquals (  (  (  (  \" Expected   message   pattern   is :     \"     +    expectedMsgPattern )     +     \"  \\ n \\ nObserved   message   is :     \"  )     +     ( containerStatus . getDiagnostics (  )  )  )  ,    true ,    pat . matcher ( containerStatus . getDiagnostics (  )  )  . find (  )  )  ;", "Assert . assertFalse (  \" Process   is   still   alive !  \"  ,    exec . signalContainer ( user ,    pid ,    ContainerExecutor . Signal . NULL )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerKillOnMemoryOverflow"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor"}, {"methodBody": ["METHOD_START", "{", "ContainersMonitor   cm    =    null ;", "long   expPmem    =     (  8  1  9  2     *     1  0  2  4  )     *     1  0  2  4 L ;", "long   expVmem    =     (  ( long )     ( expPmem    *     2  .  1 F )  )  ;", "cm    =    new   ContainersMonitorImpl ( mock ( ContainerExecutor . class )  ,    mock ( AsyncDispatcher . class )  ,    mock ( Context . class )  )  ;", "cm . init ( getConfForCM ( false ,    false ,     8  1  9  2  ,     2  .  1 F )  )  ;", "assertEquals ( expPmem ,    cm . getPmemAllocatedForContainers (  )  )  ;", "assertEquals ( expVmem ,    cm . getVmemAllocatedForContainers (  )  )  ;", "assertEquals ( false ,    cm . isPmemCheckEnabled (  )  )  ;", "assertEquals ( false ,    cm . isVmemCheckEnabled (  )  )  ;", "cm    =    new   ContainersMonitorImpl ( mock ( ContainerExecutor . class )  ,    mock ( AsyncDispatcher . class )  ,    mock ( Context . class )  )  ;", "cm . init ( getConfForCM ( true ,    false ,     8  1  9  2  ,     2  .  1 F )  )  ;", "assertEquals ( expPmem ,    cm . getPmemAllocatedForContainers (  )  )  ;", "assertEquals ( expVmem ,    cm . getVmemAllocatedForContainers (  )  )  ;", "assertEquals ( true ,    cm . isPmemCheckEnabled (  )  )  ;", "assertEquals ( false ,    cm . isVmemCheckEnabled (  )  )  ;", "cm    =    new   ContainersMonitorImpl ( mock ( ContainerExecutor . class )  ,    mock ( AsyncDispatcher . class )  ,    mock ( Context . class )  )  ;", "cm . init ( getConfForCM ( true ,    true ,     8  1  9  2  ,     2  .  1 F )  )  ;", "assertEquals ( expPmem ,    cm . getPmemAllocatedForContainers (  )  )  ;", "assertEquals ( expVmem ,    cm . getVmemAllocatedForContainers (  )  )  ;", "assertEquals ( true ,    cm . isPmemCheckEnabled (  )  )  ;", "assertEquals ( true ,    cm . isVmemCheckEnabled (  )  )  ;", "cm    =    new   ContainersMonitorImpl ( mock ( ContainerExecutor . class )  ,    mock ( AsyncDispatcher . class )  ,    mock ( Context . class )  )  ;", "cm . init ( getConfForCM ( false ,    true ,     8  1  9  2  ,     2  .  1 F )  )  ;", "assertEquals ( expPmem ,    cm . getPmemAllocatedForContainers (  )  )  ;", "assertEquals ( expVmem ,    cm . getVmemAllocatedForContainers (  )  )  ;", "assertEquals ( false ,    cm . isPmemCheckEnabled (  )  )  ;", "assertEquals ( true ,    cm . isVmemCheckEnabled (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerMonitorMemFlags"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor"}, {"methodBody": ["METHOD_START", "{", "File   procfsRootDir    =    new   File ( BaseContainerManagerTest . localDir ,     \" proc \"  )  ;", "String [  ]    pids    =    new   String [  ]  {     \"  1  0  0  \"  ,     \"  2  0  0  \"  ,     \"  3  0  0  \"  ,     \"  4  0  0  \"  ,     \"  5  0  0  \"  ,     \"  6  0  0  \"  ,     \"  7  0  0  \"     }  ;", "try    {", "TestProcfsBasedProcessTree . setupProcfsRootDir ( procfsRootDir )  ;", "TestProcfsBasedProcessTree . setupPidDirs ( procfsRootDir ,    pids )  ;", "TestProcfsBasedProcessTree [  ]    procs    =    new   TestProcfsBasedProcessTree . ProcessStatInfo [  7  ]  ;", "procs [  0  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  1  0  0  \"  ,     \" proc 1  \"  ,     \"  1  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  0  0  \"     }  )  ;", "procs [  1  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  2  0  0  \"  ,     \" proc 2  \"  ,     \"  1  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  0  0  0  \"     }  )  ;", "procs [  2  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  3  0  0  \"  ,     \" proc 3  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  \"  ,     \"  3  0  0  0  0  0  \"     }  )  ;", "procs [  3  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  4  0  0  \"  ,     \" proc 4  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  \"  ,     \"  4  0  0  0  0  0  \"     }  )  ;", "procs [  4  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  5  0  0  \"  ,     \" proc 5  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  5  0  0  0  0  0  \"     }  )  ;", "procs [  5  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  6  0  0  \"  ,     \" proc 6  \"  ,     \"  1  \"  ,     \"  6  0  0  \"  ,     \"  6  0  0  \"  ,     \"  1  0  0  0  0  0  \"     }  )  ;", "procs [  6  ]     =    new   TestProcfsBasedProcessTree . ProcessStatInfo ( new   String [  ]  {     \"  7  0  0  \"  ,     \" proc 7  \"  ,     \"  6  0  0  \"  ,     \"  6  0  0  \"  ,     \"  6  0  0  \"  ,     \"  1  0  0  0  0  0  \"     }  )  ;", "TestProcfsBasedProcessTree . writeStatFiles ( procfsRootDir ,    pids ,    procs ,    null )  ;", "long   limit    =     7  0  0  0  0  0  ;", "Impl   test    =    new   Impl ( null ,    null ,    null )  ;", "ProcfsBasedProcessTree   pTree    =    new   ProcfsBasedProcessTree (  \"  1  0  0  \"  ,    procfsRootDir . getAbsolutePath (  )  )  ;", "pTree . updateProcessTree (  )  ;", "assertTrue (  (  \" tree   rooted   at    1  0  0    should   be   over   limit    \"     +     \" after   first   iteration .  \"  )  ,    test . isProcessTreeOverLimit ( pTree ,     \" dummyId \"  ,    limit )  )  ;", "pTree    =    new   ProcfsBasedProcessTree (  \"  2  0  0  \"  ,    procfsRootDir . getAbsolutePath (  )  )  ;", "pTree . updateProcessTree (  )  ;", "assertFalse (  (  \" tree   rooted   at    2  0  0    shouldn ' t   be   over   limit    \"     +     \" after   one   iteration .  \"  )  ,    test . isProcessTreeOverLimit ( pTree ,     \" dummyId \"  ,    limit )  )  ;", "pTree . updateProcessTree (  )  ;", "assertTrue (  \" tree   rooted   at    2  0  0    should   be   over   limit   after    2    iterations \"  ,    test . isProcessTreeOverLimit ( pTree ,     \" dummyId \"  ,    limit )  )  ;", "pTree    =    new   ProcfsBasedProcessTree (  \"  6  0  0  \"  ,    procfsRootDir . getAbsolutePath (  )  )  ;", "pTree . updateProcessTree (  )  ;", "assertFalse (  \" tree   rooted   at    6  0  0    should   never   be   over   limit .  \"  ,    test . isProcessTreeOverLimit ( pTree ,     \" dummyId \"  ,    limit )  )  ;", "pTree . updateProcessTree (  )  ;", "assertFalse (  \" tree   rooted   at    6  0  0    should   never   be   over   limit .  \"  ,    test . isProcessTreeOverLimit ( pTree ,     \" dummyId \"  ,    limit )  )  ;", "}    finally    {", "FileUtil . fullyDelete ( procfsRootDir )  ;", "}", "}", "METHOD_END"], "methodName": ["testProcessTreeLimits"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.TestContainersMonitor"}, {"methodBody": ["METHOD_START", "{", "availableGB . incr (  (  ( res . getMemory (  )  )     /     1  0  2  4  )  )  ;", "availableVCores . incr ( res . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["addResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "allocatedContainers . incr (  )  ;", "allocatedGB . incr (  (  ( res . getMemory (  )  )     /     1  0  2  4  )  )  ;", "availableGB . decr (  (  ( res . getMemory (  )  )     /     1  0  2  4  )  )  ;", "allocatedVCores . incr ( res . getVirtualCores (  )  )  ;", "availableVCores . decr ( res . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["allocateContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersCompleted . incr (  )  ;", "}", "METHOD_END"], "methodName": ["completedContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "return   NodeManagerMetrics . create ( DefaultMetricsSystem . instance (  )  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "JvmMetrics . create (  \" NodeManager \"  ,    null ,    ms )  ;", "return   ms . register ( new    (  )  )  ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersIniting . decr (  )  ;", "}", "METHOD_END"], "methodName": ["endInitingContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersRunning . decr (  )  ;", "}", "METHOD_END"], "methodName": ["endRunningContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersFailed . incr (  )  ;", "}", "METHOD_END"], "methodName": ["failedContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "return   containersRunning . value (  )  ;", "}", "METHOD_END"], "methodName": ["getRunningContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersIniting . incr (  )  ;", "}", "METHOD_END"], "methodName": ["initingContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersKilled . incr (  )  ;", "}", "METHOD_END"], "methodName": ["killedContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersLaunched . incr (  )  ;", "}", "METHOD_END"], "methodName": ["launchedContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "allocatedContainers . decr (  )  ;", "allocatedGB . decr (  (  ( res . getMemory (  )  )     /     1  0  2  4  )  )  ;", "availableGB . incr (  (  ( res . getMemory (  )  )     /     1  0  2  4  )  )  ;", "allocatedVCores . decr ( res . getVirtualCores (  )  )  ;", "availableVCores . incr ( res . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["releaseContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "containersRunning . incr (  )  ;", "}", "METHOD_END"], "methodName": ["runningContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "MetricsRecordBuilder   rb    =    getMetrics (  \" NodeManagerMetrics \"  )  ;", "assertCounter (  \" ContainersLaunched \"  ,    launched ,    rb )  ;", "assertCounter (  \" ContainersCompleted \"  ,    completed ,    rb )  ;", "assertCounter (  \" ContainersFailed \"  ,    failed ,    rb )  ;", "assertCounter (  \" ContainersKilled \"  ,    killed ,    rb )  ;", "assertGauge (  \" ContainersIniting \"  ,    initing ,    rb )  ;", "assertGauge (  \" ContainersRunning \"  ,    running ,    rb )  ;", "assertGauge (  \" AllocatedGB \"  ,    allocatedGB ,    rb )  ;", "assertGauge (  \" AllocatedVCores \"  ,    allocatedVCores ,    rb )  ;", "assertGauge (  \" AllocatedContainers \"  ,    allocatedContainers ,    rb )  ;", "assertGauge (  \" AvailableGB \"  ,    availableGB ,    rb )  ;", "assertGauge (  \" AvailableVCores \"  ,    availableVCores ,    rb )  ;", "}", "METHOD_END"], "methodName": ["checkMetrics"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.TestNodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "NodeManagerMetrics   metrics    =    NodeManagerMetrics . create (  )  ;", "Resource   total    =    Records . newRecord ( Resource . class )  ;", "total . setMemory (  (  8     *     (  . GiB )  )  )  ;", "total . setVirtualCores (  1  6  )  ;", "Resource   resource    =    Records . newRecord ( Resource . class )  ;", "resource . setMemory (  (  1     *     (  . GiB )  )  )  ;", "resource . setVirtualCores (  2  )  ;", "metrics . addResource ( total )  ;", "for    ( int   i    =     5  ;     ( i -  -  )     >     0  ;  )     {", "metrics . launchedContainer (  )  ;", "metrics . allocateContainer ( resource )  ;", "}", "metrics . initingContainer (  )  ;", "metrics . endInitingContainer (  )  ;", "metrics . runningContainer (  )  ;", "metrics . endRunningContainer (  )  ;", "metrics . completedContainer (  )  ;", "metrics . releaseContainer ( resource )  ;", "metrics . failedContainer (  )  ;", "metrics . releaseContainer ( resource )  ;", "metrics . killedContainer (  )  ;", "metrics . releaseContainer ( resource )  ;", "metrics . initingContainer (  )  ;", "metrics . runningContainer (  )  ;", "checkMetrics (  5  ,     1  ,     1  ,     1  ,     1  ,     1  ,     2  ,     2  ,     6  ,     4  ,     1  2  )  ;", "}", "METHOD_END"], "methodName": ["testNames"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.metrics.TestNodeManagerMetrics"}, {"methodBody": ["METHOD_START", "{", "Version   loadedVersion    =    loadVersion (  )  ;", ". LOG . info (  (  \" Loaded   NM   state   version   info    \"     +    loadedVersion )  )  ;", "if    ( loadedVersion . equals ( getCurrentVersion (  )  )  )     {", "return ;", "}", "if    ( loadedVersion . isCompatibleTo ( getCurrentVersion (  )  )  )     {", ". LOG . info (  (  \" Storing   NM   state   version   info    \"     +     ( getCurrentVersion (  )  )  )  )  ;", "storeVersion (  )  ;", "} else    {", "throw   new   IOException (  (  (  (  \" Incompatible   version   for   NM   state :    expecting   NM   state   version    \"     +     ( getCurrentVersion (  )  )  )     +     \"  ,    but   loading   version    \"  )     +    loadedVersion )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "final   String   storeUri    =    conf . get ( NM _ RECOVERY _ DIR )  ;", "if    ( storeUri    =  =    null )     {", "throw   new   IOException (  (  \" No   store   location   directory   configured   in    \"     +     ( YarnConfiguration . NM _ RECOVERY _ DIR )  )  )  ;", "}", "Path   root    =    new   Path ( storeUri ,     . DB _ NAME )  ;", "FileSystem   fs    =    FileSystem . getLocal ( conf )  ;", "fs . mkdirs ( root ,    new   FsPermission (  (  ( short )     (  4  4  8  )  )  )  )  ;", "return   root ;", "}", "METHOD_END"], "methodName": ["createStorageDir"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "String   key    =    NMLeveldbStateStoreService . DB _ SCHEMA _ VERSION _ KEY ;", "byte [  ]    data    =     (  ( VersionPBImpl )     ( state )  )  . getProto (  )  . toByteArray (  )  ;", "try    {", "db . put ( bytes ( key )  ,    data )  ;", "}    catch    ( DBException   e )     {", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["dbStoreVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "return   NMLeveldbStateStoreService . CURRENT _ VERSION _ INFO ;", "}", "METHOD_END"], "methodName": ["getCurrentVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "return    (  ( getResourceTrackerKeyPrefix ( user ,    appId )  )     +     ( NMLeveldbStateStoreService . LOCALIZATION _ COMPLETED _ SUFFIX )  )     +    localPath ;", "}", "METHOD_END"], "methodName": ["getResourceCompletedKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "return    (  ( getResourceTrackerKeyPrefix ( user ,    appId )  )     +     ( NMLeveldbStateStoreService . LOCALIZATION _ STARTED _ SUFFIX )  )     +    localPath ;", "}", "METHOD_END"], "methodName": ["getResourceStartedKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "if    ( user    =  =    null )     {", "return    . LOCALIZATION _ PUBLIC _ KEY _ PREFIX ;", "}", "if    ( appId    =  =    null )     {", "return    (  (  (  . LOCALIZATION _ PRIVATE _ KEY _ PREFIX )     +    user )     +     \"  /  \"  )     +     (  . LOCALIZATION _ FILECACHE _ SUFFIX )  ;", "}", "return    (  (  (  (  (  . LOCALIZATION _ PRIVATE _ KEY _ PREFIX )     +    user )     +     \"  /  \"  )     +     (  . LOCALIZATION _ APPCACHE _ SUFFIX )  )     +    appId )     +     \"  /  \"  ;", "}", "METHOD_END"], "methodName": ["getResourceTrackerKeyPrefix"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "List < LocalizedResourceProto >    rsrcs    =    new   ArrayList < LocalizedResourceProto >  (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < byte [  ]  ,    byte [  ]  >    entry    =    iter . peekNext (  )  ;", "String   key    =    asString ( entry . getKey (  )  )  ;", "if    (  !  ( key . startsWith ( keyPrefix )  )  )     {", "break ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Loading   completed   resource   from    \"     +    key )  )  ;", "}", "rsrcs . add ( LocalizedResourceProto . parseFrom ( entry . getValue (  )  )  )  ;", "iter . next (  )  ;", "}", "return   rsrcs ;", "}", "METHOD_END"], "methodName": ["loadCompletedResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredContainerState   rcs    =    new   NMStateStoreService . RecoveredContainerState (  )  ;", "rcs . status    =    NMStateStoreService . RecoveredContainerStatus . REQUESTED ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < byte [  ]  ,    byte [  ]  >    entry    =    iter . peekNext (  )  ;", "String   key    =    asString ( entry . getKey (  )  )  ;", "if    (  !  ( key . startsWith ( keyPrefix )  )  )     {", "break ;", "}", "iter . next (  )  ;", "String   suffix    =    key . substring (  (  ( keyPrefix . length (  )  )     -     1  )  )  ;", "if    ( suffix . equals (  . CONTAINER _ REQUEST _ KEY _ SUFFIX )  )     {", "rcs . startRequest    =    new   StartContainerRequestPBImpl ( StartContainerRequestProto . parseFrom ( entry . getValue (  )  )  )  ;", "} else", "if    ( suffix . equals (  . CONTAINER _ DIAGS _ KEY _ SUFFIX )  )     {", "rcs . diagnostics    =    asString ( entry . getValue (  )  )  ;", "} else", "if    ( suffix . equals (  . CONTAINER _ LAUNCHED _ KEY _ SUFFIX )  )     {", "if    (  ( rcs . status )     =  =     ( NMStateStoreService . RecoveredContainerStatus . REQUESTED )  )     {", "rcs . status    =    NMStateStoreService . RecoveredContainerStatus . LAUNCHED ;", "}", "} else", "if    ( suffix . equals (  . CONTAINER _ KILLED _ KEY _ SUFFIX )  )     {", "rcs . killed    =    true ;", "} else", "if    ( suffix . equals (  . CONTAINER _ EXIT _ CODE _ KEY _ SUFFIX )  )     {", "rcs . status    =    NMStateStoreService . RecoveredContainerStatus . COMPLETED ;", "rcs . exitCode    =    Integer . parseInt ( asString ( entry . getValue (  )  )  )  ;", "} else    {", "throw   new   IOException (  (  \" Unexpected   container   state   key :     \"     +    key )  )  ;", "}", "}", "return   rcs ;", "}", "METHOD_END"], "methodName": ["loadContainerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId ;", "Long   expTime ;", "try    {", "containerId    =    ConverterUtils . toContainerId ( containerIdStr )  ;", "expTime    =    Long . parseLong ( asString ( value )  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "throw   new   IOException (  (  \" Bad   container   token   s   for    \"     +    key )  ,    e )  ;", "}", "s . activeTokens . put ( containerId ,    expTime )  ;", "}", "METHOD_END"], "methodName": ["loadContainerToken"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "final   String   completedPrefix    =    keyPrefix    +     ( NMLeveldbStateStoreService . LOCALIZATION _ COMPLETED _ SUFFIX )  ;", "final   String   startedPrefix    =    keyPrefix    +     ( NMLeveldbStateStoreService . LOCALIZATION _ STARTED _ SUFFIX )  ;", "NMStateStoreService . LocalResourceTrackerState   state    =    new   NMStateStoreService . LocalResourceTrackerState (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < byte [  ]  ,    byte [  ]  >    entry    =    iter . peekNext (  )  ;", "String   key    =    asString ( entry . getKey (  )  )  ;", "if    (  !  ( key . startsWith ( keyPrefix )  )  )     {", "break ;", "}", "if    ( key . startsWith ( completedPrefix )  )     {", "state . localizedResources    =    loadCompletedResources ( iter ,    completedPrefix )  ;", "} else", "if    ( key . startsWith ( startedPrefix )  )     {", "state . inProgressResources    =    loadStartedResources ( iter ,    startedPrefix )  ;", "} else    {", "throw   new   IOException (  (  \" Unexpected   key   in   resource   tracker   state :     \"     +    key )  )  ;", "}", "}", "return   state ;", "}", "METHOD_END"], "methodName": ["loadResourceTrackerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "Map < LocalResourceProto ,    Path >    rsrcs    =    new   HashMap < LocalResourceProto ,    Path >  (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < byte [  ]  ,    byte [  ]  >    entry    =    iter . peekNext (  )  ;", "String   key    =    asString ( entry . getKey (  )  )  ;", "if    (  !  ( key . startsWith ( keyPrefix )  )  )     {", "break ;", "}", "Path   localPath    =    new   Path ( key . substring ( keyPrefix . length (  )  )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Loading   in - progress   resource   at    \"     +    localPath )  )  ;", "}", "rsrcs . put ( LocalResourceProto . parseFrom ( entry . getValue (  )  )  ,    localPath )  ;", "iter . next (  )  ;", "}", "return   rsrcs ;", "}", "METHOD_END"], "methodName": ["loadStartedResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredUserResources   userResources    =    new   NMStateStoreService . RecoveredUserResources (  )  ;", "while    ( iter . hasNext (  )  )     {", "Map . Entry < byte [  ]  ,    byte [  ]  >    entry    =    iter . peekNext (  )  ;", "String   key    =    asString ( entry . getKey (  )  )  ;", "if    (  !  ( key . startsWith ( keyPrefix )  )  )     {", "break ;", "}", "if    ( key . startsWith (  . LOCALIZATION _ FILECACHE _ SUFFIX ,    keyPrefix . length (  )  )  )     {", "userResources . privateTrackerState    =    loadResourceTrackerState ( iter ,     ( keyPrefix    +     (  . LOCALIZATION _ FILECACHE _ SUFFIX )  )  )  ;", "} else", "if    ( key . startsWith (  . LOCALIZATION _ APPCACHE _ SUFFIX ,    keyPrefix . length (  )  )  )     {", "int   appIdStartPos    =     ( keyPrefix . length (  )  )     +     (  . LOCALIZATION _ APPCACHE _ SUFFIX . length (  )  )  ;", "int   appIdEndPos    =    key . indexOf (  '  /  '  ,    appIdStartPos )  ;", "if    ( appIdEndPos    <     0  )     {", "throw   new   IOException (  (  \" Unable   to   determine   appID   in   resource   key :     \"     +    key )  )  ;", "}", "ApplicationId   appId    =    toApplicationId ( key . substring ( appIdStartPos ,    appIdEndPos )  )  ;", "userResources . appTrackerStates . put ( appId ,    loadResourceTrackerState ( iter ,    key . substring (  0  ,     ( appIdEndPos    +     1  )  )  )  )  ;", "} else    {", "throw   new   IOException (  (  \" Unexpected   user   resource   key    \"     +    key )  )  ;", "}", "}", "return   userResources ;", "}", "METHOD_END"], "methodName": ["loadUserLocalizedResources"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    data    =    db . get ( bytes ( NMLeveldbStateStoreService . DB _ SCHEMA _ VERSION _ KEY )  )  ;", "if    (  ( data    =  =    null )     |  |     (  ( data . length )     =  =     0  )  )     {", "return   Version . newInstance (  1  ,     0  )  ;", "}", "Version   version    =    new   VersionPBImpl ( VersionProto . parseFrom ( data )  )  ;", "return   version ;", "}", "METHOD_END"], "methodName": ["loadVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "return   new   MasterKeyPBImpl ( MasterKeyProto . parseFrom ( keyData )  )  ;", "}", "METHOD_END"], "methodName": ["parseMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "MasterKeyPBImpl   pb    =     (  ( MasterKeyPBImpl )     ( key )  )  ;", "try    {", "db . put ( bytes ( dbKey )  ,    pb . getProto (  )  . toByteArray (  )  )  ;", "}    catch    ( DBException   e )     {", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["storeMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "dbStoreVersion ( NMLeveldbStateStoreService . CURRENT _ VERSION _ INFO )  ;", "}", "METHOD_END"], "methodName": ["storeVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "dbStoreVersion ( state )  ;", "}", "METHOD_END"], "methodName": ["storeVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredContainerState   rcs    =    containerStates . get ( containerId )  ;", "if    ( rcs    =  =    null )     {", "throw   new   IOException (  (  \" No   start   request   for    \"     +    containerId )  )  ;", "}", "return   rcs ;", "}", "METHOD_END"], "methodName": ["getRecoveredContainerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMMemoryStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMMemoryStateStoreService . TrackerState   ts    =    trackerStates . get ( key )  ;", "if    ( ts    =  =    null )     {", "ts    =    new   NMMemoryStateStoreService . TrackerState (  )  ;", "trackerStates . put ( key ,    ts )  ;", "}", "return   ts ;", "}", "METHOD_END"], "methodName": ["getTrackerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMMemoryStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . LocalResourceTrackerState   result    =    new   NMStateStoreService . LocalResourceTrackerState (  )  ;", "result . localizedResources . addAll ( ts . localizedResources . values (  )  )  ;", "for    ( Map . Entry < Path ,    LocalResourceProto >    entry    :    ts . inProgressMap . entrySet (  )  )     {", "result . inProgressResources . put ( entry . getValue (  )  ,    entry . getKey (  )  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["loadTrackerState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMMemoryStateStoreService"}, {"methodBody": ["METHOD_START", "{", "return   true ;", "}", "METHOD_END"], "methodName": ["canRecover"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( stateStore )     !  =    null )     {", "stateStore . close (  )  ;", "}", "FileUtil . fullyDelete (  . TMP _ DIR )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( stateStore )     !  =    null )     {", "stateStore . close (  )  ;", "}", "stateStore    =    new    (  )  ;", "stateStore . init ( conf )  ;", "stateStore . start (  )  ;", "}", "METHOD_END"], "methodName": ["restartStateStore"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestNMLeveldbStateStoreService . TMP _ DIR )  ;", "conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "conf . set ( NM _ RECOVERY _ DIR ,    TestNMLeveldbStateStoreService . TMP _ DIR . toString (  )  )  ;", "restartStateStore (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredApplicationsState   state    =    stateStore . loadApplicationsState (  )  ;", "assertTrue ( state . getApplications (  )  . isEmpty (  )  )  ;", "assertTrue ( state . getFinishedApplications (  )  . isEmpty (  )  )  ;", "final   ApplicationId   appId 1     =    ApplicationId . newInstance (  1  2  3  4  ,     1  )  ;", "ContainerManagerApplicationProto . Builder   builder    =    ContainerManagerApplicationProto . newBuilder (  )  ;", "builder . setId (  (  ( ApplicationIdPBImpl )     ( appId 1  )  )  . getProto (  )  )  ;", "builder . setUser (  \" user 1  \"  )  ;", "ContainerManagerApplicationProto   appProto 1     =    builder . build (  )  ;", "stateStore . storeApplication ( appId 1  ,    appProto 1  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadApplicationsState (  )  ;", "assertEquals (  1  ,    state . getApplications (  )  . size (  )  )  ;", "assertEquals ( appProto 1  ,    state . getApplications (  )  . get (  0  )  )  ;", "assertTrue ( state . getFinishedApplications (  )  . isEmpty (  )  )  ;", "stateStore . storeFinishedApplication ( appId 1  )  ;", "final   ApplicationId   appId 2     =    ApplicationId . newInstance (  1  2  3  4  ,     2  )  ;", "builder    =    ContainerManagerApplicationProto . newBuilder (  )  ;", "builder . setId (  (  ( ApplicationIdPBImpl )     ( appId 2  )  )  . getProto (  )  )  ;", "builder . setUser (  \" user 2  \"  )  ;", "ContainerManagerApplicationProto   appProto 2     =    builder . build (  )  ;", "stateStore . storeApplication ( appId 2  ,    appProto 2  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadApplicationsState (  )  ;", "assertEquals (  2  ,    state . getApplications (  )  . size (  )  )  ;", "assertTrue ( state . getApplications (  )  . contains ( appProto 1  )  )  ;", "assertTrue ( state . getApplications (  )  . contains ( appProto 2  )  )  ;", "assertEquals (  1  ,    state . getFinishedApplications (  )  . size (  )  )  ;", "assertEquals ( appId 1  ,    state . getFinishedApplications (  )  . get (  0  )  )  ;", "stateStore . storeFinishedApplication ( appId 2  )  ;", "stateStore . removeApplication ( appId 2  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadApplicationsState (  )  ;", "assertEquals (  1  ,    state . getApplications (  )  . size (  )  )  ;", "assertEquals ( appProto 1  ,    state . getApplications (  )  . get (  0  )  )  ;", "assertEquals (  1  ,    state . getFinishedApplications (  )  . size (  )  )  ;", "assertEquals ( appId 1  ,    state . getFinishedApplications (  )  . get (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationStorage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "Version   defaultVersion    =    stateStore . getCurrentVersion (  )  ;", "Assert . assertEquals ( defaultVersion ,    stateStore . loadVersion (  )  )  ;", "Version   compatibleVersion    =    Version . newInstance ( defaultVersion . getMajorVersion (  )  ,     (  ( defaultVersion . getMinorVersion (  )  )     +     2  )  )  ;", "stateStore . storeVersion ( compatibleVersion )  ;", "Assert . assertEquals ( compatibleVersion ,    stateStore . loadVersion (  )  )  ;", "restart (  )  ;", "Assert . assertEquals ( defaultVersion ,    stateStore . loadVersion (  )  )  ;", "Version   incompatibleVersion    =    Version . newInstance (  (  ( defaultVersion . getMajorVersion (  )  )     +     1  )  ,    defaultVersion . getMinorVersion (  )  )  ;", "stateStore . storeVersion ( incompatibleVersion )  ;", "try    {", "restart (  )  ;", "Assert . fail (  \" Incompatible   version ,    should   expect   fail   here .  \"  )  ;", "}    catch    ( ServiceStateException   e )     {", "Assert . assertTrue (  \" Exception   message   mismatch \"  ,    e . getMessage (  )  . contains (  \" Incompatible   version   for   NM   state :  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCheckVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "List < NMStateStoreService . RecoveredContainerState >    recoveredContainers    =    stateStore . loadContainersState (  )  ;", "assertTrue ( recoveredContainers . isEmpty (  )  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  2  3  4  ,     3  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,     4  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     5  )  ;", "LocalResource   lrsrc    =    LocalResource . newInstance ( URL . newInstance (  \" hdfs \"  ,     \" somehost \"  ,     1  2  3  4  5  ,     \"  / some / path / to / rsrc \"  )  ,    FILE ,    APPLICATION ,     1  2  3 L ,     1  2  3  4  5  6  7  8  9  0 L )  ;", "Map < String ,    LocalResource >    localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "localResources . put (  \" rsrc \"  ,    lrsrc )  ;", "Map < String ,    String >    env    =    new   HashMap < String ,    String >  (  )  ;", "env . put (  \" somevar \"  ,     \" someval \"  )  ;", "List < String >    containerCmds    =    new   ArrayList < String >  (  )  ;", "containerCmds . add (  \" somecmd \"  )  ;", "containerCmds . add (  \" somearg \"  )  ;", "Map < String ,    ByteBuffer >    serviceData    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "serviceData . put (  \" someservice \"  ,    ByteBuffer . wrap ( new   byte [  ]  {     1  ,     2  ,     3     }  )  )  ;", "ByteBuffer   containerTokens    =    ByteBuffer . wrap ( new   byte [  ]  {     7  ,     8  ,     9  ,     1  0     }  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "acls . put ( VIEW _ APP ,     \" viewuser \"  )  ;", "acls . put ( MODIFY _ APP ,     \" moduser \"  )  ;", "ContainerLaunchContext   clc    =    ContainerLaunchContext . newInstance ( localResources ,    env ,    containerCmds ,    serviceData ,    containerTokens ,    acls )  ;", "Resource   containerRsrc    =    Resource . newInstance (  1  3  5  7  ,     3  )  ;", "ContainerTokenIdentifier   containerTokenId    =    new   ContainerTokenIdentifier ( containerId ,     \" host \"  ,     \" user \"  ,    containerRsrc ,     9  8  7  6  5  4  3  2  1  0 L ,     4  2  ,     2  4  6  8  ,    Priority . newInstance (  7  )  ,     1  3  5  7  9  )  ;", "Token   containerToken    =    Token . newInstance ( containerTokenId . getBytes (  )  ,    KIND . toString (  )  ,     \" password \"  . getBytes (  )  ,     \" tokenservice \"  )  ;", "StartContainerRequest   containerReq    =    StartContainerRequest . newInstance ( clc ,    containerToken )  ;", "stateStore . storeContainer ( containerId ,    containerReq )  ;", "restartStateStore (  )  ;", "recoveredContainers    =    stateStore . loadContainersState (  )  ;", "assertEquals (  1  ,    recoveredContainers . size (  )  )  ;", "NMStateStoreService . RecoveredContainerState   rcs    =    recoveredContainers . get (  0  )  ;", "assertEquals ( NMStateStoreService . RecoveredContainerStatus . REQUESTED ,    rcs . getStatus (  )  )  ;", "assertEquals ( INVALID ,    rcs . getExitCode (  )  )  ;", "assertEquals ( false ,    rcs . getKilled (  )  )  ;", "assertEquals ( containerReq ,    rcs . getStartRequest (  )  )  ;", "assertTrue ( rcs . getDiagnostics (  )  . isEmpty (  )  )  ;", "StringBuilder   diags    =    new   StringBuilder (  )  ;", "stateStore . storeContainerLaunched ( containerId )  ;", "diags . append (  \" some   diags   for   container \"  )  ;", "stateStore . storeContainerDiagnostics ( containerId ,    diags )  ;", "restartStateStore (  )  ;", "recoveredContainers    =    stateStore . loadContainersState (  )  ;", "assertEquals (  1  ,    recoveredContainers . size (  )  )  ;", "rcs    =    recoveredContainers . get (  0  )  ;", "assertEquals ( NMStateStoreService . RecoveredContainerStatus . LAUNCHED ,    rcs . getStatus (  )  )  ;", "assertEquals ( INVALID ,    rcs . getExitCode (  )  )  ;", "assertEquals ( false ,    rcs . getKilled (  )  )  ;", "assertEquals ( containerReq ,    rcs . getStartRequest (  )  )  ;", "assertEquals ( diags . toString (  )  ,    rcs . getDiagnostics (  )  )  ;", "diags . append (  \" some   more   diags   for   container \"  )  ;", "stateStore . storeContainerDiagnostics ( containerId ,    diags )  ;", "stateStore . storeContainerKilled ( containerId )  ;", "restartStateStore (  )  ;", "recoveredContainers    =    stateStore . loadContainersState (  )  ;", "assertEquals (  1  ,    recoveredContainers . size (  )  )  ;", "rcs    =    recoveredContainers . get (  0  )  ;", "assertEquals ( NMStateStoreService . RecoveredContainerStatus . LAUNCHED ,    rcs . getStatus (  )  )  ;", "assertEquals ( INVALID ,    rcs . getExitCode (  )  )  ;", "assertTrue ( rcs . getKilled (  )  )  ;", "assertEquals ( containerReq ,    rcs . getStartRequest (  )  )  ;", "assertEquals ( diags . toString (  )  ,    rcs . getDiagnostics (  )  )  ;", "diags . append (  \" some   final   diags \"  )  ;", "stateStore . storeContainerDiagnostics ( containerId ,    diags )  ;", "stateStore . storeContainerCompleted ( containerId ,     2  1  )  ;", "restartStateStore (  )  ;", "recoveredContainers    =    stateStore . loadContainersState (  )  ;", "assertEquals (  1  ,    recoveredContainers . size (  )  )  ;", "rcs    =    recoveredContainers . get (  0  )  ;", "assertEquals ( NMStateStoreService . RecoveredContainerStatus . COMPLETED ,    rcs . getStatus (  )  )  ;", "assertEquals (  2  1  ,    rcs . getExitCode (  )  )  ;", "assertTrue ( rcs . getKilled (  )  )  ;", "assertEquals ( containerReq ,    rcs . getStartRequest (  )  )  ;", "assertEquals ( diags . toString (  )  ,    rcs . getDiagnostics (  )  )  ;", "stateStore . removeContainer ( containerId )  ;", "restartStateStore (  )  ;", "recoveredContainers    =    stateStore . loadContainersState (  )  ;", "assertTrue ( recoveredContainers . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerStorage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredContainerTokensState   state    =    stateStore . loadContainerTokensState (  )  ;", "assertNull ( state . getCurrentMasterKey (  )  )  ;", "assertNull ( state . getPreviousMasterKey (  )  )  ;", "assertTrue ( state . getActiveTokens (  )  . isEmpty (  )  )  ;", ". ContainerTokenKeyGeneratorForTest   keygen    =    new    . ContainerTokenKeyGeneratorForTest ( new   YarnConfiguration (  )  )  ;", "MasterKey   currentKey    =    keygen . generateKey (  )  ;", "stateStore . storeContainerTokenCurrentMasterKey ( currentKey )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadContainerTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertNull ( state . getPreviousMasterKey (  )  )  ;", "assertTrue ( state . getActiveTokens (  )  . isEmpty (  )  )  ;", "MasterKey   prevKey    =    keygen . generateKey (  )  ;", "stateStore . storeContainerTokenPreviousMasterKey ( prevKey )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadContainerTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertEquals ( prevKey ,    state . getPreviousMasterKey (  )  )  ;", "assertTrue ( state . getActiveTokens (  )  . isEmpty (  )  )  ;", "ContainerId   cid 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "Long   expTime 1     =     1  2  3  4  5  6  7  8  9  0 L ;", "ContainerId   cid 2     =    BuilderUtils . newContainerId (  2  ,     2  ,     2  ,     2  )  ;", "Long   expTime 2     =     9  8  7  6  5  4  3  2  1  0 L ;", "stateStore . storeContainerToken ( cid 1  ,    expTime 1  )  ;", "stateStore . storeContainerToken ( cid 2  ,    expTime 2  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadContainerTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertEquals ( prevKey ,    state . getPreviousMasterKey (  )  )  ;", "Map < ContainerId ,    Long >    loadedActiveTokens    =    state . getActiveTokens (  )  ;", "assertEquals (  2  ,    loadedActiveTokens . size (  )  )  ;", "assertEquals ( expTime 1  ,    loadedActiveTokens . get ( cid 1  )  )  ;", "assertEquals ( expTime 2  ,    loadedActiveTokens . get ( cid 2  )  )  ;", "ContainerId   cid 3     =    BuilderUtils . newContainerId (  3  ,     3  ,     3  ,     3  )  ;", "Long   expTime 3     =     1  3  5  7  9  8  6  4  2 L ;", "stateStore . storeContainerToken ( cid 3  ,    expTime 3  )  ;", "stateStore . removeContainerToken ( cid 1  )  ;", "expTime 2     +  =     2  4  6  8  9  7  5  3  1 L ;", "stateStore . storeContainerToken ( cid 2  ,    expTime 2  )  ;", "prevKey    =    currentKey ;", "stateStore . storeContainerTokenPreviousMasterKey ( prevKey )  ;", "currentKey    =    keygen . generateKey (  )  ;", "stateStore . storeContainerTokenCurrentMasterKey ( currentKey )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadContainerTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertEquals ( prevKey ,    state . getPreviousMasterKey (  )  )  ;", "loadedActiveTokens    =    state . getActiveTokens (  )  ;", "assertEquals (  2  ,    loadedActiveTokens . size (  )  )  ;", "assertNull ( loadedActiveTokens . get ( cid 1  )  )  ;", "assertEquals ( expTime 2  ,    loadedActiveTokens . get ( cid 2  )  )  ;", "assertEquals ( expTime 3  ,    loadedActiveTokens . get ( cid 3  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerTokenStorage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredDeletionServiceState   state    =    stateStore . loadDeletionServiceState (  )  ;", "assertTrue ( state . getTasks (  )  . isEmpty (  )  )  ;", "DeletionServiceDeleteTaskProto   proto    =    DeletionServiceDeleteTaskProto . newBuilder (  )  . setId (  7  )  . setUser (  \" someuser \"  )  . setSubdir (  \" some / subdir \"  )  . addBasedirs (  \" some / dir / path \"  )  . addBasedirs (  \" some / other / dir / path \"  )  . setDeletionTime (  1  2  3  4  5  6 L )  . addSuccessorIds (  8  )  . addSuccessorIds (  9  )  . build (  )  ;", "stateStore . storeDeletionTask ( proto . getId (  )  ,    proto )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadDeletionServiceState (  )  ;", "assertEquals (  1  ,    state . getTasks (  )  . size (  )  )  ;", "assertEquals ( proto ,    state . getTasks (  )  . get (  0  )  )  ;", "DeletionServiceDeleteTaskProto   proto 2     =    DeletionServiceDeleteTaskProto . newBuilder (  )  . setId (  8  )  . setUser (  \" user 2  \"  )  . setSubdir (  \" subdir 2  \"  )  . setDeletionTime (  7  8  9 L )  . build (  )  ;", "stateStore . storeDeletionTask ( proto 2  . getId (  )  ,    proto 2  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadDeletionServiceState (  )  ;", "assertEquals (  2  ,    state . getTasks (  )  . size (  )  )  ;", "assertTrue ( state . getTasks (  )  . contains ( proto )  )  ;", "assertTrue ( state . getTasks (  )  . contains ( proto 2  )  )  ;", "stateStore . removeDeletionTask ( proto 2  . getId (  )  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadDeletionServiceState (  )  ;", "assertEquals (  1  ,    state . getTasks (  )  . size (  )  )  ;", "assertEquals ( proto ,    state . getTasks (  )  . get (  0  )  )  ;", "stateStore . removeDeletionTask ( proto . getId (  )  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadDeletionServiceState (  )  ;", "assertTrue ( state . getTasks (  )  . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDeletionTaskStorage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "assertTrue ( stateStore . canRecover (  )  )  ;", "verifyEmptyState (  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" somebody \"  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "Path   appRsrcPath    =    new   Path (  \" hdfs :  /  / some / app / resource \"  )  ;", "LocalResourcePBImpl   rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( appRsrcPath )  ,    ARCHIVE ,    APPLICATION ,     1  2  3 L ,     4  5  6 L )  )  )  ;", "LocalResourceProto   appRsrcProto    =    rsrcPb . getProto (  )  ;", "Path   appRsrcLocalPath    =    new   Path (  \"  / some / local / dir / for / apprsrc \"  )  ;", "stateStore . startResourceLocalization ( user ,    appId ,    appRsrcProto ,    appRsrcLocalPath )  ;", "LocalizedResourceProto   appLocalizedProto    =    LocalizedResourceProto . newBuilder (  )  . setResource ( appRsrcProto )  . setLocalPath ( appRsrcLocalPath . toString (  )  )  . setSize (  1  2  3  4  5  6  7 L )  . build (  )  ;", "stateStore . finishResourceLocalization ( user ,    appId ,    appLocalizedProto )  ;", "restartStateStore (  )  ;", "NM . RecoveredLocalizationState   state    =    stateStore . loadLocalizationState (  )  ;", "NM . LocalResourceTrackerState   pubts    =    state . getPublicTrackerState (  )  ;", "assertTrue ( pubts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertTrue ( pubts . getInProgressResources (  )  . isEmpty (  )  )  ;", "Map < String ,    NM . RecoveredUserResources >    userResources    =    state . getUserResources (  )  ;", "assertEquals (  1  ,    userResources . size (  )  )  ;", "NM . RecoveredUserResources   rur    =    userResources . get ( user )  ;", "NM . LocalResourceTrackerState   privts    =    rur . getPrivateTrackerState (  )  ;", "assertNotNull ( privts )  ;", "assertTrue ( privts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertTrue ( privts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    rur . getAppTrackerStates (  )  . size (  )  )  ;", "NM . LocalResourceTrackerState   appts    =    rur . getAppTrackerStates (  )  . get ( appId )  ;", "assertNotNull ( appts )  ;", "assertTrue ( appts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    appts . getLocalizedResources (  )  . size (  )  )  ;", "assertEquals ( appLocalizedProto ,    appts . getLocalizedResources (  )  . iterator (  )  . next (  )  )  ;", "Path   pubRsrcPath 1     =    new   Path (  \" hdfs :  /  / some / public / resource 1  \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( pubRsrcPath 1  )  ,    FILE ,    PUBLIC ,     7  8  9 L ,     1  3  5 L )  )  )  ;", "LocalResourceProto   pubRsrcProto 1     =    rsrcPb . getProto (  )  ;", "Path   pubRsrcLocalPath 1     =    new   Path (  \"  / some / local / dir / for / pubrsrc 1  \"  )  ;", "stateStore . startResourceLocalization ( null ,    null ,    pubRsrcProto 1  ,    pubRsrcLocalPath 1  )  ;", "Path   pubRsrcPath 2     =    new   Path (  \" hdfs :  /  / some / public / resource 2  \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( pubRsrcPath 2  )  ,    FILE ,    PUBLIC ,     7  8  9 L ,     1  3  5 L )  )  )  ;", "LocalResourceProto   pubRsrcProto 2     =    rsrcPb . getProto (  )  ;", "Path   pubRsrcLocalPath 2     =    new   Path (  \"  / some / local / dir / for / pubrsrc 2  \"  )  ;", "stateStore . startResourceLocalization ( null ,    null ,    pubRsrcProto 2  ,    pubRsrcLocalPath 2  )  ;", "Path   privRsrcPath    =    new   Path (  \" hdfs :  /  / some / private / resource \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( privRsrcPath )  ,    PATTERN ,    PRIVATE ,     7  8  9 L ,     6  8  0 L ,     \"  * pattern *  \"  )  )  )  ;", "LocalResourceProto   privRsrcProto    =    rsrcPb . getProto (  )  ;", "Path   privRsrcLocalPath    =    new   Path (  \"  / some / local / dir / for / privrsrc \"  )  ;", "stateStore . startResourceLocalization ( user ,    null ,    privRsrcProto ,    privRsrcLocalPath )  ;", "LocalizedResourceProto   pubLocalizedProto 1     =    LocalizedResourceProto . newBuilder (  )  . setResource ( pubRsrcProto 1  )  . setLocalPath ( pubRsrcLocalPath 1  . toString (  )  )  . setSize ( pubRsrcProto 1  . getSize (  )  )  . build (  )  ;", "stateStore . finishResourceLocalization ( null ,    null ,    pubLocalizedProto 1  )  ;", "LocalizedResourceProto   privLocalizedProto    =    LocalizedResourceProto . newBuilder (  )  . setResource ( privRsrcProto )  . setLocalPath ( privRsrcLocalPath . toString (  )  )  . setSize ( privRsrcProto . getSize (  )  )  . build (  )  ;", "stateStore . finishResourceLocalization ( user ,    null ,    privLocalizedProto )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadLocalizationState (  )  ;", "pubts    =    state . getPublicTrackerState (  )  ;", "assertEquals (  1  ,    pubts . getLocalizedResources (  )  . size (  )  )  ;", "assertEquals ( pubLocalizedProto 1  ,    pubts . getLocalizedResources (  )  . iterator (  )  . next (  )  )  ;", "assertEquals (  1  ,    pubts . getInProgressResources (  )  . size (  )  )  ;", "assertEquals ( pubRsrcLocalPath 2  ,    pubts . getInProgressResources (  )  . get ( pubRsrcProto 2  )  )  ;", "userResources    =    state . getUserResources (  )  ;", "assertEquals (  1  ,    userResources . size (  )  )  ;", "rur    =    userResources . get ( user )  ;", "privts    =    rur . getPrivateTrackerState (  )  ;", "assertNotNull ( privts )  ;", "assertEquals (  1  ,    privts . getLocalizedResources (  )  . size (  )  )  ;", "assertEquals ( privLocalizedProto ,    privts . getLocalizedResources (  )  . iterator (  )  . next (  )  )  ;", "assertTrue ( privts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    rur . getAppTrackerStates (  )  . size (  )  )  ;", "appts    =    rur . getAppTrackerStates (  )  . get ( appId )  ;", "assertNotNull ( appts )  ;", "assertTrue ( appts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    appts . getLocalizedResources (  )  . size (  )  )  ;", "assertEquals ( appLocalizedProto ,    appts . getLocalizedResources (  )  . iterator (  )  . next (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFinishResourceLocalization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredNMTokensState   state    =    stateStore . loadNMTokensState (  )  ;", "assertNull ( state . getCurrentMasterKey (  )  )  ;", "assertNull ( state . getPreviousMasterKey (  )  )  ;", "assertTrue ( state . getApplicationMasterKeys (  )  . isEmpty (  )  )  ;", ". NMTokenSecretManagerForTest   secretMgr    =    new    . NMTokenSecretManagerForTest (  )  ;", "MasterKey   currentKey    =    secretMgr . generateKey (  )  ;", "stateStore . storeNMTokenCurrentMasterKey ( currentKey )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadNMTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertNull ( state . getPreviousMasterKey (  )  )  ;", "assertTrue ( state . getApplicationMasterKeys (  )  . isEmpty (  )  )  ;", "MasterKey   prevKey    =    secretMgr . generateKey (  )  ;", "stateStore . storeNMTokenPreviousMasterKey ( prevKey )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadNMTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertEquals ( prevKey ,    state . getPreviousMasterKey (  )  )  ;", "assertTrue ( state . getApplicationMasterKeys (  )  . isEmpty (  )  )  ;", "ApplicationAttemptId   attempt 1     =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  ,     1  )  ,     1  )  ;", "MasterKey   attemptKey 1     =    secretMgr . generateKey (  )  ;", "stateStore . storeNMTokenApplicationMasterKey ( attempt 1  ,    attemptKey 1  )  ;", "ApplicationAttemptId   attempt 2     =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  2  ,     3  )  ,     4  )  ;", "MasterKey   attemptKey 2     =    secretMgr . generateKey (  )  ;", "stateStore . storeNMTokenApplicationMasterKey ( attempt 2  ,    attemptKey 2  )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadNMTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertEquals ( prevKey ,    state . getPreviousMasterKey (  )  )  ;", "Map < ApplicationAttemptId ,    MasterKey >    loadedAppKeys    =    state . getApplicationMasterKeys (  )  ;", "assertEquals (  2  ,    loadedAppKeys . size (  )  )  ;", "assertEquals ( attemptKey 1  ,    loadedAppKeys . get ( attempt 1  )  )  ;", "assertEquals ( attemptKey 2  ,    loadedAppKeys . get ( attempt 2  )  )  ;", "ApplicationAttemptId   attempt 3     =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  5  ,     6  )  ,     7  )  ;", "MasterKey   attemptKey 3     =    secretMgr . generateKey (  )  ;", "stateStore . storeNMTokenApplicationMasterKey ( attempt 3  ,    attemptKey 3  )  ;", "stateStore . removeNMTokenApplicationMasterKey ( attempt 1  )  ;", "attemptKey 2     =    prevKey ;", "stateStore . storeNMTokenApplicationMasterKey ( attempt 2  ,    attemptKey 2  )  ;", "prevKey    =    currentKey ;", "stateStore . storeNMTokenPreviousMasterKey ( prevKey )  ;", "currentKey    =    secretMgr . generateKey (  )  ;", "stateStore . storeNMTokenCurrentMasterKey ( currentKey )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadNMTokensState (  )  ;", "assertEquals ( currentKey ,    state . getCurrentMasterKey (  )  )  ;", "assertEquals ( prevKey ,    state . getPreviousMasterKey (  )  )  ;", "loadedAppKeys    =    state . getApplicationMasterKeys (  )  ;", "assertEquals (  2  ,    loadedAppKeys . size (  )  )  ;", "assertNull ( loadedAppKeys . get ( attempt 1  )  )  ;", "assertEquals ( attemptKey 2  ,    loadedAppKeys . get ( attempt 2  )  )  ;", "assertEquals ( attemptKey 3  ,    loadedAppKeys . get ( attempt 3  )  )  ;", "}", "METHOD_END"], "methodName": ["testNMTokenStorage"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" somebody \"  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "Path   appRsrcPath    =    new   Path (  \" hdfs :  /  / some / app / resource \"  )  ;", "LocalResourcePBImpl   rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( appRsrcPath )  ,    ARCHIVE ,    APPLICATION ,     1  2  3 L ,     4  5  6 L )  )  )  ;", "LocalResourceProto   appRsrcProto    =    rsrcPb . getProto (  )  ;", "Path   appRsrcLocalPath    =    new   Path (  \"  / some / local / dir / for / apprsrc \"  )  ;", "stateStore . startResourceLocalization ( user ,    appId ,    appRsrcProto ,    appRsrcLocalPath )  ;", "LocalizedResourceProto   appLocalizedProto    =    LocalizedResourceProto . newBuilder (  )  . setResource ( appRsrcProto )  . setLocalPath ( appRsrcLocalPath . toString (  )  )  . setSize (  1  2  3  4  5  6  7 L )  . build (  )  ;", "stateStore . finishResourceLocalization ( user ,    appId ,    appLocalizedProto )  ;", "stateStore . removeLocalizedResource ( user ,    appId ,    appRsrcLocalPath )  ;", "restartStateStore (  )  ;", "verifyEmptyState (  )  ;", "stateStore . startResourceLocalization ( user ,    appId ,    appRsrcProto ,    appRsrcLocalPath )  ;", "stateStore . removeLocalizedResource ( user ,    appId ,    appRsrcLocalPath )  ;", "restartStateStore (  )  ;", "verifyEmptyState (  )  ;", "Path   pubRsrcPath 1     =    new   Path (  \" hdfs :  /  / some / public / resource 1  \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( pubRsrcPath 1  )  ,    FILE ,    PUBLIC ,     7  8  9 L ,     1  3  5 L )  )  )  ;", "LocalResourceProto   pubRsrcProto 1     =    rsrcPb . getProto (  )  ;", "Path   pubRsrcLocalPath 1     =    new   Path (  \"  / some / local / dir / for / pubrsrc 1  \"  )  ;", "stateStore . startResourceLocalization ( null ,    null ,    pubRsrcProto 1  ,    pubRsrcLocalPath 1  )  ;", "LocalizedResourceProto   pubLocalizedProto 1     =    LocalizedResourceProto . newBuilder (  )  . setResource ( pubRsrcProto 1  )  . setLocalPath ( pubRsrcLocalPath 1  . toString (  )  )  . setSize (  7  8  9 L )  . build (  )  ;", "stateStore . finishResourceLocalization ( null ,    null ,    pubLocalizedProto 1  )  ;", "Path   pubRsrcPath 2     =    new   Path (  \" hdfs :  /  / some / public / resource 2  \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( pubRsrcPath 2  )  ,    FILE ,    PUBLIC ,     7  8  9 L ,     1  3  5 L )  )  )  ;", "LocalResourceProto   pubRsrcProto 2     =    rsrcPb . getProto (  )  ;", "Path   pubRsrcLocalPath 2     =    new   Path (  \"  / some / local / dir / for / pubrsrc 2  \"  )  ;", "stateStore . startResourceLocalization ( null ,    null ,    pubRsrcProto 2  ,    pubRsrcLocalPath 2  )  ;", "LocalizedResourceProto   pubLocalizedProto 2     =    LocalizedResourceProto . newBuilder (  )  . setResource ( pubRsrcProto 2  )  . setLocalPath ( pubRsrcLocalPath 2  . toString (  )  )  . setSize (  7  6  5  4  3  2  1 L )  . build (  )  ;", "stateStore . finishResourceLocalization ( null ,    null ,    pubLocalizedProto 2  )  ;", "stateStore . removeLocalizedResource ( null ,    null ,    pubRsrcLocalPath 2  )  ;", "Path   privRsrcPath    =    new   Path (  \" hdfs :  /  / some / private / resource \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( privRsrcPath )  ,    PATTERN ,    PRIVATE ,     7  8  9 L ,     6  8  0 L ,     \"  * pattern *  \"  )  )  )  ;", "LocalResourceProto   privRsrcProto    =    rsrcPb . getProto (  )  ;", "Path   privRsrcLocalPath    =    new   Path (  \"  / some / local / dir / for / privrsrc \"  )  ;", "stateStore . startResourceLocalization ( user ,    null ,    privRsrcProto ,    privRsrcLocalPath )  ;", "stateStore . removeLocalizedResource ( user ,    null ,    privRsrcLocalPath )  ;", "restartStateStore (  )  ;", "NM . RecoveredLocalizationState   state    =    stateStore . loadLocalizationState (  )  ;", "NM . LocalResourceTrackerState   pubts    =    state . getPublicTrackerState (  )  ;", "assertTrue ( pubts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    pubts . getLocalizedResources (  )  . size (  )  )  ;", "assertEquals ( pubLocalizedProto 1  ,    pubts . getLocalizedResources (  )  . iterator (  )  . next (  )  )  ;", "Map < String ,    NM . RecoveredUserResources >    userResources    =    state . getUserResources (  )  ;", "assertTrue ( userResources . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoveLocalizedResource"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "String   user    =     \" somebody \"  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "Path   appRsrcPath    =    new   Path (  \" hdfs :  /  / some / app / resource \"  )  ;", "LocalResourcePBImpl   rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( appRsrcPath )  ,    ARCHIVE ,    APPLICATION ,     1  2  3 L ,     4  5  6 L )  )  )  ;", "LocalResourceProto   appRsrcProto    =    rsrcPb . getProto (  )  ;", "Path   appRsrcLocalPath    =    new   Path (  \"  / some / local / dir / for / apprsrc \"  )  ;", "stateStore . startResourceLocalization ( user ,    appId ,    appRsrcProto ,    appRsrcLocalPath )  ;", "restartStateStore (  )  ;", "NM . RecoveredLocalizationState   state    =    stateStore . loadLocalizationState (  )  ;", "NM . LocalResourceTrackerState   pubts    =    state . getPublicTrackerState (  )  ;", "assertTrue ( pubts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertTrue ( pubts . getInProgressResources (  )  . isEmpty (  )  )  ;", "Map < String ,    NM . RecoveredUserResources >    userResources    =    state . getUserResources (  )  ;", "assertEquals (  1  ,    userResources . size (  )  )  ;", "NM . RecoveredUserResources   rur    =    userResources . get ( user )  ;", "NM . LocalResourceTrackerState   privts    =    rur . getPrivateTrackerState (  )  ;", "assertNotNull ( privts )  ;", "assertTrue ( privts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertTrue ( privts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    rur . getAppTrackerStates (  )  . size (  )  )  ;", "NM . LocalResourceTrackerState   appts    =    rur . getAppTrackerStates (  )  . get ( appId )  ;", "assertNotNull ( appts )  ;", "assertTrue ( appts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    appts . getInProgressResources (  )  . size (  )  )  ;", "assertEquals ( appRsrcLocalPath ,    appts . getInProgressResources (  )  . get ( appRsrcProto )  )  ;", "Path   pubRsrcPath 1     =    new   Path (  \" hdfs :  /  / some / public / resource 1  \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( pubRsrcPath 1  )  ,    FILE ,    PUBLIC ,     7  8  9 L ,     1  3  5 L )  )  )  ;", "LocalResourceProto   pubRsrcProto 1     =    rsrcPb . getProto (  )  ;", "Path   pubRsrcLocalPath 1     =    new   Path (  \"  / some / local / dir / for / pubrsrc 1  \"  )  ;", "stateStore . startResourceLocalization ( null ,    null ,    pubRsrcProto 1  ,    pubRsrcLocalPath 1  )  ;", "Path   pubRsrcPath 2     =    new   Path (  \" hdfs :  /  / some / public / resource 2  \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( pubRsrcPath 2  )  ,    FILE ,    PUBLIC ,     7  8  9 L ,     1  3  5 L )  )  )  ;", "LocalResourceProto   pubRsrcProto 2     =    rsrcPb . getProto (  )  ;", "Path   pubRsrcLocalPath 2     =    new   Path (  \"  / some / local / dir / for / pubrsrc 2  \"  )  ;", "stateStore . startResourceLocalization ( null ,    null ,    pubRsrcProto 2  ,    pubRsrcLocalPath 2  )  ;", "Path   privRsrcPath    =    new   Path (  \" hdfs :  /  / some / private / resource \"  )  ;", "rsrcPb    =     (  ( LocalResourcePBImpl )     ( LocalResource . newInstance ( ConverterUtils . getYarnUrlFromPath ( privRsrcPath )  ,    PATTERN ,    PRIVATE ,     7  8  9 L ,     6  8  0 L ,     \"  * pattern *  \"  )  )  )  ;", "LocalResourceProto   privRsrcProto    =    rsrcPb . getProto (  )  ;", "Path   privRsrcLocalPath    =    new   Path (  \"  / some / local / dir / for / privrsrc \"  )  ;", "stateStore . startResourceLocalization ( user ,    null ,    privRsrcProto ,    privRsrcLocalPath )  ;", "restartStateStore (  )  ;", "state    =    stateStore . loadLocalizationState (  )  ;", "pubts    =    state . getPublicTrackerState (  )  ;", "assertTrue ( pubts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertEquals (  2  ,    pubts . getInProgressResources (  )  . size (  )  )  ;", "assertEquals ( pubRsrcLocalPath 1  ,    pubts . getInProgressResources (  )  . get ( pubRsrcProto 1  )  )  ;", "assertEquals ( pubRsrcLocalPath 2  ,    pubts . getInProgressResources (  )  . get ( pubRsrcProto 2  )  )  ;", "userResources    =    state . getUserResources (  )  ;", "assertEquals (  1  ,    userResources . size (  )  )  ;", "rur    =    userResources . get ( user )  ;", "privts    =    rur . getPrivateTrackerState (  )  ;", "assertNotNull ( privts )  ;", "assertTrue ( privts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    privts . getInProgressResources (  )  . size (  )  )  ;", "assertEquals ( privRsrcLocalPath ,    privts . getInProgressResources (  )  . get ( privRsrcProto )  )  ;", "assertEquals (  1  ,    rur . getAppTrackerStates (  )  . size (  )  )  ;", "appts    =    rur . getAppTrackerStates (  )  . get ( appId )  ;", "assertNotNull ( appts )  ;", "assertTrue ( appts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertEquals (  1  ,    appts . getInProgressResources (  )  . size (  )  )  ;", "assertEquals ( appRsrcLocalPath ,    appts . getInProgressResources (  )  . get ( appRsrcProto )  )  ;", "}", "METHOD_END"], "methodName": ["testStartResourceLocalization"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredLocalizationState   state    =    stateStore . loadLocalizationState (  )  ;", "assertNotNull ( state )  ;", "NMStateStoreService . LocalResourceTrackerState   pubts    =    state . getPublicTrackerState (  )  ;", "assertNotNull ( pubts )  ;", "assertTrue ( pubts . getLocalizedResources (  )  . isEmpty (  )  )  ;", "assertTrue ( pubts . getInProgressResources (  )  . isEmpty (  )  )  ;", "assertTrue ( state . getUserResources (  )  . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyEmptyState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.recovery.TestNMLeveldbStateStoreService"}, {"methodBody": ["METHOD_START", "{", "removeAnyContainerTokenIfExpired (  )  ;", "Long   expTime    =    containerTokenIdentifier . getExpiryTimeStamp (  )  ;", "List < ContainerId >    containers    =    this . recentlyStartedContainerTracker . get ( expTime )  ;", "if    (  ( containers    =  =    null )     |  |     (  !  ( containers . contains ( containerTokenIdentifier . getContainerID (  )  )  )  )  )     {", "return   true ;", "} else    {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["isValidStartContainerRequest"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredContainerTokensState   state    =    stateStore . loadContainerTokensState (  )  ;", "MasterKey   key    =    state . getCurrentMasterKey (  )  ;", "if    ( key    !  =    null )     {", "super . currentMasterKey    =    new   MasterKeyData ( key ,    createSecretKey ( key . getBytes (  )  . array (  )  )  )  ;", "}", "key    =    state . getPreviousMasterKey (  )  ;", "if    ( key    !  =    null )     {", "previousMasterKey    =    new   MasterKeyData ( key ,    createSecretKey ( key . getBytes (  )  . array (  )  )  )  ;", "}", "if    (  ( super . currentMasterKey )     !  =    null )     {", "super . serialNo    =     ( super . currentMasterKey . getMasterKey (  )  . getKeyId (  )  )     +     1  ;", "}", "for    ( Map . Entry < ContainerId ,    Long >    entry    :    state . getActiveTokens (  )  . entrySet (  )  )     {", "ContainerId   containerId    =    entry . getKey (  )  ;", "Long   expTime    =    entry . getValue (  )  ;", "List < ContainerId >    containerList    =    recentlyStartedContainerTracker . get ( expTime )  ;", "if    ( containerList    =  =    null )     {", "containerList    =    new   ArrayList < ContainerId >  (  )  ;", "recentlyStartedContainerTracker . put ( expTime ,    containerList )  ;", "}", "if    (  !  ( containerList . contains ( containerId )  )  )     {", "containerList . add ( containerId )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["recover"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "Iterator < Map . Entry < Long ,    List < ContainerId >  >  >    containersI    =    this . recentlyStartedContainerTracker . entrySet (  )  . iterator (  )  ;", "Long   currTime    =    System . currentTimeMillis (  )  ;", "while    ( containersI . hasNext (  )  )     {", "Map . Entry < Long ,    List < ContainerId >  >    containerEntry    =    containersI . next (  )  ;", "if    (  ( containerEntry . getKey (  )  )     <    currTime )     {", "for    ( ContainerId   container    :    containerEntry . getValue (  )  )     {", "try    {", "stateStore . removeContainerToken ( container )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Unable   to   remove   token   for   container    \"     +    container )  ,    e )  ;", "}", "}", "containersI . remove (  )  ;", "} else    {", "break ;", "}", "}", "}", "METHOD_END"], "methodName": ["removeAnyContainerTokenIfExpired"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( super . currentMasterKey )     =  =    null )     |  |     (  ( super . currentMasterKey . getMasterKey (  )  . getKeyId (  )  )     !  =     ( masterKeyRecord . getKeyId (  )  )  )  )     {", ". LOG . info (  (  \" Rolling   master - key   for   container - tokens ,    got   key   with   id    \"     +     ( masterKeyRecord . getKeyId (  )  )  )  )  ;", "if    (  ( super . currentMasterKey )     !  =    null )     {", "updatePreviousMasterKey ( super . currentMasterKey )  ;", "}", "updateCurrentMasterKey ( new   MasterKeyData ( masterKeyRecord ,    createSecretKey ( masterKeyRecord . getBytes (  )  . array (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "nodeHostAddr    =    nodeId . toString (  )  ;", ". LOG . info (  (  \" Updating   node   address    :     \"     +     ( nodeHostAddr )  )  )  ;", "}", "METHOD_END"], "methodName": ["setNodeId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "removeAnyContainerTokenIfExpired (  )  ;", "ContainerId   containerId    =    tokenId . getContainerID (  )  ;", "Long   expTime    =    tokenId . getExpiryTimeStamp (  )  ;", "if    (  !  ( recentlyStartedContainerTracker . containsKey ( expTime )  )  )     {", "recentlyStartedContainerTracker . put ( expTime ,    new   ArrayList < ContainerId >  (  )  )  ;", "}", "recentlyStartedContainerTracker . get ( expTime )  . add ( containerId )  ;", "try    {", "stateStore . storeContainerToken ( containerId ,    expTime )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Unable   to   store   token   for   container    \"     +    containerId )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["startContainerSuccessful"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "super . currentMasterKey    =    key ;", "try    {", "stateStore . storeContainerTokenCurrentMasterKey ( key . getMasterKey (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Unable   to   update   current   master   key   in   state   store \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["updateCurrentMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "previousMasterKey    =    key ;", "try    {", "stateStore . storeContainerTokenPreviousMasterKey ( key . getMasterKey (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Unable   to   update   previous   master   key   in   state   store \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["updatePreviousMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   appAttemptId    =    identifier . getApplicationAttemptId (  )  ;", "if    (  !  ( appToAppAttemptMap . containsKey ( appAttemptId . getApplicationId (  )  )  )  )     {", "appToAppAttemptMap . put ( appAttemptId . getApplicationId (  )  ,    new   ArrayList < ApplicationAttemptId >  (  )  )  ;", "}", "MasterKeyData   oldKey    =    oldMasterKeys . get ( appAttemptId )  ;", "if    ( oldKey    =  =    null )     {", "appToAppAttemptMap . get ( appAttemptId . getApplicationId (  )  )  . add ( appAttemptId )  ;", "}", "if    (  ( oldKey    =  =    null )     |  |     (  ( oldKey . getMasterKey (  )  . getKeyId (  )  )     !  =     ( identifier . getKeyId (  )  )  )  )     {", ". LOG . debug (  (  \" NMToken   key   updated   for   application   attempt    :     \"     +     ( identifier . getApplicationAttemptId (  )  . toString (  )  )  )  )  ;", "if    (  ( identifier . getKeyId (  )  )     =  =     ( currentMasterKey . getMasterKey (  )  . getKeyId (  )  )  )     {", "updateAppAttemptKey ( appAttemptId ,    currentMasterKey )  ;", "} else", "if    (  (  ( previousMasterKey )     !  =    null )     &  &     (  ( identifier . getKeyId (  )  )     =  =     ( previousMasterKey . getMasterKey (  )  . getKeyId (  )  )  )  )     {", "updateAppAttemptKey ( appAttemptId ,    previousMasterKey )  ;", "} else    {", "throw   new   InvalidToken (  \" Older   NMToken   should   not   be   used   while   starting   the   container .  \"  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["appAttemptStartContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "List < ApplicationAttemptId >    appAttemptList    =    appToAppAttemptMap . get ( appId )  ;", "if    ( appAttemptList    !  =    null )     {", ". LOG . debug (  (  \" Removing   application   attempts   NMToken   keys   for   application    \"     +    appId )  )  ;", "for    ( ApplicationAttemptId   appAttemptId    :    appAttemptList )     {", "removeAppAttemptKey ( appAttemptId )  ;", "}", "appToAppAttemptMap . remove ( appId )  ;", "} else    {", ". LOG . error (  (  (  \" No   application   Attempt   for   application    :     \"     +    appId )     +     \"    started   on   this   NM .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["appFinished"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeId ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "return   oldMasterKeys . containsKey ( appAttemptId )  ;", "}", "METHOD_END"], "methodName": ["isAppAttemptNMTokenKeyPresent"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "NMStateStoreService . RecoveredNMTokensState   state    =    stateStore . loadNMTokensState (  )  ;", "MasterKey   key    =    state . getCurrentMasterKey (  )  ;", "if    ( key    !  =    null )     {", "super . currentMasterKey    =    new   MasterKeyData ( key ,    createSecretKey ( key . getBytes (  )  . array (  )  )  )  ;", "}", "key    =    state . getPreviousMasterKey (  )  ;", "if    ( key    !  =    null )     {", "previousMasterKey    =    new   MasterKeyData ( key ,    createSecretKey ( key . getBytes (  )  . array (  )  )  )  ;", "}", "if    (  ( super . currentMasterKey )     !  =    null )     {", "super . serialNo    =     ( super . currentMasterKey . getMasterKey (  )  . getKeyId (  )  )     +     1  ;", "}", "for    ( Map . Entry < ApplicationAttemptId ,    MasterKey >    entry    :    state . getApplicationMasterKeys (  )  . entrySet (  )  )     {", "key    =    entry . getValue (  )  ;", "oldMasterKeys . put ( entry . getKey (  )  ,    new   MasterKeyData ( key ,    createSecretKey ( key . getBytes (  )  . array (  )  )  )  )  ;", "}", "appToAppAttemptMap . clear (  )  ;", "for    ( ApplicationAttemptId   attempt    :    oldMasterKeys . keySet (  )  )     {", "ApplicationId   app    =    attempt . getApplicationId (  )  ;", "List < ApplicationAttemptId >    attempts    =    appToAppAttemptMap . get ( app )  ;", "if    ( attempts    =  =    null )     {", "attempts    =    new   ArrayList < ApplicationAttemptId >  (  )  ;", "appToAppAttemptMap . put ( app ,    attempts )  ;", "}", "attempts . add ( attempt )  ;", "}", "}", "METHOD_END"], "methodName": ["recover"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "this . oldMasterKeys . remove ( attempt )  ;", "try    {", "stateStore . removeNMTokenApplicationMasterKey ( attempt )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Unable   to   remove   master   key   for   application    \"     +    attempt )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["removeAppAttemptKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( super . currentMasterKey )     =  =    null )     |  |     (  ( super . currentMasterKey . getMasterKey (  )  . getKeyId (  )  )     !  =     ( masterKey . getKeyId (  )  )  )  )     {", ". LOG . info (  (  \" Rolling   master - key   for   container - tokens ,    got   key   with   id    \"     +     ( masterKey . getKeyId (  )  )  )  )  ;", "if    (  ( super . currentMasterKey )     !  =    null )     {", "updatePreviousMasterKey ( super . currentMasterKey )  ;", "}", "updateCurrentMasterKey ( new   MasterKeyData ( masterKey ,    createSecretKey ( masterKey . getBytes (  )  . array (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "NMTokenSecretManagerInNM . LOG . debug (  (  \" updating   nodeId    :     \"     +    nodeId )  )  ;", "this . nodeId    =    nodeId ;", "}", "METHOD_END"], "methodName": ["setNodeId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "this . oldMasterKeys . put ( attempt ,    key )  ;", "try    {", "stateStore . storeNMTokenApplicationMasterKey ( attempt ,    key . getMasterKey (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Unable   to   store   master   key   for   application    \"     +    attempt )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["updateAppAttemptKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "super . currentMasterKey    =    key ;", "try    {", "stateStore . storeNMTokenCurrentMasterKey ( key . getMasterKey (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Unable   to   update   current   master   key   in   state   store \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["updateCurrentMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "previousMasterKey    =    key ;", "try    {", "stateStore . storeNMTokenPreviousMasterKey ( key . getMasterKey (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Unable   to   update   previous   master   key   in   state   store \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["updatePreviousMasterKey"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "long   rmid    =    cid . getApplicationAttemptId (  )  . getApplicationId (  )  . getClusterTimestamp (  )  ;", "Identifier   ctid    =    new   Identifier ( cid ,    nodeId . toString (  )  ,    user ,    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ,     (  ( System . currentTimeMillis (  )  )     +     1  0  0  0  0  0 L )  ,    secretMgr . getCurrentKey (  )  . getKeyId (  )  ,    rmid ,    Priority . newInstance (  0  )  ,     0  )  ;", "Token   token    =    BuilderUtils . new ( nodeId ,    secretMgr . createPassword ( ctid )  ,    ctid )  ;", "return   BuilderUtils . newIdentifier ( token )  ;", "}", "METHOD_END"], "methodName": ["createContainerTokenId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.TestNMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "final   NodeId   nodeId    =    NodeId . newInstance (  \" somehost \"  ,     1  2  3  4  )  ;", "final   ContainerId   cid 1     =    BuilderUtils . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "final   ContainerId   cid 2     =    BuilderUtils . newContainerId (  2  ,     2  ,     2  ,     2  )  ;", ". ContainerTokenKeyGeneratorForTest   keygen    =    new    . ContainerTokenKeyGeneratorForTest ( conf )  ;", "NMMemoryStateStoreService   stateStore    =    new   NMMemoryStateStoreService (  )  ;", "stateStore . init ( conf )  ;", "stateStore . start (  )  ;", "NMContainerTokenSecretManager   secretMgr    =    new   NMContainerTokenSecretManager ( conf ,    stateStore )  ;", "secretMgr . setNodeId ( nodeId )  ;", "MasterKey   currentKey    =    keygen . generateKey (  )  ;", "secretMgr . setMasterKey ( currentKey )  ;", "ContainerTokenIdentifier   tokenId 1     =     . createContainerTokenId ( cid 1  ,    nodeId ,     \" user 1  \"  ,    secretMgr )  ;", "ContainerTokenIdentifier   tokenId 2     =     . createContainerTokenId ( cid 2  ,    nodeId ,     \" user 2  \"  ,    secretMgr )  ;", "assertNotNull ( secretMgr . retrievePassword ( tokenId 1  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( tokenId 2  )  )  ;", "secretMgr    =    new   NMContainerTokenSecretManager ( conf ,    stateStore )  ;", "secretMgr . setNodeId ( nodeId )  ;", "secretMgr . recover (  )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertTrue ( secretMgr . isValidStartContainerRequest ( tokenId 1  )  )  ;", "assertTrue ( secretMgr . isValidStartContainerRequest ( tokenId 2  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( tokenId 1  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( tokenId 2  )  )  ;", "secretMgr . startContainerSuccessful ( tokenId 2  )  ;", "currentKey    =    keygen . generateKey (  )  ;", "secretMgr . setMasterKey ( currentKey )  ;", "secretMgr    =    new   NMContainerTokenSecretManager ( conf ,    stateStore )  ;", "secretMgr . setNodeId ( nodeId )  ;", "secretMgr . recover (  )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertTrue ( secretMgr . isValidStartContainerRequest ( tokenId 1  )  )  ;", "assertFalse ( secretMgr . isValidStartContainerRequest ( tokenId 2  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( tokenId 1  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( tokenId 2  )  )  ;", "currentKey    =    keygen . generateKey (  )  ;", "secretMgr . setMasterKey ( currentKey )  ;", "secretMgr    =    new   NMContainerTokenSecretManager ( conf ,    stateStore )  ;", "secretMgr . setNodeId ( nodeId )  ;", "secretMgr . recover (  )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertTrue ( secretMgr . isValidStartContainerRequest ( tokenId 1  )  )  ;", "assertFalse ( secretMgr . isValidStartContainerRequest ( tokenId 2  )  )  ;", "try    {", "secretMgr . retrievePassword ( tokenId 1  )  ;", "fail (  \" token   should   not   be   valid \"  )  ;", "}    catch    ( InvalidToken   e )     {", "}", "try    {", "secretMgr . retrievePassword ( tokenId 2  )  ;", "fail (  \" token   should   not   be   valid \"  )  ;", "}    catch    ( InvalidToken   e )     {", "}", "stateStore . close (  )  ;", "}", "METHOD_END"], "methodName": ["testRecovery"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.TestNMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "Token < NMTokenIdentifier >    convertedToken    =    ConverterUtils . convertFromYarn ( token ,     (  ( Text )     ( null )  )  )  ;", "return   convertedToken . decodeIdentifier (  )  ;", "}", "METHOD_END"], "methodName": ["getNMTokenId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.TestNMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( NM _ RECOVERY _ ENABLED ,    true )  ;", "final   NodeId   nodeId    =    NodeId . newInstance (  \" somehost \"  ,     1  2  3  4  )  ;", "final   ApplicationAttemptId   attempt 1     =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  ,     1  )  ,     1  )  ;", "final   ApplicationAttemptId   attempt 2     =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  2  ,     2  )  ,     2  )  ;", ". NMTokenKeyGeneratorForTest   keygen    =    new    . NMTokenKeyGeneratorForTest (  )  ;", "NMMemoryStateStoreService   stateStore    =    new   NMMemoryStateStoreService (  )  ;", "stateStore . init ( conf )  ;", "stateStore . start (  )  ;", "NMTokenSecretManagerInNM   secretMgr    =    new   NMTokenSecretManagerInNM ( stateStore )  ;", "secretMgr . setNodeId ( nodeId )  ;", "MasterKey   currentKey    =    keygen . generateKey (  )  ;", "secretMgr . setMasterKey ( currentKey )  ;", "NMTokenIdentifier   attemptToken 1     =    getNMTokenId ( secretMgr . createNMToken ( attempt 1  ,    nodeId ,     \" user 1  \"  )  )  ;", "NMTokenIdentifier   attemptToken 2     =    getNMTokenId ( secretMgr . createNMToken ( attempt 2  ,    nodeId ,     \" user 2  \"  )  )  ;", "secretMgr . appAttemptStartContainer ( attemptToken 1  )  ;", "secretMgr . appAttemptStartContainer ( attemptToken 2  )  ;", "assertTrue ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 1  )  )  ;", "assertTrue ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 2  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 1  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 2  )  )  ;", "secretMgr    =    new   NMTokenSecretManagerInNM ( stateStore )  ;", "secretMgr . recover (  )  ;", "secretMgr . setNodeId ( nodeId )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertTrue ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 1  )  )  ;", "assertTrue ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 2  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 1  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 2  )  )  ;", "currentKey    =    keygen . generateKey (  )  ;", "secretMgr . setMasterKey ( currentKey )  ;", "secretMgr . appFinished ( attempt 1  . getApplicationId (  )  )  ;", "secretMgr    =    new   NMTokenSecretManagerInNM ( stateStore )  ;", "secretMgr . recover (  )  ;", "secretMgr . setNodeId ( nodeId )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertFalse ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 1  )  )  ;", "assertTrue ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 2  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 1  )  )  ;", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 2  )  )  ;", "currentKey    =    keygen . generateKey (  )  ;", "secretMgr . setMasterKey ( currentKey )  ;", "secretMgr    =    new   NMTokenSecretManagerInNM ( stateStore )  ;", "secretMgr . recover (  )  ;", "secretMgr . setNodeId ( nodeId )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertFalse ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 1  )  )  ;", "assertTrue ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 2  )  )  ;", "try    {", "secretMgr . retrievePassword ( attemptToken 1  )  ;", "fail (  \" attempt   token   should   not   still   be   valid \"  )  ;", "}    catch    ( InvalidToken   e )     {", "}", "assertNotNull ( secretMgr . retrievePassword ( attemptToken 2  )  )  ;", "secretMgr . appFinished ( attempt 2  . getApplicationId (  )  )  ;", "secretMgr    =    new   NMTokenSecretManagerInNM ( stateStore )  ;", "secretMgr . recover (  )  ;", "secretMgr . setNodeId ( nodeId )  ;", "assertEquals ( currentKey ,    secretMgr . getCurrentKey (  )  )  ;", "assertFalse ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 1  )  )  ;", "assertFalse ( secretMgr . isAppAttemptNMTokenKeyPresent ( attempt 2  )  )  ;", "try    {", "secretMgr . retrievePassword ( attemptToken 1  )  ;", "fail (  \" attempt   token   should   not   still   be   valid \"  )  ;", "}    catch    ( InvalidToken   e )     {", "}", "try    {", "secretMgr . retrievePassword ( attemptToken 2  )  ;", "fail (  \" attempt   token   should   not   still   be   valid \"  )  ;", "}    catch    ( InvalidToken   e )     {", "}", "stateStore . close (  )  ;", "}", "METHOD_END"], "methodName": ["testRecovery"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.security.TestNMTokenSecretManagerInNM"}, {"methodBody": ["METHOD_START", "{", "if    ( isCpuWeightEnabled (  )  )     {", "delete ( pathFor ( CONTROLLER _ CPU ,    containerId . toString (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["clearLimits"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "String   path    =    pathForCgroup ( controller ,    groupName )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" createCgroup :     \"     +    path )  )  ;", "}", "if    (  !  ( new   File ( path )  . mkdir (  )  )  )     {", "throw   new   IOException (  (  \" Failed   to   create   cgroup   at    \"     +    path )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createCgroup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "boolean   deleted ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" deleteCgroup :     \"     +    cgroupPath )  )  ;", "}", "long   start    =    clock . getTime (  )  ;", "do    {", "deleted    =    new   File ( cgroupPath )  . delete (  )  ;", "if    (  ! deleted )     {", "try    {", "Thread . sleep (  2  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "}", "}", "}    while    (  (  ! deleted )     &  &     (  (  ( clock . getTime (  )  )     -    start )     <     ( deleteCgroupTimeout )  )     )  ;", "if    (  ! deleted )     {", ". LOG . warn (  (  (  (  (  \" Unable   to   delete   cgroup   at :     \"     +    cgroupPath )     +     \"  ,    tried   to   delete   for    \"  )     +     ( deleteCgroupTimeout )  )     +     \" ms \"  )  )  ;", "}", "return   deleted ;", "}", "METHOD_END"], "methodName": ["deleteCgroup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "for    ( Map . Entry < String ,    List < String >  >    e    :    entries . entrySet (  )  )     {", "if    ( e . getValue (  )  . contains ( control )  )", "return   e . getKey (  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["findControllerInMtab"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "String   containerName    =    containerId . toString (  )  ;", "StringBuilder   sb    =    new   StringBuilder (  \" c =  \"  )  ;", "if    ( isCpuWeightEnabled (  )  )     {", "sb . append (  (  ( pathForCgroup ( CONTROLLER _ CPU ,    containerName )  )     +     \"  / tasks \"  )  )  ;", "sb . append (  \"  ,  \"  )  ;", "}", "if    (  ( sb . charAt (  (  ( sb . length (  )  )     -     1  )  )  )     =  =     '  ,  '  )     {", "sb . deleteCharAt (  (  ( sb . length (  )  )     -     1  )  )  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getResourcesOption"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "initConfig (  )  ;", "if    (  ( cgroupMount )     &  &     (  ( cgroupMountPath )     !  =    null )  )     {", "ArrayList < String >    cgroupKVs    =    new   ArrayList < String >  (  )  ;", "cgroupKVs . add (  (  (  (  (  ( CONTROLLER _ CPU )     +     \"  =  \"  )     +     ( cgroupMountPath )  )     +     \"  /  \"  )     +     ( CONTROLLER _ CPU )  )  )  ;", "lce . mount ( cgroupKVs ,    cgroupPrefix )  ;", "}", "initializeControllerPaths (  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "this . cgroupPrefix    =    conf . get ( NM _ LINUX _ CONTAINER _ CGROUPS _ HIERARCHY ,     \"  / hadoop - yarn \"  )  ;", "this . cgroupMount    =    conf . getBoolean ( NM _ LINUX _ CONTAINER _ CGROUPS _ MOUNT ,    false )  ;", "this . cgroupMountPath    =    conf . get ( NM _ LINUX _ CONTAINER _ CGROUPS _ MOUNT _ PATH ,    null )  ;", "this . deleteCgroupTimeout    =    conf . getLong ( NM _ LINUX _ CONTAINER _ CGROUPS _ DELETE _ TIMEOUT ,    DEFAULT _ NM _ LINUX _ CONTAINER _ CGROUPS _ DELETE _ TIMEOUT )  ;", "if    (  ( cgroupPrefix . charAt (  0  )  )     =  =     '  /  '  )     {", "cgroupPrefix    =    cgroupPrefix . substring (  1  )  ;", "}", "int   len    =    cgroupPrefix . length (  )  ;", "if    (  ( cgroupPrefix . charAt (  ( len    -     1  )  )  )     =  =     '  /  '  )     {", "cgroupPrefix    =    cgroupPrefix . substring (  0  ,     ( len    -     1  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initConfig"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "String   controllerPath ;", "Map < String ,    List < String >  >    parsedMtab    =    parseMtab (  )  ;", "controllerPath    =    findControllerInMtab ( CONTROLLER _ CPU ,    parsedMtab )  ;", "if    ( controllerPath    !  =    null )     {", "File   f    =    new   File (  (  ( controllerPath    +     \"  /  \"  )     +     ( this . cPrefix )  )  )  ;", "if    ( FileUtil . canWrite ( f )  )     {", "controllerPaths . put ( CONTROLLER _ CPU ,    controllerPath )  ;", "} else    {", "throw   new   IOException (  (  (  \" Not   able   to   enforce   cpu   weights ;    cannot   write    \"     +     \" to   c   at :     \"  )     +    controllerPath )  )  ;", "}", "} else    {", "throw   new   IOException (  (  (  \" Not   able   to   enforce   cpu   weights ;    cannot   find    \"     +     \" c   for   cpu   controller   in    \"  )     +     ( MTAB _ FILE )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initializeControllerPaths"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "return   this . cpuWeightEnabled ;", "}", "METHOD_END"], "methodName": ["isCpuWeightEnabled"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    List < String >  >    ret    =    new   HashMap < String ,    List < String >  >  (  )  ;", "BufferedReader   in    =    null ;", "try    {", "in    =    new   BufferedReader ( new   FileReader ( new   File ( MTAB _ FILE )  )  )  ;", "for    ( String   str    =    in . readLine (  )  ;    str    !  =    null ;    str    =    in . readLine (  )  )     {", "Matcher   m    =     . MTAB _ FILE _ FORMAT . matcher ( str )  ;", "boolean   mat    =    m . find (  )  ;", "if    ( mat )     {", "String   path    =    m . group (  1  )  ;", "String   type    =    m . group (  2  )  ;", "String   options    =    m . group (  3  )  ;", "if    ( type . equals ( CGROUPS _ FSTYPE )  )     {", "List < String >    value    =    Arrays . asList ( options . split (  \"  ,  \"  )  )  ;", "ret . put ( path ,    value )  ;", "}", "}", "}", "}    catch    ( IOException   e )     {", "throw   new   IOException (  (  \" Error   while   reading    \"     +     ( MTAB _ FILE )  )  ,    e )  ;", "}    finally    {", "try    {", "in . close (  )  ;", "}    catch    ( IOException   e 2  )     {", ". LOG . warn (  (  \" Error   closing   the   stream :     \"     +     ( MTAB _ FILE )  )  ,    e 2  )  ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["parseMtab"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "String   controllerPath    =    controllerPaths . get ( controller )  ;", "return    (  (  ( controllerPath    +     \"  /  \"  )     +     ( cPrefix )  )     +     \"  /  \"  )     +    Name ;", "}", "METHOD_END"], "methodName": ["pathForCgroup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "clearLimits ( containerId )  ;", "}", "METHOD_END"], "methodName": ["postExecute"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "setupLimits ( containerId ,    containerResource )  ;", "}", "METHOD_END"], "methodName": ["preExecute"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "String   containerName    =    containerId . toString (  )  ;", "if    ( isCpuWeightEnabled (  )  )     {", "createCgroup ( CONTROLLER _ CPU ,    containerName )  ;", "int   cpuShares    =     ( CPU _ DEFAULT _ WEIGHT )     *     ( container . getVirtualCores (  )  )  ;", "updateCgroup ( CONTROLLER _ CPU ,    containerName ,     \" shares \"  ,    String . valueOf ( cpuShares )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setupLimits"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "FileWriter   f    =    null ;", "String   path    =    pathForCgroup ( controller ,    groupName )  ;", "param    =     ( controller    +     \"  .  \"  )     +    param ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  \" updateCgroup :     \"     +    path )     +     \"  :     \"  )     +    param )     +     \"  =  \"  )     +    value )  )  ;", "}", "try    {", "f    =    new   FileWriter (  (  ( path    +     \"  /  \"  )     +    param )  ,    false )  ;", "f . write ( value )  ;", "}    catch    ( IOException   e )     {", "throw   new   IOException (  (  (  (  (  (  \" Unable   to   set    \"     +    param )     +     \"  =  \"  )     +    value )     +     \"    for   cgroup   at :     \"  )     +    path )  ,    e )  ;", "}    finally    {", "if    ( f    !  =    null )     {", "try    {", "f . close (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  (  \" Unable   to   close   cgroup   file :     \"     +    path )  ,    e )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["updateCgroup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "return    \" cgroups = none \"  ;", "}", "METHOD_END"], "methodName": ["getResourcesOption"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    conf ;", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "URL   local    =    ConverterUtils . getYarnUrlFromPath ( path )  ;", "ResourceLocalizationSpec   resourceLocalizationSpec    =    Records . newRecord ( ResourceLocalizationSpec . class )  ;", "resourceLocalizationSpec . setDestinationDirectory ( local )  ;", "resourceLocalizationSpec . setResource ( rsrc )  ;", "return   resourceLocalizationSpec ;", "}", "METHOD_END"], "methodName": ["newResourceLocalizationSpec"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerBuilderUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( path    =  =    null )     {", "throw   new   IOException (  \" Trying   to   access   process   id   from   a   null   path \"  )  ;", "}", ". LOG . debug (  (  \" Accessing   pid   from   pid   file    \"     +    path )  )  ;", "String   processId    =    null ;", "FileReader   fileReader    =    null ;", "BufferedReader   bufReader    =    null ;", "try    {", "File   file    =    new   File ( path . toString (  )  )  ;", "if    ( file . exists (  )  )     {", "fileReader    =    new   FileReader ( file )  ;", "bufReader    =    new   BufferedReader ( fileReader )  ;", "while    ( true )     {", "String   line    =    bufReader . readLine (  )  ;", "if    ( line    =  =    null )     {", "break ;", "}", "String   temp    =    line . trim (  )  ;", "if    (  !  ( temp . isEmpty (  )  )  )     {", "if    ( Shell . WINDOWS )     {", "try    {", "ConverterUtils . toContainerId ( temp )  ;", "processId    =    temp ;", "break ;", "}    catch    ( Exception   e )     {", "}", "} else    {", "try    {", "Long   pid    =    Long . valueOf ( temp )  ;", "if    ( pid    >     0  )     {", "processId    =    temp ;", "break ;", "}", "}    catch    ( Exception   e )     {", "}", "}", "}", "}", "}", "}    finally    {", "if    ( fileReader    !  =    null )     {", "fileReader . close (  )  ;", "}", "if    ( bufReader    !  =    null )     {", "bufReader . close (  )  ;", "}", "}", ". LOG . debug (  (  (  (  \" Got   pid    \"     +     ( processId    !  =    null    ?    processId    :     \" null \"  )  )     +     \"    from   path    \"  )     +    path )  )  ;", "return   processId ;", "}", "METHOD_END"], "methodName": ["getProcessId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.ProcessIdFileReader"}, {"methodBody": ["METHOD_START", "{", "final   TestCgroupsLCEResourcesHandler . MockClock   clock    =    new   TestCgroupsLCEResourcesHandler . MockClock (  )  ;", "clock . time    =    System . currentTimeMillis (  )  ;", "CgroupsLCEResourcesHandler   handler    =    new   CgroupsLCEResourcesHandler (  )  ;", "handler . setConf ( new   YarnConfiguration (  )  )  ;", "handler . initConfig (  )  ;", "handler . clock    =    clock ;", "File   file    =    new   File (  \" target \"  ,    UUID . randomUUID (  )  . toString (  )  )  ;", "new   FileOutputStream ( file )  . close (  )  ;", "Assert . assertTrue ( handler . deleteCgroup ( file . getPath (  )  )  )  ;", "final   CountDownLatch   latch    =    new   CountDownLatch (  1  )  ;", "new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "latch . countDown (  )  ;", "try    {", "Thread . sleep (  2  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "}", "clock . time    +  =    YarnConfiguration . DEFAULT _ NM _ LINUX _ CONTAINER _ CGROUPS _ DELETE _ TIMEOUT ;", "}", "}  . start (  )  ;", "latch . await (  )  ;", "file    =    new   File (  \" target \"  ,    UUID . randomUUID (  )  . toString (  )  )  ;", "Assert . assertFalse ( handler . deleteCgroup ( file . getPath (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteCgroup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.TestCgroupsLCEResourcesHandler"}, {"methodBody": ["METHOD_START", "{", "String   rootDir    =    new   File ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . getAbsolutePath (  )  ;", "File   testFile    =    null ;", "String   processIdInFile    =     ( Shell . WINDOWS )     ?     \"    container _  1  3  5  3  7  4  2  6  8  0  9  4  0  _  0  0  0  2  _  0  1  _  0  0  0  0  0  1     \"     :     \"     2  3     \"  ;", "String   expectedProcessId    =    processIdInFile . trim (  )  ;", "try    {", "testFile    =    new   File ( rootDir ,     \" temp . txt \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( testFile )  ;", "fileWriter . println (  \"           \"  )  ;", "fileWriter . println (  \"  \"  )  ;", "fileWriter . println (  \" abc \"  )  ;", "fileWriter . println (  \"  -  1  2  3  \"  )  ;", "fileWriter . println (  \"  -  1  2  3     \"  )  ;", "fileWriter . println ( processIdInFile )  ;", "fileWriter . println (  \"  6  2  3  6  \"  )  ;", "fileWriter . close (  )  ;", "String   processId    =    null ;", "processId    =     . getProcessId ( new   Path (  (  ( rootDir    +     ( Path . SEPARATOR )  )     +     \" temp . txt \"  )  )  )  ;", "Assert . assertEquals ( expectedProcessId ,    processId )  ;", "}    finally    {", "if    (  ( testFile    !  =    null )     &  &     ( testFile . exists (  )  )  )     {", "testFile . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testComplexGet"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.TestProcessIdFileReader"}, {"methodBody": ["METHOD_START", "{", "String   pid    =    null ;", "try    {", "pid    =     . getProcessId ( null )  ;", "fail (  \" Expected   an   error   to   be   thrown   for   null   path \"  )  ;", "}    catch    ( Exception   e )     {", "}", "assert   pid    =  =    null ;", "}", "METHOD_END"], "methodName": ["testNullPath"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.TestProcessIdFileReader"}, {"methodBody": ["METHOD_START", "{", "String   rootDir    =    new   File ( System . getProperty (  \" test . build . data \"  ,     \"  / tmp \"  )  )  . getAbsolutePath (  )  ;", "File   testFile    =    null ;", "String   expectedProcessId    =     ( Shell . WINDOWS )     ?     \" container _  1  3  5  3  7  4  2  6  8  0  9  4  0  _  0  0  0  2  _  0  1  _  0  0  0  0  0  1  \"     :     \"  5  6  7  8  9  \"  ;", "try    {", "testFile    =    new   File ( rootDir ,     \" temp . txt \"  )  ;", "PrintWriter   fileWriter    =    new   PrintWriter ( testFile )  ;", "fileWriter . println ( expectedProcessId )  ;", "fileWriter . close (  )  ;", "String   processId    =    null ;", "processId    =     . getProcessId ( new   Path (  (  ( rootDir    +     ( Path . SEPARATOR )  )     +     \" temp . txt \"  )  )  )  ;", "Assert . assertEquals ( expectedProcessId ,    processId )  ;", "}    finally    {", "if    (  ( testFile    !  =    null )     &  &     ( testFile . exists (  )  )  )     {", "testFile . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testSimpleGet"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.util.TestProcessIdFileReader"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  ,    aaSorting :     [  [  0  ,     ' asc '  ]  ]  \"  )  . append (  \"  ,    aoColumns :  [ null ,    null ]  }     \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["appsTableInit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.AllApplicationsPage"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  ,    aoColumns :  [ null ,    null ,     { bSearchable : false }  ]  }     \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["containersTableInit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.AllContainersPage"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  , aoColumns :  [ null ]  }  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["containersTableInit"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ApplicationPage"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   callerUGI    =    null ;", "if    ( remoteUser    !  =    null )     {", "callerUGI    =    UserGroupInformation . createRemoteUser ( remoteUser )  ;", "}", "if    (  ( callerUGI    !  =    null )     &  &     (  !  ( context . getApplicationACLsManager (  )  . checkAccess ( callerUGI ,    VIEW _ APP ,    application . getUser (  )  ,    application . getAppId (  )  )  )  )  )     {", "throw   new   exceptions . YarnException (  (  (  (  \" User    [  \"     +    remoteUser )     +     \"  ]    is   not   authorized   to   view   the   logs   for   application    \"  )     +     ( application . getAppId (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( state    =  =     ( ContainerState . NEW )  )     |  |     ( state    =  =     ( ContainerState . LOCALIZING )  )  )     |  |     ( state    =  =     ( ContainerState . LOCALIZED )  )  )     {", "throw   new   NotFoundException (  (  \" Container   is   not   yet   running .    Current   state   is    \"     +    state )  )  ;", "}", "if    ( state    =  =     ( ContainerState . LOCALIZATION _ FAILED )  )     {", "throw   new   NotFoundException (  \" Container   wasn ' t   started .    Localization   failed .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "Application   application    =    context . getApplications (  )  . get ( applicationId )  ;", "if    ( application    =  =    null )     {", "throw   new   NotFoundException (  (  \" Unknown   container .       either   has   not   started   or    \"     +     (  \" has   already   completed   or    \"     +     \" doesn ' t   belong   to   this   node   at   all .  \"  )  )  )  ;", "}", "return   application ;", "}", "METHOD_END"], "methodName": ["getApplicationForContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    context . getContainers (  )  . get ( containerId )  ;", "Application   application    =     . getApplicationForContainer ( containerId ,    context )  ;", ". checkAccess ( remoteUser ,    application ,    context )  ;", "if    ( container    !  =    null )     {", ". checkState ( container . getContainerState (  )  )  ;", "}", "return    . getContainerLogDirs ( containerId ,    context . getLocalDirsHandler (  )  )  ;", "}", "METHOD_END"], "methodName": ["getContainerLogDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "List < String >    logDirs    =    dirsHandler . getLogDirs (  )  ;", "List < File >    cDirs    =    new   ArrayList < File >  ( logDirs . size (  )  )  ;", "for    ( String   logDir    :    logDirs )     {", "try    {", "logDir    =    new   URI ( logDir )  . getPath (  )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   YarnException (  \" Internal   error \"  ,    e )  ;", "}", "String   appIdStr    =    ConverterUtils . toString ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  ;", "File   appLogDir    =    new   File ( logDir ,    appIdStr )  ;", "cDirs . add ( new   File ( appLogDir ,    containerId . toString (  )  )  )  ;", "}", "return   cDirs ;", "}", "METHOD_END"], "methodName": ["getContainerLogDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    context . getContainers (  )  . get ( containerId )  ;", "Application   application    =     . getApplicationForContainer ( containerId ,    context )  ;", ". checkAccess ( remoteUser ,    application ,    context )  ;", "if    ( container    !  =    null )     {", ". checkState ( container . getContainerState (  )  )  ;", "}", "try    {", "LocalDirsHandlerService   dirsHandler    =    context . getLocalDirsHandler (  )  ;", "String   relativeContainerLogDir    =    ContainerLaunch . getRelativeContainerLogDir ( application . getAppId (  )  . toString (  )  ,    containerId . toString (  )  )  ;", "Path   logPath    =    dirsHandler . getLogPathToRead (  (  ( relativeContainerLogDir    +     ( Path . SEPARATOR )  )     +    fileName )  )  ;", "URI   logPathURI    =    new   URI ( logPath . toString (  )  )  ;", "File   logFile    =    new   File ( logPathURI . getPath (  )  )  ;", "return   logFile ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   YarnException (  \" Internal   error \"  ,    e )  ;", "}    catch    ( IOException   e )     {", ". LOG . warn (  \" Failed   to   find   log   file \"  ,    e )  ;", "throw   new   NotFoundException (  \" Cannot   find   this   log   on   the   local   disk .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["getContainerLogFile"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    ConverterUtils . toContainerId ( containerIdStr )  ;", "ApplicationId   applicationId    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "String   user    =    context . getApplications (  )  . get ( applicationId )  . getUser (  )  ;", "try    {", "return   SecureIOUtils . openForRead ( logFile ,    user ,    null )  ;", "}    catch    ( IOException   e )     {", "if    ( e . getMessage (  )  . contains (  (  (  \" did   not   match   expected   owner    '  \"     +    user )     +     \"  '  \"  )  )  )     {", ". LOG . error (  (  \" Exception   reading   log   file    \"     +     ( logFile . getAbsolutePath (  )  )  )  ,    e )  ;", "throw   new   IOException (  (  (  (  \" Exception   reading   log   file .    Application   submitted   by    '  \"     +    user )     +     \"  '    doesn ' t   own   requested   log   file    :     \"  )     +     ( logFile . getName (  )  )  )  ,    e )  ;", "} else    {", "throw   new   IOException (  (  (  \" Exception   reading   log   file .    It   might   be   because   log    \"     +     \" file   was   aggregated    :     \"  )     +     ( logFile . getName (  )  )  )  ,    e )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["openLogFileForRead"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerLogsUtils"}, {"methodBody": ["METHOD_START", "{", "return   appId ;", "}", "METHOD_END"], "methodName": ["getAppId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.MockApp"}, {"methodBody": ["METHOD_START", "{", "return   appState ;", "}", "METHOD_END"], "methodName": ["getApplicationState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.MockApp"}, {"methodBody": ["METHOD_START", "{", "return   containers ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.MockApp"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.MockApp"}, {"methodBody": ["METHOD_START", "{", "this . appState    =    state ;", "}", "METHOD_END"], "methodName": ["setState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.MockApp"}, {"methodBody": ["METHOD_START", "{", "this . state    =    state ;", "}", "METHOD_END"], "methodName": ["setState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.MockContainer"}, {"methodBody": ["METHOD_START", "{", "render ( AllApplicationsPage . class )  ;", "}", "METHOD_END"], "methodName": ["allApplications"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "render ( AllContainersPage . class )  ;", "}", "METHOD_END"], "methodName": ["allContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "render ( ApplicationPage . class )  ;", "}", "METHOD_END"], "methodName": ["application"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "render ( ContainerPage . class )  ;", "}", "METHOD_END"], "methodName": ["container"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "render ( NodePage . class )  ;", "}", "METHOD_END"], "methodName": ["info"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "String   containerIdStr    =     $  ( CONTAINER _ ID )  ;", "ContainerId   containerId    =    null ;", "try    {", "containerId    =    ConverterUtils . toContainerId ( containerIdStr )  ;", "}    catch    ( IllegalArgumentException   e )     {", "render ( ContainerLogsPage . class )  ;", "return ;", "}", "ApplicationId   appId    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "Application   app    =    nmContext . getApplications (  )  . get ( appId )  ;", "if    (  ( app    =  =    null )     &  &     ( nmConf . getBoolean ( LOG _ AGGREGATION _ ENABLED ,    DEFAULT _ LOG _ AGGREGATION _ ENABLED )  )  )     {", "String   logSUrl    =    nmConf . get ( YARN _ LOG _ SERVER _ URL )  ;", "String   redirectUrl    =    null ;", "if    (  ( logSUrl    =  =    null )     |  |     ( logSUrl . isEmpty (  )  )  )     {", "redirectUrl    =     \" false \"  ;", "} else    {", "redirectUrl    =    url ( logSUrl ,    nmContext . getNodeId (  )  . toString (  )  ,    containerIdStr ,    containerIdStr ,     $  ( APP _ OWNER )  )  ;", "}", "set ( ContainerLogsPage . REDIRECT _ URL ,    redirectUrl )  ;", "}", "render ( ContainerLogsPage . class )  ;", "}", "METHOD_END"], "methodName": ["logs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "render ( NodePage . class )  ;", "}", "METHOD_END"], "methodName": ["node"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMController"}, {"methodBody": ["METHOD_START", "{", "set ( ACCORDION _ ID ,     \" nav \"  )  ;", "set ( initID ( ACCORDION ,     \" nav \"  )  ,     \"  { autoHeight : false ,    active :  0  }  \"  )  ;", "}", "METHOD_END"], "methodName": ["commonPreHead"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMView"}, {"methodBody": ["METHOD_START", "{", "return   getNodeInfo (  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId ;", "try    {", "containerId    =    ConverterUtils . toContainerId ( containerIdStr )  ;", "}    catch    ( IllegalArgumentException   ex )     {", "return   Response . status ( BAD _ REQUEST )  . build (  )  ;", "}", "File   logFile    =    null ;", "try    {", "logFile    =    ContainerLogsUtils . getContainerLogFile ( containerId ,    filename ,    request . getRemoteUser (  )  ,    nmContext )  ;", "}    catch    ( NotFoundException   ex )     {", "return   Response . status ( NOT _ FOUND )  . entity ( ex . getMessage (  )  )  . build (  )  ;", "}    catch    ( YarnException   ex )     {", "return   ResponseError (  )  . entity ( ex . getMessage (  )  )  . build (  )  ;", "}", "try    {", "final   FileInputStream   fis    =    ContainerLogsUtils . openLogFileForRead ( containerIdStr ,    logFile ,    nmContext )  ;", "StreamingOutput   stream    =    new   StreamingOutput (  )     {", "@ Override", "public   void   write ( OutputStream   os )    throws   IOException ,    WebApplicationException    {", "int   bufferSize    =     6  5  5  3  6  ;", "byte [  ]    buf    =    new   byte [ bufferSize ]  ;", "int   len ;", "while    (  ( len    =    fis . read ( buf ,     0  ,    bufferSize )  )     >     0  )     {", "os . write ( buf ,     0  ,    len )  ;", "}", "os . flush (  )  ;", "}", "}  ;", "return   Response . ok ( stream )  . build (  )  ;", "}    catch    ( IOException   ex )     {", "return   ResponseError (  )  . entity ( ex . getMessage (  )  )  . build (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getLogs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "ApplicationId   id    =    ConverterUtils . toApplicationId (  . recordFactory ,    appId )  ;", "if    ( id    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   with   id    \"     +    appId )     +     \"    not   found \"  )  )  ;", "}", "Application   app    =    this . nmContext . getApplications (  )  . get ( id )  ;", "if    ( app    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   with   id    \"     +    appId )     +     \"    not   found \"  )  )  ;", "}", "return   new   AppInfo ( app )  ;", "}", "METHOD_END"], "methodName": ["getNodeApp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "AppsInfo   allApps    =    new   AppsInfo (  )  ;", "for    ( Map . Entry < ApplicationId ,    Application >    entry    :    this . nmContext . getApplications (  )  . entrySet (  )  )     {", "AppInfo   Info    =    new   AppInfo ( entry . getValue (  )  )  ;", "if    (  ( stateQuery    !  =    null )     &  &     (  !  ( stateQuery . isEmpty (  )  )  )  )     {", "ApplicationState . valueOf ( stateQuery )  ;", "if    (  !  ( Info . getState (  )  . equalsIgnoreCase ( stateQuery )  )  )     {", "continue ;", "}", "}", "if    ( userQuery    !  =    null )     {", "if    ( userQuery . isEmpty (  )  )     {", "String   msg    =     \" Error :    You   must   specify   a   non - empty   string   for   the   user \"  ;", "throw   new   BadRequestException ( msg )  ;", "}", "if    (  !  ( Info . getUser (  )  . toString (  )  . equals ( userQuery )  )  )     {", "continue ;", "}", "}", "allApps . add ( Info )  ;", "}", "return   allApps ;", "}", "METHOD_END"], "methodName": ["getNodeApps"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    null ;", "init (  )  ;", "try    {", "containerId    =    ConverterUtils . toContainerId ( id )  ;", "}    catch    ( Exception   e )     {", "throw   new   BadRequestException (  (  \" invalid   container   id ,     \"     +    id )  )  ;", "}", "Container   container    =    nmContext . getContainers (  )  . get ( containerId )  ;", "if    ( container    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" container   with   id ,     \"     +    id )     +     \"  ,    not   found \"  )  )  ;", "}", "return   new   ContainerInfo ( this . nmContext ,    container ,    uriInfo . getBaseUri (  )  . toString (  )  ,    name (  )  )  ;", "}", "METHOD_END"], "methodName": ["getNodeContainer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "ContainersInfo   allContainers    =    new   ContainersInfo (  )  ;", "for    ( Map . Entry < ContainerId ,    Container >    entry    :    this . nmContext . getContainers (  )  . entrySet (  )  )     {", "if    (  ( entry . getValue (  )  )     =  =    null )     {", "continue ;", "}", "ContainerInfo   info    =    new   ContainerInfo ( this . nmContext ,    entry . getValue (  )  ,    uriInfo . getBaseUri (  )  . toString (  )  ,    name (  )  )  ;", "allContainers . add ( info )  ;", "}", "return   allContainers ;", "}", "METHOD_END"], "methodName": ["getNodeContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "return   new   NInfo ( this . nmContext ,    this . rview )  ;", "}", "METHOD_END"], "methodName": ["getNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "response . setContentType ( null )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices"}, {"methodBody": ["METHOD_START", "{", "File   absLogDir    =    new   File (  \" target \"  ,     (  ( TestNMWebServer . class . getSimpleName (  )  )     +     \" LogDir \"  )  )  . getAbsoluteFile (  )  ;", "String   logdirwithFile    =    absLogDir . toURI (  )  . toString (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LOG _ DIRS ,    logdirwithFile )  ;", "NodeHealthCheckerService   healthChecker    =    new   NodeHealthCheckerService (  )  ;", "healthChecker . init ( conf )  ;", "LocalDirsHandlerService   dirsHandler    =    healthChecker . getDiskHandler (  )  ;", "NodeManager . NMContext   nmContext    =    new   NodeManager . NMContext ( null ,    null ,    dirsHandler ,    new   security . ApplicationACLsManager ( conf )  ,    new   NMNullStateStoreService (  )  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( conf )  ;", "String   user    =     \" nobody \"  ;", "long   clusterTimeStamp    =     1  2  3  4  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId ( recordFactory ,    clusterTimeStamp ,     1  )  ;", "Application   app    =    mock ( Application . class )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   container 1     =    BuilderUtils . newContainerId ( recordFactory ,    appId ,    appAttemptId ,     0  )  ;", "nmContext . getApplications (  )  . put ( appId ,    app )  ;", "MockContainer   container    =    new   MockContainer ( appAttemptId ,    new   AsyncDispatcher (  )  ,    conf ,    user ,    appId ,     1  )  ;", "container . setState ( ContainerState . RUNNING )  ;", "nmContext . getContainers (  )  . put ( container 1  ,    container )  ;", "List < File >    files    =    null ;", "files    =    ContainerLogsUtils . getContainerLogDirs ( container 1  ,    user ,    nmContext )  ;", "Assert . assertTrue (  (  !  ( files . get (  0  )  . toString (  )  . contains (  \" file :  \"  )  )  )  )  ;", "nmContext . getContainers (  )  . remove ( container 1  )  ;", "Assert . assertNull ( nmContext . getContainers (  )  . get ( container 1  )  )  ;", "files    =    ContainerLogsUtils . getContainerLogDirs ( container 1  ,    user ,    nmContext )  ;", "Assert . assertTrue (  (  !  ( files . get (  0  )  . toString (  )  . contains (  \" file :  \"  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLogDirs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage"}, {"methodBody": ["METHOD_START", "{", "assumeTrue ( NativeIO . isAvailable (  )  )  ;", "String   user    =     \" randomUser \"     +     ( System . currentTimeMillis (  )  )  ;", "File   absLogDir    =    null ;", "File   appDir    =    null ;", "File   containerDir    =    null ;", "File   syslog    =    null ;", "try    {", "absLogDir    =    new   File (  \" target \"  ,     (  (  . class . getSimpleName (  )  )     +     \" LogDir \"  )  )  . getAbsoluteFile (  )  ;", "absLogDir . mkdir (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LOG _ DIRS ,    absLogDir . toURI (  )  . toString (  )  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "NodeHealthCheckerService   healthChecker    =    new   NodeHealthCheckerService (  )  ;", "healthChecker . init ( conf )  ;", "LocalDirsHandlerService   dirsHandler    =    healthChecker . getDiskHandler (  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( conf )  ;", "long   clusterTimeStamp    =     1  2  3  4  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId ( recordFactory ,    clusterTimeStamp ,     1  )  ;", "Application   app    =    mock ( Application . class )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   container 1     =    BuilderUtils . newContainerId ( recordFactory ,    appId ,    appAttemptId ,     0  )  ;", "appDir    =    new   File ( absLogDir ,    appId . toString (  )  )  ;", "appDir . mkdir (  )  ;", "containerDir    =    new   File ( appDir ,    container 1  . toString (  )  )  ;", "containerDir . mkdir (  )  ;", "syslog    =    new   File ( containerDir ,     \" syslog \"  )  ;", "syslog . createNewFile (  )  ;", "BufferedOutputStream   out    =    new   BufferedOutputStream ( new   FileOutputStream ( syslog )  )  ;", "out . write (  \" Log   file   Content \"  . getBytes (  )  )  ;", "out . close (  )  ;", "Context   context    =    mock ( Context . class )  ;", "ConcurrentMap < ApplicationId ,    Application >    appMap    =    new   ConcurrentHashMap < ApplicationId ,    Application >  (  )  ;", "appMap . put ( appId ,    app )  ;", "when ( context . getApplications (  )  )  . thenReturn ( appMap )  ;", "ConcurrentHashMap < ContainerId ,    Container >    containers    =    new   ConcurrentHashMap < ContainerId ,    Container >  (  )  ;", "when ( context . getContainers (  )  )  . thenReturn ( containers )  ;", "when ( context . getLocalDirsHandler (  )  )  . thenReturn ( dirsHandler )  ;", "MockContainer   container    =    new   MockContainer ( appAttemptId ,    new   AsyncDispatcher (  )  ,    conf ,    user ,    appId ,     1  )  ;", "container . setState ( ContainerState . RUNNING )  ;", "context . getContainers (  )  . put ( container 1  ,    container )  ;", "ContainerLogsPage . ContainersLogsBlock   cLogsBlock    =    new   ContainerLogsPage . ContainersLogsBlock ( context )  ;", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( CONTAINER _ ID ,    container 1  . toString (  )  )  ;", "params . put ( CONTAINER _ LOG _ TYPE ,     \" syslog \"  )  ;", "Injector   injector    =    WebAppTests . testPage ( ContainerLogsPage . class ,    ContainerLogsPage . ContainersLogsBlock . class ,    cLogsBlock ,    params ,     (  ( Module [  ]  )     ( null )  )  )  ;", "PrintWriter   spyPw    =    WebAppTests . getPrintWriter ( injector )  ;", "verify ( spyPw )  . write (  (  (  \" Exception   reading   log   file .    Application   submitted   by    '  \"     +    user )     +     \"  '    doesn ' t   own   requested   log   file    :    syslog \"  )  )  ;", "}    finally    {", "if    ( syslog    !  =    null )     {", "syslog . delete (  )  ;", "}", "if    ( containerDir    !  =    null )     {", "containerDir . delete (  )  ;", "}", "if    ( appDir    !  =    null )     {", "appDir . delete (  )  ;", "}", "if    ( absLogDir    !  =    null )     {", "absLogDir . delete (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testContainerLogPageAccess"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestContainerLogsPage"}, {"methodBody": ["METHOD_START", "{", "TestNMWebServer . testRootDir . mkdirs (  )  ;", "TestNMWebServer . testLogDir . mkdir (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "Context   nmContext    =    new   NodeManager . NMContext ( null ,    null ,    null ,    null ,    null )  ;", "ResourceView   resourceView    =    new   ResourceView (  )     {", "@ Override", "public   long   getVmemAllocatedForContainers (  )     {", "return    0  ;", "}", "@ Override", "public   long   getPmemAllocatedForContainers (  )     {", "return    0  ;", "}", "@ Override", "public   long   getVCoresAllocatedForContainers (  )     {", "return    0  ;", "}", "@ Override", "public   boolean   isVmemCheckEnabled (  )     {", "return   true ;", "}", "@ Override", "public   boolean   isPmemCheckEnabled (  )     {", "return   true ;", "}", "}  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,    TestNMWebServer . testRootDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    TestNMWebServer . testLogDir . getAbsolutePath (  )  )  ;", "NodeHealthCheckerService   healthChecker    =    new   NodeHealthCheckerService (  )  ;", "healthChecker . init ( conf )  ;", "LocalDirsHandlerService   dirsHandler    =    healthChecker . getDiskHandler (  )  ;", "conf . set ( NM _ WEBAPP _ ADDRESS ,    webAddr )  ;", "WebServer   server    =    new   WebServer ( nmContext ,    resourceView ,    new   security . ApplicationACLsManager ( conf )  ,    dirsHandler )  ;", "try    {", "server . init ( conf )  ;", "server . start (  )  ;", "return   server . getPort (  )  ;", "}    finally    {", "server . stop (  )  ;", "healthChecker . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["startNMWebAppServer"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestNMWebServer . testRootDir )  ;", "FileUtil . fullyDelete ( TestNMWebServer . testLogDir )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "Context   nmContext    =    new   NodeManager . NMContext ( null ,    null ,    null ,    null ,    null )  ;", "ResourceView   resourceView    =    new   ResourceView (  )     {", "@ Override", "public   long   getVmemAllocatedForContainers (  )     {", "return    0  ;", "}", "@ Override", "public   long   getPmemAllocatedForContainers (  )     {", "return    0  ;", "}", "@ Override", "public   long   getVCoresAllocatedForContainers (  )     {", "return    0  ;", "}", "@ Override", "public   boolean   isVmemCheckEnabled (  )     {", "return   true ;", "}", "@ Override", "public   boolean   isPmemCheckEnabled (  )     {", "return   true ;", "}", "}  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( NM _ LOCAL _ DIRS ,    TestNMWebServer . testRootDir . getAbsolutePath (  )  )  ;", "conf . set ( NM _ LOG _ DIRS ,    TestNMWebServer . testLogDir . getAbsolutePath (  )  )  ;", "NodeHealthCheckerService   healthChecker    =    new   NodeHealthCheckerService (  )  ;", "healthChecker . init ( conf )  ;", "LocalDirsHandlerService   dirsHandler    =    healthChecker . getDiskHandler (  )  ;", "WebServer   server    =    new   WebServer ( nmContext ,    resourceView ,    new   security . ApplicationACLsManager ( conf )  ,    dirsHandler )  ;", "server . init ( conf )  ;", "server . start (  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( conf )  ;", "Dispatcher   dispatcher    =    new   AsyncDispatcher (  )  ;", "String   user    =     \" nobody \"  ;", "long   clusterTimeStamp    =     1  2  3  4  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId ( recordFactory ,    clusterTimeStamp ,     1  )  ;", "Application   app    =    mock ( Application . class )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "when ( app . getAppId (  )  )  . thenReturn ( appId )  ;", "nmContext . getApplications (  )  . put ( appId ,    app )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   container 1     =    BuilderUtils . newContainerId ( recordFactory ,    appId ,    appAttemptId ,     0  )  ;", "ContainerId   container 2     =    BuilderUtils . newContainerId ( recordFactory ,    appId ,    appAttemptId ,     1  )  ;", "NodeManagerMetrics   metrics    =    mock ( NodeManagerMetrics . class )  ;", "NMStateStoreService   stateStore    =    new   NMNullStateStoreService (  )  ;", "for    ( ContainerId   containerId    :    new   ContainerId [  ]  {    container 1  ,    container 2     }  )     {", "ContainerLaunchContext   launchContext    =    recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "long   currentTime    =    System . currentTimeMillis (  )  ;", "Token   containerToken    =    BuilderUtils . newContainerToken ( containerId ,     \"  1  2  7  .  0  .  0  .  1  \"  ,     1  2  3  4  ,    user ,    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ,     ( currentTime    +     1  0  0  0  0 L )  ,     1  2  3  ,     \" password \"  . getBytes (  )  ,    currentTime )  ;", "Container   container    =    new   ContainerImpl ( conf ,    dispatcher ,    stateStore ,    launchContext ,    null ,    metrics ,    BuilderUtils . newContainerTokenIdentifier ( containerToken )  )     {", "@ Override", "public   ContainerState   getContainerState (  )     {", "return   ContainerState . RUNNING ;", "}", "}  ;", "nmContext . getContainers (  )  . put ( containerId ,    container )  ;", "ApplicationId   applicationId    =    containerId . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "nmContext . getApplications (  )  . get ( applicationId )  . getContainers (  )  . put ( containerId ,    container )  ;", "writeContainerLogs ( nmContext ,    containerId ,    dirsHandler )  ;", "}", "}", "METHOD_END"], "methodName": ["testNMWebApp"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "int   port    =    startNMWebAppServer (  \"  0  .  0  .  0  .  0  :  0  \"  )  ;", "validatePortVal ( port )  ;", "}", "METHOD_END"], "methodName": ["testNMWebAppWithEphemeralPort"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "int   port    =    startNMWebAppServer (  \"  0  .  0  .  0  .  0  \"  )  ;", "validatePortVal ( port )  ;", "}", "METHOD_END"], "methodName": ["testNMWebAppWithOutPort"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "Assert . assertTrue (  \" Port   is   not   updated \"  ,     ( portVal    >     0  )  )  ;", "Assert . assertTrue (  (  \" Port   is   default    \"     +     ( YarnConfiguration . DEFAULT _ NM _ PORT )  )  ,     ( portVal    !  =     ( YarnConfiguration . DEFAULT _ NM _ PORT )  )  )  ;", "}", "METHOD_END"], "methodName": ["validatePortVal"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "File   containerLogDir    =    ContainerLogsUtils . getContainerLogDirs ( containerId ,    dirsHandler )  . get (  0  )  ;", "containerLogDir . mkdirs (  )  ;", "for    ( String   fileType    :    new   String [  ]  {     \" stdout \"  ,     \" stderr \"  ,     \" syslog \"     }  )     {", "Writriter    =    new   FileWriter ( new   File ( containerLogDir ,    fileType )  )  ;", "writrite (  (  (  (  ( ConverterUtils . toString ( containerId )  )     +     \"  \\ n   Hello    \"  )     +    fileType )     +     \"  !  \"  )  )  ;", "writer . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["writeContainerLogs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServer"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestNMWebServices . testRootDir )  ;", "FileUtil . fullyDelete ( TestNMWebServices . testLogDir )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "final   ContainerId   containerId    =    BuilderUtils . newContainerId (  0  ,     0  ,     0  ,     0  )  ;", "final   String   containerIdStr    =    BuilderUtils . newContainerId (  0  ,     0  ,     0  ,     0  )  . toString (  )  ;", "final   ApplicationAttemptId   appAttemptId    =    containerId . getApplicationAttemptId (  )  ;", "final   ApplicationId   appId    =    appAttemptId . getApplicationId (  )  ;", "final   String   appIdStr    =    appId . toString (  )  ;", "final   String   filename    =     \" logfile 1  \"  ;", "final   String   logMessage    =     \" log   message \\ n \"  ;", ". nmContext . getApplications (  )  . put ( appId ,    new   ApplicationImpl ( null ,     \" user \"  ,    appId ,    null ,     . nmContext )  )  ;", "MockContainer   container    =    new   MockContainer ( appAttemptId ,    new   AsyncDispatcher (  )  ,    new   Configuration (  )  ,     \" user \"  ,    appId ,     1  )  ;", "container . setState ( ContainerState . RUNNING )  ;", ". nmContext . getContainers (  )  . put ( containerId ,    container )  ;", "Path   path    =     . dirsHandler . getLogPathForWrite (  (  (  ( ContainerLaunch . getRelativeContainerLogDir ( appIdStr ,    containerIdStr )  )     +     \"  /  \"  )     +    filename )  ,    false )  ;", "File   logFile    =    new   File ( path . toUri (  )  . getPath (  )  )  ;", "logFile . deleteOnExit (  )  ;", "assertTrue (  \" Failed   to   create   log   dir \"  ,    logFile . getParentFile (  )  . mkdirs (  )  )  ;", "PrintWriter   pw    =    new   PrintWriter ( logFile )  ;", "pw . print ( logMessage )  ;", "pw . close (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containerlogs \"  )  . path ( containerIdStr )  . path ( filename )  . accept ( TEXT _ PLAIN )  . get ( ClientResponse . class )  ;", "String   responseText    =    response . getEntity ( String . class )  ;", "assertEquals ( logMessage ,    responseText )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containerlogs \"  )  . path ( containerIdStr )  . path (  \" uhhh \"  )  . accept ( TEXT _ PLAIN )  . get ( ClientResponse . class )  ;", "Assert . assertEquals ( NOT _ FOUND . getStatusCode (  )  ,    response . getStatus (  )  )  ;", "responseText    =    response . getEntity ( String . class )  ;", "assertTrue ( responseText . contains (  \" Cannot   find   this   log   on   the   local   disk .  \"  )  )  ;", ". nmContext . getContainers (  )  . remove ( containerId )  ;", "Assert . assertNull (  . nmContext . getContainers (  )  . get ( containerId )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containerlogs \"  )  . path ( containerIdStr )  . path ( filename )  . accept ( TEXT _ PLAIN )  . get ( ClientResponse . class )  ;", "responseText    =    response . getEntity ( String . class )  ;", "assertEquals ( logMessage ,    responseText )  ;", "}", "METHOD_END"], "methodName": ["testContainerLogs"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . accept ( TEXT _ PLAIN )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( INTERNAL _ SERVER _ ERROR ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidAccept"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" bogus \"  )  . accept ( APPLICATION _ JSON )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidUri"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . accept ( APPLICATION _ JSON )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidUri2"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testNode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testNodeDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" info \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" info \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testNodeInfoDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" info /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testNodeInfoSlash"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testNodeSlash"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" info /  \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   s    =    dom . getElementsByTagName (  \" Info \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    s . getLength (  )  )  ;", "verifyNodesXML ( s )  ;", "}", "METHOD_END"], "methodName": ["testSingleNodesXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   info    =    json . getJSONObject (  \" nodeInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  6  ,    info . length (  )  )  ;", "verifyNodeInfoGeneric ( info . getString (  \" id \"  )  ,    info . getString (  \" healthReport \"  )  ,    info . getLong (  \" totalVmemAllocatedContainersMB \"  )  ,    info . getLong (  \" totalPmemAllocatedContainersMB \"  )  ,    info . getLong (  \" totalVCoresAllocatedContainers \"  )  ,    info . getBoolean (  \" vmemCheckEnabled \"  )  ,    info . getBoolean (  \" pmemCheckEnabled \"  )  ,    info . getLong (  \" lastNodeUpdateTime \"  )  ,    info . getBoolean (  \" nodeHealthy \"  )  ,    info . getString (  \" nodeHostName \"  )  ,    info . getString (  \" VersionBuiltOn \"  )  ,    info . getString (  \" BuildVersion \"  )  ,    info . getString (  \" Version \"  )  ,    info . getString (  \" nodeManagerVersionBuiltOn \"  )  ,    info . getString (  \" nodeManagerBuildVersion \"  )  ,    info . getString (  \" nodeManagerVersion \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebServicesTestUtils . checkStringMatch (  \" id \"  ,     \" testhost . foo . com :  8  0  4  2  \"  ,    id )  ;", "WebServicesTestUtils . checkStringMatch (  \" healthReport \"  ,     \" Healthy \"  ,    healthReport )  ;", "assertEquals (  \" totalVmemAllocatedContainersMB   incorrect \"  ,     1  5  8  7  2  ,    totalVmemAllocatedContainersMB )  ;", "assertEquals (  \" totalPmemAllocatedContainersMB   incorrect \"  ,     1  6  3  8  4  ,    totalPmemAllocatedContainersMB )  ;", "assertEquals (  \" totalVCoresAllocatedContainers   incorrect \"  ,     4  0  0  0  ,    totalVCoresAllocatedContainers )  ;", "assertEquals (  \" vmemCheckEnabled   incorrect \"  ,    true ,    vmemCheckEnabled )  ;", "assertEquals (  \" pmemCheckEnabled   incorrect \"  ,    true ,    pmemCheckEnabled )  ;", "assertTrue (  \" lastNodeUpdateTime   incorrect \"  ,     ( lastNodeUpdateTime    =  =     (  . nmContext . getNodeHealthStatus (  )  . getLastHealthReportTime (  )  )  )  )  ;", "assertTrue (  \" nodeHealthy   isn ' t   true \"  ,    nodeHealthy )  ;", "WebServicesTestUtils . checkStringMatch (  \" nodeHostName \"  ,     \" testhost . foo . com \"  ,    nodeHostName )  ;", "WebServicesTestUtils . checkStringMatch (  \" hadoopVersionBuiltOn \"  ,    VersionInfo . getDate (  )  ,    hadoopVersionBuiltOn )  ;", "WebServicesTestUtils . checkStringEqual (  \" hadoopBuildVersion \"  ,    VersionInfo . getBuildVersion (  )  ,    hadoopBuildVersion )  ;", "WebServicesTestUtils . checkStringMatch (  \" hadoopVersion \"  ,    VersionInfo . getVersion (  )  ,    hadoopVersion )  ;", "WebServicesTestUtils . checkStringMatch (  \" resourceManagerVersionBuiltOn \"  ,    YarnVersionInfo . getDate (  )  ,    resourceManagerVersionBuiltOn )  ;", "WebServicesTestUtils . checkStringEqual (  \" resourceManagerBuildVersion \"  ,    YarnVersionInfo . getBuildVersion (  )  ,    resourceManagerBuildVersion )  ;", "WebServicesTestUtils . checkStringMatch (  \" resourceManagerVersion \"  ,    YarnVersionInfo . getVersion (  )  ,    resourceManagerVersion )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeInfoGeneric"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyNodeInfoGeneric ( TestUtils . getXmlString ( element ,     \" id \"  )  ,    TestUtils . getXmlString ( element ,     \" healthReport \"  )  ,    TestUtils . getXmlLong ( element ,     \" totalVmemAllocatedContainersMB \"  )  ,    TestUtils . getXmlLong ( element ,     \" totalPmemAllocatedContainersMB \"  )  ,    TestUtils . getXmlLong ( element ,     \" totalVCoresAllocatedContainers \"  )  ,    TestUtils . getXmlBoolean ( element ,     \" vmemCheckEnabled \"  )  ,    TestUtils . getXmlBoolean ( element ,     \" pmemCheckEnabled \"  )  ,    TestUtils . getXmlLong ( element ,     \" lastNodeUpdateTime \"  )  ,    TestUtils . getXmlBoolean ( element ,     \" nodeHealthy \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeHostName \"  )  ,    TestUtils . getXmlString ( element ,     \" hadoopVersionBuiltOn \"  )  ,    TestUtils . getXmlString ( element ,     \" hadoopBuildVersion \"  )  ,    TestUtils . getXmlString ( element ,     \" hadoopVersion \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeManagerVersionBuiltOn \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeManagerBuildVersion \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeManagerVersion \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyNodesXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServices"}, {"methodBody": ["METHOD_START", "{", "Dispatcher   dispatcher    =    new   AsyncDispatcher (  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( app . getAppId (  )  ,     1  )  ;", "Container   container 1     =    new   MockContainer ( appAttemptId ,    dispatcher ,     . conf ,    app . getUser (  )  ,    app . getAppId (  )  ,     1  )  ;", "Container   container 2     =    new   MockContainer ( appAttemptId ,    dispatcher ,     . conf ,    app . getUser (  )  ,    app . getAppId (  )  ,     2  )  ;", ". nmContext . getContainers (  )  . put ( container 1  . getContainerId (  )  ,    container 1  )  ;", ". nmContext . getContainers (  )  . put ( container 2  . getContainerId (  )  ,    container 2  )  ;", "app . getContainers (  )  . put ( container 1  . getContainerId (  )  ,    container 1  )  ;", "app . getContainers (  )  . put ( container 2  . getContainerId (  )  ,    container 2  )  ;", "HashMap < String ,    String >    hash    =    new   HashMap < String ,    String >  (  )  ;", "hash . put ( container 1  . getContainerId (  )  . toString (  )  ,    container 1  . getContainerId (  )  . toString (  )  )  ;", "hash . put ( container 2  . getContainerId (  )  . toString (  )  ,    container 2  . getContainerId (  )  . toString (  )  )  ;", "return   hash ;", "}", "METHOD_END"], "methodName": ["addAppContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestNMWebServicesApps . testRootDir )  ;", "FileUtil . fullyDelete ( TestNMWebServicesApps . testLogDir )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "testNodeHelper (  \" apps \"  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeApps"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "testNodeHelper (  \" apps /  \"  ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" apps \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" apps   isn ' t   NULL \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsNone"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "testNodeHelper (  \" apps /  \"  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsSlash"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "MockApp   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "HashMap < String ,    String >    hash 2     =    addAppContainers ( app 2  )  ;", "app 2  . setState ( ApplicationState . RUNNING )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,    ApplicationState . RUNNING . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   info    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "JSONArray   appInfo    =    info . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appInfo . length (  )  )  ;", "verifyNodeAppInfo ( appInfo . getJSONObject (  0  )  ,    app 2  ,    hash 2  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,     \" FOO _ STATE \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "verifyStateInvalidException ( message ,    type ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeAppsStateInvalid"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,     \" FOO _ STATE \"  )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "verifyStateInvalidException ( message ,    type ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeAppsStateInvalidDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,     \" FOO _ STATE \"  )  . accept ( APPLICATION _ XML )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   msg    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( msg )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" RemoteException \"  )  ;", "Element   element    =     (  ( Element )     ( nodes . item (  0  )  )  )  ;", "String   message    =    WebServicesTestUtils . getXmlString ( element ,     \" message \"  )  ;", "String   type    =    WebServicesTestUtils . getXmlString ( element ,     \" exception \"  )  ;", "String   classname    =    WebServicesTestUtils . getXmlString ( element ,     \" javaClassName \"  )  ;", "verifyStateInvalidException ( message ,    type ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeAppsStateInvalidXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,    ApplicationState . INITING . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" apps   is   not   null \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsStateNone"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" user \"  ,     \" mockUser \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   info    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "JSONArray   appInfo    =    info . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appInfo . length (  )  )  ;", "verifyNodeAppInfo ( appInfo . getJSONObject (  0  )  ,    app ,    hash )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", "TestNMWebServicesApps . nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", "TestNMWebServicesApps . nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" user \"  ,     \"  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    Error :    You   must   specify   a   non - empty   string   for   the   user \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" webapp . BadRequestException \"  ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeAppsUserEmpty"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  \" foo \"  ,     1  2  3  4  ,     2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . queryParam (  \" user \"  ,     \" george \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" apps   is   not   null \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsUserNone"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    nodes . getLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeAppsXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "HashMap < String ,    String >    hash 2     =    addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path ( path )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   info    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "JSONArray   appInfo    =    info . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    appInfo . length (  )  )  ;", "String   id    =    appInfo . getJSONObject (  0  )  . getString (  \" id \"  )  ;", "if    ( id . matches ( app . getAppId (  )  . toString (  )  )  )     {", "verifyNodeAppInfo ( appInfo . getJSONObject (  0  )  ,    app ,    hash )  ;", "verifyNodeAppInfo ( appInfo . getJSONObject (  1  )  ,    app 2  ,    hash 2  )  ;", "} else    {", "verifyNodeAppInfo ( appInfo . getJSONObject (  0  )  ,    app 2  ,    hash 2  )  ;", "verifyNodeAppInfo ( appInfo . getJSONObject (  1  )  ,    app ,    hash )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . path ( app . getAppId (  )  . toString (  )  )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeAppInfo ( json . getJSONObject (  \" app \"  )  ,    app ,    hash )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleAppHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "testNodeSingleAppHelper ( APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleApps"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "testNodeSingleAppHelper (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleAppsDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . path (  \" app _ foo _  0  0  0  0  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" For   input   string :     \\  \" foo \\  \"  \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NumberFormatException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" NumberFormatException \"  ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeSingleAppsInvalid"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", "TestNMWebServicesApps . nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", "TestNMWebServicesApps . nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . path (  \" application _  1  2  3  4  _  0  0  0  9  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    app   with   id   application _  1  2  3  4  _  0  0  0  9    not   found \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NotFoundException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" webapp . NotFoundException \"  ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeSingleAppsMissing"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . path (  (  ( app . getAppId (  )  . toString (  )  )     +     \"  /  \"  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeAppInfo ( json . getJSONObject (  \" app \"  )  ,    app ,    hash )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleAppsSlash"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" apps \"  )  . path (  (  ( app . getAppId (  )  . toString (  )  )     +     \"  /  \"  )  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "verifyNodeAppInfoXML ( nodes ,    app ,    hash )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleAppsXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     4  ,    info . length (  )  )  ;", "verifyNodeAppInfoGeneric ( app ,    info . getString (  \" id \"  )  ,    info . getString (  \" state \"  )  ,    info . getString (  \" user \"  )  )  ;", "JSONArray   containerids    =    info . getJSONArray (  \" containerids \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( containerids . length (  )  )  ;    i +  +  )     {", "String   id    =    containerids . getString ( i )  ;", "assertEquals (  (  \" extra   containerid :     \"     +    id )  ,    id ,    hash . remove ( id )  )  ;", "}", "assertTrue (  \" missing   containerids \"  ,    hash . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeAppInfo"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebServicesTestUtils . checkStringMatch (  \" id \"  ,    app . getAppId (  )  . toString (  )  ,    id )  ;", "WebServicesTestUtils . checkStringMatch (  \" state \"  ,    app . getApplicationState (  )  . toString (  )  ,    state )  ;", "WebServicesTestUtils . checkStringMatch (  \" user \"  ,    app . getUser (  )  . toString (  )  ,    user )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeAppInfoGeneric"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyNodeAppInfoGeneric ( app ,    TestUtils . getXmlString ( element ,     \" id \"  )  ,    TestUtils . getXmlString ( element ,     \" state \"  )  ,    TestUtils . getXmlString ( element ,     \" user \"  )  )  ;", "NodeList   ids    =    element . getElementsByTagName (  \" containerids \"  )  ;", "for    ( int   j    =     0  ;    j    <     ( ids . getLength (  )  )  ;    j +  +  )     {", "Element   line    =     (  ( Element )     ( ids . item ( j )  )  )  ;", "Node   first    =    line . getFirstChild (  )  ;", "String   val    =    first . getNodeValue (  )  ;", "assertEquals (  (  \" extra   containerid :     \"     +    val )  ,    val ,    hash . remove ( val )  )  ;", "}", "assertTrue (  \" missing   containerids \"  ,    hash . isEmpty (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyNodeAppInfoXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" FOO _ STATE \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" IllegalArgumentException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" IllegalArgumentException \"  ,    classname )  ;", "}", "METHOD_END"], "methodName": ["verifyStateInvalidException"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "Dispatcher   dispatcher    =    new   AsyncDispatcher (  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( app . getAppId (  )  ,     1  )  ;", "Container   container 1     =    new   MockContainer ( appAttemptId ,    dispatcher ,     . conf ,    app . getUser (  )  ,    app . getAppId (  )  ,     1  )  ;", "Container   container 2     =    new   MockContainer ( appAttemptId ,    dispatcher ,     . conf ,    app . getUser (  )  ,    app . getAppId (  )  ,     2  )  ;", ". nmContext . getContainers (  )  . put ( container 1  . getContainerId (  )  ,    container 1  )  ;", ". nmContext . getContainers (  )  . put ( container 2  . getContainerId (  )  ,    container 2  )  ;", "app . getContainers (  )  . put ( container 1  . getContainerId (  )  ,    container 1  )  ;", "app . getContainers (  )  . put ( container 2  . getContainerId (  )  ,    container 2  )  ;", "HashMap < String ,    String >    hash    =    new   HashMap < String ,    String >  (  )  ;", "hash . put ( container 1  . getContainerId (  )  . toString (  )  ,    container 1  . getContainerId (  )  . toString (  )  )  ;", "hash . put ( container 2  . getContainerId (  )  . toString (  )  ,    container 2  . getContainerId (  )  . toString (  )  )  ;", "return   hash ;", "}", "METHOD_END"], "methodName": ["addAppContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( TestNMWebServicesContainers . testRootDir )  ;", "FileUtil . fullyDelete ( TestNMWebServicesContainers . testLogDir )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containers \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" container \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     4  ,    nodes . getLength (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeContainerXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "testNodeHelper (  \" containers \"  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "testNodeHelper (  \" containers /  \"  ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodeContainersDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" c \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" apps   isn ' t   NULL \"  ,    NULL ,    json . get (  \" c \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeContainersNone"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "testNodeHelper (  \" containers /  \"  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeContainersSlash"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path ( path )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   info    =    json . getJSONObject (  \" containers \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "JSONArray   conInfo    =    info . getJSONArray (  \" container \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     4  ,    conInfo . length (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( conInfo . length (  )  )  ;    i +  +  )     {", "verifyNodeContainerInfo ( conInfo . getJSONObject ( i )  ,     . nmContext . getContainers (  )  . get ( ConverterUtils . toContainerId ( conInfo . getJSONObject ( i )  . getString (  \" id \"  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "for    ( String   id    :    hash . keySet (  )  )     {", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containers \"  )  . path ( id )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" container \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "verifyContainersInfoXML ( nodes ,     . nmContext . getContainers (  )  . get ( ConverterUtils . toContainerId ( id )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeSingleContainerXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "testNodeSingleContainersHelper ( APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "testNodeSingleContainersHelper (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleContainersDefault"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "HashMap < String ,    String >    hash    =    addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "for    ( String   id    :    hash . keySet (  )  )     {", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containers \"  )  . path ( id )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyNodeContainerInfo ( json . getJSONObject (  \" container \"  )  ,     . nmContext . getContainers (  )  . get ( ConverterUtils . toContainerId ( id )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeSingleContainersHelper"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "testNodeSingleContainersHelper ( APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodeSingleContainersSlash"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containers \"  )  . path (  \" container _ foo _  1  2  3  4  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    invalid   container   id ,    container _ foo _  1  2  3  4  \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" BadRequestException \"  ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleContainerInvalid"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containers \"  )  . path (  \" container _  1  2  3  4  _  0  0  0  1  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    invalid   container   id ,    container _  1  2  3  4  _  0  0  0  1  \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" BadRequestException \"  ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleContainerInvalid2"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "Application   app    =    new   MockApp (  1  )  ;", ". nmContext . getApplications (  )  . put ( app . getAppId (  )  ,    app )  ;", "addAppContainers ( app )  ;", "Application   app 2     =    new   MockApp (  2  )  ;", ". nmContext . getApplications (  )  . put ( app 2  . getAppId (  )  ,    app 2  )  ;", "addAppContainers ( app 2  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" node \"  )  . path (  \" containers \"  )  . path (  \" container _  1  2  3  4  _  0  0  0  1  _  0  1  _  0  0  0  0  0  5  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   user   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    container   with   id ,    container _  1  2  3  4  _  0  0  0  1  _  0  1  _  0  0  0  0  0  5  ,    not   found \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NotFoundException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" NotFoundException \"  ,    classname )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleContainerWrong"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyNodeContainerInfoGeneric ( cont ,    TestUtils . getXmlString ( element ,     \" id \"  )  ,    TestUtils . getXmlString ( element ,     \" state \"  )  ,    TestUtils . getXmlString ( element ,     \" user \"  )  ,    TestUtils . getXmlInt ( element ,     \" exitCode \"  )  ,    TestUtils . getXmlString ( element ,     \" diagnostics \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeId \"  )  ,    TestUtils . getXmlInt ( element ,     \" totalMemoryNeededMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" totalVCoresNeeded \"  )  ,    TestUtils . getXmlString ( element ,     \" containerLogsLink \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyContainersInfoXML"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     9  ,    info . length (  )  )  ;", "verifyNodeInfoGeneric ( cont ,    info . getString (  \" id \"  )  ,    info . getString (  \" state \"  )  ,    info . getString (  \" user \"  )  ,    info . getInt (  \" exitCode \"  )  ,    info . getString (  \" diagnostics \"  )  ,    info . getString (  \" nodeId \"  )  ,    info . getInt (  \" totalMemoryNeededMB \"  )  ,    info . getInt (  \" totalVCoresNeeded \"  )  ,    info . getString (  \" containerLogsLink \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeContainerInfo"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "WebServicesTestUtils . checkStringMatch (  \" id \"  ,    cont . getContainerId (  )  . toString (  )  ,    id )  ;", "WebServicesTestUtils . checkStringMatch (  \" state \"  ,    cont . getContainerState (  )  . toString (  )  ,    state )  ;", "WebServicesTestUtils . checkStringMatch (  \" user \"  ,    cont . getUser (  )  . toString (  )  ,    user )  ;", "assertEquals (  \" exitCode   wrong \"  ,     0  ,    exitCode )  ;", "WebServicesTestUtils . checkStringMatch (  \" diagnostics \"  ,     \" testing \"  ,    diagnostics )  ;", "WebServicesTestUtils . checkStringMatch (  \" nodeId \"  ,     . nmContext . getNodeId (  )  . toString (  )  ,    nodeId )  ;", "assertEquals (  \" totalMemoryNeededMB   wrong \"  ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    totalMemoryNeededMB )  ;", "assertEquals (  \" totalVCoresNeeded   wrong \"  ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,    totalVCoresNeeded )  ;", "String   shortLink    =    ujoin (  \" containerlogs \"  ,    cont . getContainerId (  )  . toString (  )  ,    cont . getUser (  )  )  ;", "assertTrue (  \" containerLogsLink   wrong \"  ,    logsLink . contains ( shortLink )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeContainerInfoGeneric"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.TestNMWebServicesContainers"}, {"methodBody": ["METHOD_START", "{", "return   this . port ;", "}", "METHOD_END"], "methodName": ["getPort"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer"}, {"methodBody": ["METHOD_START", "{", "return   this . containerids ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . state ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "app . add ( appInfo )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.AppsInfo"}, {"methodBody": ["METHOD_START", "{", "return   app ;", "}", "METHOD_END"], "methodName": ["getApps"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.AppsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnostics ;", "}", "METHOD_END"], "methodName": ["getDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . exitCode ;", "}", "METHOD_END"], "methodName": ["getExitCode"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . exitStatus ;", "}", "METHOD_END"], "methodName": ["getExitStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . containerLogsLink ;", "}", "METHOD_END"], "methodName": ["getLogLink"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalMemoryNeededMB ;", "}", "METHOD_END"], "methodName": ["getMemoryNeeded"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeId ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . containerLogsShortLink ;", "}", "METHOD_END"], "methodName": ["getShortLogLink"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . state ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalVCoresNeeded ;", "}", "METHOD_END"], "methodName": ["getVCoresNeeded"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "container . add ( containerInfo )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainersInfo"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.ContainersInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . hadoopBuildVersion ;", "}", "METHOD_END"], "methodName": ["getHadoopBuildVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . hadoopVersion ;", "}", "METHOD_END"], "methodName": ["getHadoopVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . hadoopVersionBuiltOn ;", "}", "METHOD_END"], "methodName": ["getHadoopVersionBuiltOn"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . healthReport ;", "}", "METHOD_END"], "methodName": ["getHealthReport"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeHealthy ;", "}", "METHOD_END"], "methodName": ["getHealthStatus"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . lastNodeUpdateTime ;", "}", "METHOD_END"], "methodName": ["getLastNodeUpdateTime"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeManagerBuildVersion ;", "}", "METHOD_END"], "methodName": ["getNMBuildVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeManagerVersion ;", "}", "METHOD_END"], "methodName": ["getNMVersion"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeManagerVersionBuiltOn ;", "}", "METHOD_END"], "methodName": ["getNMVersionBuiltOn"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeHostName ;", "}", "METHOD_END"], "methodName": ["getNodeHostName"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalPmemAllocatedContainersMB ;", "}", "METHOD_END"], "methodName": ["getTotalPmemAllocated"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalVCoresAllocatedContainers ;", "}", "METHOD_END"], "methodName": ["getTotalVCoresAllocated"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalVmemAllocatedContainersMB ;", "}", "METHOD_END"], "methodName": ["getTotalVmemAllocated"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . pmemCheckEnabled ;", "}", "METHOD_END"], "methodName": ["isPmemCheckEnabled"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . vmemCheckEnabled ;", "}", "METHOD_END"], "methodName": ["isVmemCheckEnabled"], "fileName": "org.apache.hadoop.yarn.server.nodemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return    ( getMajorVersion (  )  )     =  =     ( version . getMajorVersion (  )  )  ;", "}", "METHOD_END"], "methodName": ["isCompatibleTo"], "fileName": "org.apache.hadoop.yarn.server.records.Version"}, {"methodBody": ["METHOD_START", "{", "Version   version    =    Records . newRecord ( Version . class )  ;", "version . setMajorVersion ( majorVersion )  ;", "version . setMinorVersion ( minorVersion )  ;", "return   version ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.records.Version"}, {"methodBody": ["METHOD_START", "{", "return    (  ( getMajorVersion (  )  )     +     \"  .  \"  )     +     ( getMinorVersion (  )  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.server.records.Version"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.records.impl.pb.VersionPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.records.impl.pb.VersionPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RMServerUtils . verifyAccess ( adminAcl ,    method ,    AdminService . LOG )  ;", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   ckAccess ( method )  ;", "}    catch    ( IOException   ioe )     {", "throw   RPCUtil . getRemoteException ( ioe )  ;", "}", "}", "METHOD_END"], "methodName": ["checkAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "switch    ( req . getSource (  )  )     {", "case   REQUEST _ BY _ USER    :", "if    ( autoFailoverEnabled )     {", "throw   new   AccessControlException (  (  \" Manual   failover   for   this   ResourceManager   is   disallowed ,     \"     +     \" because   automatic   failover   is   enabled .  \"  )  )  ;", "}", "break ;", "case   REQUEST _ BY _ USER _ FORCED    :", "if    ( autoFailoverEnabled )     {", ". LOG . warn (  (  (  (  \" Allowing   manual   failover   from    \"     +     ( Server . getRemoteAddress (  )  )  )     +     \"    even   though   automatic   failover   is   enabled ,    because   the   user    \"  )     +     \" specified   the   force   flag \"  )  )  ;", "}", "break ;", "case   REQUEST _ BY _ ZKFC    :", "if    (  !  ( autoFailoverEnabled )  )     {", "throw   new   AccessControlException (  (  (  (  \" Request   from   ZK   failover   controller   at    \"     +     ( Server . getRemoteAddress (  )  )  )     +     \"    denied    \"  )     +     \" since   automatic   failover   is   not   enabled \"  )  )  ;", "}", "break ;", "}", "}", "METHOD_END"], "methodName": ["checkHaStateChange"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "return   new   EmbeddedElectorService ( rmContext )  ;", "}", "METHOD_END"], "methodName": ["createEmbeddedElectorService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "return   this . adminAcl ;", "}", "METHOD_END"], "methodName": ["getAccessControlList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "InputStream   confFileInputStream    =    this . rmContext . getConfigurationProvider (  )  . getConfigurationInputStream ( conf ,    confFileName )  ;", "if    ( confFileInputStream    !  =    null )     {", "conf . addR ( confFileInputStream )  ;", "}", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "return   this . server ;", "}", "METHOD_END"], "methodName": ["getServer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "return    ( HAServiceState . ACTIVE )     =  =     ( rmContext . getHAServiceState (  )  )  ;", "}", "METHOD_END"], "methodName": ["isRMActive"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "String   argName    =     \" refreshAdminAcls \"  ;", "UserGroupInformation   user    =    checkAcls ( argName )  ;", "if    ( checkRMHAState    &  &     (  !  ( isRMActive (  )  )  )  )     {", "RMAuditLogger . logFailure ( user . getShortUserName (  )  ,    argName ,    adminAcl . toString (  )  ,     \" AdminService \"  ,     \" ResourceManager   is   not   active .    Can   not   refresh   user - groups .  \"  )  ;", "throwStandbyException (  )  ;", "}", "Configuration   conf    =    getConfiguration ( new   Configuration ( false )  ,    YARN _ SITE _ CONFIGURATION _ FILE )  ;", "adminAcl    =    new   security . authorize . AccessControlList ( conf . get ( YARN _ ADMIN _ ACL ,    DEFAULT _ YARN _ ADMIN _ ACL )  )  ;", "RMAuditLogger . logSuccess ( user . getShortUserName (  )  ,    argName ,     \" AdminService \"  )  ;", "return   recordFactory . newRecordInstance ( RefreshAdminAclsResponse . class )  ;", "}", "METHOD_END"], "methodName": ["refreshAdminAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "try    {", "refreshQueues ( RefreshQueuesRequest . newInstance (  )  )  ;", "refreshNodes ( RefreshNodesRequest . newInstance (  )  )  ;", "refreshSuperUserGroupsConfiguration ( RefreshSuperUserGroupsConfigurationRequest . newInstance (  )  )  ;", "refreshUserToGroupsMappings ( RefreshUserToGroupsMappingsRequest . newInstance (  )  )  ;", "if    ( getConfig (  )  . getBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    false )  )     {", "refreshAcls ( RefreshAclsRequest . newInstance (  )  )  ;", "}", "}    catch    ( Exception   ex )     {", "throw   new   FailedException ( ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["refreshAll"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "this . server . refreshServiceAclWithLoadedConfiguration ( configuration ,    policyProvider )  ;", "}", "METHOD_END"], "methodName": ["refreshServiceAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( embeddedElector )     !  =    null )     {", "embeddedElectoetLeaderElection (  )  ;", "}", "}", "METHOD_END"], "methodName": ["resetLeaderElection"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "this =     (  ( Server )     ( rpc . getServer ( ResourceManagerAdministrationProtocol . class ,    this ,    masterServiceBindAddress ,    conf ,    null ,    conf . getInt ( RM _ ADMIN _ CLIENT _ THREAD _ COUNT ,    DEFAULT _ RM _ ADMIN _ CLIENT _ THREAD _ COUNT )  )  )  )  ;", "if    ( conf . getBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    false )  )     {", "refreshServiceAcls ( getConfiguration ( conf ,    HADOOP _ POLICY _ CONFIGURATION _ FILE )  ,    RMPolicyProvider . getInstance (  )  )  ;", "}", "if    ( rmContext . isHAEnabled (  )  )     {", "RPC . setProtocolEngine ( conf ,    HAServiceProtocolPB . class ,    ProtobufRpcEngine . class )  ;", "HAServiceProtocolServerSideTranslatorPB   haServiceProtocolXlator    =    new   HAServiceProtocolServerSideTranslatorPB ( this )  ;", "BlockingService   haPbService    =    HAServiceProtocolService . newReflectiveBlockingService ( haServiceProtocolXlator )  ;", "addProtocol ( RPC _ PROTOCOL _ BUFFER ,    HAServiceProtocol . class ,    haPbService )  ;", "}", "thisstart (  )  ;", "conf . updateConnectAddr ( RM _ BIND _ HOST ,    RM _ ADMIN _ ADDRESS ,    DEFAULT _ RM _ ADMIN _ ADDRESS , getListenerAddress (  )  )  ;", "}", "METHOD_END"], "methodName": ["startServer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this    !  =    null )     {", "thisstop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stopServer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "throw   new   StandbyException (  (  (  \" ResourceManager    \"     +     ( rmId )  )     +     \"    is   not   Active !  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["throwStandbyException"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.AdminService"}, {"methodBody": ["METHOD_START", "{", "nodes . put (  (  ( host    +     \"  :  \"  )     +    containerManagerPort )  ,    nodeManager )  ;", "}", "METHOD_END"], "methodName": ["addNodeManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    requests . get ( resourceName )  ;", "if    ( request    =  =    null )     {", "request    =    BuilderUtils . newResourceRequest ( priority ,    resourceName ,    capability ,     1  )  ;", "requests . put ( resourceName ,    request )  ;", "} else    {", "request . setNumContainers (  (  ( request . getNumContainers (  )  )     +     1  )  )  ;", "}", "ask . remove ( request )  ;", "ask . add ( BuilderUtils . newResourceRequest ( request )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  \" addResourceRequest :    applicationId =  \"     +     ( applicationId . getId (  )  )  )     +     \"    priority =  \"  )     +     ( priority . getPriority (  )  )  )     +     \"    resourceName =  \"  )     +    resourceName )     +     \"    capability =  \"  )     +    capability )     +     \"    numContainers =  \"  )     +     ( request . getNumContainers (  )  )  )     +     \"     # asks =  \"  )     +     ( ask . size (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "Resource   currentSpec    =    requestSpec . put ( priority ,    capability )  ;", "if    ( currentSpec    !  =    null )     {", "throw   new   IllegalStateException (  (  (  (  (  \" Resource   spec   already   exists   for    \"     +     \" priority    \"  )     +     ( priority . getPriority (  )  )  )     +     \"     -     \"  )     +     ( currentSpec . getMemory (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addResourceRequestSpec"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "Priority   priority    =    task . getPriority (  )  ;", "Map < String ,    ResourceRequest >    requests    =    this . requests . get ( priority )  ;", "if    ( requests    =  =    null )     {", "requests    =    new   HashMap < String ,    ResourceRequest >  (  )  ;", "this . requests . put ( priority ,    requests )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Added   priority =  \"     +    priority )     +     \"    application =  \"  )     +     ( applicationId )  )  )  ;", "}", "}", "final   Resource   capability    =    requestSpec . get ( priority )  ;", "Set < Task >    tasks    =    this . tasks . get ( priority )  ;", "if    ( tasks    =  =    null )     {", "tasks    =    new   HashSet < Task >  (  )  ;", "this . tasks . put ( priority ,    tasks )  ;", "}", "tasks . add ( task )  ;", ". LOG . info (  (  (  (  (  (  \" Added   task    \"     +     ( task . getTaskId (  )  )  )     +     \"    to   application    \"  )     +     ( applicationId )  )     +     \"    at   priority    \"  )     +    priority )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" addTask :    application =  \"     +     ( applicationId )  )     +     \"     # asks =  \"  )     +     ( ask . size (  )  )  )  )  ;", "}", "for    ( String   host    :    task . getHosts (  )  )     {", "addResourceRequest ( priority ,    requests ,    host ,    capability )  ;", "}", "for    ( String   rack    :    task . getRacks (  )  )     {", "addResourceRequest ( priority ,    requests ,    rack ,    capability )  ;", "}", "addResourceRequest ( priority ,    requests ,    ANY ,    capability )  ;", "}", "METHOD_END"], "methodName": ["addTask"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "int   numContainers    =    containers . size (  )  ;", "for    ( Priority   priority    :    requests . keySet (  )  )     {", "assign ( priority ,    NodeType . NODE _ LOCAL ,    containers )  ;", "assign ( priority ,    NodeType . RACK _ LOCAL ,    containers )  ;", "assign ( priority ,    NodeType . OFF _ SWITCH ,    containers )  ;", "if    ( containers . isEmpty (  )  )     {", "break ;", "}", "}", "int   assignedContainers    =    numContainers    -     ( containers . size (  )  )  ;", ". LOG . info (  (  (  (  (  (  \"     \"     +     ( applicationId )  )     +     \"    assigned    \"  )     +    assignedContainers )     +     \"  /  \"  )     +    numContainers )  )  ;", "}", "METHOD_END"], "methodName": ["assign"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "for    ( Iterator < Container >    i    =    containers . iterator (  )  ;    i . hasNext (  )  ;  )     {", "Container   container    =    i . next (  )  ;", "String   host    =    container . getNodeId (  )  . toString (  )  ;", "if    ( Resources . equals ( requestSpec . get ( priority )  ,    container . getResource (  )  )  )     {", "for    ( Iterator < Task >    t    =    tasks . get ( priority )  . iterator (  )  ;    t . hasNext (  )  ;  )     {", "Task   task    =    t . next (  )  ;", "if    (  (  ( task . getState (  )  )     =  =     ( Task . State . PENDING )  )     &  &     ( task . canSchedule ( type ,    host )  )  )     {", "NodeManager   nodeManager    =    getNodeManager ( host )  ;", "task . start ( nodeManager ,    container . getId (  )  )  ;", "i . remove (  )  ;", "Resources . addTo ( used ,    container . getResource (  )  )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  \" Assigned   container    (  \"     +    container )     +     \"  )    of   type    \"  )     +    type )     +     \"    to   task    \"  )     +     ( task . getTaskId (  )  )  )     +     \"    at   priority    \"  )     +    priority )     +     \"    on   node    \"  )     +     ( nodeManager . getHostName (  )  )  )     +     \"  ,    currently   using    \"  )     +     ( used )  )     +     \"    resources \"  )  )  ;", "updateResourceRequests ( requests . get ( priority )  ,    type ,    task )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( createCLC (  )  ,    container . getContainerToken (  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "nodeManager . startContainers ( allRequests )  ;", "break ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["assign"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunchContext   clc    =    Application . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "return   clc ;", "}", "METHOD_END"], "methodName": ["createCLC"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "Set < Task >    tasks    =    this . tasks . get ( task . getPriority (  )  )  ;", "if    (  !  ( tasks . remove ( task )  )  )     {", "throw   new   IllegalStateException (  (  (  (  \" Finishing   unknown   task    \"     +     ( task . getTaskId (  )  )  )     +     \"    from   application    \"  )     +     ( applicationId )  )  )  ;", "}", "NodeManager   nodeManager    =    task . getNodeManager (  )  ;", "ContainerId   containerId    =    task . getContainerId (  )  ;", "task . stop (  )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( containerId )  ;", "StopContainersRequest   stopRequest    =    StopContainersRequest . newInstance ( containerIds )  ;", "nodeManager . stopContainers ( stopRequest )  ;", "Resources . subtractFrom ( used ,    requestSpec . get ( task . getPriority (  )  )  )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  \" Finished   task    \"     +     ( task . getTaskId (  )  )  )     +     \"    of   application    \"  )     +     ( applicationId )  )     +     \"    on   node    \"  )     +     ( nodeManager . getHostName (  )  )  )     +     \"  ,    currently   using    \"  )     +     ( used )  )     +     \"    resources \"  )  )  ;", "}", "METHOD_END"], "methodName": ["finishTask"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   taskCounter . incrementAndGet (  )  ;", "}", "METHOD_END"], "methodName": ["getNextTaskId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   nodes . get ( host )  ;", "}", "METHOD_END"], "methodName": ["getNodeManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "if    ( Application . LOG . isDebugEnabled (  )  )     {", "Application . LOG . debug (  (  (  (  (  \" getResources   begin :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"     # ask =  \"  )     +     ( ask . size (  )  )  )  )  ;", "for    ( ResourceRequest   request    :    ask )     {", "Application . LOG . debug (  (  (  (  (  \" getResources :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"    ask - request =  \"  )     +    request )  )  ;", "}", "}", "Allocation   allocation    =    resourceManager . getResourceScheduler (  )  . allocate ( applicationAttemptId ,    new   ArrayList < ResourceRequest >  ( ask )  ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "System . out . println (  (  \"  -  =  =  =  =  =  =  =  \"     +     ( applicationAttemptId )  )  )  ;", "System . out . println (  (  \"  -  -  -  -  -  -  -  -  -  -  \"     +     ( resourceManager . getRMContext (  )  . getRMApps (  )  . get ( applicationId )  . getRMAppAttempt ( applicationAttemptId )  )  )  )  ;", "List < Container >    containers    =    allocation . getContainers (  )  ;", "ask . clear (  )  ;", "if    ( Application . LOG . isDebugEnabled (  )  )     {", "Application . LOG . debug (  (  (  (  (  (  (  \" getResources (  )    for    \"     +     ( applicationId )  )     +     \"  :  \"  )     +     \"    ask =  \"  )     +     ( ask . size (  )  )  )     +     \"    recieved =  \"  )     +     ( containers . size (  )  )  )  )  ;", "}", "return   containers ;", "}", "METHOD_END"], "methodName": ["getResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   used ;", "}", "METHOD_END"], "methodName": ["getUsedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "return   NetworkTopology . DEFAULT _ RACK ;", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "assign ( getResources (  )  )  ;", "}", "METHOD_END"], "methodName": ["schedule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   context    =    Application . recordFactory . newRecordInstance ( ApplicationSubmissionContext . class )  ;", "context . setApplicationId ( this . applicationId )  ;", "context . setQueue ( this . queue )  ;", "ContainerLaunchContext   amContainer    =    Records . newRecord ( ContainerLaunchContext . class )  ;", "context . setAMContainerSpec ( amContainer )  ;", "context . setResource ( Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )  ;", "SubmitApplicationRequest   request    =    Application . recordFactory . newRecordInstance ( SubmitApplicationRequest . class )  ;", "request . setApplicationSubmissionContext ( context )  ;", "final   ResourceScheduler   scheduler    =    resourceManager . getResourceScheduler (  )  ;", "resourceManager . getClientRMService (  )  . submitApplication ( request )  ;", "AppAddedSchedulerEvent   addAppEvent    =    new   AppAddedSchedulerEvent ( this . applicationId ,    this . queue ,     \" user \"  )  ;", "scheduler . handle ( addAppEvent )  ;", "AppAttemptAddedSchedulerEvent   addAttemptEvent    =    new   AppAttemptAddedSchedulerEvent ( this . applicationAttemptId ,    false )  ;", "scheduler . handle ( addAttemptEvent )  ;", "}", "METHOD_END"], "methodName": ["submit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "request . setNumContainers (  (  ( request . getNumContainers (  )  )     -     1  )  )  ;", "ask . remove ( request )  ;", "ask . add ( BuilderUtils . newResourceRequest ( request )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  \" updateResourceRequest :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"    request =  \"  )     +    request )  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "if    ( type    =  =     ( NodeType . NODE _ LOCAL )  )     {", "for    ( String   host    :    task . getHosts (  )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  \" updateResourceRequests :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"    type =  \"  )     +    type )     +     \"    host =  \"  )     +    host )     +     \"    request =  \"  )     +     ( requests    =  =    null    ?     \" null \"     :    requests . get ( host )  )  )  )  ;", "}", "updateResourceRequest ( requests . get ( host )  )  ;", "}", "}", "if    (  ( type    =  =     ( NodeType . NODE _ LOCAL )  )     |  |     ( type    =  =     ( NodeType . RACK _ LOCAL )  )  )     {", "for    ( String   rack    :    task . getRacks (  )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  \" updateResourceRequests :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"    type =  \"  )     +    type )     +     \"    rack =  \"  )     +    rack )     +     \"    request =  \"  )     +     ( requests    =  =    null    ?     \" null \"     :    requests . get ( rack )  )  )  )  ;", "}", "updateResourceRequest ( requests . get ( rack )  )  ;", "}", "}", "updateResourceRequest ( requests . get ( ANY )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  \" updateResourceRequests :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"     # asks =  \"  )     +     ( ask . size (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Application"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   remoteUgi ;", "try    {", "remoteUgi    =    UserGroupInformation . getCurrentUser (  )  ;", "}    catch    ( IOException   e )     {", "String   msg    =     (  \" Cannot   obtain   the   user - name   for   authorizing   ApplicationMaster .     \"     +     \" Got   exception :     \"  )     +     ( StringUtils . stringifyException ( e )  )  ;", ". LOG . warn ( msg )  ;", "throw   RPCUtil . getRemoteException ( msg )  ;", "}", "boolean   tokenFound    =    false ;", "String   message    =     \"  \"  ;", "AMRMTokenIdentifier   appTokenIdentifier    =    null ;", "try    {", "appTokenIdentifier    =    selectAMRMTokenIdentifier ( remoteUgi )  ;", "if    ( appTokenIdentifier    =  =    null )     {", "tokenFound    =    false ;", "message    =     \" No   AMRMToken   found   for   user    \"     +     ( remoteUgi . getUserName (  )  )  ;", "} else    {", "tokenFound    =    true ;", "}", "}    catch    ( IOException   e )     {", "tokenFound    =    false ;", "message    =     \" Got   exception   while   looking   for   AMRMToken   for   user    \"     +     ( remoteUgi . getUserName (  )  )  ;", "}", "if    (  ! tokenFound )     {", ". LOG . warn ( message )  ;", "throw   RPCUtil . getRemoteException ( message )  ;", "}", "return   appTokenIdentifier ;", "}", "METHOD_END"], "methodName": ["authorizeRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "PreemptionMessage   pMsg    =    null ;", "if    (  ( allocation . getStrictContainerPreemptions (  )  )     !  =    null )     {", "pMsg    =    recordFactory . newRecordInstance ( PreemptionMessage . class )  ;", "StrictPreemptionContract   pStrict    =    recordFactory . newRecordInstance ( StrictPreemptionContract . class )  ;", "Set < PreemptionContainer >    pCont    =    new   HashSet < PreemptionContainer >  (  )  ;", "for    ( ContainerId   cId    :    allocation . getStrictContainerPreemptions (  )  )     {", "PreemptionContainer   pc    =    recordFactory . newRecordInstance ( PreemptionContainer . class )  ;", "pc . setId ( cId )  ;", "pCont . add ( pc )  ;", "}", "pStrict . setContainers ( pCont )  ;", "pMsg . setStrictContract ( pStrict )  ;", "}", "if    (  (  (  (  ( allocation . getRPreemptions (  )  )     !  =    null )     &  &     (  ( allocation . getRPreemptions (  )  . size (  )  )     >     0  )  )     &  &     (  ( allocation . getContainerPreemptions (  )  )     !  =    null )  )     &  &     (  ( allocation . getContainerPreemptions (  )  . size (  )  )     >     0  )  )     {", "if    ( pMsg    =  =    null )     {", "pMsg    =    recordFactory . newRecordInstance ( PreemptionMessage . class )  ;", "}", "PreemptionContract   contract    =    recordFactory . newRecordInstance ( PreemptionContract . class )  ;", "Set < PreemptionContainer >    pCont    =    new   HashSet < PreemptionContainer >  (  )  ;", "for    ( ContainerId   cId    :    allocation . getContainerPreemptions (  )  )     {", "PreemptionContainer   pc    =    recordFactory . newRecordInstance ( PreemptionContainer . class )  ;", "pc . setId ( cId )  ;", "pCont . add ( pc )  ;", "}", "List < PreemptionRRequest >    pRes    =    new   ArrayList < PreemptionRRequest >  (  )  ;", "for    ( RRequest   crr    :    allocation . getRPreemptions (  )  )     {", "PreemptionRRequest   prr    =    recordFactory . newRecordInstance ( PreemptionRRequest . class )  ;", "prr . setRRequest ( crr )  ;", "pRes . add ( prr )  ;", "}", "contract . setContainers ( pCont )  ;", "contract . setRRequest ( pRes )  ;", "pMsg . setContract ( contract )  ;", "}", "return   pMsg ;", "}", "METHOD_END"], "methodName": ["generatePreemptionMessage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "return   this . bindAddress ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "return   this . server ;", "}", "METHOD_END"], "methodName": ["getServer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "boolean   hasApplicationMasterRegistered    =    false ;", ". AllocateResponseLock   lastResponse    =    responseMap . get ( appAttemptId )  ;", "if    ( lastResponse    !  =    null )     {", "synchronized ( lastResponse )     {", "if    (  (  ( lastResponse . getAllocateResponse (  )  )     !  =    null )     &  &     (  ( lastResponse . getAllocateResponse (  )  . getResponseId (  )  )     >  =     0  )  )     {", "hasApplicationMasterRegistered    =    true ;", "}", "}", "}", "return   hasApplicationMasterRegistered ;", "}", "METHOD_END"], "methodName": ["hasApplicationMasterRegistered"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "this . server . refreshServiceAclWithLoadedConfiguration ( configuration ,    policyProvider )  ;", "}", "METHOD_END"], "methodName": ["refreshServiceAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "AllocateResponse   response    =    recordFactory . newRecordInstance ( AllocateResponse . class )  ;", "response . setResponseId (  (  -  1  )  )  ;", ". LOG . info (  (  \" Registering   app   attempt    :     \"     +    attemptId )  )  ;", "responseMap . put ( attemptId ,    new    . AllocateResponseLock ( response )  )  ;", "rmContext . getNMTokenSecretManager (  )  . registerApplicationAttempt ( attemptId )  ;", "}", "METHOD_END"], "methodName": ["registerAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "AMRMTokenIdentifier   result    =    null ;", "Set < TokenIdentifier >    tokenIds    =    remoteUgi . getTokenIdentifiers (  )  ;", "for    ( TokenIdentifier   tokenId    :    tokenIds )     {", "if    ( tokenId   instanceof   AMRMTokenIdentifier )     {", "result    =     (  ( AMRMTokenIdentifier )     ( tokenId )  )  ;", "break ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["selectAMRMTokenIdentifier"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "String   message    =     \" Application   doesn ' t   exist   in   cache    \"     +    appAttemptId ;", ". LOG . error ( message )  ;", "throw   new   InvalidApplicationMasterRequestException ( message )  ;", "}", "METHOD_END"], "methodName": ["throwApplicationDoesNotExistInCacheException"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "ApplicationMasterService . LOG . info (  (  \" Unregistering   app   attempt    :     \"     +    attemptId )  )  ;", "responseMap . remove ( attemptId )  ;", "rmContext . getNMTokenSecretManager (  )  . unregisterApplicationAttempt ( attemptId )  ;", "}", "METHOD_END"], "methodName": ["unregisterAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "return    ( applicationsACLsManager . checkAccess ( callerUGI ,    operationPerformed ,    owner ,    application . getApplicationId (  )  )  )     |  |     ( queueACLsManager . checkAccess ( callerUGI ,    ADMINISTER _ QUEUE ,    application . getQueue (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "SchedulerNodeReport   schedulerNodeReport    =    scheduler . getNodeReport ( rmNode . getNodeID (  )  )  ;", "R   used    =    BuilderUtils . newR (  0  ,     0  )  ;", "int   numContainers    =     0  ;", "if    ( schedulerNodeReport    !  =    null )     {", "used    =    schedulerNodeReport . getUsedR (  )  ;", "numContainers    =    schedulerNodeReport . getNumContainers (  )  ;", "}", "NodeReport   report    =    BuilderUtils . newNodeReport ( rmNode . getNodeID (  )  ,    rmNode . getState (  )  ,    rmNode . getHttpAddress (  )  ,    rmNode . getRackName (  )  ,    used ,    rmNode . getTotalCapability (  )  ,    numContainers ,    rmNode . getHealthReport (  )  ,    rmNode . getLastHealthReportTime (  )  )  ;", "return   report ;", "}", "METHOD_END"], "methodName": ["createNodeReports"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   callerUGI ;", "try    {", "callerUGI    =    UserGroupInformation . getCurrentUser (  )  ;", "}    catch    ( IOException   ie )     {", ". LOG . info (  \" Error   getting   UGI    \"  ,    ie )  ;", "throw   RPCUtil . getRemoteException ( ie )  ;", "}", "Set < String >    applicationTypes    =    request . getApplicationTypes (  )  ;", "EnumSet < YarnApplicationState >    applicationStates    =    request . getApplicationStates (  )  ;", "Set < String >    users    =    request . getUsers (  )  ;", "Set < String >    queues    =    request . getQueues (  )  ;", "Set < String >    tags    =    request . getApplicationTags (  )  ;", "long   limit    =    request . getLimit (  )  ;", "LongRange   start    =    request . getStartRange (  )  ;", "LongRange   finish    =    request . getFinishRange (  )  ;", "ApplicationsRequestScope   scope    =    request . getScope (  )  ;", "final   Map < ApplicationId ,    RMApp >    apps    =    rmContext . getRMApps (  )  ;", "Iterator < RMApp >    appsIter ;", "if    (  ( queues    !  =    null )     &  &     (  !  ( queues . isEmpty (  )  )  )  )     {", "final   List < List < ApplicationAttemptId >  >    queueAppLists    =    new   ArrayList < List < ApplicationAttemptId >  >  (  )  ;", "for    ( String   queue    :    queues )     {", "List < ApplicationAttemptId >    appsInQueue    =    scheduler . getAppsInQueue ( queue )  ;", "if    (  ( appsInQueue    !  =    null )     &  &     (  !  ( appsInQueue . isEmpty (  )  )  )  )     {", "queueAppLists . add ( appsInQueue )  ;", "}", "}", "appsIter    =    new   Iterator < RMApp >  (  )     {", "Iterator < List < ApplicationAttemptId >  >    appListIter    =    queueAppLists . iterator (  )  ;", "Iterator < ApplicationAttemptId >    schedAppsIter ;", "@ Override", "public   boolean   hasNext (  )     {", "return    (  (  ( schedAppsIter )     !  =    null )     &  &     ( schedAppsIter . hasNext (  )  )  )     |  |     ( appListIter . hasNext (  )  )  ;", "}", "@ Override", "public   RMApp   next (  )     {", "if    (  (  ( schedAppsIter )     =  =    null )     |  |     (  !  ( schedAppsIter . hasNext (  )  )  )  )     {", "schedAppsIter    =    appListIter . next (  )  . iterator (  )  ;", "}", "return   apps . get ( schedAppsIter . next (  )  . getApplicationId (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  \" Remove   not   supported \"  )  ;", "}", "}  ;", "} else    {", "appsIter    =    apps . values (  )  . iterator (  )  ;", "}", "List < ApplicationReport >    reports    =    new   ArrayList < ApplicationReport >  (  )  ;", "while    (  ( appsIter . hasNext (  )  )     &  &     (  ( reports . size (  )  )     <    limit )  )     {", "RMApp   application    =    appsIter . next (  )  ;", "boolean   allowAccess    =    checkAccess ( callerUGI ,    application . getUser (  )  ,    VIEW _ APP ,    application )  ;", "if    (  ( scope    =  =     ( ApplicationsRequestScope . OWN )  )     &  &     (  !  ( callerUGI . getUserName (  )  . equals ( application . getUser (  )  )  )  )  )     {", "continue ;", "} else", "if    (  ( scope    =  =     ( ApplicationsRequestScope . VIEWABLE )  )     &  &     (  ! allowAccess )  )     {", "continue ;", "}", "if    (  ( applicationTypes    !  =    null )     &  &     (  !  ( applicationTypes . isEmpty (  )  )  )  )     {", "String   appTypeToMatch    =     ( caseSensitive )     ?    application . getApplicationType (  )     :    application . getApplicationType (  )  . toLowerCase (  )  ;", "if    (  !  ( applicationTypes . contains ( appTypeToMatch )  )  )     {", "continue ;", "}", "}", "if    (  ( applicationStates    !  =    null )     &  &     (  !  ( applicationStates . isEmpty (  )  )  )  )     {", "if    (  !  ( applicationStates . contains ( application . createApplicationState (  )  )  )  )     {", "continue ;", "}", "}", "if    (  (  ( users    !  =    null )     &  &     (  !  ( users . isEmpty (  )  )  )  )     &  &     (  !  ( users . contains ( application . getUser (  )  )  )  )  )     {", "continue ;", "}", "if    (  ( start    !  =    null )     &  &     (  !  ( start . containsLong ( application . getStartTime (  )  )  )  )  )     {", "continue ;", "}", "if    (  ( finish    !  =    null )     &  &     (  !  ( finish . containsLong ( application . getFinishTime (  )  )  )  )  )     {", "continue ;", "}", "if    (  ( tags    !  =    null )     &  &     (  !  ( tags . isEmpty (  )  )  )  )     {", "Set < String >    appTags    =    application . getApplicationTags (  )  ;", "if    (  ( appTags    =  =    null )     |  |     ( appTags . isEmpty (  )  )  )     {", "continue ;", "}", "boolean   match    =    false ;", "for    ( String   tag    :    tags )     {", "if    ( appTags . contains ( tag )  )     {", "match    =    true ;", "break ;", "}", "}", "if    (  ! match )     {", "continue ;", "}", "}", "reports . add ( application . createAndGetApplicationReport ( callerUGI . getUserName (  )  ,    allowAccess )  )  ;", "}", "GetApplicationsResponse   response    =    recordFactory . newRecordInstance ( GetApplicationsResponse . class )  ;", "response . setApplicationList ( reports )  ;", "return   response ;", "}", "METHOD_END"], "methodName": ["getApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   clientBindAddress ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ ADDRESS ,    DEFAULT _ RM _ ADDRESS ,    DEFAULT _ RM _ PORT )  ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    BuilderUtils . newApplicationId ( recordFactory ,    ResourceManager . getClusterTimeStamp (  )  ,    applicationCounter . incrementAndGet (  )  )  ;", ". LOG . info (  (  \" Allocated   new   applicationId :     \"     +     ( applicationId . getId (  )  )  )  )  ;", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getNewApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   user    =    UserGroupInformation . getCurrentUser (  )  ;", "UserGroupInformation   loginUser    =    UserGroupInformation . getLoginUser (  )  ;", "return   loginUser . getUserName (  )  . equals ( user . getUserName (  )  )     ?    token . decodeIdentifier (  )  . getRenewer (  )  . toString (  )     :    user . getShortUserName (  )  ;", "}", "METHOD_END"], "methodName": ["getRenewerForToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   this . server ;", "}", "METHOD_END"], "methodName": ["getServer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "return   EnumSet . of ( KERBEROS ,    KERBEROS _ SSL ,    CERTIFICATE )  . contains ( UserGroupInformation . getCurrentUser (  )  . getRealAuthenticationMethod (  )  )  ;", "} else    {", "return   true ;", "}", "}", "METHOD_END"], "methodName": ["isAllowedDelegationTokenOp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "this . server . refreshServiceAclWithLoadedConfiguration ( configuration ,    policyProvider )  ;", "}", "METHOD_END"], "methodName": ["refreshServiceAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService"}, {"methodBody": ["METHOD_START", "{", "numDecommissionedNMs . decr (  )  ;", "}", "METHOD_END"], "methodName": ["decrDecommisionedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numActiveNMs . decr (  )  ;", "}", "METHOD_END"], "methodName": ["decrNumActiveNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numLostNMs . decr (  )  ;", "}", "METHOD_END"], "methodName": ["decrNumLostNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numRebootedNMs . decr (  )  ;", "}", "METHOD_END"], "methodName": ["decrNumRebootedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numUnhealthyNMs . decr (  )  ;", "}", "METHOD_END"], "methodName": ["decrNumUnhealthyNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "ClusterMetrics . isInitialized . set ( false )  ;", "ClusterMetrics . INSTANCE    =    null ;", "}", "METHOD_END"], "methodName": ["destroy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( ClusterMetrics . isInitialized . get (  )  )  )     {", "synchronized ( ClusterMetrics . class )     {", "if    (  ( ClusterMetrics . INSTANCE )     =  =    null )     {", "ClusterMetrics . INSTANCE    =    new   ClusterMetrics (  )  ;", "ClusterMetrics . registerMetrics (  )  ;", "ClusterMetrics . isInitialized . set ( true )  ;", "}", "}", "}", "return   ClusterMetrics . INSTANCE ;", "}", "METHOD_END"], "methodName": ["getMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numActiveNMs . value (  )  ;", "}", "METHOD_END"], "methodName": ["getNumActiveNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numDecommissionedNMs . value (  )  ;", "}", "METHOD_END"], "methodName": ["getNumDecommisionedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numLostNMs . value (  )  ;", "}", "METHOD_END"], "methodName": ["getNumLostNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numRebootedNMs . value (  )  ;", "}", "METHOD_END"], "methodName": ["getNumRebootedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numUnhealthyNMs . value (  )  ;", "}", "METHOD_END"], "methodName": ["getUnhealthyNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numDecommissionedNMs . incr (  )  ;", "}", "METHOD_END"], "methodName": ["incrDecommisionedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numActiveNMs . incr (  )  ;", "}", "METHOD_END"], "methodName": ["incrNumActiveNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numLostNMs . incr (  )  ;", "}", "METHOD_END"], "methodName": ["incrNumLostNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numRebootedNMs . incr (  )  ;", "}", "METHOD_END"], "methodName": ["incrNumRebootedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numUnhealthyNMs . incr (  )  ;", "}", "METHOD_END"], "methodName": ["incrNumUnhealthyNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "ClusterMetrics . registry    =    new   MetricsRegistry ( ClusterMetrics . RECORD _ INFO )  ;", "ClusterMetrics . registry . tag ( ClusterMetrics . RECORD _ INFO ,     \" ResourceManager \"  )  ;", "MetricsSystem   ms    =    DefaultMetricsSystem . instance (  )  ;", "if    ( ms    !  =    null )     {", "ms . register (  \" ClusterMetrics \"  ,     \" Metrics   for   the   Yarn   Cluster \"  ,    ClusterMetrics . INSTANCE )  ;", "}", "}", "METHOD_END"], "methodName": ["registerMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "numDecommissionedNMs . set ( num )  ;", "}", "METHOD_END"], "methodName": ["setDecommisionedNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics"}, {"methodBody": ["METHOD_START", "{", "return   ActiveRMInfoProto . newBuilder (  )  . setClusterId ( clusterId )  . setRmId ( rmId )  . build (  )  . toByteArray (  )  ;", "}", "METHOD_END"], "methodName": ["createActiveNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    data ;", "try    {", "data    =    elector . getActiveData (  )  ;", "}    catch    ( ActiveStandbyElector   e )     {", "return   true ;", "}", "YarnServerResourceManagerServiceProtos . ActiveRMInfoProto   proto ;", "try    {", "proto    =    ActiveRMInfoProto . parseFrom ( data )  ;", "}    catch    ( InvalidProtocolBufferException   e )     {", ". LOG . error (  (  \" Invalid   data   in   ZK :     \"     +     ( StringUtils . byteToHexString ( data )  )  )  )  ;", "return   false ;", "}", "if    (  !  ( proto . getClusterId (  )  . equals ( clusterId )  )  )     {", ". LOG . error (  (  (  (  (  \" Mismatched   cluster !    The   other   RM   seems    \"     +     \" to   be   from   a   different   cluster .    Current   cluster    =     \"  )     +    clusterId )     +     \" Other   RM ' s   cluster    =     \"  )     +     ( proto . getClusterId (  )  )  )  )  ;", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["isParentZnodeSafe"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService"}, {"methodBody": ["METHOD_START", "{", "elector . quitElection ( false )  ;", "elector . joinElection ( localActiveNodeInfo )  ;", "}", "METHOD_END"], "methodName": ["resetLeaderElection"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService"}, {"methodBody": ["METHOD_START", "{", "releases . add ( containerId )  ;", "}", "METHOD_END"], "methodName": ["addContainerToBeReleased"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "requests . addAll ( createReq ( hosts ,    memory ,    priority ,    containers )  )  ;", "}", "METHOD_END"], "methodName": ["addRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    reqs    =    createReq ( new   String [  ]  {    host    }  ,    memory ,     1  ,    numContainers )  ;", "return   allocate ( reqs ,    releases )  ;", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "final   AllocateRequest   req    =    AllocateRequest . newInstance (  (  +  +  ( responseId )  )  ,     0  .  0 F ,    resourceRequest ,    releases ,    null )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( attemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    context . getRMApps (  )  . get ( attemptId . getApplicationId (  )  )  . getRMAppAttempt ( attemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "try    {", "return   ugi . doAs ( new   PrivilegedExceptionAction < AllocateResponse >  (  )     {", "@ Override", "public   AllocateResponse   run (  )    throws   Exception    {", "return   amRMProtocol . allocate ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   e )     {", "throw    (  ( Exception )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "final   AllocateRequest   req    =    allocateRequest ;", "reqtRespoId (  (  +  +  ( respoId )  )  )  ;", "rGroupInformation   ugi    =    rGroupInformation . createRemoter ( attemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    context . getRMApps (  )  . get ( attemptId . getApplicationId (  )  )  . getRMAppAttempt ( attemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "try    {", "return   ugi . doAs ( new   PrivilegedExceptionAction < AllocateRespo >  (  )     {", "@ Override", "public   AllocateRespo   run (  )    throws   Exception    {", "return   amRMProtocol . allocate ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   e )     {", "throw    (  ( Exception )     ( e . getCa (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "allocate (  \" ANY \"  ,    memory ,    nContainer ,    null )  ;", "nm . nodeHeartbeat ( true )  ;", "List < Container >    conts    =    allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    null )  . getAllocatedContainers (  )  ;", "while    (  ( conts . size (  )  )     <    nContainer )     {", "nm . nodeHeartbeat ( true )  ;", "conts . addAll ( allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "return   conts ;", "}", "METHOD_END"], "methodName": ["allocateAndWaitForContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    reqs    =    new   ArrayList < ResourceRequest >  (  )  ;", "for    ( String   host    :    hosts )     {", "ResourceRequest   hostReq    =    createResourceReq ( host ,    memory ,    priority ,    containers )  ;", "reqs . add ( hostReq )  ;", "ResourceRequest   rackReq    =    createResourceReq (  \"  / default - rack \"  ,    memory ,    priority ,    containers )  ;", "reqs . add ( rackReq )  ;", "}", "ResourceRequest   offRackReq    =    createResourceReq ( ANY ,    memory ,    priority ,    containers )  ;", "reqs . add ( offRackReq )  ;", "return   reqs ;", "}", "METHOD_END"], "methodName": ["createReq"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   req    =    Records . newRecord ( ResourceRequest . class )  ;", "req . setResourceName (  )  ;", "req . setNumContainers ( containers )  ;", "Priority   pri    =    Records . newRecord ( Priority . class )  ;", "pri . setPriority ( priority )  ;", "req . setPriority ( pri )  ;", "Resource   capability    =    Records . newRecord ( Resource . class )  ;", "capability . setMemory ( memory )  ;", "req . setCapability ( capability )  ;", "return   req ;", "}", "METHOD_END"], "methodName": ["createResourceReq"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "return   this . attemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "return   registerAppAttempt ( true )  ;", "}", "METHOD_END"], "methodName": ["registerAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "if    ( wait )     {", "waitForState ( RMAppAttemptState . LAUNCHED )  ;", "}", "ponseId    =     0  ;", "final   RegisterApplicationMasterRequest   req    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "req . setHost (  \"  \"  )  ;", "req . setRpcPort (  1  )  ;", "req . setTrackingUrl (  \"  \"  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( attemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    context . getRMApps (  )  . get ( attemptId . getApplicationId (  )  )  . getRMAppAttempt ( attemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "try    {", "return   ugi . doAs ( new   PrivilegedExceptionAction < RegisterApplicationMasterResponse >  (  )     {", "@ Override", "public   RegisterApplicationMasterResponse   run (  )    throws   Exception    {", "return   amRMProtocol . registerApplicationMaster ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   e )     {", "throw    (  ( Exception )     ( e . getCause (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["registerAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "AllocateResponse   response    =    allocate ( requests ,    releases )  ;", "requests . clear (  )  ;", "releases . clear (  )  ;", "return   response ;", "}", "METHOD_END"], "methodName": ["schedule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "this . context    =    context ;", "thismRMProtocol    = mRMProtocol ;", "}", "METHOD_END"], "methodName": ["setAMRMProtocol"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "waitForState ( RMAppAttemptState . RUNNING )  ;", "final   FinishApplicationMasterRequest   req    =    FinishApplicationMasterRequest . newInstance ( SUCCEEDED ,     \"  \"  ,     \"  \"  )  ;", "unregisterAppAttempt ( req ,    true )  ;", "}", "METHOD_END"], "methodName": ["unregisterAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "if    ( waitForStateRunning )     {", "waitForState ( RMAppAttemptState . RUNNING )  ;", "}", "UGroupInformation   ugi    =    UGroupInformation . createRemoteU ( attemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    context . getRMApps (  )  . get ( attemptId . getApplicationId (  )  )  . getRMAppAttempt ( attemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "ugi . doAs ( new   PrivilegedExceptionAction < Object >  (  )     {", "@ Override", "public   Object   run (  )    throws   Exception    {", "amRMProtocol . finishApplicationMaster ( req )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["unregisterAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    context . getRMApps (  )  . get ( attemptId . getApplicationId (  )  )  ;", "RMAppAttempt   attempt    =    app . getRMAppAttempt ( attemptId )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( finalState . equals ( attempt . getAppAttemptState (  )  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "System . out . println (  (  (  (  (  (  \" AppAttempt    :     \"     +     ( attemptId )  )     +     \"    State   is    :     \"  )     +     ( attempt . getAppAttemptState (  )  )  )     +     \"    Waiting   for   state    :     \"  )     +    finalState )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "System . out . println (  (  \" AppAttempt   State   is    :     \"     +     ( attempt . getAppAttemptState (  )  )  )  )  ;", "Ast . astEquals (  \" AppAttempt   state   is   not   correct    ( timedout )  \"  ,    finalState ,    attempt . getAppAttemptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockAM"}, {"methodBody": ["METHOD_START", "{", "Map < ApplicationId ,    List < ContainerStatus >  >    conts    =    new   HashMap < ApplicationId ,    List < ContainerStatus >  >  (  )  ;", "conts . put ( containerStatus . getContainerId (  )  . getApplicationAttemptId (  )  . getApplicationId (  )  ,    Arrays . asList ( new   ContainerStatus [  ]  {    containerStatus    }  )  )  ;", "nodeHeartbeat ( conts ,    true )  ;", "}", "METHOD_END"], "methodName": ["containerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   httpPort ;", "}", "METHOD_END"], "methodName": ["getHttpPort"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   memory ;", "}", "METHOD_END"], "methodName": ["getMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   nodeId ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   vCores ;", "}", "METHOD_END"], "methodName": ["getvCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   nodeHeartbeat ( new   HashMap < ApplicationId ,    List < ContainerStatus >  >  (  )  ,    isHealthy ,     (  +  +  ( responseId )  )  )  ;", "}", "METHOD_END"], "methodName": ["nodeHeartbeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   nodeHeartbeat ( conts ,    isHealthy ,     (  +  +  ( responseId )  )  )  ;", "}", "METHOD_END"], "methodName": ["nodeHeartbeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatRequest   req    =    Records . newRecord ( NodeHeartbeatRequest . class )  ;", "NodeStatus   status    =    Records . newRecord ( NodeStatus . class )  ;", "status . setResponseId ( resId )  ;", "status . setNodeId ( nodeId )  ;", "for    ( Map . Entry < ApplicationId ,    List < ContainerStatus >  >    entry    :    conts . entrySet (  )  )     {", "Log . info (  (  \" entry . getValue (  )     \"     +     ( entry . getValue (  )  )  )  )  ;", "status . setContainersStatuses ( entry . getValue (  )  )  ;", "}", "NodeHealthStatus   healthStatus    =    Records . newRecord ( NodeHealthStatus . class )  ;", "healthStatus . setHealthReport (  \"  \"  )  ;", "healthStatus . setIsNodeHealthy ( isHealthy )  ;", "healthStatus . setLastHealthReportTime (  1  )  ;", "status . setNodeHealthStatus ( healthStatus )  ;", "req . setNodeStatus ( status )  ;", "req . setLastKnownContainerTokenMasterKey ( this . currentContainerTokenMasterKey )  ;", "req . setLastKnownNMTokenMasterKey ( this . currentNMTokenMasterKey )  ;", "NodeHeartbeatResponse   heartbeatResponse    =    Tracker . nodeHeartbeat ( req )  ;", "MasterKey   masterKeyFromRM    =    heartbeatResponse . getContainerTokenMasterKey (  )  ;", "if    (  ( masterKeyFromRM    !  =    null )     &  &     (  ( masterKeyFromRM . getKeyId (  )  )     !  =     ( this . currentContainerTokenMasterKey . getKeyId (  )  )  )  )     {", "this . currentContainerTokenMasterKey    =    masterKeyFromRM ;", "}", "masterKeyFromRM    =    heartbeatResponse . getNMTokenMasterKey (  )  ;", "if    (  ( masterKeyFromRM    !  =    null )     &  &     (  ( masterKeyFromRM . getKeyId (  )  )     !  =     ( this . currentNMTokenMasterKey . getKeyId (  )  )  )  )     {", "this . currentNMTokenMasterKey    =    masterKeyFromRM ;", "}", "return   heartbeatResponse ;", "}", "METHOD_END"], "methodName": ["nodeHeartbeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "HashMap < ApplicationId ,    List < ContainerStatus >  >    nodeUpdate    =    new   HashMap < ApplicationId ,    List < ContainerStatus >  >  (  1  )  ;", "ContainerStatus   containerStatus    =    BuilderUtils . newContainerStatus ( BuilderUtils . newContainerId ( attemptId ,    containerId )  ,    containerState ,     \" Success \"  ,     0  )  ;", "ArrayList < ContainerStatus >    containerStatusList    =    new   ArrayList < ContainerStatus >  (  1  )  ;", "containerStatusList . add ( containerStatus )  ;", "Log . info (  (  \" ContainerStatus :     \"     +    containerStatus )  )  ;", "nodeUpdate . put ( attemptId . getApplicationId (  )  ,    containerStatusList )  ;", "return   nodeHeartbeat ( nodeUpdate ,    true )  ;", "}", "METHOD_END"], "methodName": ["nodeHeartbeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   registerNode ( null ,    null )  ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   registerNode ( null ,    runningApplications )  ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "RegisterNodeManagerRequest   req    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "req . setNodeId ( nodeId )  ;", "req . setHttpPort ( httpPort )  ;", "Resource       =    BuilderUtils . newResource ( memory ,    vCores )  ;", "req . setResource (  )  ;", "req . setContainerStatuses ( containerReports )  ;", "req . setNMVersion ( version )  ;", "req . setRunningApplications ( runningApplications )  ;", "RegisterNodeManagerResponse   registrationResponse    =    Tracker . registerNodeManager ( req )  ;", "this . currentContainerTokenMasterKey    =    registrationResponse . getContainerTokenMasterKey (  )  ;", "this . currentNMTokenMasterKey    =    registrationResponse . getNMTokenMasterKey (  )  ;", "return   registrationResponse ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "this . resourceTracker    =    resourceTracker ;", "}", "METHOD_END"], "methodName": ["setResourceTrackerService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNM"}, {"methodBody": ["METHOD_START", "{", "return   MockNodes . buildRMNode ( rack ,    perNode ,    state ,    httpAddr ,     (  ( MockNodes . NODE _ ID )  +  +  )  ,    null ,     1  2  3  )  ;", "}", "METHOD_END"], "methodName": ["buildRMNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "final   String   rackName    =     \" rack \"     +    rack ;", "final   int   nid    =    hostnum ;", "final   String   nodeAddr    =     ( hostName    +     \"  :  \"  )     +    nid ;", "if    ( hostName    =  =    null )     {", "hostName    =     \" host \"     +    nid ;", "}", "final   NodeId   nodeID    =    NodeId . newInstance ( hostName ,    port )  ;", "final   String   httpAddress    =    httpAddr ;", "String   healthReport    =     ( state    =  =     ( NodeState . UNHEALTHY )  )     ?    null    :     \" HealthyMe \"  ;", "return   new    . MockRMNodeImpl ( nodeID ,    nodeAddr ,    httpAddress ,    ResourceOption . newInstance ( perNode ,    RMNode . OVER _ COMMIT _ TIMEOUT _ MILLIS _ DEFAULT )  ,    rackName ,    healthReport ,     0  ,    nid ,    hostName ,    state )  ;", "}", "METHOD_END"], "methodName": ["buildRMNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "List < RMNode >    list    =    Lists . newArrayList (  )  ;", "for    ( int   i    =     0  ;    i    <    racks ;     +  + i )     {", "for    ( int   j    =     0  ;    j    <    nodesPerRack ;     +  + j )     {", "NodeState [  ]    allStates    =    NodeState . values (  )  ;", "list . add (  . nodeInfo ( i ,    perNode ,    allStates [  ( j    %     ( allStates . length )  )  ]  )  )  ;", "}", "}", "return   list ;", "}", "METHOD_END"], "methodName": ["deactivatedNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "Resource   rs    =    MockNodes . recordFactory . newRecordInstance ( Resource . class )  ;", "rs . setMemory (  (  ( total . getMemory (  )  )     -     ( used . getMemory (  )  )  )  )  ;", "return   rs ;", "}", "METHOD_END"], "methodName": ["newAvailResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "return   MockNodes . buildRMNode ( rack ,    perNode ,    RUNNING ,     \" localhost :  0  \"  )  ;", "}", "METHOD_END"], "methodName": ["newNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "return   MockNodes . buildRMNode ( rack ,    perNode ,    null ,     \" localhost :  0  \"  ,    hostnum ,    null ,     1  2  3  )  ;", "}", "METHOD_END"], "methodName": ["newNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "return   MockNodes . buildRMNode ( rack ,    perNode ,    null ,     \" localhost :  0  \"  ,    hostnum ,    hostName ,     1  2  3  )  ;", "}", "METHOD_END"], "methodName": ["newNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "return   MockNodes . buildRMNode ( rack ,    perNode ,    null ,     \" localhost :  0  \"  ,    hostnum ,    hostName ,    port )  ;", "}", "METHOD_END"], "methodName": ["newNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "List < RMNode >    list    =    Lists . newArrayList (  )  ;", "for    ( int   i    =     0  ;    i    <    racks ;     +  + i )     {", "for    ( int   j    =     0  ;    j    <    nodesPerRack ;     +  + j )     {", "if    ( j    =  =     ( nodesPerRack    -     1  )  )     {", "list . add (  . nodeInfo ( i ,    perNode ,    UNHEALTHY )  )  ;", "}", "list . add (  . newNodeInfo ( i ,    perNode )  )  ;", "}", "}", "return   list ;", "}", "METHOD_END"], "methodName": ["newNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "Resource   rs    =    MockNodes . recordFactory . newRecordInstance ( Resource . class )  ;", "rs . setMemory ( mem )  ;", "return   rs ;", "}", "METHOD_END"], "methodName": ["newResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "Resource   rs    =    MockNodes . recordFactory . newRecordInstance ( Resource . class )  ;", "rs . setMemory (  (  ( int )     (  ( Math . random (  )  )     *     ( total . getMemory (  )  )  )  )  )  ;", "return   rs ;", "}", "METHOD_END"], "methodName": ["newUsedResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "return   MockNodes . buildRMNode ( rack ,    perNode ,    state ,     \" N / A \"  )  ;", "}", "METHOD_END"], "methodName": ["nodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockNodes"}, {"methodBody": ["METHOD_START", "{", "RMNode   node    =    getRMContext (  )  . getRMNodes (  )  . get ( nodeid )  ;", "Ast . astNotNull (  \" node   shouldn ' t   be   null \"  ,    node )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( finalState . equals ( node . getState (  )  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "System . out . println (  (  (  (  \" Node   State   is    :     \"     +     ( node . getState (  )  )  )     +     \"    Waiting   for   state    :     \"  )     +    finalState )  )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "System . out . println (  (  \" Node   State   is    :     \"     +     ( node . getState (  )  )  )  )  ;", "Ast . astEquals (  \" Node   state   is   not   correct    ( timedout )  \"  ,    finalState ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["NMwaitForState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "(  ( AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >  )     ( getResourceScheduler (  )  )  )  . getSchedulerApplications (  )  . get ( app . getApplicationId (  )  )  . getQueue (  )  . getMetrics (  )  . clearQueueMetrics (  )  ;", "}", "METHOD_END"], "methodName": ["clearQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "FinishApplicationMasterRequest   req    =    FinishApplicationMasterRequest . newInstance ( SUCCEEDED ,     \"  \"  ,     \"  \"  )  ;", "am . unregisterAppAttempt ( req ,    true )  ;", "am . waitForState ( RMAppAttemptState . FINISHING )  ;", "nm . nodeHeartbeat ( am . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "rm . waitForState ( rmApp . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "}", "METHOD_END"], "methodName": ["finishAMAndVerifyAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "ApplicationClientProtocol   client    =    getClientRMService (  )  ;", "GetApplicationReportResponse   response    =    client . getApplicationReport ( GetApplicationReportRequest . newInstance ( appId )  )  ;", "return   response . getApplicationReport (  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   this . getRMContext (  )  . getClientToAMTokenSecretManager (  )  ;", "}", "METHOD_END"], "methodName": ["getClientToAMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "ApplicationClientProtocol   client    =    getClientRMService (  )  ;", "return   client . getNewApplication ( Records . newRecord ( GetNewApplicationRequest . class )  )  ;", "}", "METHOD_END"], "methodName": ["getNewAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   this . nodesListManager ;", "}", "METHOD_END"], "methodName": ["getNodesListManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   activeServices ;", "}", "METHOD_END"], "methodName": ["getRMActiveService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   this . rmAppManager ;", "}", "METHOD_END"], "methodName": ["getRMAppManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "ApplicationClientProtocol   client    =    getClientRMService (  )  ;", "KillApplicationRequest   req    =    KillApplicationRequest . newInstance ( appId )  ;", "return   client . forceKillApplication ( req )  ;", "}", "METHOD_END"], "methodName": ["killApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "System . out . println (  (  \" Launch   AM    \"     +     ( attempt . getAppAttemptId (  )  )  )  )  ;", "nm . nodeHeartbeat ( true )  ;", "AM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "rm . waitForState ( attempt . getAppAttemptId (  )  ,    RMAppAttemptState . LAUNCHED )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["launchAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "MockAM   am    =    MockRM . launchAM ( app ,    rm ,    nm )  ;", "am . registerAppAttempt (  )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["launchAndRegisterAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm    =    new   MockNM ( nodeIdStr ,    memory ,    getResourceTrackerService (  )  )  ;", "nm . registerNode (  )  ;", "return   nm ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm    =    new   MockNM ( nodeIdStr ,    memory ,    vCores ,    getResourceTrackerService (  )  )  ;", "nm . registerNode (  )  ;", "return   nm ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm    =    new   MockNM ( nodeIdStr ,    memory ,    vCores ,    getResourceTrackerService (  )  ,    YarnVersionInfo . getVersion (  )  )  ;", "nm . registerNode ( runningApplications )  ;", "return   nm ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "MockAM   am    =    new   MockAM ( getRMContext (  )  ,    masterService ,    appAttemptId )  ;", "am . waitForState ( RMAppAttemptState . ALLOCATED )  ;", "getRMContext (  )  . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppAttemptLaunchFailedEvent ( appAttemptId ,     \" Failed \"  )  )  ;", "}", "METHOD_END"], "methodName": ["sendAMLaunchFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "MockAM   am    =    new   MockAM ( getRMContext (  )  ,    masterService ,    appAttemptId )  ;", "am . waitForState ( RMAppAttemptState . ALLOCATED )  ;", "Token < AMRMTokenIdentifier >    amrmToken    =    this . rmContext . getAMRMTokenSecretM (  )  . createAndGetAMRMToken ( appAttemptId )  ;", "(  ( RMAppAttemptImpl )     ( this . rmContext . getRMApps (  )  . get ( appAttemptId . getApplicationId (  )  )  . getRMAppAttempt ( appAttemptId )  )  )  . setAMRMToken ( amrmToken )  ;", "getRMContext (  )  . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppAttemptEvent ( appAttemptId ,    RMAppAttemptEventType . LAUNCHED )  )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["sendAMLaunched"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =     (  ( RMNodeImpl )     ( getRMContext (  )  . getRMNodes (  )  . get ( nm . getNodeId (  )  )  )  )  ;", "nodndle ( new   RMNodeEvent ( nm . getNodeId (  )  ,    RMNodeEventType . EXPIRE )  )  ;", "}", "METHOD_END"], "methodName": ["sendNodeLost"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =     (  ( RMNodeImpl )     ( getRMContext (  )  . getRMNodes (  )  . get ( nm . getNodeId (  )  )  )  )  ;", "nodndle ( new   RMNodeStartedEvent ( nm . getNodeId (  )  ,    null ,    null )  )  ;", "}", "METHOD_END"], "methodName": ["sendNodeStarted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    false )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    unmanaged )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    false )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    null ,    unmanaged ,    null ,    super . getConfig (  )  . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    acls ,    false ,    null ,    super . getConfig (  )  . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    acls ,    unmanaged ,    queue ,    maxAppAttempts ,    ts ,    null )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    acls ,    unmanaged ,    queue ,    maxAppAttempts ,    ts ,    appType ,    true )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    acls ,    unmanaged ,    queue ,    maxAppAttempts ,    ts ,    appType ,    waitForAccepted ,    false )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    acls ,    unmanaged ,    queue ,    maxAppAttempts ,    ts ,    appType ,    waitForAccepted ,    keepContainers ,    false ,    null )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =     ( isAppIdProvided )     ?    applicationId    :    null ;", "ApplicationClientProtocol   client    =    getClientRMService (  )  ;", "if    (  ! isAppIdProvided )     {", "GetNewApplicationResponse   resp    =    client . getNewApplication ( Records . newRecord ( GetNewApplicationRequest . class )  )  ;", "appId    =    resp . getApplicationId (  )  ;", "}", "SubmitApplicationRequest   req    =    Records . newRecord ( SubmitApplicationRequest . class )  ;", "ApplicationSubmissionContext   sub    =    Records . newRecord ( ApplicationSubmissionContext . class )  ;", "sub . setKeepContainersAcrossApplicationAttempts ( keepContainers )  ;", "sub . setApplicationId ( appId )  ;", "sub . setApplicationName ( name )  ;", "sub . setMaxAppAttempts ( maxAppAttempts )  ;", "if    ( unmanaged )     {", "sub . setUnmanagedAM ( true )  ;", "}", "if    ( queue    !  =    null )     {", "sub . setQueue ( queue )  ;", "}", "sub . setApplicationType ( appType )  ;", "ContainerLaunchContext   clc    =    Records . newRecord ( ContainerLaunchContext . class )  ;", "final   R   capability    =    Records . newRecord ( R . class )  ;", "capability . setMemory ( masterMemory )  ;", "sub . setR ( capability )  ;", "clc . setApplicationACLs ( acls )  ;", "if    (  ( ts    !  =    null )     &  &     ( UserGroupInformation . isSecurityEnabled (  )  )  )     {", "DataOutputBuffer   dob    =    new   DataOutputBuffer (  )  ;", "ts . writeTokenStorageToStream ( dob )  ;", "ByteBuffer   securityTokens    =    ByteBuffer . wrap ( dob . getData (  )  ,     0  ,    dob . getLength (  )  )  ;", "clc . setTokens ( securityTokens )  ;", "}", "sub . setAMContainerSpec ( clc )  ;", "req . setApplicationSubmissionContext ( sub )  ;", "UserGroupInformation   fakeUser    =    UserGroupInformation . createUserForTesting ( user ,    new   String [  ]  {     \" someGroup \"     }  )  ;", "PrivilegedAction < SubmitApplicationResponse >    action    =    new   PrivilegedAction < SubmitApplicationResponse >  (  )     {", "ApplicationClientProtocol   client ;", "SubmitApplicationRequest   req ;", "@ Override", "public   SubmitApplicationResponse   run (  )     {", "try    {", "return   client . submitApplication ( req )  ;", "}    catch    ( YarnException   e )     {", "e . printStackTrace (  )  ;", "}    catch    ( IOException   e )     {", "e . printStackTrace (  )  ;", "}", "return   null ;", "}", "PrivilegedAction < SubmitApplicationResponse >    setClientReq ( ApplicationClientProtocol   client ,    SubmitApplicationRequest   req )     {", "this . client    =    client ;", "this . req    =    req ;", "return   this ;", "}", "}  . setClientReq ( client ,    req )  ;", "fakeUser . doAs ( action )  ;", "if    ( waitForAccepted )     {", "waitForState ( appId ,    RMAppState . ACCEPTED )  ;", "}", "return   getRMContext (  )  . getRMApps (  )  . get ( appId )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "return   submitApp ( masterMemory ,    name ,    user ,    acls ,    false ,    queue ,    super . getConfig (  )  . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null )  ;", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "int   timeoutSecs    =     0  ;", "while    (  (  ( getRScheduler (  )  . getRMContainer ( containerId )  )     =  =    null )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "System . out . println (  (  (  \" Waiting   for \"     +    containerId )     +     \"    to   be   allocated .  \"  )  )  ;", "nm . nodeHeartbeat ( true )  ;", "Thread . sleep (  2  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForContainerAllocated"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "while    ( true )     {", "List < Containtatus >    contain    =    attempt . getJustFinishedContain (  )  ;", "System . out . println (  (  \" Received   completed   contain    \"     +    contain )  )  ;", "for    ( Containtatus   contain :    contain )     {", "if    ( containgetContaind (  )  . equals ( completedContaingetContaind (  )  )  )     {", "return ;", "}", "}", "Thread . sleep (  2  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForContainerToComplete"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    getRMContext (  )  . getRMApps (  )  . get ( appId )  ;", "Assert . assertNotNull ( app )  ;", "while    (  ( app . getAppAttempts (  )  . size (  )  )     !  =    attemptSize )     {", "System . out . println (  (  (  (  (  \" Application    \"     +    appId )     +     \"    is   waiting   for   AM   to   restart .    Current   has    \"  )     +     ( app . getAppAttempts (  )  . size (  )  )  )     +     \"    attempts .  \"  )  )  ;", "Thread . sleep (  2  0  0  )  ;", "}", "return    . launchAndRegisterAM ( app ,    this ,    nm )  ;", "}", "METHOD_END"], "methodName": ["waitForNewAMToLaunchAndRegister"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    getRMContext (  )  . getRMApps (  )  . get ( attemptId . getApplicationId (  )  )  ;", "Ast . astNotNull (  \" app   shouldn ' t   be   null \"  ,    app )  ;", "RMAppAttempt   attempt    =    app . getRMAppAttempt ( attemptId )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( finalState . equals ( attempt . getAppAttemptState (  )  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "System . out . println (  (  (  (  (  (  \" AppAttempt    :     \"     +    attemptId )     +     \"    State   is    :     \"  )     +     ( attempt . getAppAttemptState (  )  )  )     +     \"    Waiting   for   state    :     \"  )     +    finalState )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "System . out . println (  (  \" Attempt   State   is    :     \"     +     ( attempt . getAppAttemptState (  )  )  )  )  ;", "Ast . astEquals (  \" Attempt   state   is   not   correct    ( timedout )  \"  ,    finalState ,    attempt . getAppAttemptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    getRMContext (  )  . getRMApps (  )  . get ( appId )  ;", "Ast . astNotNull (  \" app   shouldn ' t   be   null \"  ,    app )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  !  ( finalState . equals ( app . getState (  )  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "System . out . println (  (  (  (  (  (  \" App    :     \"     +    appId )     +     \"    State   is    :     \"  )     +     ( app . getState (  )  )  )     +     \"    Waiting   for   state    :     \"  )     +    finalState )  )  ;", "Thread . sleep (  2  0  0  0  )  ;", "}", "System . out . println (  (  \" App   State   is    :     \"     +     ( app . getState (  )  )  )  )  ;", "Ast . astEquals (  \" App   state   is   not   correct    ( timedout )  \"  ,    finalState ,    app . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "RMContainer   container    =    getResourceScheduler (  )  . getRMContainer ( containerId )  ;", "int   timeoutSecs    =     0  ;", "while    (  ( container    =  =    null )     &  &     (  ( timeoutSecs +  +  )     <     1  0  0  )  )     {", "nm . nodeHeartbeat ( true )  ;", "container    =    getResourceScheduler (  )  . getRMContainer ( containerId )  ;", "System . out . println (  (  (  \" Waiting   for   container    \"     +    containerId )     +     \"    to   be   allocated .  \"  )  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "Assert . assertNotNull (  \" Container   shouldn ' t   be   null \"  ,    container )  ;", "timeoutSecs    =     0  ;", "while    (  (  !  ( containerState . equals ( container . getState (  )  )  )  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "System . out . println (  (  (  (  (  (  \" Container    :     \"     +    containerId )     +     \"    State   is    :     \"  )     +     ( container . getState (  )  )  )     +     \"    Waiting   for   state    :     \"  )     +    containerState )  )  ;", "nm . nodeHeartbeat ( true )  ;", "Thread . sleep (  3  0  0  )  ;", "}", "System . out . println (  (  \" Container   State   is    :     \"     +     ( container . getState (  )  )  )  )  ;", "Assert . assertEquals (  \" Container   state   is   not   correct    ( timedout )  \"  ,    containerState ,    container . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["waitForState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.MockRM"}, {"methodBody": ["METHOD_START", "{", "int   expireIntvl    =    conf . getInt ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,    DEFAULT _ RM _ NM _ EXPIRY _ INTERVAL _ MS )  ;", "setExpireInterval ( expireIntvl )  ;", "setInterval (  ( expireIntvl    /     3  )  )  ;", "super . serviceInit ( conf )  ;", "}", "METHOD_END"], "methodName": ["serviceInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NMLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "NodeManager . LOG . info (  (  \" Checking   resource   usage   for    \"     +     ( containerManagerAddress )  )  )  ;", "Assert . assertEquals ( available . getMemory (  )  ,    resourceManager . getResourceScheduler (  )  . getNodeReport ( this . nodeId )  . getAvailableResource (  )  . getMemory (  )  )  ;", "Assert . assertEquals ( used . getMemory (  )  ,    resourceManager . getResourceScheduler (  )  . getNodeReport ( this . nodeId )  . getUsedResource (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "Status   nodeStatus    =    recordFactory . newRecordInstance ( Status . class )  ;", "nodeStatus . setId ( nodeId )  ;", "nodeStatus . setContainersStatuses ( containers )  ;", "HealthStatus   nodeHealthStatus    =    recordFactory . newRecordInstance ( HealthStatus . class )  ;", "nodeHealthStatus . setIsHealthy ( true )  ;", "nodeStatus . setHealthStatus ( nodeHealthStatus )  ;", "return   nodeStatus ;", "}", "METHOD_END"], "methodName": ["createNodeStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   available ;", "}", "METHOD_END"], "methodName": ["getAvailable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   capability ;", "}", "METHOD_END"], "methodName": ["getCapability"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "List < ContainerStatus >    containerStatuses    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( List < Container >    appContainers    :    containers . values (  )  )     {", "for    ( Container   container    :    appContainers )     {", "containerStatuses . add ( containerStatusMap . get ( container )  )  ;", "}", "}", "return   containerStatuses ;", "}", "METHOD_END"], "methodName": ["getContainerStatuses"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   containerManagerAddress ;", "}", "METHOD_END"], "methodName": ["getHostName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   nodeId ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   rackName ;", "}", "METHOD_END"], "methodName": ["getRackName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "return   used ;", "}", "METHOD_END"], "methodName": ["getUsed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "NodeStatus   nodeStatus    =    NodeManager . createNodeStatus ( nodeId ,    getContainerStatuses ( containers )  )  ;", "nodeStatus . setResponseId ( responseID )  ;", "NodeHeartbeatRequest   request    =    NodeManager . recordFactory . newRecordInstance ( NodeHeartbeatRequest . class )  ;", "request . setNodeStatus ( nodeStatus )  ;", "NodeHeartbeatResponse   response    =    resourceTrackerService . nodeHeartbeat ( request )  ;", "responseID    =    response . getResponseId (  )  ;", "}", "METHOD_END"], "methodName": ["heartbeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodeManager"}, {"methodBody": ["METHOD_START", "{", "HostsFileReader   hostsReader    =    new   HostsFileReader ( includesFile ,     (  ( includesFile    =  =    null )     |  |     ( includesFile . isEmpty (  )  )     ?    null    :    this . rmContext . getConfigurationProvider (  )  . getConfigurationInputStream ( this . conf ,    includesFile )  )  ,    excludesFile ,     (  ( excludesFile    =  =    null )     |  |     ( excludesFile . isEmpty (  )  )     ?    null    :    this . rmContext . getConfigurationProvider (  )  . getConfigurationInputStream ( this . conf ,    excludesFile )  )  )  ;", "return   hostsReader ;", "}", "METHOD_END"], "methodName": ["createHostsFileReader"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "NodesListManager . LOG . warn (  \" Failed   to   init   hostsReader ,    disabling \"  ,    ex )  ;", "try    {", "this . includesFile    =    conf . get ( DEFAULT _ RM _ NODES _ INCLUDE _ FILE _ PATH )  ;", "this . excludesFile    =    conf . get ( DEFAULT _ RM _ NODES _ EXCLUDE _ FILE _ PATH )  ;", "this . hostsReader    =    createHostsFileReader ( this . includesFile ,    this . excludesFile )  ;", "setDecomissionedNMsMetrics (  )  ;", "}    catch    ( IOException   ioe 2  )     {", "this . hostsReader    =    null ;", "throw   new   YarnRuntimeException ( ioe 2  )  ;", "}    catch    ( YarnException   e )     {", "this . hostsReader    =    null ;", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["disableHostsFileReader"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "return   this . hostsReader ;", "}", "METHOD_END"], "methodName": ["getHostsReader"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "unUsableNodes . addAll ( unusableRMNodesConcurrentSet )  ;", "return   unusableRMNodesConcurrentSet . size (  )  ;", "}", "METHOD_END"], "methodName": ["getUnusableNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "synchronized ( hostsReader )     {", "Set < String >    host    =    hostsReader . getHosts (  )  ;", "Set < String >    excludeList    =    hostsReader . getExcludedHosts (  )  ;", "String   ip    =    NetUtils . normalizeHostName ( hostName )  ;", "return    (  (  ( host . isEmpty (  )  )     |  |     ( host . contains ( hostName )  )  )     |  |     ( host . contains ( ip )  )  )     &  &     (  !  (  ( excludeList . contains ( hostName )  )     |  |     ( excludeList . contains ( ip )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["isValidNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( NodesListManager . LOG . isDebugEnabled (  )  )  )     {", "return ;", "}", "NodesListManager . LOG . debug (  (  (  (  \" hostsReader :    in =  \"     +     ( conf . get ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    DEFAULT _ RM _ NODES _ INCLUDE _ FILE _ PATH )  )  )     +     \"    out =  \"  )     +     ( conf . get ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,    DEFAULT _ RM _ NODES _ EXCLUDE _ FILE _ PATH )  )  )  )  ;", "for    ( String   include    :    hostsReader . getHosts (  )  )     {", "NodesListManager . LOG . debug (  (  \" include :     \"     +    include )  )  ;", "}", "for    ( String   exclude    :    hostsReader . getExcludedHosts (  )  )     {", "NodesListManager . LOG . debug (  (  \" exclude :     \"     +    exclude )  )  ;", "}", "}", "METHOD_END"], "methodName": ["printConfiguredHosts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "synchronized ( hostsReader )     {", "if    ( null    =  =    yarnConf )     {", "yarnConf    =    new   YarnConfiguration (  )  ;", "}", "includesFile    =    yarnConf . get ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    DEFAULT _ RM _ NODES _ INCLUDE _ FILE _ PATH )  ;", "excludesFile    =    yarnConf . get ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,    DEFAULT _ RM _ NODES _ EXCLUDE _ FILE _ PATH )  ;", "hostsReader . updateFileNames ( includesFile ,    excludesFile )  ;", "hostsReadfresh (  ( includesFile . isEmpty (  )     ?    null    :    this . rmContext . getConfigurationProvider (  )  . getConfigurationInputStream ( this . conf ,    includesFile )  )  ,     ( excludesFile . isEmpty (  )     ?    null    :    this . rmContext . getConfigurationProvider (  )  . getConfigurationInputStream ( this . conf ,    excludesFile )  )  )  ;", "setDecomissionedNMsMetrics (  )  ;", "printConfiguredHosts (  )  ;", "}", "}", "METHOD_END"], "methodName": ["refreshNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "Set < String >    excludeList    =    hostsReader . getExcludedHosts (  )  ;", "ClusterMetrics . getMetrics (  )  . setDecommisionedNMs ( excludeList . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["setDecomissionedNMsMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManager"}, {"methodBody": ["METHOD_START", "{", "return   node ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent"}, {"methodBody": ["METHOD_START", "{", "AccessControlList   viewACL    =    new   AccessControlList (  \"  \"  )  ;", "AccessControlList   modifyACL    =    new   AccessControlList (  \"  \"  )  ;", "if    ( setupACLs )     {", "viewACL . addUser ( submitter )  ;", "viewACL . addUser (  . COMMON _ USER )  ;", "modifyACL . addUser ( submitter )  ;", "modifyACL . addUser (  . COMMON _ USER )  ;", "}", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "acls . put ( VIEW _ APP ,    viewACL . getAclString (  )  )  ;", "acls . put ( MODIFY _ APP ,    modifyACL . getAclString (  )  )  ;", "return   acls ;", "}", "METHOD_END"], "methodName": ["createACLs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   userUGI    =    UserGroupInformation . createRemoteUser ( user )  ;", "ApplicationClientProtocol   userClient    =    userUGI . doAs ( new   PrivilegedExceptionAction < ApplicationClientProtocol >  (  )     {", "@ Override", "public   ApplicationClientProtocol   run (  )    throws   Exception    {", "return    (  ( ApplicationClientProtocol )     ( rpc . getProxy ( ApplicationClientProtocol . class ,    rmAddress ,    conf )  )  )  ;", "}", "}  )  ;", "return   userClient ;", "}", "METHOD_END"], "methodName": ["getRMClientForUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "conf    =    createConfiguration (  )  ;", "rpc    =    YarnRPC . create ( conf )  ;", "rmAddress    =    conf . getSocketAddr ( RM _ ADDRESS ,    DEFAULT _ RM _ ADDRESS ,    DEFAULT _ RM _ PORT )  ;", "AccessControlList   adminACL    =    new   AccessControlList (  \"  \"  )  ;", "conf . set ( YARN _ ADMIN _ ACL ,    adminACL . getAclString (  )  )  ;", "resourceManager    =    new   MockRM ( conf )     {", "protected   ClientRMService   createClientRMService (  )     {", "return   new   ClientRMService ( getRMContext (  )  ,    this . scheduler ,    this . rmAppManager ,    this . applicationACLsManager ,    this . queueACLsManager ,    getRMContext (  )  . getRMDelegationTokenSecretManager (  )  )  ;", "}", "@ Override", "protected   void   doSecureLogin (  )    throws   IOException    {", "}", "}  ;", "new   Thread (  )     {", "public   void   run (  )     {", "resourceManager . start (  )  ;", "}", "}  . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( resourceManager . getServiceState (  )  )     =  =     ( STATE . INITED )  )     &  &     (  ( waitCount +  +  )     <     6  0  )  )     {", ". LOG . info (  \" Waiting   for   RM   to   start .  .  .  \"  )  ;", "Thread . sleep (  1  5  0  0  )  ;", "}", "if    (  ( resourceManager . getServiceState (  )  )     !  =     ( STATE . STARTED )  )     {", "throw   new   IOException (  (  \" ResourceManager   failed   to   start .    Final   state   is    \"     +     ( resourceManager . getServiceState (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "GetNewApplicationRequest   newAppRequest    =    GetNewApplicationRequest . newInstance (  )  ;", "ApplicationClientProtocol   submitterClient    =    getRMClientForUser ( submitter )  ;", "ApplicationId   applicationId    =    submitterClient . getNewApplication ( newAppRequest )  . getApplicationId (  )  ;", "Resource       =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    createACLs ( submitter ,    setupACLs )  ;", "ContainerLaunchContext   amContainerSpec    =    ContainerLaunchContext . newInstance ( null ,    null ,    null ,    null ,    null ,    acls )  ;", "ApplicationSubmissionContext   appSubmissionContext    =    ApplicationSubmissionContext . newInstance ( applicationId ,     \" applicationName \"  ,    queueName ,    null ,    amContainerSpec ,    false ,    true ,     1  ,     ,     \" applicationType \"  )  ;", "appSubmissionContext . setApplicationId ( applicationId )  ;", "appSubmissionContext . setQueue ( queueName )  ;", "SubmitApplicationRequest   submitRequest    =    SubmitApplicationRequest . newInstance ( appSubmissionContext )  ;", "submitterClient . submitApplication ( submitRequest )  ;", "Manager . waitForState ( applicationId ,    RMAppState . ACCEPTED )  ;", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["submitAppAndGetAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManager )     !  =    null )     {", "resourceManager . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . QUEUE _ A _ ADMIN ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . COMMON _ USER ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "verifyKillAppFailure ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "verifyKillAppFailure ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . QUEUE _ B _ ADMIN ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . QUEUEB ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . QUEUE _ B _ ADMIN ,    QueueACLsTestBase . QUEUEB ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . COMMON _ USER ,    QueueACLsTestBase . QUEUEB ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . QUEUEB ,    true )  ;", "verifyKillAppFailure ( QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . QUEUEB ,    true )  ;", "verifyKillAppFailure ( QueueACLsTestBase . QUEUE _ B _ USER ,    QueueACLsTestBase . QUEUE _ A _ ADMIN ,    QueueACLsTestBase . QUEUEB ,    true )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . QUEUEA ,    false )  ;", "verifyKillAppSuccess ( QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . QUEUEB ,    false )  ;", "verifyGetClientAMToken ( QueueACLsTestBase . QUEUE _ A _ USER ,    QueueACLsTestBase . ROOT _ ADMIN ,    QueueACLsTestBase . QUEUEA ,    true )  ;", "}", "METHOD_END"], "methodName": ["testApplicationACLs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    submitAppAndGetAppId ( submitter ,    queueName ,    setupACLs )  ;", "final   GetApplicationReportRequest   appReportRequest    =    GetApplicationReportRequest . newInstance ( applicationId )  ;", "ApplicationClientProtocol   submitterClient    =    getRMClientForUser ( submitter )  ;", "ApplicationClientProtocol   adMinUserClient    =    getRMClientForUser ( qdmin )  ;", "GetApplicationReportResponse   submitterGetReport    =    submitterClient . getApplicationReport ( appReportRequest )  ;", "GetApplicationReportResponse   adMinUserGetReport    =    adMinUserClient . getApplicationReport ( appReportRequest )  ;", "Assert . assertEquals ( submitterGetReport . getApplicationReport (  )  . getClientToAMToken (  )  ,    adMinUserGetReport . getApplicationReport (  )  . getClientToAMToken (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyGetClientAMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    submitAppAndGetAppId ( submitter ,    queueName ,    setupACLs )  ;", "final   KillApplicationRequest   finishAppRequest    =    KillApplicationRequest . newInstance ( applicationId )  ;", "ApplicationClientProtocol   killerClient    =    getRMClientForUser ( killer )  ;", "try    {", "killerClient . forceKillApplication ( finishAppRequest )  ;", "Assert . fail (  \" App   killing   by   the   enemy   should   fail !  !  \"  )  ;", "}    catch    ( YarnException   e )     {", ". LOG . info (  \" Got   exception   while   killing   app   as   the   enemy \"  ,    e )  ;", "Assert . assertTrue ( e . getMessage (  )  . contains (  (  (  (  \" User    \"     +    killer )     +     \"    cannot   perform   operation   MODIFY _ APP   on    \"  )     +    applicationId )  )  )  ;", "}", "getRMClientForUser ( submitter )  . forceKillApplication ( finishAppRequest )  ;", "}", "METHOD_END"], "methodName": ["verifyKillAppFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    submitAppAndGetAppId ( submitter ,    queueName ,    setupACLs )  ;", "final   KillApplicationRequest   finishAppRequest    =    KillApplicationRequest . newInstance ( applicationId )  ;", "ApplicationClientProtocol   ownerClient    =    getRMClientForUser ( killer )  ;", "ownerClient . forceKillApplication ( finishAppRequest )  ;", "Manager . waitForState ( applicationId ,    RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["verifyKillAppSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.QueueACLsTestBase"}, {"methodBody": ["METHOD_START", "{", "while    (  ( completedAppsInStateStore )     >     ( this . maxCompletedAppsInStateStore )  )     {", "ApplicationId   removeId    =    completedApps . get (  (  ( completedApps . size (  )  )     -     ( completedAppsInStateStore )  )  )  ;", "RMApp   removeApp    =    rmContext . getRMApps (  )  . get ( removeId )  ;", ". LOG . info (  (  (  (  (  (  \" Max   number   of   completed   apps   kept   in   state   store   met :  \"     +     \"    maxCompletedAppsInStateStore    =     \"  )     +     ( maxCompletedAppsInStateStore )  )     +     \"  ,    removing   app    \"  )     +     ( removeApp . getApplicationId (  )  )  )     +     \"    from   state   store .  \"  )  )  ;", "rmContext . getStateStore (  )  . removeApplication ( removeApp )  ;", "( completedAppsInStateStore )  -  -  ;", "}", "while    (  ( completedApps . size (  )  )     >     ( this . maxCompletedAppsInMemory )  )     {", "ApplicationId   removeId    =    completedApps . remove (  )  ;", ". LOG . info (  (  (  (  (  (  \" Application   should   be   expired ,    max   number   of   completed   apps \"     +     \"    kept   in   memory   met :    maxCompletedAppsInMemory    =     \"  )     +     ( this . maxCompletedAppsInMemory )  )     +     \"  ,    removing   app    \"  )     +    removeId )     +     \"    from   memory :     \"  )  )  ;", "rmContext . getRMApps (  )  . remove ( removeId )  ;", "this . applicationACLsManager . removeApplication ( removeId )  ;", "}", "}", "METHOD_END"], "methodName": ["checkAppNumCompletedLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    submissionContext . getApplicationId (  )  ;", "validateResourceRequest ( submissionContext )  ;", "RMAppImpl   application    =    new   RMAppImpl ( applicationId ,    rmContext ,    this . conf ,    submissionContext . getApplicationName (  )  ,    user ,    submissionContext . getQueue (  )  ,    submissionContext ,    this . scheduler ,    this . masterService ,    submitTime ,    submissionContext . getApplicationType (  )  ,    submissionContext . getApplicationTags (  )  )  ;", "if    (  ( rmContext . getRMApps (  )  . putIfAbsent ( applicationId ,    application )  )     !  =    null )     {", "String   message    =     (  \" Application   with   id    \"     +    applicationId )     +     \"    is   already   present !    Cannot   add   a   duplicate !  \"  ;", ". LOG . warn ( message )  ;", "throw   RPCUtil . getRemoteException ( message )  ;", "}", "this . applicationACLsManager . addApplication ( applicationId ,    submissionContext . getAMContainerSpec (  )  . getApplicationACLs (  )  )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["createAndPopulateNewRMApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "if    ( applicationId    =  =    null )     {", ". LOG . error (  \"    received   completed   appId   of   null ,    skipping \"  )  ;", "} else    {", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "rmContext . getDelegationTokenRenewer (  )  . applicationFinished ( applicationId )  ;", "}", "completedApps . add ( applicationId )  ;", "( completedAppsInStateStore )  +  +  ;", "writeAuditLog ( applicationId )  ;", "}", "}", "METHOD_END"], "methodName": ["finishApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "return   this . completedApps . size (  )  ;", "}", "METHOD_END"], "methodName": ["getCompletedAppsListSize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( rmAppState    =  =     ( RMAppState . FINISHED )  )     |  |     ( rmAppState    =  =     ( RMAppState . FAILED )  )  )     |  |     ( rmAppState    =  =     ( RMAppState . KILLED )  )  )     {", "return   true ;", "} else    {", "return   false ;", "}", "}", "METHOD_END"], "methodName": ["isApplicationInFinalState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "RMAppManager . ApplicationSummary . logAppSummary ( rmContext . getRMApps (  )  . get ( appId )  )  ;", "}", "METHOD_END"], "methodName": ["logApplicationSummary"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "Credentials   credentials    =    new   Credentials (  )  ;", "DataInputByteBuffer   dibb    =    new   DataInputByteBuffer (  )  ;", "ByteBuffer   tokens    =    application . getAMContainerSpec (  )  . getTokens (  )  ;", "if    ( tokens    !  =    null )     {", "dibbet ( tokens )  ;", "credentials . readTokenStorageStream ( dibb )  ;", "tokens . rewind (  )  ;", "}", "return   credentials ;", "}", "METHOD_END"], "methodName": ["parseCredentials"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   appContext    =    appState . getApplicationSubmissionContext (  )  ;", "ApplicationId   appId    =    appState . getAppId (  )  ;", "RMAppImpl   application    =    createAndPopulateNewRMApp ( appContext ,    appState . getSubmitTime (  )  ,    appState . getUser (  )  )  ;", "application . recover ( rmState )  ;", "if    ( isApplicationInFinalState ( appState . getState (  )  )  )     {", "application . handle ( new   RMAppEvent ( appId ,    RMAppEventType . RECOVER )  )  ;", "return ;", "}", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "Credentials   credentials    =    null ;", "try    {", "credentials    =    parseCredentials ( appContext )  ;", "rmContext . getDelegationTokenRenewer (  )  . addApplicationSync ( appId ,    credentials ,    appContext . getCancelTokensWhenComplete (  )  )  ;", "application . handle ( new   RMAppEvent ( appId ,    RMAppEventType . RECOVER )  )  ;", "}    catch    ( Exception   e )     {", ". LOG . warn (  \" Unable   to   parse   and   renew   delegation   tokens .  \"  ,    e )  ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( appId ,    e . getMessage (  )  )  )  ;", "throw   e ;", "}", "} else    {", "application . handle ( new   RMAppEvent ( appId ,    RMAppEventType . RECOVER )  )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    submissionContext . getApplicationId (  )  ;", "RMAppImpl   application    =    createAndPopulateNewRMApp ( submissionContext ,    submitTime ,    user )  ;", "ApplicationId   appId    =    submissionContext . getApplicationId (  )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "Credentials   credentials    =    null ;", "try    {", "credentials    =    parseCredentials ( submissionContext )  ;", "this . rmContext . getDelegationTokenRenewer (  )  . addApplicationAsync ( appId ,    credentials ,    submissionContext . getCancelTokensWhenComplete (  )  )  ;", "}    catch    ( Exception   e )     {", ". LOG . warn (  \" Unable   to   parse   credentials .  \"  ,    e )  ;", "assert    ( application . getState (  )  )     =  =     ( RMAppState . NEW )  ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    e . getMessage (  )  )  )  ;", "throw   RPCUtil . getRemoteException ( e )  ;", "}", "} else    {", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppEvent ( applicationId ,    RMAppEventType . START )  )  ;", "}", "}", "METHOD_END"], "methodName": ["submitApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( submissionContext . getUnmanagedAM (  )  )  )     {", "ResourceRequest   amReq    =    BuilderUtils . newResourceRequest ( RMAppAttemptImpl . AM _ CONTAINER _ PRIORITY ,    ANY ,    submissionContext . getResource (  )  ,     1  )  ;", "try    {", "SchedulerUtils . validateResourceRequest ( amReq ,    scheduler . getMaximumResourceCapability (  )  )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", ". LOG . warn (  (  (  \" RM   app   submission   failed   in   validating   AM   resource   request \"     +     \"    for   application    \"  )     +     ( submissionContext . getApplicationId (  )  )  )  ,    e )  ;", "throw   e ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    rmContext . getRMApps (  )  . get ( appId )  ;", "String   operation    =     \" UNKONWN \"  ;", "boolean   success    =    false ;", "switch    ( app . getState (  )  )     {", "case   FAILED    :", "operation    =    RMAuditLogger . AuditConstants . FINISH _ FAILED _ APP ;", "break ;", "case   FINISHED    :", "operation    =    RMAuditLogger . AuditConstants . FINISH _ SUCCESS _ APP ;", "success    =    true ;", "break ;", "case   KILLED    :", "operation    =    RMAuditLogger . AuditConstants . FINISH _ KILLED _ APP ;", "success    =    true ;", "break ;", "default    :", "}", "if    ( success )     {", "RMAuditLogger . logSuccess ( app . getUser (  )  ,    operation ,     \"  \"  ,    app . getApplicationId (  )  )  ;", "} else    {", "StringBuilder   diag    =    app . getDiagnostics (  )  ;", "String   msg    =     ( diag    =  =    null )     ?    null    :    diag . toString (  )  ;", "RMAuditLogger . logFailure ( app . getUser (  )  ,    operation ,    msg ,     \"  \"  ,     (  \" App   failed   with   state :     \"     +     ( app . getState (  )  )  )  ,    appId )  ;", "}", "}", "METHOD_END"], "methodName": ["writeAuditLog"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"}, {"methodBody": ["METHOD_START", "{", "return   this . appId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEvent"}, {"methodBody": ["METHOD_START", "{", "b . append ( RMAuditLogger . AuditConstants . PAIR _ SEPARATOR )  . append ( key . name (  )  )  . append ( RMAuditLogger . AuditConstants . KEY _ VAL _ SEPARATOR )  . append ( value )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "if    ( ip    !  =    null )     {", ". add (  . Keys . IP ,    ip . getHostAddress (  )  ,    b )  ;", "}", "}", "METHOD_END"], "methodName": ["addRemoteIP"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   b    =    new   StringBuilder (  )  ;", ". start (  . Keys . USER ,    user ,    b )  ;", ". addRemoteIP ( b )  ;", ". add (  . Keys . OPERATION ,    operation ,    b )  ;", ". add (  . Keys . TARGET ,    target ,    b )  ;", ". add (  . Keys . RESULT ,     . AuditConstants . FAILURE ,    b )  ;", ". add (  . Keys . DESCRIPTION ,    description ,    b )  ;", ". add (  . Keys . PERMISSIONS ,    perm ,    b )  ;", "if    ( appId    !  =    null )     {", ". add (  . Keys . APPID ,    appId . toString (  )  ,    b )  ;", "}", "if    ( attemptId    !  =    null )     {", ". add (  . Keys . APPATTEMPTID ,    attemptId . toString (  )  ,    b )  ;", "}", "if    ( containerId    !  =    null )     {", ". add (  . Keys . CONTAINERID ,    containerId . toString (  )  ,    b )  ;", "}", "return   b . toString (  )  ;", "}", "METHOD_END"], "methodName": ["createFailureLog"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   b    =    new   StringBuilder (  )  ;", ". start (  . Keys . USER ,    user ,    b )  ;", ". addRemoteIP ( b )  ;", ". add (  . Keys . OPERATION ,    operation ,    b )  ;", ". add (  . Keys . TARGET ,    target ,    b )  ;", ". add (  . Keys . RESULT ,     . AuditConstants . SUCCESS ,    b )  ;", "if    ( appId    !  =    null )     {", ". add (  . Keys . APPID ,    appId . toString (  )  ,    b )  ;", "}", "if    ( attemptId    !  =    null )     {", ". add (  . Keys . APPATTEMPTID ,    attemptId . toString (  )  ,    b )  ;", "}", "if    ( containerId    !  =    null )     {", ". add (  . Keys . CONTAINERID ,    containerId . toString (  )  ,    b )  ;", "}", "return   b . toString (  )  ;", "}", "METHOD_END"], "methodName": ["createSuccessLog"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isWarnEnabled (  )  )     {", "RMAuditLogger . LOG . warn ( RMAuditLogger . createFailureLog ( user ,    operation ,    perm ,    target ,    description ,    null ,    null ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isWarnEnabled (  )  )     {", "RMAuditLogger . LOG . warn ( RMAuditLogger . createFailureLog ( user ,    operation ,    perm ,    target ,    description ,    appId ,    null ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isWarnEnabled (  )  )     {", "RMAuditLogger . LOG . warn ( RMAuditLogger . createFailureLog ( user ,    operation ,    perm ,    target ,    description ,    appId ,    attemptId ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isWarnEnabled (  )  )     {", "RMAuditLogger . LOG . warn ( RMAuditLogger . createFailureLog ( user ,    operation ,    perm ,    target ,    description ,    appId ,    null ,    containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isInfoEnabled (  )  )     {", "RMAuditLogger . LOG . info ( RMAuditLogger . createSuccessLog ( user ,    operation ,    target ,    null ,    null ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isInfoEnabled (  )  )     {", "RMAuditLogger . LOG . info ( RMAuditLogger . createSuccessLog ( user ,    operation ,    target ,    appId ,    null ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isInfoEnabled (  )  )     {", "RMAuditLogger . LOG . info ( RMAuditLogger . createSuccessLog ( user ,    operation ,    target ,    appId ,    attemptId ,    null )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "if    ( RMAuditLogger . LOG . isInfoEnabled (  )  )     {", "RMAuditLogger . LOG . info ( RMAuditLogger . createSuccessLog ( user ,    operation ,    target ,    appId ,    null ,    containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["logSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "b . append ( key . name (  )  )  . append ( RMAuditLogger . AuditConstants . KEY _ VAL _ SEPARATOR )  . append ( value )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "this . amFinishingMonitor    =    amFinishingMonitor ;", "}", "METHOD_END"], "methodName": ["setAMFinishingMonitor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . amLivelinessMonitor    =    amLivelinessMonitor ;", "}", "METHOD_END"], "methodName": ["setAMLivelinessMonitor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . amRMTokenSecretManager    =    amRMTokenSecretManager ;", "}", "METHOD_END"], "methodName": ["setAMRMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . applicationMasterService    =    applicationMasterService ;", "}", "METHOD_END"], "methodName": ["setApplicationMasterService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . clientToAMTokenSecretManager    =    clientToAMTokenSecretManager ;", "}", "METHOD_END"], "methodName": ["setClientToAMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . configurationProvider    =    configurationProvider ;", "}", "METHOD_END"], "methodName": ["setConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . containerAllocationExpirer    =    containerAllocationExpirer ;", "}", "METHOD_END"], "methodName": ["setContainerAllocationExpirer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . containerTokenSecretManager    =    containerTokenSecretManager ;", "}", "METHOD_END"], "methodName": ["setContainerTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . delegationTokenRenewer    =    delegationTokenRenewer ;", "}", "METHOD_END"], "methodName": ["setDelegationTokenRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . rmDispatcher    =    dispatcher ;", "}", "METHOD_END"], "methodName": ["setDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . epoch    =    epoch ;", "}", "METHOD_END"], "methodName": ["setEpoch"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . isHAEnabled    =    isHAEnabled ;", "}", "METHOD_END"], "methodName": ["setHAEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "synchronized ( haServiceState )     {", "this . haServiceState    =    haServiceState ;", "}", "}", "METHOD_END"], "methodName": ["setHAServiceState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . nmTokenSecretManager    =    nmTokenSecretManager ;", "}", "METHOD_END"], "methodName": ["setNMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . nodesListManager    =    nodesListManager ;", "}", "METHOD_END"], "methodName": ["setNodesListManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . adminService    =    adminService ;", "}", "METHOD_END"], "methodName": ["setRMAdminService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . resourceTrackerService    =    resourceTrackerService ;", "}", "METHOD_END"], "methodName": ["setResourceTrackerService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . scheduler    =    scheduler ;", "}", "METHOD_END"], "methodName": ["setScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "stateStore    =    store ;", "}", "METHOD_END"], "methodName": ["setStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "this . isWorkPreservingRecoveryEnabled    =    enabled ;", "}", "METHOD_END"], "methodName": ["setWorkPreservingRecoveryEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . cause ;", "}", "METHOD_END"], "methodName": ["getCause"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent"}, {"methodBody": ["METHOD_START", "{", "RMHATestBase . rm 1  . adminService . transitionToStandby ( RMHATestBase . requestInfo )  ;", "RMHATestBase . rm 2  . adminService . transitionToActive ( RMHATestBase . requestInfo )  ;", "Assert . assertTrue (  (  ( RMHATestBase . rm 1  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )  )  ;", "Assert . assertTrue (  (  ( RMHATestBase . rm 2  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . ACTIVE )  )  )  ;", "}", "METHOD_END"], "methodName": ["explicitFailover"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "return    (  (  ( state . equals ( RMAppState . FINISHING )  )     |  |     ( state . equals ( RMAppState . FINISHED )  )  )     |  |     ( state . equals ( RMAppState . FAILED )  )  )     |  |     ( state . equals ( RMAppState . KILLED )  )  ;", "}", "METHOD_END"], "methodName": ["isFinalState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "nm . nodeHeartbeat ( true )  ;", "MockAM   am    =    rm . sendAMLaund ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "rm . waitForState ( app . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,    RMAppAttemptState . RUNNING )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["launchAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "configuration . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "configuration . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  \"  )  ;", "configuration . setBoolean ( RECOVERY _ ENABLED ,    true )  ;", "configuration . set ( RM _ STORE ,    ZKRMStateStore . class . getName (  )  )  ;", "configuration . set ( RM _ ZK _ ADDRESS ,    hostPort )  ;", "configuration . setInt ( RM _ ZK _ TIMEOUT _ MS ,    RMHATestBase . ZK _ TIMEOUT _ MS )  ;", "configuration . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "configuration . set ( RM _ CLUSTER _ ID ,     \" test - yarn - cluster \"  )  ;", "int   base    =     1  0  0  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( configuration )  )     {", "configuration . set ( HAUtil . addSuffix ( confKey ,     \" rm 1  \"  )  ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     2  0  )  )  )  ;", "configuration . set ( HAUtil . addSuffix ( confKey ,     \" rm 2  \"  )  ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     4  0  )  )  )  ;", "base    =    base    *     2  ;", "}", "confForRM 1     =    new   conf . Configuration ( configuration )  ;", "confForRM 1  . set ( RM _ HA _ ID ,     \" rm 1  \"  )  ;", "confForRM 2     =    new   conf . Configuration ( configuration )  ;", "confForRM 2  . set ( RM _ HA _ ID ,     \" rm 2  \"  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "RMHATestBase . rm 1     =    new   MockRM ( confForRM 1  )  ;", "RMHATestBase . rm 2     =    new   MockRM ( confForRM 2  )  ;", "startRMs ( RMHATestBase . rm 1  ,    confForRM 1  ,    RMHATestBase . rm 2  ,    confForRM 2  )  ;", "}", "METHOD_END"], "methodName": ["startRMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "rm 1  . init ( confForRM 1  )  ;", "rm 1  . start (  )  ;", "Assert . assertTrue (  (  ( rm 1  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )  )  ;", "rm 2  . init ( confForRM 2  )  ;", "rm 2  . start (  )  ;", "Assert . assertTrue (  (  ( rm 2  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )  )  ;", "rm 1  . adminService . transitionToActive (  . requestInfo )  ;", "Assert . assertTrue (  (  ( rm 1  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . ACTIVE )  )  )  ;", "}", "METHOD_END"], "methodName": ["startRMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf 1     =    new   Configuration ( confForRM 1  )  ;", ". rm 1     =    new   MockRM ( conf 1  )     {", "@ Override", "protected   RMAppManager   createRMAppManager (  )     {", "return   new    . MyRMAppManager ( this . rmContext ,    this . scheduler ,    this . masterService ,    this . applicationACLsManager ,    conf 1  )  ;", "}", "}  ;", ". rm 2     =    new   MockRM ( confForRM 2  )  ;", "startRMs (  . rm 1  ,    conf 1  ,     . rm 2  ,    confForRM 2  )  ;", "}", "METHOD_END"], "methodName": ["startRMsWithCustomizedRMAppManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "if    (  ( RMHATestBase . rm 1  )     !  =    null )     {", "RMHATestBase . rm 1  . stop (  )  ;", "}", "if    (  ( RMHATestBase . rm 2  )     !  =    null )     {", "RMHATestBase . rm 2  . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["teardown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMHATestBase"}, {"methodBody": ["METHOD_START", "{", "return   new   AMRMTokenSecretManager ( conf ,    rmContext )  ;", "}", "METHOD_END"], "methodName": ["createAMRMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "return   new   ClientToAMTokenSecretManagerInRM (  )  ;", "}", "METHOD_END"], "methodName": ["createClientToAMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "return   new   RMContainerTokenSecretManager ( conf )  ;", "}", "METHOD_END"], "methodName": ["createContainerTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "return   new   NMTokenSecretManagerInRM ( conf )  ;", "}", "METHOD_END"], "methodName": ["createNMTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "long   secretKeyInterval    =    conf . getLong ( DELEGATION _ KEY _ UPDATE _ INTERVAL _ KEY ,    DELEGATION _ KEY _ UPDATE _ INTERVAL _ DEFAULT )  ;", "long   tokenMaxLifetime    =    conf . getLong ( DELEGATION _ TOKEN _ MAX _ LIFETIME _ KEY ,    DELEGATION _ TOKEN _ MAX _ LIFETIME _ DEFAULT )  ;", "long   tokenRenewInterval    =    conf . getLong ( DELEGATION _ TOKEN _ RENEW _ INTERVAL _ KEY ,    DELEGATION _ TOKEN _ RENEW _ INTERVAL _ DEFAULT )  ;", "return   new   RMDelegationToken ( secretKeyInterval ,    tokenMaxLifetime ,    tokenRenewInterval ,     3  6  0  0  0  0  0  ,    rmContext )  ;", "}", "METHOD_END"], "methodName": ["createRMDelegationTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "switch    ( rmAppAttemptState )     {", "case   NEW    :", "return   YApplicationAttemptState . NEW ;", "case   SUBMITTED    :", "return   YApplicationAttemptState . SUBMITTED ;", "case   SCHEDULED    :", "return   YApplicationAttemptState . SCHEDULED ;", "case   ALLOCATED    :", "return   YApplicationAttemptState . ALLOCATED ;", "case   LAUNCHED    :", "return   YApplicationAttemptState . LAUNCHED ;", "case   ALLOCATED _ SAVING    :", "case   LAUNCHED _ UNMANAGED _ SAVING    :", "return   YApplicationAttemptState . ALLOCATED _ SAVING ;", "case   RUNNING    :", "return   YApplicationAttemptState . RUNNING ;", "case   FINISHING    :", "return   YApplicationAttemptState . FINISHING ;", "case   FINISHED    :", "return   YApplicationAttemptState . FINISHED ;", "case   KILLED    :", "return   YApplicationAttemptState . KILLED ;", "case   FAILED    :", "return   YApplicationAttemptState . FAILED ;", "default    :", "throw   new   YRuntimeException (  \" Unknown   state   passed !  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["createApplicationAttemptState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "switch    ( rmAppState )     {", "case   NEW    :", "return   YApplicationState . NEW ;", "case   NEW _ SAVING    :", "return   YApplicationState . NEW _ SAVING ;", "case   SUBMITTED    :", "return   YApplicationState . SUBMITTED ;", "case   ACCEPTED    :", "return   YApplicationState . ACCEPTED ;", "case   RUNNING    :", "return   YApplicationState . RUNNING ;", "case   FINISHING    :", "case   FINISHED    :", "return   YApplicationState . FINISHED ;", "case   KILLED    :", "return   YApplicationState . KILLED ;", "case   FAILED    :", "return   YApplicationState . FAILED ;", "default    :", "throw   new   YRuntimeException (  \" Unknown   state   passed !  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["createApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "ArrayList < RMNode >    results    =    new   ArrayList < RMNode >  (  )  ;", "if    (  (  ( acceptedStates . contains ( NEW )  )     |  |     ( acceptedStates . contains ( RUNNING )  )  )     |  |     ( acceptedStates . contains ( UNHEALTHY )  )  )     {", "for    ( RMNode   rmNode    :    context . getRMNodes (  )  . values (  )  )     {", "if    ( acceptedStates . contains ( rmNode . getState (  )  )  )     {", "results . add ( rmNode )  ;", "}", "}", "}", "if    (  (  ( acceptedStates . contains ( DECOMMISSIONED )  )     |  |     ( acceptedStates . contains ( LOST )  )  )     |  |     ( acceptedStates . contains ( REBOOTED )  )  )     {", "for    ( RMNode   rmNode    :    context . getInactiveRMNodes (  )  . values (  )  )     {", "if    ( acceptedStates . contains ( rmNode . getState (  )  )  )     {", "results . add ( rmNode )  ;", "}", "}", "}", "return   results ;", "}", "METHOD_END"], "methodName": ["queryRMNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( blacklistRequest    !  =    null )     {", "List < String >    plus    =    blacklistRequest . getBlacklistAdditions (  )  ;", "if    (  ( plus    !  =    null )     &  &     ( plus . contains ( ANY )  )  )     {", "throw   new   exceptions . InvalidResourceBlacklistRequestException (  (  (  \" Cannot   add    \"     +     ( ResourceRequest . ANY )  )     +     \"    to   the   blacklist !  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateBlacklistRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "for    ( ContainerId   cId    :    containerReleaseList )     {", "if    (  !  ( appAttemptId . equals ( cId . getApplicationAttemptId (  )  )  )  )     {", "throw   new   exceptions . InvalidContainerReleaseException (  (  (  (  \" Cannot   release   container    :     \"     +     ( cId . toString (  )  )  )     +     \"    not   belonging   to   this   application   attempt    :     \"  )     +    appAttemptId )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateContainerReleaseRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "for    ( ResourceRequest   resReq    :    ask )     {", "SchedulerUtils . validateResourceRequest ( resReq ,    maximumResource )  ;", "}", "}", "METHOD_END"], "methodName": ["validateResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   user ;", "try    {", "user    =    UserGroupInformation . getCurrentUser (  )  ;", "}    catch    ( IOException   ioe )     {", "LOG . warn (  \" Couldn ' t   get   current   user \"  ,    ioe )  ;", "RMAuditLogger . logFailure (  \" UNKNOWN \"  ,    method ,    acl . toString (  )  ,     \" AdminService \"  ,     \" Couldn ' t   get   current   user \"  )  ;", "throw   ioe ;", "}", "if    (  !  ( acl . isUserAllowed ( user )  )  )     {", "LOG . warn (  (  (  (  (  (  \" User    \"     +     ( user . getShortUserName (  )  )  )     +     \"    doesn ' t   have   permission \"  )     +     \"    to   call    '  \"  )     +    method )     +     \"  '  \"  )  )  ;", "RMAuditLogger . logFailure ( user . getShortUserName (  )  ,    method ,    acl . toString (  )  ,     \" AdminService \"  ,    RMAuditLogger . AuditConstants . UNAUTHORIZED _ USER )  ;", "throw   new   security . AccessControlException (  (  (  (  (  (  \" User    \"     +     ( user . getShortUserName (  )  )  )     +     \"    doesn ' t   have   permission \"  )     +     \"    to   call    '  \"  )     +    method )     +     \"  '  \"  )  )  ;", "}", "if    ( LOG . isTraceEnabled (  )  )     {", "LOG . trace (  (  ( method    +     \"    invoked   by   user    \"  )     +     ( user . getShortUserName (  )  )  )  )  ;", "}", "return   user ;", "}", "METHOD_END"], "methodName": ["verifyAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils"}, {"methodBody": ["METHOD_START", "{", "String   zkAclConf    =    conf . get ( RM _ ZK _ ACL ,    DEFAULT _ RM _ ZK _ ACL )  ;", "try    {", "zkAclConf    =    ZKUtil . resolveConfIndirection ( zkAclConf )  ;", "return   ZKUtil . parseACLs ( zkAclConf )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  (  \" Couldn ' t   read   ACLs   based   on    \"     +     ( YarnConfiguration . RM _ ZK _ ACL )  )  )  ;", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["getZKAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMZKUtils"}, {"methodBody": ["METHOD_START", "{", "String   zkAuthConf    =    conf . get ( RM _ ZK _ AUTH )  ;", "try    {", "zkAuthConf    =    ZKUtil . resolveConfIndirection ( zkAuthConf )  ;", "if    ( zkAuthConf    !  =    null )     {", "return   ZKUtil . parseAuth ( zkAuthConf )  ;", "} else    {", "return   Collections . emptyList (  )  ;", "}", "}    catch    ( Exception   e )     {", ". LOG . error (  (  \" Couldn ' t   read   Auth   based   on    \"     +     ( YarnConfiguration . RM _ ZK _ AUTH )  )  )  ;", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["getZKAuths"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.RMZKUtils"}, {"methodBody": ["METHOD_START", "{", "return    (  ( activeServices )     !  =    null )     &  &     ( activeServices . isInState ( STARTED )  )  ;", "}", "METHOD_END"], "methodName": ["areActiveServicesRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationMasterLauncher ( this . rmContext )  ;", "}", "METHOD_END"], "methodName": ["createAMLauncher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   AMLivelinessMonitor ( this . rmDispatcher )  ;", "}", "METHOD_END"], "methodName": ["createAMLivelinessMonitor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   AdminService ( this ,    rmContext )  ;", "}", "METHOD_END"], "methodName": ["createAdminService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "activeServices    =    new   ResourceManager . RMActiveServices (  )  ;", "activeServices . init ( conf )  ;", "}", "METHOD_END"], "methodName": ["createAndInitActiveServices"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationMasterService ( this . rmContext ,    scheduler )  ;", "}", "METHOD_END"], "methodName": ["createApplicationMasterService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   ClientRMService ( this . rmContext ,    scheduler ,    this . rmAppManager ,    this . applicationACLsManager ,    this . queueACLsManager ,    this . rmContext . getRMDelegationTokenSecretManager (  )  )  ;", "}", "METHOD_END"], "methodName": ["createClientRMService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   DelegationTokenRenewer (  )  ;", "}", "METHOD_END"], "methodName": ["createDelegationTokenRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   AsyncDispatcher (  )  ;", "}", "METHOD_END"], "methodName": ["createDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   NMLivelinessMonitor ( this . rmContext . getDispatcher (  )  )  ;", "}", "METHOD_END"], "methodName": ["createNMLivelinessMonitor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   QueueACLsManager ( scheduler ,    conf )  ;", "}", "METHOD_END"], "methodName": ["createQueueACLsManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   RMAppManager ( this . rmContext ,    this . scheduler ,    this . masterService ,    this . applicationACLsManager ,    this . conf )  ;", "}", "METHOD_END"], "methodName": ["createRMAppManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   RMApplicationHistoryWriter (  )  ;", "}", "METHOD_END"], "methodName": ["createRMApplicationHistoryWriter"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   RMSecretManagerService ( conf ,    rmContext )  ;", "}", "METHOD_END"], "methodName": ["createRMSecretManagerService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceTrackerService ( this . rmContext ,    this . nodesListManager ,    this . nmLivelinessMonitor ,    this . rmContext . getContainerTokenSecretManager (  )  ,    this . rmContext . getNMTokenSecretManager (  )  )  ;", "}", "METHOD_END"], "methodName": ["createResourceTrackerService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "String   schedulerClassName    =    conf . get ( RM _ SCHEDULER ,    DEFAULT _ RM _ SCHEDULER )  ;", ". LOG . info (  (  \" Using   Scheduler :     \"     +    schedulerClassName )  )  ;", "try    {", "Class <  ?  >    schedulerClazz    =    Class . forName ( schedulerClassName )  ;", "if    ( ResourceScheduler . class . isAssignableFrom ( schedulerClazz )  )     {", "return    (  ( ResourceScheduler )     ( ReflectionUtils . newInstance ( schedulerClazz ,    this . conf )  )  )  ;", "} else    {", "throw   new   YarnRuntimeException (  (  (  (  \" Class :     \"     +    schedulerClassName )     +     \"    not   instance   of    \"  )     +     ( ResourceScheduler . class . getCanonicalName (  )  )  )  )  ;", "}", "}    catch    ( ClassNotFoundException   e )     {", "throw   new   YarnRuntimeException (  (  \" Could   not   instantiate   Scheduler :     \"     +    schedulerClassName )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["createScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceManager . SchedulerEventDispatcher ( this . scheduler )  ;", "}", "METHOD_END"], "methodName": ["createSchedulerEventDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   rmStore    =    RMStateStoreFactory . getStore ( conf )  ;", "rmStore . init ( conf )  ;", "rmStore . start (  )  ;", "try    {", ". LOG . info (  \" Deleting      state   store .  .  .  \"  )  ;", "rmStore . deleteStore (  )  ;", ". LOG . info (  \" State   store   deleted \"  )  ;", "}    finally    {", "rmStore . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteRMStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   socAddr    =    ResourceManager . getBindAddress ( conf )  ;", "SecurityUtil . login ( this . conf ,    RM _ KEYTAB ,    RM _ PRINCIPAL ,    socAddr . getHostName (  )  )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "this . rmLoginUGI    =    UserGroupInformation . getLoginUser (  )  ;", "}", "}", "METHOD_END"], "methodName": ["doSecureLogin"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationACLsManager ;", "}", "METHOD_END"], "methodName": ["getApplicationACLsManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . masterService ;", "}", "METHOD_END"], "methodName": ["getApplicationMasterService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   conf . getSocketAddr ( RM _ ADDRESS ,    DEFAULT _ RM _ ADDRESS ,    DEFAULT _ RM _ PORT )  ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . clientRM ;", "}", "METHOD_END"], "methodName": ["getClientRMService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   ResourceManager . clusterTimeStamp ;", "}", "METHOD_END"], "methodName": ["getClusterTimeStamp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . queueACLsManager ;", "}", "METHOD_END"], "methodName": ["getQueueACLsManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . rmContext ;", "}", "METHOD_END"], "methodName": ["getRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . scheduler ;", "}", "METHOD_END"], "methodName": ["getResourceScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceTracker ;", "}", "METHOD_END"], "methodName": ["getResourceTrackerService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . webApp ;", "}", "METHOD_END"], "methodName": ["getWebapp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "Thread . setDefaultUncaughtExceptionHandler ( new   YarnUncaughtExceptionHandler (  )  )  ;", "StringUtils . startupShutdownMessage (  . class ,    argv ,     . LOG )  ;", "try    {", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "if    (  (  ( argv . length )     =  =     1  )     &  &     ( argv [  0  ]  . equals (  \"  - format - state - store \"  )  )  )     {", ". deleteRMStateStore ( conf )  ;", "} else    {", "resourceManager    =    new    (  )  ;", "ShutdownHookManager . get (  )  . addShutdownHook ( new   CompositeServiceShutdownHook ( resourceManager )  ,     . SHUTDOWN _ HOOK _ PRIORITY )  ;", "resourceManager . init ( conf )  ;", "resourceManager . start (  )  ;", "}", "}    catch    ( Throwable   t )     {", ". LOG . fatal (  \" Error   starting    \"  ,    t )  ;", "System . exit (  (  -  1  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "Dispatcher   dispatcher    =    setupDispatcher (  )  ;", "(  ( Service )     ( dispatcher )  )  . init ( this . conf )  ;", "(  ( Service )     ( dispatcher )  )  . start (  )  ;", "removeService (  (  ( Service )     ( rmDispatcher )  )  )  ;", "(  ( Service )     ( rmDispatcher )  )  . stop (  )  ;", "rmDispatcher    =    dispatcher ;", "addIfService ( rmDispatcher )  ;", "rmContext . setDispatcher ( rmDispatcher )  ;", "}", "METHOD_END"], "methodName": ["resetDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "ResourceManager . clusterTimeStamp    =    timestamp ;", "}", "METHOD_END"], "methodName": ["setClusterTimeStamp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "rmStore . setRMDispatcher ( rmDispatcher )  ;", "rmContext . setStateStore ( rmStore )  ;", "}", "METHOD_END"], "methodName": ["setRMStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "Dispatcher   dispatcher    =    createDispatcher (  )  ;", "dispatcher . register ( RMFatalEventType . class ,    new    . RMFatalEventDispatcher ( this . rmContext ,    this )  )  ;", "return   dispatcher ;", "}", "METHOD_END"], "methodName": ["setupDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "if    (  ( activeServices )     !  =    null )     {", ". clusterTimeStamp    =    System . currentTimeMillis (  )  ;", "activeServices . start (  )  ;", "}", "}", "METHOD_END"], "methodName": ["startActiveServices"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "boolean   useYarnAuthenticationFilter    =    conf . getBoolean ( RM _ WEBAPP _ DELEGATION _ TOKEN _ AUTH _ FILTER ,    DEFAULT _ RM _ WEBAPP _ DELEGATION _ TOKEN _ AUTH _ FILTER )  ;", "String   authPrefix    =     \" hadoop . http . authentication .  \"  ;", "String   authTypeKey    =    authPrefix    +     \" type \"  ;", "String   filterInitializerConfKey    =     \" hadoop . http . filter . initializers \"  ;", "String   actualInitializers    =     \"  \"  ;", "Class <  ?  >  [  ]    initializersClasses    =    conf . getClasses ( filterInitializerConfKey )  ;", "boolean   hasHadoopAuthFilterInitializer    =    false ;", "boolean   hasRMAuthFilterInitializer    =    false ;", "if    ( initializersClasses    !  =    null )     {", "for    ( Class <  ?  >    initializer    :    initializersClasses )     {", "if    ( initializer . getName (  )  . equals ( AuthenticationFilterInitializer . class . getName (  )  )  )     {", "hasHadoopAuthFilterInitializer    =    true ;", "}", "if    ( initializer . getName (  )  . equals ( RMAuthenticationFilterInitializer . class . getName (  )  )  )     {", "hasRMAuthFilterInitializer    =    true ;", "}", "}", "if    (  (  (  ( UserGroupInformation . isSecurityEnabled (  )  )     &  &    useYarnAuthenticationFilter )     &  &    hasHadoopAuthFilterInitializer )     &  &     ( conf . get ( authTypeKey ,     \"  \"  )  . equals ( TYPE )  )  )     {", "ArrayList < String >    target    =    new   ArrayList < String >  (  )  ;", "for    ( Class <  ?  >    filterInitializer    :    initializersClasses )     {", "if    ( filterInitializer . getName (  )  . equals ( AuthenticationFilterInitializer . class . getName (  )  )  )     {", "if    ( hasRMAuthFilterInitializer    =  =    false )     {", "target . add ( RMAuthenticationFilterInitializer . class . getName (  )  )  ;", "}", "continue ;", "}", "target . add ( filterInitializer . getName (  )  )  ;", "}", "actualInitializers    =    StringUtils . join (  \"  ,  \"  ,    target )  ;", "ResourceManager . LOG . info (  (  \" Using   RM   authentication   filter ( kerberos / delegation - token )  \"     +     \"    for   RM   webapp   authentication \"  )  )  ;", "RMAuthenticationHandler . setSecretManager ( getClientRMService (  )  . rmDTSecretManager )  ;", "String   yarnAuthKey    =    authPrefix    +     ( RMAuthenticationFilter . AUTH _ HANDLER _ PROPERTY )  ;", "conf . setStrings ( yarnAuthKey ,    RMAuthenticationHandler . class . getName (  )  )  ;", "conf . set ( filterInitializerConfKey ,    actualInitializers )  ;", "}", "}", "String   initializers    =    conf . get ( filterInitializerConfKey )  ;", "if    (  !  ( UserGroupInformation . isSecurityEnabled (  )  )  )     {", "if    (  ( initializersClasses    =  =    null )     |  |     (  ( initializersClasses . length )     =  =     0  )  )     {", "conf . set ( filterInitializerConfKey ,    RMAuthenticationFilterInitializer . class . getName (  )  )  ;", "conf . set ( authTypeKey ,     \" simple \"  )  ;", "} else", "if    ( initializers . equals ( http . lib . StaticUserWebFilter . class . getName (  )  )  )     {", "conf . set ( filterInitializerConfKey ,     (  (  ( RMAuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,  \"  )     +    initializers )  )  ;", "conf . set ( authTypeKey ,     \" simple \"  )  ;", "}", "}", "Builder < ApplicationMasterService >    builder    =    WebApps .  $ for (  \" cluster \"  ,    ApplicationMasterService . class ,    masterService ,     \" ws \"  )  . with ( conf )  . withHttpSpnegoPrincipalKey ( RM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY )  . withHttpSpnegoKeytabKey ( RM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY )  . at ( webAppAddress )  ;", "String   proxyHostAndPort    =    WebAppUtils . getProxyHostAndPort ( conf )  ;", "if    ( WebAppUtils . getResolvedRMWebAppURLWithoutScheme ( conf )  . equals ( proxyHostAndPort )  )     {", "if    ( HAUtil . isHAEnabled ( conf )  )     {", "fetcher    =    new   AppReportFetcher ( conf )  ;", "} else    {", "fetcher    =    new   AppReportFetcher ( conf ,    getClientRMService (  )  )  ;", "}", "builder . withServlet ( ProxyUriUtils . PROXY _ SERVLET _ NAME ,    ProxyUriUtils . PROXY _ PATH _ SPEC ,    WebAppProxyServlet . class )  ;", "builder . withAttribute ( WebAppProxy . FETCHER _ ATTRIBUTE ,    fetcher )  ;", "String [  ]    proxyParts    =    proxyHostAndPort . split (  \"  :  \"  )  ;", "builder . withAttribute ( WebAppProxy . PROXY _ HOST _ ATTRIBUTE ,    proxyParts [  0  ]  )  ;", "}", "webApp    =    builder . start ( new   RMWebApp ( this )  )  ;", "}", "METHOD_END"], "methodName": ["startWepApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "if    (  ( activeServices )     !  =    null )     {", "activeServices . stop (  )  ;", "activeServices    =    null ;", "rmContext . getRMNodes (  )  . clear (  )  ;", "rmContext . getInactiveRMNodes (  )  . clear (  )  ;", "rmContext . getRMApps (  )  . clear (  )  ;", "ClusterMetrics . destroy (  )  ;", "QueueMetrics . clearQueueMetrics (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stopActiveServices"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rmContext . getHAServiceState (  )  )     =  =     ( HAServiceState . ACTIVE )  )     {", ". LOG . info (  \" Already   in   active   state \"  )  ;", "return ;", "}", ". LOG . info (  \" Transitioning   to   active   state \"  )  ;", "this . rmLoginUGI . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "startActiveServices (  )  ;", "return   null ;", "}", "}  )  ;", "rmContext . setHAServiceState ( ACTIVE )  ;", ". LOG . info (  \" Transitioned   to   active   state \"  )  ;", "}", "METHOD_END"], "methodName": ["transitionToActive"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rmContext . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )     {", ". LOG . info (  \" Already   in   standby   state \"  )  ;", "return ;", "}", ". LOG . info (  \" Transitioning   to   standby   state \"  )  ;", "if    (  ( rmContext . getHAServiceState (  )  )     =  =     ( HAServiceState . ACTIVE )  )     {", "stopActiveServices (  )  ;", "if    ( initialize )     {", "resetDispatcher (  )  ;", "createAndInitActiveServices (  )  ;", "}", "}", "rmContext . setHAServiceState ( STANDBY )  ;", ". LOG . info (  \" Transitioned   to   standby   state \"  )  ;", "}", "METHOD_END"], "methodName": ["transitionToStandby"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "int   globalMaxAppAttempts    =    conf . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "if    ( globalMaxAppAttempts    <  =     0  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  \" Invalid   global   max   attempts   configuration \"     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ AM _ MAX _ ATTEMPTS )  )     +     \"  =  \"  )     +    globalMaxAppAttempts )     +     \"  ,    it   should   be   a   positive   integer .  \"  )  )  ;", "}", "long   expireIntvl    =    conf . getLong ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,    DEFAULT _ RM _ NM _ EXPIRY _ INTERVAL _ MS )  ;", "long   heartbeatIntvl    =    conf . getLong ( RM _ NM _ HEARTBEAT _ INTERVAL _ MS ,    DEFAULT _ RM _ NM _ HEARTBEAT _ INTERVAL _ MS )  ;", "if    ( expireIntvl    <    heartbeatIntvl )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  (  (  (  \" Nodemanager   expiry   interval   should   be   no \"     +     \"    less   than   heartbeat   interval ,     \"  )     +     ( YarnConfiguration . RM _ NM _ EXPIRY _ INTERVAL _ MS )  )     +     \"  =  \"  )     +    expireIntvl )     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ NM _ HEARTBEAT _ INTERVAL _ MS )  )     +     \"  =  \"  )     +    heartbeatIntvl )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateConfigs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"}, {"methodBody": ["METHOD_START", "{", "return   this . server ;", "}", "METHOD_END"], "methodName": ["getServer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   appAttemptId    =    containerStatus . getContainerId (  )  . getApplicationAttemptId (  )  ;", "RMApp   rmApp    =    rmContext . getRMApps (  )  . get ( appAttemptId . getApplicationId (  )  )  ;", "if    ( rmApp    =  =    null )     {", ". LOG . error (  (  (  (  (  \" Received   finished   container    :     \"     +     ( containerStatus . getContainerId (  )  )  )     +     \" for   unknown   application    \"  )     +     ( appAttemptId . getApplicationId (  )  )  )     +     \"    Skipping .  \"  )  )  ;", "return ;", "}", "if    ( rmApp . getApplicationSubmissionContext (  )  . getUnmanagedAM (  )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Ignoring   container   completion   status   for   unmanaged   AM \"     +     ( rmApp . getApplicationId (  )  )  )  )  ;", "}", "return ;", "}", "RMAppAttempt   rmAppAttempt    =    rmApp . getRMAppAttempt ( appAttemptId )  ;", "Container   masterContainer    =    rmAppAttempt . getMasterContainer (  )  ;", "if    (  ( masterContainer . getId (  )  . equals ( containerStatus . getContainerId (  )  )  )     &  &     (  ( containerStatus . getContainerState (  )  )     =  =     ( ContainerState . COMPLETE )  )  )     {", "ContainerStatus   status    =    ContainerStatus . newInstance ( containerStatus . getContainerId (  )  ,    containerStatus . getContainerState (  )  ,    containerStatus . getDiagnostics (  )  ,    containerStatus . getContainerExitStatus (  )  )  ;", "RMAppAttemptContainerFinishedEvent   evt    =    new   RMAppAttemptContainerFinishedEvent ( appAttemptId ,    status )  ;", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( evt )  ;", "}", "}", "METHOD_END"], "methodName": ["handleNMContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "MasterKey   nextMasterKeyForNode    =    this . containerTokenSecretManager . getNextKey (  )  ;", "if    (  ( nextMasterKeyForNode    !  =    null )     &  &     (  ( request . getLastKnownContainerTokenMasterKey (  )  . getKeyId (  )  )     !  =     ( nextMasterKeyForNode . getKeyId (  )  )  )  )     {", "nodeHeartBeatResponse . setContainerTokenMasterKey ( nextMasterKeyForNode )  ;", "}", "nextMasterKeyForNode    =    this . nmTokenSecretManager . getNextKey (  )  ;", "if    (  ( nextMasterKeyForNode    !  =    null )     &  &     (  ( request . getLastKnownNMTokenMasterKey (  )  . getKeyId (  )  )     !  =     ( nextMasterKeyForNode . getKeyId (  )  )  )  )     {", "nodeHeartBeatResponse . setNMTokenMasterKey ( nextMasterKeyForNode )  ;", "}", "}", "METHOD_END"], "methodName": ["populateKeys"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "this . server . refreshServiceAclWithLoadedConfiguration ( configuration ,    policyProvider )  ;", "}", "METHOD_END"], "methodName": ["refreshServiceAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "return   RackResolver . resolve ( hostName )  ;", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "if    ( type    =  =     ( NodeType . NODE _ LOCAL )  )     {", "return   hosts . contains ( hostName )  ;", "} else", "if    ( type    =  =     ( NodeType . RACK _ LOCAL )  )     {", "return   racks . contains ( Applicationlve ( hostName )  )  ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["canSchedule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationID"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   hosts . toArray ( new   String [ hosts . size (  )  ]  )  ;", "}", "METHOD_END"], "methodName": ["getHosts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   nodeManager ;", "}", "METHOD_END"], "methodName": ["getNodeManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   racks . toArray ( new   String [ racks . size (  )  ]  )  ;", "}", "METHOD_END"], "methodName": ["getRacks"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   state ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "return   taskId ;", "}", "METHOD_END"], "methodName": ["getTaskId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "this . state    =    state ;", "}", "METHOD_END"], "methodName": ["setState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "this . nodeManager    =    nodeManager ;", "this . containerId    =    containerId ;", "setState ( Task . State . RUNNING )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "if    (  ( getState (  )  )     !  =     ( Task . State . RUNNING )  )     {", "throw   new   IllegalStateException (  (  (  (  \" Trying   to   stop   a   non - running   task :     \"     +     ( getTaskId (  )  )  )     +     \"    of   application    \"  )     +     ( getApplicationID (  )  )  )  )  ;", "}", "this . nodeM    =    null ;", "this . containerId    =    null ;", "setState ( Task . State . COMPLETE )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.Task"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Configuration   confWithSecurity    =    new   Configuration (  )  ;", "confWithSecurity . set ( HADOOP _ SECURITY _ AUTHENTICATION ,    KERBEROS . toString (  )  )  ;", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {    conf    }  ,    new   Object [  ]  {    confWithSecurity    }     }  )  ;", "}", "METHOD_END"], "methodName": ["configs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization"}, {"methodBody": ["METHOD_START", "{", "return    ( e    !  =    null )     &  &     (  ( expected . isInstance ( e )  )     |  |     ( TestAMAuthorization . isCause ( expected ,    e . getCause (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["isCause"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rm )     !  =    null )     {", "rmtop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization"}, {"methodBody": ["METHOD_START", "{", "TestAMAuthorization . MyContainerManager   containerManager    =    new   TestAMAuthorization . MyContainerManager (  )  ;", "rm    =    new   TestAMAuthorization . MockRMWithAMS ( conf ,    containerManager )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  2  )  ;", "acls . put ( VIEW _ APP ,     \"  *  \"  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  ,     \" appname \"  ,     \" appuser \"  ,    acls )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "int   waitCount    =     0  ;", "while    (  (  ( containerManager . containerTokens )     =  =    null )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", "TestAMAuthorization . LOG . info (  \" Waiting   for   AM   Launch   to   happen .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertNotNull ( containerManager . containerTokens )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    attempt . getAppAttemptId (  )  ;", "waitForLaunchedState ( attempt )  ;", "final   Configuration   conf    =    rm . getConfig (  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( applicationAttemptId . toString (  )  )  ;", "Credentials   credentials    =    containerManager . getContainerCredentials (  )  ;", "final   InetSocketAddress   rmBindAddress    =    rm . getApplicationMasterService (  )  . getBindAddress (  )  ;", "Token <  ?    extends   TokenIdentifier >    amRMToken    =    TestAMAuthorization . MockRMWithAMS . setupAndReturnAMRMToken ( rmBindAddress ,    credentials . getAllTokens (  )  )  ;", "currentUser . addToken ( amRMToken )  ;", "ApplicationMasterProtocol   client    =    currentUser . doAs ( new   PrivilegedAction < ApplicationMasterProtocol >  (  )     {", "@ Override", "public   ApplicationMasterProtocol   run (  )     {", "return    (  ( ApplicationMasterProtocol )     ( rpc . getProxy ( ApplicationMasterProtocol . class ,    rm . getApplicationMasterService (  )  . getBindAddress (  )  ,    conf )  )  )  ;", "}", "}  )  ;", "RegisterApplicationMasterRequest   request    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "RegisterApplicationMasterResponse   response    =    client . registerApplicationMaster ( request )  ;", "Assert . assertNotNull ( response . getClientToAMTokenMasterKey (  )  )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "Assert . assertTrue (  (  ( response . getClientToAMTokenMasterKey (  )  . array (  )  . length )     >     0  )  )  ;", "}", "Assert . assertEquals (  \" Register   response   has   bad   ACLs \"  ,     \"  *  \"  ,    response . getApplicationACLs (  )  . get ( VIEW _ APP )  )  ;", "}", "METHOD_END"], "methodName": ["testAuthorizedAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization"}, {"methodBody": ["METHOD_START", "{", "TestAMAuthorization . MyContainerManager   containerManager    =    new   TestAMAuthorization . MyContainerManager (  )  ;", "rm    =    new   TestAMAuthorization . MockRMWithAMS ( conf ,    containerManager )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "int   waitCount    =     0  ;", "while    (  (  ( containerManager . containerTokens )     =  =    null )     &  &     (  ( waitCount +  +  )     <     4  0  )  )     {", "TestAMAuthorization . LOG . info (  \" Waiting   for   AM   Launch   to   happen .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertNotNull ( containerManager . containerTokens )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    attempt . getAppAttemptId (  )  ;", "waitForLaunchedState ( attempt )  ;", "final   Configuration   conf    =    rm . getConfig (  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "final   InetSocketAddress   serviceAddr    =    conf . getSocketAddr ( RM _ SCHEDULER _ ADDRESS ,    DEFAULT _ RM _ SCHEDULER _ ADDRESS ,    DEFAULT _ RM _ SCHEDULER _ PORT )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( applicationAttemptId . toString (  )  )  ;", "ApplicationMasterProtocol   client    =    currentUser . doAs ( new   PrivilegedAction < ApplicationMasterProtocol >  (  )     {", "@ Override", "public   ApplicationMasterProtocol   run (  )     {", "return    (  ( ApplicationMasterProtocol )     ( rpc . getProxy ( ApplicationMasterProtocol . class ,    serviceAddr ,    conf )  )  )  ;", "}", "}  )  ;", "RegisterApplicationMasterRequest   request    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "try    {", "client . registerApplicationMaster ( request )  ;", "Assert . fail (  \" Should   fail   with   authorization   error \"  )  ;", "}    catch    ( Exception   e )     {", "if    ( TestAMAuthorization . isCause ( AccessControlException . class ,    e )  )     {", "String   expectedMessage    =     \"  \"  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "expectedMessage    =     \" Client   cannot   authenticate   via :  [ TOKEN ]  \"  ;", "} else    {", "expectedMessage    =     \" SIMPLE   authentication   is   not   enabled .       Available :  [ TOKEN ]  \"  ;", "}", "Assert . assertTrue ( e . getCause (  )  . getMessage (  )  . contains ( expectedMessage )  )  ;", "} else    {", "throw   e ;", "}", "}", "}", "METHOD_END"], "methodName": ["testUnauthorizedAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization"}, {"methodBody": ["METHOD_START", "{", "int   waitCount    =     0  ;", "while    (  (  ( attempt . getAppAttemptState (  )  )     !  =     ( RMAppAttemptState . LAUNCHED )  )     &  &     (  ( waitCount +  +  )     <     4  0  )  )     {", ". LOG . info (  (  (  \" Waiting   for   AppAttempt   to   reach   LAUNCHED   state .     \"     +     \" Current   state   is    \"  )     +     ( attempt . getAppAttemptState (  )  )  )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertEquals ( attempt . getAppAttemptState (  )  ,    RMAppAttemptState . LAUNCHED )  ;", "}", "METHOD_END"], "methodName": ["waitForLaunchedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAMAuthorization"}, {"methodBody": ["METHOD_START", "{", "for    ( RMApp   app    :    rmContext . getRMApps (  )  . values (  )  )     {", "if    (  (  (  ( app . getState (  )  )     =  =     ( RMAppState . FINISHED )  )     |  |     (  ( app . getState (  )  )     =  =     ( RMAppState . KILLED )  )  )     |  |     (  ( app . getState (  )  )     =  =     ( RMAppState . FAILED )  )  )     {", "appMonitor . finishApplication ( app . gelicationId (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addToCompletedApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "return   TestAppManager . appEventType ;", "}", "METHOD_END"], "methodName": ["getAppEventType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunchContext   amContainer    =    recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "amContainer . setApplicationACLs ( new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  )  ;", "return   amContainer ;", "}", "METHOD_END"], "methodName": ["mockContainerLaunchContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "final   List < RMApp >    apps    =    TestAppManager . newRMApps ( n ,    time ,    RMAppState . FINISHED )  ;", "final   ConcurrentMap < ApplicationId ,    RMApp >    map    =    Maps . newConcurrentMap (  )  ;", "for    ( RMApp   app    :    apps )     {", "map . put ( app . getApplicationId (  )  ,    app )  ;", "}", "Dispatcher   rmDispatcher    =    new   AsyncDispatcher (  )  ;", "ContainerAllocationExpirer   containerAllocationExpirer    =    new   ContainerAllocationExpirer ( rmDispatcher )  ;", "AMLivelinessMonitor   amLivelinessMonitor    =    new   AMLivelinessMonitor ( rmDispatcher )  ;", "AMLivelinessMonitor   amFinishingMonitor    =    new   AMLivelinessMonitor ( rmDispatcher )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   context    =    new   RMContextImpl ( rmDispatcher ,    containerAllocationExpirer ,    amLivelinessMonitor ,    amFinishingMonitor ,    null ,    null ,    null ,    null ,    null ,    writer )     {", "@ Override", "public   ConcurrentMap < ApplicationId ,    RMApp >    getRMApps (  )     {", "return   map ;", "}", "}  ;", "(  ( RMContextImpl )     ( context )  )  . setStateStore ( mock ( RMStateStore . class )  )  ;", "return   context ;", "}", "METHOD_END"], "methodName": ["mockRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "return   Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "}", "METHOD_END"], "methodName": ["mockResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   scheduler    =    mock ( ResourceScheduler . class )  ;", "when ( scheduler . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )  ;", "when ( scheduler . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )  ;", "return   scheduler ;", "}", "METHOD_END"], "methodName": ["mockResourceScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "List < RMApp >    list    =    Lists . newArrayList (  )  ;", "for    ( int   i    =     0  ;    i    <    n ;     +  + i )     {", "list . add ( new   MockRMApp ( i ,    time ,    state )  )  ;", "}", "return   list ;", "}", "METHOD_END"], "methodName": ["newRMApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "TestAppManager . appEventType    =    newType ;", "}", "METHOD_END"], "methodName": ["setAppEventType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "rmContext    =    TestAppManager . mockRMContext (  1  ,     ( now    -     1  0  )  )  ;", "ResourceScheduler   scheduler    =    TestAppManager . mockResourceScheduler (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "ApplicationMasterService   masterService    =    new   ApplicationMasterService ( rmContext ,    scheduler )  ;", "appMonitor    =    new   TestAppManager . TestRMAppManager ( rmContext ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    scheduler ,    masterService ,    new   security . ApplicationACLsManager ( conf )  ,    conf )  ;", "appId    =    MockApps . newAppID (  1  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "asContext    =    recordFactory . newRecordInstance ( ApplicationSubmissionContext . class )  ;", "asContext . setApplicationId ( appId )  ;", "asContext . setAMContainerSpec ( TestAppManager . mockContainerLaunchContext ( recordFactory )  )  ;", "asContext . setResource ( TestAppManager . mockResource (  )  )  ;", "setupDispatcher ( rmContext ,    conf )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "TestAppManager . TestDispatcher   testDispatcher    =    new   TestAppManager . TestDispatcher (  )  ;", "TestAppMDispatcher   testAppManagerDispatcher    =    new   TestAppMDispatcher (  )  ;", "rmContext . getDispatcher (  )  . register ( RMAppEventType . class ,    testDispatcher )  ;", "rmContext . getDispatcher (  )  . register ( RMAppManagerEventType . class ,    testAppManagerDispatcher )  ;", "(  ( Service )     ( rmContext . getDispatcher (  )  )  )  . init ( conf )  ;", "(  ( Service )     ( rmContext . getDispatcher (  )  )  )  . start (  )  ;", "Assert . assertEquals (  \" app   event   type   is   wrong   before \"  ,    RMAppEventType . KILL ,    TestAppManager . appEventType )  ;", "}", "METHOD_END"], "methodName": ["setupDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "setAppEventType ( RMAppEventType . KILL )  ;", "(  ( Service )     ( rmContext . getDispatcher (  )  )  )  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    mock ( RMAppImpl . class )  ;", "when ( app . getApplicationId (  )  )  . thenReturn ( ApplicationId . newInstance (  1  0  0 L ,     1  )  )  ;", "when ( app . getName (  )  )  . thenReturn (  \" Multiline \\ n \\ n \\ r \\ rAppName \"  )  ;", "when ( app . getUser (  )  )  . thenReturn (  \" Multiline \\ n \\ n \\ r \\ rUserName \"  )  ;", "when ( app . getQueue (  )  )  . thenReturn (  \" Multiline \\ n \\ n \\ r \\ rQueueName \"  )  ;", "when ( app . getState (  )  )  . thenReturn ( RMAppState . RUNNING )  ;", "RM . ApplicationSummary . SummaryBuilder   summary    =    new   RM . ApplicationSummary (  )  . createAppSummary ( app )  ;", "String   msg    =    summary . toString (  )  ;", "LOG . info (  (  \" summary :     \"     +    msg )  )  ;", "Assert . assertFalse ( msg . contains (  \"  \\ n \"  )  )  ;", "Assert . assertFalse ( msg . contains (  \"  \\ r \"  )  )  ;", "String   escaped    =     \"  \\  \\ n \\  \\ n \\  \\ r \\  \\ r \"  ;", "Assert . assertTrue ( msg . contains (  (  (  \" Multiline \"     +    escaped )     +     \" AppName \"  )  )  )  ;", "Assert . assertTrue ( msg . contains (  (  (  \" Multiline \"     +    escaped )     +     \" UserName \"  )  )  )  ;", "Assert . assertTrue ( msg . contains (  (  (  \" Multiline \"     +    escaped )     +     \" QueueName \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testEscapeApplicationSummary"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     1  0  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,     1  0  )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    conf )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   before   checkAppTimeLimit \"  ,     1  0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "addToCompletedApps ( appMonitor ,    rmContext )  ;", "appMonitor . checkAppNumCompletedLimit (  )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   after    #    completed   check \"  ,     1  0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,     1  0  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "verify ( rmContext . getStateStore (  )  ,    never (  )  )  . removeApplication ( isA ( RMApp . class )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppRetireNone"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     2  0  0  0  0  )  )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    new   Configuration (  )  )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   before \"  ,     1  0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "appMonitor . finishApplication ( null )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,     0  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppRetireNullApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     2  0  0  0  0  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ STATE _ STORE _ MAX _ COMPLETED _ APPLICATIONS ,     3  )  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,     3  )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    conf )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   before \"  ,     1  0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "addToCompletedApps ( appMonitor ,    rmContext )  ;", "appMonitor . checkAppNumCompletedLimit (  )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   after    #    completed   check \"  ,     3  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,     3  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "verify ( rmContext . getStateStore (  )  ,    times (  7  )  )  . removeApplication ( isA ( RMApp . class )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppRetireSome"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     2  0  0  0  0  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ STATE _ STORE _ MAX _ COMPLETED _ APPLICATIONS ,     2  )  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,     2  )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    conf )  ;", "rmContext . getRMApps (  )  . clear (  )  ;", "Assert . assertEquals (  \" map   isn ' t   empty \"  ,     0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "RMApp   app    =    new   MockRMApp (  0  ,     ( now    -     2  0  0  0  0  )  ,    RMAppState . KILLED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  1  ,     ( now    -     2  0  0  0  0  0  )  ,    RMAppState . FAILED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  2  ,     ( now    -     3  0  0  0  0  )  ,    RMAppState . FINISHED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  3  ,     ( now    -     2  0  0  0  0  )  ,    RMAppState . RUNNING )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  4  ,     ( now    -     2  0  0  0  0  )  ,    RMAppState . NEW )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  5  ,     ( now    -     1  0  0  0  1  )  ,    RMAppState . KILLED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  6  ,     ( now    -     3  0  0  0  0  )  ,    RMAppState . ACCEPTED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  7  ,     ( now    -     2  0  0  0  0  )  ,    RMAppState . SUBMITTED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  8  ,     ( now    -     1  0  0  0  1  )  ,    RMAppState . FAILED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "app    =    new   MockRMApp (  9  ,     ( now    -     2  0  0  0  0  )  ,    RMAppState . FAILED )  ;", "rmContext . getRMApps (  )  . put ( app . getApplicationId (  )  ,    app )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   before \"  ,     1  0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "addToCompletedApps ( appMonitor ,    rmContext )  ;", "appMonitor . checkAppNumCompletedLimit (  )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   after    #    completed   check \"  ,     6  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,     2  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "verify ( rmContext . getStateStore (  )  ,    times (  4  )  )  . removeApplication ( isA ( RMApp . class )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppRetireSomeDifferentStates"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     2  0  0  0  0  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ STATE _ STORE _ MAX _ COMPLETED _ APPLICATIONS ,     0  )  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,     0  )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    conf )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   before \"  ,     1  0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "addToCompletedApps ( appMonitor ,    rmContext )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect \"  ,     1  0  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "appMonitor . checkAppNumCompletedLimit (  )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   after    #    completed   check \"  ,     0  ,    rmContext . getRMApps (  )  . size (  )  )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,     0  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "verify ( rmContext . getStateStore (  )  ,    times (  1  0  )  )  . removeApplication ( isA ( RMApp . class )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppRetireZeroSetting"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "appMonitor . submitApplication ( asContext ,     \" test \"  )  ;", "RMApp   app    =    rmContext . getRMApps (  )  . get ( appId )  ;", "Assert . assertNotNull (  \" app   is   null \"  ,    app )  ;", "Assert . assertEquals (  \" app   id   doesn ' t   match \"  ,    appId ,    app . getApplicationId (  )  )  ;", "Assert . assertEquals (  \" app   state   doesn ' t   match \"  ,    RMAppState . NEW ,    app . getState (  )  )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  ( getAppEventType (  )  )     =  =     ( RMAppEventType . KILL )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertEquals (  \" app   event   type   sent   is   wrong \"  ,    RMAppEventType . START ,    getAppEventType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppSubmit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    MockApps . newAppID (  0  )  ;", "asContext . setApplicationId ( appId )  ;", "RMApp   appOrig    =    rmContext . getRMApps (  )  . get ( appId )  ;", "Assert . assertTrue (  \" app   name   matches   but   shouldn ' t \"  ,     (  \" t 1  \"     !  =     ( appOrig . getName (  )  )  )  )  ;", "try    {", "appMonitor . submitApplication ( asContext ,     \" test \"  )  ;", "Assert . fail (  \" Exception   is   expected   when   applicationId   is   duplicate .  \"  )  ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue (  \" The   thrown   exception   is   not   the   expectd   one .  \"  ,    e . getMessage (  )  . contains (  \" Cannot   add   a   duplicate !  \"  )  )  ;", "}", "RMApp   app    =    rmContext . getRMApps (  )  . get ( appId )  ;", "Assert . assertNotNull (  \" app   is   null \"  ,    app )  ;", "Assert . assertEquals (  \" app   id   doesn ' t   match \"  ,    appId ,    app . getApplicationId (  )  )  ;", "Assert . assertEquals (  \" app   state   doesn ' t   match \"  ,    RMAppState . FINISHED ,    app . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppSubmitDuplicateApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "asContext . setResource ( Resources . createResource (  (  ( YarnConfiguration . DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )     +     1  )  )  )  ;", "try    {", "appMonitor . submitApplication ( asContext ,     \" test \"  )  ;", "Assert . fail (  (  \" Application   submission   should   fail   because    \"     +     \"    request   is   invalid .  \"  )  )  ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue (  (  \" The   thrown   exception   is   not \"     +     \"    InvalidResourceRequestException \"  )  ,    e . getMessage (  )  . contains (  \" Invalid      request \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRMAppSubmitInvalidResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "int [  ]    globalMaxAppAttempts    =    new   int [  ]  {     1  0  ,     1     }  ;", "int [  ]  [  ]    individualMaxAppAttempts    =    new   int [  ]  [  ]  {    new   int [  ]  {     9  ,     1  0  ,     1  1  ,     0     }  ,    new   int [  ]  {     1  ,     1  0  ,     0  ,     -  1     }     }  ;", "int [  ]  [  ]    expectedNums    =    new   int [  ]  [  ]  {    new   int [  ]  {     9  ,     1  0  ,     1  0  ,     1  0     }  ,    new   int [  ]  {     1  ,     1  ,     1  ,     1     }     }  ;", "for    ( int   i    =     0  ;    i    <     ( globalMaxAppAttempts . length )  ;     +  + i )     {", "for    ( int   j    =     0  ;    j    <     ( individualMaxAppAttempts . length )  ;     +  + j )     {", "ResourceScheduler   scheduler    =    TestAppManager . mockResourceScheduler (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    globalMaxAppAttempts [ i ]  )  ;", "ApplicationMasterService   masterService    =    new   ApplicationMasterService ( rmContext ,    scheduler )  ;", "TestAppManager . TestRMAppManager   appMonitor    =    new   TestAppManager . TestRMAppManager ( rmContext ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    scheduler ,    masterService ,    new   security . ApplicationACLsManager ( conf )  ,    conf )  ;", "ApplicationId   appID    =    MockApps . newAppID (  (  (  ( i    *     4  )     +    j )     +     1  )  )  ;", "asContext . setApplicationId ( appID )  ;", "if    (  ( individualMaxAppAttempts [ i ]  [ j ]  )     !  =     0  )     {", "asContext . setMaxAppAttempts ( individualMaxAppAttempts [ i ]  [ j ]  )  ;", "}", "appMonitor . submitApplication ( asContext ,     \" test \"  )  ;", "RMApp   app    =    rmContext . getRMApps (  )  . get ( appID )  ;", "Assert . assertEquals (  \" max   application   attempts   doesn ' t   match \"  ,    expectedNums [ i ]  [ j ]  ,    app . getMaxAppAttempts (  )  )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  ( getAppEventType (  )  )     =  =     ( RMAppEventType . KILL )  )     &  &     (  ( timeoutSecs +  +  )     <     2  0  )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "}", "setAppEventType ( RMAppEventType . KILL )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testRMAppSubmitMaxAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     2  0  0  0  0  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "int   maxAppsInMemory    =     8  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,    maxAppsInMemory )  ;", "conf . setInt ( RM _ STATE _ STORE _ MAX _ COMPLETED _ APPLICATIONS ,     1  0  0  0  )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    conf )  ;", "addToCompletedApps ( appMonitor ,    rmContext )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect \"  ,     1  0  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "appMonitor . checkAppNumCompletedLimit (  )  ;", "int   numRemoveApps    =     1  0     -    maxAppsInMemory ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   after    #    completed   check \"  ,    maxAppsInMemory ,    rmContext . getRMApps (  )  . size (  )  )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,    maxAppsInMemory ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "verify ( rmContext . getStateStore (  )  ,    times ( numRemoveApps )  )  . removeApplication ( isA ( RMApp . class )  )  ;", "Assert . assertEquals ( maxAppsInMemory ,    appMonitor . getCompletedAppsInStateStore (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStateStoreAppLimitLargerThanMemoryAppLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "RMContext   rmContext    =     . mockRMContext (  1  0  ,     ( now    -     2  0  0  0  0  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "int   maxAppsInMemory    =     8  ;", "int   maxAppsInStateStore    =     4  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,    maxAppsInMemory )  ;", "conf . setInt ( RM _ STATE _ STORE _ MAX _ COMPLETED _ APPLICATIONS ,    maxAppsInStateStore )  ;", ". TestRMAppManager   appMonitor    =    new    . TestRMAppManager ( rmContext ,    conf )  ;", "addToCompletedApps ( appMonitor ,    rmContext )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect \"  ,     1  0  ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "appMonitor . checkAppNumCompletedLimit (  )  ;", "Assert . assertEquals (  \" Number   of   apps   incorrect   after    #    completed   check \"  ,    maxAppsInMemory ,    rmContext . getRMApps (  )  . size (  )  )  ;", "Assert . assertEquals (  \" Number   of   completed   apps   incorrect   after   check \"  ,    maxAppsInMemory ,    appMonitor . getCompletedAppsListSize (  )  )  ;", "int   numRemoveAppsFromStateStore    =     1  0     -    maxAppsInStateStore ;", "verify ( rmContext . getStateStore (  )  ,    times ( numRemoveAppsFromStateStore )  )  . removeApplication ( isA ( RMApp . class )  )  ;", "Assert . assertEquals ( maxAppsInStateStore ,    appMonitor . getCompletedAppsInStateStore (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStateStoreAppLimitLessThanMemoryAppLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestAppManager"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   userUGI    =    UserGroupInformation . createRemoteUser ( user )  ;", "ApplicationClientProtocol   userClient    =    userUGI . doAs ( new   PrivilegedExceptionAction < ApplicationClientProtocol >  (  )     {", "@ Override", "public   ApplicationClientProtocol   run (  )    throws   Exception    {", "return    (  ( ApplicationClientProtocol )     (  . rpc . getProxy ( ApplicationClientProtocol . class ,     . rmAddress ,     . conf )  )  )  ;", "}", "}  )  ;", "return   userClient ;", "}", "METHOD_END"], "methodName": ["getRMClientForUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    RMStateStoreFactory . getStore ( TestApplicationACLs . conf )  ;", "TestApplicationACLs . conf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "AccessControlList   adminACL    =    new   AccessControlList (  \"  \"  )  ;", "adminACL . addGroup ( TestApplicationACLs . SUPER _ GROUP )  ;", "TestApplicationACLs . conf . set ( YARN _ ADMIN _ ACL ,    adminACL . getAclString (  )  )  ;", "TestApplicationACLs . resourceManager    =    new   MockRM ( TestApplicationACLs . conf )     {", "@ Override", "protected   QueueACLsManager   createQueueACLsManager ( ResourceScheduler   scheduler ,    Configuration   conf )     {", "QueueACLsManager   mockQueueACLsManager    =    mock ( QueueACLsManager . class )  ;", "when ( mockQueueACLsManager . checkAccess ( any ( UserGroupInformation . class )  ,    any ( QueueACL . class )  ,    anyString (  )  )  )  . thenAnswer ( new   Answer (  )     {", "public   Object   answer ( InvocationOnMock   invocation )     {", "return   TestApplicationACLs . isQueueUser ;", "}", "}  )  ;", "return   mockQueueACLsManager ;", "}", "protected   ClientRMService   createClientRMService (  )     {", "return   new   ClientRMService ( getRMContext (  )  ,    this . scheduler ,    this . rmAppManager ,    this . applicationACLsManager ,    this . queueACLsManager ,    null )  ;", "}", "}  ;", "new   Thread (  )     {", "public   void   run (  )     {", "UserGroupInformation . createUserForTesting ( TestApplicationACLs . ENEMY ,    new   String [  ]  {        }  )  ;", "UserGroupInformation . createUserForTesting ( TestApplicationACLs . FRIEND ,    new   String [  ]  {    TestApplicationACLs . FRIENDLY _ GROUP    }  )  ;", "UserGroupInformation . createUserForTesting ( TestApplicationACLs . SUPER _ USER ,    new   String [  ]  {    TestApplicationACLs . SUPER _ GROUP    }  )  ;", "TestApplicationACLs . resourceManager . start (  )  ;", "}", "}  . start (  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( TestApplicationACLs . resourceManager . getServiceState (  )  )     =  =     ( STATE . INITED )  )     &  &     (  ( waitCount +  +  )     <     6  0  )  )     {", "TestApplicationACLs . LOG . info (  \" Waiting   for   RM   to   start .  .  .  \"  )  ;", "Thread . sleep (  1  5  0  0  )  ;", "}", "if    (  ( TestApplicationACLs . resourceManager . getServiceState (  )  )     !  =     ( STATE . STARTED )  )     {", "throw   new   IOException (  (  \" ResourceManager   failed   to   start .    Final   state   is    \"     +     ( TestApplicationACLs . resourceManager . getServiceState (  )  )  )  )  ;", "}", "UserGroupInformation   owner    =    UserGroupInformation . createRemoteUser ( TestApplicationACLs . APP _ OWNER )  ;", "TestApplicationACLs . rmClient    =    owner . doAs ( new   PrivilegedExceptionAction < ApplicationClientProtocol >  (  )     {", "@ Override", "public   ApplicationClientProtocol   run (  )    throws   Exception    {", "return    (  ( ApplicationClientProtocol )     ( TestApplicationACLs . rpc . getProxy ( ApplicationClientProtocol . class ,    TestApplicationACLs . rmAddress ,    TestApplicationACLs . conf )  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "SubmitApplicationRequest   submitRequest    =    TestApplicationACLs . recordFactory . newRecordInstance ( SubmitApplicationRequest . class )  ;", "ApplicationSubmissionContext   context    =    TestApplicationACLs . recordFactory . newRecordInstance ( ApplicationSubmissionContext . class )  ;", "ApplicationId   applicationId    =    TestApplicationACLs . rmClient . getNewApplication ( TestApplicationACLs . recordFactory . newRecordInstance ( GetNewApplicationRequest . class )  )  . getApplicationId (  )  ;", "context . setApplicationId ( applicationId )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "acls . put ( VIEW _ APP ,    viewACL . getAclString (  )  )  ;", "acls . put ( MODIFY _ APP ,    modifyACL . getAclString (  )  )  ;", "ContainerLaunchContext   amContainer    =    TestApplicationACLs . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "Resource   resource    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "context . setResource ( resource )  ;", "amContainer . setApplicationACLs ( acls )  ;", "context . setAMContainerSpec ( amContainer )  ;", "submitRequest . setApplicationSubmissionContext ( context )  ;", "TestApplicationACLs . rmClient . submitApplication ( submitRequest )  ;", "TestApplicationACLs . resourceManager . waitForState ( applicationId ,    RMAppState . ACCEPTED )  ;", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["submitAppAndGetAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestApplicationACLs . resourceManager )     !  =    null )     {", "TestApplicationACLs . resourceManager . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "verifyOwnerAccess (  )  ;", "verifySuperUserAccess (  )  ;", "verifyFriendAccess (  )  ;", "verifyEnemyAccess (  )  ;", "verifyAdministerQueueUserAccess (  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationACLs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "TestApplicationACLs . isQueueUser    =    true ;", "AccessControlList   viewACL    =    new   AccessControlList (  \"  \"  )  ;", "viewACL . addGroup ( TestApplicationACLs . FRIENDLY _ GROUP )  ;", "AccessControlList   modifyACL    =    new   AccessControlList (  \"  \"  )  ;", "modifyACL . addUser ( TestApplicationACLs . FRIEND )  ;", "ApplicationId   applicationId    =    submitAppAndGetAppId ( viewACL ,    modifyACL )  ;", "final   GetApplicationReportRequest   appReportRequest    =    TestApplicationACLs . recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "appReportRequest . setApplicationId ( applicationId )  ;", "final   KillApplicationRequest   finishAppRequest    =    TestApplicationACLs . recordFactory . newRecordInstance ( KillApplicationRequest . class )  ;", "finishAppRequest . setApplicationId ( applicationId )  ;", "ApplicationClientProtocol   administerQueueUserRmClient    =    getRMClientForUser ( TestApplicationACLs . QUEUE _ ADMIN _ USER )  ;", "administerQueueUserRmClient . getApplicationReport ( appReportRequest )  ;", "Assert . assertEquals (  \" App   view   by   queue - admin - user   should   list   the   apps !  !  \"  ,     5  ,    administerQueueUserRmClient . getApplications ( TestApplicationACLs . recordFactory . newRecordInstance ( GetApplicationsRequest . class )  )  . getApplicationList (  )  . size (  )  )  ;", "administerQueueUserRmClient . forceKillApplication ( finishAppRequest )  ;", "TestApplicationACLs . resourceManager . waitForState ( applicationId ,    RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["verifyAdministerQueueUserAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "AccessControlList   viewACL    =    new   AccessControlList (  \"  \"  )  ;", "viewACL . addGroup (  . FRIENDLY _ GROUP )  ;", "AccessControlList   modifyACL    =    new   AccessControlList (  \"  \"  )  ;", "modifyACL . addUser (  . FRIEND )  ;", "ApplicationId   applicationId    =    submitAppAndGetAppId ( viewACL ,    modifyACL )  ;", "final   GetApplicationReportRequest   appReportRequest    =     . recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "appReportRequest . setApplicationId ( applicationId )  ;", "final   KillApplicationRequest   finishAppRequest    =     . recordFactory . newRecordInstance ( KillApplicationRequest . class )  ;", "finishAppRequest . setApplicationId ( applicationId )  ;", "ApplicationClientProtocol   enemyRmClient    =    getRMClientForUser (  . ENEMY )  ;", "ApplicationReport   appReport    =    enemyRmClient . getApplicationReport ( appReportRequest )  . getApplicationReport (  )  ;", "verifyEnemyAppReport ( appReport )  ;", "List < ApplicationReport >    appReports    =    enemyRmClient . getApplications (  . recordFactory . newRecordInstance ( GetApplicationsRequest . class )  )  . getApplicationList (  )  ;", "Assert . assertEquals (  \" App   view   by   enemy   should   list   the   apps !  !  \"  ,     4  ,    appReports . size (  )  )  ;", "for    ( ApplicationReport   report    :    appReports )     {", "verifyEnemyAppReport ( report )  ;", "}", "try    {", "enemyRmClient . forceKillApplication ( finishAppRequest )  ;", "Assert . fail (  \" App   killing   by   the   enemy   should   fail !  !  \"  )  ;", "}    catch    ( YarnException   e )     {", ". LOG . info (  \" Got   exception   while   killing   app   as   the   enemy \"  ,    e )  ;", "Assert . assertTrue ( e . getMessage (  )  . contains (  (  \" User   enemy   cannot   perform   operation   MODIFY _ APP   on    \"     +    applicationId )  )  )  ;", "}", ". rmClient . forceKillApplication ( finishAppRequest )  ;", "}", "METHOD_END"], "methodName": ["verifyEnemyAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals (  \" Enemy   should   not   see   app   host !  \"  ,    TestApplicationACLs . UNAVAILABLE ,    appReport . getHost (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   rpc   port !  \"  ,     (  -  1  )  ,    appReport . getRpcPort (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   client   token !  \"  ,    null ,    appReport . getClientToAMToken (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   diagnostics !  \"  ,    TestApplicationACLs . UNAVAILABLE ,    appReport . getDiagnostics (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   tracking   url !  \"  ,    TestApplicationACLs . UNAVAILABLE ,    appReport . getTrackingUrl (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   original   tracking   url !  \"  ,    TestApplicationACLs . UNAVAILABLE ,    appReport . getOriginalTrackingUrl (  )  )  ;", "ApplicationResourceUsageReport   usageReport    =    appReport . getApplicationResourceUsageReport (  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   used   containers \"  ,     (  -  1  )  ,    usageReport . getNumUsedContainers (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   reserved   containers \"  ,     (  -  1  )  ,    usageReport . getNumReservedContainers (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   used   resources \"  ,     (  -  1  )  ,    usageReport . getUsedResources (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   reserved   resources \"  ,     (  -  1  )  ,    usageReport . getReservedResources (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  \" Enemy   should   not   see   app   needed   resources \"  ,     (  -  1  )  ,    usageReport . getNeededResources (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyEnemyAppReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "AccessControlList   viewACL    =    new   AccessControlList (  \"  \"  )  ;", "viewACL . addGroup (  . FRIENDLY _ GROUP )  ;", "AccessControlList   modifyACL    =    new   AccessControlList (  \"  \"  )  ;", "modifyACL . addUser (  . FRIEND )  ;", "ApplicationId   applicationId    =    submitAppAndGetAppId ( viewACL ,    modifyACL )  ;", "final   GetApplicationReportRequest   appReportRequest    =     . recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "appReportRequest . setApplicationId ( applicationId )  ;", "final   KillApplicationRequest   finishAppRequest    =     . recordFactory . newRecordInstance ( KillApplicationRequest . class )  ;", "finishAppRequest . setApplicationId ( applicationId )  ;", "ApplicationClientProtocol   friendClient    =    getRMClientForUser (  . FRIEND )  ;", "friendClient . getApplicationReport ( appReportRequest )  ;", "Assert . assertEquals (  \" App   view   by   a   friend   should   list   the   apps !  !  \"  ,     3  ,    friendClient . getApplications (  . recordFactory . newRecordInstance ( GetApplicationsRequest . class )  )  . getApplicationList (  )  . size (  )  )  ;", "friendClient . forceKillApplication ( finishAppRequest )  ;", ". resourceManager . waitForState ( applicationId ,    RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["verifyFriendAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "AccessControlList   viewACL    =    new   AccessControlList (  \"  \"  )  ;", "viewACL . addGroup (  . FRIENDLY _ GROUP )  ;", "AccessControlList   modifyACL    =    new   AccessControlList (  \"  \"  )  ;", "modifyACL . addUser (  . FRIEND )  ;", "ApplicationId   applicationId    =    submitAppAndGetAppId ( viewACL ,    modifyACL )  ;", "final   GetApplicationReportRequest   appReportRequest    =     . recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "appReportRequest . setApplicationId ( applicationId )  ;", "final   KillApplicationRequest   finishAppRequest    =     . recordFactory . newRecordInstance ( KillApplicationRequest . class )  ;", "finishAppRequest . setApplicationId ( applicationId )  ;", ". rmClient . getApplicationReport ( appReportRequest )  ;", "Assert . assertEquals (  \" App   view   by   owner   should   list   the   apps !  !  \"  ,     1  ,     . rmClient . getApplications (  . recordFactory . newRecordInstance ( GetApplicationsRequest . class )  )  . getApplicationList (  )  . size (  )  )  ;", ". rmClient . forceKillApplication ( finishAppRequest )  ;", ". resourceManager . waitForState ( applicationId ,    RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["verifyOwnerAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "AccessControlList   viewACL    =    new   AccessControlList (  \"  \"  )  ;", "viewACL . addGroup (  . FRIENDLY _ GROUP )  ;", "AccessControlList   modifyACL    =    new   AccessControlList (  \"  \"  )  ;", "modifyACL . addUser (  . FRIEND )  ;", "ApplicationId   applicationId    =    submitAppAndGetAppId ( viewACL ,    modifyACL )  ;", "final   GetApplicationReportRequest   appReportRequest    =     . recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "appReportRequest . setApplicationId ( applicationId )  ;", "final   KillApplicationRequest   finishAppRequest    =     . recordFactory . newRecordInstance ( KillApplicationRequest . class )  ;", "finishAppRequest . setApplicationId ( applicationId )  ;", "ApplicationClientProtocol   superUserClient    =    getRMClientForUser (  . SUPER _ USER )  ;", "superUserClient . getApplicationReport ( appReportRequest )  ;", "Assert . assertEquals (  \" App   view   by   super - user   should   list   the   apps !  !  \"  ,     2  ,    superUserClient . getApplications (  . recordFactory . newRecordInstance ( GetApplicationsRequest . class )  )  . getApplicationList (  )  . size (  )  )  ;", "superUserClient . forceKillApplication ( finishAppRequest )  ;", ". resourceManager . waitForState ( applicationId ,    RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["verifySuperUserAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs"}, {"methodBody": ["METHOD_START", "{", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "nm . nodeHeartbeat ( true )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "rm . waitForState ( app . geId (  )  ,    RMAppState . RUNNING )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["launchAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "TestApplicationCleanup   t    =    new   TestApplicationCleanup (  )  ;", "t . testAppCleanup (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "conf    =    new   YarnConfiguration (  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "conf . set ( RECOVERY _ ENABLED ,     \" true \"  )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", "Assert . assertTrue (  (  ( YarnConfiguration . DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )     >     1  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "MockRM   rm    =    new   MockRM (  )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     5  0  0  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "int   request    =     2  ;", "am . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  0  0  ,    request ,    new   ArrayList < ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "List < Container >    conts    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "int   contReceived    =    conts . size (  )  ;", "int   waitCount    =     0  ;", "while    (  ( contReceived    <    request )     &  &     (  ( waitCount +  +  )     <     2  0  0  )  )     {", "TestApplicationCleanup . LOG . info (  (  (  (  \" Got    \"     +    contReceived )     +     \"    containers .    Waiting   to   get    \"  )     +    request )  )  ;", "Thread . sleep (  1  0  0  )  ;", "conts    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "contReceived    +  =    conts . size (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "}", "Assert . assertEquals ( request ,    contReceived )  ;", "am . unregisterAppAttempt (  )  ;", "NodeHeartbeatResponse   resp    =    nm 1  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "resp    =    nm 1  . nodeHeartbeat ( true )  ;", "List < ContainerId >    containersToCleanup    =    resp . getContainersToCleanup (  )  ;", "List < ApplicationId >    appsToCleanup    =    resp . getApplicationsToCleanup (  )  ;", "int   numCleanedContainers    =    containersToCleanup . size (  )  ;", "int   numCleanedApps    =    appsToCleanup . size (  )  ;", "waitCount    =     0  ;", "while    (  (  ( numCleanedContainers    <     2  )     |  |     ( numCleanedApps    <     1  )  )     &  &     (  ( waitCount +  +  )     <     2  0  0  )  )     {", "TestApplicationCleanup . LOG . info (  (  (  (  \" Waiting   to   get   cleanup   events .  .    cleanedConts :     \"     +    numCleanedContainers )     +     \"    cleanedApps :     \"  )     +    numCleanedApps )  )  ;", "Thread . sleep (  1  0  0  )  ;", "resp    =    nm 1  . nodeHeartbeat ( true )  ;", "List < ContainerId >    deltaContainersToCleanup    =    resp . getContainersToCleanup (  )  ;", "List < ApplicationId >    deltaAppsToCleanup    =    resp . getApplicationsToCleanup (  )  ;", "containersToCleanup . addAll ( deltaContainersToCleanup )  ;", "appsToCleanup . addAll ( deltaAppsToCleanup )  ;", "numCleanedContainers    =    containersToCleanup . size (  )  ;", "numCleanedApps    =    appsToCleanup . size (  )  ;", "}", "Assert . assertEquals (  1  ,    appsToCleanup . size (  )  )  ;", "Assert . assertEquals ( app . getApplicationId (  )  ,    appsToCleanup . get (  0  )  )  ;", "Assert . assertEquals (  1  ,    numCleanedApps )  ;", "Assert . assertEquals (  2  ,    numCleanedContainers )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppCleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 0  . geAttemptId (  )  ,     1  ,    COMPLETE )  ;", "rm 1  . waitForState ( app 0  . geId (  )  ,    RMAppState . FAILED )  ;", "waitForAppCleanupMessageRecved ( nm 1  ,    app 0  . geId (  )  )  ;", "nm 1  . registerNode ( Arrays . asList ( app 0  . geId (  )  )  )  ;", "waitForAppCleanupMessageRecved ( nm 1  ,    app 0  . geId (  )  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppCleanupWhenNMReconnects"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 0  . geAttemptId (  )  ,     1  ,    COMPLETE )  ;", "rm 1  . waitForState ( app 0  . geId (  )  ,    RMAppState . FAILED )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode ( Arrays . asList ( app 0  . geId (  )  )  )  ;", "rm 2  . waitForState ( app 0  . geId (  )  ,    RMAppState . FAILED )  ;", "waitForAppCleanupMessageRecved ( nm 1  ,    app 0  . geId (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppCleanupWhenRMRestartedAfterAppFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  0  2  4  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  5  6  7  8  \"  ,     1  0  2  4  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 2  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "AllocateResponse   allocResponse    =    am 0  . allocate ( Arrays . asList ( ResourceRequest . newInstance ( Priority . newInstance (  1  )  ,     \"  *  \"  ,    Resource . newInstance (  1  0  2  4  ,     0  )  ,     1  )  )  ,    null )  ;", "while    (  ( null    =  =     ( allocResponse . getAllocatedContainers (  )  )  )     |  |     ( allocResponse . getAllocatedContainers (  )  . isEmpty (  )  )  )     {", "nm 2  . nodeHeartbeat ( true )  ;", "allocResponse    =    am 0  . allocate ( null ,    null )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode ( Arrays . asList ( NMContainerStatus . newInstance ( ContainerId . newInstance ( am 0  . geAttemptId (  )  ,     1  )  ,    COMPLETE ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \"  \"  ,     0  ,    Priority . newInstance (  0  )  ,     1  2  3  4  )  )  ,    Arrays . asList ( app 0  . geId (  )  )  )  ;", "nm 2  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 2  . registerNode ( Arrays . asList ( app 0  . geId (  )  )  )  ;", "rm 2  . waitForState ( app 0  . geId (  )  ,    RMAppState . FAILED )  ;", "waitForAppCleanupMessageRecved ( nm 1  ,    app 0  . geId (  )  )  ;", "waitForAppCleanupMessageRecved ( nm 2  ,    app 0  . geId (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppCleanupWhenRMRestartedBeforeAppFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "final   DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "MockRM   rm    =    new   MockRM (  )     {", "@ Override", "protected   EventHandler < SchedulerEvent >    createSchedulerEventDispatcher (  )     {", "return   new   ResourceManager . SchedulerEventDispatcher ( this . scheduler )     {", "@ Override", "public   void   handle ( SchedulerEvent   event )     {", "this . scheduler . handle ( event )  ;", "}", "}  ;", "}", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "}  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     5  0  0  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "int   request    =     2  ;", "am . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  0  0  ,    request ,    new   ArrayList < ContainerId >  (  )  )  ;", "dispatcher . await (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "List < Container >    conts    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "int   contReceived    =    conts . size (  )  ;", "int   waitCount    =     0  ;", "while    (  ( contReceived    <    request )     &  &     (  ( waitCount +  +  )     <     2  0  0  )  )     {", "TestApplicationCleanup . LOG . info (  (  (  (  \" Got    \"     +    contReceived )     +     \"    containers .    Waiting   to   get    \"  )     +    request )  )  ;", "Thread . sleep (  1  0  0  )  ;", "conts    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "dispatcher . await (  )  ;", "contReceived    +  =    conts . size (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "}", "Assert . assertEquals ( request ,    contReceived )  ;", "ArrayList < ContainerId >    release    =    new   ArrayList < ContainerId >  (  )  ;", "release . add ( conts . get (  0  )  . getId (  )  )  ;", "am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    release )  ;", "dispatcher . await (  )  ;", "Map < ApplicationId ,    List < ContainerStatus >  >    containerStatuses    =    new   HashMap < ApplicationId ,    List < ContainerStatus >  >  (  )  ;", "ArrayList < ContainerStatus >    containerStatusList    =    new   ArrayList < ContainerStatus >  (  )  ;", "containerStatusList . add ( BuilderUtils . newContainerStatus ( conts . get (  0  )  . getId (  )  ,    RUNNING ,     \" nothing \"  ,     0  )  )  ;", "containerStatuses . put ( app . getApplicationId (  )  ,    containerStatusList )  ;", "NodeHeartbeatResponse   resp    =    nm 1  . nodeHeartbeat ( containerStatuses ,    true )  ;", "waitForContainerCleanup ( dispatcher ,    nm 1  ,    resp )  ;", "TestApplicationCleanup . LOG . info (  (  \" Testing   container   launch   much   after   release   and    \"     +     \" NM   getting   cleanup \"  )  )  ;", "containerStatuses . clear (  )  ;", "containerStatusList . clear (  )  ;", "containerStatusList . add ( BuilderUtils . newContainerStatus ( conts . get (  0  )  . getId (  )  ,    RUNNING ,     \" nothing \"  ,     0  )  )  ;", "containerStatuses . put ( app . getApplicationId (  )  ,    containerStatusList )  ;", "resp    =    nm 1  . nodeHeartbeat ( containerStatuses ,    true )  ;", "waitForContainerCleanup ( dispatcher ,    nm 1  ,    resp )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testContainerCleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "final   DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )     {", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "}  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 0  . geAttemptId (  )  ,     1  ,    RUNNING )  ;", "rm 1  . waitForState ( app 0  . geId (  )  ,    RMAppState . RUNNING )  ;", "final   DrainDispatcher   dispatcher 2     =    new   DrainDispatcher (  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )     {", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher 2  ;", "}", "}  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode ( Arrays . asList ( app 0  . geId (  )  )  )  ;", "rm 2  . waitForState ( app 0  . geId (  )  ,    RMAppState . ACCEPTED )  ;", "NodeHeartbeatResponse   response    =    nm 1  . nodeHeartbeat ( am 0  . geAttemptId (  )  ,     2  ,    RUNNING )  ;", "waitForContainerCleanup ( dispatcher 2  ,    nm 1  ,    response )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testContainerCleanupWhenRMRestartedAppNotRegistered"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "while    ( true )     {", "NodeHeartbeatResponse   response    =    nm . nodeHeartbeat ( true )  ;", "if    (  (  (  ( response . getApplicationsToCleanup (  )  )     !  =    null )     &  &     (  ( response . getApplicationsToCleanup (  )  . size (  )  )     =  =     1  )  )     &  &     ( appId . equals ( response . getApplicationsToCleanup (  )  . get (  0  )  )  )  )     {", "return ;", "}", ". LOG . info (  (  (  (  \" Haven ' t   got   application =  \"     +     ( appId . toString (  )  )  )     +     \"    in   cleanup   list   from   node   heartbeat   response ,     \"  )     +     \" sleep   for   a   while   before   next   heartbeat \"  )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForAppCleanupMessageRecved"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "int   waitCount    =     0  ;", "int   cleanedConts    =     0  ;", "List < ContainerId >    contsToClean ;", "do    {", "dispatcher . await (  )  ;", "contsToClean    =    resp . getContainersToCleanup (  )  ;", "cleanedConts    +  =    contsToClean . size (  )  ;", "if    ( cleanedConts    >  =     1  )     {", "break ;", "}", "Thread . sleep (  1  0  0  )  ;", "resp    =    nm . nodeHeartbeat ( true )  ;", "}    while    (  ( waitCount +  +  )     <     2  0  0     )  ;", "if    ( contsToClean . isEmpty (  )  )     {", ". LOG . error (  \" Failed   to   get   any   containers   to   cleanup \"  )  ;", "} else    {", ". LOG . info (  (  \" Got   cleanup   for    \"     +     ( contsToClean . get (  0  )  )  )  )  ;", "}", "Assert . assertEquals (  1  ,    cleanedConts )  ;", "}", "METHOD_END"], "methodName": ["waitForContainerCleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", ". MyContainerManagerImpl   containerManager    =    new    . MyContainerManagerImpl (  )  ;", "MockRMWithCustomAMLauncher   rm    =    new   MockRMWithCustomAMLauncher ( containerManager )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "int   waitCount    =     0  ;", "while    (  (  ( containerManager . launched )     =  =    false )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", ". LOG . info (  \" Waiting   for   AM   Launch   to   happen .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertTrue ( containerManager . launched )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   appAttemptId    =    attempt . getAppAttemptId (  )  ;", "Assert . assertEquals ( appAttemptId . toString (  )  ,    containerManager . attemptIdAtContainerManager )  ;", "Assert . assertEquals ( app . getSubmitTime (  )  ,    containerManager . submitTimeAtContainerManager )  ;", "Assert . assertEquals ( app . getRMAppAttempt ( appAttemptId )  . getMasterContainer (  )  . getId (  )  . toString (  )  ,    containerManager . containerIdAtContainerManager )  ;", "Assert . assertEquals ( nm 1  . getNodeId (  )  . toString (  )  ,    containerManager . nmHostAtContainerManager )  ;", "Assert . assertEquals ( DEFAULT _ RM _ AM _ MAX _ ATTEMPTS ,    containerManager . maxAppAttempts )  ;", "MockAM   am    =    new   MockAM ( rm . getRMContext (  )  ,    rm . getApplicationMasterService (  )  ,    appAttemptId )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "nm 1  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "waitCount    =     0  ;", "while    (  (  ( containerManager . cleanedup )     =  =    false )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", ". LOG . info (  \" Waiting   for   AM   Cleanup   to   happen .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertTrue ( containerManager . cleanedup )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAMLaunchAndCleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "boolean   thrown    =    false ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "MockRM   rm    =    new   MockRM (  )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  0  0  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "int   request    =     2  ;", "AllocateResponse   ar    =    am . allocate (  \" h 1  \"  ,     1  0  0  0  ,    request ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "Assert . assertTrue (  (  ( ar . getAMCommand (  )  )     =  =     ( AMCommand . AM _ RESYNC )  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "AllocateResponse   amrs    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "Assert . assertTrue (  (  ( ar . getAMCommand (  )  )     =  =     ( AMCommand . AM _ RESYNC )  )  )  ;", "am . registerAppAttempt (  )  ;", "thrown    =    false ;", "try    {", "am . registerAppAttempt ( false )  ;", "}    catch    ( Exception   e )     {", "Assert . assertEquals (  (  \" Application   Master   is   already   registered    :     \"     +     ( attempt . getAppAttemptId (  )  . getApplicationId (  )  )  )  ,    e . getMessage (  )  )  ;", "thrown    =    true ;", "}", "Assert . assertTrue ( thrown )  ;", "am . unregisterAppAttempt (  )  ;", "nm 1  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "AllocateResponse   amrs 2     =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "Assert . assertTrue (  (  ( amrs 2  . getAMCommand (  )  )     =  =     ( AMCommand . AM _ SHUTDOWN )  )  )  ;", "}", "METHOD_END"], "methodName": ["testallocateBeforeAMRegistration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterLauncher"}, {"methodBody": ["METHOD_START", "{", "TestApplicationMasterService . conf    =    new   YarnConfiguration (  )  ;", "TestApplicationMasterService . conf . setClass ( RM _ SCHEDULER ,    FifoScheduler . class ,    ResourceScheduler . class )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( TestApplicationMasterService . conf )  ;", "try    {", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp (  2  0  4  8  )  ;", "MockAM   am 1     =    MockRM . launchAM ( app 1  ,    rm ,    nm 1  )  ;", "FinishApplicationMasterRequest   req    =    FinishApplicationMasterRequest . newInstance ( FAILED ,     \"  \"  ,     \"  \"  )  ;", "Throwable   cause    =    null ;", "try    {", "am 1  . unregisterAppAttempt ( req ,    false )  ;", "}    catch    ( Exception   e )     {", "cause    =    e . getCause (  )  ;", "}", "Assert . assertNotNull ( cause )  ;", "Assert . assertTrue (  ( cause   instanceof   ApplicationMasterNotRegisteredException )  )  ;", "Assert . assertNotNull ( cause . getMessage (  )  )  ;", "Assert . assertTrue ( cause . getMessage (  )  . contains (  \" Application   Master   is   trying   to   unregister   before   registering   for :  \"  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "am 1  . unregisterAppAttempt ( req ,    false )  ;", "}    finally    {", "if    ( rm    !  =    null )     {", "rm . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFinishApplicationMasterBeforeRegistering"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( TestApplicationMasterService . conf )  ;", "try    {", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp (  1  0  2  4  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "am 1  . addRequests ( new   String [  ]  {     \"  1  2  7  .  0  .  0  .  1  \"     }  ,    GB ,     1  ,     1  )  ;", "AllocateResponse   alloc 1 Response    =    am 1  . schedule (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "while    (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     <     1  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   containers   to   be   created   for   app    1  .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "alloc 1 Response    =    am 1  . schedule (  )  ;", "}", "Assert . assertTrue (  (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     >     0  )  )  ;", "RMApp   app 2     =    rm . submitApp (  1  0  2  4  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 2     =    app 2  . getCurrentAppAttempt (  )  ;", "MockAM   am 2     =    rm . sendAMLaunched ( attempt 2  . getAppAttemptId (  )  )  ;", "am 2  . registerAppAttempt (  )  ;", "ContainerId   cId    =    alloc 1 Response . getAllocatedContainers (  )  . get (  0  )  . getId (  )  ;", "am 2  . addContainerToBeReleased ( cId )  ;", "try    {", "am 2  . schedule (  )  ;", "Assert . fail (  \" Exception   was   expected !  !  \"  )  ;", "}    catch    ( InvalidContainerReleaseException   e )     {", "StringBuilder   sb    =    new   StringBuilder (  \" Cannot   release   container    :     \"  )  ;", "sb . append ( cId . toString (  )  )  ;", "sb . append (  \"    not   belonging   to   this   application   attempt    :     \"  )  ;", "sb . append ( attempt 2  . getAppAttemptId (  )  . toString (  )  )  ;", "Assert . assertTrue ( e . getMessage (  )  . contains ( sb . toString (  )  )  )  ;", "}", "}    finally    {", "if    ( rm    !  =    null )     {", "rm . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testInvalidContainerReleaseRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( TestApplicationMasterService . conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp (  2  0  4  8  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "AllocateRequestPBImpl   allocateRequest    =    new   AllocateRequestPBImpl (  )  ;", "List < ContainerId >    release    =    new   ArrayList < ContainerId >  (  )  ;", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "allocateRequest . setReleaseList ( release )  ;", "allocateRequest . setAskList ( ask )  ;", "allocateRequest . setProgress ( Float . POSITIVE _ INFINITY )  ;", "am 1  . allocate ( allocateRequest )  ;", "while    (  ( attempt 1  . getProgress (  )  )     !  =     1  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   allocate   event   to   be   handled    .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "allocateRequest . setProgress ( Float . NaN )  ;", "am 1  . allocate ( allocateRequest )  ;", "while    (  ( attempt 1  . getProgress (  )  )     !  =     0  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   allocate   event   to   be   handled    .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "allocateRequest . setProgress (  (  ( float )     (  9  )  )  )  ;", "am 1  . allocate ( allocateRequest )  ;", "while    (  ( attempt 1  . getProgress (  )  )     !  =     1  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   allocate   event   to   be   handled    .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "allocateRequest . setProgress ( Float . NEGATIVE _ INFINITY )  ;", "am 1  . allocate ( allocateRequest )  ;", "while    (  ( attempt 1  . getProgress (  )  )     !  =     0  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   allocate   event   to   be   handled    .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "allocateRequest . setProgress (  (  ( float )     (  0  .  5  )  )  )  ;", "am 1  . allocate ( allocateRequest )  ;", "while    (  ( attempt 1  . getProgress (  )  )     !  =     0  .  5  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   allocate   event   to   be   handled    .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "allocateRequest . setProgress (  (  ( float )     (  -  1  )  )  )  ;", "am 1  . allocate ( allocateRequest )  ;", "while    (  ( attempt 1  . getProgress (  )  )     !  =     0  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   allocate   event   to   be   handled    .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["testProgressFilter"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( TestApplicationMasterService . conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp (  2  0  4  8  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "am 1  . addRequests ( new   String [  ]  {     \"  1  2  7  .  0  .  0  .  1  \"     }  ,    GB ,     1  ,     1  )  ;", "AllocateResponse   alloc 1 Response    =    am 1  . schedule (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "while    (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     <     1  )     {", "TestApplicationMasterService . LOG . info (  \" Waiting   for   containers   to   be   created   for   app    1  .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "alloc 1 Response    =    am 1  . schedule (  )  ;", "}", "Container   allocatedContainer    =    alloc 1 Response . getAllocatedContainers (  )  . get (  0  )  ;", "ContainerTokenIdentifier   tokenId    =    BuilderUtils . newContainerTokenIdentifier ( allocatedContainer . getContainerToken (  )  )  ;", "Assert . assertEquals ( MockRM . getClusterTimeStamp (  )  ,    tokenId . getRMIdentifer (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMIdentifierOnContainerAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestApplicationMasterService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "final   ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,     . dtsm )  ;", "checkTokenCancellation ( rmService ,    owner ,    renewer )  ;", "}", "METHOD_END"], "methodName": ["checkTokenCancellation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMDelegationTokenIdentifier   tokenIdentifier    =    new   RMDelegationTokenIdentifier ( new   Text ( owner . getUserName (  )  )  ,    new   Text ( renewer . getUserName (  )  )  ,    null )  ;", "Token <  ?  >    token    =    new   Token < RMDelegationTokenIdentifier >  ( tokenIdentifier ,    TestClientRMService . dtsm )  ;", "api . records . Token   dToken    =    BuilderUtils . newDelegationToken ( token . getIdentifier (  )  ,    token . getKind (  )  . toString (  )  ,    token . getPassword (  )  ,    token . getService (  )  . toString (  )  )  ;", "CancelDelegationTokenRequest   request    =    Records . newRecord ( CancelDelegationTokenRequest . class )  ;", "request . setDelegationToken ( dToken )  ;", "rmService . cancelDelegationToken ( request )  ;", "}", "METHOD_END"], "methodName": ["checkTokenCancellation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMDelegationTokenIdentifier   tokenIdentifier    =    new   RMDelegationTokenIdentifier ( new   Text ( owner . getUserName (  )  )  ,    new   Text ( renewer . getUserName (  )  )  ,    null )  ;", "Token <  ?  >    token    =    new   Token < RMDelegationTokenIdentifier >  ( tokenIdentifier ,    TestClientRMService . dtsm )  ;", "api . records . Token   dToken    =    BuilderUtils . newDelegationToken ( token . getIdentifier (  )  ,    token . getKind (  )  . toString (  )  ,    token . getPassword (  )  ,    token . getService (  )  . toString (  )  )  ;", "RenewDelegationTokenRequest   request    =    Records . newRecord ( RenewDelegationTokenRequest . class )  ;", "request . setDelegationToken ( dToken )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,    TestClientRMService . dtsm )  ;", "rmService . renewDelegationToken ( request )  ;", "}", "METHOD_END"], "methodName": ["checkTokenRenewal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnScheduler   yarnScheduler    =    TestClientRMService . mockYarnScheduler (  )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "mockRMContext ( yarnScheduler ,    rmContext )  ;", "ConcurrentHashMap < ApplicationId ,    RMApp >    apps    =    getRMApps ( rmContext ,    yarnScheduler )  ;", "when ( rmContext . getRMApps (  )  )  . thenReturn ( apps )  ;", "RMAppManager   appManager    =    new   RMAppManager ( rmContext ,    yarnScheduler ,    null ,    mock ( ApplicationACLsManager . class )  ,    new   Configuration (  )  )  ;", "when ( rmContext . getDispatcher (  )  . getEventHandler (  )  )  . thenReturn ( new   event . EventHandler < Event >  (  )     {", "public   void   handle ( Event   event )     {", "}", "}  )  ;", "ApplicationACLsManager   mockAclsManager    =    mock ( ApplicationACLsManager . class )  ;", "QueueACLsManager   mockQueueACLsManager    =    mock ( QueueACLsManager . class )  ;", "when ( mockQueueACLsManager . checkAccess ( any ( UserGroupInformation . class )  ,    any ( QueueACL . class )  ,    anyString (  )  )  )  . thenReturn ( true )  ;", "return   new   ClientRMService ( rmContext ,    yarnScheduler ,    appManager ,    mockAclsManager ,    mockQueueACLsManager ,    null )  ;", "}", "METHOD_END"], "methodName": ["createRMService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationAttemptId . newInstance ( TestClientRMService . getApplicationId ( id )  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationId . newInstance (  1  2  3  4  5  6  ,    id )  ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   asContext    =    mock ( ApplicationSubmissionContext . class )  ;", "when ( asContext . getMaxAppAttempts (  )  )  . thenReturn (  1  )  ;", "RMAppImpl   app    =    spy ( new   RMAppImpl ( applicationId 3  ,    rmContext ,    config ,    null ,    null ,    queueName ,    asContext ,    yarnScheduler ,    null ,    System . currentTimeMillis (  )  ,     \" YARN \"  ,    null )  )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  5  6  ,     1  )  ,     1  )  ;", "RMAppAttemptImpl   rmAppAttemptImpl    =    spy ( new   RMAppAttemptImpl ( attemptId ,    rmContext ,    yarnScheduler ,    null ,    asContext ,    config ,    false )  )  ;", "Container   container    =    Container . newInstance ( ContainerId . newInstance ( attemptId ,     1  )  ,    null ,     \"  \"  ,    null ,    null ,    null )  ;", "RMContainerImpl   containerimpl    =    spy ( new   RMContainerImpl ( container ,    attemptId ,    null ,     \"  \"  ,    rmContext )  )  ;", "Map < ApplicationAttemptId ,    RMAppAttempt >    attempts    =    new   HashMap < ApplicationAttemptId ,    RMAppAttempt >  (  )  ;", "attempts . put ( attemptId ,    rmAppAttemptImpl )  ;", "when ( app . getCurrentAppAttempt (  )  )  . thenReturn ( rmAppAttemptImpl )  ;", "when ( app . getAppAttempts (  )  )  . thenReturn ( attempts )  ;", "when ( rmAppAttemptImpl . getMasterContainer (  )  )  . thenReturn ( container )  ;", "RScheduler   rs    =    mock ( RScheduler . class )  ;", "when ( rmContext . getScheduler (  )  )  . thenReturn ( rs )  ;", "when ( rmContext . getScheduler (  )  . getRMContainer ( any ( ContainerId . class )  )  )  . thenReturn ( containerimpl )  ;", "SchedulerAppReport   sAppReport    =    mock ( SchedulerAppReport . class )  ;", "when ( rmContext . getScheduler (  )  . getSchedulerAppInfo ( any ( ApplicationAttemptId . class )  )  )  . thenReturn ( sAppReport )  ;", "List < RMContainer >    rmContainers    =    new   ArrayList < RMContainer >  (  )  ;", "rmContainers . add ( containerimpl )  ;", "when ( rmContext . getScheduler (  )  . getSchedulerAppInfo ( attemptId )  . getLiveContainers (  )  )  . thenReturn ( rmContainers )  ;", "ContainerStatus   cs    =    mock ( ContainerStatus . class )  ;", "when ( containerimpl . getFinishedStatus (  )  )  . thenReturn ( cs )  ;", "when ( containerimpl . getDiagnosticsInfo (  )  )  . thenReturn (  \" N / A \"  )  ;", "when ( containerimpl . getContainerExitStatus (  )  )  . thenReturn (  0  )  ;", "when ( containerimpl . getContainerState (  )  )  . thenReturn ( COMPLETE )  ;", "return   app ;", "}", "METHOD_END"], "methodName": ["getRMApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ConcurrentHashMap < ApplicationId ,    RMApp >    apps    =    new   ConcurrentHashMap < ApplicationId ,    RMApp >  (  )  ;", "ApplicationId   applicationId 1     =     . getApplicationId (  1  )  ;", "ApplicationId   applicationId 2     =     . getApplicationId (  2  )  ;", "ApplicationId   applicationId 3     =     . getApplicationId (  3  )  ;", "YarnConfiguration   config    =    new   YarnConfiguration (  )  ;", "apps . put ( applicationId 1  ,    getRMApp ( rmContext ,    yarnScheduler ,    applicationId 1  ,    config ,     \" testqueue \"  )  )  ;", "apps . put ( applicationId 2  ,    getRMApp ( rmContext ,    yarnScheduler ,    applicationId 2  ,    config ,     \" a \"  )  )  ;", "apps . put ( applicationId 3  ,    getRMApp ( rmContext ,    yarnScheduler ,    applicationId 3  ,    config ,     \" testqueue \"  )  )  ;", "return   apps ;", "}", "METHOD_END"], "methodName": ["getRMApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "List < ApplicationAttemptId >    schedApps    =    new   ArrayList < ApplicationAttemptId >  (  )  ;", "schedApps . add ( ApplicationAttemptId . newInstance (  . getApplicationId (  1  )  ,     0  )  )  ;", "schedApps . add ( ApplicationAttemptId . newInstance (  . getApplicationId (  3  )  ,     0  )  )  ;", "return   schedApps ;", "}", "METHOD_END"], "methodName": ["getSchedulerApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "Dispatcher   dispatcher    =    mock ( Dispatcher . class )  ;", "when ( rmContext . getDispatcher (  )  )  . thenReturn ( dispatcher )  ;", "EventHandler   eventHandler    =    mock ( EventHandler . class )  ;", "when ( dispatcher . getEventHandler (  )  )  . thenReturn ( eventHandler )  ;", "QueueInfo   queInfo    =    recordFactory . newRecordInstance ( QueueInfo . class )  ;", "queInfo . setQueueName (  \" testqueue \"  )  ;", "when ( yarnScheduler . getQueueInfo ( eq (  \" testqueue \"  )  ,    anyBoolean (  )  ,    anyBoolean (  )  )  )  . thenReturn ( queInfo )  ;", "when ( yarnScheduler . getQueueInfo ( eq (  \" nonexistentqueue \"  )  ,    anyBoolean (  )  ,    anyBoolean (  )  )  )  . thenThrow ( new   IOException (  \" queue   does   not   exist \"  )  )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "when ( rmContext . getRMApplicationHistoryWriter (  )  )  . thenReturn ( writer )  ;", "ConcurrentHashMap < ApplicationId ,    RMApp >    apps    =    getRMApps ( rmContext ,    yarnScheduler )  ;", "when ( rmContext . getRMApps (  )  )  . thenReturn ( apps )  ;", "when ( yarnScheduler . getAppsInQueue ( eq (  \" testqueue \"  )  )  )  . thenReturn ( getSchedulerApps ( apps )  )  ;", "RScheduler   rs    =    mock ( RScheduler . class )  ;", "when ( rmContext . getScheduler (  )  )  . thenReturn ( rs )  ;", "}", "METHOD_END"], "methodName": ["mockRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   mockSubmitAppRequest ( appId ,    name ,    queue ,    null )  ;", "}", "METHOD_END"], "methodName": ["mockSubmitAppRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "return   mockSubmitAppRequest ( appId ,    name ,    queue ,    tags ,    false )  ;", "}", "METHOD_END"], "methodName": ["mockSubmitAppRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunchContext   amContainerSpec    =    mock ( ContainerLaunchContext . class )  ;", "Resource       =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "ApplicationSubmissionContext   submissionContext    =    recordFactory . newRecordInstance ( ApplicationSubmissionContext . class )  ;", "submissionContext . setAMContainerSpec ( amContainerSpec )  ;", "submissionContext . setApplicationName ( name )  ;", "submissionContext . setQueue ( queue )  ;", "submissionContext . setApplicationId ( appId )  ;", "submissionContext . setResource (  )  ;", "submissionContext . setApplicationType ( appType )  ;", "submissionContext . setApplicationTags ( tags )  ;", "submissionContext . setUnmanagedAM ( unmanaged )  ;", "SubmitApplicationRequest   submitRequest    =    recordFactory . newRecordInstance ( SubmitApplicationRequest . class )  ;", "submitRequest . setApplicationSubmissionContext ( submissionContext )  ;", "return   submitRequest ;", "}", "METHOD_END"], "methodName": ["mockSubmitAppRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnScheduler   yarnScheduler    =    mock ( YarnScheduler . class )  ;", "when ( yarnScheduler . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )  ;", "when ( yarnScheduler . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )  ;", "when ( yarnScheduler . getAppsInQueue (  . QUEUE _  1  )  )  . thenReturn ( Arrays . asList (  . getApplicationAttemptId (  1  0  1  )  ,     . getApplicationAttemptId (  1  0  2  )  )  )  ;", "when ( yarnScheduler . getAppsInQueue (  . QUEUE _  2  )  )  . thenReturn ( Arrays . asList (  . getApplicationAttemptId (  1  0  3  )  )  )  ;", "ApplicationAttemptId   attemptId    =     . getApplicationAttemptId (  1  )  ;", "when ( yarnScheduler . getAppResourceUsageReport ( attemptId )  )  . thenReturn ( null )  ;", "return   yarnScheduler ;", "}", "METHOD_END"], "methodName": ["mockYarnScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( new   NullRMStateStore (  )  )  ;", ". dtsm    =    new   RMDelegationTokenSecretManager (  6  0  0  0  0  ,     6  0  0  0  0  ,     6  0  0  0  0  ,     6  0  0  0  0  ,    rmContext )  ;", ". dtsm . startThreads (  )  ;", "}", "METHOD_END"], "methodName": ["setupSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestClientRMService . dtsm )     !  =    null )     {", "TestClientRMService . dtsm . stopThreads (  )  ;", "}", "}", "METHOD_END"], "methodName": ["teardownSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnScheduler   yarnScheduler    =    TestClientRMService . mockYarnScheduler (  )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "mockRMContext ( yarnScheduler ,    rmContext )  ;", "RMStateStore   stateStore    =    mock ( RMStateStore . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( stateStore )  ;", "RMAppManager   appManager    =    new   RMAppManager ( rmContext ,    yarnScheduler ,    null ,    mock ( ApplicationACLsManager . class )  ,    new   Configuration (  )  )  ;", "when ( rmContext . getDispatcher (  )  . getEventHandler (  )  )  . thenReturn ( new   event . EventHandler < Event >  (  )     {", "public   void   handle ( Event   event )     {", "}", "}  )  ;", "ApplicationId   appId 1     =    TestClientRMService . getApplicationId (  1  0  0  )  ;", "ApplicationACLsManager   mockAclsManager    =    mock ( ApplicationACLsManager . class )  ;", "when ( mockAclsManager . checkAccess ( UserGroupInformation . getCurrentUser (  )  ,    VIEW _ APP ,    null ,    appId 1  )  )  . thenReturn ( true )  ;", "QueueACLsManager   mockQueueACLsManager    =    mock ( QueueACLsManager . class )  ;", "when ( mockQueueACLsManager . checkAccess ( any ( UserGroupInformation . class )  ,    any ( QueueACL . class )  ,    anyString (  )  )  )  . thenReturn ( true )  ;", "ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    yarnScheduler ,    appManager ,    mockAclsManager ,    mockQueueACLsManager ,    null )  ;", "SubmitApplicationRequest   submitRequest 1     =    mockSubmitAppRequest ( appId 1  ,    null ,    null )  ;", "try    {", "rmService . submitApplication ( submitRequest 1  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" Exception   is   not   expected .  \"  )  ;", "}", "RMApp   app 1     =    rmContext . getRMApps (  )  . get ( appId 1  )  ;", "Assert . assertNotNull (  \" app   doesn ' t   exist \"  ,    app 1  )  ;", "Assert . assertEquals (  \" app   name   doesn ' t   match \"  ,    DEFAULT _ APPLICATION _ NAME ,    app 1  . getName (  )  )  ;", "Assert . assertEquals (  \" app   queue   doesn ' t   match \"  ,    DEFAULT _ QUEUE _ NAME ,    app 1  . getQueue (  )  )  ;", "String   name    =    MockApps . newAppName (  )  ;", "String   queue    =    MockApps . newQueue (  )  ;", "ApplicationId   appId 2     =    TestClientRMService . getApplicationId (  1  0  1  )  ;", "SubmitApplicationRequest   submitRequest 2     =    mockSubmitAppRequest ( appId 2  ,    name ,    queue )  ;", "submitRequest 2  . getApplicationSubmissionContext (  )  . setApplicationType (  \" matchType \"  )  ;", "try    {", "rmService . submitApplication ( submitRequest 2  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" Exception   is   not   expected .  \"  )  ;", "}", "RMApp   app 2     =    rmContext . getRMApps (  )  . get ( appId 2  )  ;", "Assert . assertNotNull (  \" app   doesn ' t   exist \"  ,    app 2  )  ;", "Assert . assertEquals (  \" app   name   doesn ' t   match \"  ,    name ,    app 2  . getName (  )  )  ;", "Assert . assertEquals (  \" app   queue   doesn ' t   match \"  ,    queue ,    app 2  . getQueue (  )  )  ;", "try    {", "rmService . submitApplication ( submitRequest 2  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" Exception   is   not   expected .  \"  )  ;", "}", "GetApplicationsRequest   getAllAppsRequest    =    GetApplicationsRequest . newInstance ( new   HashSet < String >  (  )  )  ;", "GetApplicationsResponse   getAllApplicationsResponse    =    rmService . getApplications ( getAllAppsRequest )  ;", "Assert . assertEquals (  5  ,    getAllApplicationsResponse . getApplicationList (  )  . size (  )  )  ;", "Set < String >    appTypes    =    new   HashSet < String >  (  )  ;", "appTypes . add (  \" matchType \"  )  ;", "getAllAppsRequest    =    GetApplicationsRequest . newInstance ( appTypes )  ;", "getAllApplicationsResponse    =    rmService . getApplications ( getAllAppsRequest )  ;", "Assert . assertEquals (  1  ,    getAllApplicationsResponse . getApplicationList (  )  . size (  )  )  ;", "Assert . assertEquals ( appId 2  ,    getAllApplicationsResponse . getApplicationList (  )  . get (  0  )  . getApplicationId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppSubmit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnScheduler   yarnScheduler    =    TestClientRMService . mockYarnScheduler (  )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "mockRMContext ( yarnScheduler ,    rmContext )  ;", "RMStateStore   stateStore    =    mock ( RMStateStore . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( stateStore )  ;", "RMAppManager   appManager    =    new   RMAppManager ( rmContext ,    yarnScheduler ,    null ,    mock ( ApplicationACLsManager . class )  ,    new   Configuration (  )  )  ;", "final   ApplicationId   appId 1     =    TestClientRMService . getApplicationId (  1  0  0  )  ;", "final   ApplicationId   appId 2     =    TestClientRMService . getApplicationId (  1  0  1  )  ;", "final   SubmitApplicationRequest   submitRequest 1     =    mockSubmitAppRequest ( appId 1  ,    null ,    null )  ;", "final   SubmitApplicationRequest   submitRequest 2     =    mockSubmitAppRequest ( appId 2  ,    null ,    null )  ;", "final   CyclicBarrier   startBarrier    =    new   CyclicBarrier (  2  )  ;", "final   CyclicBarrier   endBarrier    =    new   CyclicBarrier (  2  )  ;", "@ SuppressWarnings (  \" rawtypes \"  )", "EventHandler   eventHandler    =    new   EventHandler (  )     {", "@ Override", "public   void   handle ( Event   rawEvent )     {", "if    ( rawEvent   instanceof   RMAppEvent )     {", "RMAppEvent   event    =     (  ( RMAppEvent )     ( rawEvent )  )  ;", "if    ( event . getApplicationId (  )  . equals ( appId 1  )  )     {", "try    {", "startBarrier . await (  )  ;", "endBarrier . await (  )  ;", "}    catch    ( BrokenBarrierException   e )     {", "TestClientRMService . LOG . warn (  \" Broken   Barrier \"  ,    e )  ;", "}    catch    ( InterruptedException   e )     {", "TestClientRMService . LOG . warn (  \" Interrupted   while   awaiting   barriers \"  ,    e )  ;", "}", "}", "}", "}", "}  ;", "when ( rmContext . getDispatcher (  )  . getEventHandler (  )  )  . thenReturn ( eventHandler )  ;", "final   ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    yarnScheduler ,    appManager ,    null ,    null ,    null )  ;", "Thread   t    =    new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "rmService . submitApplication ( submitRequest 1  )  ;", "}    catch    ( YarnException   e )     {", "}", "}", "}  ;", "t . start (  )  ;", "startBarrier . await (  )  ;", "rmService . submitApplication ( submitRequest 2  )  ;", "endBarrier . await (  )  ;", "t . join (  )  ;", "}", "METHOD_END"], "methodName": ["testConcurrentAppSubmit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "MockRM   rm    =    new   MockRM (  )  ;", "rm . init ( conf )  ;", "rm . start (  )  ;", "ClientRMService   rmService    =    rm . ge (  )  ;", "GetApplicationsRequest   getRequest    =    GetApplicationsRequest . newInstance ( EnumSet . of ( KILLED )  )  ;", "RMApp   app 1     =    rm . submitApp (  1  0  2  4  )  ;", "RMApp   app 2     =    rm . submitApp (  1  0  2  4  ,    true )  ;", "assertEquals (  \" Incorrect   number   of   apps   in   the   RM \"  ,     0  ,    rmService . getApplications ( getRequest )  . getApplicationList (  )  . size (  )  )  ;", "KillApplicationRequest   killRequest 1     =    KillApplicationRequest . newInstance ( app 1  . getApplicationId (  )  )  ;", "KillApplicationRequest   killRequest 2     =    KillApplicationRequest . newInstance ( app 2  . getApplicationId (  )  )  ;", "int   killAttemptCount    =     0  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "KillApplicationResponse   killResponse 1     =    rmService . forceKillApplication ( killRequest 1  )  ;", "killAttemptCount +  +  ;", "if    ( killResponse 1  . getIsKillCompleted (  )  )     {", "break ;", "}", "Thread . sleep (  1  0  )  ;", "}", "assertTrue (  \" Kill   attempt   count   should   be   greater   than    1    for   managed   AMs \"  ,     ( killAttemptCount    >     1  )  )  ;", "assertEquals (  \" Incorrect   number   of   apps   in   the   RM \"  ,     1  ,    rmService . getApplications ( getRequest )  . getApplicationList (  )  . size (  )  )  ;", "KillApplicationResponse   killResponse 2     =    rmService . forceKillApplication ( killRequest 2  )  ;", "assertTrue (  \" Killing   UnmanagedAM   should   falsely   acknowledge   true \"  ,    killResponse 2  . getIsKillCompleted (  )  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  0  ;    i +  +  )     {", "if    (  2     =  =     ( rmService . getApplications ( getRequest )  . getApplicationList (  )  . size (  )  )  )     {", "break ;", "}", "Thread . sleep (  1  0  )  ;", "}", "assertEquals (  \" Incorrect   number   of   apps   in   the   RM \"  ,     2  ,    rmService . getApplications ( getRequest )  . getApplicationList (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testForceKillApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getRMApps (  )  )  . thenReturn ( new   ConcurrentHashMap < ApplicationId ,    rmapp . RMApp >  (  )  )  ;", "ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,    null )  ;", "ApplicationId   applicationId    =    BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,     0  )  ;", "KillApplicationRequest   request    =    KillApplicationRequest . newInstance ( applicationId )  ;", "try    {", "rmService . forceKillApplication ( request )  ;", "Assert . fail (  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "Assert . assertEquals ( ex . getMessage (  )  ,     (  (  \" Trying   to   kill   an   absent    \"     +     \" application    \"  )     +     ( request . getApplicationId (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testForceKillNonExistingApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ClientRMService   rmService    =    createRMService (  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "GetApplicationAttemptReportRequest   request    =    recordFactory . newRecordInstance ( GetApplicationAttemptReportRequest . class )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  5  6  ,     1  )  ,     1  )  ;", "request . setApplicationAttemptId ( attemptId )  ;", "try    {", "GetApplicationAttemptReportResponse   response    =    rmService . getApplicationAttemptReport ( request )  ;", "Assert . assertEquals ( attemptId ,    response . getApplicationAttemptReport (  )  . getApplicationAttemptId (  )  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "Assert . fail ( ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetApplicationAttemptReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ClientRMService   rmService    =    createRMService (  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "GetApplicationAttemptsRequest   request    =    recordFactory . newRecordInstance ( GetApplicationAttemptsRequest . class )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  5  6  ,     1  )  ,     1  )  ;", "request . setApplicationId ( ApplicationId . newInstance (  1  2  3  4  5  6  ,     1  )  )  ;", "try    {", "GetApplicationAttemptsResponse   response    =    rmService . getApplicationAttempts ( request )  ;", "Assert . assertEquals (  1  ,    response . getApplicationAttemptList (  )  . size (  )  )  ;", "Assert . assertEquals ( attemptId ,    response . getApplicationAttemptList (  )  . get (  0  )  . getApplicationAttemptId (  )  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "Assert . fail ( ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetApplicationAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getRMApps (  )  )  . thenReturn ( new   ConcurrentHashMap < ApplicationId ,    rmapp . RMApp >  (  )  )  ;", "ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,    null )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "GetApplicationReportRequest   request    =    recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "request . setApplicationId ( ApplicationId . newInstance (  0  ,     0  )  )  ;", "try    {", "rmService . getApplicationReport ( request )  ;", "Assert . fail (  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "Assert . assertEquals ( ex . getMessage (  )  ,     (  (  \" Application   with   id    '  \"     +     ( request . getApplicationId (  )  )  )     +     \"  '    doesn ' t   exist   in   RM .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   attemptId    =    TestClientRMService . getApplicationAttemptId (  1  )  ;", "YarnScheduler   yarnScheduler    =    TestClientRMService . mockYarnScheduler (  )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "mockRMContext ( yarnScheduler ,    rmContext )  ;", "when ( rmContext . getDispatcher (  )  . getEventHandler (  )  )  . thenReturn ( new   event . EventHandler < Event >  (  )     {", "public   void   handle ( Event   event )     {", "}", "}  )  ;", "ApplicationSubmissionContext   asContext    =    mock ( ApplicationSubmissionContext . class )  ;", "YarnConfiguration   config    =    new   YarnConfiguration (  )  ;", "RMAppAttemptImpl   rmAppAttemptImpl    =    new   RMAppAttemptImpl ( attemptId ,    rmContext ,    yarnScheduler ,    null ,    asContext ,    config ,    false )  ;", "ApplicationResourceUsageReport   report    =    rmAppAttemptImpl . getApplicationResourceUsageReport (  )  ;", "assertEquals ( report ,    RMServerUtils . DUMMY _ APPLICATION _ RESOURCE _ USAGE _ REPORT )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationResourceUsageReportDummy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnScheduler   yarnScheduler    =    TestClientRMService . mockYarnScheduler (  )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "mockRMContext ( yarnScheduler ,    rmContext )  ;", "RMStateStore   stateStore    =    mock ( RMStateStore . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( stateStore )  ;", "RMAppManager   appManager    =    new   RMAppManager ( rmContext ,    yarnScheduler ,    null ,    mock ( ApplicationACLsManager . class )  ,    new   Configuration (  )  )  ;", "when ( rmContext . getDispatcher (  )  . getEventHandler (  )  )  . thenReturn ( new   event . EventHandler < Event >  (  )     {", "public   void   handle ( Event   event )     {", "}", "}  )  ;", "ApplicationACLsManager   mockAclsManager    =    mock ( ApplicationACLsManager . class )  ;", "QueueACLsManager   mockQueueACLsManager    =    mock ( QueueACLsManager . class )  ;", "when ( mockQueueACLsManager . checkAccess ( any ( UserGroupInformation . class )  ,    any ( QueueACL . class )  ,    anyString (  )  )  )  . thenReturn ( true )  ;", "ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    yarnScheduler ,    appManager ,    mockAclsManager ,    mockQueueACLsManager ,    null )  ;", "String [  ]    queues    =    new   String [  ]  {    TestClientRMService . QUEUE _  1  ,    TestClientRMService . QUEUE _  2     }  ;", "String [  ]    appNames    =    new   String [  ]  {    MockApps . newAppName (  )  ,    MockApps . newAppName (  )  ,    MockApps . newAppName (  )     }  ;", "ApplicationId [  ]    appIds    =    new   ApplicationId [  ]  {    TestClientRMService . getApplicationId (  1  0  1  )  ,    TestClientRMService . getApplicationId (  1  0  2  )  ,    TestClientRMService . getApplicationId (  1  0  3  )     }  ;", "List < String >    tags    =    Arrays . asList (  \" Tag 1  \"  ,     \" Tag 2  \"  ,     \" Tag 3  \"  )  ;", "long [  ]    submitTimeMillis    =    new   long [  3  ]  ;", "for    ( int   i    =     0  ;    i    <     ( appIds . length )  ;    i +  +  )     {", "ApplicationId   appId    =    appIds [ i ]  ;", "when ( mockAclsManager . checkAccess ( UserGroupInformation . getCurrentUser (  )  ,    VIEW _ APP ,    null ,    appId )  )  . thenReturn ( true )  ;", "SubmitApplicationRequest   submitRequest    =    mockSubmitAppRequest ( appId ,    appNames [ i ]  ,    queues [  ( i    %     ( queues . length )  )  ]  ,    new   HashSet < String >  ( tags . subList (  0  ,     ( i    +     1  )  )  )  )  ;", "rmService . submitApplication ( submitRequest )  ;", "submitTimeMillis [ i ]     =    System . currentTimeMillis (  )  ;", "}", "GetApplicationsRequest   request    =    GetApplicationsRequest . newInstance (  )  ;", "assertEquals (  \" Incorrect   total   number   of   apps \"  ,     6  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request . setLimit (  1 L )  ;", "assertEquals (  \" Failed   to   limit   applications \"  ,     1  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request    =    GetApplicationsRequest . newInstance (  )  ;", "request . setStartRange ( submitTimeMillis [  0  ]  ,    System . currentTimeMillis (  )  )  ;", "assertEquals (  \" Incorrect   number   of   matching   start   range \"  ,     2  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request . setStartRange ( submitTimeMillis [  1  ]  ,    System . currentTimeMillis (  )  )  ;", "assertEquals (  \" Incorrect   number   of   matching   start   range \"  ,     1  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request . setStartRange ( submitTimeMillis [  2  ]  ,    System . currentTimeMillis (  )  )  ;", "assertEquals (  \" Incorrect   number   of   matching   start   range \"  ,     0  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request    =    GetApplicationsRequest . newInstance (  )  ;", "Set < String >    queueSet    =    new   HashSet < String >  (  )  ;", "request . setQueues ( queueSet )  ;", "queueSet . add ( queues [  0  ]  )  ;", "assertEquals (  \" Incorrect   number   of   applications   in   queue \"  ,     2  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "assertEquals (  \" Incorrect   number   of   applications   in   queue \"  ,     2  ,    rmService . getApplications ( request ,    false )  . getApplicationList (  )  . size (  )  )  ;", "queueSet . add ( queues [  1  ]  )  ;", "assertEquals (  \" Incorrect   number   of   applications   in   queue \"  ,     3  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request    =    GetApplicationsRequest . newInstance (  )  ;", "Set < String >    userSet    =    new   HashSet < String >  (  )  ;", "request . setUsers ( userSet )  ;", "userSet . add (  \" random - user - name \"  )  ;", "assertEquals (  \" Incorrect   number   of   applications   for   user \"  ,     0  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "userSet . add ( UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  ;", "assertEquals (  \" Incorrect   number   of   applications   for   user \"  ,     3  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request    =    GetApplicationsRequest . newInstance ( ALL ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )  ;", "Set < String >    tagSet    =    new   HashSet < String >  (  )  ;", "request . setApplicationTags ( tagSet )  ;", "assertEquals (  \" Incorrect   number   of   matching   tags \"  ,     6  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "tagSet    =    Sets . newHashSet ( tags . get (  0  )  )  ;", "request . setApplicationTags ( tagSet )  ;", "assertEquals (  \" Incorrect   number   of   matching   tags \"  ,     3  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "tagSet    =    Sets . newHashSet ( tags . get (  1  )  )  ;", "request . setApplicationTags ( tagSet )  ;", "assertEquals (  \" Incorrect   number   of   matching   tags \"  ,     2  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "tagSet    =    Sets . newHashSet ( tags . get (  2  )  )  ;", "request . setApplicationTags ( tagSet )  ;", "assertEquals (  \" Incorrect   number   of   matching   tags \"  ,     1  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request    =    GetApplicationsRequest . newInstance ( VIEWABLE )  ;", "assertEquals (  \" Incorrect   number   of   applications   for   the   scope \"  ,     6  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "request    =    GetApplicationsRequest . newInstance ( OWN )  ;", "assertEquals (  \" Incorrect   number   of   applications   for   the   scope \"  ,     3  ,    rmService . getApplications ( request )  . getApplicationList (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM (  )     {", "protected   ClientRMService   createClientRMService (  )     {", "return   new   ClientRMService ( this . rmContext ,    scheduler ,    this . rmAppManager ,    this . applicationACLsManager ,    this . queueACLsManager ,    this . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  )  ;", "}", "}  ;", "rm . start (  )  ;", "MockNM   node    =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     1  0  2  4  )  ;", "rm . sendNodeStarted ( node )  ;", "node . nodeHeartbeat ( true )  ;", "MockNM   lostNode    =    rm . registerNode (  \" host 2  :  1  2  3  5  \"  ,     1  0  2  4  )  ;", "rm . sendNodeStarted ( lostNode )  ;", "lostNode . nodeHeartbeat ( true )  ;", "rm . NMwaitForState ( lostNode . getNodeId (  )  ,    RUNNING )  ;", "rm . sendNodeLost ( lostNode )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "InetSocketAddress   rmAddress    =    rm . getClientRMService (  )  . getBindAddress (  )  ;", ". LOG . info (  (  \" Connecting   to   ResourceManager   at    \"     +    rmAddress )  )  ;", "ApplicationClientProtocol   client    =     (  ( ApplicationClientProtocol )     ( rpc . getProxy ( ApplicationClientProtocol . class ,    rmAddress ,    conf )  )  )  ;", "GetClusterNodesRequest   request    =    GetClusterNodesRequest . newInstance ( EnumSet . of ( RUNNING )  )  ;", "List < NodeReport >    nodeReports    =    client . getClusterNodes ( request )  . getNodeReports (  )  ;", "Assert . assertEquals (  1  ,    nodeReports . size (  )  )  ;", "Assert . assertNotSame (  \" Node   is   expected   to   be   healthy !  \"  ,    UNHEALTHY ,    nodeReports . get (  0  )  . getNodeState (  )  )  ;", "node . nodeHeartbeat ( false )  ;", "nodeReports    =    client . getClusterNodes ( request )  . getNodeReports (  )  ;", "Assert . assertEquals (  \" Unhealthy   nodes   should   not   show   up   by   default \"  ,     0  ,    nodeReports . size (  )  )  ;", "request    =    GetClusterNodesRequest . newInstance ( EnumSet . of ( UNHEALTHY )  )  ;", "nodeReports    =    client . getClusterNodes ( request )  . getNodeReports (  )  ;", "Assert . assertEquals (  1  ,    nodeReports . size (  )  )  ;", "Assert . assertEquals (  \" Node   is   expected   to   be   unhealthy !  \"  ,    UNHEALTHY ,    nodeReports . get (  0  )  . getNodeState (  )  )  ;", "rm . registerNode (  \" host 3  :  1  2  3  6  \"  ,     1  0  2  4  )  ;", "request    =    GetClusterNodesRequest . newInstance ( EnumSet . allOf ( NodeState . class )  )  ;", "nodeReports    =    client . getClusterNodes ( request )  . getNodeReports (  )  ;", "Assert . assertEquals (  3  ,    nodeReports . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetClusterNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ClientRMService   rmService    =    createRMService (  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "GetContainerReportRequest   request    =    recordFactory . newRecordInstance ( GetContainerReportRequest . class )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  5  6  ,     1  )  ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( attemptId ,     1  )  ;", "request . setContainerId ( containerId )  ;", "try    {", "GetContainerReportResponse   response    =    rmService . getContainerReport ( request )  ;", "Assert . assertEquals ( containerId ,    response . getContainerReport (  )  . getContainerId (  )  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "Assert . fail ( ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetContainerReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "ClientRMService   rmService    =    createRMService (  )  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "GetContainersRequest   request    =    recordFactory . newRecordInstance ( GetContainersRequest . class )  ;", "ApplicationAttemptId   attemptId    =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  5  6  ,     1  )  ,     1  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( attemptId ,     1  )  ;", "request . setApplicationAttemptId ( attemptId )  ;", "try    {", "GetContainersResponse   response    =    rmService . getContainers ( request )  ;", "Assert . assertEquals ( containerId ,    response . getContainerList (  )  . get (  0  )  . getContainerId (  )  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "Assert . fail ( ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "YarnScheduler   yarnScheduler    =    mock ( YarnScheduler . class )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "mockRMContext ( yarnScheduler ,    rmContext )  ;", "rmService    =    new    ( rmContext ,    yarnScheduler ,    null ,    null ,    null ,    null )  ;", "GetQueueInfoRequest   request    =    recordFactory . newRecordInstance ( GetQueueInfoRequest . class )  ;", "request . setQueueName (  \" testqueue \"  )  ;", "request . setIncludeApplications ( true )  ;", "GetQueueInfoResponse   queueInfo    =    rmService . getQueueInfo ( request )  ;", "List < ApplicationReport >    applications    =    queueInfo . getQueueInfo (  )  . getApplications (  )  ;", "Assert . assertEquals (  2  ,    applications . size (  )  )  ;", "request . setQueueName (  \" nonexistentqueue \"  )  ;", "request . setIncludeApplications ( true )  ;", "queueInfo    =    rmService . getQueueInfo ( request )  ;", "}", "METHOD_END"], "methodName": ["testGetQueueInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getRMApps (  )  )  . thenReturn ( new   ConcurrentHashMap < ApplicationId ,    rmapp . RMApp >  (  )  )  ;", "ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,    null )  ;", "ApplicationId   applicationId    =    BuilderUtils . newApplicationId ( System . currentTimeMillis (  )  ,     0  )  ;", "MoveApplicationAcrossQueuesRequest   request    =    MoveApplicationAcrossQueuesRequest . newInstance ( applicationId ,     \" newqueue \"  )  ;", "rmService . moveApplicationAcrossQueues ( request )  ;", "}", "METHOD_END"], "methodName": ["testMoveAbsentApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "final   ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,     . dtsm )  ;", ". testerKerb . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "checkTokenCancellation ( rmService ,     . testerKerb ,     . other )  ;", "return   null ;", "}", "}  )  ;", ". owner . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "checkTokenCancellation (  . owner ,     . other )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["testTokenCancellationByOwner"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "final   ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,     . dtsm )  ;", ". testerKerb . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "checkTokenCancellation ( rmService ,     . owner ,     . testerKerb )  ;", "return   null ;", "}", "}  )  ;", ". other . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "checkTokenCancellation (  . owner ,     . other )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["testTokenCancellationByRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "final   ClientRMService   rmService    =    new   ClientRMService ( rmContext ,    null ,    null ,    null ,    null ,     . dtsm )  ;", "UserGroupInformation [  ]    kerbTestOwners    =    new   UserGroupInformation [  ]  {     . owner ,     . other ,     . tester ,     . ownerKerb ,     . otherKerb    }  ;", "UserGroupInformation [  ]    kerbTestRenewers    =    new   UserGroupInformation [  ]  {     . owner ,     . other ,     . ownerKerb ,     . otherKerb    }  ;", "for    ( final   UserGroupInformation   tokOwner    :    kerbTestOwners )     {", "for    ( final   UserGroupInformation   tokRenewer    :    kerbTestRenewers )     {", "try    {", ". testerKerb . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "try    {", "checkTokenCancellation ( rmService ,    tokOwner ,    tokRenewer )  ;", "Assert . fail (  (  (  (  \" We   should   not   reach   here ;    token   owner    =     \"     +     ( tokOwner . getUserName (  )  )  )     +     \"  ,    renewer    =     \"  )     +     ( tokRenewer . getUserName (  )  )  )  )  ;", "return   null ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  (  (  . testerKerb . getUserName (  )  )     +     \"    is   not   authorized   to   cancel   the   token \"  )  )  )  ;", "return   null ;", "}", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  (  \" Unexpected   exception ;     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "}", "UserGroupInformation [  ]    simpleTestOwners    =    new   UserGroupInformation [  ]  {     . owner ,     . other ,     . ownerKerb ,     . otherKerb ,     . testerKerb    }  ;", "UserGroupInformation [  ]    simpleTestRenewers    =    new   UserGroupInformation [  ]  {     . owner ,     . other ,     . ownerKerb ,     . otherKerb    }  ;", "for    ( final   UserGroupInformation   tokOwner    :    simpleTestOwners )     {", "for    ( final   UserGroupInformation   tokRenewer    :    simpleTestRenewers )     {", "try    {", ". tester . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "try    {", "checkTokenCancellation ( tokOwner ,    tokRenewer )  ;", "Assert . fail (  (  (  (  \" We   should   not   reach   here ;    token   owner    =     \"     +     ( tokOwner . getUserName (  )  )  )     +     \"  ,    renewer    =     \"  )     +     ( tokRenewer . getUserName (  )  )  )  )  ;", "return   null ;", "}    catch    ( YarnException   ex )     {", "Assert . assertTrue ( ex . getMessage (  )  . contains (  (  (  . tester . getUserName (  )  )     +     \"    is   not   authorized   to   cancel   the   token \"  )  )  )  ;", "return   null ;", "}", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  (  \" Unexpected   exception ;     \"     +     ( e . getMessage (  )  )  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["testTokenCancellationByWrongUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation . getLoginUser (  )  . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "checkTokenRenewal (  . owner ,     . owner )  ;", "checkTokenRenewal (  . owner ,     . other )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["testTokenRenewalByLoginUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "TestClientRMService . owner . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "checkTokenRenewal ( TestClientRMService . owner ,    TestClientRMService . owner )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["testTokenRenewalByOwner"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "try    {", ". owner . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "try    {", "checkTokenRenewal (  . owner ,     . other )  ;", "return   null ;", "}    catch    ( YarnException   ex )     {", "Assert . assertTrue ( ex . getMessage (  )  . contains (  (  (  (  . owner . getUserName (  )  )     +     \"    tries   to   renew   a   token   with   renewer    \"  )     +     (  . other . getUserName (  )  )  )  )  )  ;", "throw   ex ;", "}", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "return ;", "}", "Assert . fail (  \" renew   should   have   failed \"  )  ;", "}", "METHOD_END"], "methodName": ["testTokenRenewalWrongUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMService"}, {"methodBody": ["METHOD_START", "{", "loggedInUser . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   IOException ,    YarnException    {", "CancelDelegationTokenRequest   request    =    Records . newRecord ( CancelDelegationTokenRequest . class )  ;", "request . setDelegationToken ( dToken )  ;", "cService . cancelDelegationToken ( request )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["cancelDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( IPC _ RPC _ IMPL ,     . YarnBadRPC . class ,    YarnRPC . class )  ;", "RMDelegationTokenSecretManager   secretManager    =    mock ( RMDelegationTokenSecretManager . class )  ;", "Renewer . setSecretManager ( secretManager ,    rmAddr )  ;", "RMDelegationTokenIdentifier   ident    =    new   RMDelegationTokenIdentifier ( new   Text (  \" owner \"  )  ,    new   Text (  \" renewer \"  )  ,    null )  ;", "Token < RMDelegationTokenIdentifier >    token    =    new   Token < RMDelegationTokenIdentifier >  ( ident ,    secretManager )  ;", "SecurityUtil . setTokenService ( token ,    serviceAddr )  ;", "if    ( shouldShortCircuit )     {", "token . renew ( conf )  ;", "verify ( secretManager )  . renewToken ( eq ( token )  ,    eq (  \" renewer \"  )  )  ;", "reset ( secretManager )  ;", "token . cancel ( conf )  ;", "verify ( secretManager )  . cancelToken ( eq ( token )  ,    eq (  \" renewer \"  )  )  ;", "} else    {", "try    {", "token . renew ( conf )  ;", "fail (  )  ;", "}    catch    ( RuntimeException   e )     {", "assertEquals (  \" getProxy \"  ,    e . getMessage (  )  )  ;", "}", "verify ( secretManager ,    never (  )  )  . renewToken ( any ( Token . class )  ,    anyString (  )  )  ;", "try    {", "token . cancel ( conf )  ;", "fail (  )  ;", "}    catch    ( RuntimeException   e )     {", "assertEquals (  \" getProxy \"  ,    e . getMessage (  )  )  ;", "}", "verify ( secretManager ,    never (  )  )  . cancelToken ( any ( Token . class )  ,    anyString (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkShortCircuitRenewCancel"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   mockSched    =    mock ( ResourceScheduler . class )  ;", "doReturn ( BuilderUtils . newResource (  5  1  2  ,     0  )  )  . when ( mockSched )  . getMinimumResourceCapability (  )  ;", "doReturn ( BuilderUtils . newResource (  5  1  2  0  ,     0  )  )  . when ( mockSched )  . getMaximumResourceCapability (  )  ;", "return   mockSched ;", "}", "METHOD_END"], "methodName": ["createMockScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( new   NullRMStateStore (  )  )  ;", "RMDelegationTokenSecretM   rmDtSecretM    =    new   RMDelegationTokenSecretM ( secretKeyInterval ,    tokenMaxLifetime ,    tokenRenewInterval ,     3  6  0  0  0  0  0  ,    rmContext )  ;", "return   rmDtSecretM ;", "}", "METHOD_END"], "methodName": ["createRMDelegationTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( user )  ;", "ugi . addToken ( ConverterUtils . convertFromYarn ( token ,    rmAddress )  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "ApplicationClientProtocol   cWithDT    =    ugi . doAs ( new   PrivilegedAction < ApplicationClientProtocol >  (  )     {", "@ Override", "public   ApplicationClientProtocol   run (  )     {", "return    (  ( ApplicationClientProtocol )     ( rpc . getProxy ( ApplicationClientProtocol . class ,    rmAddress ,    conf )  )  )  ;", "}", "}  )  ;", "return   cWithDT ;", "}", "METHOD_END"], "methodName": ["getClientRMProtocolWithDT"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "Token   token    =    loggedInUser . doAs ( new   PrivilegedExceptionAction < Token >  (  )     {", "@ Override", "public   security . token . Token   run (  )    throws   IOException ,    YarnException    {", "GetDelegationTokenRequest   request    =    Records . newRecord ( GetDelegationTokenRequest . class )  ;", "request . setRenewer ( renewerString )  ;", "return   clientRMService . getDelegationToken ( request )  . getRMDelegationToken (  )  ;", "}", "}  )  ;", "return   token ;", "}", "METHOD_END"], "methodName": ["getDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "long   nextExpTime    =    loggedInUser . doAs ( new   PrivilegedExceptionAction < Long >  (  )     {", "@ Override", "public   Long   run (  )    throws   IOException ,    YarnException    {", "RenewDelegationTokenRequest   request    =    Records . newRecord ( RenewDelegationTokenRequest . class )  ;", "request . setDelegationToken ( dToken )  ;", "return   cService . renewDelegationToken ( request )  . getNextExpirationTime (  )  ;", "}", "}  )  ;", "return   nextExpTime ;", "}", "METHOD_END"], "methodName": ["renewDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "Renewer . setSecretManager ( null ,    null )  ;", "}", "METHOD_END"], "methodName": ["resetSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "final   YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ PRINCIPAL ,     \" testuser / localhost @ apache . org \"  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "ResourceScheduler   scheduler    =     . createMockScheduler ( conf )  ;", "long   initialInterval    =     1  0  0  0  0 L ;", "long   maxLifetime    =     2  0  0  0  0 L ;", "long   renewInterval    =     1  0  0  0  0 L ;", "RMDelegationTokenSecretManager   rmDtSecretManager    =     . createRMDelegationTokenSecretManager ( initialInterval ,    maxLifetime ,    renewInterval )  ;", "rmDtSecretManager . startThreads (  )  ;", ". LOG . info (  (  (  (  (  (  \" Creating   DelegationTokenSecretManager   with   initialInterval :     \"     +    initialInterval )     +     \"  ,    maxLifetime :     \"  )     +    maxLifetime )     +     \"  ,    renewInterval :     \"  )     +    renewInterval )  )  ;", "final   ClientRMService   clientRMService    =    new    . ClientRMServiceForTest ( conf ,    scheduler ,    rmDtSecretManager )  ;", "clientRMService . init ( conf )  ;", "clientRMService . start (  )  ;", "ApplicationClientProtocol   clientRMWithDT    =    null ;", "try    {", "UserGroupInformation   loggedInUser    =    UserGroupInformation . createRemoteUser (  \" testrenewer @ APACHE . ORG \"  )  ;", "Assert . assertEquals (  \" testrenewer \"  ,    loggedInUser . getShortUserName (  )  )  ;", "loggedInUser . setAuthenticationMethod ( KERBEROS )  ;", "Token   token    =    getDelegationToken ( loggedInUser ,    clientRMService ,    loggedInUser . getShortUserName (  )  )  ;", "long   tokenFetchTime    =    System . currentTimeMillis (  )  ;", ". LOG . info (  (  \" Got   delegation   token   at :     \"     +    tokenFetchTime )  )  ;", "clientRMWithDT    =    getClientRMProtocolWithDT ( token ,    clientRMService . getBindAddress (  )  ,     \" loginuser 1  \"  ,    conf )  ;", "GetNewApplicationRequest   request    =    Records . newRecord ( GetNewApplicationRequest . class )  ;", "try    {", "clientRMWithDT . getNewApplication ( request )  ;", "}    catch    ( IOException   e )     {", "fail (  (  \" Unexpected   exception \"     +    e )  )  ;", "}    catch    ( YarnException   e )     {", "fail (  (  \" Unexpected   exception \"     +    e )  )  ;", "}", "while    (  ( System . currentTimeMillis (  )  )     <     ( tokenFetchTime    +     ( initialInterval    /     2  )  )  )     {", "Thread . sleep (  5  0  0 L )  ;", "}", "long   nextExpTime    =    renewDelegationToken ( loggedInUser ,    clientRMService ,    token )  ;", "long   renewalTime    =    System . currentTimeMillis (  )  ;", ". LOG . info (  (  (  (  \" Renewed   token   at :     \"     +    renewalTime )     +     \"  ,    NextExpiryTime :     \"  )     +    nextExpTime )  )  ;", "while    (  (  ( System . currentTimeMillis (  )  )     >     ( tokenFetchTime    +    initialInterval )  )     &  &     (  ( System . currentTimeMillis (  )  )     <    nextExpTime )  )     {", "Thread . sleep (  5  0  0 L )  ;", "}", "Thread . sleep (  5  0 L )  ;", "try    {", "clientRMWithDT . getNewApplication ( request )  ;", "}    catch    ( IOException   e )     {", "fail (  (  \" Unexpected   exception \"     +    e )  )  ;", "}    catch    ( YarnException   e )     {", "fail (  (  \" Unexpected   exception \"     +    e )  )  ;", "}", "while    (  ( System . currentTimeMillis (  )  )     <     ( renewalTime    +    renewInterval )  )     {", "Thread . sleep (  5  0  0 L )  ;", "}", "Thread . sleep (  5  0 L )  ;", ". LOG . info (  (  (  \" At   time :     \"     +     ( System . currentTimeMillis (  )  )  )     +     \"  ,    token   should   be   invalid \"  )  )  ;", "try    {", "clientRMWithDT . getNewApplication ( request )  ;", "fail (  \" Should   not   have   succeeded   with   an   expired   token \"  )  ;", "}    catch    ( Exception   e )     {", "assertEquals ( InvalidToken . class . getName (  )  ,    e . getClass (  )  . getName (  )  )  ;", "assertTrue ( e . getMessage (  )  . contains (  \" is   expired \"  )  )  ;", "}", "if    ( clientRMWithDT    !  =    null )     {", "RPC . stopProxy ( clientRMWithDT )  ;", "clientRMWithDT    =    null ;", "}", "token    =    getDelegationToken ( loggedInUser ,    clientRMService ,    loggedInUser . getShortUserName (  )  )  ;", "tokenFetchTime    =    System . currentTimeMillis (  )  ;", ". LOG . info (  (  \" Got   delegation   token   at :     \"     +    tokenFetchTime )  )  ;", "clientRMWithDT    =    getClientRMProtocolWithDT ( token ,    clientRMService . getBindAddress (  )  ,     \" loginuser 2  \"  ,    conf )  ;", "request    =    Records . newRecord ( GetNewApplicationRequest . class )  ;", "try    {", "clientRMWithDT . getNewApplication ( request )  ;", "}    catch    ( IOException   e )     {", "fail (  (  \" Unexpected   exception \"     +    e )  )  ;", "}    catch    ( YarnException   e )     {", "fail (  (  \" Unexpected   exception \"     +    e )  )  ;", "}", "cancelDelegationToken ( loggedInUser ,    clientRMService ,    token )  ;", "if    ( clientRMWithDT    !  =    null )     {", "RPC . stopProxy ( clientRMWithDT )  ;", "clientRMWithDT    =    null ;", "}", "clientRMWithDT    =    getClientRMProtocolWithDT ( token ,    clientRMService . getBindAddress (  )  ,     \" loginuser 2  \"  ,    conf )  ;", ". LOG . info (  (  \" Cancelled   delegation   token   at :     \"     +     ( System . currentTimeMillis (  )  )  )  )  ;", "try    {", "clientRMWithDT . getNewApplication ( request )  ;", "fail (  \" Should   not   have   succeeded   with   a   cancelled   delegation   token \"  )  ;", "}    catch    ( IOException   e )     {", "}    catch    ( YarnException   e )     {", "}", "}    finally    {", "rmDtSecretManager . stopThreads (  )  ;", "if    ( clientRMWithDT    !  =    null )     {", "RPC . stopProxy ( clientRMWithDT )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    NetUtils . createSocketAddr ( InetAddress . getLocalHost (  )  . getHostName (  )  ,     1  2  3  ,    null )  ;", "checkShortCircuitRenewCancel ( addr ,    addr ,    true )  ;", "}", "METHOD_END"], "methodName": ["testShortCircuitRenewCancel"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   rmAddr    =    NetUtils . createSocketAddr ( InetAddress . getLocalHost (  )  . getHostName (  )  ,     1  2  3  ,    null )  ;", "checkShortCircuitRenewCancel ( rmAddr ,    new   InetSocketAddress (  \"  1  .  1  .  1  .  1  \"  ,     (  ( rmAddr . getPort (  )  )     +     1  )  )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testShortCircuitRenewCancelDifferentHostDifferentPort"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   rmAddr    =    NetUtils . createSocketAddr ( InetAddress . getLocalHost (  )  . getHostName (  )  ,     1  2  3  ,    null )  ;", "checkShortCircuitRenewCancel ( rmAddr ,    new   InetSocketAddress (  \"  1  .  1  .  1  .  1  \"  ,    rmAddr . getPort (  )  )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testShortCircuitRenewCancelDifferentHostSamePort"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   rmAddr    =    NetUtils . createSocketAddr ( InetAddress . getLocalHost (  )  . getHostName (  )  ,     1  2  3  ,    null )  ;", "checkShortCircuitRenewCancel ( rmAddr ,    new   InetSocketAddress ( rmAddr . getAddress (  )  ,     (  ( rmAddr . getPort (  )  )     +     1  )  )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testShortCircuitRenewCancelSameHostDifferentPort"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   rmAddr    =    new   InetSocketAddress (  1  2  3  )  ;", "InetSocketAddress   iceAddr    =    NetUtils . createSocketAddr ( InetAddress . getLocalHost (  )  . getHostName (  )  ,    rmAddr . getPort (  )  ,    null )  ;", "checkShortCircuitRenewCancel ( rmAddr ,    iceAddr ,    true )  ;", "}", "METHOD_END"], "methodName": ["testShortCircuitRenewCancelWildcardAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestClientRMTokens"}, {"methodBody": ["METHOD_START", "{", "TestFifoScheduler   t    =    new   TestFifoScheduler (  )  ;", "t . test (  )  ;", "t . testDefaultMinimumAllocation (  )  ;", "t . testNonDefaultMinimumAllocation (  )  ;", "t . testReconnectedNode (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "TestFifoScheduler . conf    =    new   YarnConfiguration (  )  ;", "TestFifoScheduler . conf . setClass ( RM _ SCHEDULER ,    FifoScheduler . class ,    ResourceScheduler . class )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "MockRM   rm    =    new   MockRM (  . conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "MockNM   nm 2     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  2  :  5  6  7  8  \"  ,     (  4     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp (  2  0  4  8  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "SchedulerNodeReport   report _ nm 1     =    rm . getResourceScheduler (  )  . getNodeReport ( nm 1  . getNodeId (  )  )  ;", "Assert . assertEquals (  (  2     *     ( GB )  )  ,    report _ nm 1  . getUsedResource (  )  . getMemory (  )  )  ;", "RMApp   app 2     =    rm . submitApp (  2  0  4  8  )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 2     =    app 2  . getCurrentAppAttempt (  )  ;", "MockAM   am 2     =    rm . sendAMLaunched ( attempt 2  . getAppAttemptId (  )  )  ;", "am 2  . registerAppAttempt (  )  ;", "SchedulerNodeReport   report _ nm 2     =    rm . getResourceScheduler (  )  . getNodeReport ( nm 2  . getNodeId (  )  )  ;", "Assert . assertEquals (  (  2     *     ( GB )  )  ,    report _ nm 2  . getUsedResource (  )  . getMemory (  )  )  ;", "am 1  . addRequests ( new   String [  ]  {     \"  1  2  7  .  0  .  0  .  1  \"  ,     \"  1  2  7  .  0  .  0  .  2  \"     }  ,    GB ,     1  ,     1  )  ;", "AllocateResponse   alloc 1 Response    =    am 1  . schedule (  )  ;", "am 2  . addRequests ( new   String [  ]  {     \"  1  2  7  .  0  .  0  .  1  \"  ,     \"  1  2  7  .  0  .  0  .  2  \"     }  ,     (  3     *     ( GB )  )  ,     0  ,     1  )  ;", "AllocateResponse   alloc 2 Response    =    am 2  . schedule (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "while    (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     <     1  )     {", ". LOG . info (  \" Waiting   for   containers   to   be   created   for   app    1  .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "alloc 1 Response    =    am 1  . schedule (  )  ;", "}", "while    (  ( alloc 2 Response . getAllocatedContainers (  )  . size (  )  )     <     1  )     {", ". LOG . info (  \" Waiting   for   containers   to   be   created   for   app    2  .  .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "alloc 2 Response    =    am 2  . schedule (  )  ;", "}", "nm 2  . nodeHeartbeat ( true )  ;", "List < Container >    allocated 1     =    alloc 1 Response . getAllocatedContainers (  )  ;", "Assert . assertEquals (  1  ,    allocated 1  . size (  )  )  ;", "Assert . assertEquals (  (  1     *     ( GB )  )  ,    allocated 1  . get (  0  )  . getResource (  )  . getMemory (  )  )  ;", "Assert . assertEquals ( nm 1  . getNodeId (  )  ,    allocated 1  . get (  0  )  . getNodeId (  )  )  ;", "List < Container >    allocated 2     =    alloc 2 Response . getAllocatedContainers (  )  ;", "Assert . assertEquals (  1  ,    allocated 2  . size (  )  )  ;", "Assert . assertEquals (  (  3     *     ( GB )  )  ,    allocated 2  . get (  0  )  . getResource (  )  . getMemory (  )  )  ;", "Assert . assertEquals ( nm 1  . getNodeId (  )  ,    allocated 2  . get (  0  )  . getNodeId (  )  )  ;", "report _ nm 1     =    rm . getResourceScheduler (  )  . getNodeReport ( nm 1  . getNodeId (  )  )  ;", "report _ nm 2     =    rm . getResourceScheduler (  )  . getNodeReport ( nm 2  . getNodeId (  )  )  ;", "Assert . assertEquals (  0  ,    report _ nm 1  . getAvailableResource (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  (  2     *     ( GB )  )  ,    report _ nm 2  . getAvailableResource (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  (  6     *     ( GB )  )  ,    report _ nm 1  . getUsedResource (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  (  2     *     ( GB )  )  ,    report _ nm 2  . getUsedResource (  )  . getMemory (  )  )  ;", "Container   c 1     =    allocated 1  . get (  0  )  ;", "Assert . assertEquals ( GB ,    c 1  . getResource (  )  . getMemory (  )  )  ;", "ContainerStatus   containerStatus    =    BuilderUtils . newContainerStatus ( c 1  . getId (  )  ,    COMPLETE ,     \"  \"  ,     0  )  ;", "nm 1  . containerStatus ( containerStatus )  ;", "int   waitCount    =     0  ;", "while    (  (  ( attempt 1  . getJustFinishedContainers (  )  . size (  )  )     <     1  )     &  &     (  ( waitCount +  +  )     !  =     2  0  )  )     {", ". LOG . info (  (  (  \" Waiting   for   containers   to   be   finished   for   app    1  .  .  .    Tried    \"     +    waitCount )     +     \"    times   already .  .  \"  )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertEquals (  1  ,    attempt 1  . getJustFinishedContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    am 1  . schedule (  )  . getCompletedContainersStatuses (  )  . size (  )  )  ;", "report _ nm 1     =    rm . getResourceScheduler (  )  . getNodeReport ( nm 1  . getNodeId (  )  )  ;", "Assert . assertEquals (  (  5     *     ( GB )  )  ,    report _ nm 1  . getUsedResource (  )  . getMemory (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "MockRM   rm    =    new   MockRM (  . conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp (  2  0  4  8  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "List < ResourceRequest >    requests    =    new   ArrayList < ResourceRequest >  (  )  ;", "requests . add ( am 1  . createResourceReq (  \"  1  2  7  .  0  .  0  .  1  \"  ,     (  1     *     ( GB )  )  ,     1  ,     1  )  )  ;", "requests . add ( am 1  . createResourceReq (  \"  / default - rack \"  ,     (  1     *     ( GB )  )  ,     1  ,     1  )  )  ;", "am 1  . allocate ( requests ,    null )  ;", "try    {", "nm 1  . nodeHeartbeat ( true )  ;", "}    catch    ( NullPointerException   e )     {", "Assert . fail (  (  \" NPE   when   allocating   container   on   node   but    \"     +     \" forget   to   set   off - switch   request   should   be   handled \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAllocateContainerOnNodeWithoutOffSwitchSpecified"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "fs    =     (  (  )     ( rm . getResourceScheduler (  )  )  )  ;", "int   rack _ num _  0     =     0  ;", "int   rack _ num _  1     =     1  ;", "String   host _  0  _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "RMNode   n 1     =    MockNodes . newNodeInfo ( rack _ num _  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host _  0  _  0  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "String   host _  0  _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "RMNode   n 2     =    MockNodes . newNodeInfo ( rack _ num _  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host _  0  _  1  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 2  )  )  ;", "String   host _  1  _  0     =     \"  1  2  7  .  0  .  0  .  3  \"  ;", "RMNode   n 3     =    MockNodes . newNodeInfo ( rack _ num _  1  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host _  1  _  0  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 3  )  )  ;", "String   host _  1  _  1     =     \"  1  2  7  .  0  .  0  .  4  \"  ;", "RMNode   n 4     =    MockNodes . newNodeInfo ( rack _ num _  1  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host _  1  _  1  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 4  )  )  ;", "ApplicationId   appId 1     =    BuilderUtils . newApplicationId (  1  0  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId 1     =    BuilderUtils . newApplicationAttemptId ( appId 1  ,     1  )  ;", "SchedulerEvent   appEvent    =    new   AppAddedSchedulerEvent ( appId 1  ,     \" queue \"  ,     \" user \"  )  ;", "fs . handle ( appEvent )  ;", "SchedulerEvent   attemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId 1  ,    false )  ;", "fs . handle ( attemptEvent )  ;", "List < ContainerId >    emptyId    =    new   ArrayList < ContainerId >  (  )  ;", "List < ResourceRequest >    emptyAsk    =    new   ArrayList < ResourceRequest >  (  )  ;", "List < ResourceRequest >    ask 1     =    new   ArrayList < ResourceRequest >  (  )  ;", "ask 1  . add ( BuilderUtils . newResourceRequest ( BuilderUtils . newPriority (  0  )  ,     \" rack 1  \"  ,    BuilderUtils . newResource ( GB ,     1  )  ,     1  )  )  ;", "ask 1  . add ( BuilderUtils . newResourceRequest ( BuilderUtils . newPriority (  0  )  ,    ANY ,    BuilderUtils . newResource ( GB ,     1  )  ,     1  )  )  ;", "fs . allocate ( appAttemptId 1  ,    ask 1  ,    emptyId ,    Collections . singletonList ( host _  1  _  0  )  ,    null )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 3  )  )  ;", "Allocation   allocation 1     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" allocation 1  \"  ,     0  ,    allocation 1  . getContainers (  )  . size (  )  )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 4  )  )  ;", "Allocation   allocation 2     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" allocation 2  \"  ,     1  ,    allocation 2  . getContainers (  )  . size (  )  )  ;", "List < Container >    containerList    =    allocation 2  . getContainers (  )  ;", "for    ( Container   container    :    containerList )     {", "Assert . assertEquals (  \" Container   is   allocated   on   n 4  \"  ,    container . getNodeId (  )  ,    n 4  . getNodeID (  )  )  ;", "}", "List < ResourceRequest >    ask 2     =    new   ArrayList < ResourceRequest >  (  )  ;", "ask 2  . add ( BuilderUtils . newResourceRequest ( BuilderUtils . newPriority (  0  )  ,    ANY ,    BuilderUtils . newResource ( GB ,     1  )  ,     1  )  )  ;", "fs . allocate ( appAttemptId 1  ,    ask 2  ,    emptyId ,    Collections . singletonList (  \" rack 0  \"  )  ,    null )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 1  )  )  ;", "Allocation   allocation 3     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" allocation 3  \"  ,     0  ,    allocation 3  . getContainers (  )  . size (  )  )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 2  )  )  ;", "Allocation   allocation 4     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" allocation 4  \"  ,     0  ,    allocation 4  . getContainers (  )  . size (  )  )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 3  )  )  ;", "Allocation   allocation 5     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" allocation 5  \"  ,     0  ,    allocation 5  . getContainers (  )  . size (  )  )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 4  )  )  ;", "Allocation   allocation 6     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" allocation 6  \"  ,     1  ,    allocation 6  . getContainers (  )  . size (  )  )  ;", "containerList    =    allocation 6  . getContainers (  )  ;", "for    ( Container   container    :    containerList )     {", "Assert . assertEquals (  \" Container   is   allocated   on   n 4  \"  ,    container . getNodeId (  )  ,    n 4  . getNodeID (  )  )  ;", "}", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testBlackListNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FifoScheduler   scheduler    =    new   FifoScheduler (  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     2  0  4  8  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,     1  0  2  4  )  ;", "try    {", "scheduler . serviceInit ( conf )  ;", "fail (  (  \" Exception   is   expected   because   the   min   memory   allocation   is \"     +     \"    larger   than   the   max   memory   allocation .  \"  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertTrue (  \" The   thrown   exception   is   not   the   expected   one .  \"  ,    e . getMessage (  )  . startsWith (  \" Invalid   resource   scheduler   memory \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testConfValidation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "testMinimumAllocation ( new   YarnConfiguration ( TestFifoScheduler . conf )  ,     (  ( YarnConfiguration . DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )     /     2  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultMinimumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "fs    =     (  (  )     ( rm . getResourceScheduler (  )  )  )  ;", "RMNode   n 1     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "ApplicationId   appId 1     =    BuilderUtils . newApplicationId (  1  0  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId 1     =    BuilderUtils . newApplicationAttemptId ( appId 1  ,     1  )  ;", "SchedulerEvent   appEvent    =    new   AppAddedSchedulerEvent ( appId 1  ,     \" queue \"  ,     \" user \"  )  ;", "fs . handle ( appEvent )  ;", "SchedulerEvent   attemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId 1  ,    false )  ;", "fs . handle ( attemptEvent )  ;", "ApplicationId   appId 2     =    BuilderUtils . newApplicationId (  2  0  0  ,     2  )  ;", "ApplicationAttemptId   appAttemptId 2     =    BuilderUtils . newApplicationAttemptId ( appId 2  ,     1  )  ;", "SchedulerEvent   appEvent 2     =    new   AppAddedSchedulerEvent ( appId 2  ,     \" queue \"  ,     \" user \"  )  ;", "fs . handle ( appEvent 2  )  ;", "SchedulerEvent   attemptEvent 2     =    new   AppAttemptAddedSchedulerEvent ( appAttemptId 2  ,    false )  ;", "fs . handle ( attemptEvent 2  )  ;", "List < ContainerId >    emptyId    =    new   ArrayList < ContainerId >  (  )  ;", "List < ResourceRequest >    emptyAsk    =    new   ArrayList < ResourceRequest >  (  )  ;", "List < ResourceRequest >    ask 1     =    new   ArrayList < ResourceRequest >  (  )  ;", "ask 1  . add ( BuilderUtils . newResourceRequest ( BuilderUtils . newPriority (  0  )  ,    ANY ,    BuilderUtils . newResource ( GB ,     1  )  ,     1  )  )  ;", "fs . allocate ( appAttemptId 1  ,    ask 1  ,    emptyId ,    null ,    null )  ;", "List < ResourceRequest >    ask 2     =    new   ArrayList < ResourceRequest >  (  )  ;", "ask 2  . add ( BuilderUtils . newResourceRequest ( BuilderUtils . newPriority (  0  )  ,    ANY ,    BuilderUtils . newResource (  (  2     *     ( GB )  )  ,     1  )  ,     1  )  )  ;", "fs . allocate ( appAttemptId 2  ,    ask 2  ,    emptyId ,    null ,    null )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 1  )  )  ;", "Allocation   allocation 1     =    fs . allocate ( appAttemptId 1  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" Allocation   headroom \"  ,     (  1     *     ( GB )  )  ,    allocation 1  . getResourceLimit (  )  . getMemory (  )  )  ;", "Allocation   allocation 2     =    fs . allocate ( appAttemptId 2  ,    emptyAsk ,    emptyId ,    null ,    null )  ;", "Assert . assertEquals (  \" Allocation   headroom \"  ,     (  1     *     ( GB )  )  ,    allocation 2  . getResourceLimit (  )  . getMemory (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testHeadroom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  6     *     ( GB )  )  )  ;", "RMApp   app 1     =    rm . submitApp ( testAlloc )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "NodeReport   report _ nm 1     =    rm . getResource (  )  . getNodeReport ( nm 1  . getNodeId (  )  )  ;", "int   checkAlloc    =    conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "Assert . assertEquals ( checkAlloc ,    report _ nm 1  . getUsedResource (  )  . getMemory (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMinimumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FifoScheduler   scheduler    =    new   FifoScheduler (  )  ;", "MockRM   rm    =    new   MockRM (  . conf )  ;", "scheduler . setRMContext ( rm . getRMContext (  )  )  ;", "scheduler . init (  . conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize (  . conf ,    rm . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "scheduler . handle ( new   NodeAddedSchedulerEvent ( node )  )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,     1  )  ;", "scheduler . addApplication ( appId ,     \" queue 1  \"  ,     \" user 1  \"  ,    false )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "try    {", "scheduler . handle ( updateEvent )  ;", "}    catch    ( NullPointerException   e )     {", "Assert . fail (  )  ;", "}", "ApplicationAttemptId   attId    =    ApplicationAttemptId . newInstance ( appId ,     1  )  ;", "scheduler . addApplicationAttempt ( attId ,    false ,    false )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNodeUpdateBeforeAppAttemptInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   allocMB    =     1  5  3  6  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  . conf )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    allocMB )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,     ( allocMB    *     1  0  )  )  ;", "testMinimumAllocation ( conf ,     ( allocMB    /     2  )  )  ;", "}", "METHOD_END"], "methodName": ["testNonDefaultMinimumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "conf . setQueues (  \" default \"  ,    new   String [  ]  {     \" default \"     }  )  ;", "conf . setCapacity (  \" default \"  ,     1  0  0  )  ;", "fs    =    new    (  )  ;", "fs . init ( conf )  ;", "fs . start (  )  ;", "RMContext   context    =    mock ( RMContext . class )  ;", "fs . reinitialize ( conf ,    null )  ;", "fs . setRMContext ( context )  ;", "RMNode   n 1     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "RMNode   n 2     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  2     *     ( GB )  )  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  3  \"  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 2  )  )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 1  )  )  ;", "Assert . assertEquals (  (  6     *     ( GB )  )  ,    fs . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "n 1     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  2     *     ( GB )  )  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "fs . handle ( new   NodeRemovedSchedulerEvent ( n 1  )  )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "fs . handle ( new   NodeUpdateSchedulerEvent ( n 1  )  )  ;", "Assert . assertEquals (  (  4     *     ( GB )  )  ,    fs . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "fs . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testReconnectedNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( initialRMAppState ,    RMHATestBase . rm 1  . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getState (  )  )  ;", "Assert . assertEquals ( initialRMAppAttemptState ,    RMHATestBase . rm 1  . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getAppAttempts (  )  . get ( appAttemptId )  . getState (  )  )  ;", "explicitFailover (  )  ;", "Assert . assertEquals ( expectedAppStateBeforeKillApp ,    RMHATestBase . rm 2  . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getState (  )  )  ;", "k ( RMHATestBase . rm 2  ,    appId ,    appAttemptId ,    initialRMAppState )  ;", "}", "METHOD_END"], "methodName": ["failOverAndKillApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( initialRMAppState ,    RMHATestBase . rm 1  . getRMContext (  )  . getRMApps (  )  . get ( appId )  . getState (  )  )  ;", "explicitFailover (  )  ;", "Assert . assertTrue (  (  ( RMHATestBase . rm 2  . getRMContext (  )  . getRMApps (  )  . get ( appId )  )     =  =    null )  )  ;", "k ( RMHATestBase . rm 2  ,    appId ,    null ,    initialRMAppState )  ;", "}", "METHOD_END"], "methodName": ["failOverAndKillApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "KillApplicationResponse   response    =    rm . killApp ( appId )  ;", "Assert . assertTrue (  (  ( response . getIsKillCompleted (  )  )     =  =     ( isFinalState ( rmAppState )  )  )  )  ;", "RMApp   loadedApp 0     =    rm . getRMContext (  )  . getRMApps (  )  . get ( appId )  ;", "rm . waitForState ( appId ,    RMAppState . KILLED )  ;", "if    ( appAttemptId    !  =    null )     {", "rm . waitForState ( appAttemptId ,    RMAppAttemptState . KILLED )  ;", "}", "Assert . assertEquals (  1  ,    loadedApp 0  . getAppAttempts (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["killApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf 1     =    new   Configuration ( confForRM 1  )  ;", "RMHATestBase . rm 1     =    new   MockRM ( conf 1  )     {", "@ Override", "protected   ClientRMService   createClientRMService (  )     {", "return   new    . MyClientRMService ( this . rmContext ,    this . scheduler ,    this . rmAppManager ,    this . applicationACLsManager ,    this . queueACLsManager ,    getRMContext (  )  . getRMDelegationTokenSecretManager (  )  )  ;", "}", "}  ;", "RMHATestBase . rm 2     =    new   MockRM ( confForRM 2  )  ;", "startRMs ( RMHATestBase . rm 1  ,    conf 1  ,    RMHATestBase . rm 2  ,    confForRM 2  )  ;", "}", "METHOD_END"], "methodName": ["startRMsWithCustomizedClientRMService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMsWithCustomizedClientRMService (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    RMHATestBase . rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    RMHATestBase . rm 1  ,    nm 1  )  ;", "Assert . assertEquals ( app 0  . getState (  )  ,    RMAppState . RUNNING )  ;", "RMHATestBase . rm 1  . killApp ( app 0  . getId (  )  )  ;", "failOverAndKillApp ( app 0  . getId (  )  ,    am 0  . getAttemptId (  )  ,    RMAppState . RUNNING ,    RMAppAttemptState . RUNNING ,    RMAppState . ACCEPTED )  ;", "}", "METHOD_END"], "methodName": ["testKillAppWhenFailOverHappensDuringApplicationKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMs (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    RMHATestBase . rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    RMHATestBase . rm 1  ,    nm 1  )  ;", "RMHATestBase . rm 1  . killApp ( app 0  . getId (  )  )  ;", "RMHATestBase . rm 1  . waitForState ( app 0  . getId (  )  ,    RMAppState . KILLED )  ;", "RMHATestBase . rm 1  . waitForState ( am 0  . getAttemptId (  )  ,    RMAppAttemptState . KILLED )  ;", "failOverAndKillApp ( app 0  . getId (  )  ,    am 0  . getAttemptId (  )  ,    RMAppState . KILLED ,    RMAppAttemptState . KILLED ,    RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testKillAppWhenFailoverHappensAtFinalState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMsWithCustomizedRMAppManager (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    RMHATestBase . rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false )  ;", "try    {", "failOverAndKillApp ( app 0  . getId (  )  ,    RMAppState . NEW )  ;", "fail (  \" Should   get   an   exception   here \"  )  ;", "}    catch    ( NotFoundException   ex )     {", "Assert . assertTrue ( ex . getMessage (  )  . contains (  (  \" Trying   to   kill   an   absent   application    \"     +     ( app 0  . getId (  )  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testKillAppWhenFailoverHappensAtNewState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMs (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    RMHATestBase . rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    RMHATestBase . rm 1  ,    nm 1  )  ;", "failOverAndKillApp ( app 0  . getId (  )  ,    am 0  . getAttemptId (  )  ,    RMAppState . RUNNING ,    RMAppAttemptState . RUNNING ,    RMAppState . ACCEPTED )  ;", "}", "METHOD_END"], "methodName": ["testKillAppWhenFailoverHappensAtRunningState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestKillApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . FifoSchedulerWithMove . class ,     . FifoSchedulerWithMove . class )  ;", "conf . set ( YARN _ ADMIN _ ACL ,     \"     \"  )  ;", "conf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( conf )  ;", "resourceManager . getRMContext (  )  . getContainerTokenSecretManager (  )  . rollMasterKey (  )  ;", "resourceManager . getRMContext (  )  . getNMTokenSecretManager (  )  . rollMasterKey (  )  ;", "resourceManager . start (  )  ;", ". failMove    =    false ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication"}, {"methodBody": ["METHOD_START", "{", "resourceManager . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication"}, {"methodBody": ["METHOD_START", "{", "TestMoveApplication . failMove    =    true ;", "final   Application   application    =    new   Application (  \" user 1  \"  ,    resourceManager )  ;", "application . submit (  )  ;", "final   ClientRMService   clientRMService    =    resourceManager . getClientRMService (  )  ;", "try    {", "UserGroupInformation . createRemoteUser (  \" otheruser \"  )  . doAs ( new   PrivilegedExceptionAction < MoveApplicationAcrossQueuesResponse >  (  )     {", "@ Override", "public   MoveApplicationAcrossQueuesResponse   run (  )    throws   Exception    {", "return   clientRMService . moveApplicationAcrossQueues ( MoveApplicationAcrossQueuesRequest . newInstance ( application . getApplicationId (  )  ,     \" newqueue \"  )  )  ;", "}", "}  )  ;", "fail (  \" Should   have   hit   exception \"  )  ;", "}    catch    ( Exception   ex )     {", "assertEquals ( AccessControlException . class ,    ex . getCause (  )  . getCause (  )  . getClass (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMoveRejectedByPermissions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication"}, {"methodBody": ["METHOD_START", "{", "TestMoveApplication . failMove    =    true ;", "Application   application    =    new   Application (  \" user 1  \"  ,    resourceManager )  ;", "application . submit (  )  ;", "RMApp   app    =    resourceManager . rmContext . getRMApps (  )  . get ( application . getApplicationId (  )  )  ;", "while    (  ( app . getState (  )  )     !  =     ( RMAppState . ACCEPTED )  )     {", "Thread . sleep (  1  0  0  )  ;", "}", "ClientRMService   clientRMService    =    resourceManager . getClientRMService (  )  ;", "try    {", "clientRMService . moveApplicationAcrossQueues ( MoveApplicationAcrossQueuesRequest . newInstance ( application . getApplicationId (  )  ,     \" newqueue \"  )  )  ;", "fail (  \" Should   have   hit   exception \"  )  ;", "}    catch    ( YarnException   ex )     {", "assertEquals (  \" Move   not   supported \"  ,    ex . getCause (  )  . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMoveRejectedByScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "RMApp   app    =    rm 1  . submitApp (  1  0  2  4  )  ;", "ClientRMService   clientRMService    =    rm 1  . getClientRMService (  )  ;", "clientRMService . moveApplicationAcrossQueues ( AcrossQueuesRequest . newInstance ( app . getApplicationId (  )  ,     \" newqueue \"  )  )  ;", "RMApp   rmApp    =    rm 1  . getRMContext (  )  . getRMApps (  )  . get ( app . getApplicationId (  )  )  ;", "assertEquals (  \" newqueue \"  ,    rmApp . getQueue (  )  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMoveSuccessful"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication"}, {"methodBody": ["METHOD_START", "{", "Application   application    =    new   Application (  \" user 1  \"  ,    resourceManager )  ;", "ApplicationId   appId    =    application . getApplicationId (  )  ;", "application . submit (  )  ;", "ClientRMService   clientRMService    =    resourceManager . getClientRMService (  )  ;", "clientRMService . forceKillApplication ( KillApplicationRequest . newInstance ( appId )  )  ;", "RMApp   rmApp    =    resourceManager . getRMContext (  )  . getRMApps (  )  . get ( appId )  ;", "while    (  ( rmApp . getState (  )  )     !  =     ( RMAppState . KILLED )  )     {", "Thread . sleep (  1  0  0  )  ;", "}", "try    {", "clientRMService . moveApplicationAcrossQueues ( AcrossQueuesRequest . newInstance ( appId ,     \" newqueue \"  )  )  ;", "fail (  \" Should   have   hit   exception \"  )  ;", "}    catch    ( YarnException   ex )     {", "assertEquals ( YarnException . class ,    ex . getClass (  )  )  ;", "assertEquals (  \" App   in   KILLED   state   cannot   be   moved .  \"  ,    ex . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMoveTooLate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestMoveApplication"}, {"methodBody": ["METHOD_START", "{", "ArrayList < ContainerId >    releaseContainerList    =    new   ArrayList < ContainerId >  (  )  ;", "AllocateResponse   response ;", "ArrayList < ResourceRequest >    Request    =    new   ArrayList < ResourceRequest >  (  )  ;", "while    (  ( containersReceived . size (  )  )     <    totalContainerRequested )     {", "nm . nodeHeartbeat ( true )  ;", "TestRM . LOG . info (  \" requesting   containers .  .  \"  )  ;", "response    =    am . allocate ( Request ,    releaseContainerList )  ;", "containersReceived . addAll ( response . getAllocatedContainers (  )  )  ;", "if    (  !  ( response . getNMTokens (  )  . isEmpty (  )  )  )     {", "for    ( NMToken   nmToken    :    response . getNMTokens (  )  )     {", "String   nodeId    =    nmToken . getNodeId (  )  . toString (  )  ;", "if    ( nmTokens . containsKey ( nodeId )  )     {", "Assert . fail (  (  \" Duplicate   NMToken   received   for    :     \"     +    nodeId )  )  ;", "}", "nmTokens . put ( nodeId ,    nmToken . getToken (  )  )  ;", "}", "}", "TestRM . LOG . info (  (  (  (  \" Got    \"     +     ( containersReceived . size (  )  )  )     +     \"    containers .    Waiting   to   get    \"  )     +    totalContainerRequested )  )  ;", "Thread . sleep ( TestRM . WAIT _ SLEEP _ MS )  ;", "}", "}", "METHOD_END"], "methodName": ["allocateContainersAndValidateNMTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "TestRM   t    =    new   TestRM (  )  ;", "t . testGetNewAppId (  )  ;", "t . testAppWithNoContainers (  )  ;", "t . testAppOnMultiNode (  )  ;", "t . testNMToken (  )  ;", "t . testActivatingApplicationAfterAddingNM (  )  ;", "t . testInvalidateAMHostPortWhenAMFailedOrKilled (  )  ;", "t . testInvalidatedAMHostPortOnAMRestart (  )  ;", "t . testApplicationKillAtAcceptedState (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "ClusterMetrics . destroy (  )  ;", "QueueMetrics . clearQueueMetrics (  )  ;", "DefaultMetricsSystem . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "RMApp   app 2     =    rm 1  . submitApp (  2  0  0  )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 1     =    attempt 1  . getAppAttemptId (  )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . SCHEDULED )  ;", "RMAppAttempt   attempt 2     =    app 2  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 2     =    attempt 2  . getAppAttemptId (  )  ;", "rm 1  . waitForState ( attemptId 2  ,    RMAppAttemptState . SCHEDULED )  ;", "MockNM   nm 1     =    new   MockNM (  \" h 1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "MockNM   nm 2     =    new   MockNM (  \" h 2  :  5  6  7  8  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "nm 2  . registerNode (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "rm 1  . waitForState ( attemptId 2  ,    RMAppAttemptState . SCHEDULED )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "rm 1  . waitForState ( attemptId 2  ,    RMAppAttemptState . ALLOCATED )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testActivatingApplicationAfterAddingNM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set (  \" yarn . scheduler . capacity . node - locality - delay \"  ,     \"  -  1  \"  )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" h 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "int   request    =     1  3  ;", "am . allocate (  \" h 1  \"  ,     1  0  0  0  ,    request ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "List < Container >    conts    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "int   contReceived    =    conts . size (  )  ;", "while    ( contReceived    <     3  )     {", "nm 1  . nodeHeartbeat ( true )  ;", "conts . addAll ( am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "contReceived    =    conts . size (  )  ;", "TestRM . LOG . info (  (  (  (  \" Got    \"     +    contReceived )     +     \"    containers .    Waiting   to   get    \"  )     +     3  )  )  ;", "Thread . sleep ( TestRM . WAIT _ SLEEP _ MS )  ;", "}", "Assert . assertEquals (  3  ,    conts . size (  )  )  ;", "conts    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "contReceived    =    conts . size (  )  ;", "while    ( contReceived    <     1  0  )     {", "nm 2  . nodeHeartbeat ( true )  ;", "conts . addAll ( am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "contReceived    =    conts . size (  )  ;", "TestRM . LOG . info (  (  (  (  \" Got    \"     +    contReceived )     +     \"    containers .    Waiting   to   get    \"  )     +     1  0  )  )  ;", "Thread . sleep ( TestRM . WAIT _ SLEEP _ MS )  ;", "}", "Assert . assertEquals (  1  0  ,    conts . size (  )  )  ;", "am . unregisterAppAttempt (  )  ;", "nm 1  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppOnMultiNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "MockRM   rm    =    new   MockRM (  )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "nm 1  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppWithNoContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "final   Dispatcher   dispatcher    =    new   AsyncDispatcher (  )     {", "@ Override", "public   EventHandler   getEventHandler (  )     {", "class   EventArgMatcher   extends   ArgumentMatcher < AbstractEvent >     {", "@ Override", "public   boolean   matches ( Object   argument )     {", "if    ( argument   instanceof   RMAppAttemptEvent )     {", "if    (  (  ( RMAppAttemptEvent )     ( argument )  )  . getType (  )  . equals ( RMAppAttemptEventType . KILL )  )     {", "return   true ;", "}", "}", "return   false ;", "}", "}", "EventHandler   handler    =    spy ( super . getEventHandler (  )  )  ;", "doNothing (  )  . when ( handler )  . handle ( argThat ( new   EventArgMatcher (  )  )  )  ;", "return   handler ;", "}", "}  ;", "MockRM   rm    =    new   MockRM ( conf )     {", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "}  ;", "QueueMetrics   metrics    =    rm . getRScheduler (  )  . getRootQueueMetrics (  )  ;", "int   appsKilled    =    metrics . getAppsKilled (  )  ;", "int   appsSubmitted    =    metrics . getAppsSubmitted (  )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   application    =    rm . submitApp (  2  0  0  )  ;", "MockAM   am    =    MockRM . launchAM ( application ,    rm ,    nm 1  )  ;", "am . waitForState ( RMAppAttemptState . LAUNCHED )  ;", "nm 1  . nodeHeartbeat ( am . getApplicationAttemptId (  )  ,     1  ,    RUNNING )  ;", "rm . waitForState ( application . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "KillApplicationRequest   request    =    KillApplicationRequest . newInstance ( application . getApplicationId (  )  )  ;", "rm . getClientRMService (  )  . forceKillApplication ( request )  ;", "am . registerAppAttempt ( false )  ;", "rm . waitForState ( application . getApplicationId (  )  ,    RMAppState . KILLING )  ;", "rm . waitForState ( am . getApplicationAttemptId (  )  ,    RMAppAttemptState . RUNNING )  ;", "rm . getRMContext (  )  . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ KILLED )  )  ;", "rm . waitForState ( application . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "metrics    =    rm . getRScheduler (  )  . getRootQueueMetrics (  )  ;", "Assert . assertEquals (  ( appsKilled    +     1  )  ,    metrics . getAppsKilled (  )  )  ;", "Assert . assertEquals (  ( appsSubmitted    +     1  )  ,    metrics . getAppsSubmitted (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationKillAtAcceptedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "MockRM   rm    =    new   MockRM (  )  ;", "rm . start (  )  ;", "GetNewApplicationResponse   resp    =    rm . getNewAppId (  )  ;", "assert    ( resp . getApplicationId (  )  . getId (  )  )     !  =     0  ;", "assert    ( resp . getMaximumResourceCapability (  )  . getMemory (  )  )     >     0  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testGetNewAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "MockRM . finishAMAndVerifyAppState ( app 1  ,    rm 1  ,    nm 1  ,    am 1  )  ;", "RMApp   app 2     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 2     =    MockRM . launchAndRegisterAM ( app 2  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 2  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 2  . waitForState ( RMAppAttemptState . FAILED )  ;", "rm 1  . waitForState ( app 2  . getApplicationId (  )  ,    RMAppState . FAILED )  ;", "RMApp   app 3     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 3     =    MockRM . launchAndRegisterAM ( app 3  ,    rm 1  ,    nm 1  )  ;", "rm 1  . killApp ( app 3  . getApplicationId (  )  )  ;", "rm 1  . waitForState ( app 3  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "rm 1  . waitForState ( am 3  . getApplicationAttemptId (  )  ,    RMAppAttemptState . KILLED )  ;", "GetApplicationsRequest   request 1     =    GetApplicationsRequest . newInstance ( EnumSet . of ( FINISHED ,    KILLED ,    FAILED )  )  ;", "GetApplicationsResponse   response 1     =    rm 1  . getClientRMService (  )  . getApplications ( request 1  )  ;", "List < ApplicationReport >    appList 1     =    response 1  . getApplicationList (  )  ;", "Assert . assertEquals (  3  ,    appList 1  . size (  )  )  ;", "for    ( ApplicationReport   report    :    appList 1  )     {", "if    (  ( report . getApplicationId (  )  . equals ( app 2  . getApplicationId (  )  )  )     |  |     ( report . getApplicationId (  )  . equals ( app 3  . getApplicationId (  )  )  )  )     {", "Assert . assertEquals (  \" N / A \"  ,    report . getHost (  )  )  ;", "Assert . assertEquals (  (  -  1  )  ,    report . getRpcPort (  )  )  ;", "}", "if    ( report . getApplicationId (  )  . equals ( app 1  . getApplicationId (  )  )  )     {", "Assert . assertFalse ( report . getHost (  )  . equals (  \" N / A \"  )  )  ;", "Assert . assertTrue (  (  ( report . getRpcPort (  )  )     !  =     (  -  1  )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testInvalidateAMHostPortWhenAMFailedOrKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 2     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 2     =    MockRM . launchAndRegisterAM ( app 2  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 2  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 2  . waitForState ( RMAppAttemptState . FAILED )  ;", "rm 1  . waitForState ( app 2  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "GetApplicationReportRequest   request 1     =    GetApplicationReportRequest . newInstance ( app 2  . getApplicationId (  )  )  ;", "ApplicationReport   report 1     =    rm 1  . getClientRMService (  )  . getApplicationReport ( request 1  )  . getApplicationReport (  )  ;", "Assert . assertEquals (  \" N / A \"  ,    report 1  . getHost (  )  )  ;", "Assert . assertEquals (  (  -  1  )  ,    report 1  . getRpcPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["testInvalidatedAMHostPortOnAMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM (  )  ;", "try    {", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "NMTokenSecretMnRM   nmTokenSecretM =    rm . getRMContext (  )  . getNMTokenSecretM )  ;", "RMApp   app    =    rm . submitApp (  1  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptRegistered ( attempt . getAppAttemptId (  )  )  )  ;", "am . registerAppAttempt (  )  ;", "ArrayList < Container >    containersReceivedForNM 1     =    new   ArrayList < Container >  (  )  ;", "List < ContainerId >    releaseContainerList    =    new   ArrayList < ContainerId >  (  )  ;", "HashMap < String ,    Token >    nmTokens    =    new   HashMap < String ,    Token >  (  )  ;", "AllocateResponse   response    =    am . allocate (  \" h 1  \"  ,     1  0  0  0  ,     2  ,    releaseContainerList )  ;", "Assert . assertEquals (  0  ,    response . getAllocatedContainers (  )  . size (  )  )  ;", "allocateContainersAndValidateNMTokens ( am ,    containersReceivedForNM 1  ,     2  ,    nmTokens ,    nm 1  )  ;", "Assert . assertEquals (  1  ,    nmTokens . size (  )  )  ;", "response    =    am . allocate (  \" h 1  \"  ,     1  0  0  0  ,     2  ,    releaseContainerList )  ;", "Assert . assertEquals (  0  ,    response . getAllocatedContainers (  )  . size (  )  )  ;", "allocateContainersAndValidateNMTokens ( am ,    containersReceivedForNM 1  ,     4  ,    nmTokens ,    nm 1  )  ;", "Assert . assertEquals (  1  ,    nmTokens . size (  )  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" h 2  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "ArrayList < Container >    containersReceivedForNM 2     =    new   ArrayList < Container >  (  )  ;", "response    =    am . allocate (  \" h 2  \"  ,     1  0  0  0  ,     2  ,    releaseContainerList )  ;", "Assert . assertEquals (  0  ,    response . getAllocatedContainers (  )  . size (  )  )  ;", "allocateContainersAndValidateNMTokens ( am ,    containersReceivedForNM 2  ,     2  ,    nmTokens ,    nm 2  )  ;", "Assert . assertEquals (  2  ,    nmTokens . size (  )  )  ;", "nm 2     =    rm . registerNode (  \" h 2  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "Map < NodeId ,    RMNode >    nodes    =    rm . getRMContext (  )  . getRMNodes (  )  ;", "while    (  ( nodes . get ( nm 2  . getNodeId (  )  )  . getLastNodeHeartBeatResponse (  )  . getResponseId (  )  )     >     0  )     {", "Thread . sleep ( TestRM . WAIT _ SLEEP _ MS )  ;", "}", "int   interval    =     4  0  ;", "while    (  ( nmTokenSecretMisApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 2  . getNodeId (  )  )  )     &  &     (  ( interval -  -  )     >     0  )  )     {", "TestRM . LOG . info (  (  \" waiting   for   nmToken   to   be   cleared   for    :     \"     +     ( nm 2  . getNodeId (  )  )  )  )  ;", "Thread . sleep ( TestRM . WAIT _ SLEEP _ MS )  ;", "}", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptRegistered ( attempt . getAppAttemptId (  )  )  )  ;", "nmTokens . remove ( nm 2  . getNodeId (  )  . toString (  )  )  ;", "Assert . assertEquals (  1  ,    nmTokens . size (  )  )  ;", "response    =    am . allocate (  \" h 2  \"  ,     1  0  0  0  ,     2  ,    releaseContainerList )  ;", "Assert . assertEquals (  0  ,    response . getAllocatedContainers (  )  . size (  )  )  ;", "allocateContainersAndValidateNMTokens ( am ,    containersReceivedForNM 2  ,     4  ,    nmTokens ,    nm 2  )  ;", "Assert . assertEquals (  2  ,    nmTokens . size (  )  )  ;", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 1  . getNodeId (  )  )  )  ;", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 2  . getNodeId (  )  )  )  ;", "nmTokenSecretMrollMasterKey (  )  ;", "nmTokenSecretMactivateNextMasterKey (  )  ;", "Assert . assertFalse ( nmTokenSecretMisApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 1  . getNodeId (  )  )  )  ;", "Assert . assertFalse ( nmTokenSecretMisApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 2  . getNodeId (  )  )  )  ;", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptRegistered ( attempt . getAppAttemptId (  )  )  )  ;", "nmTokens . clear (  )  ;", "Assert . assertEquals (  0  ,    nmTokens . size (  )  )  ;", "response    =    am . allocate (  \" h 2  \"  ,     1  0  0  0  ,     1  ,    releaseContainerList )  ;", "Assert . assertEquals (  0  ,    response . getAllocatedContainers (  )  . size (  )  )  ;", "allocateContainersAndValidateNMTokens ( am ,    containersReceivedForNM 2  ,     5  ,    nmTokens ,    nm 2  )  ;", "Assert . assertEquals (  1  ,    nmTokens . size (  )  )  ;", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 2  . getNodeId (  )  )  )  ;", "Assert . assertTrue ( nmTokenSecretMisApplicationAttemptRegistered ( attempt . getAppAttemptId (  )  )  )  ;", "am . unregisterAppAttempt (  )  ;", "for    ( Container   container    :    containersReceivedForNM 1  )     {", "nm 1  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,    container . getId (  )  . getId (  )  ,    COMPLETE )  ;", "}", "for    ( Container   container    :    containersReceivedForNM 2  )     {", "nm 2  . nodeHeartbeat ( attempt . getAppAttemptId (  )  ,    container . getId (  )  . getId (  )  ,    COMPLETE )  ;", "}", "nm 1  . nodeHeartbeat ( am . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "Assert . assertFalse ( nmTokenSecretMisApplicationAttemptRegistered ( attempt . getAppAttemptId (  )  )  )  ;", "}    finally    {", "rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ SCHEDULER ,    CapacityScheduler . class . getCanonicalName (  )  )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "cs . getApplicationAttempt ( attempt . getAppAttemptId (  )  )  . getNewContainerId (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "MockAM   am    =    MockRM . launchAM ( app ,    rm ,    nm 1  )  ;", "Assert . assertTrue (  (  ( attempt . getMasterContainer (  )  . getId (  )  . getId (  )  )     !  =     1  )  )  ;", "Assert . assertFalse ( rm . getRMContext (  )  . getNMTokenSecretManager (  )  . isApplicationAttemptNMTokenPresent ( attempt . getAppAttemptId (  )  ,    nm 1  . getNodeId (  )  )  )  ;", "am . registerAppAttempt (  )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "int   NUM _ CONTAINERS    =     1  ;", "List < Container >    containers    =    new   ArrayList < Container >  (  )  ;", "List < NMToken >    expectedNMTokens    =    new   ArrayList < NMToken >  (  )  ;", "while    ( true )     {", "AllocateResponse   response    =    am . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     2  0  0  0  ,    NUM _ CONTAINERS ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "containers . addAll ( response . getAllocatedContainers (  )  )  ;", "expectedNMTokens . addAll ( response . getNMTokens (  )  )  ;", "if    (  ( containers . size (  )  )     =  =    NUM _ CONTAINERS )     {", "break ;", "}", "Thread . sleep (  2  0  0  )  ;", "System . out . println (  \" Waiting   for   container   to   be   allocated .  \"  )  ;", "}", "NodeId   nodeId    =    expectedNMTokens . get (  0  )  . getNodeId (  )  ;", "Assert . assertEquals ( nm 1  . getNodeId (  )  ,    nodeId )  ;", "}", "METHOD_END"], "methodName": ["testNMTokenSentForNormalContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRM"}, {"methodBody": ["METHOD_START", "{", "configuration    =    new   YarnConfiguration (  )  ;", "configuration . set ( RM _ SCHEDULER ,    CapacityScheduler . class . getCanonicalName (  )  )  ;", "fs    =    FileSystem . get ( configuration )  ;", "workingPath    =    new   Path ( new   File (  \" target \"  ,     (  ( this . getClass (  )  . getSimpleName (  )  )     +     \"  - remoteDir \"  )  )  . getAbsolutePath (  )  )  ;", "configuration . set ( FS _ BASED _ RM _ CONF _ STORE ,    workingPath . toString (  )  )  ;", "tmpDir    =    new   Path ( new   File (  \" target \"  ,     (  ( this . getClass (  )  . getSimpleName (  )  )     +     \"  - tmpDir \"  )  )  . getAbsolutePath (  )  )  ;", "fs . delete ( workingPath ,    true )  ;", "fs . delete ( tmpDir ,    true )  ;", "fs . mkdirs ( workingPath )  ;", "fs . mkdirs ( tmpDir )  ;", ". MockUnixGroupsMapping . resetGroups (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rm )     !  =    null )     {", "rmtop (  )  ;", "}", ". delete ( workingPath ,    true )  ;", ". delete ( tmpDir ,    true )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "uploadDefaultConfiguration (  )  ;", "try    {", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "String   aclStringBefore    =    rm . adminService . getAccessControlList (  )  . getAclString (  )  . trim (  )  ;", "YarnConfiguration   yarnConf    =    new   YarnConfiguration (  )  ;", "yarnConf . set ( YARN _ ADMIN _ ACL ,     \" world : anyone : rwcda \"  )  ;", "uploadConfiguration ( yarnConf ,     \" yarn - site . xml \"  )  ;", "rm . adminService . refreshAdminAcls ( RefreshAdminAclsRequest . newInstance (  )  )  ;", "String   aclStringAfter    =    rm . adminService . getAccessControlList (  )  . getAclString (  )  . trim (  )  ;", "Assert . assertTrue (  (  !  ( aclStringAfter . equals ( aclStringBefore )  )  )  )  ;", "Assert . assertEquals ( aclStringAfter ,     \" world : anyone : rwcda \"  )  ;", "}", "METHOD_END"], "methodName": ["testAdminAclsWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "try    {", "rm . a . refreshAdminAcls ( RefreshAdminAclsRequest . newInstance (  )  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Using   localConfigurationProvider .    Should   not   get   any   exception .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAdminAclsWithLocalConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "uploadDefaultConfiguration (  )  ;", "try    {", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( rm . getRMContext (  )  . getScheduler (  )  )  )  ;", "int   maxAppsBefore    =    cs . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csConf . set (  \" yarn . scheduler . capacity . maximum - applications \"  ,     \"  5  0  0  0  \"  )  ;", "uploadConfiguration ( csConf ,     \" capacity - scheduler . xml \"  )  ;", "rm . adminService . refreshQueues ( RefreshQueuesRequest . newInstance (  )  )  ;", "int   maxAppsAfter    =    cs . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "Assert . assertEquals ( maxAppsAfter ,     5  0  0  0  )  ;", "Assert . assertTrue (  ( maxAppsAfter    !  =    maxAppsBefore )  )  ;", "}", "METHOD_END"], "methodName": ["testAdminRefreshQueuesWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( rm . getRMContext (  )  . getScheduler (  )  )  )  ;", "int   maxAppsBefore    =    cs . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "try    {", "rm . a . refreshQueues ( RefreshQueuesRequest . newInstance (  )  )  ;", "Assert . assertEquals ( maxAppsBefore ,    cs . getConfiguration (  )  . getMaximumSystemApplications (  )  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Using   localConfigurationProvider .    Should   not   get   any   exception .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAdminRefreshQueuesWithLocalConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "StateChangeRequestInfo   requestInfo    =    new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER )  ;", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "configuration . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "configuration . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "configuration . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  \"  )  ;", "int   base    =     1  0  0  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( configuration )  )     {", "configuration . set ( HAUtil . addSuffix ( confKey ,     \" rm 1  \"  )  ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     2  0  )  )  )  ;", "configuration . set ( HAUtil . addSuffix ( confKey ,     \" rm 2  \"  )  ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     4  0  )  )  )  ;", "base    =    base    *     2  ;", "}", "Configuration   conf 1     =    new   Configuration ( configuration )  ;", "conf 1  . set ( RM _ HA _ ID ,     \" rm 1  \"  )  ;", "Configuration   conf 2     =    new   Configuration ( configuration )  ;", "conf 2  . set ( RM _ HA _ ID ,     \" rm 2  \"  )  ;", "uploadDefaultConfiguration (  )  ;", "MockRM   rm 1     =    null ;", "MockRM   rm 2     =    null ;", "try    {", "rm 1     =    new   MockRM ( conf 1  )  ;", "rm 1  . init ( conf 1  )  ;", "rm 1  . start (  )  ;", "Assert . assertTrue (  (  ( rm 1  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )  )  ;", "rm 2     =    new   MockRM ( conf 2  )  ;", "rm 2  . init ( conf 1  )  ;", "rm 2  . start (  )  ;", "Assert . assertTrue (  (  ( rm 2  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )  )  ;", "rm 1  . adminService . transitionToActive ( requestInfo )  ;", "Assert . assertTrue (  (  ( rm 1  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . ACTIVE )  )  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csConf . set (  \" yarn . scheduler . capacity . maximum - applications \"  ,     \"  5  0  0  0  \"  )  ;", "uploadConfiguration ( csConf ,     \" capacity - scheduler . xml \"  )  ;", "rm 1  . adminService . refreshQueues ( RefreshQueuesRequest . newInstance (  )  )  ;", "int   maxApps    =     (  ( CapacityScheduler )     ( rm 1  . getRMContext (  )  . getScheduler (  )  )  )  . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "Assert . assertEquals ( maxApps ,     5  0  0  0  )  ;", "int   maxAppsBeforeFailOver    =     (  ( CapacityScheduler )     ( rm 2  . getRMContext (  )  . getScheduler (  )  )  )  . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "Assert . assertEquals ( maxAppsBeforeFailOver ,     1  0  0  0  0  )  ;", "rm 1  . adminService . transitionToStandby ( requestInfo )  ;", "rm 2  . adminService . transitionToActive ( requestInfo )  ;", "Assert . assertTrue (  (  ( rm 1  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  )  )  ;", "Assert . assertTrue (  (  ( rm 2  . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . ACTIVE )  )  )  ;", "int   maxAppsAfter    =     (  ( CapacityScheduler )     ( rm 2  . getRMContext (  )  . getScheduler (  )  )  )  . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "Assert . assertEquals ( maxAppsAfter ,     5  0  0  0  )  ;", "}    finally    {", "if    ( rm 1     !  =    null )     {", "rm 1  . stop (  )  ;", "}", "if    ( rm 2     !  =    null )     {", "rm 2  . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testRMHAWithFileSystemBasedConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "final   File   excludeHostsFile    =    new   File ( tmpDir . toString (  )  ,     \" excludeHosts \"  )  ;", "if    ( excludeHostsFile . exists (  )  )     {", "excludeHostsFile . delete (  )  ;", "}", "if    (  !  ( excludeHostsFile . createNewFile (  )  )  )     {", "Assert . fail (  (  \" Can   not   create    \"     +     \" excludeHosts \"  )  )  ;", "}", "PrintWriter   fileWriter    =    new   PrintWriter ( excludeHostsFile )  ;", "fileWriter . write (  \"  0  .  0  .  0  .  0  :  1  2  3  \"  )  ;", "fileWriter . close (  )  ;", "uploadToRemoteFileSystem ( new   Path ( excludeHostsFile . getAbsolutePath (  )  )  )  ;", "YarnConfiguration   yarnConf    =    new   YarnConfiguration (  )  ;", "yarnConf . set ( YARN _ ADMIN _ ACL ,     \" world : anyone : rwcda \"  )  ;", "yarnConf . set ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,     (  ( this . workingPath )     +     \"  / excludeHosts \"  )  )  ;", "uploadConfiguration ( yarnConf ,     \" yarn - site . xml \"  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csConf . set (  \" yarn . scheduler . capacity . maximum - applications \"  ,     \"  5  0  0  0  \"  )  ;", "uploadConfiguration ( csConf ,     \" capacity - scheduler . xml \"  )  ;", "String   aclsString    =     \" alice , bob   users , wheel \"  ;", "Configuration   newConf    =    new   Configuration (  )  ;", "newConf . set (  \" security . applicationclient . protocol . acl \"  ,    aclsString )  ;", "uploadConfiguration ( newConf ,     \" hadoop - policy . xml \"  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    true )  ;", "conf . set (  \" hadoop . proxyuser . test . groups \"  ,     \" test _ groups \"  )  ;", "conf . set (  \" hadoop . proxyuser . test . hosts \"  ,     \" test _ hosts \"  )  ;", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    TestRMAdminService . MockUnixGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "uploadConfiguration ( conf ,     \" core - site . xml \"  )  ;", "TestRMAdminService . MockUnixGroupsMapping . updateGroups (  )  ;", "ResourceManager   resourceManager    =    null ;", "try    {", "try    {", "resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( configuration )  ;", "resourceManager . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "Set < String >    excludeHosts    =    resourceManager . getRMContext (  )  . getNodesListManager (  )  . getHostsReader (  )  . getExcludedHosts (  )  ;", "Assert . assertTrue (  (  ( excludeHosts . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue ( excludeHosts . contains (  \"  0  .  0  .  0  .  0  :  1  2  3  \"  )  )  ;", "String   aclStringAfter    =    resourceManager . adminService . getAccessControlList (  )  . getAclString (  )  . trim (  )  ;", "Assert . assertEquals ( aclStringAfter ,     \" world : anyone : rwcda \"  )  ;", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( resourceManager . getRMContext (  )  . getScheduler (  )  )  )  ;", "int   maxAppsAfter    =    cs . getConfiguration (  )  . getMaximumSystemApplications (  )  ;", "Assert . assertEquals ( maxAppsAfter ,     5  0  0  0  )  ;", "ServiceAuthorizationManager   adminServiceServiceManager    =    resourceManager . adminService . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( adminServiceServiceManager ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "ServiceAuthorizationManager   clientRMServiceServiceManager    =    resourceManager . getRMContext (  )  . getClientRMService (  )  . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( clientRMServiceServiceManager ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "ServiceAuthorizationManager   appMasterService    =    resourceManager . getRMContext (  )  . getApplicationMasterService (  )  . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( appMasterService ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "ServiceAuthorizationManager   RTService    =    resourceManager . getRMContext (  )  . getResourceTrackerService (  )  . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( RTService ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "Assert . assertTrue (  (  ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyGroups (  )  . get (  \" hadoop . proxyuser . test . groups \"  )  . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyGroups (  )  . get (  \" hadoop . proxyuser . test . groups \"  )  . contains (  \" test _ groups \"  )  )  ;", "Assert . assertTrue (  (  ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyHosts (  )  . get (  \" hadoop . proxyuser . test . hosts \"  )  . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyHosts (  )  . get (  \" hadoop . proxyuser . test . hosts \"  )  . contains (  \" test _ hosts \"  )  )  ;", "List < String >    groupAfter    =    Groups . getUserToGroupsMappingService ( configuration )  . getGroups ( UserGroupInformation . getCurrentUser (  )  . getUserName (  )  )  ;", "Assert . assertTrue (  (  (  (  ( groupAfter . contains (  \" test _ group _ D \"  )  )     &  &     ( groupAfter . contains (  \" test _ group _ E \"  )  )  )     &  &     ( groupAfter . contains (  \" test _ group _ F \"  )  )  )     &  &     (  ( groupAfter . size (  )  )     =  =     3  )  )  )  ;", "}    finally    {", "if    ( resourceManager    !  =    null )     {", "resourceManager . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testRMInitialsWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "try    {", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRMStartsWithoutConfigurationFilesProvided"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "uploadDefaultConfiguration (  )  ;", "try    {", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "final   File   excludeHostsFile    =    new   File ( tmpDir . toString (  )  ,     \" excludeHosts \"  )  ;", "if    ( excludeHostsFile . exists (  )  )     {", "excludeHostsFile . delete (  )  ;", "}", "if    (  !  ( excludeHostsFile . createNewFile (  )  )  )     {", "Assert . fail (  (  \" Can   not   create    \"     +     \" excludeHosts \"  )  )  ;", "}", "PrintWriter   fileWriter    =    new   PrintWriter ( excludeHostsFile )  ;", "fileWriter . write (  \"  0  .  0  .  0  .  0  :  1  2  3  \"  )  ;", "fileWriter . close (  )  ;", "uploadToRemoteFileSystem ( new   Path ( excludeHostsFile . getAbsolutePath (  )  )  )  ;", "Configuration   yarnConf    =    new   YarnConfiguration (  )  ;", "yarnConf . set ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,     (  ( this . workingPath )     +     \"  / excludeHosts \"  )  )  ;", "uploadConfiguration ( yarnConf ,    YARN _ SITE _ CONFIGURATION _ FILE )  ;", "rm . adminService . refreshNodes ( RefreshNodesRequest . newInstance (  )  )  ;", "Set < String >    excludeHosts    =    rm . getNodesListManager (  )  . getHostsReader (  )  . getExcludedHosts (  )  ;", "Assert . assertTrue (  (  ( excludeHosts . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue ( excludeHosts . contains (  \"  0  .  0  .  0  .  0  :  1  2  3  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRefreshNodesWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "try    {", "rm . a . refreshNodes ( RefreshNodesRequest . newInstance (  )  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Using   localConfigurationProvider .    Should   not   get   any   exception .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRefreshNodesWithLocalConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "uploadDefaultConfiguration (  )  ;", "try    {", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "Configuration   coreConf    =    new   Configuration ( false )  ;", "coreConf . set (  \" hadoop . proxyuser . test . groups \"  ,     \" test _ groups \"  )  ;", "coreConf . set (  \" hadoop . proxyuser . test . hosts \"  ,     \" test _ hosts \"  )  ;", "uploadConfiguration ( coreConf ,     \" core - site . xml \"  )  ;", "rm . adminService . refreshSuperUserGroupsConfiguration ( RefreshSuperUserGroupsConfigurationRequest . newInstance (  )  )  ;", "Assert . assertTrue (  (  ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyGroups (  )  . get (  \" hadoop . proxyuser . test . groups \"  )  . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyGroups (  )  . get (  \" hadoop . proxyuser . test . groups \"  )  . contains (  \" test _ groups \"  )  )  ;", "Assert . assertTrue (  (  ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyHosts (  )  . get (  \" hadoop . proxyuser . test . hosts \"  )  . size (  )  )     =  =     1  )  )  ;", "Assert . assertTrue ( ProxyUsers . getDefaultImpersonationProvider (  )  . getProxyHosts (  )  . get (  \" hadoop . proxyuser . test . hosts \"  )  . contains (  \" test _ hosts \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testRefreshSuperUserGroupsWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "try    {", "rm . a . refreshSuperUserGroupsConfiguration ( RefreshSuperUserGroupsConfigurationRequest . newInstance (  )  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Using   localConfigurationProvider .    Should   not   get   any   exception .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRefreshSuperUserGroupsWithLocalConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "String [  ]    defaultTestUserGroups    =    new   String [  ]  {     \" dummy _ group 1  \"  ,     \" dummy _ group 2  \"     }  ;", "UserGroupInformation   ugi    =    UserGroupInformation . createUserForTesting (  \" dummyUser \"  ,    defaultTestUserGroups )  ;", "String   user    =    ugi . getUserName (  )  ;", "List < String >    groupWithInit    =    new   ArrayList < String >  (  2  )  ;", "for    ( int   i    =     0  ;    i    <     ( ugi . getGroupNames (  )  . length )  ;    i +  +  )     {", "groupWithInit . add ( ugi . getGroupNames (  )  [ i ]  )  ;", "}", "uploadDefaultConfiguration (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    TestRMAdminService . MockUnixGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "uploadConfiguration ( conf ,     \" core - site . xml \"  )  ;", "try    {", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "List < String >    groupBefore    =    new   ArrayList < String >  ( Groups . getUserToGroupsMappingService ( configuration )  . getGroups ( user )  )  ;", "Assert . assertTrue (  (  (  (  ( groupBefore . contains (  \" test _ group _ A \"  )  )     &  &     ( groupBefore . contains (  \" test _ group _ B \"  )  )  )     &  &     ( groupBefore . contains (  \" test _ group _ C \"  )  )  )     &  &     (  ( groupBefore . size (  )  )     =  =     3  )  )  )  ;", "Assert . assertTrue (  (  ( groupWithInit . size (  )  )     !  =     ( groupBefore . size (  )  )  )  )  ;", "Assert . assertFalse (  (  (  ( groupWithInit . contains (  \" test _ group _ A \"  )  )     |  |     ( groupWithInit . contains (  \" test _ group _ B \"  )  )  )     |  |     ( groupWithInit . contains (  \" test _ group _ C \"  )  )  )  )  ;", "TestRMAdminService . MockUnixGroupsMapping . updateGroups (  )  ;", "rm . adminService . refreshUserToGroupsMappings ( RefreshUserToGroupsMappingsRequest . newInstance (  )  )  ;", "List < String >    groupAfter    =    Groups . getUserToGroupsMappingService ( configuration )  . getGroups ( user )  ;", "Assert . assertTrue (  (  (  (  ( groupAfter . contains (  \" test _ group _ D \"  )  )     &  &     ( groupAfter . contains (  \" test _ group _ E \"  )  )  )     &  &     ( groupAfter . contains (  \" test _ group _ F \"  )  )  )     &  &     (  ( groupAfter . size (  )  )     =  =     3  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRefreshUserToGroupsMappingsWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "rm    =    new   MockRM ( configuration )  ;", "rm . init ( configuration )  ;", "rm . start (  )  ;", "try    {", "rm . a . refreshUserToGroupsMappings ( RefreshUserToGroupsMappingsRequest . newInstance (  )  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Using   localConfigurationProvider .    Should   not   get   any   exception .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRefreshUserToGroupsMappingsWithLocalConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . setBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    true )  ;", "configuration . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "ResourceManager   resourceManager    =    null ;", "try    {", "uploadDefaultConfiguration (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    true )  ;", "uploadConfiguration ( conf ,     \" core - site . xml \"  )  ;", "try    {", "resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( configuration )  ;", "resourceManager . start (  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Should   not   get   any   exceptions \"  )  ;", "}", "String   aclsString    =     \" alice , bob   users , wheel \"  ;", "Configuration   newConf    =    new   Configuration (  )  ;", "newConf . set (  \" security . applicationclient . protocol . acl \"  ,    aclsString )  ;", "uploadConfiguration ( newConf ,     \" hadoop - policy . xml \"  )  ;", "resourceManager . adminService . refreshServiceAcls ( RefreshServiceAclsRequest . newInstance (  )  )  ;", "ServiceAuthorizationManager   adminServiceServiceManager    =    resourceManager . adminService . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( adminServiceServiceManager ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "ServiceAuthorizationManager   clientRMServiceServiceManager    =    resourceManager . getRMContext (  )  . getClientRMService (  )  . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( clientRMServiceServiceManager ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "ServiceAuthorizationManager   appMasterService    =    resourceManager . getRMContext (  )  . getApplicationMasterService (  )  . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( appMasterService ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "ServiceAuthorizationManager   RTService    =    resourceManager . getRMContext (  )  . getResourceTrackerService (  )  . getServer (  )  . getServiceAuthorizationManager (  )  ;", "verifyServiceACLsRefresh ( RTService ,    ApplicationClientProtocolPB . class ,    aclsString )  ;", "}    finally    {", "if    ( resourceManager    !  =    null )     {", "resourceManager . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testServiceAclsRefreshWithFileSystemBasedConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "configuration . setBoolean ( HADOOP _ SECURITY _ AUTHORIZATION ,    true )  ;", "ResourceManager   resourceManager    =    null ;", "try    {", "resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( configuration )  ;", "resourceManager . start (  )  ;", "resourceManager . a . refreshServiceAcls ( RefreshServiceAclsRequest . newInstance (  )  )  ;", "}    catch    ( Exception   ex )     {", "fail (  \" Using   localConfigurationProvider .    Should   not   get   any   exception .  \"  )  ;", "}    finally    {", "if    ( resourceManager    !  =    null )     {", "resourceManager . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testServiceAclsRefreshWithLocalConfigurationProvider"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "String   csConfFile    =    writeConfigurationXML ( conf ,    confFileName )  ;", "uploadToRemoteFileSystem ( new   Path ( csConfFile )  )  ;", "}", "METHOD_END"], "methodName": ["uploadConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "uploadConfiguration ( conf ,     \" core - site . xml \"  )  ;", "YarnConfiguration   yarnConf    =    new   YarnConfiguration (  )  ;", "yarnConf . set ( RM _ CONFIGURATION _ PROVIDER _ CLASS ,     \" FileSystemBasedConfigurationProvider \"  )  ;", "uploadConfiguration ( yarnConf ,     \" yarn - site . xml \"  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "uploadConfiguration ( csConf ,     \" capacity - scheduler . xml \"  )  ;", "Configuration   hadoopPolicyConf    =    new   Configuration ( false )  ;", "hadoopPolicyConf . addResource ( HADOOP _ POLICY _ CONFIGURATION _ FILE )  ;", "uploadConfiguration ( hadoopPolicyConf ,     \" hadoop - policy . xml \"  )  ;", "}", "METHOD_END"], "methodName": ["uploadDefaultConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "fs . copyFromLocalFile ( filePath ,    workingPath )  ;", "}", "METHOD_END"], "methodName": ["uploadToRemoteFileSystem"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "for    ( Class <  ?  >    protocolClass    :    manager . getProtocolsWithAcls (  )  )     {", "AccessControlList   accessList    =    manager . getProtocolsAcls ( protocolClass )  ;", "if    ( protocolClass    =  =    protocol )     {", "Assert . assertEquals ( accessList . getAclString (  )  ,    aclString )  ;", "} else    {", "Assert . assertEquals ( accessList . getAclString (  )  ,     \"  *  \"  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["verifyServiceACLsRefresh"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "DataOutputStream   output    =    null ;", "try    {", "final   File   confFile    =    new   File ( tmpDir . toString (  )  ,    confXMLName )  ;", "if    ( confFile . exists (  )  )     {", "confFile . delete (  )  ;", "}", "if    (  !  ( confFile . createNewFile (  )  )  )     {", "Ast . fail (  (  \" Can   not   create    \"     +    confXMLName )  )  ;", "}", "output    =    new   DataOutputStream ( new   FileOutputStream ( confFile )  )  ;", "conf . writeXml ( output )  ;", "return   confFile . getAbsolutePath (  )  ;", "}    finally    {", "if    ( output    !  =    null )     {", "output . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["writeConfigurationXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAdminService"}, {"methodBody": ["METHOD_START", "{", "when ( TestRMAuditLogger . APPID . toString (  )  )  . thenReturn (  \" app _  1  \"  )  ;", "when ( TestRMAuditLogger . ATTEMPTID . toString (  )  )  . thenReturn (  \" app _ attempt _  1  \"  )  ;", "when ( TestRMAuditLogger . CONTAINERID . toString (  )  )  . thenReturn (  \" container _  1  \"  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "testFailureLogFormatHelper ( checkIP ,    null ,    null ,    null )  ;", "testFailureLogFormatHelper ( checkIP ,     . APPID ,    null ,    null )  ;", "testFailureLogFormatHelper ( checkIP ,    null ,    null ,     . CONTAINERID )  ;", "testFailureLogFormatHelper ( checkIP ,    null ,     . ATTEMPTID ,    null )  ;", "testFailureLogFormatHelper ( checkIP ,     . APPID ,     . ATTEMPTID ,    null )  ;", "testFailureLogFormatHelper ( checkIP ,     . APPID ,    null ,     . CONTAINERID )  ;", "testFailureLogFormatHelper ( checkIP ,    null ,     . ATTEMPTID ,     . CONTAINERID )  ;", "testFailureLogFormatHelper ( checkIP ,     . APPID ,     . ATTEMPTID ,     . CONTAINERID )  ;", "}", "METHOD_END"], "methodName": ["testFailureLogFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "String   fLog    =    RMAuditLogger . createFailureLog ( TestRMAuditLogger . USER ,    TestRMAuditLogger . OPERATION ,    TestRMAuditLogger . PERM ,    TestRMAuditLogger . TARGET ,    TestRMAuditLogger . DESC ,    appId ,    attemptId ,    containerId )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "expLog . append (  \" USER = test \\ t \"  )  ;", "if    ( checkIP )     {", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "expLog . append (  (  (  (  ( RMAuditLogger . Keys . IP . name (  )  )     +     \"  =  \"  )     +     ( ip . getHostAddress (  )  )  )     +     \"  \\ t \"  )  )  ;", "}", "expLog . append (  \" OPERATION = oper \\ tTARGET = tgt \\ tRESULT = FAILURE \\ t \"  )  ;", "expLog . append (  \" DESCRIPTION = description   of   an   audit   log \"  )  ;", "expLog . append (  \"  \\ tPERMISSIONS = admin   group \"  )  ;", "if    ( appId    !  =    null )     {", "expLog . append (  \"  \\ tAPPID = app _  1  \"  )  ;", "}", "if    ( attemptId    !  =    null )     {", "expLog . append (  \"  \\ tAPPATTEMPTID = app _ attempt _  1  \"  )  ;", "}", "if    ( containerId    !  =    null )     {", "expLog . append (  \"  \\ tCONTAINERID = container _  1  \"  )  ;", "}", "assertEquals ( expLog . toString (  )  ,    fLog )  ;", "}", "METHOD_END"], "methodName": ["testFailureLogFormatHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   actLog    =    new   StringBuilder (  )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "RMAuditLogger . start ( RMAuditLogger . Keys . USER ,     . USER ,    actLog )  ;", "expLog . append (  \" USER = test \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "RMAuditLogger . add ( RMAuditLogger . Keys . OPERATION ,     . OPERATION ,    actLog )  ;", "expLog . append (  \"  \\ tOPERATION = oper \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "RMAuditLogger . add ( RMAuditLogger . Keys . APPID ,     (  ( String )     ( null )  )  ,    actLog )  ;", "expLog . append (  \"  \\ tAPPID = null \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "RMAuditLogger . add ( RMAuditLogger . Keys . TARGET ,     . TARGET ,    actLog )  ;", "expLog . append (  \"  \\ tTARGET = tgt \"  )  ;", "assertEquals ( expLog . toString (  )  ,    actLog . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testKeyValLogFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Server   server    =    new   RPC . Builder ( conf )  . setProtocol ( TestProtocol . class )  . setInstance ( new    . MyTestRPCServer (  )  )  . setBindAddress (  \"  0  .  0  .  0  .  0  \"  )  . setPort (  0  )  . setNumHandlers (  5  )  . setVerbose ( true )  . build (  )  ;", "server . start (  )  ;", "InetSocketAddress   addr    =    NetUtils . getConnectAddress ( server )  ;", "TestProtocol   proxy    =     (  ( TestProtocol )     ( RPC . getProxy ( TestProtocol . class ,    versionID ,    addr ,    conf )  )  )  ;", "proxy . ping (  )  ;", "server . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMAuditLoggerWithIP"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "testSuccessLogFormat ( false )  ;", "testFailureLogFormat ( false )  ;", "}", "METHOD_END"], "methodName": ["testRMAuditLoggerWithoutIP"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "testSuccessLogFormatHelper ( checkIP ,    null ,    null ,    null )  ;", "testSuccessLogFormatHelper ( checkIP ,     . APPID ,    null ,    null )  ;", "testSuccessLogFormatHelper ( checkIP ,    null ,    null ,     . CONTAINERID )  ;", "testSuccessLogFormatHelper ( checkIP ,    null ,     . ATTEMPTID ,    null )  ;", "testSuccessLogFormatHelper ( checkIP ,     . APPID ,     . ATTEMPTID ,    null )  ;", "testSuccessLogFormatHelper ( checkIP ,     . APPID ,    null ,     . CONTAINERID )  ;", "testSuccessLogFormatHelper ( checkIP ,    null ,     . ATTEMPTID ,     . CONTAINERID )  ;", "testSuccessLogFormatHelper ( checkIP ,     . APPID ,     . ATTEMPTID ,     . CONTAINERID )  ;", "testSuccessLogNulls ( checkIP )  ;", "}", "METHOD_END"], "methodName": ["testSuccessLogFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "String   sLog    =    RMAuditLogger . createSuccessLog ( TestRMAuditLogger . USER ,    TestRMAuditLogger . OPERATION ,    TestRMAuditLogger . TARGET ,    appId ,    attemptId ,    containerId )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "expLog . append (  \" USER = test \\ t \"  )  ;", "if    ( checkIP )     {", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "expLog . append (  (  (  (  ( RMAuditLogger . Keys . IP . name (  )  )     +     \"  =  \"  )     +     ( ip . getHostAddress (  )  )  )     +     \"  \\ t \"  )  )  ;", "}", "expLog . append (  \" OPERATION = oper \\ tTARGET = tgt \\ tRESULT = SUCCESS \"  )  ;", "if    ( appId    !  =    null )     {", "expLog . append (  \"  \\ tAPPID = app _  1  \"  )  ;", "}", "if    ( attemptId    !  =    null )     {", "expLog . append (  \"  \\ tAPPATTEMPTID = app _ attempt _  1  \"  )  ;", "}", "if    ( containerId    !  =    null )     {", "expLog . append (  \"  \\ tCONTAINERID = container _  1  \"  )  ;", "}", "assertEquals ( expLog . toString (  )  ,    sLog )  ;", "}", "METHOD_END"], "methodName": ["testSuccessLogFormatHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "String   sLog    =    RMAuditLogger . createSuccessLog ( null ,    null ,    null ,    null ,    null ,    null )  ;", "StringBuilder   expLog    =    new   StringBuilder (  )  ;", "expLog . append (  \" USER = null \\ t \"  )  ;", "if    ( checkIP )     {", "InetAddress   ip    =    Server . getRemoteIp (  )  ;", "expLog . append (  (  (  (  ( RMAuditLogger . Keys . IP . name (  )  )     +     \"  =  \"  )     +     ( ip . getHostAddress (  )  )  )     +     \"  \\ t \"  )  )  ;", "}", "expLog . append (  \" OPERATION = null \\ tTARGET = null \\ tRESULT = SUCCESS \"  )  ;", "assertEquals ( expLog . toString (  )  ,    sLog )  ;", "}", "METHOD_END"], "methodName": ["testSuccessLogNulls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger"}, {"methodBody": ["METHOD_START", "{", "conf . set ( HAUtil . addSuffix ( prefix ,    rmId )  ,    value )  ;", "}", "METHOD_END"], "methodName": ["setConfForRM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector"}, {"methodBody": ["METHOD_START", "{", "setConfForRM ( rmId ,    RM _ ADDRESS ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     ( YarnConfiguration . DEFAULT _ RM _ PORT )  )  )  )  ;", "setConfForRM ( rmId ,    RM _ SCHEDULER _ ADDRESS ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     ( YarnConfiguration . DEFAULT _ RM _ SCHEDULER _ PORT )  )  )  )  ;", "setConfForRM ( rmId ,    RM _ ADMIN _ ADDRESS ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     ( YarnConfiguration . DEFAULT _ RM _ ADMIN _ PORT )  )  )  )  ;", "setConfForRM ( rmId ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     ( YarnConfiguration . DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  )  )  )  ;", "setConfForRM ( rmId ,    RM _ WEBAPP _ ADDRESS ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     ( YarnConfiguration . DEFAULT _ RM _ WEBAPP _ PORT )  )  )  )  ;", "setConfForRM ( rmId ,    RM _ WEBAPP _ HTTPS _ ADDRESS ,     (  \"  0  .  0  .  0  .  0  :  \"     +     ( base    +     ( YarnConfiguration . DEFAULT _ RM _ WEBAPP _ HTTPS _ PORT )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setRpcAddressForRM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    true )  ;", "conf . setBoolean ( AUTO _ FAILOVER _ EMBEDDED ,    true )  ;", "conf . set ( RM _ CLUSTER _ ID ,     \" yarn - test - cluster \"  )  ;", "conf . set ( RM _ ZK _ ADDRESS ,    hostPort )  ;", "conf . setInt ( RM _ ZK _ TIMEOUT _ MS ,     2  0  0  0  )  ;", "conf . set ( RM _ HA _ IDS ,     (  (  (  . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 2  _ NODE _ ID )  )  )  ;", "conf . set ( RM _ HA _ ID ,     . RM 1  _ NODE _ ID )  ;", "setRpcAddressForRM (  . RM 1  _ NODE _ ID ,     . RM 1  _ PORT _ BASE )  ;", "setRpcAddressForRM (  . RM 2  _ NODE _ ID ,     . RM 2  _ PORT _ BASE )  ;", "conf . setLong ( CLIENT _ FAILOVER _ SLEEPTIME _ BASE _ MS ,     1  0  0 L )  ;", "callbackCalled    =    new   AtomicBoolean ( false )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   TestRMEmbeddedElector . MockRMWithElector ( conf ,     1  0  0  0  )  ;", "rm . start (  )  ;", "TestRMEmbeddedElector . LOG . info (  \" Waiting   for   callback \"  )  ;", "while    (  !  ( callbackCalled . get (  )  )  )  ;", "TestRMEmbeddedElector . LOG . info (  \" Stopping   RM \"  )  ;", "rm . stop (  )  ;", "TestRMEmbeddedElector . LOG . info (  \" Stopped   RM \"  )  ;", "}", "METHOD_END"], "methodName": ["testDeadlockShutdownBecomeActive"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMEmbeddedElector"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  (  \" Incorrect   value   for   metric    \"     +    metricName )  ,    expected ,    actual )  ;", "}", "METHOD_END"], "methodName": ["assertMetric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( TestRMHA . STATE _ ERR ,    ACTIVE ,    rm . adminService . getServiceStatus (  )  . getState (  )  )  ;", "assertTrue (  \" Active   RM   services   aren ' t   started \"  ,    rm . areActiveServicesRunning (  )  )  ;", "assertTrue (  \" RM   is   not   ready   to   become   active \"  ,    rm . adminService . getServiceStatus (  )  . isReadyToBecomeActive (  )  )  ;", "try    {", "rm . getNewAppId (  )  ;", "rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  0  \"  ,     2  0  4  8  )  ;", "app    =    rm . submitApp (  1  0  2  4  )  ;", "attempt    =    app . getCurrentAppAttempt (  )  ;", "rm . waitForState ( attempt . getAppAttemptId (  )  ,    RMAppAttemptState . SCHEDULED )  ;", "}    catch    ( Exception   e )     {", "fail (  \" Unable   to   perform   Active   RM   functions \"  )  ;", "LOG . error (  \" ActiveRM   check   failed \"  ,    e )  ;", "}", "checkActiveRMWebServices (  )  ;", "}", "METHOD_END"], "methodName": ["checkActiveRMFunctionality"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "Client   webServiceClient    =    Client . create ( new   DefaultClientConfig (  )  )  ;", "InetSocketAddress   rmWebappAddr    =    NetUtils . getConnectAddress ( rm . getWebapp (  )  . getListenerAddress (  )  )  ;", "String   webappURL    =     (  (  \" http :  /  /  \"     +     ( rmWebappAddr . getHostName (  )  )  )     +     \"  :  \"  )     +     ( rmWebappAddr . getPort (  )  )  ;", "WebResource   webResource    =    webServiceClient ( webappURL )  ;", "String   path    =    app . getApplicationId (  )  . toString (  )  ;", "ClientResponse   response    =    webResource . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path ( path )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   appJson    =    json . getJSONObject (  \" app \"  )  ;", "assertEquals (  \" ACCEPTED \"  ,    appJson . getString (  \" state \"  )  )  ;", "}", "METHOD_END"], "methodName": ["checkActiveRMWebServices"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "try    {", "rm . adminService . monitorHealth (  )  ;", "}    catch    ( HealthCheckFailedException   e )     {", "fail (  (  \" The   RM   is   in   bad   health :    it   is   Active ,    but   the   active   ices    \"     +     \" are   not   running \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkMonitorHealth"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( TestRMHA . STATE _ ERR ,    STANDBY ,    rm . adminService . getServiceStatus (  )  . getState (  )  )  ;", "assertFalse (  \" Active   RM   services   are   started \"  ,    rm . areActiveServicesRunning (  )  )  ;", "assertTrue (  \" RM   is   not   ready   to   become   active \"  ,    rm . adminService . getServiceStatus (  )  . isReadyToBecomeActive (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkStandbyRMFunctionality"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "if    ( includeBindHost )     {", "configuration . set ( RM _ BIND _ HOST ,     \"  9  .  9  .  9  .  9  \"  )  ;", "}", "configuration . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     . RM 1  _ NODE _ ID )  ,     \"  1  .  1  .  1  .  1  \"  )  ;", "configuration . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     . RM 2  _ NODE _ ID )  ,     \"  0  .  0  .  0  .  0  \"  )  ;", "configuration . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     . RM 3  _ NODE _ ID )  ,     \"  2  .  2  .  2  .  2  \"  )  ;", "try    {", "Configuration   conf    =    new   YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "assertEquals (  (  \" RPC   address   not   set   for    \"     +    confKey )  ,     . RM 1  _ ADDRESS ,    conf . get ( HAUtil . addSuffix ( confKey ,     . RM 1  _ NODE _ ID )  )  )  ;", "assertEquals (  (  \" RPC   address   not   set   for    \"     +    confKey )  ,     . RM 2  _ ADDRESS ,    conf . get ( HAUtil . addSuffix ( confKey ,     . RM 2  _ NODE _ ID )  )  )  ;", "assertEquals (  (  \" RPC   address   not   set   for    \"     +    confKey )  ,     . RM 3  _ ADDRESS ,    conf . get ( HAUtil . addSuffix ( confKey ,     . RM 3  _ NODE _ ID )  )  )  ;", "if    ( includeBindHost )     {", "assertEquals (  \" Web   address   misconfigured   WITH   bind - host \"  ,    rm . webAppAddress . substring (  0  ,     7  )  ,     \"  9  .  9  .  9  .  9  \"  )  ;", "} else    {", "assertEquals (  \" Web   address   misconfigured   WITHOUT   bind - host \"  ,    rm . webAppAddress . substring (  0  ,     7  )  ,     \"  0  .  0  .  0  .  0  \"  )  ;", "}", "}", "}    catch    ( YarnRuntimeException   e )     {", "fail (  \" Should   not   throw   any   exceptions .  \"  )  ;", "}", "configuration . clear (  )  ;", "configuration . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "configuration . set ( RM _ HA _ IDS ,     (  (  (  . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 2  _ NODE _ ID )  )  )  ;", "configuration . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     . RM 1  _ NODE _ ID )  ,     \"  1  .  1  .  1  .  1  \"  )  ;", "configuration . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     . RM 2  _ NODE _ ID )  ,     \"  0  .  0  .  0  .  0  \"  )  ;", "try    {", "Configuration   conf    =    new   YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "assertEquals (  (  \" RPC   address   not   set   for    \"     +     ( YarnConfiguration . RM _ ADDRESS )  )  ,     \"  1  .  1  .  1  .  1  :  8  0  3  2  \"  ,    conf . get ( HAUtil . addSuffix ( RM _ ADDRESS ,     . RM 1  _ NODE _ ID )  )  )  ;", "assertEquals (  (  \" RPC   address   not   set   for    \"     +     ( YarnConfiguration . RM _ ADDRESS )  )  ,     \"  0  .  0  .  0  .  0  :  8  0  3  2  \"  ,    conf . get ( HAUtil . addSuffix ( RM _ ADDRESS ,     . RM 2  _ NODE _ ID )  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "fail (  \" Should   not   throw   any   exceptions .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["innerTestHAWithRMHostName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "configuration . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "configuration . set ( RM _ HA _ IDS ,     (  (  (  . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 2  _ NODE _ ID )  )  )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( configuration )  )     {", "configuration . set ( HAUtil . addSuffix ( confKey ,     . RM 1  _ NODE _ ID )  ,     . RM 1  _ ADDRESS )  ;", "configuration . set ( HAUtil . addSuffix ( confKey ,     . RM 2  _ NODE _ ID )  ,     . RM 2  _ ADDRESS )  ;", "configuration . set ( HAUtil . addSuffix ( confKey ,     . RM 3  _ NODE _ ID )  ,     . RM 3  _ ADDRESS )  ;", "}", "configuration . setBoolean ( MockRM . ENABLE _ WEBAPP ,    true )  ;", "configuration . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "ClusterMetrics . destroy (  )  ;", "QueueMetrics . clearQueueMetrics (  )  ;", "DefaultMetricsSystem . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "configuration . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "Configuration   conf    =    new   conf . YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "StateChangeRequestInfo   requestInfo    =    new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER )  ;", "assertEquals ( TestRMHA . STATE _ ERR ,    INITIALIZING ,    rm . adminService . getServiceStatus (  )  . getState (  )  )  ;", "assertFalse (  \" RM   is   ready   to   become   active   before   being   started \"  ,    rm . adminService . getServiceStatus (  )  . isReadyToBecomeActive (  )  )  ;", "checkMonitorHealth (  )  ;", "rm . start (  )  ;", "checkMonitorHealth (  )  ;", "checkStandbyRMFunctionality (  )  ;", "verifyClusterMetrics (  0  ,     0  ,     0  ,     0  ,     0  ,     0  )  ;", "rm . adminService . transitionToStandby ( requestInfo )  ;", "checkMonitorHealth (  )  ;", "checkStandbyRMFunctionality (  )  ;", "verifyClusterMetrics (  0  ,     0  ,     0  ,     0  ,     0  ,     0  )  ;", "rm . adminService . transitionToActive ( requestInfo )  ;", "checkMonitorHealth (  )  ;", "checkActiveRMFunctionality (  )  ;", "verifyClusterMetrics (  1  ,     1  ,     1  ,     1  ,     2  0  4  8  ,     1  )  ;", "rm . adminService . transitionToActive ( requestInfo )  ;", "checkMonitorHealth (  )  ;", "checkActiveRMFunctionality (  )  ;", "verifyClusterMetrics (  1  ,     2  ,     2  ,     2  ,     2  0  4  8  ,     2  )  ;", "rm . adminService . transitionToStandby ( requestInfo )  ;", "checkMonitorHealth (  )  ;", "checkStandbyRMFunctionality (  )  ;", "verifyClusterMetrics (  0  ,     0  ,     0  ,     0  ,     0  ,     0  )  ;", "rm . adminService . transitionToActive ( requestInfo )  ;", "checkMonitorHealth (  )  ;", "checkActiveRMFunctionality (  )  ;", "verifyClusterMetrics (  1  ,     1  ,     1  ,     1  ,     2  0  4  8  ,     1  )  ;", "rm . stop (  )  ;", "assertEquals ( TestRMHA . STATE _ ERR ,    STOPPING ,    rm . adminService . getServiceStatus (  )  . getState (  )  )  ;", "assertFalse (  \" RM   is   ready   to   become   active   even   after   it   is   stopped \"  ,    rm . adminService . getServiceStatus (  )  . isReadyToBecomeActive (  )  )  ;", "assertFalse (  \" Active   RM   services   are   started \"  ,    rm . areActiveServicesRunning (  )  )  ;", "checkMonitorHealth (  )  ;", "}", "METHOD_END"], "methodName": ["testFailoverAndTransitions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "assertEquals ( conf . get ( RM _ HA _ ID )  ,    TestRMHA . RM 2  _ NODE _ ID )  ;", "configuration . set ( RM _ HA _ ID ,    TestRMHA . RM 1  _ NODE _ ID )  ;", "conf    =    new   YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "assertEquals ( conf . get ( RM _ HA _ ID )  ,    TestRMHA . RM 1  _ NODE _ ID )  ;", "configuration . set ( RM _ HA _ IDS ,     (  (  ( TestRMHA . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     ( TestRMHA . RM 3  _ NODE _ ID )  )  )  ;", "configuration . unset ( RM _ HA _ ID )  ;", "conf    =    new   YarnConfiguration ( configuration )  ;", "try    {", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "fail (  \" Should   get   an   exception   here .  \"  )  ;", "}    catch    ( Exception   ex )     {", "Assert . assertTrue ( ex . getMessage (  )  . contains (  \" Invalid   configuration !    Can   not   find   valid   RM _ HA _ ID .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testHAIDLookup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "innerTestHAWithRMHostName ( false )  ;", "configuration . clear (  )  ;", "setUp (  )  ;", "innerTestHAWithRMHostName ( true )  ;", "}", "METHOD_END"], "methodName": ["testHAWithRMHostName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "String   errorMessageForEventHandler    =     \" Expect   to   get   the   same   number   of   handlers \"  ;", "String   errorMessageForService    =     \" Expect   to   get   the   same   number   of   services \"  ;", "configuration . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "Configuration   conf    =    new   conf . YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )     {", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   new   TestRMHA . MyCountingDispatcher (  )  ;", "}", "}  ;", "rm . init ( conf )  ;", "int   expectedEventHandlerCount    =     (  ( TestRMHA . MyCountingDispatcher )     ( rm . getRMContext (  )  . getDispatcher (  )  )  )  . getEventHandlerCount (  )  ;", "int   expectedServiceCount    =    rm . getServices (  )  . size (  )  ;", "assertTrue (  ( expectedEventHandlerCount    !  =     0  )  )  ;", "StateChangeRequestInfo   requestInfo    =    new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER )  ;", "assertEquals ( TestRMHA . STATE _ ERR ,    INITIALIZING ,    rm . adminService . getServiceStatus (  )  . getState (  )  )  ;", "assertFalse (  \" RM   is   ready   to   become   active   before   being   started \"  ,    rm . adminService . getServiceStatus (  )  . isReadyToBecomeActive (  )  )  ;", "rm . start (  )  ;", "rm . adminService . transitionToStandby ( requestInfo )  ;", "rm . adminService . transitionToActive ( requestInfo )  ;", "rm . adminService . transitionToStandby ( requestInfo )  ;", "rm . adminService . transitionToActive ( requestInfo )  ;", "rm . adminService . transitionToStandby ( requestInfo )  ;", "TestRMHA . MyCountingDispatcher   dispatcher    =     (  ( TestRMHA . MyCountingDispatcher )     ( rm . getRMContext (  )  . getDispatcher (  )  )  )  ;", "assertTrue (  (  !  ( dispatcher . isStopped (  )  )  )  )  ;", "rm . adminService . transitionToActive ( requestInfo )  ;", "assertEquals ( errorMessageForEventHandler ,    expectedEventHandlerCount ,     (  ( TestRMHA . MyCountingDispatcher )     ( rm . getRMContext (  )  . getDispatcher (  )  )  )  . getEventHandlerCount (  )  )  ;", "assertEquals ( errorMessageForService ,    expectedServiceCount ,    rm . getServices (  )  . size (  )  )  ;", "dispatcher    =     (  ( TestRMHA . MyCountingDispatcher )     ( rm . getRMContext (  )  . getDispatcher (  )  )  )  ;", "rm . adminService . transitionToStandby ( requestInfo )  ;", "assertEquals ( errorMessageForEventHandler ,    expectedEventHandlerCount ,     (  ( TestRMHA . MyCountingDispatcher )     ( rm . getRMContext (  )  . getDispatcher (  )  )  )  . getEventHandlerCount (  )  )  ;", "assertEquals ( errorMessageForService ,    expectedServiceCount ,    rm . getServices (  )  . size (  )  )  ;", "assertTrue ( dispatcher . isStopped (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMDispatcherForHA"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "final   String   ERR _ UNFORCED _ REQUEST    =     \" User   request   succeeded   even   when    \"     +     \" automatic   failover   is   enabled \"  ;", "Configuration   conf    =    new   conf . YarnConfiguration ( configuration )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "rm . start (  )  ;", "StateChangeRequestInfo   requestInfo    =    new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER )  ;", "try    {", "rm . adminService . transitionToStandby ( requestInfo )  ;", "fail ( ERR _ UNFORCED _ REQUEST )  ;", "}    catch    ( AccessControlException   e )     {", "}", "checkMonitorHealth (  )  ;", "checkStandbyRMFunctionality (  )  ;", "try    {", "rm . adminService . transitionToActive ( requestInfo )  ;", "fail ( ERR _ UNFORCED _ REQUEST )  ;", "}    catch    ( AccessControlException   e )     {", "}", "checkMonitorHealth (  )  ;", "checkStandbyRMFunctionality (  )  ;", "final   String   ERR _ FORCED _ REQUEST    =     \" Forced   request   by   user   should   work    \"     +     \" even   if   automatic   failover   is   enabled \"  ;", "requestInfo    =    new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER _ FORCED )  ;", "try    {", "rm . adminService . transitionToStandby ( requestInfo )  ;", "}    catch    ( AccessControlException   e )     {", "fail ( ERR _ FORCED _ REQUEST )  ;", "}", "checkMonitorHealth (  )  ;", "checkStandbyRMFunctionality (  )  ;", "try    {", "rm . adminService . transitionToActive ( requestInfo )  ;", "}    catch    ( AccessControlException   e )     {", "fail ( ERR _ FORCED _ REQUEST )  ;", "}", "checkMonitorHealth (  )  ;", "checkActiveRMFunctionality (  )  ;", "}", "METHOD_END"], "methodName": ["testTransitionsWhenAutomaticFailoverEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "int   timeoutSecs    =     0  ;", "QueueMetrics   metrics    =    rm . getRScheduler (  )  . getRootQueueMetrics (  )  ;", "ClusterMetrics   clusterMetrics    =    ClusterMetrics . getMetrics (  )  ;", "boolean   isAllMetricAssertionDone    =    false ;", "String   message    =    null ;", "while    (  ( timeoutSecs +  +  )     <     5  )     {", "try    {", "assertMetric (  \" appsSubmitted \"  ,    appsSubmitted ,    metrics . getAppsSubmitted (  )  )  ;", "assertMetric (  \" appsPending \"  ,    appsPending ,    metrics . getAppsPending (  )  )  ;", "assertMetric (  \" containersPending \"  ,    containersPending ,    metrics . getPendingContainers (  )  )  ;", "assertMetric (  \" availableMB \"  ,    availableMB ,    metrics . getAvailableMB (  )  )  ;", "assertMetric (  \" activeApplications \"  ,    activeApplications ,    metrics . getActiveApps (  )  )  ;", "assertMetric (  \" activeNodes \"  ,    activeNodes ,    clusterMetrics . getNumActiveNMs (  )  )  ;", "isAllMetricAssertionDone    =    true ;", "break ;", "}    catch    ( AssertionError   e )     {", "message    =    e . getMessage (  )  ;", "System . out . println (  \" Waiting   for   metrics   assertion   to   complete \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "}", "assertTrue ( message ,    isAllMetricAssertionDone )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMHA"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatResponse   response    =    mock ( NodeHeartbeatResponse . class )  ;", "NodeHealthStatus   healthStatus    =    mock ( NodeHealthStatus . class )  ;", "Boolean   yes    =    new   Boolean ( true )  ;", "doReturn ( yes )  . when ( healthStatus )  . getIsNodeHealthy (  )  ;", "StatusEvent   event    =    mock ( StatusEvent . class )  ;", "doReturn ( healthStatus )  . when ( event )  . getNodeHealthStatus (  )  ;", "doReturn ( response )  . when ( event )  . getLatestResponse (  )  ;", "doReturn ( EventType . STATUS _ UPDATE )  . when ( event )  . getType (  )  ;", "return   event ;", "}", "METHOD_END"], "methodName": ["getMockRMNodeStatusEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" localhost \"  ,     0  )  ;", "Impl   node    =    new   Impl ( nodeId ,    rmContext ,    null ,     0  ,     0  ,    null ,    null ,    null )  ;", "return   node ;", "}", "METHOD_END"], "methodName": ["getNewNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "return   getRunningNode ( null )  ;", "}", "METHOD_END"], "methodName": ["getRunningNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" localhost \"  ,     0  )  ;", "R   capability    =    R . newInstance (  4  0  9  6  ,     4  )  ;", "RMNodeImpl   node    =    new   RMNodeImpl ( nodeId ,    rmContext ,    null ,     0  ,     0  ,    null ,    ROption . newInstance ( capability ,    RMNode . OVER _ COMMIT _ TIMEOUT _ MILLIS _ DEFAULT )  ,    nmVersion )  ;", "node . handle ( new   RMNodeStartedEvent ( node . getNodeID (  )  ,    null ,    null )  )  ;", "Assert . assertEquals ( RUNNING ,    node . getState (  )  )  ;", "return   node ;", "}", "METHOD_END"], "methodName": ["getRunningNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getRunningNode (  )  ;", "NodeHealthStatus   status    =    NodeHealthStatus . newInstance ( false ,     \" sick \"  ,    System . currentTimeMillis (  )  )  ;", "node . handle ( new   RMNodeStatusEvent ( node . getNodeID (  )  ,    status ,    new   ArrayList < api . records . ContainerStatus >  (  )  ,    null ,    null )  )  ;", "Assert . assertEquals ( UNHEALTHY ,    node . getState (  )  )  ;", "return   node ;", "}", "METHOD_END"], "methodName": ["getUnhealthyNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "InlineDispatcher   rmDispatcher    =    new   InlineDispatcher (  )  ;", "rmContext    =    new   RMContextImpl ( rmDispatcher ,    null ,    null ,    null ,    mock ( DelegationTokenRenewer . class )  ,    null ,    null ,    null ,    null ,    null )  ;", "NodesListManager   nodesListManager    =    mock ( NodesListManager . class )  ;", "HostsFileReader   reader    =    mock ( HostsFileReader . class )  ;", "when ( nodesListManager . getHostsReader (  )  )  . thenReturn ( reader )  ;", "(  ( RMContextImpl )     ( rmContext )  )  . setNodesListManager ( nodesListManager )  ;", "scheduler    =    mock ( YarnScheduler . class )  ;", "doAnswer ( new   Answer < Void >  (  )     {", "@ Override", "public   Void   answer ( InvocationOnMock   invocation )    throws   Throwable    {", "final   SchedulerEvent   event    =     (  ( SchedulerEvent )     ( invocation . getArguments (  )  [  0  ]  )  )  ;", "eventType    =    event . getType (  )  ;", "if    (  ( eventType )     =  =     ( SchedulerEventType . NODE _ UPDATE )  )     {", "List < UpdatedContainerInfo >    lastestContainersInfoList    =     (  ( NodeUpdateSchedulerEvent )     ( event )  )  . getRMNode (  )  . pullContainerUpdates (  )  ;", "for    ( UpdatedContainerInfo   lastestContainersInfo    :    lastestContainersInfoList )     {", "completedContainers . addAll ( lastestContainersInfo . getCompletedContainers (  )  )  ;", "}", "}", "return   null ;", "}", "}  )  . when ( scheduler )  . handle ( any ( SchedulerEvent . class )  )  ;", "rmDispatcher . register ( SchedulerEventType . class ,    new    . TestSchedulerEventDispatcher (  )  )  ;", "rmDispatcher . register ( NodesListManagerEventType . class ,    new    . TestNodeListManagerEventDispatcher (  )  )  ;", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" localhost \"  ,     0  )  ;", "node    =    new   RMNodeImpl ( nodeId ,    rmContext ,    null ,     0  ,     0  ,    null ,    null ,    null )  ;", "nodesListManagerEvent    =    null ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getNewNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeStartedEvent ( node . getNodeID (  )  ,    null ,    null )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,     ( initialActive    +     1  )  ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,    initialLost ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,    initialUnhealthy ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,    initialDecommissioned ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,    initialRebooted ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( RUNNING ,    node . getState (  )  )  ;", "Assert . assertNotNull ( nodesListManagerEvent )  ;", "Assert . assertEquals ( NodesListManagerEventType . NODE _ USABLE ,    nodesListManagerEvent . getType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAdd"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "node . handle ( new   RMNodeStartedEvent ( null ,    null ,    null )  )  ;", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" localhost :  1  \"  ,     1  )  ;", "RMNodeImpl   node 2     =    new   RMNodeImpl ( nodeId ,    rmContext ,    null ,     0  ,     0  ,    null ,    null ,    null )  ;", "node 2  . handle ( new   RMNodeStartedEvent ( null ,    null ,    null )  )  ;", "ContainerId   completedContainerIdFromNode 1     =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  0  ,     0  )  ,     0  )  ,     0  )  ;", "ContainerId   completedContainerIdFromNode 2  _  1     =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  1  ,     1  )  ,     1  )  ,     1  )  ;", "ContainerId   completedContainerIdFromNode 2  _  2     =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  1  ,     1  )  ,     1  )  ,     2  )  ;", "RMNodeStatusEvent   statusEventFromNode 1     =    getMockRMNodeStatusEvent (  )  ;", "RMNodeStatusEvent   statusEventFromNode 2  _  1     =    getMockRMNodeStatusEvent (  )  ;", "RMNodeStatusEvent   statusEventFromNode 2  _  2     =    getMockRMNodeStatusEvent (  )  ;", "ContainerStatus   containerStatusFromNode 1     =    mock ( ContainerStatus . class )  ;", "ContainerStatus   containerStatusFromNode 2  _  1     =    mock ( ContainerStatus . class )  ;", "ContainerStatus   containerStatusFromNode 2  _  2     =    mock ( ContainerStatus . class )  ;", "doReturn ( completedContainerIdFromNode 1  )  . when ( containerStatusFromNode 1  )  . getContainerId (  )  ;", "doReturn ( Collections . singletonList ( containerStatusFromNode 1  )  )  . when ( statusEventFromNode 1  )  . getContainers (  )  ;", "node . handle ( statusEventFromNode 1  )  ;", "Assert . assertEquals (  1  ,    completedContainers . size (  )  )  ;", "Assert . assertEquals ( completedContainerIdFromNode 1  ,    completedContainers . get (  0  )  . getContainerId (  )  )  ;", "completedContainers . clear (  )  ;", "doReturn ( completedContainerIdFromNode 2  _  1  )  . when ( containerStatusFromNode 2  _  1  )  . getContainerId (  )  ;", "doReturn ( Collections . singletonList ( containerStatusFromNode 2  _  1  )  )  . when ( statusEventFromNode 2  _  1  )  . getContainers (  )  ;", "doReturn ( completedContainerIdFromNode 2  _  2  )  . when ( containerStatusFromNode 2  _  2  )  . getContainerId (  )  ;", "doReturn ( Collections . singletonList ( containerStatusFromNode 2  _  2  )  )  . when ( statusEventFromNode 2  _  2  )  . getContainers (  )  ;", "node 2  . setNextHeartBeat ( false )  ;", "node 2  . handle ( statusEventFromNode 2  _  1  )  ;", "node 2  . setNextHeartBeat ( true )  ;", "node 2  . handle ( statusEventFromNode 2  _  2  )  ;", "Assert . assertEquals (  2  ,    completedContainers . size (  )  )  ;", "Assert . assertEquals ( completedContainerIdFromNode 2  _  1  ,    completedContainers . get (  0  )  . getContainerId (  )  )  ;", "Assert . assertEquals ( completedContainerIdFromNode 2  _  2  ,    completedContainers . get (  1  )  . getContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "node . handle ( new   RMNodeStartedEvent ( null ,    null ,    null )  )  ;", "verify ( scheduler )  . handle ( any ( NodeAddedSchedulerEvent . class )  )  ;", "ContainerId   completedContainerId    =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  0  ,     0  )  ,     0  )  ,     0  )  ;", "node . handle ( new   RMNodeCleanContainerEvent ( null ,    completedContainerId )  )  ;", "Assert . assertEquals (  1  ,    node . getContainersToCleanUp (  )  . size (  )  )  ;", "RMNodeStatusEvent   statusEvent    =    getMockRMNodeStatusEvent (  )  ;", "ContainerStatus   containerStatus    =    mock ( ContainerStatus . class )  ;", "doReturn ( completedContainerId )  . when ( containerStatus )  . getContainerId (  )  ;", "doReturn ( Collections . singletonList ( containerStatus )  )  . when ( statusEvent )  . getContainers (  )  ;", "node . handle ( statusEvent )  ;", "verify ( scheduler ,    times (  2  )  )  . handle ( any ( NodeUpdateSchedulerEvent . class )  )  ;", "}", "METHOD_END"], "methodName": ["testExpiredContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getRunningNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeReconnectEvent ( node . getNodeID (  )  ,    node ,    null )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,    initialActive ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,    initialLost ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,    initialUnhealthy ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,    initialDecommissioned ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,    initialRebooted ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( RUNNING ,    node . getState (  )  )  ;", "Assert . assertNotNull ( nodesListManagerEvent )  ;", "Assert . assertEquals ( NodesListManagerEventType . NODE _ USABLE ,    nodesListManagerEvent . getType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReconnect"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "final   String   nmVersion 1     =     \" nm   version    1  \"  ;", "final   String   nmVersion 2     =     \" nm   version    2  \"  ;", "Impl   node    =    getRunningNode ( nmVersion 1  )  ;", "Assert . assertEquals ( nmVersion 1  ,    node . getNodeManagerVersion (  )  )  ;", "Impl   reconnectingNode    =    getRunningNode ( nmVersion 2  )  ;", "node . handle ( new   ReconnectEvent ( node . getNodeID (  )  ,    reconnectingNode ,    null )  )  ;", "Assert . assertEquals ( nmVersion 2  ,    node . getNodeManagerVersion (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReconnnectUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getRunningNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . DECOMMISSION )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,     ( initialActive    -     1  )  ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,    initialLost ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,    initialUnhealthy ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,     ( initialDecommissioned    +     1  )  ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,    initialRebooted ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( DECOMMISSIONED ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRunningDecommission"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getRunningNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . EXPIRE )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,     ( initialActive    -     1  )  ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,     ( initialLost    +     1  )  ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,    initialUnhealthy ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,    initialDecommissioned ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,    initialRebooted ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( LOST ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRunningExpire"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getRunningNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . REBOOTING )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,     ( initialActive    -     1  )  ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,    initialLost ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,    initialUnhealthy ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,    initialDecommissioned ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,     ( initialRebooted    +     1  )  ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( REBOOTED ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRunningRebooting"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "node . handle ( new   RMNodeStartedEvent ( null ,    null ,    null )  )  ;", "node . setNextHeartBeat ( false )  ;", "ContainerId   completedContainerId 1     =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  0  ,     0  )  ,     0  )  ,     0  )  ;", "ContainerId   completedContainerId 2     =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  1  ,     1  )  ,     1  )  ,     1  )  ;", "RMNodeStatusEvent   statusEvent 1     =    getMockRMNodeStatusEvent (  )  ;", "RMNodeStatusEvent   statusEvent 2     =    getMockRMNodeStatusEvent (  )  ;", "ContainerStatus   containerStatus 1     =    mock ( ContainerStatus . class )  ;", "ContainerStatus   containerStatus 2     =    mock ( ContainerStatus . class )  ;", "doReturn ( completedContainerId 1  )  . when ( containerStatus 1  )  . getContainerId (  )  ;", "doReturn ( Collections . singletonList ( containerStatus 1  )  )  . when ( statusEvent 1  )  . getContainers (  )  ;", "doReturn ( completedContainerId 2  )  . when ( containerStatus 2  )  . getContainerId (  )  ;", "doReturn ( Collections . singletonList ( containerStatus 2  )  )  . when ( statusEvent 2  )  . getContainers (  )  ;", "verify ( scheduler ,    times (  1  )  )  . handle ( any ( NodeUpdateSchedulerEvent . class )  )  ;", "node . handle ( statusEvent 1  )  ;", "node . handle ( statusEvent 2  )  ;", "verify ( scheduler ,    times (  1  )  )  . handle ( any ( NodeUpdateSchedulerEvent . class )  )  ;", "Assert . assertEquals (  2  ,    node . getQueueSize (  )  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . EXPIRE )  )  ;", "Assert . assertEquals (  0  ,    node . getQueueSize (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStatusChange"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getUnhealthyNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . DECOMMISSION )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,    initialActive ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,    initialLost ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,     ( initialUnhealthy    -     1  )  ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,     ( initialDecommissioned    +     1  )  ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,    initialRebooted ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( DECOMMISSIONED ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnhealthyDecommission"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getUnhealthyNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . EXPIRE )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,    initialActive ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,     ( initialLost    +     1  )  ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,     ( initialUnhealthy    -     1  )  ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,    initialDecommissioned ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,    initialRebooted ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( LOST ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnhealthyExpire"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getUnhealthyNode (  )  ;", "verify ( scheduler ,    times (  2  )  )  . handle ( any ( NodeRemovedSchedulerEvent . class )  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . EXPIRE )  )  ;", "verify ( scheduler ,    times (  2  )  )  . handle ( any ( NodeRemovedSchedulerEvent . class )  )  ;", "Assert . assertEquals ( LOST ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnhealthyExpireForSchedulerRemove"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getUnhealthyNode (  )  ;", "ClusterMetrics   cm    =    ClusterMetrics . getMetrics (  )  ;", "int   initialActive    =    cm . getNumActiveNMs (  )  ;", "int   initialLost    =    cm . getNumLostNMs (  )  ;", "int   initialUnhealthy    =    cm . getUnhealthyNMs (  )  ;", "int   initialDecommissioned    =    cm . getNumDecommisionedNMs (  )  ;", "int   initialRebooted    =    cm . getNumRebootedNMs (  )  ;", "node . handle ( new   RMNodeEvent ( node . getNodeID (  )  ,    RMNodeEventType . REBOOTING )  )  ;", "Assert . assertEquals (  \" Active   Nodes \"  ,    initialActive ,    cm . getNumActiveNMs (  )  )  ;", "Assert . assertEquals (  \" Lost   Nodes \"  ,    initialLost ,    cm . getNumLostNMs (  )  )  ;", "Assert . assertEquals (  \" Unhealthy   Nodes \"  ,     ( initialUnhealthy    -     1  )  ,    cm . getUnhealthyNMs (  )  )  ;", "Assert . assertEquals (  \" Decommissioned   Nodes \"  ,    initialDecommissioned ,    cm . getNumDecommisionedNMs (  )  )  ;", "Assert . assertEquals (  \" Rebooted   Nodes \"  ,     ( initialRebooted    +     1  )  ,    cm . getNumRebootedNMs (  )  )  ;", "Assert . assertEquals ( REBOOTED ,    node . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnhealthyRebooting"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl   node    =    getRunningNode (  )  ;", "NodeId   nodeId    =    node . getNodeID (  )  ;", "ContainerId   completedContainerId    =    BuilderUtils . newContainerId ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId (  0  ,     0  )  ,     0  )  ,     0  )  ;", "node . handle ( new   RMNodeCleanContainerEvent ( nodeId ,    completedContainerId )  )  ;", "Assert . assertEquals (  1  ,    node . getContainersToCleanUp (  )  . size (  )  )  ;", "ApplicationId   finishedAppId    =    BuilderUtils . newApplicationId (  0  ,     1  )  ;", "node . handle ( new   RMNodeCleanAppEvent ( nodeId ,    finishedAppId )  )  ;", "Assert . assertEquals (  1  ,    node . getAppsToCleanup (  )  . size (  )  )  ;", "RMNodeStatusEvent   statusEvent    =    getMockRMNodeStatusEvent (  )  ;", "node . handle ( statusEvent )  ;", "Assert . assertEquals (  1  ,    node . getContainersToCleanUp (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    node . getAppsToCleanup (  )  . size (  )  )  ;", "NodeHeartbeatResponse   hbrsp    =    Records . newRecord ( NodeHeartbeatResponse . class )  ;", "node . updateNodeHeartbeatResponseForCleanup ( hbrsp )  ;", "Assert . assertEquals (  0  ,    node . getContainersToCleanUp (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    node . getAppsToCleanup (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    hbrsp . getContainersToCleanup (  )  . size (  )  )  ;", "Assert . assertEquals ( completedContainerId ,    hbrsp . getContainersToCleanup (  )  . get (  0  )  )  ;", "Assert . assertEquals (  1  ,    hbrsp . getApplicationsToCleanup (  )  . size (  )  )  ;", "Assert . assertEquals ( finishedAppId ,    hbrsp . getApplicationsToCleanup (  )  . get (  0  )  )  ;", "}", "METHOD_END"], "methodName": ["testUpdateHeartbeatResponseForCleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMNodeTransitions"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( qm . getAppsSubmitted (  )  ,     ( appsSubmitted    +     ( appsSubmittedCarryOn )  )  )  ;", "Assert . assertEquals ( qm . getAppsPending (  )  ,     ( appsPending    +     ( appsPendingCarryOn )  )  )  ;", "Assert . assertEquals ( qm . getAppsRunning (  )  ,     ( appsRunning    +     ( appsRunningCarryOn )  )  )  ;", "Assert . assertEquals ( qm . getAppsCompleted (  )  ,     ( appsCompleted    +     ( appsCompletedCarryOn )  )  )  ;", "}", "METHOD_END"], "methodName": ["assertQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    id )  ;", "NMContainerStatus   containerReport    =    NMContainerStatus . newInstance ( containerId ,    containerState ,    R . newInstance (  1  0  2  4  ,     1  )  ,     \" recover   container \"  ,     0  ,    Priority . newInstance (  0  )  ,     0  )  ;", "return   containerReport ;", "}", "METHOD_END"], "methodName": ["createNMContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "final   FinishApplicationMasterRequest   req    =    FinishApplicationMasterRequest . newInstance ( SUCCEEDED ,     \"  \"  ,     \"  \"  )  ;", "finishApplicationMaster ( rmApp ,    rm ,    nm ,    am ,    req )  ;", "}", "METHOD_END"], "methodName": ["finishApplicationMaster"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "RMStateStore . RMState   rmState    =     (  ( MemoryRMStateStore )     ( rm . getRMContext (  )  . getStateStore (  )  )  )  . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "am . unregisterAppAttempt ( req ,    true )  ;", "am . waitForState ( RMAppAttemptState . FINISHING )  ;", "nm . nodeHeartbeat ( am . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "rm . waitForState ( rmApp . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( rmApp . getApplicationId (  )  )  ;", "Assert . assertEquals ( RMAppState . FINISHED ,    appState . getState (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . FINISHED ,    appState . getAttempt ( am . getApplicationAttemptId (  )  )  . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["finishApplicationMaster"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "nm . nodeHebeat ( true )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["launchAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "appsSubmittedCarryOn    =    qm . getAppsSubmitted (  )  ;", "appsPendingCarryOn    =    qm . getAppsPending (  )  ;", "appsRunningCarryOn    =    qm . getAppsRunning (  )  ;", "appsCompletedCarryOn    =    qm . getAppsCompleted (  )  ;", "}", "METHOD_END"], "methodName": ["resetQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "conf    =    new   YarnConfiguration (  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "conf . set ( RECOVERY _ ENABLED ,     \" true \"  )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", ". rmAddr    =    new   InetSocketAddress (  \" localhost \"  ,     8  0  3  2  )  ;", "Assert . assertTrue (  (  ( YarnConfiguration . DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )     >     1  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "TestRMRestart . TEMP _ DIR . delete (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  0  .  0  .  0  .  0  :  4  3  2  1  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,     \" default \"  )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 1  . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 1     =    attempt 1  . getAppAttemptId (  )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "RMStateStore . ApplicationAttemptState   attemptState    =    appState . getAttempt ( attemptId 1  )  ;", "Assert . assertNotNull ( attemptState )  ;", "Assert . assertEquals ( BuilderUtils . newContainerId ( attemptId 1  ,     1  )  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "byte [  ]    clientTokenMasterKey    =    attempt 1  . getClientTokenMasterKey (  )  . getEncoded (  )  ;", "Credentials   savedCredentials    =    attemptState . getAppAttemptCredentials (  )  ;", "Assert . assertArrayEquals (  \" client   token   master   key   not   saved \"  ,    clientTokenMasterKey ,    savedCredentials . getSecretKey ( RMStateStore . AM _ CLIENT _ TOKEN _ MASTER _ KEY _ NAME )  )  ;", "MockRM   rm 2     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "RMApp   loadedApp 1     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "RMAppAttempt   loadedAttempt 1     =    loadedApp 1  . getRMAppAttempt ( attemptId 1  )  ;", "Assert . assertNotNull ( loadedAttempt 1  )  ;", "Assert . assertEquals (  \" client   token   master   key   not   restored \"  ,    attempt 1  . getClientTokenMasterKey (  )  ,    loadedAttempt 1  . getClientTokenMasterKey (  )  )  ;", "Assert . assertArrayEquals ( clientTokenMasterKey ,    rm 2  . getClientToAMTokenSecretManager (  )  . getMasterKey ( attemptId 1  )  . getEncoded (  )  )  ;", "Token < AMRMTokenIdentifier >    amrmToken    =    loadedAttempt 1  . getAMRMToken (  )  ;", "Assert . assertArrayEquals ( amrmToken . getPassword (  )  ,    rm 2  . getRMContext (  )  . getAMRMTokenSecretManager (  )  . retrievePassword ( amrmToken . decodeIdentifier (  )  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptTokensRestoredOnRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "conf . set ( RM _ ADDRESS ,     \" localhost :  8  0  3  2  \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "GetDelegationTokenRequest   request 1     =    GetDelegationTokenRequest . newInstance (  \" renewer 1  \"  )  ;", "UserGroupInformation . getCurrentUser (  )  . setAuthenticationMethod ( KERBEROS )  ;", "GetDelegationTokenResponse   response 1     =    rm 1  . getClientRMService (  )  . getDelegationToken ( request 1  )  ;", "Token < RMDelegationTokenIdentifier >    token 1     =    ConverterUtils . convertFromYarn ( response 1  . getRMDelegationToken (  )  ,    TestRMRestart . rmAddr )  ;", "MockRM   rm 2     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "ts . addToken ( token 1  . getService (  )  ,    token 1  )  ;", "RMApp   app    =    rm 2  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     1  ,    ts )  ;", "rm 2  . waitForState ( app . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "}", "METHOD_END"], "methodName": ["testAppSubmissionWithOldDelegationTokenAfterRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   TestRMRestart . TestMemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    null ,    false ,     \" default \"  ,     1  ,    null ,     \" myType \"  )  ;", "MockAM   am 1     =    launchAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "KillApplicationResponse   response ;", "int   count    =     0  ;", "while    ( true )     {", "response    =    rm 1  . killApp ( app 1  . getApplicationId (  )  )  ;", "if    ( response . getIsKillCompleted (  )  )     {", "break ;", "}", "Thread . sleep (  1  0  0  )  ;", "count +  +  ;", "}", "Assert . assertTrue (  ( count    >  =     1  )  )  ;", "rm 1  . waitForState ( am 1  . getApplicationAttemptId (  )  ,    RMAppAttemptState . KILLED )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "Assert . assertEquals (  1  ,     (  ( TestRMRestart . TestMemoryRMStateStore )     ( memStore )  )  . updateAttempt )  ;", "Assert . assertEquals (  2  ,     (  ( TestRMRestart . TestMemoryRMStateStore )     ( memStore )  )  . updateApp )  ;", "}", "METHOD_END"], "methodName": ["testClientRetryOnKillingApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "writeToHostsFile (  \"  \"  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "rm 1  . registerNode (  \" localhost :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "rm 1  . registerNode (  \" host 2  :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "Assert . assertEquals (  0  ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "String   ip    =    NetUtils . normalizeHostName (  \" localhost \"  )  ;", "writeToHostsFile (  \" host 2  \"  ,    ip )  ;", "rm 1  . getNodesListM (  )  . refreshNodes ( conf )  ;", "Assert . assertEquals (  2  ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf )  ;", "rm 2  . start (  )  ;", "Assert . assertEquals (  2  ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testDecomissionedNMsMetricsOnRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "HashSet < Token < RMDelegationTokenIdentifier >  >    tokenSet    =    new   HashSet < Token < RMDelegationTokenIdentifier >  >  (  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "Text   userText 1     =    new   Text (  \" user 1  \"  )  ;", "RMDelegationTokenIdentifier   dtId 1     =    new   RMDelegationTokenIdentifier ( userText 1  ,    new   Text (  \" renewer 1  \"  )  ,    userText 1  )  ;", "Token < RMDelegationTokenIdentifier >    token 1     =    new   Token < RMDelegationTokenIdentifier >  ( dtId 1  ,    rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  )  ;", "SecurityUtil . setTokenService ( token 1  ,    TestRMRestart . rmAddr )  ;", "ts . addToken ( userText 1  ,    token 1  )  ;", "tokenSet . add ( token 1  )  ;", "Text   userText 2     =    new   Text (  \" user 2  \"  )  ;", "RMDelegationTokenIdentifier   dtId 2     =    new   RMDelegationTokenIdentifier ( userText 2  ,    new   Text (  \" renewer 2  \"  )  ,    userText 2  )  ;", "Token < RMDelegationTokenIdentifier >    token 2     =    new   Token < RMDelegationTokenIdentifier >  ( dtId 2  ,    rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  )  ;", "SecurityUtil . setTokenService ( token 2  ,    TestRMRestart . rmAddr )  ;", "ts . addToken ( userText 2  ,    token 2  )  ;", "tokenSet . add ( token 2  )  ;", "RMApp   app    =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     1  ,    ts )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "Assert . assertEquals ( tokenSet ,    rm 1  . getRMContext (  )  . getDelegationTokenRenewer (  )  . getDelegationTokens (  )  )  ;", "DataOutputBuffer   dob    =    new   DataOutputBuffer (  )  ;", "ts . writeTokenStorageToStream ( dob )  ;", "ByteBuffer   securityTokens    =    ByteBuffer . wrap ( dob . getData (  )  ,     0  ,    dob . getLength (  )  )  ;", "securityTokens . rewind (  )  ;", "Assert . assertEquals ( securityTokens ,    appState . getApplicationSubmissionContext (  )  . getAMContainerSpec (  )  . getTokens (  )  )  ;", "MockRM   rm 2     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "waitForTokensToBeRenewed ( rm 2  )  ;", "Assert . assertEquals ( tokenSet ,    rm 2  . getRMContext (  )  . getDelegationTokenRenewer (  )  . getDelegationTokens (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testDelegationTokenRestoredInDelegationTokenRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "conf . setInt ( RM _ MAX _ COMPLETED _ APPLICATIONS ,     1  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "finishApplicationMaster ( app 0  ,    rm 1  ,    nm 1  ,    am 0  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setRTrackerService ( rm 2  . getRTrackerService (  )  )  ;", "nm 1     =    rm 2  . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "Assert . assertEquals ( RMAppState . FINISHED ,    rmAppState . get ( app 0  . getApplicationId (  )  )  . getState (  )  )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "RMApp   app 1     =    rm 2  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    launchAM ( app 1  ,    rm 2  ,    nm 1  )  ;", "finishApplicationMaster ( app 1  ,    rm 2  ,    nm 1  ,    am 1  )  ;", "Assert . assertNull ( rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 0  . getApplicationId (  )  )  )  ;", "Assert . assertNull ( rmAppState . get ( app 0  . getApplicationId (  )  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testFinishedAppRemovalAfterRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "QueueMetrics   qm 1     =    rm 1  . getResourceScheduler (  )  . getRootQueueMetrics (  )  ;", "resetQueueMetrics ( qm 1  )  ;", "assertQueueMetrics ( qm 1  ,     0  ,     0  ,     0  ,     0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 1     =    attempt 1  . getAppAttemptId (  )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . SCHEDULED )  ;", "assertQueueMetrics ( qm 1  ,     1  ,     1  ,     0  ,     0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "MockAM   am 1     =    rm 1  . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  0  0  ,     1  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "List < Container >    conts    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "while    (  ( conts . size (  )  )     =  =     0  )     {", "nm 1  . nodeHeartbeat ( true )  ;", "conts . addAll ( am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "assertQueueMetrics ( qm 1  ,     1  ,     0  ,     1  ,     0  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "QueueMetrics   qm 2     =    rm 2  . getResourceScheduler (  )  . getRootQueueMetrics (  )  ;", "resetQueueMetrics ( qm 2  )  ;", "assertQueueMetrics ( qm 2  ,     0  ,     0  ,     0  ,     0  )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "RMApp   loadedApp 1     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "am 1  . setAMRMProtocol ( rm 2  . getApplicationMasterService (  )  ,    rm 2  . getRMContext (  )  )  ;", "am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 2  . getResourceTrackerService (  )  )  ;", "NMContainerStatus   status    =    TestRMRestart . createNMContainerStatus ( loadedApp 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( status )  ,    null )  ;", "while    (  ( loadedApp 1  . getAppAttempts (  )  . size (  )  )     !  =     2  )     {", "Thread . sleep (  2  0  0  )  ;", "}", "attempt 1     =    loadedApp 1  . getCurrentAppAttempt (  )  ;", "attemptId 1     =    attempt 1  . getAppAttemptId (  )  ;", "rm 2  . waitForState ( attemptId 1  ,    RMAppAttemptState . SCHEDULED )  ;", "assertQueueMetrics ( qm 2  ,     1  ,     1  ,     0  ,     0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "rm 2  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "assertQueueMetrics ( qm 2  ,     1  ,     0  ,     1  ,     0  )  ;", "am 1     =    rm 2  . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  0  0  ,     3  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "conts    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "while    (  ( conts . size (  )  )     =  =     0  )     {", "nm 1  . nodeHeartbeat ( true )  ;", "conts . addAll ( am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "finishApplicationMaster ( loadedApp 1  ,    rm 2  ,    nm 1  ,    am 1  )  ;", "assertQueueMetrics ( qm 2  ,     1  ,     0  ,     0  ,     1  )  ;", "rm 2  . stop (  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testQueueMetricsOnRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "conf . set ( RM _ ADDRESS ,     \" localhost :  8  0  3  2  \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "Map < RMDelegationTokenIdentifier ,    Long >    rmDTState    =    rmState . getRMDTSecretManagerState (  )  . getTokenState (  )  ;", "Set < DelegationKey >    rmDTMasterKeyState    =    rmState . getRMDTSecretManagerState (  )  . getMasterKeyState (  )  ;", "MockRM   rm 1     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "GetDelegationTokenRequest   request 1     =    GetDelegationTokenRequest . newInstance (  \" renewer 1  \"  )  ;", "UserGroupInformation . getCurrentUser (  )  . setAuthenticationMethod ( KERBEROS )  ;", "GetDelegationTokenResponse   response 1     =    rm 1  . getClientRMService (  )  . getDelegationToken ( request 1  )  ;", "Token   delegationToken 1     =    response 1  . getRMDelegationToken (  )  ;", "Token < RMDelegationTokenIdentifier >    token 1     =    ConverterUtils . convertFromYarn ( delegationToken 1  ,    TestRMRestart . rmAddr )  ;", "RMDelegationTokenIdentifier   dtId 1     =    token 1  . decodeIdentifier (  )  ;", "HashSet < RMDelegationTokenIdentifier >    tokenIdentSet    =    new   HashSet < RMDelegationTokenIdentifier >  (  )  ;", "ts . addToken ( token 1  . getService (  )  ,    token 1  )  ;", "tokenIdentSet . add ( dtId 1  )  ;", "RMApp   app    =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     1  ,    ts )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "Set < DelegationKey >    allKeysRM 1     =    rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllMasterKeys (  )  ;", "Assert . assertEquals ( allKeysRM 1  ,    rmDTMasterKeyState )  ;", "Map < RMDelegationTokenIdentifier ,    Long >    allTokensRM 1     =    rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllTokens (  )  ;", "Assert . assertEquals ( tokenIdentSet ,    allTokensRM 1  . keySet (  )  )  ;", "Assert . assertEquals ( allTokensRM 1  ,    rmDTState )  ;", "Assert . assertEquals ( rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getLatestDTSequenceNumber (  )  ,    rmState . getRMDTSecretManagerState (  )  . getDTSequenceNumber (  )  )  ;", "GetDelegationTokenRequest   request 2     =    GetDelegationTokenRequest . newInstance (  \" renewer 2  \"  )  ;", "GetDelegationTokenResponse   response 2     =    rm 1  . getClientRMService (  )  . getDelegationToken ( request 2  )  ;", "Token   delegationToken 2     =    response 2  . getRMDelegationToken (  )  ;", "Token < RMDelegationTokenIdentifier >    token 2     =    ConverterUtils . convertFromYarn ( delegationToken 2  ,    TestRMRestart . rmAddr )  ;", "RMDelegationTokenIdentifier   dtId 2     =    token 2  . decodeIdentifier (  )  ;", "try    {", "rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . cancelToken ( token 2  ,    UserGroupInformation . getCurrentUser (  )  . getUserName (  )  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  )  ;", "}", "Assert . assertEquals ( rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getLatestDTSequenceNumber (  )  ,    dtId 2  . getSequenceNumber (  )  )  ;", "Assert . assertFalse ( rmDTState . containsKey ( dtId 2  )  )  ;", "MockRM   rm 2     =    new   TestRMRestart . TestSecurityMockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "Map < RMDelegationTokenIdentifier ,    Long >    allTokensRM 2     =    rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllTokens (  )  ;", "Assert . assertEquals ( allTokensRM 2  . keySet (  )  ,    allTokensRM 1  . keySet (  )  )  ;", "Assert . assertTrue ( rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllMasterKeys (  )  . containsAll ( allKeysRM 1  )  )  ;", "Assert . assertEquals ( rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getLatestDTSequenceNumber (  )  ,    rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getLatestDTSequenceNumber (  )  )  ;", "Long   renewDateBeforeRenew    =    allTokensRM 2  . get ( dtId 1  )  ;", "try    {", "Thread . sleep (  1  )  ;", "rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . renewToken ( token 1  ,     \" renewer 1  \"  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  )  ;", "}", "allTokensRM 2     =    rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllTokens (  )  ;", "Long   renewDateAfterRenew    =    allTokensRM 2  . get ( dtId 1  )  ;", "Assert . assertTrue (  ( renewDateAfterRenew    >    renewDateBeforeRenew )  )  ;", "Assert . assertTrue ( rmDTState . containsValue ( renewDateAfterRenew )  )  ;", "Assert . assertFalse ( rmDTState . containsValue ( renewDateBeforeRenew )  )  ;", "try    {", "rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . cancelToken ( token 1  ,    UserGroupInformation . getCurrentUser (  )  . getUserName (  )  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  )  ;", "}", "allTokensRM 2     =    rm 2  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllTokens (  )  ;", "Assert . assertFalse ( allTokensRM 2  . containsKey ( dtId 1  )  )  ;", "Assert . assertFalse ( rmDTState . containsKey ( dtId 1  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMDelegationTokenRestoredOnRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  2  :  5  6  7  8  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "nm 2  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "RMAppAttempt   attempt 0     =    app 0  . getCurrentAppAttempt (  )  ;", "Assert . assertEquals (  1  ,    rmAppState . size (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "MockAM   am 0     =    rm 1  . sendAMLaunched ( attempt 0  . getAppAttemptId (  )  )  ;", "am 0  . registerAppAttempt (  )  ;", "finishApplicationMaster ( app 0  ,    rm 1  ,    nm 1  ,    am 0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 1  . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "Assert . assertEquals (  0  ,    appState . getAttemptCount (  )  )  ;", "Assert . assertEquals ( appState . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    app 1  . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 1     =    attempt 1  . getAppAttemptId (  )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "Assert . assertEquals (  1  ,    appState . getAttemptCount (  )  )  ;", "RMStateStore . ApplicationAttemptState   attemptState    =    appState . getAttempt ( attemptId 1  )  ;", "Assert . assertNotNull ( attemptState )  ;", "Assert . assertEquals ( BuilderUtils . newContainerId ( attemptId 1  ,     1  )  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "MockAM   am 1     =    rm 1  . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  0  0  ,     1  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "List < Container >    conts    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "while    (  ( conts . size (  )  )     =  =     0  )     {", "nm 1  . nodeHeartbeat ( true )  ;", "conts . addAll ( am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "RMApp   app 2     =    rm 1  . submitApp (  2  0  0  )  ;", "appState    =    rmAppState . get ( app 2  . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "Assert . assertEquals (  0  ,    appState . getAttemptCount (  )  )  ;", "Assert . assertEquals ( appState . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    app 2  . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "RMApp   appUnmanaged    =    rm 1  . submitApp (  2  0  0  ,     \" someApp \"  ,     \" someUser \"  ,    null ,    true ,    null ,    conf . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null )  ;", "ApplicationAttemptId   unmanagedAttemptId    =    appUnmanaged . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ;", "ApplicationId   unmanagedAppId    =    appUnmanaged . getApplicationId (  )  ;", "appState    =    rmAppState . get ( unmanagedAppId )  ;", "Assert . assertNotNull ( appState )  ;", "rm 1  . waitForState ( unmanagedAttemptId ,    RMAppAttemptState . LAUNCHED )  ;", "rm 1  . waitForState ( unmanagedAppId ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals (  1  ,    appState . getAttemptCount (  )  )  ;", "Assert . assertEquals ( appState . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    appUnmanaged . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 2  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "Assert . assertEquals (  4  ,    rm 2  . getRMContext (  )  . getRMApps (  )  . size (  )  )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "rm 2  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FINISHED )  ;", "RMApp   loadedApp 1     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "Assert . assertNotNull ( loadedApp 1  )  ;", "Assert . assertEquals (  1  ,    loadedApp 1  . getAppAttempts (  )  . size (  )  )  ;", "Assert . assertEquals ( app 1  . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    loadedApp 1  . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "RMApp   loadedApp 2     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 2  . getApplicationId (  )  )  ;", "Assert . assertNotNull ( loadedApp 2  )  ;", "Assert . assertEquals ( app 2  . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    loadedApp 2  . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "rm 2  . waitForState ( loadedApp 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "rm 2  . waitForState ( loadedApp 2  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals (  1  ,    loadedApp 1  . getAppAttempts (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    loadedApp 2  . getAppAttempts (  )  . size (  )  )  ;", "am 1  . setAMRMProtocol ( rm 2  . getApplicationMasterService (  )  ,    rm 2  . getRMContext (  )  )  ;", "AllocateResponse   allocResponse    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "Assert . assertEquals ( AM _ SHUTDOWN ,    allocResponse . getAMCommand (  )  )  ;", "NodeHeartbeatResponse   hbResponse    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . RESYNC ,    hbResponse . getNodeAction (  )  )  ;", "hbResponse    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . RESYNC ,    hbResponse . getNodeAction (  )  )  ;", "nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 2  . getResourceTrackerService (  )  )  ;", "nm 2     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  2  :  5  6  7  8  \"  ,     1  5  1  2  0  ,    rm 2  . getResourceTrackerService (  )  )  ;", "NMContainerStatus   status    =    TestRMRestart . createNMContainerStatus ( loadedApp 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( status )  ,    null )  ;", "nm 2  . registerNode (  )  ;", "rm 2  . waitForState ( loadedApp 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  ( loadedApp 1  . getAppAttempts (  )  . size (  )  )     !  =     2  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "Thread . sleep (  2  0  0  )  ;", "}", "hbResponse    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue (  (  ( NodeAction . RESYNC )     !  =     ( hbResponse . getNodeAction (  )  )  )  )  ;", "hbResponse    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertTrue (  (  ( NodeAction . RESYNC )     !  =     ( hbResponse . getNodeAction (  )  )  )  )  ;", "attempt 1     =    loadedApp 1  . getCurrentAppAttempt (  )  ;", "attemptId 1     =    attempt 1  . getAppAttemptId (  )  ;", "rm 2  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "appState    =    rmAppState . get ( loadedApp 1  . getApplicationId (  )  )  ;", "attemptState    =    appState . getAttempt ( attemptId 1  )  ;", "Assert . assertNotNull ( attemptState )  ;", "Assert . assertEquals ( BuilderUtils . newContainerId ( attemptId 1  ,     1  )  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "MockNM   am 1 Node    =    nm 1  ;", "if    ( attemptState . getMasterContainer (  )  . getNodeId (  )  . toString (  )  . contains (  \"  1  2  7  .  0  .  0  .  2  \"  )  )     {", "am 1 Node    =    nm 2  ;", "}", "RMAppAttempt   attempt 2     =    loadedApp 2  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 2     =    attempt 2  . getAppAttemptId (  )  ;", "rm 2  . waitForState ( attemptId 2  ,    RMAppAttemptState . ALLOCATED )  ;", "appState    =    rmAppState . get ( loadedApp 2  . getApplicationId (  )  )  ;", "attemptState    =    appState . getAttempt ( attemptId 2  )  ;", "Assert . assertNotNull ( attemptState )  ;", "Assert . assertEquals ( BuilderUtils . newContainerId ( attemptId 2  ,     1  )  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "MockNM   am 2 Node    =    nm 1  ;", "if    ( attemptState . getMasterContainer (  )  . getNodeId (  )  . toString (  )  . contains (  \"  1  2  7  .  0  .  0  .  2  \"  )  )     {", "am 2 Node    =    nm 2  ;", "}", "am 1     =    rm 2  . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "MockAM   am 2     =    rm 2  . sendAMLaunched ( attempt 2  . getAppAttemptId (  )  )  ;", "am 2  . registerAppAttempt (  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  0  0  ,     3  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "am 2  . allocate (  \"  1  2  7  .  0  .  0  .  2  \"  ,     1  0  0  0  ,     1  ,    new   ArrayList < api . records . ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "conts    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "while    (  ( conts . size (  )  )     =  =     0  )     {", "nm 1  . nodeHeartbeat ( true )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "conts . addAll ( am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < api . records . ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "finishApplicationMaster ( loadedApp 1  ,    rm 2  ,    am 1 Node ,    am 1  )  ;", "finishApplicationMaster ( loadedApp 2  ,    rm 2  ,    am 2 Node ,    am 2  )  ;", "rm 2  . stop (  )  ;", "rm 1  . stop (  )  ;", "Assert . assertEquals (  4  ,    rmAppState . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 0  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 0  . waitForState ( RMAppAttemptState . FAILED )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 0  . getApplicationId (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . FAILED ,    appState . getAttempt ( am 0  . getApplicationAttemptId (  )  )  . getState (  )  )  ;", "Assert . assertNull ( rmAppState . get ( app 0  . getApplicationId (  )  )  . getState (  )  )  ;", "rm 1  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "rm 2  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartAppRunningAMFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 0  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 0  . waitForState ( RMAppAttemptState . FAILED )  ;", "rm 1  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . FAILED )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 0  . getApplicationId (  )  )  ;", "Assert . assertEquals ( RMAppState . FAILED ,    appState . getState (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . FAILED ,    appState . getAttempt ( am 0  . getApplicationAttemptId (  )  )  . getState (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "RMApp   loadedApp 0     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 0  . getApplicationId (  )  )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . FAILED )  ;", "rm 2  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "Assert . assertEquals (  1  ,    loadedApp 0  . getAppAttempts (  )  . size (  )  )  ;", "verifyAppReportAfter ( app 0  ,    rm 2  )  ;", "Assert . assertTrue ( app 0  . getDiagnostics (  )  . toString (  )  . contains (  \" Failing   the   application .  \"  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartFailedApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    null ,    false ,     \" default \"  ,     1  ,    null ,     \" myType \"  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "finishApplicationMaster ( app 0  ,    rm 1  ,    nm 1  ,    am 0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    null ,    false ,     \" default \"  ,     1  ,    null ,     \" myType \"  )  ;", "MockAM   am 1     =    launchAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 1  . waitForState ( RMAppAttemptState . FAILED )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . FAILED )  ;", "RMApp   app 2     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    null ,    false ,     \" default \"  ,     1  ,    null ,     \" myType \"  )  ;", "MockAM   am 2     =    launchAM ( app 2  ,    rm 1  ,    nm 1  )  ;", "rm 1  . killApp ( app 2  . getApplicationId (  )  )  ;", "rm 1  . waitForState ( app 2  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "rm 1  . waitForState ( am 2  . getApplicationAttemptId (  )  ,    RMAppAttemptState . KILLED )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )     {", "@ Override", "protected   RMAppManager   createRMAppManager (  )     {", "return   spy ( super . createRMAppManager (  )  )  ;", "}", "}  ;", "rm 2  . start (  )  ;", "GetApplicationsRequest   request 1     =    GetApplicationsRequest . newInstance ( EnumSet . of ( FINISHED ,    KILLED ,    FAILED )  )  ;", "GetApplicationsResponse   response 1     =    rm 2  . getClientRMService (  )  . getApplications ( request 1  )  ;", "List < ApplicationReport >    appList 1     =    response 1  . getApplicationList (  )  ;", "boolean   forApp 0     =    false ;", "boolean   forApp 1     =    false ;", "boolean   forApp 2     =    false ;", "for    ( ApplicationReport   report    :    appList 1  )     {", "if    ( report . getApplicationId (  )  . equals ( app 0  . getApplicationId (  )  )  )     {", "Assert . assertEquals ( FINISHED ,    report . getYarnApplicationState (  )  )  ;", "forApp 0     =    true ;", "}", "if    ( report . getApplicationId (  )  . equals ( app 1  . getApplicationId (  )  )  )     {", "Assert . assertEquals ( FAILED ,    report . getYarnApplicationState (  )  )  ;", "forApp 1     =    true ;", "}", "if    ( report . getApplicationId (  )  . equals ( app 2  . getApplicationId (  )  )  )     {", "Assert . assertEquals ( KILLED ,    report . getYarnApplicationState (  )  )  ;", "forApp 2     =    true ;", "}", "}", "Assert . assertTrue (  (  ( forApp 0     &  &    forApp 1  )     &  &    forApp 2  )  )  ;", "Set < String >    appTypes    =    new   HashSet < String >  (  )  ;", "appTypes . add (  \" myType \"  )  ;", "GetApplicationsRequest   request 2     =    GetApplicationsRequest . newInstance ( appTypes )  ;", "GetApplicationsResponse   response 2     =    rm 2  . getClientRMService (  )  . getApplications ( request 2  )  ;", "List < ApplicationReport >    appList 2     =    response 2  . getApplicationList (  )  ;", "Assert . assertTrue (  (  3     =  =     ( appList 2  . size (  )  )  )  )  ;", "verify ( rm 2  . getRMAppManager (  )  ,    times (  3  )  )  . logApplicationSummary ( isA ( ApplicationId . class )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartGetApplicationList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "rm 1  . killApp ( app 0  . getApplicationId (  )  )  ;", "rm 1  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "rm 1  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . KILLED )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 0  . getApplicationId (  )  )  ;", "Assert . assertEquals ( RMAppState . KILLED ,    appState . getState (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . KILLED ,    appState . getAttempt ( am 0  . getApplicationAttemptId (  )  )  . getState (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "RMApp   loadedApp 0     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 0  . getApplicationId (  )  )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "rm 2  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . KILLED )  ;", "Assert . assertEquals (  1  ,    loadedApp 0  . getAppAttempts (  )  . size (  )  )  ;", "ApplicationReport   appReport    =    verifyAppReportAfter ( app 0  ,    rm 2  )  ;", "Assert . assertEquals ( app 0  . getDiagnostics (  )  . toString (  )  ,    appReport . getDiagnostics (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartKilledApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )     {", "@ Override", "public   synchronized   void   storeApplicationAttemptStateInternal ( ApplicationAttemptId   attemptId ,    ApplicationAttemptStateData   attemptStateData )    throws   Exception    {", "}", "@ Override", "public   synchronized   void   updateApplicationAttemptStateInternal ( ApplicationAttemptId   attemptId ,    ApplicationAttemptStateData   attemptStateData )    throws   Exception    {", "}", "}  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     (  -  1  )  ,    null ,     \" MAPREDUCE \"  ,    false )  ;", "rm 1  . killApp ( app 0  . getApplicationId (  )  )  ;", "rm 1  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "RMApp   loadedApp 0     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 0  . getApplicationId (  )  )  ;", "rm 2  . waitForState ( loadedApp 0  . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "Assert . assertTrue (  (  ( loadedApp 0  . getAppAttempts (  )  . size (  )  )     =  =     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartKilledAppWithNoAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     1  ,    null )  ;", "RMApp   app 2     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     (  -  1  )  ,    null )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 1  . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "Assert . assertEquals (  0  ,    appState . getAttemptCount (  )  )  ;", "Assert . assertEquals ( appState . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    app 1  . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app 1  . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   attemptId 1     =    attempt . getAppAttemptId (  )  ;", "rm 1  . waitForState ( attemptId 1  ,    RMAppAttemptState . ALLOCATED )  ;", "Assert . assertEquals (  1  ,    appState . getAttemptCount (  )  )  ;", "RMStateStore . ApplicationAttemptState   attemptState    =    appState . getAttempt ( attemptId 1  )  ;", "Assert . assertNotNull ( attemptState )  ;", "Assert . assertEquals ( BuilderUtils . newContainerId ( attemptId 1  ,     1  )  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "conf . setInt ( RM _ AM _ EXPIRY _ INTERVAL _ MS ,     3  0  0  0  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "Assert . assertEquals (  2  ,    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 2  . getApplicationId (  )  )  . getMaxAppAttempts (  )  )  ;", "Assert . assertEquals (  2  ,    rm 2  . getRMContext (  )  . getRMApps (  )  . size (  )  )  ;", "rm 2  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . FAILED )  ;", "rm 2  . waitForState ( app 2  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals ( RMAppState . FAILED ,    rmAppState . get ( app 1  . getApplicationId (  )  )  . getState (  )  )  ;", "Assert . assertNull ( rmAppState . get ( app 2  . getApplicationId (  )  )  . getState (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartOnMaxAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "FinishApplicationMasterRequest   req    =    FinishApplicationMasterRequest . newInstance ( SUCCEEDED ,     \" diagnostics \"  ,     \" trackingUrl \"  )  ;", "finishApplicationMaster ( app 0  ,    rm 1  ,    nm 1  ,    am 0  ,    req )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app 0  . getApplicationId (  )  )  ;", "RMStateStore . ApplicationAttemptState   attemptState 0     =    appState . getAttempt ( am 0  . getApplicationAttemptId (  )  )  ;", "Assert . assertEquals (  \" diagnostics \"  ,    attemptState 0  . getDiagnostics (  )  )  ;", "Assert . assertEquals ( SUCCEEDED ,    attemptState 0  . getFinalApplicationStatus (  )  )  ;", "Assert . assertEquals (  \" trackingUrl \"  ,    attemptState 0  . getFinalTrackingUrl (  )  )  ;", "Assert . assertEquals ( app 0  . getFinishTime (  )  ,    appState . getFinishTime (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "ApplicationReport   appReport    =    verifyAppReportAfter ( app 0  ,    rm 2  )  ;", "Assert . assertEquals ( SUCCEEDED ,    appReport . getFinalApplicationStatus (  )  )  ;", "Assert . assertEquals (  \" trackingUrl \"  ,    appReport . getOriginalTrackingUrl (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartSucceededApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration ( this . conf )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     4  0  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "final   MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  6  3  8  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "MockAM   am 1     =    launchAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 1  . waitForState ( RMAppAttemptState . FAILED )  ;", "MockAM   am 2     =    launchAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "Assert . assertEquals (  1  ,    rmAppState . size (  )  )  ;", "Assert . assertEquals ( app 1  . getState (  )  ,    RMAppState . RUNNING )  ;", "Assert . assertEquals ( app 1  . getAppAttempts (  )  . get ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  . getAppAttemptState (  )  ,    RMAppAttemptState . RUNNING )  ;", "MockRM   rm 2     =    null ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "NodeHeartbeatResponse   res    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . RESYNC ,    res . getNodeAction (  )  )  ;", "RMApp   rmApp    =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "rm 2  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals ( RMAppState . ACCEPTED ,    rmApp . getState (  )  )  ;", "Assert . assertEquals (  2  ,    rmApp . getAppAttempts (  )  . size (  )  )  ;", "rm 2  . waitForState ( am 1  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "rm 2  . waitForState ( am 2  . getApplicationAttemptId (  )  ,    RMAppAttemptState . LAUNCHED )  ;", "Assert . assertEquals ( RMAppAttemptState . FAILED ,    rmApp . getAppAttempts (  )  . get ( am 1  . getApplicationAttemptId (  )  )  . getAppAttemptState (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . LAUNCHED ,    rmApp . getAppAttempts (  )  . get ( am 2  . getApplicationAttemptId (  )  )  . getAppAttemptState (  )  )  ;", "NMContainerStatus   status    =     . createNMContainerStatus ( am 2  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( status )  ,    null )  ;", "rm 2  . waitForState ( am 2  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "launchAM ( rmApp ,    rm 2  ,    nm 1  )  ;", "Assert . assertEquals (  3  ,    rmApp . getAppAttempts (  )  . size (  )  )  ;", "rm 2  . waitForState ( rmApp . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,    RMAppAttemptState . RUNNING )  ;", "conf . setInt ( RM _ AM _ EXPIRY _ INTERVAL _ MS ,     1  0  0  0  0  )  ;", "MockRM   rm 3     =    null ;", "rm 3     =    new   MockRM ( conf ,    memStore )  ;", "rm 3  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 3  . getResourceTrackerService (  )  )  ;", "rmApp    =    rm 3  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "rm 3  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals ( rmApp . getState (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals (  3  ,    rmApp . getAppAttempts (  )  . size (  )  )  ;", "rm 3  . waitForState ( am 1  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "rm 3  . waitForState ( am 2  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "ApplicationAttemptId   latestAppAttemptId    =    rmApp . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ;", "rm 3  . waitForState ( latestAppAttemptId ,    RMAppAttemptState . LAUNCHED )  ;", "Assert . assertEquals ( RMAppAttemptState . FAILED ,    rmApp . getAppAttempts (  )  . get ( am 1  . getApplicationAttemptId (  )  )  . getAppAttemptState (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . FAILED ,    rmApp . getAppAttempts (  )  . get ( am 2  . getApplicationAttemptId (  )  )  . getAppAttemptState (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . LAUNCHED ,    rmApp . getAppAttempts (  )  . get ( latestAppAttemptId )  . getAppAttemptState (  )  )  ;", "rm 3  . waitForState ( latestAppAttemptId ,    RMAppAttemptState . FAILED )  ;", "rm 3  . waitForState ( rmApp . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals (  4  ,    rmApp . getAppAttempts (  )  . size (  )  )  ;", "Assert . assertEquals ( RMAppAttemptState . FAILED ,    rmApp . getAppAttempts (  )  . get ( latestAppAttemptId )  . getAppAttemptState (  )  )  ;", "latestAppAttemptId    =    rmApp . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ;", "RMApp   app 2     =    rm 3  . submitApp (  2  0  0  )  ;", "rm 3  . waitForState ( app 2  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals (  1  ,    app 2  . getAppAttempts (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    memStore . getState (  )  . getApplicationState (  )  . get ( app 2  . getApplicationId (  )  )  . getAttemptCount (  )  )  ;", "MockRM   rm 4     =    null ;", "rm 4     =    new   MockRM ( conf ,    memStore )  ;", "rm 4  . start (  )  ;", "rmApp    =    rm 4  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "rm 4  . waitForState ( rmApp . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  ( rmApp . getAppAttempts (  )  . size (  )  )     !  =     2  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "Thread . sleep (  2  0  0  )  ;", "}", "Assert . assertEquals (  4  ,    rmApp . getAppAttempts (  )  . size (  )  )  ;", "Assert . assertEquals ( RMAppState . ACCEPTED ,    rmApp . getState (  )  )  ;", "rm 4  . waitForState ( latestAppAttemptId ,    RMAppAttemptState . SCHEDULED )  ;", "Assert . assertEquals ( RMAppAttemptState . SCHEDULED ,    rmApp . getAppAttempts (  )  . get ( latestAppAttemptId )  . getAppAttemptState (  )  )  ;", "app 2     =    rm 4  . getRMContext (  )  . getRMApps (  )  . get ( app 2  . getApplicationId (  )  )  ;", "rm 4  . waitForState ( app 2  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "Assert . assertEquals ( RMAppState . ACCEPTED ,    app 2  . getState (  )  )  ;", "Assert . assertEquals (  1  ,    app 2  . getAppAttempts (  )  . size (  )  )  ;", "rm 4  . waitForState ( app 2  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,    RMAppAttemptState . SCHEDULED )  ;", "Assert . assertEquals ( RMAppAttemptState . SCHEDULED ,    app 2  . getCurrentAppAttempt (  )  . getAppAttemptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartWaitForPreviousAMToFinish"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )     {", "int   count    =     0  ;", "@ Override", "public   void   updateApplicationStateInternal ( ApplicationId   appId ,    ApplicationStateData   appStateData )    throws   Exception    {", "if    (  ( count )     =  =     0  )     {", "RMStateStore . LOG . info (  ( appId    +     \"    final   state   is   not   saved .  \"  )  )  ;", "( count )  +  +  ;", "} else    {", "super . updateApplicationStateInternal ( appId ,    appStateData )  ;", "}", "}", "}  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    rmState . getApplicationState (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    rm 1  . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    MockRM . launchAndRegisterAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "FinishApplicationMasterRequest   req    =    FinishApplicationMasterRequest . newInstance ( SUCCEEDED ,     \"  \"  ,     \"  \"  )  ;", "am 0  . unregisterAppAttempt ( req ,    true )  ;", "am 0  . waitForState ( RMAppAttemptState . FINISHING )  ;", "Assert . assertNull ( rmAppState . get ( app 0  . getApplicationId (  )  )  . getState (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "nm 1  . setRTrackerService ( rm 2  . getRTrackerService (  )  )  ;", "rm 2  . start (  )  ;", "rm 2  . waitForState ( app 0  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,    RMAppAttemptState . FINISHED )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "Assert . assertEquals ( RMAppState . FINISHED ,    rmAppState . get ( app 0  . getApplicationId (  )  )  . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartWaitForPreviousSucceededAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )     {", "@ Override", "public   synchronized   void   checkVersion (  )    throws   Exception    {", "throw   new   Exception (  \" Invalid   version .  \"  )  ;", "}", "}  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    null ;", "try    {", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  .  (  )  ;", "Assert . fail (  )  ;", "}    catch    ( Exception   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" Invalid   version .  \"  )  )  ;", "}", "Assert . assertTrue (  (  ( rm 1  . getServiceState (  )  )     =  =     ( STATE . STOPPED )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMShutdown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )     {", "volatile   boolean   wait    =    true ;", "@ Override", "public   void   serviceStop (  )    throws   Exception    {", "wait    =    false ;", "super . serviceStop (  )  ;", "}", "@ Override", "protected   void   handleStoreEvent ( RMStateStoreEvent   event )     {", "while    ( wait )  ;", "super . handleStoreEvent ( event )  ;", "}", "}  ;", "memStore . init ( conf )  ;", "final   MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "final   ArrayList < RMApp >    appList    =    new   ArrayList < RMApp >  (  )  ;", "final   int   NUM _ APPS    =     5  ;", "for    ( int   i    =     0  ;    i    <    NUM _ APPS ;    i +  +  )     {", "RMApp   app    =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     (  -  1  )  ,    null ,     \" MAPREDUCE \"  ,    false )  ;", "appList . add ( app )  ;", "rm 1  . waitForState ( app . getApplicationId (  )  ,    RMAppState . NEW _ SAVING )  ;", "}", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    memStore . getState (  )  . getApplicationState (  )  ;", "Assert . assertTrue (  (  ( rmAppState . size (  )  )     =  =     0  )  )  ;", "rm 1  . stop (  )  ;", "for    ( RMApp   app    :    appList )     {", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( app . getApplicationId (  )  )  ;", "Assert . assertNotNull ( appState )  ;", "Assert . assertEquals (  0  ,    appState . getAttemptCount (  )  )  ;", "Assert . assertEquals ( appState . getApplicationSubmissionContext (  )  . getApplicationId (  )  ,    app . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "}", "Assert . assertTrue (  (  ( rmAppState . size (  )  )     =  =    NUM _ APPS )  )  ;", "}", "METHOD_END"], "methodName": ["testRMStateStoreDispatcherDrainedOnRMStop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "final   MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "final   MockAM   am 0     =    MockRM . launchAndRegisterAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )     {", "@ Override", "protected   ResourceTrackerService   createResourceTrackerService (  )     {", "return   new   ResourceTrackerService ( this . rmContext ,    this . nodesListManager ,    this . nmLivelinessMonitor ,    this . rmContext . getContainerTokenSecretManager (  )  ,    this . rmContext . getNMTokenSecretManager (  )  )     {", "@ Override", "protected   void   serviceStart (  )    throws   Exception    {", "super . serviceStart (  )  ;", "nm 1  . setResourceTrackerService ( getResourceTrackerService (  )  )  ;", "NMContainerStatus   status    =     . createNMContainerStatus ( am 0  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( status )  ,    null )  ;", "}", "}  ;", "}", "}  ;", "rm 2  . start (  )  ;", "RMApp   loadedApp 0     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 0  . getApplicationId (  )  )  ;", "int   timeoutSecs    =     0  ;", "while    (  (  ( loadedApp 0  . getAppAttempts (  )  . size (  )  )     !  =     2  )     &  &     (  ( timeoutSecs +  +  )     <     4  0  )  )     {", "Thread . sleep (  2  0  0  )  ;", "}", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( loadedApp 0  ,    rm 2  ,    nm 1  )  ;", "MockRM . finishAMAndVerifyAppState ( loadedApp 0  ,    rm 2  ,    nm 1  ,    am 1  )  ;", "}", "METHOD_END"], "methodName": ["testSynchronouslyRenewDTOnRecovery"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "GetApplicationReportRequest   reportRequest    =    GetApplicationReportRequest . newInstance ( app . getApplicationId (  )  )  ;", "GetApplicationReportResponse   response    =    rm . getClientRMService (  )  . getApplicationReport ( reportRequest )  ;", "ApplicationReport   report    =    response . getApplicationReport (  )  ;", "Assert . assertEquals ( app . getSTime (  )  ,    report . getSTime (  )  )  ;", "Assert . assertEquals ( app . getFinishTime (  )  ,    report . getFinishTime (  )  )  ;", "Assert . assertEquals ( app . createApplicationState (  )  ,    report . getYarnApplicationState (  )  )  ;", "Assert . assertTrue (  (  1     =  =     ( report . getProgress (  )  )  )  )  ;", "return   response . getApplicationReport (  )  ;", "}", "METHOD_END"], "methodName": ["verifyAppReportAfterRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "int   waitCnt    =     2  0  ;", "boolean   atleastOneAppInNEWState    =    true ;", "while    (  (  ( waitCnt -  -  )     >     0  )     &  &    atleastOneAppInNEWState )     {", "atleastOneAppInNEWState    =    false ;", "for    ( RMApp   rmApp    :    rm 2  . geContext (  )  . geApps (  )  . values (  )  )     {", "if    (  ( rmApp . getState (  )  )     =  =     ( RMAppState . NEW )  )     {", "Thread . sleep (  1  0  0  0  )  ;", "atleastOneAppInNEWState    =    true ;", "break ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["waitForTokensToBeRenewed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( hostFile . exists (  )  )  )     {", ". TEMP _ DIR . mkdirs (  )  ;", "hostFile . createNewFile (  )  ;", "}", "FileOutputStream   fStream    =    null ;", "try    {", "fStream    =    new   FileOutputStream ( hostFile )  ;", "for    ( int   i    =     0  ;    i    <     ( hosts . length )  ;    i +  +  )     {", "fStream . write ( hosts [ i ]  . getBytes (  )  )  ;", "fStream . write ( System . getProperty (  \" line . separator \"  )  . getBytes (  )  )  ;", "}", "}    finally    {", "if    ( fStream    !  =    null )     {", "IOUtils . closeStream ( fStream )  ;", "fStream    =    null ;", "}", "}", "}", "METHOD_END"], "methodName": ["writeToHostsFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart"}, {"methodBody": ["METHOD_START", "{", "for    ( NodeManager   nodeManager    :    nodes )     {", "nodeManager . checkResourceUsage (  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "RMNode   node    =    resourceManager . getRMContext (  )  . getRMNodes (  )  . get ( nm 1  . getNodeId (  )  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate    =    new   NodeUpdateSchedulerEvent ( node )  ;", "resourceManager . getResourceScheduler (  )  . handle ( nodeUpdate )  ;", "}", "METHOD_END"], "methodName": ["nodeUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "NodeManager   nm    =    new   NodeManager ( hostName ,    containerManagerPort ,    httpPort ,    rackName ,    capability ,    resourceManager )  ;", "NodeAddedSchedulerEvent   nodeAddEvent 1     =    new   NodeAddedSchedulerEvent ( resourceManager . getRMContext (  )  . getRMNodes (  )  . get ( nm . getNodeId (  )  )  )  ;", "resourceManager . getResourceScheduler (  )  . handle ( nodeAddEvent 1  )  ;", "return   nm ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "resourceManager    =    new    (  )  ;", "resourceManager . init ( conf )  ;", "resourceManager . getRMContext (  )  . getContainerTokenSecretManager (  )  . rollMasterKey (  )  ;", "resourceManager . getRMContext (  )  . getNMTokenSecretManager (  )  . rollMasterKey (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "String   filterInitializerConfKey    =     \" hadoop . http . filter . initializers \"  ;", "String [  ]    filterInitializers    =    new   String [  ]  {    AuthenticationFilterInitializer . class . getName (  )  ,    RMAuthenticationFilterInitializer . class . getName (  )  ,     (  ( AuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,  \"  )     +     ( RMAuthenticationFilterInitializer . class . getName (  )  )  ,     (  ( AuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,     \"  )     +     ( RMAuthenticationFilterInitializer . class . getName (  )  )  ,     (  ( AuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,     \"  )     +     ( this . getClass (  )  . getName (  )  )     }  ;", "for    ( String   filterInitializer    :    filterInitializers )     {", "resourceManager    =    new   ResourceManager (  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( filterInitializerConfKey ,    filterInitializer )  ;", "conf . set (  \" hadoop . security . authentication \"  ,     \" kerberos \"  )  ;", "conf . set (  \" hadoop . http . authentication . type \"  ,     \" kerberos \"  )  ;", "try    {", "try    {", "UserGroupInformation . setConfiguration ( conf )  ;", "}    catch    ( Exception   e )     {", ". LOG . info (  \" Got   expected   exception \"  )  ;", "}", "resourceManager . init ( conf )  ;", "resourceManager . startWepApp (  )  ;", "}    catch    ( RuntimeException   e )     {", "String   tmp    =    resourceManager . getConfig (  )  . get ( filterInitializerConfKey )  ;", "if    ( filterInitializer . contains ( this . getClass (  )  . getName (  )  )  )     {", "Assert . assertEquals (  (  (  ( RMAuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,  \"  )     +     ( this . getClass (  )  . getName (  )  )  )  ,    tmp )  ;", "} else    {", "Assert . assertEquals ( RMAuthenticationFilterInitializer . class . getName (  )  ,    tmp )  ;", "}", "resourceManager . stop (  )  ;", "}", "}", "String [  ]    simpleFilterInitializers    =    new   String [  ]  {     \"  \"  ,    StaticUserWebFilter . class . getName (  )     }  ;", "for    ( String   filterInitializer    :    simpleFilterInitializers )     {", "resourceManager    =    new   ResourceManager (  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( filterInitializerConfKey ,    filterInitializer )  ;", "try    {", "UserGroupInformation . setConfiguration ( conf )  ;", "resourceManager . init ( conf )  ;", "resourceManager . startWepApp (  )  ;", "}    catch    ( RuntimeException   e )     {", "String   tmp    =    resourceManager . getConfig (  )  . get ( filterInitializerConfKey )  ;", "if    ( filterInitializer . equals ( StaticUserWebFilter . class . getName (  )  )  )     {", "Assert . assertEquals (  (  (  ( RMAuthenticationFilterInitializer . class . getName (  )  )     +     \"  ,  \"  )     +     ( StaticUserWebFilter . class . getName (  )  )  )  ,    tmp )  ;", "} else    {", "Assert . assertEquals ( RMAuthenticationFilterInitializer . class . getName (  )  ,    tmp )  ;", "}", "resourceManager . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFilterOverrides"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setLong ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,     1  0  0  0  )  ;", "conf . setLong ( RM _ NM _ HEARTBEAT _ INTERVAL _ MS ,     1  0  0  1  )  ;", "resourceManager    =    new    (  )  ;", "try    {", "resourceManager . init ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "if    (  !  ( e . getMessage (  )  . startsWith (  (  \" Nodemanager   expiry   interval   should   be   no \"     +     \"    less   than   heartbeat   interval \"  )  )  )  )     {", "throw   e ;", "}", "}", "}", "METHOD_END"], "methodName": ["testNMExpiryAndHeartbeatIntervalsValidation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "String   host 1     =     \" host 1  \"  ;", "final   int   memory    =     4     *     1  0  2  4  ;", "NodeManager   nm 1     =    registerNode ( host 1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource ( memory ,     1  )  )  ;", "nm 1  . heartbeat (  )  ;", "nm 1  . heartbeat (  )  ;", "Collection < RMNode >    values    =    r . getRMContext (  )  . getRMNodes (  )  . values (  )  ;", "for    ( RMNode   ni    :    values )     {", "assertNotNull ( ni . getHealthReport (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodeHealthReportIsNotNull"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "TestResourceManager . LOG . info (  \"  -  -  -    START :    testResourceAllocation    -  -  -  \"  )  ;", "final   int   memory    =     4     *     1  0  2  4  ;", "final   int   vcores    =     4  ;", "String   host 1     =     \" host 1  \"  ;", "NodeManager   nm 1     =    registerNode ( host 1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource ( memory ,    vcores )  )  ;", "String   host 2     =     \" host 2  \"  ;", "NodeManager   nm 2     =    registerNode ( host 2  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  ( memory    /     2  )  ,     ( vcores    /     2  )  )  )  ;", "Application   application    =    new   Application (  \" user 1  \"  ,    resourceManager )  ;", "application . submit (  )  ;", "application . addNodeManager ( host 1  ,     1  2  3  4  ,    nm 1  )  ;", "application . addNodeManager ( host 2  ,     1  2  3  4  ,    nm 2  )  ;", "final   int   memory 1     =     1  0  2  4  ;", "Resource   capability 1     =    Resources . createResource ( memory 1  ,     1  )  ;", "Priority   priority 1     =    resource . Priority . create (  1  )  ;", "application . addResourceRequestSpec ( priority 1  ,    capability 1  )  ;", "Task   t 1     =    new   Task ( application ,    priority 1  ,    new   String [  ]  {    host 1  ,    host 2     }  )  ;", "application . addTask ( t 1  )  ;", "final   int   memory 2     =     2  0  4  8  ;", "Resource   capability 2     =    Resources . createResource ( memory 2  ,     1  )  ;", "Priority   priority 0     =    resource . Priority . create (  0  )  ;", "application . addResourceRequestSpec ( priority 0  ,    capability 2  )  ;", "application . schedule (  )  ;", "nodeUpdate ( nm 1  )  ;", "application . schedule (  )  ;", "checkResourceUsage ( nm 1  ,    nm 2  )  ;", "TestResourceManager . LOG . info (  \" Adding   new   tasks .  .  .  \"  )  ;", "Task   t 2     =    new   Task ( application ,    priority 1  ,    new   String [  ]  {    host 1  ,    host 2     }  )  ;", "application . addTask ( t 2  )  ;", "Task   t 3     =    new   Task ( application ,    priority 0  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application . addTask ( t 3  )  ;", "application . schedule (  )  ;", "checkResourceUsage ( nm 1  ,    nm 2  )  ;", "nodeUpdate ( nm 2  )  ;", "nodeUpdate ( nm 2  )  ;", "nodeUpdate ( nm 1  )  ;", "nodeUpdate ( nm 1  )  ;", "TestResourceManager . LOG . info (  \" Trying   to   allocate .  .  .  \"  )  ;", "application . schedule (  )  ;", "checkResourceUsage ( nm 1  ,    nm 2  )  ;", "TestResourceManager . LOG . info (  \" Finishing   up   tasks .  .  .  \"  )  ;", "application . finishTask ( t 1  )  ;", "application . finishTask ( t 2  )  ;", "application . finishTask ( t 3  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 1     =    new   AppAttemptRemovedSchedulerEvent ( application . getApplicationAttemptId (  )  ,    RMAppAttemptState . FINISHED ,    false )  ;", "resourceManager . getResourceScheduler (  )  . handle ( appRemovedEvent 1  )  ;", "checkResourceUsage ( nm 1  ,    nm 2  )  ;", "TestResourceManager . LOG . info (  \"  -  -  -    END :    testResourceAllocation    -  -  -  \"  )  ;", "}", "METHOD_END"], "methodName": ["testResourceAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     (  -  1  )  )  ;", "resourceManager    =    new    (  )  ;", "try    {", "resourceManager . init ( conf )  ;", "fail (  (  \" Exception   is   expected   because   the   global   max   attempts \"     +     \"    is   negative .  \"  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "if    (  !  ( e . getMessage (  )  . startsWith (  \" Invalid   global   max   attempts   configuration \"  )  )  )", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["testResourceManagerInitConfigValidation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager"}, {"methodBody": ["METHOD_START", "{", "int   waitCount    =     0  ;", "while    (  (  ( ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )     !  =    count )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", "synchronized ( this )     {", "wait (  1  0  0  )  ;", "}", "}", "Ast . astEquals ( count ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "Ast . astEquals (  \" The   decommisioned   metrics   are   not   updated \"  ,    count ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkDecommissionedNMCount"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "int   waitCount    =     0  ;", "while    (  (  ( ClusterMetrics . getMetrics (  )  . getNumRebootedNMs (  )  )     !  =    count )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", "synchronized ( this )     {", "wait (  1  0  0  )  ;", "}", "}", "Ast . astEquals (  \" The   rebooted   metrics   are   not   updated \"  ,    count ,    ClusterMetrics . getMetrics (  )  . getNumRebootedNMs (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkRebootedNMCount"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "int   waitCount    =     0  ;", "while    (  (  (  ( rm . getRMContext (  )  . getRMNodes (  )  . get ( nm 1  . getNodeId (  )  )  . getState (  )  )     !  =     ( NodeState . UNHEALTHY )  )     =  =    health )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", "synchronized ( this )     {", "wait (  1  0  0  )  ;", "}", "}", "Ast . astFalse (  (  (  ( rm . getRMContext (  )  . getRMNodes (  )  . get ( nm 1  . getNodeId (  )  )  . getState (  )  )     !  =     ( NodeState . UNHEALTHY )  )     =  =    health )  )  ;", "Ast . astEquals (  \" Unhealthy   metrics   not   incremented \"  ,    count ,    ClusterMetrics . getMetrics (  )  . getUnhealthyNMs (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkUnealthyNMCount"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( hostFile )     !  =    null )     &  &     ( hostFile . exists (  )  )  )     {", "hostFile . delete (  )  ;", "}", "ClusterMetrics . destroy (  )  ;", "if    (  ( rm )     !  =    null )     {", "rm . stop (  )  ;", "}", "MetricsSystem   ms    =    DefaultMetricsSystem . instance (  )  ;", "if    (  ( ms . getS (  \" ClusterMetrics \"  )  )     !  =    null )     {", "DefaultMetricsSystem . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "ClusterMetrics   metrics    =    ClusterMetrics . getMetrics (  )  ;", "assert   metrics    !  =    null ;", "int   initialMetricCount    =    metrics . getNumDecommisionedNMs (  )  ;", "NodeHeartbeatResponse   nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . NORMAL ,    nodeHeartbeat . getNodeAction (  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . NORMAL ,    nodeHeartbeat . getNodeAction (  )  )  ;", "writeToHostsFile (  \" host 2  \"  )  ;", "conf . set ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "rm . getNodesListM (  )  . refreshNodes ( conf )  ;", "nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals (  \" Node   should   not   have   been   decomissioned .  \"  ,    NodeAction . NORMAL ,    nodeHeartbeat . getNodeAction (  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertEquals (  (  \" Node   should   have   been   decomissioned   but   is   in   state \"     +     ( nodeHeartbeat . getNodeAction (  )  )  )  ,    NodeAction . SHUTDOWN ,    nodeHeartbeat . getNodeAction (  )  )  ;", "checkDecommissionedNMCount ( rm ,     (  +  + initialMetricCount )  )  ;", "}", "METHOD_END"], "methodName": ["testAddNewExcludePathToConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "ClusterMetrics   metrics    =    ClusterMetrics . getMetrics (  )  ;", "assert   metrics    !  =    null ;", "int   initialMetricCount    =    metrics . getNumDecommisionedNMs (  )  ;", "NodeHeartbeatResponse   nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . NORMAL ,    nodeHeartbeat . getNodeAction (  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertEquals ( NodeAction . NORMAL ,    nodeHeartbeat . getNodeAction (  )  )  ;", "writeToHostsFile (  \" host 1  \"  )  ;", "conf . set ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "rm . getNodesListM (  )  . refreshNodes ( conf )  ;", "nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals (  \" Node   should   not   have   been   decomissioned .  \"  ,    NodeAction . NORMAL ,    nodeHeartbeat . getNodeAction (  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertEquals (  (  \" Node   should   have   been   decomissioned   but   is   in   state \"     +     ( nodeHeartbeat . getNodeAction (  )  )  )  ,    NodeAction . SHUTDOWN ,    nodeHeartbeat . getNodeAction (  )  )  ;", "checkDecommissionedNMCount ( rm ,     (  +  + initialMetricCount )  )  ;", "}", "METHOD_END"], "methodName": ["testAddNewIncludePathToConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "writeToHostsFile (  \"  \"  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "MockNM   nm 3     =    rm . registerNode (  \" localhost :  4  4  3  3  \"  ,     1  0  2  4  )  ;", "int   metricCount    =    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  ;", "NodeHeartbeatResponse   nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "String   ip    =    NetUtils . normalizeHostName (  \" localhost \"  )  ;", "writeToHostsFile (  \" host 2  \"  ,    ip )  ;", "rm . getNodesListM (  )  . refreshNodes ( conf )  ;", "checkDecommissionedNMCount ( rm ,     ( metricCount    +     2  )  )  ;", "nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertTrue (  \" The   decommisioned   metrics   are   not   updated \"  ,    NodeAction . SHUTDOWN . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "nodeHeartbeat    =    nm 3  . nodeHeartbeat ( true )  ;", "Assert . assertTrue (  \" The   decommisioned   metrics   are   not   updated \"  ,    NodeAction . SHUTDOWN . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDecommissionWithExcludeHosts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "writeToHostsFile (  \" localhost \"  ,     \" host 1  \"  ,     \" host 2  \"  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "MockNM   nm 3     =    rm . registerNode (  \" localhost :  4  4  3  3  \"  ,     1  0  2  4  )  ;", "ClusterMetrics   metrics    =    ClusterMetrics . getMetrics (  )  ;", "assert   metrics    !  =    null ;", "int   metricCount    =    metrics . getNumDecommisionedNMs (  )  ;", "NodeHeartbeatResponse   nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "nodeHeartbeat    =    nm 3  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "String   ip    =    NetUtils . normalizeHostName (  \" localhost \"  )  ;", "writeToHostsFile (  \" host 1  \"  ,    ip )  ;", "rm . getNodesListM (  )  . refreshNodes ( conf )  ;", "nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "Assert . assertEquals (  0  ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertTrue (  \" Node   is   not   decommisioned .  \"  ,    NodeAction . SHUTDOWN . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "checkDecommissionedNMCount ( rm ,     (  +  + metricCount )  )  ;", "nodeHeartbeat    =    nm 3  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "Assert . assertEquals ( metricCount ,    ClusterMetrics . getMetrics (  )  . getNumDecommisionedNMs (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDecommissionWithIncludeHosts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ NM _ HEARTBEAT _ INTERVAL _ MS ,     \"  4  0  0  0  \"  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "NodeHeartbeaponse   nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertEquals (  4  0  0  0  ,    nodeHeartbeat . getNextHeartBeatInterval (  )  )  ;", "NodeHeartbeaponse   nodeHeartbeat 2     =    nm 2  . nodeHeartbeat ( true )  ;", "Assert . assertEquals (  4  0  0  0  ,    nodeHeartbeat 2  . getNextHeartBeatInterval (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetNextHeartBeatInterval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "rm    =    new   MockRM ( new   YarnConfiguration (  )  )  ;", "rm . start (  )  ;", "EventHandler   handler    =    spy ( rm . getRMContext (  )  . getDispatcher (  )  . getEventHandler (  )  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  ,    true )  ;", "NMContainerStatus   report    =    NMContainerStatus . newInstance ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( app . getApplicationId (  )  ,     2  )  ,     1  )  ,    COMPLETE ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" Dummy   Completed \"  ,     0  ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "rm . ge (  )  . handleNMContainerStatus ( report )  ;", "verify ( handler ,    never (  )  )  . handle (  (  ( Event )     ( any (  )  )  )  )  ;", "RMAppAttemptImpl   currentAttempt    =     (  ( RMAppAttemptImpl )     ( app . getCurrentAppAttempt (  )  )  )  ;", "currentAttempt . setMasterContainer ( null )  ;", "report    =    NMContainerStatus . newInstance ( ContainerId . newInstance ( currentAttempt . getAppAttemptId (  )  ,     0  )  ,    COMPLETE ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" Dummy   Completed \"  ,     0  ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "rm . ge (  )  . handleNMContainerStatus ( report )  ;", "verify ( handler ,    never (  )  )  . handle (  (  ( Event )     ( any (  )  )  )  )  ;", "app    =    rm . submitApp (  1  0  2  4  )  ;", "report    =    NMContainerStatus . newInstance ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( app . getApplicationId (  )  ,     2  )  ,     1  )  ,    COMPLETE ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" Dummy   Completed \"  ,     0  ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "try    {", "rm . ge (  )  . handleNMContainerStatus ( report )  ;", "}    catch    ( Exception   e )     {", "}", "verify ( handler ,    never (  )  )  . handle (  (  ( Event )     ( any (  )  )  )  )  ;", "currentAttempt    =     (  ( RMAppAttemptImpl )     ( app . getCurrentAppAttempt (  )  )  )  ;", "currentAttempt . setMasterContainer ( null )  ;", "report    =    NMContainerStatus . newInstance ( ContainerId . newInstance ( currentAttempt . getAppAttemptId (  )  ,     0  )  ,    COMPLETE ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" Dummy   Completed \"  ,     0  ,    Priority . newInstance (  1  0  )  ,     1  2  3  4  )  ;", "try    {", "rm . ge (  )  . handleNMContainerStatus ( report )  ;", "}    catch    ( Exception   e )     {", "}", "verify ( handler ,    never (  )  )  . handle (  (  ( Event )     ( any (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testHandleContainerStatusInvalidCompletions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "writeToHostsFile (  \" host 1  \"  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "ResourceTrackerService   resourceTrackerService    =    rm . ge (  )  ;", "RegisterNodeManagerRequest   req    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId    =    NodeId . newInstance (  \" host 2  \"  ,     1  2  3  4  )  ;", "req . setNodeId ( nodeId )  ;", "req . setHttpPort (  1  2  3  4  )  ;", "RegisterNodeManagerResponse   response    =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . SHUTDOWN ,    response . getNodeAction (  )  )  ;", "Assert . assertEquals (  \" Disallowed   NodeManager   from      host 2  ,    Sending   SHUTDOWN   signal   to   the   NodeManager .  \"  ,    response . getDiagnosticsMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeRegistrationFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "writeToHostsFile (  \" host 2  \"  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "ResourceTrackerService   resourceTrackerService    =    rm . ge (  )  ;", "RegisterNodeManagerRequest   req    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId    =    NodeId . newInstance (  \" host 2  \"  ,     1  2  3  4  )  ;", "Resource   capability    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "req . setResource ( capability )  ;", "req . setNodeId ( nodeId )  ;", "req . setHttpPort (  1  2  3  4  )  ;", "req . setNMVersion ( YarnVersionInfo . getVersion (  )  )  ;", "RegisterNodeManagerResponse   response    =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . NORMAL ,    response . getNodeAction (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeRegistrationSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "writeToHostsFile (  \" host 2  \"  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ NODES _ INCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "conf . set ( RM _ NODEMANAGER _ MINIMUM _ VERSION ,     \" EqualToRM \"  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "String   nmVersion    =     \"  1  .  9  .  9  \"  ;", "ResourceTrackerService   resourceTrackerService    =    rm . ge (  )  ;", "RegisterNodeManagerRequest   req    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId    =    NodeId . newInstance (  \" host 2  \"  ,     1  2  3  4  )  ;", "Resource   capability    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "req . setResource ( capability )  ;", "req . setNodeId ( nodeId )  ;", "req . setHttpPort (  1  2  3  4  )  ;", "req . setNMVersion ( nmVersion )  ;", "RegisterNodeManagerResponse   response    =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . SHUTDOWN ,    response . getNodeAction (  )  )  ;", "Assert . assertTrue (  (  (  (  \" Diagnostic   message   did   not   contain :     ' Disallowed   NodeManager    \"     +     \" Version    \"  )     +    nmVersion )     +     \"  ,    is   less   than   the   minimum   version '  \"  )  ,    response . getDiagnosticsMessage (  )  . contains (  (  (  \" Disallowed   NodeManager   Version    \"     +    nmVersion )     +     \"  ,    is   less   than   the   minimum   version    \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeRegistrationVersionLessThanRM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     \"  2  0  4  8  \"  )  ;", "conf . set ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,     \"  4  \"  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "ResourceTrackerService   resourceTrackerService    =    rm . ge (  )  ;", "RegisterNodeManagerRequest   req    =    Records . newRecord ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" host \"  ,     1  2  3  4  )  ;", "req . setNodeId ( nodeId )  ;", "Resource   capability    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "req . setResource ( capability )  ;", "RegisterNodeManagerResponse   response 1     =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . SHUTDOWN ,    response 1  . getNodeAction (  )  )  ;", "capability . setMemory (  2  0  4  8  )  ;", "capability . setVirtualCores (  1  )  ;", "req . setResource ( capability )  ;", "RegisterNodeManagerResponse   response 2     =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . SHUTDOWN ,    response 2  . getNodeAction (  )  )  ;", "capability . setMemory (  1  0  2  4  )  ;", "capability . setVirtualCores (  4  )  ;", "req . setResource ( capability )  ;", "RegisterNodeManagerResponse   response 3     =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . SHUTDOWN ,    response 3  . getNodeAction (  )  )  ;", "capability . setMemory (  2  0  4  8  )  ;", "capability . setVirtualCores (  4  )  ;", "req . setResource ( capability )  ;", "RegisterNodeManagerResponse   response 4     =    resourceTrackerService . registerNodeManager ( req )  ;", "Assert . assertEquals ( NodeAction . NORMAL ,    response 4  . getNodeAction (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeRegistrationWithMinimumAllocations"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "int   initialMetricCount    =    ClusterMetrics . getMetrics (  )  . getNumRebootedNMs (  )  ;", "NodeHeartbeatResponse   nodeHeartbeat    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "nodeHeartbeat    =    nm 2  . nodeHeartbeat ( new   HashMap < api . records . ApplicationId ,    List < api . records . ContainerStatus >  >  (  )  ,    true ,     (  -  1  0  0  )  )  ;", "Assert . assertTrue ( NodeAction . RESYNC . equals ( nodeHeartbeat . getNodeAction (  )  )  )  ;", "Assert . assertEquals (  \" Too   far   behind   rm   response   id :  0    nm   response   id :  -  1  0  0  \"  ,    nodeHeartbeat . getDiagnosticsMessage (  )  )  ;", "checkRebootedNMCount ( rm ,     (  +  + initialMetricCount )  )  ;", "}", "METHOD_END"], "methodName": ["testReboot"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "final   DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "rm    =    new   MockRM (  )     {", "@ Override", "protected   EventHandler < SchedulerEvent >    createSchedulerEventDispatcher (  )     {", "return   new   ResourceManager . SchedulerEventDispatcher ( this . scheduler )     {", "@ Override", "public   void   handle ( SchedulerEvent   event )     {", "this . scheduler . handle ( event )  ;", "}", "}  ;", "}", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "}  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     5  1  2  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "nm 2  . nodeHeartbeat ( false )  ;", "dispatcher . await (  )  ;", "checkUnealthyNMCount ( rm ,    nm 2  ,    true ,     1  )  ;", "final   int   expectedNMs    =    ClusterMetrics . getMetrics (  )  . getNumActiveNMs (  )  ;", "QueueMetrics   metrics    =    rm . geScheduler (  )  . getRootQueueMetrics (  )  ;", "Assert . assertEquals (  5  1  2  0  ,    metrics . getAvailableMB (  )  )  ;", "nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "NodeHeartbeatResponse   response    =    nm 1  . nodeHeartbeat ( true )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( response . getNodeAction (  )  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals ( expectedNMs ,    ClusterMetrics . getMetrics (  )  . getNumActiveNMs (  )  )  ;", "checkUnealthyNMCount ( rm ,    nm 2  ,    true ,     1  )  ;", "nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     5  1  2  0  )  ;", "response    =    nm 2  . nodeHeartbeat ( false )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( response . getNodeAction (  )  )  )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals ( expectedNMs ,    ClusterMetrics . getMetrics (  )  . getNumActiveNMs (  )  )  ;", "checkUnealthyNMCount ( rm ,    nm 2  ,    true ,     1  )  ;", "nm 2     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     5  1  2  0  )  ;", "dispatcher . await (  )  ;", "response    =    nm 2  . nodeHeartbeat ( true )  ;", "response    =    nm 2  . nodeHeartbeat ( true )  ;", "dispatcher . await (  )  ;", "Assert . assertEquals (  (  5  1  2  0     +     5  1  2  0  )  ,    metrics . getAvailableMB (  )  )  ;", "nm 1     =    rm . registerNode (  \" host 2  :  5  6  7  8  \"  ,     1  0  2  4  0  )  ;", "dispatcher . await (  )  ;", "response    =    nm 1  . nodeHeartbeat ( true )  ;", "dispatcher . await (  )  ;", "Assert . assertTrue ( NodeAction . NORMAL . equals ( response . getNodeAction (  )  )  )  ;", "Assert . assertEquals (  (  5  1  2  0     +     1  0  2  4  0  )  ,    metrics . getAvailableMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReconnectNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm    =    new   MockNM (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  ,    rm . ge (  )  )  ;", "RegisterNodeManagerResponse   response    =    nm . registerNode (  )  ;", "Assert . assertEquals ( ResourceManager . getClusterTimeStamp (  )  ,    response . getRMIdentifier (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSetRMIdentifierInRegistration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conft ( RM _ NODES _ EXCLUDE _ FILE _ PATH ,    hostFile . getAbsolutePath (  )  )  ;", "rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" host 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "Art . artEquals (  0  ,    ClusterMetrics . getMetrics (  )  . getUnhealthyNMs (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "nm 1  . nodeHeartbeat ( fa )  ;", "checkUnealthyNMCount ( rm ,    nm 1  ,    true ,     1  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "checkUnealthyNMCount ( rm ,    nm 1  ,    fa ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testUnhealthyNodeStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( hostFile . exists (  )  )  )     {", ". TEMP _ DIR . mkdirs (  )  ;", "hostFile . createNewFile (  )  ;", "}", "FileOutputStream   fStream    =    null ;", "try    {", "fStream    =    new   FileOutputStream ( hostFile )  ;", "for    ( int   i    =     0  ;    i    <     ( hosts . length )  ;    i +  +  )     {", "fStream . write ( hosts [ i ]  . getBytes (  )  )  ;", "fStream . write (  \"  \\ n \"  . getBytes (  )  )  ;", "}", "}    finally    {", "if    ( fStream    !  =    null )     {", "IOUtils . closeStream ( fStream )  ;", "fStream    =    null ;", "}", "}", "}", "METHOD_END"], "methodName": ["writeToHostsFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService"}, {"methodBody": ["METHOD_START", "{", "startRMs (  )  ;", "RMApp   app    =    RMHATestBase . rm 1  . submitApp (  2  0  0  )  ;", "ApplicationReport   appReport 1     =    RMHATestBase . rm 1  . geReport ( app . geId (  )  )  ;", "Assert . assertTrue (  (  (  ( appReport 1  . getYarnApplicationState (  )  )     =  =     ( YarnApplicationState . ACCEPTED )  )     |  |     (  ( appReport 1  . getYarnApplicationState (  )  )     =  =     ( YarnApplicationState . SUBMITTED )  )  )  )  ;", "ApplicationReport   appReport 2     =    RMHATestBase . rm 1  . geReport ( app . geId (  )  )  ;", "Assert . assertEquals ( appReport 1  . geId (  )  ,    appReport 2  . geId (  )  )  ;", "Assert . assertEquals ( appReport 1  . getYarnApplicationState (  )  ,    appReport 2  . getYarnApplicationState (  )  )  ;", "explicitFailover (  )  ;", "ApplicationReport   appReport 3     =    RMHATestBase . rm 2  . geReport ( app . geId (  )  )  ;", "Assert . assertEquals ( appReport 1  . geId (  )  ,    appReport 3  . geId (  )  )  ;", "Assert . assertEquals ( appReport 1  . getYarnApplicationState (  )  ,    appReport 3  . getYarnApplicationState (  )  )  ;", "ApplicationReport   appReport 4     =    RMHATestBase . rm 2  . geReport ( app . geId (  )  )  ;", "Assert . assertEquals ( appReport 3  . geId (  )  ,    appReport 4  . geId (  )  )  ;", "Assert . assertEquals ( appReport 3  . getYarnApplicationState (  )  ,    appReport 4  . getYarnApplicationState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationReportIdempotent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMs (  )  ;", "ApplicationId   appId    =    RMHATestBase . rm 1  . getNewAppId (  )  . geId (  )  ;", "explicitFailover (  )  ;", "RMApp   app 1     =    RMHATestBase . rm 2  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false ,    true ,    appId )  ;", "verifySubmitApp ( RMHATestBase . rm 2  ,    app 1  ,    appId )  ;", "}", "METHOD_END"], "methodName": ["testHandleRMHABeforeSubmitApplicationCallWithSavedApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMs (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  )  ;", "explicitFailover (  )  ;", "Assert . assertTrue ( RMHATestBase . rm 2  . getRMContext (  )  . getRMApps (  )  . containsKey ( app 0  . geId (  )  )  )  ;", "RMApp   app 1     =    RMHATestBase . rm 2  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false ,    true ,    app 0  . geId (  )  )  ;", "Assert . assertEquals ( app 1  . geId (  )  ,    app 0  . geId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHandleRMHADuringSubmitApplicationCallWithSavedApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMsWithCustomizedRMAppManager (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false )  ;", "explicitFailover (  )  ;", "Assert . assertFalse ( RMHATestBase . rm 2  . getRMContext (  )  . getRMApps (  )  . containsKey ( app 0  . geId (  )  )  )  ;", "RMApp   app 1     =    RMHATestBase . rm 2  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false ,    true ,    app 0  . geId (  )  )  ;", "verifySubmitApp ( RMHATestBase . rm 2  ,    app 1  ,    app 0  . geId (  )  )  ;", "Assert . assertTrue ( RMHATestBase . rm 2  . getRMContext (  )  . getRMApps (  )  . containsKey ( app 0  . geId (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testHandleRMHADuringSubmitApplicationCallWithoutSavedApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMs (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  )  ;", "explicitFailover (  )  ;", "ApplicationReport   appReport    =    RMHATestBase . rm 2  . geReport ( app 0  . geId (  )  )  ;", "Assert . assertTrue (  (  (  ( appReport . getYarnApplicationState (  )  )     =  =     ( YarnApplicationState . ACCEPTED )  )     |  |     (  ( appReport . getYarnApplicationState (  )  )     =  =     ( YarnApplicationState . SUBMITTED )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testHandleRMHAafterSubmitApplicationCallWithSavedApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "startRMsWithCustomizedRMAppManager (  )  ;", "RMApp   app 0     =    RMHATestBase . rm 1  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false )  ;", "explicitFailover (  )  ;", "try    {", "RMHATestBase . rm 2  . geReport ( app 0  . geId (  )  )  ;", "Assert . fail (  \" Should   get   ApplicationNotFoundException   here \"  )  ;", "}    catch    ( ApplicationNotFoundException   ex )     {", "}", "RMApp   app 1     =    RMHATestBase . rm 2  . submitApp (  2  0  0  ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,    configuration . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ,    null ,    null ,    false ,    false ,    true ,    app 0  . geId (  )  )  ;", "verifySubmitApp ( RMHATestBase . rm 2  ,    app 1  ,    app 0  . geId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHandleRMHAafterSubmitApplicationCallWithoutSavedApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "int   maxWaittingTimes    =     2  0  ;", "int   count    =     0  ;", "while    ( true )     {", "YarnApplicationState   state    =    rm . geReport ( app . geId (  )  )  . getYarnApplicationState (  )  ;", "if    (  (  !  ( state . equals ( NEW )  )  )     &  &     (  !  ( state . equals ( NEW _ SAVING )  )  )  )     {", "break ;", "}", "if    ( count    >    maxWaittingTimes )     {", "break ;", "}", "Thread . sleep (  2  0  0  )  ;", "count +  +  ;", "}", "YarnApplicationState   state    =    rm . geReport ( app . geId (  )  )  . getYarnApplicationState (  )  ;", "Assert . assertTrue (  (  ( state    =  =     ( YarnApplicationState . ACCEPTED )  )     |  |     ( state    =  =     ( YarnApplicationState . SUBMITTED )  )  )  )  ;", "Assert . assertEquals ( expectedAppId ,    app . geId (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifySubmitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestSubmitApplicationWithRMHA"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( appsSubmitted ,    qm . getAppsSubmitted (  )  )  ;", "assertEquals ( appsPending ,    qm . getAppsPending (  )  )  ;", "assertEquals ( appsRunning ,    qm . getAppsRunning (  )  )  ;", "assertEquals ( appsCompleted ,    qm . getAppsCompleted (  )  )  ;", "assertEquals ( allocatedContainers ,    qm . getAllocatedContainers (  )  )  ;", "assertEquals ( availableMB ,    qm . getAvailableMB (  )  )  ;", "assertEquals ( availableVirtualCores ,    qm . getAvailableVirtualCores (  )  )  ;", "assertEquals ( allocatedMB ,    qm . getAllocatedMB (  )  )  ;", "assertEquals ( allocatedVirtualCores ,    qm . getAllocatedVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["asserteMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   leafQueue    =     (  ( LeafQueue )     ( app . getQueue (  )  )  )  ;", "assertEquals ( usedR ,    leafQueue . getUsedRs (  )  )  ;", "assertEquals ( numContainers ,    leafQueue . getNumContainers (  )  )  ;", "RCalculator   calc    =     (  ( CapacityScheduler )     ( rm . getRScheduler (  )  )  )  . getRCalculator (  )  ;", "float   usedCapacity    =    Rs . divide ( calc ,    clusterR ,    usedR ,    queueR )  ;", "assertEquals ( usedCapacity ,    leafQueue . getUsedCapacity (  )  ,     1  .  0 E -  8  )  ;", "float   absoluteUsedCapacity    =    Rs . divide ( calc ,    clusterR ,    usedR ,    clusterR )  ;", "assertEquals ( absoluteUsedCapacity ,    leafQueue . getAbsoluteUsedCapacity (  )  ,     1  .  0 E -  8  )  ;", "assertEquals ( usedR ,    leafQueue . getUser ( app . getUser (  )  )  . getConsumedRs (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkCSLeafQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "checkCSLeafQueue ( rm 2  ,    app ,    clusterResource ,    queueResource ,    usedResource ,    numContainers )  ;", "LeafQueue   queue    =     (  ( LeafQueue )     ( app . getQueue (  )  )  )  ;", "Resource   availableResources    =    Resources . subtract ( queueResource ,    usedResource )  ;", "QueueMetrics   queueMetrics    =    queue . getMetrics (  )  ;", "asserteMetrics ( queueMetrics ,     1  ,     0  ,     1  ,     0  ,     2  ,    availableResources . getMemory (  )  ,    availableResources . getVirtualCores (  )  ,    usedResource . getMemory (  )  ,    usedResource . getVirtualCores (  )  )  ;", "QueueMetrics   userMetrics    =    queueMetrics . getUserMetrics ( app . getUser (  )  )  ;", "asserteMetrics ( userMetrics ,     1  ,     0  ,     1  ,     0  ,     2  ,    availableResources . getMemory (  )  ,    availableResources . getVirtualCores (  )  ,    usedResource . getMemory (  )  ,    usedResource . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkCSQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "FifoScheduler   scheduler    =     (  ( FifoScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  ;", "assertEquals ( usedResources ,    scheduler . getUsedResource (  )  )  ;", "SchedulerApplicationAttempt   schedulerAttempt    =    schedulerApp . getCurrentAppAttempt (  )  ;", "assertEquals ( availableResources ,    schedulerAttempt . getHeadroom (  )  )  ;", "QueueMetrics   queueMetrics    =    scheduler . getRootQueueMetrics (  )  ;", "asserteMetrics ( queueMetrics ,     1  ,     0  ,     1  ,     0  ,     2  ,    availableResources . getMemory (  )  ,    availableResources . getVirtualCores (  )  ,    usedResources . getMemory (  )  ,    usedResources . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkFifoQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( numContainers ,    parentQueue . getNumContainers (  )  )  ;", "assertEquals ( usedR ,    parentQueue . getUsedRs (  )  )  ;", "assertEquals ( UsedCapacity ,    parentQueue . getUsedCapacity (  )  ,     1  .  0 E -  8  )  ;", "assertEquals ( absoluteUsedCapacity ,    parentQueue . getAbsoluteUsedCapacity (  )  ,     1  .  0 E -  8  )  ;", "}", "METHOD_END"], "methodName": ["checkParentQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "List < NMContainerStatus >    list    =    new   ArrayList < NMContainerStatus >  (  )  ;", "NMContainerStatus   amContainer    =    Test . createNMContainerStatus ( am . getApplicationAttemptId (  )  ,     1  ,    RUNNING )  ;", "NMContainerStatus   runningContainer    =    Test . createNMContainerStatus ( am . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "NMContainerStatus   completedContainer    =    Test . createNMContainerStatus ( am . getApplicationAttemptId (  )  ,     3  ,    COMPLETE )  ;", "list . add ( amContainer )  ;", "list . add ( runningContainer )  ;", "list . add ( completedContainer )  ;", "return   list ;", "}", "METHOD_END"], "methodName": ["createNMContainerStatusForApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {    CapacityScheduler . class    }  ,    new   Object [  ]  {    FifoScheduler . class    }  ,    new   Object [  ]  {    FairScheduler . class    }     }  )  ;", "}", "METHOD_END"], "methodName": ["getTestParameters"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "conf    =    new   YarnConfiguration (  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "conf . set ( RECOVERY _ ENABLED ,     \" true \"  )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", "conf . setClass ( RM _ SCHEDULER ,    schedulerClass ,    ResourceScheduler . class )  ;", "conf . setBoolean ( RM _ WORK _ PRESERVING _ RECOVERY _ ENABLED ,    true )  ;", "DefaultMetricsSystem . setMiniClusterMode ( true )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    TestWorkPreservingRMRestart . R    }  )  ;", "final   String   Q _ R    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestWorkPreservingRMRestart . R )  ;", "conf . setCapacity ( Q _ R ,     1  0  0  )  ;", "final   String   Q _ A    =     ( Q _ R    +     \"  .  \"  )     +     ( TestWorkPreservingRMRestart . A )  ;", "final   String   Q _ B    =     ( Q _ R    +     \"  .  \"  )     +     ( TestWorkPreservingRMRestart . B )  ;", "conf . setQueues ( Q _ R ,    new   String [  ]  {    TestWorkPreservingRMRestart . A ,    TestWorkPreservingRMRestart . B    }  )  ;", "conf . setCapacity ( Q _ A ,     5  0  )  ;", "conf . setCapacity ( Q _ B ,     5  0  )  ;", "conf . setDouble ( CapacitySchedulerConfiguration . MAXIMUM _ APPLICATION _ MASTERS _ RESOURCE _ PERCENT ,     0  .  5 F )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rm 1  )     !  =    null )     {", "rm 1 top (  )  ;", "}", "if    (  ( rm 2  )     !  =    null )     {", "rm 2 top (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  1  9  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1  _  1     =    rm 1  . submitApp (  1  0  2  4  )  ;", "MockAM   am 1  _  1     =    MockRM . launchAndRegisterAM ( app 1  _  1  ,    rm 1  ,    nm 1  )  ;", "RMAppAttempt   attempt 0     =    app 1  _  1  . getCurrentAppAttempt (  )  ;", "AbstractYarnScheduler   scheduler    =     (  ( AbstractYarnScheduler )     ( rm 1  . getResourceScheduler (  )  )  )  ;", "Assert . assertTrue ( scheduler . getRMContainer ( attempt 0  . getMasterContainer (  )  . getId (  )  )  . isAMContainer (  )  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "List < NMContainerStatus >    am 1  _  1 Containers    =     . createNMContainerStatusForApp ( am 1  _  1  )  ;", "nm 1  . registerNode ( am 1  _  1 Containers ,    null )  ;", ". waitForNumContainersToRecover (  2  ,    rm 2  ,    am 1  _  1  . getApplicationAttemptId (  )  )  ;", "scheduler    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  ;", "Assert . assertTrue ( scheduler . getRMContainer ( attempt 0  . getMasterContainer (  )  . getId (  )  )  . isAMContainer (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAMContainerStatusWithRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  1  9  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "NMContainerStatus   amContainer    =    Test . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "NMContainerStatus   runningContainer    =    Test . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "NMContainerStatus   completedContainer    =    Test . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     3  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( amContainer ,    runningContainer ,    completedContainer )  ,    null )  ;", "rm 2  . waitForState ( am 1  . getApplicationAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "Thread . sleep (  3  0  0  0  )  ;", "AbstractYarnScheduler   scheduler    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  ;", "assertNull ( scheduler . getRMContainer ( runningContainer . getContainerId (  )  )  )  ;", "assertNull ( scheduler . getRMContainer ( completedContainer . getContainerId (  )  )  )  ;", "rm 2  . waitForNewAMToLaunchAndRegister ( app 1  . getApplicationId (  )  ,     2  ,    nm 1  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  1  .  1  .  1  :  4  3  2  1  \"  ,     8  1  9  2  ,    rm 2  . getResourceTrackerService (  )  )  ;", "NMContainerStatus   previousAttemptContainer    =    Test . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     4  ,    RUNNING )  ;", "nm 2  . registerNode ( Arrays . asList ( previousAttemptContainer )  ,    null )  ;", "Thread . sleep (  3  0  0  0  )  ;", "assertNull ( scheduler . getRMContainer ( previousAttemptContainer . getContainerId (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testAMfailedBetweenRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getRTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    MockRM . launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "rm 2  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . LAUNCHED )  ;", "am 0  . setAMRMProtocol ( rm 2  . getApplicationMasterService (  )  ,    rm 2  . getRMContext (  )  )  ;", "am 0  . registerAppAttempt ( true )  ;", "rm 2  . waitForState ( app 0  . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "rm 2  . waitForState ( am 0  . getApplicationAttemptId (  )  ,    RMAppAttemptState . RUNNING )  ;", "}", "METHOD_END"], "methodName": ["testAppReregisterOnRMWorkPreservingRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( schedulerClass . equals ( CapacityScheduler . class )  )  )     {", "return ;", "}", "conf . setBoolean ( CapacitySchedulerConfiguration . ENABLE _ USER _ METRICS ,    true )  ;", "conf . set ( CapacitySchedulerConfiguration . RESOURCE _ CALCULATOR _ CLASS ,    DominantResourceCalculator . class . getName (  )  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration ( conf )  ;", "setupQueueConfiguration ( csConf )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( csConf )  ;", "rm 1     =    new   MockRM ( csConf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  1  9  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  1  .  1  .  1  :  4  3  2  1  \"  ,     8  1  9  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "nm 2  . registerNode (  )  ;", "RMApp   app 1  _  1     =    rm 1  . submitApp (  1  0  2  4  ,     \" app 1  _  1  \"  ,     . USER _  1  ,    null ,     . A )  ;", "MockAM   am 1  _  1     =    MockRM . launchAndRegisterAM ( app 1  _  1  ,    rm 1  ,    nm 1  )  ;", "RMApp   app 1  _  2     =    rm 1  . submitApp (  1  0  2  4  ,     \" app 1  _  2  \"  ,     . USER _  1  ,    null ,     . A )  ;", "MockAM   am 1  _  2     =    MockRM . launchAndRegisterAM ( app 1  _  2  ,    rm 1  ,    nm 2  )  ;", "RMApp   app 2     =    rm 1  . submitApp (  1  0  2  4  ,     \" app 2  \"  ,     . USER _  2  ,    null ,     . B )  ;", "MockAM   am 2     =    MockRM . launchAndRegisterAM ( app 2  ,    rm 1  ,    nm 2  )  ;", "rm 1  . clearQueueMetrics ( app 1  _  1  )  ;", "rm 1  . clearQueueMetrics ( app 1  _  2  )  ;", "rm 1  . clearQueueMetrics ( app 2  )  ;", "rm 2     =    new   MockRM ( csConf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 2  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "List < NMContainerStatus >    am 1  _  1 Containers    =     . createNMContainerStatusForApp ( am 1  _  1  )  ;", "List < NMContainerStatus >    am 1  _  2 Containers    =     . createNMContainerStatusForApp ( am 1  _  2  )  ;", "am 1  _  1 Containers . addAll ( am 1  _  2 Containers )  ;", "nm 1  . registerNode ( am 1  _  1 Containers ,    null )  ;", "List < NMContainerStatus >    am 2 Containers    =     . createNMContainerStatusForApp ( am 2  )  ;", "nm 2  . registerNode ( am 2 Containers ,    null )  ;", ". waitForNumContainersToRecover (  2  ,    rm 2  ,    am 1  _  1  . getApplicationAttemptId (  )  )  ;", ". waitForNumContainersToRecover (  2  ,    rm 2  ,    am 1  _  2  . getApplicationAttemptId (  )  )  ;", ". waitForNumContainersToRecover (  2  ,    rm 2  ,    am 1  _  2  . getApplicationAttemptId (  )  )  ;", "Resource   containerResource    =    Resource . newInstance (  1  0  2  4  ,     1  )  ;", "Resource   nmResource    =    Resource . newInstance ( nm 1  . getMemory (  )  ,    nm 1  . getvCores (  )  )  ;", "Resource   clusterResource    =    Resources . multiply ( nmResource ,     2  )  ;", "Resource   q 1 Resource    =    Resources . multiply ( clusterResource ,     0  .  5  )  ;", "Resource   q 2 Resource    =    Resources . multiply ( clusterResource ,     0  .  5  )  ;", "Resource   q 1 UsedResource    =    Resources . multiply ( containerResource ,     4  )  ;", "Resource   q 2 UsedResource    =    Resources . multiply ( containerResource ,     2  )  ;", "Resource   totalUsedResource    =    Resources . add ( q 1 UsedResource ,    q 2 UsedResource )  ;", "Resource   q 1 availableResources    =    Resources . subtract ( q 1 Resource ,    q 1 UsedResource )  ;", "Resource   q 2 availableResources    =    Resources . subtract ( q 2 Resource ,    q 2 UsedResource )  ;", "Resource   totalAvailableResource    =    Resources . add ( q 1 availableResources ,    q 2 availableResources )  ;", "Map < ApplicationId ,    SchedulerApplication >    schedulerApps    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  . getSchedulerApplications (  )  ;", "SchedulerApplication   schedulerApp 1  _  1     =    schedulerApps . get ( app 1  _  1  . getApplicationId (  )  )  ;", "checkCSLeafQueue ( rm 2  ,    schedulerApp 1  _  1  ,    clusterResource ,    q 1 Resource ,    q 1 UsedResource ,     4  )  ;", "QueueMetrics   queue 1 Metrics    =    schedulerApp 1  _  1  . getQueue (  )  . getMetrics (  )  ;", "asserteMetrics ( queue 1 Metrics ,     2  ,     0  ,     2  ,     0  ,     4  ,    q 1 availableResources . getMemory (  )  ,    q 1 availableResources . getVirtualCores (  )  ,    q 1 UsedResource . getMemory (  )  ,    q 1 UsedResource . getVirtualCores (  )  )  ;", "SchedulerApplication   schedulerApp 2     =    schedulerApps . get ( app 2  . getApplicationId (  )  )  ;", "checkCSLeafQueue ( rm 2  ,    schedulerApp 2  ,    clusterResource ,    q 2 Resource ,    q 2 UsedResource ,     2  )  ;", "QueueMetrics   queue 2 Metrics    =    schedulerApp 2  . getQueue (  )  . getMetrics (  )  ;", "asserteMetrics ( queue 2 Metrics ,     1  ,     0  ,     1  ,     0  ,     2  ,    q 2 availableResources . getMemory (  )  ,    q 2 availableResources . getVirtualCores (  )  ,    q 2 UsedResource . getMemory (  )  ,    q 2 UsedResource . getVirtualCores (  )  )  ;", "LeafQueue   leafQueue    =     (  ( LeafQueue )     ( schedulerApp 2  . getQueue (  )  )  )  ;", "ParentQueue   parentQueue    =     (  ( ParentQueue )     ( leafQueue . getParent (  )  )  )  ;", "checkParentQueue ( parentQueue ,     6  ,    totalUsedResource ,     (  (  ( float )     (  6  )  )     /     1  6  )  ,     (  (  ( float )     (  6  )  )     /     1  6  )  )  ;", "asserteMetrics ( parentQueue . getMetrics (  )  ,     3  ,     0  ,     3  ,     0  ,     6  ,    totalAvailableResource . getMemory (  )  ,    totalAvailableResource . getVirtualCores (  )  ,    totalUsedResource . getMemory (  )  ,    totalUsedResource . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCapacitySchedulerRecovery"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  1  9  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "MockRM . finishAMAndVerifyAppState ( app 1  ,    rm 1  ,    nm 1  ,    am 1  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "NMContainerStatus   runningContainer    =    Test . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "NMContainerStatus   completedContainer    =    Test . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     3  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( runningContainer ,    completedContainer )  ,    null )  ;", "RMApp   recoveredApp 1     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "assertEquals ( RMAppState . FINISHED ,    recoveredApp 1  . getState (  )  )  ;", "Thread . sleep (  3  0  0  0  )  ;", "AbstractYarnScheduler   scheduler    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  ;", "assertNull ( scheduler . getRMContainer ( runningContainer . getContainerId (  )  )  )  ;", "assertNull ( scheduler . getRMContainer ( completedContainer . getContainerId (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainersNotRecoveredForCompletedApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 0     =    MockRM . launchAndRegisterAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "Assert . assertNotNull ( rm 2  . getResourceScheduler (  )  . getSchedulerAppInfo ( am 0  . getApplicationAttemptId (  )  )  )  ;", "(  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  . getTransferredContainers ( am 0  . getApplicationAttemptId (  )  )  ;", "List < NMContainerStatus >    containers    =     . createNMContainerStatusForApp ( am 0  )  ;", "nm 1  . registerNode ( containers ,    null )  ;", ". waitForNumContainersToRecover (  2  ,    rm 2  ,    am 0  . getApplicationAttemptId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRecoverSchedulerAppAndAttemptSynchronously"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "MockNM   nm 1     =    new   MockNM (  \" h 1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "rm 1  . start (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  1  0  2  4  )  ;", "final   MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "conf . setInt ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,     8  0  0  0  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "rm 2  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "am 1  . setAMRMProtocol ( rm 2  . getApplicationMasterService (  )  ,    rm 2  . getRMContext (  )  )  ;", "am 1  . registerAppAttempt ( true )  ;", "final   ContainerId   runningContainer    =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "am 1  . allocate ( null ,    Arrays . asList ( runningContainer )  )  ;", "List < NMContainerStatus >    containerStatuses    =     . createNMContainerStatusForApp ( am 1  )  ;", "nm 1  . registerNode ( containerStatuses ,    null )  ;", ". waitForNumContainersToRecover (  1  ,    rm 2  ,    am 1  . getApplicationAttemptId (  )  )  ;", "final   AbstractYarnScheduler   scheduler    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  ;", "AllocateResponse   response    =    am 1  . allocate ( null ,    null )  ;", "boolean   receivedCompletedContainer    =    false ;", "for    ( ContainerStatus   status    :    response . getCompletedContainersStatuses (  )  )     {", "if    ( status . getContainerId (  )  . equals ( runningContainer )  )     {", "receivedCompletedContainer    =    true ;", "}", "}", "assertTrue ( receivedCompletedContainer )  ;", "GenericTestUtils . waitFor ( new   com . google . common . base . Supplier < Boolean >  (  )     {", "public   Boolean   get (  )     {", "return    ( scheduler . getApplicationAttempt ( am 1  . getApplicationAttemptId (  )  )  . getPendingRelease (  )  . isEmpty (  )  )     &  &     (  ( scheduler . getRMContainer ( runningContainer )  )     =  =    null )  ;", "}", "}  ,     1  0  0  0  ,     2  0  0  0  0  )  ;", "}", "METHOD_END"], "methodName": ["testReleasedContainerNotRecovered"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( CapacitySchedulerConfiguration . ENABLE _ USER _ METRICS ,    true )  ;", "conf . set ( CapacitySchedulerConfiguration . RESOURCE _ CALCULATOR _ CLASS ,    DominantResourceCalculator . class . getName (  )  )  ;", "int   containerMemory    =     1  0  2  4  ;", "Resource   containerResource    =    Resource . newInstance ( containerMemory ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  1  9  2  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "rm 1  . clearQueueMetrics ( app 1  )  ;", "rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "RMApp   recoveredApp 1     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getApplicationId (  )  )  ;", "RMAppAttempt   loadedAttempt 1     =    recoveredApp 1  . getCurrentAppAttempt (  )  ;", "NMContainerStatus   amContainer    =    TestRMRestart . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     1  ,    RUNNING )  ;", "NMContainerStatus   runningContainer    =    TestRMRestart . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "NMContainerStatus   completedContainer    =    TestRMRestart . createNMContainerStatus ( am 1  . getApplicationAttemptId (  )  ,     3  ,    COMPLETE )  ;", "nm 1  . registerNode ( Arrays . asList ( amContainer ,    runningContainer ,    completedContainer )  ,    null )  ;", "TestWorkPreservingRMRestart . waitForNumContainersToRecover (  2  ,    rm 2  ,    am 1  . getApplicationAttemptId (  )  )  ;", "Set < ContainerId >    launchedContainers    =     (  ( RMNodeImpl )     ( rm 2  . getRMContext (  )  . getRMNodes (  )  . get ( nm 1  . getNodeId (  )  )  )  )  . getLaunchedContainers (  )  ;", "assertTrue ( launchedContainers . contains ( amContainer . getContainerId (  )  )  )  ;", "assertTrue ( launchedContainers . contains ( runningContainer . getContainerId (  )  )  )  ;", "rm 2  . waitForState ( nm 1  ,    amContainer . getContainerId (  )  ,    RMContainerState . RUNNING )  ;", "rm 2  . waitForState ( nm 1  ,    runningContainer . getContainerId (  )  ,    RMContainerState . RUNNING )  ;", "rm 2  . waitForContainerToComplete ( loadedAttempt 1  ,    completedContainer )  ;", "AbstractYarnScheduler   scheduler    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  ;", "SchedulerNode   schedulerNode 1     =    scheduler . getSchedulerNode ( nm 1  . getNodeId (  )  )  ;", "Resource   usedResources    =    Resources . multiply ( containerResource ,     2  )  ;", "Resource   nmResource    =    Resource . newInstance ( nm 1  . getMemory (  )  ,    nm 1  . getvCores (  )  )  ;", "assertTrue ( schedulerNode 1  . isValidContainer ( amContainer . getContainerId (  )  )  )  ;", "assertTrue ( schedulerNode 1  . isValidContainer ( runningContainer . getContainerId (  )  )  )  ;", "assertFalse ( schedulerNode 1  . isValidContainer ( completedContainer . getContainerId (  )  )  )  ;", "assertEquals (  2  ,    schedulerNode 1  . getNumContainers (  )  )  ;", "assertEquals ( Resources . subtract ( nmResource ,    usedResources )  ,    schedulerNode 1  . getAvailableResource (  )  )  ;", "assertEquals ( usedResources ,    schedulerNode 1  . getUsedResource (  )  )  ;", "Resource   availableResources    =    Resources . subtract ( nmResource ,    usedResources )  ;", "Map < ApplicationId ,    SchedulerApplication >    schedulerApps    =     (  ( AbstractYarnScheduler )     ( rm 2  . getResourceScheduler (  )  )  )  . getSchedulerApplications (  )  ;", "SchedulerApplication   schedulerApp    =    schedulerApps . get ( recoveredApp 1  . getApplicationId (  )  )  ;", "if    ( schedulerClass . equals ( CapacityScheduler . class )  )     {", "checkCSQueue ( rm 2  ,    schedulerApp ,    nmResource ,    nmResource ,    usedResources ,     2  )  ;", "} else", "if    ( schedulerClass . equals ( scheduler . fifo . FifoScheduler . class )  )     {", "checkFifoQueue ( schedulerApp ,    usedResources ,    availableResources )  ;", "}", "SchedulerApplicationAttempt   schedulerAttempt    =    schedulerApp . getCurrentAppAttempt (  )  ;", "assertTrue ( schedulerAttempt . getLiveContainers (  )  . contains ( scheduler . getRMContainer ( amContainer . getContainerId (  )  )  )  )  ;", "assertTrue ( schedulerAttempt . getLiveContainers (  )  . contains ( scheduler . getRMContainer ( runningContainer . getContainerId (  )  )  )  )  ;", "assertEquals ( schedulerAttempt . getCurrentConsumption (  )  ,    usedResources )  ;", "if    (  ( scheduler . getClass (  )  )     !  =     ( FairScheduler . class )  )     {", "assertEquals ( availableResources ,    schedulerAttempt . getHeadroom (  )  )  ;", "}", "assertEquals (  (  (  1     <  <     2  2  )     +     1  )  ,    schedulerAttempt . getNewContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSchedulerRecovery"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "AbstractYarnScheduler   scheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "SchedulerApplicationAttempt   attempt    =    scheduler . getApplicationAttempt ( attemptId )  ;", "while    ( attempt    =  =    null )     {", "System . out . println (  (  (  \" Wait   for   scheduler   attempt    \"     +    attemptId )     +     \"    to   be   created \"  )  )  ;", "Thread . sleep (  2  0  0  )  ;", "attempt    =    scheduler . getApplicationAttempt ( attemptId )  ;", "}", "while    (  ( attempt . getLiveContainers (  )  . size (  )  )     <    num )     {", "System . out . println (  (  (  (  \" Wait   for    \"     +    num )     +     \"    containers   to   recover .    currently :     \"  )     +     ( attempt . getLiveContainers (  )  . size (  )  )  )  )  ;", "Thread . sleep (  2  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForNumContainersToRecover"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.TestWorkPreservingRMRestart"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "dispatcher . getEventHandler (  )  . handle ( new   WritingAttemptFinishEvent ( appAttempt . getAppAttemptId (  )  ,    AttemptFinishData . newInstance ( appAttempt . getAppAttemptId (  )  ,    appAttempt . getDiagnostics (  )  . toString (  )  ,    appAttempt . getTrackingUrl (  )  ,    appAttempt . getFinalStatus (  )  ,    RMServerUtils . createAttemptState ( finalState )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["applicationAttemptFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "dispatcher . getEventHandler (  )  . handle ( new   WritingAttemptStartEvent ( appAttempt . getAppAttemptId (  )  ,    AttemptStartData . newInstance ( appAttempt . getAppAttemptId (  )  ,    appAttempt . getHost (  )  ,    appAttempt . getRpcPort (  )  ,    appAttempt . getMasterContainer (  )  . getId (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["applicationAttemptStarted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "dispatcher . getEventHandler (  )  . handle ( new   WritingFinishEvent ( app . getId (  )  ,    FinishData . newInstance ( app . getId (  )  ,    app . getFinishTime (  )  ,    app . getDiagnostics (  )  . toString (  )  ,    app . getFinalStatus (  )  ,    RMServerUtils . createState ( finalState )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["applicationFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "dispatcher . getEventHandler (  )  . handle ( new   WritingStartEvent ( app . getId (  )  ,    StartData . newInstance ( app . getId (  )  ,    app . getName (  )  ,    app . getType (  )  ,    app . getQueue (  )  ,    app . getUser (  )  ,    app . getSubmitTime (  )  ,    app . getStartTime (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["applicationStarted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "dispatcher . getEventHandler (  )  . handle ( new   WritingContainerFinishEvent ( container . getContainerId (  )  ,    ContainerFinishData . newInstance ( container . getContainerId (  )  ,    container . getFinishTime (  )  ,    container . getDiagnosticsInfo (  )  ,    container . getContainerExitStatus (  )  ,    container . getContainerState (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["containerFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "dispatcher . getEventHandler (  )  . handle ( new   WritingContainerStartEvent ( container . getContainerId (  )  ,    ContainerStartData . newInstance ( container . getContainerId (  )  ,    container . getAllocatedR (  )  ,    container . getAllocatedNode (  )  ,    container . getAllocatedPriority (  )  ,    container . getCreationTime (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["containerStarted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "if    ( historyServiceEnabled )     {", "try    {", "Class <  ?    extends   ApplicationHistoryStore >    storeClass    =    conf . getClass ( APPLICATION _ HISTORY _ STORE ,    FileSystemApplicationHistoryStore . class ,    ApplicationHistoryStore . class )  ;", "return   storeClass . newInstance (  )  ;", "}    catch    ( Exception   e )     {", "String   msg    =     \" Could   not   instantiate   ApplicationHistoryWriter :     \"     +     ( conf . get ( APPLICATION _ HISTORY _ STORE ,    FileSystemApplicationHistoryStore . class . getName (  )  )  )  ;", ". LOG . error ( msg ,    e )  ;", "throw   new   YarnRuntimeException ( msg ,    e )  ;", "}", "} else    {", "return   new   NullApplicationHistoryStore (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createApplicationHistoryStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMApplicationHistoryWriter . MultiThreadedDispatcher   dispatcher    =    new   RMApplicationHistoryWriter . MultiThreadedDispatcher ( conf . getInt ( RM _ HISTORY _ WRITER _ MULTI _ THREADED _ DISPATCHER _ POOL _ SIZE ,    DEFAULT _ RM _ HISTORY _ WRITER _ MULTI _ THREADED _ DISPATCHER _ POOL _ SIZE )  )  ;", "dispatcher . setDrainEventsOnStop (  )  ;", "return   dispatcher ;", "}", "METHOD_END"], "methodName": ["createDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "switch    ( event . getType (  )  )     {", "case   APP _ START    :", "WritingApplicationStartEvent   wasEvent    =     (  ( WritingApplicationStartEvent )     ( event )  )  ;", "try    {", "writer . applicationStarted ( wasEvent . getApplicationStartData (  )  )  ;", ". LOG . info (  (  \" Stored   the   start   data   of   application    \"     +     ( wasEvent . getApplicationId (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Error   when   storing   the   start   data   of   application    \"     +     ( wasEvent . getApplicationId (  )  )  )  )  ;", "}", "break ;", "case   APP _ FINISH    :", "WritingApplicationFinishEvent   wafEvent    =     (  ( WritingApplicationFinishEvent )     ( event )  )  ;", "try    {", "writer . applicationFinished ( wafEvent . getApplicationFinishData (  )  )  ;", ". LOG . info (  (  \" Stored   the   finish   data   of   application    \"     +     ( wafEvent . getApplicationId (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Error   when   storing   the   finish   data   of   application    \"     +     ( wafEvent . getApplicationId (  )  )  )  )  ;", "}", "break ;", "case   APP _ ATTEMPT _ START    :", "WritingApplicationAttemptStartEvent   waasEvent    =     (  ( WritingApplicationAttemptStartEvent )     ( event )  )  ;", "try    {", "writer . applicationAttemptStarted ( waasEvent . getApplicationAttemptStartData (  )  )  ;", ". LOG . info (  (  \" Stored   the   start   data   of   application   attempt    \"     +     ( waasEvent . getApplicationAttemptId (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Error   when   storing   the   start   data   of   application   attempt    \"     +     ( waasEvent . getApplicationAttemptId (  )  )  )  )  ;", "}", "break ;", "case   APP _ ATTEMPT _ FINISH    :", "WritingApplicationAttemptFinishEvent   waafEvent    =     (  ( WritingApplicationAttemptFinishEvent )     ( event )  )  ;", "try    {", "writer . applicationAttemptFinished ( waafEvent . getApplicationAttemptFinishData (  )  )  ;", ". LOG . info (  (  \" Stored   the   finish   data   of   application   attempt    \"     +     ( waafEvent . getApplicationAttemptId (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Error   when   storing   the   finish   data   of   application   attempt    \"     +     ( waafEvent . getApplicationAttemptId (  )  )  )  )  ;", "}", "break ;", "case   CONTAINER _ START    :", "WritingContainerStartEvent   wcsEvent    =     (  ( WritingContainerStartEvent )     ( event )  )  ;", "try    {", "writer . containerStarted ( wcsEvent . getContainerStartData (  )  )  ;", ". LOG . info (  (  \" Stored   the   start   data   of   container    \"     +     ( wcsEvent . getContainerId (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Error   when   storing   the   start   data   of   container    \"     +     ( wcsEvent . getContainerId (  )  )  )  )  ;", "}", "break ;", "case   CONTAINER _ FINISH    :", "WritingContainerFinishEvent   wcfEvent    =     (  ( WritingContainerFinishEvent )     ( event )  )  ;", "try    {", "writer . containerFinished ( wcfEvent . getContainerFinishData (  )  )  ;", ". LOG . info (  (  \" Stored   the   finish   data   of   container    \"     +     ( wcfEvent . getContainerId (  )  )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  (  \" Error   when   storing   the   finish   data   of   container    \"     +     ( wcfEvent . getContainerId (  )  )  )  )  ;", "}", "break ;", "default    :", ". LOG . error (  (  \" Unknown   WritingApplicationHistoryEvent   type :     \"     +     ( event . getType (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["handleWritingApplicationHistoryEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "int   actual    =     0  ;", "for    (  . CounterDispatcher   dispatcher    :    dispatchers )     {", "for    ( Integer   count    :    dispatcher . counts . values (  )  )     {", "actual    +  =    count ;", "}", "}", "return   actual    =  =    expected ;", "}", "METHOD_END"], "methodName": ["allEventsHandled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    mock ( RMApp . class )  ;", "when ( app . getId (  )  )  . thenReturn ( appId )  ;", "when ( app . getName (  )  )  . thenReturn (  \" test   app \"  )  ;", "when ( app . getType (  )  )  . thenReturn (  \" test   app   type \"  )  ;", "when ( app . getUser (  )  )  . thenReturn (  \" test   user \"  )  ;", "when ( app . getQueue (  )  )  . thenReturn (  \" test   queue \"  )  ;", "when ( app . getSubmitTime (  )  )  . thenReturn (  0 L )  ;", "when ( app . getStartTime (  )  )  . thenReturn (  1 L )  ;", "when ( app . getFinishTime (  )  )  . thenReturn (  2 L )  ;", "when ( app . getDiagnostics (  )  )  . thenReturn ( new   StringBuilder (  \" test   diagnostics   info \"  )  )  ;", "when ( app . getFinalStatus (  )  )  . thenReturn ( UNDEFINED )  ;", "return   app ;", "}", "METHOD_END"], "methodName": ["createRMApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMAppAttempt   appAttempt    =    mock ( RMAppAttempt . class )  ;", "when ( appAttempt . getAppAttemptId (  )  )  . thenReturn ( appAttemptId )  ;", "when ( appAttempt . getHost (  )  )  . thenReturn (  \" test   host \"  )  ;", "when ( appAttempt . getRpcPort (  )  )  . thenReturn (  (  -  1  0  0  )  )  ;", "Container   container    =    mock ( Container . class )  ;", "when ( container . getId (  )  )  . thenReturn ( ContainerId . newInstance ( appAttemptId ,     1  )  )  ;", "when ( appAttempt . getMasterContainer (  )  )  . thenReturn ( container )  ;", "when ( appAttempt . getDiagnostics (  )  )  . thenReturn (  \" test   diagnostics   info \"  )  ;", "when ( appAttempt . getTrackingUrl (  )  )  . thenReturn (  \" test   url \"  )  ;", "when ( appAttempt . getFinalStatus (  )  )  . thenReturn ( UNDEFINED )  ;", "return   appAttempt ;", "}", "METHOD_END"], "methodName": ["createRMAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMContainer   container    =    mock ( RMContainer . class )  ;", "when ( container . getContainerId (  )  )  . thenReturn ( containerId )  ;", "when ( container . getAllocatedNode (  )  )  . thenReturn ( NodeId . newInstance (  \" test   host \"  ,     (  -  1  0  0  )  )  )  ;", "when ( container . getAllocatedR (  )  )  . thenReturn ( R . newInstance (  (  -  1  )  ,     (  -  1  )  )  )  ;", "when ( container . getAllocatedPriority (  )  )  . thenReturn ( UNDEFINED )  ;", "when ( container . getCreationTime (  )  )  . thenReturn (  0 L )  ;", "when ( container . getFinishTime (  )  )  . thenReturn (  1 L )  ;", "when ( container . getDiagnosticsInfo (  )  )  . thenReturn (  \" test   diagnostics   info \"  )  ;", "when ( container . getLogURL (  )  )  . thenReturn (  \" test   log   url \"  )  ;", "when ( container . getContainerExitStatus (  )  )  . thenReturn (  (  -  1  )  )  ;", "when ( container . getContainerState (  )  )  . thenReturn ( COMPLETE )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createRMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "int   count    =     0  ;", "for    (  . CounterDispatcher   dispatcher    :    dispatchers )     {", "if    ( dispatcher . counts . containsKey ( appId )  )     {", "+  + count ;", "}", "}", "return   count    =  =     1  ;", "}", "METHOD_END"], "methodName": ["handledByOne"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "store    =    new   MemoryApplicationHistoryStore (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setBoolean ( APPLICATION _ HISTORY _ ENABLED ,    true )  ;", "writer    =    new   RMApplicationHistoryWriter (  )     {", "@ Override", "protected   ApplicationHistoryStore   createApplicationHistoryStore ( Configuration   conf )     {", "return   store ;", "}", "@ Override", "protected   Dispatcher   createDispatcher ( Configuration   conf )     {", "MultiThreadedDispatcher   dispatcher    =    new   MultiThreadedDispatcher ( conf . getInt ( RM _ HISTORY _ WRITER _ MULTI _ THREADED _ DISPATCHER _ POOL _ SIZE ,    DEFAULT _ RM _ HISTORY _ WRITER _ MULTI _ THREADED _ DISPATCHER _ POOL _ SIZE )  )  ;", "dispatcher . setDrainEventsOnStop (  )  ;", "return   dispatcher ;", "}", "class   MultiThreadedDispatcher   extends   RMApplicationHistoryWriter . MultiThreadedDispatcher    {", "public   MultiThreadedDispatcher ( int   num )     {", "super ( num )  ;", "}", "@ Override", "protected   AsyncDispatcher   createDispatcher (  )     {", ". CounterDispatcher   dispatcher    =    new    . CounterDispatcher (  )  ;", ". this . dispatchers . add ( dispatcher )  ;", "return   dispatcher ;", "}", "}", "}  ;", "writer . init ( conf )  ;", "writer . start (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "writer . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "List < ApplicationId >    appIds    =    new   ArrayList < ApplicationId >  (  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;     +  + i )     {", "Random   rand    =    new   Random ( i )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  0  ,    rand . nextInt (  )  )  ;", "appIds . add ( appId )  ;", "RMApp   app    =     . createRMApp ( appId )  ;", "writer . applicationStarted ( app )  ;", "for    ( int   j    =     1  ;    j    <  =     1  0  ;     +  + j )     {", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    j )  ;", "RMAppAttempt   appAttempt    =     . createRMAppAttempt ( appAttemptId )  ;", "writer . applicationAttemptStarted ( appAttempt )  ;", "for    ( int   k    =     1  ;    k    <  =     1  0  ;     +  + k )     {", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,    k )  ;", "RMContainer   container    =     . createRMContainer ( containerId )  ;", "writer . containerStarted ( container )  ;", "writer . containerFinished ( container )  ;", "}", "writer . applicationAttemptFinished ( appAttempt ,    RMAppAttemptState . FINISHED )  ;", "}", "writer . applicationFinished ( app ,    RMAppState . FINISHED )  ;", "}", "for    ( int   i    =     0  ;    i    <     (  . MAX _ RETRIES )  ;     +  + i )     {", "if    ( allEventsHandled (  (  (  (  (  2  0     *     1  0  )     *     1  0  )     +     (  2  0     *     1  0  )  )     +     2  0  )  )  )     {", "break ;", "} else    {", "Thread . sleep (  5  0  0  )  ;", "}", "}", "Assert . assertTrue ( allEventsHandled (  (  (  (  (  2  0     *     1  0  )     *     1  0  )     +     (  2  0     *     1  0  )  )     +     2  0  )  )  )  ;", "for    ( ApplicationId   appId    :    appIds )     {", "Assert . assertTrue ( handledByOne ( appId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testParallelWrite"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "MockRM   rm    =    new   MockRM ( conf )     {", "@ Override", "protected      create (  )     {", "return   new    (  )     {", "@ Override", "public   void   applicationStarted ( RMApp   app )     {", "}", "@ Override", "public   void   applicationFinished ( RMApp   app ,    RMAppState   finalState )     {", "}", "@ Override", "public   void   applicationAttemptStarted ( RMAppAttempt   appAttempt )     {", "}", "@ Override", "public   void   applicationAttemptFinished ( RMAppAttempt   appAttempt ,    RMAppAttemptState   finalState )     {", "}", "@ Override", "public   void   containerStarted ( RMContainer   container )     {", "}", "@ Override", "public   void   containerFinished ( RMContainer   container )     {", "}", "}  ;", "}", "}  ;", "long   startTime 1     =    System . currentTimeMillis (  )  ;", "testRMWritingMassiveHistory ( rm )  ;", "long   finishTime 1     =    System . currentTimeMillis (  )  ;", "long   elapsedTime 1     =    finishTime 1     -    startTime 1  ;", "rm    =    new   MockRM ( conf )  ;", "long   startTime 2     =    System . currentTimeMillis (  )  ;", "testRMWritingMassiveHistory ( rm )  ;", "long   finishTime 2     =    System . currentTimeMillis (  )  ;", "long   elapsedTime 2     =    finishTime 2     -    startTime 2  ;", "Assert . assertTrue (  (  ( elapsedTime 2     -    elapsedTime 1  )     <     ( elapsedTime 1     /     1  0  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testRMWritingMassiveHistory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "rm . start (  )  ;", "MockNM   nm    =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  1  0  2  4     *     1  0  1  0  0  )  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  )  ;", "nm . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "int   request    =     1  0  0  0  0  ;", "am . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,    request ,    new   ArrayList < ContainerId >  (  )  )  ;", "nm . nodeHeartbeat ( true )  ;", "List < Container >    allocated    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "int   waitCount    =     0  ;", "int   allocatedSize    =    allocated . size (  )  ;", "while    (  ( allocatedSize    <    request )     &  &     (  ( waitCount +  +  )     <     2  0  0  )  )     {", "Thread . sleep (  3  0  0  )  ;", "allocated    =    am . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "allocatedSize    +  =    allocated . size (  )  ;", "nm . nodeHeartbeat ( true )  ;", "}", "Assert . assertEquals ( request ,    allocatedSize )  ;", "am . unregisterAppAttempt (  )  ;", "am . waitForState ( RMAppAttemptState . FINISHING )  ;", "nm . nodeHeartbeat ( am . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FINISHED )  ;", "NodeHeartbeatResponse   resp    =    nm . nodeHeartbeat ( true )  ;", "List < ContainerId >    cleaned    =    resp . getContainersToCleanup (  )  ;", "int   cleanedSize    =    cleaned . size (  )  ;", "waitCount    =     0  ;", "while    (  ( cleanedSize    <    allocatedSize )     &  &     (  ( waitCount +  +  )     <     2  0  0  )  )     {", "Thread . sleep (  3  0  0  )  ;", "resp    =    nm . nodeHeartbeat ( true )  ;", "cleaned    =    resp . getContainersToCleanup (  )  ;", "cleanedSize    +  =    cleaned . size (  )  ;", "}", "Assert . assertEquals ( allocatedSize ,    cleanedSize )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMWritingMassiveHistory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    TestRMApplicationHistoryWriter . createRMApp ( ApplicationId . newInstance (  0  ,     1  )  )  ;", "writer . applicationStarted ( app )  ;", "ApplicationHistoryData   appHD    =    null ;", "for    ( int   i    =     0  ;    i    <     ( TestRMApplicationHistoryWriter . MAX _ RETRIES )  ;     +  + i )     {", "appHD    =    store . getApplication ( ApplicationId . newInstance (  0  ,     1  )  )  ;", "if    ( appHD    !  =    null )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "Assert . assertNotNull ( appHD )  ;", "Assert . assertEquals (  \" test   app \"  ,    appHD . getApplicationName (  )  )  ;", "Assert . assertEquals (  \" test   app   type \"  ,    appHD . getApplicationType (  )  )  ;", "Assert . assertEquals (  \" test   user \"  ,    appHD . getUser (  )  )  ;", "Assert . assertEquals (  \" test   queue \"  ,    appHD . getQueue (  )  )  ;", "Assert . assertEquals (  0 L ,    appHD . getSubmitTime (  )  )  ;", "Assert . assertEquals (  1 L ,    appHD . getStartTime (  )  )  ;", "writer . applicationFinished ( app ,    RMAppState . FINISHED )  ;", "for    ( int   i    =     0  ;    i    <     ( TestRMApplicationHistoryWriter . MAX _ RETRIES )  ;     +  + i )     {", "appHD    =    store . getApplication ( ApplicationId . newInstance (  0  ,     1  )  )  ;", "if    (  ( appHD . getYarnApplicationState (  )  )     !  =    null )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "Assert . assertEquals (  2 L ,    appHD . getFinishTime (  )  )  ;", "Assert . assertEquals (  \" test   diagnostics   info \"  ,    appHD . getDiagnosticsInfo (  )  )  ;", "Assert . assertEquals ( UNDEFINED ,    appHD . getFinalApplicationStatus (  )  )  ;", "Assert . assertEquals ( FINISHED ,    appHD . getYarnApplicationState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testWriteApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMAppAttempt   appAttempt    =    TestRMApplicationHistoryWriter . createRMAppAttempt ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  )  ;", "writer . applicationAttemptStarted ( appAttempt )  ;", "ApplicationAttemptHistoryData   appAttemptHD    =    null ;", "for    ( int   i    =     0  ;    i    <     ( TestRMApplicationHistoryWriter . MAX _ RETRIES )  ;     +  + i )     {", "appAttemptHD    =    store . getApplicationAttempt ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  )  ;", "if    ( appAttemptHD    !  =    null )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "Assert . assertNotNull ( appAttemptHD )  ;", "Assert . assertEquals (  \" test   host \"  ,    appAttemptHD . getHost (  )  )  ;", "Assert . assertEquals (  (  -  1  0  0  )  ,    appAttemptHD . getRPCPort (  )  )  ;", "Assert . assertEquals ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  ,     1  )  ,    appAttemptHD . getMasterContainerId (  )  )  ;", "writer . applicationAttemptFinished ( appAttempt ,    RMAppAttemptState . FINISHED )  ;", "for    ( int   i    =     0  ;    i    <     ( TestRMApplicationHistoryWriter . MAX _ RETRIES )  ;     +  + i )     {", "appAttemptHD    =    store . getApplicationAttempt ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  )  ;", "if    (  ( appAttemptHD . getYarnApplicationAttemptState (  )  )     !  =    null )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "Assert . assertEquals (  \" test   diagnostics   info \"  ,    appAttemptHD . getDiagnosticsInfo (  )  )  ;", "Assert . assertEquals (  \" test   url \"  ,    appAttemptHD . getTrackingURL (  )  )  ;", "Assert . assertEquals ( UNDEFINED ,    appAttemptHD . getFinalApplicationStatus (  )  )  ;", "Assert . assertEquals ( FINISHED ,    appAttemptHD . getYarnApplicationAttemptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testWriteApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "RMContainer   container    =    TestRMApplicationHistoryWriter . createRMContainer ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  ,     1  )  )  ;", "writer . containerStarted ( container )  ;", "ContainerHistoryData   containerHD    =    null ;", "for    ( int   i    =     0  ;    i    <     ( TestRMApplicationHistoryWriter . MAX _ RETRIES )  ;     +  + i )     {", "containerHD    =    store . getContainer ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  ,     1  )  )  ;", "if    ( containerHD    !  =    null )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "Assert . assertNotNull ( containerHD )  ;", "Assert . assertEquals ( NodeId . newInstance (  \" test   host \"  ,     (  -  1  0  0  )  )  ,    containerHD . getAssignedNode (  )  )  ;", "Assert . assertEquals ( Resource . newInstance (  (  -  1  )  ,     (  -  1  )  )  ,    containerHD . getAllocatedResource (  )  )  ;", "Assert . assertEquals ( UNDEFINED ,    containerHD . getPriority (  )  )  ;", "Assert . assertEquals (  0 L ,    container . getCreationTime (  )  )  ;", "writer . containerFinished ( container )  ;", "for    ( int   i    =     0  ;    i    <     ( TestRMApplicationHistoryWriter . MAX _ RETRIES )  ;     +  + i )     {", "containerHD    =    store . getContainer ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  0  ,     1  )  ,     1  )  ,     1  )  )  ;", "if    (  ( containerHD . getContainerState (  )  )     !  =    null )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "Assert . assertEquals (  \" test   diagnostics   info \"  ,    containerHD . getDiagnosticsInfo (  )  )  ;", "Assert . assertEquals (  (  -  1  )  ,    containerHD . getContainerExitStatus (  )  )  ;", "Assert . assertEquals ( COMPLETE ,    containerHD . getContainerState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testWriteContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.TestRMApplicationHistoryWriter"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptFinish ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptFinishData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationAttemptFinishEvent"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationAttemptFinishEvent"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationAttemptStartEvent"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptStart ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptStartData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationAttemptStartEvent"}, {"methodBody": ["METHOD_START", "{", "return   appFinish ;", "}", "METHOD_END"], "methodName": ["getApplicationFinishData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationFinishEvent"}, {"methodBody": ["METHOD_START", "{", "return   appId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationFinishEvent"}, {"methodBody": ["METHOD_START", "{", "return   appId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationStartEvent"}, {"methodBody": ["METHOD_START", "{", "return   appStart ;", "}", "METHOD_END"], "methodName": ["getApplicationStartData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingApplicationStartEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerFinish ;", "}", "METHOD_END"], "methodName": ["getContainerFinishData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerFinishEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerFinishEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerStart ;", "}", "METHOD_END"], "methodName": ["getContainerStartData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent"}, {"methodBody": ["METHOD_START", "{", "connect (  )  ;", "Containd   containd    =    mastontaingetId (  )  ;", "List < Containd >    containds    =    new   ArrayList < Containd >  (  )  ;", "containds . add ( containd )  ;", "StopContainRequest   stopRequest    =    StopContainRequest . newInstance ( containds )  ;", "StopContainResponse   response    =    containgrProxy . stopContain ( stopRequest )  ;", "if    (  (  ( response . getFailedRequests (  )  )     !  =    null )     &  &     ( response . getFailedRequests (  )  . containsKey ( containd )  )  )     {", "Throwable   t    =    response . getFailedRequests (  )  . get ( containd )  . deSalize (  )  ;", "parseAndThrowException ( t )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "ContainerId   masterContainerID    =    masterContainer . getId (  )  ;", "containerMgrProxy    =    getContainerMgrProxy ( masterContainerID )  ;", "}", "METHOD_END"], "methodName": ["connect"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunchContext   container    =    applicationMasterContext . getAMContainerSpec (  )  ;", ". LOG . info (  (  (  (  \" Command   to   launch   container    \"     +    containerID )     +     \"     :     \"  )     +     ( StringUtils . arrayToString ( container . getCommands (  )  . toArray ( new   String [  0  ]  )  )  )  )  )  ;", "setupTokens ( container ,    containerID )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createAMContainerLaunchContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "Token < AMRMTokenIdentifier >    amrmToken    =    this . rmContext . getAMRMTokenSecretManager (  )  . createAndGetAMRMToken ( application . getAppAttemptId (  )  )  ;", "(  ( RMAppAttemptImpl )     ( application )  )  . setAMRMToken ( amrmToken )  ;", "return   amrmToken ;", "}", "METHOD_END"], "methodName": ["createAndSetAMRMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "final   NodeId   node    =    masterContainer . getNodeId (  )  ;", "final   InetSocketAddress   containerMBindAddress    =    NetUtils . createSocketAddrForHost ( node . getHost (  )  ,    node . getPort (  )  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( containerId . getApplicationAttemptId (  )  . toString (  )  )  ;", "String   user    =    rmContext . getRMApps (  )  . get ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  . getUser (  )  ;", "Token   token    =    rmContext . getNMTokenSecretM (  )  . createNMToken ( containerId . getApplicationAttemptId (  )  ,    node ,    user )  ;", "currentUser . addToken ( ConverterUtils . convertFromYarn ( token ,    containerMBindAddress )  )  ;", "return   currentUser . doAs ( new   PrivilegedAction < ContainerManagementProtocol >  (  )     {", "@ Override", "public   ContainerManagementProtocol   run (  )     {", "return    (  ( ContainerManagementProtocol )     ( rpc . getProxy ( ContainerManagementProtocol . class ,    containerMBindAddress ,    conf )  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getContainerMgrProxy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "connect (  )  ;", "ContainerId   masterContainerID    =    masterContainer . getId (  )  ;", "ApplicationSubmissionContext   applicationContext    =    application . getSubmissionContext (  )  ;", ". LOG . info (  (  (  (  \" Setting   up   container    \"     +     ( masterContainer )  )     +     \"    for   AM    \"  )     +     ( application . getAppAttemptId (  )  )  )  )  ;", "ContainerLaunchContext   launchContext    =    createAMContainerLaunchContext ( applicationContext ,    masterContainerID )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( launchContext ,    masterContainer . getContainerToken (  )  )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "StartContainersResponse   response    =    containerMgrProxy . startContainers ( allRequests )  ;", "if    (  (  ( response . getFailedRequests (  )  )     !  =    null )     &  &     ( response . getFailedRequests (  )  . containsKey ( masterContainerID )  )  )     {", "Throwable   t    =    response . getFailedRequests (  )  . get ( masterContainerID )  . deSerialize (  )  ;", "parseAndThrowException ( t )  ;", "} else    {", ". LOG . info (  (  (  (  \" Done   launching   container    \"     +     ( masterContainer )  )     +     \"    for   AM    \"  )     +     ( application . getAppAttemptId (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["launch"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "if    ( t   instanceof   YarnException )     {", "throw    (  ( YarnException )     ( t )  )  ;", "} else", "if    ( t   instanceof   security . token . SecretManager . InvalidToken )     {", "throw    (  ( security . token . SecretManager . InvalidToken )     ( t )  )  ;", "} else    {", "throw    (  ( IOException )     ( t )  )  ;", "}", "}", "METHOD_END"], "methodName": ["parseAndThrowException"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "switch    ( eventType )     {", "case   LAUNCH    :", "try    {", ". LOG . info (  (  \" Launching   master \"     +     ( application . getAppAttemptId (  )  )  )  )  ;", "launch (  )  ;", "handler . handle ( new   RMAppAttemptEvent ( application . getAppAttemptId (  )  ,    RMAppAttemptEventType . LAUNCHED )  )  ;", "}    catch    ( Exception   ie )     {", "String   message    =     (  (  \" Error   launching    \"     +     ( application . getAppAttemptId (  )  )  )     +     \"  .    Got   exception :     \"  )     +     ( StringUtils . stringifyException ( ie )  )  ;", ". LOG . info ( message )  ;", "handler . handle ( new   RMAppAttemptLaunchFailedEvent ( application . getAppAttemptId (  )  ,    message )  )  ;", "}", "break ;", "case   CLEANUP    :", "try    {", ". LOG . info (  (  \" Cleaning   master    \"     +     ( application . getAppAttemptId (  )  )  )  )  ;", "cleanup (  )  ;", "}    catch    ( IOException   ie )     {", ". LOG . info (  \" Error   cleaning   master    \"  ,    ie )  ;", "}    catch    ( YarnException   e )     {", "StringBuilder   sb    =    new   StringBuilder (  \" Container    \"  )  ;", "sb . append ( masterContainer . getId (  )  . toString (  )  )  ;", "sb . append (  \"    is   not   handled   by   this   NodeManager \"  )  ;", "if    (  !  ( e . getMessage (  )  . contains ( sb . toString (  )  )  )  )     {", ". LOG . info (  \" Error   cleaning   master    \"  ,    e )  ;", "}", "}", "break ;", "default    :", ". LOG . warn (  (  (  \" Received   unknown   event - type    \"     +     ( eventType )  )     +     \"  .    Ignoring .  \"  )  )  ;", "break ;", "}", "}", "METHOD_END"], "methodName": ["run"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    environment    =    container . getEnvironment (  )  ;", "environment . put ( APPLICATION _ WEB _ PROXY _ BASE _ ENV ,    application . getWebProxyBase (  )  )  ;", "ApplicationId   applicationId    =    application . getAppAttemptId (  )  . getApplicationId (  )  ;", "environment . put ( APP _ SUBMIT _ TIME _ ENV ,    String . valueOf ( rmContext . getRMApps (  )  . get ( applicationId )  . getSubmitTime (  )  )  )  ;", "environment . put ( MAX _ APP _ ATTEMPTS _ ENV ,    String . valueOf ( rmContext . getRMApps (  )  . get ( applicationId )  . getMaxAppAttempts (  )  )  )  ;", "Credentials   credentials    =    new   Credentials (  )  ;", "DataInputByteBuffer   dibb    =    new   DataInputByteBuffer (  )  ;", "if    (  ( container . getTokens (  )  )     !  =    null )     {", "dibbet ( container . getTokens (  )  )  ;", "credentials . readTokenStorageStream ( dibb )  ;", "}", "Token < AMRMTokenIdentifier >    amrmToken    =    createAndSetAMRMToken (  )  ;", "if    ( amrmToken    !  =    null )     {", "credentials . addToken ( amrmToken . getService (  )  ,    amrmToken )  ;", "}", "DataOutputBuffer   dob    =    new   DataOutputBuffer (  )  ;", "credentials . writeTokenStorageToStream ( dob )  ;", "container . setTokens ( ByteBuffer . wrap ( dob . getData (  )  ,     0  ,    dob . getLength (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setupTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"}, {"methodBody": ["METHOD_START", "{", "return   this . appAttempt ;", "}", "METHOD_END"], "methodName": ["getAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEvent"}, {"methodBody": ["METHOD_START", "{", "Runnable   launcher    =    createRunnableLauncher ( application ,    AMLauncherEventType . CLEANUP )  ;", "masterEvents . add ( launcher )  ;", "}", "METHOD_END"], "methodName": ["cleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher"}, {"methodBody": ["METHOD_START", "{", "Runnable   launcher    =    new   AMLauncher ( context ,    application ,    event ,    getConfig (  )  )  ;", "return   launcher ;", "}", "METHOD_END"], "methodName": ["createRunnableLauncher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher"}, {"methodBody": ["METHOD_START", "{", "Runnable   launcher    =    createRunnableLauncher ( application ,    AMLauncherEventType . LAUNCH )  ;", "masterEvents . add ( launcher )  ;", "}", "METHOD_END"], "methodName": ["launch"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher"}, {"methodBody": ["METHOD_START", "{", "final   ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( newAppID ( i )  ,     0  )  ;", "final   Container   masterContainer    =    Records . newRecord ( Container . class )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptId ,     0  )  ;", "masterContainer . setId ( containerId )  ;", "masterContainer . setNodeHttpAddress (  \" node : port \"  )  ;", "final   String   user    =    newUserName (  )  ;", "final   String   name    =    newAppName (  )  ;", "final   String   queue    =    newQueue (  )  ;", "final   long   start    =     1  2  3  4  5  6     +     ( i    *     1  0  0  0  )  ;", "final   long   finish    =     2  3  4  5  6  7     +     ( i    *     1  0  0  0  )  ;", "final   String   type    =    YarnConfiguration . DEFAULT _ APPLICATION _ TYPE ;", "YarnApplicationState [  ]    allStates    =    YarnApplicationState . values (  )  ;", "final   YarnApplicationState   state    =    allStates [  ( i    %     ( allStates . length )  )  ]  ;", "final   int   maxAppAttempts    =    i    %     1  0  0  0  ;", "return   new   MockAsm . ApplicationBase (  )     {", "@ Override", "public   ApplicationId   getApplicationId (  )     {", "return   appAttemptId . getApplicationId (  )  ;", "}", "@ Override", "public   String   getUser (  )     {", "return   user ;", "}", "@ Override", "public   String   getName (  )     {", "return   name ;", "}", "@ Override", "public   String   getApplicationType (  )     {", "return   type ;", "}", "@ Override", "public   String   getQueue (  )     {", "return   queue ;", "}", "@ Override", "public   long   getStartTime (  )     {", "return   start ;", "}", "@ Override", "public   long   getFinishTime (  )     {", "return   finish ;", "}", "@ Override", "public   String   getTrackingUrl (  )     {", "return   null ;", "}", "@ Override", "public   YarnApplicationState   createApplicationState (  )     {", "return   state ;", "}", "@ Override", "public   StringBuilder   getDiagnostics (  )     {", "return   new   StringBuilder (  )  ;", "}", "@ Override", "public   float   getProgress (  )     {", "return    (  ( float )     ( Math . random (  )  )  )  ;", "}", "@ Override", "public   FinalApplicationStatus   getFinalApplicationStatus (  )     {", "return   FinalApplicationStatus . UNDEFINED ;", "}", "@ Override", "public   RMAppAttempt   getCurrentAppAttempt (  )     {", "return   null ;", "}", "@ Override", "public   int   getMaxAppAttempts (  )     {", "return   maxAppAttempts ;", "}", "@ Override", "public   Set < String >    getApplicationTags (  )     {", "return   null ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["newApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.MockAsm"}, {"methodBody": ["METHOD_START", "{", "List < RMApp >    list    =    Lists . newArrayList (  )  ;", "for    ( int   i    =     0  ;    i    <    n ;     +  + i )     {", "list . add ( MockAsm . newA ( i )  )  ;", "}", "return   list ;", "}", "METHOD_END"], "methodName": ["newApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.MockAsm"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( attemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    rm . getRMContext (  )  . getRMApps (  )  . get ( attemptId . getAId (  )  )  . getRMAppAttempt ( attemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "return   ugi . doAs ( new   PrivilegedExceptionAction < AllocateResponse >  (  )     {", "@ Override", "public   AllocateResponse   run (  )    throws   Exception    {", "return   amService . allocate ( req )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"}, {"methodBody": ["METHOD_START", "{", "dispatcher    =    new   DrainDispatcher (  )  ;", "this . rm    =    new   MockRM (  )     {", "@ Override", "protected   EventHandler < SchedulerEvent >    createSchedulerEventDispatcher (  )     {", "return   new   ResourceManager . SchedulerEventDispatcher ( this . scheduler )     {", "@ Override", "public   void   handle ( SchedulerEvent   event )     {", "this . scheduler . handle ( event )  ;", "}", "}  ;", "}", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "}  ;", "rm . start (  )  ;", "amService    =    rm . getAMasterService (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"}, {"methodBody": ["METHOD_START", "{", "nm . nodeHeartbeat ( health )  ;", "dispatchwait (  )  ;", "}", "METHOD_END"], "methodName": ["syncNodeHeartbeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"}, {"methodBody": ["METHOD_START", "{", "rm . sendNodeStarted ( nm )  ;", "rm . NMwaitForState ( nm . getNodeId (  )  ,    RUNNING )  ;", "rm . sendNodeLost ( nm )  ;", "dispatcher . await (  )  ;", "}", "METHOD_END"], "methodName": ["syncNodeLost"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"}, {"methodBody": ["METHOD_START", "{", "if    ( m )     !  =    null )     {", "thism . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "MockNM   nm 2     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  2  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "MockNM   nm 3     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  3  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "MockNM   nm 4     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  4  :  1  2  3  4  \"  ,     1  0  0  0  0  )  ;", "dispatcher . await (  )  ;", "RMApp   app 1     =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", "AllocateRequest   allocateRequest 1     =    AllocateRequest . newInstance (  0  ,     0  .  0 F ,    null ,    null ,    null )  ;", "AllocateResponse   response 1     =    allocate ( attempt 1  . getAppAttemptId (  )  ,    allocateRequest 1  )  ;", "List < NodeReport >    updatedNodes    =    response 1  . getdNodes (  )  ;", "Assert . assertEquals (  0  ,    updatedNodes . size (  )  )  ;", "syncNodeHeartbeat ( nm 4  ,    false )  ;", "allocateRequest 1     =    AllocateRequest . newInstance ( response 1  . getResponseId (  )  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response 1     =    allocate ( attempt 1  . getAppAttemptId (  )  ,    allocateRequest 1  )  ;", "updatedNodes    =    response 1  . getdNodes (  )  ;", "Assert . assertEquals (  1  ,    updatedNodes . size (  )  )  ;", "NodeReport   nr    =    updatedNodes . iterator (  )  . next (  )  ;", "Assert . assertEquals ( nm 4  . getNodeId (  )  ,    nr . getNodeId (  )  )  ;", "Assert . assertEquals ( UNHEALTHY ,    nr . getNodeState (  )  )  ;", "response 1     =    allocate ( attempt 1  . getAppAttemptId (  )  ,    allocateRequest 1  )  ;", "updatedNodes    =    response 1  . getdNodes (  )  ;", "Assert . assertEquals (  1  ,    updatedNodes . size (  )  )  ;", "nr    =    updatedNodes . iterator (  )  . next (  )  ;", "Assert . assertEquals ( nm 4  . getNodeId (  )  ,    nr . getNodeId (  )  )  ;", "Assert . assertEquals ( UNHEALTHY ,    nr . getNodeState (  )  )  ;", "syncNodeLost ( nm 3  )  ;", "allocateRequest 1     =    AllocateRequest . newInstance ( response 1  . getResponseId (  )  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response 1     =    allocate ( attempt 1  . getAppAttemptId (  )  ,    allocateRequest 1  )  ;", "updatedNodes    =    response 1  . getdNodes (  )  ;", "Assert . assertEquals (  1  ,    updatedNodes . size (  )  )  ;", "nr    =    updatedNodes . iterator (  )  . next (  )  ;", "Assert . assertEquals ( nm 3  . getNodeId (  )  ,    nr . getNodeId (  )  )  ;", "Assert . assertEquals ( LOST ,    nr . getNodeState (  )  )  ;", "RMApp   app 2     =    rm . submitApp (  2  0  0  0  )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 2     =    app 2  . getCurrentAppAttempt (  )  ;", "MockAM   am 2     =    rm . sendAMLaunched ( attempt 2  . getAppAttemptId (  )  )  ;", "am 2  . registerAppAttempt (  )  ;", "AllocateRequest   allocateRequest 2     =    AllocateRequest . newInstance (  0  ,     0  .  0 F ,    null ,    null ,    null )  ;", "AllocateResponse   response 2     =    allocate ( attempt 2  . getAppAttemptId (  )  ,    allocateRequest 2  )  ;", "updatedNodes    =    response 2  . getdNodes (  )  ;", "Assert . assertEquals (  0  ,    updatedNodes . size (  )  )  ;", "syncNodeHeartbeat ( nm 4  ,    true )  ;", "allocateRequest 1     =    AllocateRequest . newInstance ( response 1  . getResponseId (  )  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response 1     =    allocate ( attempt 1  . getAppAttemptId (  )  ,    allocateRequest 1  )  ;", "updatedNodes    =    response 1  . getdNodes (  )  ;", "Assert . assertEquals (  1  ,    updatedNodes . size (  )  )  ;", "nr    =    updatedNodes . iterator (  )  . next (  )  ;", "Assert . assertEquals ( nm 4  . getNodeId (  )  ,    nr . getNodeId (  )  )  ;", "Assert . assertEquals ( RUNNING ,    nr . getNodeState (  )  )  ;", "allocateRequest 2     =    AllocateRequest . newInstance ( response 2  . getResponseId (  )  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response 2     =    allocate ( attempt 2  . getAppAttemptId (  )  ,    allocateRequest 2  )  ;", "updatedNodes    =    response 2  . getdNodes (  )  ;", "Assert . assertEquals (  1  ,    updatedNodes . size (  )  )  ;", "nr    =    updatedNodes . iterator (  )  . next (  )  ;", "Assert . assertEquals ( nm 4  . getNodeId (  )  ,    nr . getNodeId (  )  )  ;", "Assert . assertEquals ( RUNNING ,    nr . getNodeState (  )  )  ;", "allocateRequest 2     =    AllocateRequest . newInstance ( response 2  . getResponseId (  )  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response 2     =    allocate ( attempt 2  . getAppAttemptId (  )  ,    allocateRequest 2  )  ;", "updatedNodes    =    response 2  . getdNodes (  )  ;", "Assert . assertEquals (  0  ,    updatedNodes . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAMRMUnusableNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser ( attemptId . toString (  )  )  ;", "Token < AMRMTokenIdentifier >    token    =    rm . getRMContext (  )  . getRMApps (  )  . get ( attemptId . getAId (  )  )  . getRMAppAttempt ( attemptId )  . getAMRMToken (  )  ;", "ugi . addTokenIdentifier ( token . decodeIdentifier (  )  )  ;", "return   ugi . doAs ( new   PrivilegedExceptionAction < AllocateResponse >  (  )     {", "@ Override", "public   AllocateResponse   run (  )    throws   Exception    {", "return   amService . allocate ( req )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId"}, {"methodBody": ["METHOD_START", "{", "this . rm    =    new   MockRM (  )  ;", "rm . start (  )  ;", "amService    =    rm . getAMasterService (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId"}, {"methodBody": ["METHOD_START", "{", "if    ( m )     !  =    null )     {", "thism . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm 1     =    rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  0  0  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  0  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "MockAM   am    =    rm . sendAMLaunched ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "AllocateRequest   allocateRequest    =    AllocateRequest . newInstance (  0  ,     0  .  0 F ,    null ,    null ,    null )  ;", "AllocateResponse   response    =    allocate ( attempt . getAppAttemptId (  )  ,    allocateRequest )  ;", "Assert . assertEquals (  1  ,    response . get (  )  )  ;", "Assert . assertTrue (  (  ( response . getAMCommand (  )  )     =  =    null )  )  ;", "allocateRequest    =    AllocateRequest . newInstance ( response . get (  )  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response    =    allocate ( attempt . getAppAttemptId (  )  ,    allocateRequest )  ;", "Assert . assertEquals (  2  ,    response . get (  )  )  ;", "response    =    allocate ( attempt . getAppAttemptId (  )  ,    allocateRequest )  ;", "Assert . assertEquals (  2  ,    response . get (  )  )  ;", "allocateRequest    =    AllocateRequest . newInstance (  0  ,     0  .  0 F ,    null ,    null ,    null )  ;", "response    =    allocate ( attempt . getAppAttemptId (  )  ,    allocateRequest )  ;", "Assert . assertTrue (  (  ( response . getAMCommand (  )  )     =  =     ( AMCommand . AM _ RESYNC )  )  )  ;", "}", "METHOD_END"], "methodName": ["testARRMResponseId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     2  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  ,     \" name \"  ,     \" user \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     (  -  1  )  ,    null ,     \" MAPREDUCE \"  ,    false ,    true )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  0  2  4  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  2  3  5  1  \"  ,     4  0  8  9  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 2  . registerNode (  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "int   NUM _ CONTAINERS    =     3  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,    NUM _ CONTAINERS ,    new   ArrayList < ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "List < Container >    containers    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "while    (  ( containers . size (  )  )     !  =    NUM _ CONTAINERS )     {", "nm 1  . nodeHeartbeat ( true )  ;", "containers . addAll ( am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  )  ;", "Thread . sleep (  2  0  0  )  ;", "}", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "ContainerId   containerId 2     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . RUNNING )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     3  ,    RUNNING )  ;", "ContainerId   containerId 3     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     3  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 3  ,    RMContainerState . RUNNING )  ;", "ContainerId   containerId 4     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     4  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 4  ,    RMContainerState . ACQUIRED )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "ContainerId   containerId 5     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     5  )  ;", "rm 1  . waitForContainerAllocated ( nm 1  ,    containerId 5  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 5  ,    RMContainerState . ALLOCATED )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     6  0  0  0  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "ContainerId   containerId 6     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     6  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "SchedulerApplicationAttempt   schedulerAttempt    =     (  ( AbstractYarnScheduler )     ( rm 1  . getResourceScheduler (  )  )  )  . getCurrentAttemptForContainer ( containerId 6  )  ;", "while    ( schedulerAttempt . getReservedContainers (  )  . isEmpty (  )  )     {", "System . out . println (  (  (  \" Waiting   for   container    \"     +    containerId 6  )     +     \"    to   be   reserved .  \"  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "Thread . sleep (  2  0  0  )  ;", "}", "Assert . assertEquals ( containerId 6  ,    schedulerAttempt . getReservedContainers (  )  . get (  0  )  . getContainerId (  )  )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 1  . waitForState ( RMAppAttemptState . FAILED )  ;", "Thread . sleep (  3  0  0  0  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . RUNNING )  ;", "Assert . assertNull ( rm 1  . getResourceScheduler (  )  . getRMContainer ( containerId 4  )  )  ;", "Assert . assertNull ( rm 1  . getResourceScheduler (  )  . getRMContainer ( containerId 5  )  )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "ApplicationAttemptId   newAttemptId    =    app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ;", "Assert . assertFalse ( newAttemptId . equals ( am 1  . getApplicationAttemptId (  )  )  )  ;", "RMAppAttempt   attempt 2     =    app 1  . getCurrentAppAttempt (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "MockAM   am 2     =    rm 1  . sendAMLaunched ( attempt 2  . getAppAttemptId (  )  )  ;", "RegisterApplicationMasterResponse   registerResponse    =    am 2  . registerAppAttempt (  )  ;", "Assert . assertEquals (  2  ,    registerResponse . getContainersFromPreviousAttempts (  )  . size (  )  )  ;", "boolean   containerId 2 Exists    =    false ;", "boolean   containerId 3 Exists    =    false ;", "for    ( Container   container    :    registerResponse . getContainersFromPreviousAttempts (  )  )     {", "if    ( container . getId (  )  . equals ( containerId 2  )  )     {", "containerId 2 Exists    =    true ;", "}", "if    ( container . getId (  )  . equals ( containerId 3  )  )     {", "containerId 3 Exists    =    true ;", "}", "}", "Assert . assertTrue (  ( containerId 2 Exists    &  &    containerId 3 Exists )  )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     3  ,    COMPLETE )  ;", "RMAppAttempt   newAttempt    =    app 1  . getRMAppAttempt ( am 2  . getApplicationAttemptId (  )  )  ;", "waitForContainersToFinish (  4  ,    newAttempt )  ;", "boolean   container 3 Exists    =    false ;", "boolean   container 4 Exists    =    false ;", "boolean   container 5 Exists    =    false ;", "boolean   container 6 Exists    =    false ;", "for    ( ContainerStatus   status    :    newAttempt . getJustFinishedContainers (  )  )     {", "if    ( status . getContainerId (  )  . equals ( containerId 3  )  )     {", "container 3 Exists    =    true ;", "}", "if    ( status . getContainerId (  )  . equals ( containerId 4  )  )     {", "container 4 Exists    =    true ;", "}", "if    ( status . getContainerId (  )  . equals ( containerId 5  )  )     {", "container 5 Exists    =    true ;", "}", "if    ( status . getContainerId (  )  . equals ( containerId 6  )  )     {", "container 6 Exists    =    true ;", "}", "}", "Assert . assertTrue (  (  (  ( container 3 Exists    &  &    container 4 Exists )     &  &    container 5 Exists )     &  &    container 6 Exists )  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . RUNNING )  ;", "SchedulerApplicationAttempt   schedulerNewAttempt    =     (  ( AbstractYarnScheduler )     ( rm 1  . getResourceScheduler (  )  )  )  . getCurrentAttemptForContainer ( containerId 2  )  ;", "MockRM . finishAMAndVerifyAppState ( app 1  ,    rm 1  ,    nm 1  ,    am 2  )  ;", "Assert . assertFalse ( schedulerNewAttempt . getLiveContainers (  )  . contains ( containerId 2  )  )  ;", "System . out . println (  (  \" New   attempt ' s   just   finished   containers :     \"     +     ( newAttempt . getJustFinishedContainers (  )  )  )  )  ;", "waitForContainersToFinish (  5  ,    newAttempt )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAMRestartWithExistingContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     3  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  ,     \" myname \"  ,     \" myuser \"  ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  ,    false ,     \" default \"  ,     (  -  1  )  ,    null ,     \" MAPREDUCE \"  ,    false ,    true )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  1  .  1  .  1  :  4  3  2  1  \"  ,     8  0  0  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 2  . registerNode (  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "List < Container >    containers    =    new   ArrayList < Container >  (  )  ;", "List < NMToken >    expectedNMTokens    =    new   ArrayList < NMToken >  (  )  ;", "while    ( true )     {", "AllocateResponse   response    =    am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     2  0  0  0  ,     2  ,    new   ArrayList < ContainerId >  (  )  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "containers . addAll ( response . getAllocatedContainers (  )  )  ;", "expectedNMTokens . addAll ( response . getNMTokens (  )  )  ;", "if    (  ( containers . size (  )  )     =  =     2  )     {", "break ;", "}", "Thread . sleep (  2  0  0  )  ;", "System . out . println (  \" Waiting   for   container   to   be   allocated .  \"  )  ;", "}", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "ContainerId   containerId 2     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . RUNNING )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     3  ,    RUNNING )  ;", "ContainerId   containerId 3     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     3  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 3  ,    RMContainerState . RUNNING )  ;", "nm 1  . nodeHeartbeat ( am 1  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 1  . waitForState ( RMAppAttemptState . FAILED )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "MockAM   am 2     =    MockRM . launchAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "RegisterApplicationMasterResponse   registerResponse    =    am 2  . registerAppAttempt (  )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "Assert . assertEquals ( expectedNMTokens ,    registerResponse . getNMTokensFromPreviousAttempts (  )  )  ;", "containers    =    new   ArrayList < Container >  (  )  ;", "while    ( true )     {", "AllocateResponse   allocateResponse    =    am 2  . allocate (  \"  1  2  7  .  1  .  1  .  1  \"  ,     4  0  0  0  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "containers . addAll ( allocateResponse . getAllocatedContainers (  )  )  ;", "expectedNMTokens . addAll ( allocateResponse . getNMTokens (  )  )  ;", "if    (  ( containers . size (  )  )     =  =     1  )     {", "break ;", "}", "Thread . sleep (  2  0  0  )  ;", "System . out . println (  \" Waiting   for   container   to   be   allocated .  \"  )  ;", "}", "nm 1  . nodeHeartbeat ( am 2  . getApplicationAttemptId (  )  ,     2  ,    RUNNING )  ;", "ContainerId   am 2 ContainerId 2     =    ContainerId . newInstance ( am 2  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    am 2 ContainerId 2  ,    RMContainerState . RUNNING )  ;", "nm 1  . nodeHeartbeat ( am 2  . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 2  . waitForState ( RMAppAttemptState . FAILED )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "MockAM   am 3     =    MockRM . launchAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "registerResponse    =    am 3  . registerAppAttempt (  )  ;", "rm 1  . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "List < NMToken >    transferredTokens    =    registerResponse . getNMTokensFromPreviousAttempts (  )  ;", "Assert . assertEquals (  2  ,    transferredTokens . size (  )  )  ;", "Assert . assertTrue ( transferredTokens . containsAll ( expectedNMTokens )  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNMTokensRebindOnAMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "conf . setBoolean ( RECOVERY _ ENABLED ,    true )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "CapacityScheduler   scheduler    =     (  ( CapacityScheduler )     ( rm 1  . getResourceScheduler (  )  )  )  ;", "ContainerId   amContainer    =    ContainerId . newInstance ( am 1  . getAAttemptId (  )  ,     1  )  ;", "scheduler . killContainer ( scheduler . getRMContainer ( amContainer )  )  ;", "am 1  . waitForState ( RMAppAttemptState . FAILED )  ;", "Assert . assertTrue (  (  !  ( attempt 1  . shouldCountTowardsMaxAttemptRetry (  )  )  )  )  ;", "rm 1  . waitForState ( app 1  . getAId (  )  ,    RMAppState . ACCEPTED )  ;", "RMStateStore . AState   appState    =    memStore . getState (  )  . getAState (  )  . get ( app 1  . getAId (  )  )  ;", "Assert . assertEquals (  1  ,    appState . getAttemptCount (  )  )  ;", "Assert . assertEquals ( PREEMPTED ,    appState . getAttempt ( am 1  . getAAttemptId (  )  )  . getAMContainerExitStatus (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "rm 2  . start (  )  ;", "MockAM   am 2     =    rm 2  . waitForNewAMToLaunchAndRegister ( app 1  . getAId (  )  ,     2  ,    nm 1  )  ;", "MockRM . finishAMAndVerifyAppState ( app 1  ,    rm 2  ,    nm 1  ,    am 2  )  ;", "RMAppAttempt   attempt 2     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getAId (  )  )  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue ( attempt 2  . shouldCountTowardsMaxAttemptRetry (  )  )  ;", "Assert . assertEquals ( INVALID ,    appState . getAttempt ( am 2  . getAAttemptId (  )  )  . getAMContainerExitStatus (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testPreemptedAMRestartOnRMRestart"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "conf . setBoolean ( RECOVERY _ ENABLED ,    true )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue (  (  ( RMAppAttemptImpl )     ( attempt 1  )  )  . mayBeLastAttempt (  )  )  ;", "MockRM   rm 2     =    new   MockRM ( conf ,    memStore )  ;", "rm 2  . start (  )  ;", "RMStateStore . AState   appState    =    memStore . getState (  )  . getAState (  )  . get ( app 1  . getAId (  )  )  ;", "nm 1  . setResourceTrackerService ( rm 2  . getResourceTrackerService (  )  )  ;", "NMContainerStatus   status    =    Records . newRecord ( NMContainerStatus . class )  ;", "status . setContainerExitStatus ( KILLED _ BY _ RESOURCEMANAGER )  ;", "status . setContainerId ( attempt 1  . getMasterContainer (  )  . getId (  )  )  ;", "status . setContainerState ( COMPLETE )  ;", "status . setDiagnostics (  \"  \"  )  ;", "nm 1  . registerNode ( Collections . singletonList ( status )  ,    null )  ;", "rm 2  . waitForState ( attempt 1  . getAppAttemptId (  )  ,    RMAppAttemptState . FAILED )  ;", "Assert . assertEquals ( KILLED _ BY _ RESOURCEMANAGER ,    appState . getAttempt ( am 1  . getAAttemptId (  )  )  . getAMContainerExitStatus (  )  )  ;", "rm 2  . waitForState ( app 1  . getAId (  )  ,    RMAppState . ACCEPTED )  ;", "MockAM   am 2     =    rm 2  . waitForNewAMToLaunchAndRegister ( app 1  . getAId (  )  ,     2  ,    nm 1  )  ;", "MockRM . finishAMAndVerifyAppState ( app 1  ,    rm 2  ,    nm 1  ,    am 2  )  ;", "RMAppAttempt   attempt 3     =    rm 2  . getRMContext (  )  . getRMApps (  )  . get ( app 1  . getAId (  )  )  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue ( attempt 3  . shouldCountTowardsMaxAttemptRetry (  )  )  ;", "Assert . assertEquals ( INVALID ,    appState . getAttempt ( am 2  . getAAttemptId (  )  )  . getAMContainerExitStatus (  )  )  ;", "rm 1  . stop (  )  ;", "rm 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMRestartOrFailoverNotCountedForAMFailures"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     1  )  ;", "conf . setBoolean ( RECOVERY _ ENABLED ,    true )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "MockRM   rm 1     =    new   MockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "CapacityScheduler   scheduler    =     (  ( CapacityScheduler )     ( rm 1  . getResourceScheduler (  )  )  )  ;", "ContainerId   amContainer    =    ContainerId . newInstance ( am 1  . getAAttemptId (  )  ,     1  )  ;", "scheduler . killContainer ( scheduler . getRMContainer ( amContainer )  )  ;", "am 1  . waitForState ( RMAppAttemptState . FAILED )  ;", "Assert . assertTrue (  (  !  ( attempt 1  . shouldCountTowardsMaxAttemptRetry (  )  )  )  )  ;", "rm 1  . waitForState ( app 1  . getAId (  )  ,    RMAppState . ACCEPTED )  ;", "RMStateStore . AState   appState    =    memStore . getState (  )  . getAState (  )  . get ( app 1  . getAId (  )  )  ;", "MockAM   am 2     =    rm 1  . waitForNewAMToLaunchAndRegister ( app 1  . getAId (  )  ,     2  ,    nm 1  )  ;", "RMAppAttempt   attempt 2     =    app 1  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue (  (  ( RMAppAttemptImpl )     ( attempt 2  )  )  . mayBeLastAttempt (  )  )  ;", "ContainerId   amContainer 2     =    ContainerId . newInstance ( am 2  . getAAttemptId (  )  ,     1  )  ;", "scheduler . killContainer ( scheduler . getRMContainer ( amContainer 2  )  )  ;", "am 2  . waitForState ( RMAppAttemptState . FAILED )  ;", "Assert . assertTrue (  (  !  ( attempt 2  . shouldCountTowardsMaxAttemptRetry (  )  )  )  )  ;", "rm 1  . waitForState ( app 1  . getAId (  )  ,    RMAppState . ACCEPTED )  ;", "MockAM   am 3     =    rm 1  . waitForNewAMToLaunchAndRegister ( app 1  . getAId (  )  ,     3  ,    nm 1  )  ;", "RMAppAttempt   attempt 3     =    app 1  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue (  (  ( RMAppAttemptImpl )     ( attempt 3  )  )  . mayBeLastAttempt (  )  )  ;", "ContainerStatus   containerStatus    =    Records . newRecord ( ContainerStatus . class )  ;", "containerStatus . setContainerId ( attempt 3  . getMasterContainer (  )  . getId (  )  )  ;", "containerStatus . setDiagnostics (  \" mimic   NM   disk _ failure \"  )  ;", "containerStatus . setState ( COMPLETE )  ;", "containerStatus . setExitStatus ( DISKS _ FAILED )  ;", "Map < AId ,    List < ContainerStatus >  >    conts    =    new   HashMap < AId ,    List < ContainerStatus >  >  (  )  ;", "conts . put ( app 1  . getAId (  )  ,    Collections . singletonList ( containerStatus )  )  ;", "nm 1  . nodeHeartbeat ( conts ,    true )  ;", "am 3  . waitForState ( RMAppAttemptState . FAILED )  ;", "Assert . assertTrue (  (  !  ( attempt 3  . shouldCountTowardsMaxAttemptRetry (  )  )  )  )  ;", "Assert . assertEquals ( DISKS _ FAILED ,    appState . getAttempt ( am 3  . getAAttemptId (  )  )  . getAMContainerExitStatus (  )  )  ;", "rm 1  . waitForState ( app 1  . getAId (  )  ,    RMAppState . ACCEPTED )  ;", "MockAM   am 4     =    rm 1  . waitForNewAMToLaunchAndRegister ( app 1  . getAId (  )  ,     4  ,    nm 1  )  ;", "RMAppAttempt   attempt 4     =    app 1  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue (  (  ( RMAppAttemptImpl )     ( attempt 4  )  )  . mayBeLastAttempt (  )  )  ;", "MockNM   nm 2     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  2  2  3  4  \"  ,     8  0  0  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 2  . registerNode (  )  ;", "nm 1  . nodeHeartbeat ( false )  ;", "am 4  . waitForState ( RMAppAttemptState . FAILED )  ;", "Assert . assertTrue (  (  !  ( attempt 4  . shouldCountTowardsMaxAttemptRetry (  )  )  )  )  ;", "Assert . assertEquals ( ABORTED ,    appState . getAttempt ( am 4  . getAAttemptId (  )  )  . getAMContainerExitStatus (  )  )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "MockAM   am 5     =    rm 1  . waitForNewAMToLaunchAndRegister ( app 1  . getAId (  )  ,     5  ,    nm 2  )  ;", "RMAppAttempt   attempt 5     =    app 1  . getCurrentAppAttempt (  )  ;", "Assert . assertTrue (  (  ( RMAppAttemptImpl )     ( attempt 5  )  )  . mayBeLastAttempt (  )  )  ;", "nm 2  . nodeHeartbeat ( am 5  . getAAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am 5  . waitForState ( RMAppAttemptState . FAILED )  ;", "Assert . assertTrue ( attempt 5  . shouldCountTowardsMaxAttemptRetry (  )  )  ;", "rm 1  . waitForState ( app 1  . getAId (  )  ,    RMAppState . FAILED )  ;", "Assert . assertEquals (  5  ,    app 1  . getAppAttempts (  )  . size (  )  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testShouldNotCountFailureToMaxAttemptRetry"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart"}, {"methodBody": ["METHOD_START", "{", "int   count    =     0  ;", "while    (  (  ( attempt . getJustFinishedContainers (  )  . size (  )  )     !  =    expectedNum )     &  &     ( count    <     5  0  0  )  )     {", "Thread . sleep (  1  0  0  )  ;", "count +  +  ;", "}", "}", "METHOD_END"], "methodName": ["waitForContainersToFinish"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRestart"}, {"methodBody": ["METHOD_START", "{", "return   monitorInterval ;", "}", "METHOD_END"], "methodName": ["getMonitorInterval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor"}, {"methodBody": ["METHOD_START", "{", "return   scheduleEditPolicy ;", "}", "METHOD_END"], "methodName": ["getSchedulingEditPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor"}, {"methodBody": ["METHOD_START", "{", "scheduleEditPolicy . editSchedule (  )  ;", "}", "METHOD_END"], "methodName": ["invokePolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor"}, {"methodBody": ["METHOD_START", "{", "scheduleEditPolicy . init ( conf ,    rmContext . getDispatcher (  )  . getEventHandler (  )  ,     (  ( PreemptableResourceScheduler )     ( rmContext . getScheduler (  )  )  )  )  ;", "thisInterval    =    scheduleEditPolicy . getMonitoringInterval (  )  ;", "super . serviceInit ( conf )  ;", "}", "METHOD_END"], "methodName": ["serviceInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( RM _ SCHEDULER _ ENABLE _ MONITORS ,    true )  ;", "conf . set ( RM _ SCHEDULER _ MONITOR _ POLICIES ,    ProportionalCapacityPreemptionPolicy . class . getCanonicalName (  )  )  ;", "RManager   rm    =    new   RManager (  )  ;", "try    {", "rm . init ( conf )  ;", "}    catch    ( Exception   e )     {", "fail (  (  (  \" RManager   does   not   start   when    \"     +     ( YarnConfiguration . RM _ SCHEDULER _ ENABLE _ MONITORS )  )     +     \"    is   set   to   true \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRMStarts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.TestSchedulingMonitor"}, {"methodBody": ["METHOD_START", "{", "ProportionalCapacityPreemptionPolicy . TempQueue   ret ;", "synchronized ( root )     {", "String   queueName    =    root . getQueueName (  )  ;", "float   absUsed    =    root . getAbsoluteUsedCapacity (  )  ;", "float   absCap    =    root . getAbsoluteCapacity (  )  ;", "float   absMaxCap    =    root . getAbsoluteMaximumCapacity (  )  ;", "Resource   current    =    Resources . multiply ( clusterResources ,    absUsed )  ;", "Resource   guaranteed    =    Resources . multiply ( clusterResources ,    absCap )  ;", "Resource   maxCapacity    =    Resources . multiply ( clusterResources ,    absMaxCap )  ;", "if    ( root   instanceof   LeafQueue )     {", "LeafQueue   l    =     (  ( LeafQueue )     ( root )  )  ;", "Resource   pending    =    l . getTotalResourcePending (  )  ;", "ret    =    new   ProportionalCapacityPreemptionPolicy . TempQueue ( queueName ,    current ,    pending ,    guaranteed ,    maxCapacity )  ;", "ret . setLeafQueue ( l )  ;", "} else    {", "Resource   pending    =    Resource . newInstance (  0  ,     0  )  ;", "ret    =    new   ProportionalCapacityPreemptionPolicy . TempQueue ( root . getQueueName (  )  ,    current ,    pending ,    guaranteed ,    maxCapacity )  ;", "for    ( CSQueue   c    :    root . getChildQueues (  )  )     {", "ret . addChild ( cloneQueues ( c ,    clusterResources )  )  ;", "}", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["cloneQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "while    (  (  !  ( qAlloc . isEmpty (  )  )  )     &  &     ( Resources . greaterThan ( rc ,    tot _ guarant ,    unassigned ,    Resources . none (  )  )  )  )     {", "Resource   wQassigned    =    Resource . newInstance (  0  ,     0  )  ;", "resetCapacity ( rc ,    unassigned ,    qAlloc ,    ignoreGuarantee )  ;", "for    ( Iterator <  . TempQueue >    i    =    qAlloc . iterator (  )  ;    i . hasNext (  )  ;  )     {", ". TempQueue   sub    =    i . next (  )  ;", "Resource   wQavail    =    Resources . multiply ( unassigned ,    sub . normalizedGuarantee )  ;", "Resource   wQidle    =    sub . offer ( wQavail ,    rc ,    tot _ guarant )  ;", "Resource   wQdone    =    Resources . subtract ( wQavail ,    wQidle )  ;", "if    (  !  ( Resources . greaterThan ( rc ,    tot _ guarant ,    wQdone ,    Resources . none (  )  )  )  )     {", "i . remove (  )  ;", "}", "Resources . addTo ( wQassigned ,    wQdone )  ;", "}", "Resources . subtractFrom ( unassigned ,    wQassigned )  ;", "}", "}", "METHOD_END"], "methodName": ["computeFixpointAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "List < ProportionalCapacityPreemptionPolicy . TempQueue >    qAlloc    =    new   ArrayList < ProportionalCapacityPreemptionPolicy . TempQueue >  ( queues )  ;", "Resource   unassigned    =    Resources . clone ( tot _ guarant )  ;", "Set < ProportionalCapacityPreemptionPolicy . TempQueue >    nonZeroGuarQueues    =    new   HashSet < ProportionalCapacityPreemptionPolicy . TempQueue >  (  )  ;", "Set < ProportionalCapacityPreemptionPolicy . TempQueue >    zeroGuarQueues    =    new   HashSet < ProportionalCapacityPreemptionPolicy . TempQueue >  (  )  ;", "for    ( ProportionalCapacityPreemptionPolicy . TempQueue   q    :    qAlloc )     {", "if    ( Resources . greaterThan ( rc ,    tot _ guarant ,    q . guaranteed ,    Resources . none (  )  )  )     {", "nonZeroGuarQueues . add ( q )  ;", "} else    {", "zeroGuarQueues . add ( q )  ;", "}", "}", "computeFixpointAllocation ( rc ,    tot _ guarant ,    nonZeroGuarQueues ,    unassigned ,    false )  ;", "if    (  (  !  ( zeroGuarQueues . isEmpty (  )  )  )     &  &     ( Resources . greaterThan ( rc ,    tot _ guarant ,    unassigned ,    Resources . none (  )  )  )  )     {", "computeFixpointAllocation ( rc ,    tot _ guarant ,    zeroGuarQueues ,    unassigned ,    true )  ;", "}", "Resource   totPreemptionNeeded    =    Resource . newInstance (  0  ,     0  )  ;", "for    ( ProportionalCapacityPreemptionPolicy . TempQueue   t    :    queues )     {", "if    ( Resources . greaterThan ( rc ,    tot _ guarant ,    t . current ,    t . idealAssigned )  )     {", "Resources . addTo ( totPreemptionNeeded ,    Resources . subtract ( t . current ,    t . idealAssigned )  )  ;", "}", "}", "float   scalingFactor    =     1  .  0 F ;", "if    ( Resources . greaterThan ( rc ,    tot _ guarant ,    totPreemptionNeeded ,    totalPreemptionAllowed )  )     {", "scalingFactor    =    Resources . divide ( rc ,    tot _ guarant ,    totalPreemptionAllowed ,    totPreemptionNeeded )  ;", "}", "for    ( ProportionalCapacityPreemptionPolicy . TempQueue   t    :    queues )     {", "t . assignPreemption ( scalingFactor ,    rc ,    tot _ guarant )  ;", "}", "if    ( ProportionalCapacityPreemptionPolicy . LOG . isDebugEnabled (  )  )     {", "long   time    =    clock . getTime (  )  ;", "for    ( ProportionalCapacityPreemptionPolicy . TempQueue   t    :    queues )     {", "ProportionalCapacityPreemptionPolicy . LOG . debug (  (  ( time    +     \"  :     \"  )     +    t )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["computeIdealResourceDistribution"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "ProportionalCapacityPreemptionPolicy . TempQueue   tRoot ;", "synchronized ( scheduler )     {", "tRoot    =    cloneQueues ( root ,    clusterResources )  ;", "}", "tRoot . idealAssigned    =    tRoot . guaranteed ;", "Resource   totalPreemptionAllowed    =    Resources . multiply ( clusterResources ,    percentageClusterPreemptionAllowed )  ;", "List < ProportionalCapacityPreemptionPolicy . TempQueue >    queues    =    recursivelyComputeIdealAssignment ( tRoot ,    totalPreemptionAllowed )  ;", "Map < ApplicationAttemptId ,    Set < RMContainer >  >    toPreempt    =    getContainersToPreempt ( queues ,    clusterResources )  ;", "if    ( ProportionalCapacityPreemptionPolicy . LOG . isDebugEnabled (  )  )     {", "logToCSV ( queues )  ;", "}", "if    ( observeOnly )     {", "return ;", "}", "for    ( Map . Entry < ApplicationAttemptId ,    Set < RMContainer >  >    e    :    toPreempt . entrySet (  )  )     {", "for    ( RMContainer   container    :    e . getValue (  )  )     {", "if    (  (  ( preempted . get ( container )  )     !  =    null )     &  &     (  (  ( preempted . get ( container )  )     +     ( maxWaitTime )  )     <     ( clock . getTime (  )  )  )  )     {", "dispatcher . handle ( new   ContainerPreemptEvent ( e . getKey (  )  ,    container ,    ContainerPreemptEventType . KILL _ CONTAINER )  )  ;", "preempted . remove ( container )  ;", "} else    {", "dispatcher . handle ( new   ContainerPreemptEvent ( e . getKey (  )  ,    container ,    ContainerPreemptEventType . PREEMPT _ CONTAINER )  )  ;", "if    (  ( preempted . get ( container )  )     =  =    null )     {", "preempted . put ( container ,    clock . getTime (  )  )  ;", "}", "}", "}", "}", "for    ( Iterator < RMContainer >    i    =    preempted . keySet (  )  . iterator (  )  ;    i . hasNext (  )  ;  )     {", "RMContainer   id    =    i . next (  )  ;", "if    (  (  ( preempted . get ( id )  )     +     (  2     *     ( maxWaitTime )  )  )     <     ( clock . getTime (  )  )  )     {", "i . remove (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["containerBasedPreemptOrKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "Map < ApplicationAttemptId ,    Set < RMContainer >  >    preemptMap    =    new   HashMap < ApplicationAttemptId ,    Set < RMContainer >  >  (  )  ;", "List < RMContainer >    skippedAMContainerlist    =    new   ArrayList < RMContainer >  (  )  ;", "for    (  . TempQueue   qT    :    queues )     {", "if    ( Resources . greaterThan ( rc ,    clusterResource ,    qT . current ,    Resources . multiply ( qT . guaranteed ,     (  1  .  0     +     ( maxIgnoredOverCapacity )  )  )  )  )     {", "Resource   resToObtain    =    Resources . multiply ( qT . toBePreempted ,    naturalTerminationFactor )  ;", "Resource   skippedAMSize    =    Resource . newInstance (  0  ,     0  )  ;", "synchronized ( qT . leafQueue )     {", "NavigableSet < FiCaSchedulerApp >    ns    =     (  ( NavigableSet < FiCaSchedulerApp >  )     ( qT . leafQueue . getApplications (  )  )  )  ;", "Iterator < FiCaSchedulerApp >    desc    =    ns . descendingIterator (  )  ;", "qT . actuallyPreempted    =    Resources . clone ( resToObtain )  ;", "while    ( desc . hasNext (  )  )     {", "FiCaSchedulerApp   fc    =    desc . next (  )  ;", "if    ( Resources . lessThanOrEqual ( rc ,    clusterResource ,    resToObtain ,    Resources . none (  )  )  )     {", "break ;", "}", "preemptMap . put ( fc . getApplicationAttemptId (  )  ,    preemptFrom ( fc ,    clusterResource ,    resToObtain ,    skippedAMContainerlist ,    skippedAMSize )  )  ;", "}", "Resource   maxAMCapacityForThisQueue    =    Resources . multiply ( Resources . multiply ( clusterResource ,    qT . leafQueue . getAbsoluteCapacity (  )  )  ,    qT . leafQueue . getMaxAMResourcePerQueuePercent (  )  )  ;", "preemptAMContainers ( clusterResource ,    preemptMap ,    skippedAMContainerlist ,    resToObtain ,    skippedAMSize ,    maxAMCapacityForThisQueue )  ;", "}", "}", "}", "return   preemptMap ;", "}", "METHOD_END"], "methodName": ["getContainersToPreempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "return   rc ;", "}", "METHOD_END"], "methodName": ["getResourceCalculator"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "ProportionalCapacityPreemptionPolicy . LOG . info (  (  \" Preemption   monitor :  \"     +     ( this . getClass (  )  . getCanonicalName (  )  )  )  )  ;", "assert   null    =  =     ( scheduler )     :     \" Unexpected   duplicate   call   to   init \"  ;", "if    (  !  ( sched   instanceof   CapacityScheduler )  )     {", "throw   new   YarnRuntimeException (  (  (  (  \" Class    \"     +     ( sched . getClass (  )  . getCanonicalName (  )  )  )     +     \"    not   instance   of    \"  )     +     ( CapacityScheduler . class . getCanonicalName (  )  )  )  )  ;", "}", "dispatcher    =    disp ;", "scheduler    =     (  ( CapacityScheduler )     ( sched )  )  ;", "maxIgnoredOverCapacity    =    config . getDouble ( ProportionalCapacityPreemptionPolicy . MAX _ IGNORED _ OVER _ CAPACITY ,     0  .  1  )  ;", "naturalTerminationFactor    =    config . getDouble ( ProportionalCapacityPreemptionPolicy . NATURAL _ TERMINATION _ FACTOR ,     0  .  2  )  ;", "maxWaitTime    =    config . getLong ( ProportionalCapacityPreemptionPolicy . WAIT _ TIME _ BEFORE _ KILL ,     1  5  0  0  0  )  ;", "monitoringInterval    =    config . getLong ( ProportionalCapacityPreemptionPolicy . MONITORING _ INTERVAL ,     3  0  0  0  )  ;", "percentageClusterPreemptionAllowed    =    config . getFloat ( ProportionalCapacityPreemptionPolicy . TOTAL _ PREEMPTION _ PER _ ROUND ,     (  ( float )     (  0  .  1  )  )  )  ;", "observeOnly    =    config . getBoolean ( ProportionalCapacityPreemptionPolicy . OBSERVE _ ONLY ,    false )  ;", "rc    =    scheduler . getResourceCalculator (  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "List < ProportionalCapacityPreemptionPolicy . TempQueue >    queues    =    new   ArrayList < ProportionalCapacityPreemptionPolicy . TempQueue >  ( unorderedqueues )  ;", "Collections . sort ( queues ,    new   Comparator < ProportionalCapacityPreemptionPolicy . TempQueue >  (  )     {", "@ Override", "public   int   compare ( ProportionalCapacityPreemptionPolicy . TempQueue   o 1  ,    ProportionalCapacityPreemptionPolicy . TempQueue   o 2  )     {", "return   o 1  . queueName . compareTo ( o 2  . queueName )  ;", "}", "}  )  ;", "String   queueState    =     \"    QUEUESTATE :     \"     +     ( clock . getTime (  )  )  ;", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append ( queueState )  ;", "for    ( ProportionalCapacityPreemptionPolicy . TempQueue   tq    :    queues )     {", "sb . append (  \"  ,     \"  )  ;", "tq . appendLogString ( sb )  ;", "}", "ProportionalCapacityPreemptionPolicy . LOG . debug ( sb . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["logToCSV"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "for    ( RMContainer   c    :    skippedAMContainerlist )     {", "if    ( Resources . lessThanOrEqual ( rc ,    clusterResource ,    resToObtain ,    Resources . none (  )  )  )     {", "break ;", "}", "if    ( Resources . lessThanOrEqual ( rc ,    clusterResource ,    skippedAMSize ,    maxAMForThisQueue )  )     {", "break ;", "}", "Set < RMContainer >    contToPrempt    =    preemptMap . get ( c . getApplicationAttemptId (  )  )  ;", "if    ( null    =  =    contToPrempt )     {", "contToPrempt    =    new   HashSet < RMContainer >  (  )  ;", "preemptMap . put ( c . getApplicationAttemptId (  )  ,    contToPrempt )  ;", "}", "contToPrempt . add ( c )  ;", "Resources . subtractFrom ( resToObtain ,    c . getContainer (  )  . getResource (  )  )  ;", "Resources . subtractFrom ( skippedAMSize ,    c . getContainer (  )  . getResource (  )  )  ;", "}", "skippedAMContainerlist . clear (  )  ;", "}", "METHOD_END"], "methodName": ["preemptAMContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "Set < RMContainer >    ret    =    new   HashSet < RMContainer >  (  )  ;", "ApplicationAttemptId   appId    =    app . getApplicationAttemptId (  )  ;", "List < RMContainer >    reservations    =    new   ArrayList < RMContainer >  ( app . getReservedContainers (  )  )  ;", "for    ( RMContainer   c    :    reservations )     {", "if    ( Resources . lessThanOrEqual ( rc ,    clusterResource ,    rsrcPreempt ,    Resources . none (  )  )  )     {", "return   ret ;", "}", "if    (  !  ( observeOnly )  )     {", "dispatcher . handle ( new   ContainerPreemptEvent ( appId ,    c ,    ContainerPreemptEventType . DROP _ RESERVATION )  )  ;", "}", "Resources . subtractFrom ( rsrcPreempt ,    c . getContainer (  )  . getResource (  )  )  ;", "}", "List < RMContainer >    containers    =    new   ArrayList < RMContainer >  ( app . getLiveContainers (  )  )  ;", ". sortContainers ( containers )  ;", "for    ( RMContainer   c    :    containers )     {", "if    ( Resources . lessThanOrEqual ( rc ,    clusterResource ,    rsrcPreempt ,    Resources . none (  )  )  )     {", "return   ret ;", "}", "if    ( c . isAMContainer (  )  )     {", "skippedAMContainerlist . add ( c )  ;", "Resources . addTo ( skippedAMSize ,    c . getContainer (  )  . getResource (  )  )  ;", "continue ;", "}", "ret . add ( c )  ;", "Resources . subtractFrom ( rsrcPreempt ,    c . getContainer (  )  . getResource (  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["preemptFrom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "List < ProportionalCapacityPreemptionPolicy . TempQueue >    leafs    =    new   ArrayList < ProportionalCapacityPreemptionPolicy . TempQueue >  (  )  ;", "if    (  (  ( root . getChildren (  )  )     !  =    null )     &  &     (  ( root . getChildren (  )  . size (  )  )     >     0  )  )     {", "computeIdealResourceDistribution ( rc ,    root . getChildren (  )  ,    totalPreemptionAllowed ,    root . idealAssigned )  ;", "for    ( ProportionalCapacityPreemptionPolicy . TempQueue   t    :    root . getChildren (  )  )     {", "leafs . addAll ( recursivelyComputeIdealAssignment ( t ,    totalPreemptionAllowed )  )  ;", "}", "} else    {", "return   Collections . singletonList ( root )  ;", "}", "return   leafs ;", "}", "METHOD_END"], "methodName": ["recursivelyComputeIdealAssignment"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "Resource   activeCap    =    Resource . newInstance (  0  ,     0  )  ;", "if    ( ignoreGuar )     {", "for    (  . TempQueue   q    :    queues )     {", "q . normalizedGuarantee    =     (  ( float )     (  1  .  0 F )  )     /     (  ( float )     ( queues . size (  )  )  )  ;", "}", "} else    {", "for    (  . TempQueue   q    :    queues )     {", "Resources . addTo ( activeCap ,    q . guaranteed )  ;", "}", "for    (  . TempQueue   q    :    queues )     {", "q . normalizedGuarantee    =    Resources . divide ( rc ,    clusterResource ,    q . guaranteed ,    activeCap )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["resetCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "Collections . sort ( containers ,    new   Comparator < RMContainer >  (  )     {", "@ Override", "public   int   compare ( RMContainer   a ,    RMContainer   b )     {", "Comparator < Priority >    c    =    new   resource . Priority . Comparator (  )  ;", "int   priorityComp    =    c . compare ( b . getContainer (  )  . getPriority (  )  ,    a . getContainer (  )  . getPriority (  )  )  ;", "if    ( priorityComp    !  =     0  )     {", "return   priorityComp ;", "}", "return    ( b . getContainerId (  )  . getId (  )  )     -     ( a . getContainerId (  )  . getId (  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["sortContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]    abs    =    queueData [  0  ]  ;", "int [  ]    maxCap    =    queueData [  1  ]  ;", "int [  ]    used    =    queueData [  2  ]  ;", "int [  ]    pending    =    queueData [  3  ]  ;", "int [  ]    red    =    queueData [  4  ]  ;", "int [  ]    apps    =    queueData [  5  ]  ;", "int [  ]    gran    =    queueData [  6  ]  ;", "int [  ]    queues    =    queueData [  7  ]  ;", "return   mockNested ( abs ,    maxCap ,    used ,    pending ,    red ,    apps ,    gran ,    queues )  ;", "}", "METHOD_END"], "methodName": ["buildMockRootQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "ProportionalCapacityPreemptionPolicy   policy    =    new   ProportionalCapacityPreemptionPolicy ( conf ,    mDisp ,    mCS ,    mClock )  ;", "ParentQueue   mRoot    =    buildMockRootQueue ( rand ,    qData )  ;", "when ( mCS . getRootQueue (  )  )  . thenReturn ( mRoot )  ;", "Resource   clusterResources    =    Resource . newInstance (  . leafAbsCapacities ( qData [  0  ]  ,    qData [  7  ]  )  ,     0  )  ;", "when ( mCS . getClusterResource (  )  )  . thenReturn ( clusterResources )  ;", "return   policy ;", "}", "METHOD_END"], "methodName": ["buildPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int   ret    =     0  ;", "for    ( int   i    =     0  ;    i    <     ( abs . length )  ;     +  + i )     {", "if    (  0     =  =     ( subqueues [ i ]  )  )     {", "ret    +  =    abs [ i ]  ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["leafAbsCapacities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerApp   app    =    mock ( FiCaSchedulerApp . class )  ;", "ApplicationId   appId    =    ApplicationId . newInstance (  . TS ,    id )  ;", "ApplicationAttemptId   appAttId    =    ApplicationAttemptId . newInstance ( appId ,     0  )  ;", "when ( app . getApplicationId (  )  )  . thenReturn ( appId )  ;", "when ( app . getApplicationAttemptId (  )  )  . thenReturn ( appAttId )  ;", "int   cAlloc    =     0  ;", "Resource   unit    =    Resource . newInstance ( gran ,     0  )  ;", "List < RMContainer >    cReserved    =    new   ArrayList < RMContainer >  (  )  ;", "for    ( int   i    =     0  ;    i    <    reserved ;    i    +  =    gran )     {", "cReserved . add ( mockContainer ( appAttId ,    cAlloc ,    unit ,     1  )  )  ;", "+  + cAlloc ;", "}", "when ( app . getReservedContainers (  )  )  . thenReturn ( cReserved )  ;", "List < RMContainer >    cLive    =    new   ArrayList < RMContainer >  (  )  ;", "for    ( int   i    =     0  ;    i    <    used ;    i    +  =    gran )     {", "if    (  ( setAMContainer )     &  &     ( i    =  =     0  )  )     {", "cLive . add ( mockContainer ( appAttId ,    cAlloc ,    unit ,     0  )  )  ;", "} else    {", "cLive . add ( mockContainer ( appAttId ,    cAlloc ,    unit ,     1  )  )  ;", "}", "+  + cAlloc ;", "}", "when ( app . getLiveContainers (  )  )  . thenReturn ( cLive )  ;", "return   app ;", "}", "METHOD_END"], "methodName": ["mockApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "ContainerId   cId    =    ContainerId . newInstance ( appAttId ,    id )  ;", "Container   c    =    mock ( Container . class )  ;", "when ( c . getR (  )  )  . thenReturn ( r )  ;", "when ( c . getPriority (  )  )  . thenReturn ( Priority . create ( priority )  )  ;", "RMContainer   mC    =    mock ( RMContainer . class )  ;", "when ( mC . getContainerId (  )  )  . thenReturn ( cId )  ;", "when ( mC . getContainer (  )  )  . thenReturn ( c )  ;", "when ( mC . getApplicationAttemptId (  )  )  . thenReturn ( appAttId )  ;", "if    (  0     =  =    priority )     {", "when ( mC . isAMContainer (  )  )  . thenReturn ( true )  ;", "}", "return   mC ;", "}", "METHOD_END"], "methodName": ["mockContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   lq    =    mock ( LeafQueue . class )  ;", "when ( lq . getTotalRPending (  )  )  . thenReturn ( R . newInstance ( pending [ i ]  ,     0  )  )  ;", "NavigableSet < FiCaSchedulerApp >    qApps    =    new   TreeSet < FiCaSchedulerApp >  ( new   Comparator < FiCaSchedulerApp >  (  )     {", "@ Override", "public   int   compare ( FiCaSchedulerApp   a 1  ,    FiCaSchedulerApp   a 2  )     {", "return   a 1  . getApplicationAttemptId (  )  . compareTo ( a 2  . getApplicationAttemptId (  )  )  ;", "}", "}  )  ;", "if    (  ( apps [ i ]  )     !  =     0  )     {", "int   aUsed    =     ( used [ i ]  )     /     ( apps [ i ]  )  ;", "int   aPending    =     ( pending [ i ]  )     /     ( apps [ i ]  )  ;", "int   aReserve    =     ( reserved [ i ]  )     /     ( apps [ i ]  )  ;", "for    ( int   a    =     0  ;    a    <     ( apps [ i ]  )  ;     +  + a )     {", "qApps . add ( mockApp ( i ,    appAlloc ,    aUsed ,    aPending ,    aReserve ,    gran [ i ]  )  )  ;", "+  +  ( appAlloc )  ;", "}", "}", "when ( lq . getApplications (  )  )  . thenReturn ( qApps )  ;", "if    (  ( setAMRPercent )     !  =     0  .  0 F )     {", "when ( lq . getMaxAMRPerQueuePercent (  )  )  . thenReturn ( setAMRPercent )  ;", "}", "p . getChildQueues (  )  . add ( lq )  ;", "return   lq ;", "}", "METHOD_END"], "methodName": ["mockLeafQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "float   tot    =    TestProportionalCapacityPreemptionPolicy . leafAbsCapacities ( abs ,    queues )  ;", "Deque < ParentQueue >    pqs    =    new   LinkedList < ParentQueue >  (  )  ;", "ParentQueue   root    =    mockParentQueue ( null ,    queues [  0  ]  ,    pqs )  ;", "when ( root . getQueueName (  )  )  . thenReturn (  \"  /  \"  )  ;", "when ( root . getAbsoluteUsedCapacity (  )  )  . thenReturn (  (  ( used [  0  ]  )     /    tot )  )  ;", "when ( root . getAbsoluteCapacity (  )  )  . thenReturn (  (  ( abs [  0  ]  )     /    tot )  )  ;", "when ( root . getAbsoluteMaximumCapacity (  )  )  . thenReturn (  (  ( maxCap [  0  ]  )     /    tot )  )  ;", "for    ( int   i    =     1  ;    i    <     ( queues . length )  ;     +  + i )     {", "final   CSQueue   q ;", "final   ParentQueue   p    =    pqs . removeLast (  )  ;", "final   String   queueName    =     \" queue \"     +     (  ( char )     (  (  ' A '     +    i )     -     1  )  )  ;", "if    (  ( queues [ i ]  )     >     0  )     {", "q    =    mockParentQueue ( p ,    queues [ i ]  ,    pqs )  ;", "} else    {", "q    =    mockLeafQueue ( p ,    tot ,    i ,    abs ,    used ,    pending ,    reserved ,    apps ,    gran )  ;", "}", "when ( q . getParent (  )  )  . thenReturn ( p )  ;", "when ( q . getQueueName (  )  )  . thenReturn ( queueName )  ;", "when ( q . getAbsoluteUsedCapacity (  )  )  . thenReturn (  (  ( used [ i ]  )     /    tot )  )  ;", "when ( q . getAbsoluteCapacity (  )  )  . thenReturn (  (  ( abs [ i ]  )     /    tot )  )  ;", "when ( q . getAbsoluteMaximumCapacity (  )  )  . thenReturn (  (  ( maxCap [ i ]  )     /    tot )  )  ;", "}", "assert    0     =  =     ( pqs . size (  )  )  ;", "return   root ;", "}", "METHOD_END"], "methodName": ["mockNested"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "ParentQueue   pq    =    mock ( ParentQueue . class )  ;", "List < CSQueue >    cqs    =    new   ArrayList < CSQueue >  (  )  ;", "when ( pq . getChildQueues (  )  )  . thenReturn ( cqs )  ;", "for    ( int   i    =     0  ;    i    <    subqueues ;     +  + i )     {", "pqs . add ( pq )  ;", "}", "if    ( p    !  =    null )     {", "p . getChildQueues (  )  . add ( pq )  ;", "}", "return   pq ;", "}", "METHOD_END"], "methodName": ["mockParentQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "if    ( nq   instanceof   ParentQueue )     {", "System . out . println (  (  (  (  (  ( indent    +     ( nq . getQueueName (  )  )  )     +     \"    cur :  \"  )     +     ( nq . getAbsoluteUsed (  )  )  )     +     \"    guar :  \"  )     +     ( nq . getAbsolute (  )  )  )  )  ;", "for    ( CSQueue   q    :     (  ( ParentQueue )     ( nq )  )  . getChildQueues (  )  )     {", "printString ( q ,     ( indent    +     \"        \"  )  )  ;", "}", "} else    {", "System . out . println (  (  (  (  (  (  (  ( indent    +     ( nq . getQueueName (  )  )  )     +     \"    pen :  \"  )     +     (  (  ( LeafQueue )     ( nq )  )  . getTotalResourcePending (  )  )  )     +     \"    cur :  \"  )     +     ( nq . getAbsoluteUsed (  )  )  )     +     \"    guar :  \"  )     +     ( nq . getAbsolute (  )  )  )  )  ;", "for    ( FiCaSchedulerApp   a    :     (  ( LeafQueue )     ( nq )  )  . getApplications (  )  )     {", "System . out . println (  (  ( indent    +     \"        \"  )     +     ( a . getApplicationId (  )  )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["printString"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   Configuration ( false )  ;", "conf . setLong (  . WAIT _ TIME _ BEFORE _ KILL ,     1  0  0  0  0  )  ;", "conf . setLong (  . MONITORING _ INTERVAL ,     3  0  0  0  )  ;", "conf . setFloat (  . TOTAL _ PREEMPTION _ PER _ ROUND ,     (  ( float )     (  1  .  0  )  )  )  ;", "conf . setFloat (  . NATURAL _ TERMINATION _ FACTOR ,     (  ( float )     (  1  .  0  )  )  )  ;", "mClock    =    mock ( Clock . class )  ;", "mCS    =    mock ( CapacityScheduler . class )  ;", "when ( mCS . getResourceCalculator (  )  )  . thenReturn ( rc )  ;", "mDisp    =    mock ( EventHandler . class )  ;", "rand    =    new   Random (  )  ;", "long   seed    =    rand . nextLong (  )  ;", "System . out . println (  (  (  ( name . getMethodName (  )  )     +     \"    SEED :     \"  )     +    seed )  )  ;", "rand . setSeed ( seed )  ;", "appAlloc    =     0  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     1  0  ,     9  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     0     }  ,    new   int [  ]  {     7  0  ,     2  0  ,     9  0     }  ,    new   int [  ]  {     0  ,     0  ,     0     }  ,    new   int [  ]  {     5  ,     4  ,     1     }  ,    new   int [  ]  {     -  1  ,     5  ,     5     }  ,    new   int [  ]  {     2  ,     0  ,     0     }     }  ;", "setAMContainer    =    true ;", "setAMResourcePercent    =     0  .  5 F ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  5  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appD )  )  )  ;", "verify ( mDisp ,    times (  5  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "verify ( mDisp ,    times (  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appB )  )  )  ;", "verify ( mDisp ,    times (  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "setAMContainer    =    false ;", "}", "METHOD_END"], "methodName": ["testAMResourcePercentForSkippedAMContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "List < RMContainer >    containers    =    new   ArrayList < RMContainer >  (  )  ;", "ApplicationAttemptId   appAttId    =    ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  . TS ,     1  0  )  ,     0  )  ;", "RMContainer   rm 1     =    mockContainer ( appAttId ,     5  ,    mock ( Resource . class )  ,     3  )  ;", "RMContainer   rm 2     =    mockContainer ( appAttId ,     3  ,    mock ( Resource . class )  ,     3  )  ;", "RMContainer   rm 3     =    mockContainer ( appAttId ,     2  ,    mock ( Resource . class )  ,     2  )  ;", "RMContainer   rm 4     =    mockContainer ( appAttId ,     1  ,    mock ( Resource . class )  ,     2  )  ;", "RMContainer   rm 5     =    mockContainer ( appAttId ,     4  ,    mock ( Resource . class )  ,     1  )  ;", "containers . add ( rm 3  )  ;", "containers . add ( rm 2  )  ;", "containers . add ( rm 1  )  ;", "containers . add ( rm 5  )  ;", "containers . add ( rm 4  )  ;", "ProportionalCapacityPreemptionPolicy . sortContainers ( containers )  ;", "assert   containers . get (  0  )  . equals ( rm 1  )  ;", "assert   containers . get (  1  )  . equals ( rm 2  )  ;", "assert   containers . get (  2  )  . equals ( rm 3  )  ;", "assert   containers . get (  3  )  . equals ( rm 4  )  ;", "assert   containers . get (  4  )  . equals ( rm 5  )  ;", "}", "METHOD_END"], "methodName": ["testContainerOrdering"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     3  9  ,     4  3  ,     2  1     }  ,    new   int [  ]  {     1  0  ,     1  0  ,     0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     3  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "conf . setFloat (  . MAX _ IGNORED _ OVER _ CAPACITY ,     (  ( float )     (  0  .  1  )  )  )  ;", "policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    never (  )  )  . handle ( isA ( ContainerPreemptEvent . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDeadzone"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "final   long   killTime    =     1  0  0  0  0 L ;", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     0  ,     6  0  ,     4  0     }  ,    new   int [  ]  {     1  0  ,     1  0  ,     0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     3  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "conf . setLong ( ProportionalCapacityPreemptionPolicy . WAIT _ TIME _ BEFORE _ KILL ,    killTime )  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "when ( mClock . getTime (  )  )  . thenReturn (  0 L )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  1  0  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "when ( mClock . getTime (  )  )  . thenReturn (  ( killTime    /     2  )  )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  2  0  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "when ( mClock . getTime (  )  )  . thenReturn (  ( killTime    +     1  )  )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  3  0  )  )  . handle ( evtCaptor . capture (  )  )  ;", "List < ContainerPreemptEvent >    events    =    evtCaptor . getAllValues (  )  ;", "for    ( ContainerPreemptEvent   e    :    events . subList (  2  0  ,     3  0  )  )     {", "assertEquals ( appC ,    e . getAppId (  )  )  ;", "assertEquals ( ContainerPreemptEventType . KILL _ CONTAINER ,    e . getType (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testExpireKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     2  0  0  ,     1  0  0  ,     5  0  ,     5  0  ,     1  0  0  ,     1  0  ,     9  0     }  ,    new   int [  ]  {     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0     }  ,    new   int [  ]  {     2  0  0  ,     1  1  0  ,     6  0  ,     5  0  ,     9  0  ,     9  0  ,     0     }  ,    new   int [  ]  {     1  0  ,     0  ,     0  ,     0  ,     1  0  ,     0  ,     1  0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     4  ,     2  ,     1  ,     1  ,     2  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     -  1  ,     1  ,     1  ,     -  1  ,     1  ,     1     }  ,    new   int [  ]  {     2  ,     2  ,     0  ,     0  ,     2  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  1  0  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "}", "METHOD_END"], "methodName": ["testHierarchical"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     4  0  0  ,     2  0  0  ,     6  0  ,     1  4  0  ,     1  0  0  ,     7  0  ,     3  0  ,     1  0  0  ,     1  0  ,     9  0     }  ,    new   int [  ]  {     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0  ,     4  0  0     }  ,    new   int [  ]  {     4  0  0  ,     2  1  0  ,     7  0  ,     1  4  0  ,     1  0  0  ,     5  0  ,     5  0  ,     9  0  ,     9  0  ,     0     }  ,    new   int [  ]  {     1  0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     1  5     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     6  ,     2  ,     1  ,     1  ,     2  ,     1  ,     1  ,     2  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     -  1  ,     1  ,     1  ,     -  1  ,     1  ,     1  ,     -  1  ,     1  ,     1     }  ,    new   int [  ]  {     3  ,     2  ,     0  ,     0  ,     2  ,     0  ,     0  ,     2  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  9  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "verify ( mDisp ,    times (  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appE )  )  )  ;", "}", "METHOD_END"], "methodName": ["testHierarchicalLarge"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     0  ,     6  0  ,     4  0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     3  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    never (  )  )  . handle ( isA ( ContainerPreemptEvent . class )  )  ;", "}", "METHOD_END"], "methodName": ["testIgnore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     4  5  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     5  5  ,     4  5  ,     0     }  ,    new   int [  ]  {     2  0  ,     1  0  ,     1  0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     2  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    never (  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "}", "METHOD_END"], "methodName": ["testMaxCap"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     5  5  ,     4  5  ,     0     }  ,    new   int [  ]  {     2  0  ,     1  0  ,     1  0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     2  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "conf . setFloat (  . NATURAL _ TERMINATION _ FACTOR ,     (  ( float )     (  0  .  1  )  )  )  ;", "policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    never (  )  )  . handle ( isA ( ContainerPreemptEvent . class )  )  ;", "}", "METHOD_END"], "methodName": ["testNaturalTermination"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     9  0  ,     1  0  ,     0     }  ,    new   int [  ]  {     8  0  ,     1  0  ,     2  0  ,     5  0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     2  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "conf . setBoolean (  . OBSERVE _ ONLY ,    true )  ;", "policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    never (  )  )  . handle ( isA ( ContainerPreemptEvent . class )  )  ;", "}", "METHOD_END"], "methodName": ["testObserveOnly"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     5  5  ,     4  5  ,     0     }  ,    new   int [  ]  {     2  0  ,     1  0  ,     1  0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     2  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  5  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "}", "METHOD_END"], "methodName": ["testOverCapacityImbalance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( RM _ SCHEDULER _ MONITOR _ POLICIES ,     . class . getCanonicalName (  )  )  ;", "conf . setBoolean ( RM _ SCHEDULER _ ENABLE _ MONITORS ,    true )  ;", "@ SuppressWarnings (  \" resource \"  )", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . init ( conf )  ;", "for    ( Service   service    :    rm . getRMActiveService (  )  . getServices (  )  )     {", "if    ( service   instanceof   SchedulingMonitor )     {", "policy    =     (  (  )     (  (  ( SchedulingMonitor )     ( service )  )  . getSchedulingEditPolicy (  )  )  )  ;", "assertNotNull ( policy . getResourceCalculator (  )  )  ;", "return ;", "}", "}", "fail (  \" Failed   to   find   SchedulingMonitor   service ,    please   check   what   happened \"  )  ;", "}", "METHOD_END"], "methodName": ["testPolicyInitializeAfterSchedulerInitialized"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     4  0  ,     4  0  ,     2  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     0  ,     6  0  ,     4  0     }  ,    new   int [  ]  {     1  0  ,     1  0  ,     0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     3  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     3  ,     0  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  1  0  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "}", "METHOD_END"], "methodName": ["testPreemptCycle"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     1  0  ,     9  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     0     }  ,    new   int [  ]  {     7  0  ,     2  0  ,     9  0     }  ,    new   int [  ]  {     0  ,     0  ,     0     }  ,    new   int [  ]  {     5  ,     4  ,     1     }  ,    new   int [  ]  {     -  1  ,     5  ,     5     }  ,    new   int [  ]  {     2  ,     0  ,     0     }     }  ;", "setAMContainer    =    true ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  5  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appD )  )  )  ;", "verify ( mDisp ,    times (  5  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "verify ( mDisp ,    times (  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appB )  )  )  ;", "verify ( mDisp ,    times (  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "setAMContainer    =    false ;", "}", "METHOD_END"], "methodName": ["testPreemptSkippedAMContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     1  0  ,     4  0  ,     2  0  ,     3  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     3  0  ,     6  0  ,     1  0  ,     0     }  ,    new   int [  ]  {     4  5  ,     2  0  ,     5  ,     2  0  ,     0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     3  ,     1  ,     1  ,     1  ,     0     }  ,    new   int [  ]  {     -  1  ,     1  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     4  ,     0  ,     0  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  1  6  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "}", "METHOD_END"], "methodName": ["testProportionalPreemption"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     1  0  0  ,     5  0  ,     5  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     1  0  0  ,     1  0  0  ,     0     }  ,    new   int [  ]  {     7  0  ,     2  0  ,     5  0     }  ,    new   int [  ]  {     0  ,     0  ,     0     }  ,    new   int [  ]  {     5  ,     4  ,     1     }  ,    new   int [  ]  {     -  1  ,     1  ,     1     }  ,    new   int [  ]  {     2  ,     0  ,     0     }     }  ;", "setAMContainer    =    true ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  2  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appD )  )  )  ;", "verify ( mDisp ,    times (  2  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "verify ( mDisp ,    times (  2  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appB )  )  )  ;", "setAMContainer    =    false ;", "}", "METHOD_END"], "methodName": ["testSkipAMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     2  0  0  ,     1  0  0  ,     0  ,     9  9  ,     1  0  0  ,     1  0  ,     9  0     }  ,    new   int [  ]  {     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0     }  ,    new   int [  ]  {     1  7  0  ,     8  0  ,     6  0  ,     2  0  ,     9  0  ,     9  0  ,     0     }  ,    new   int [  ]  {     1  0  ,     0  ,     0  ,     0  ,     1  0  ,     0  ,     1  0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     4  ,     2  ,     1  ,     1  ,     2  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     -  1  ,     1  ,     1  ,     -  1  ,     1  ,     1     }  ,    new   int [  ]  {     2  ,     2  ,     0  ,     0  ,     2  ,     0  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    never (  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appA )  )  )  ;", "}", "METHOD_END"], "methodName": ["testZeroGuar"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "int [  ]  [  ]    qData    =    new   int [  ]  [  ]  {    new   int [  ]  {     2  0  0  ,     1  0  0  ,     0  ,     9  9  ,     0  ,     1  0  0  ,     1  0  0     }  ,    new   int [  ]  {     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0  ,     2  0  0     }  ,    new   int [  ]  {     1  7  0  ,     1  7  0  ,     6  0  ,     2  0  ,     9  0  ,     0  ,     0     }  ,    new   int [  ]  {     8  5  ,     5  0  ,     3  0  ,     1  0  ,     1  0  ,     2  0  ,     2  0     }  ,    new   int [  ]  {     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0     }  ,    new   int [  ]  {     4  ,     3  ,     1  ,     1  ,     1  ,     1  ,     1     }  ,    new   int [  ]  {     -  1  ,     -  1  ,     1  ,     1  ,     1  ,     -  1  ,     1     }  ,    new   int [  ]  {     2  ,     3  ,     0  ,     0  ,     0  ,     1  ,     0     }     }  ;", "ProportionalCapacityPreemptionPolicy   policy    =    buildPolicy ( qData )  ;", "policy . editSchedule (  )  ;", "verify ( mDisp ,    times (  1  4  )  )  . handle ( argThat ( new    . IsPreemptionRequestFor ( appC )  )  )  ;", "}", "METHOD_END"], "methodName": ["testZeroGuarOverCap"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.TestProportionalCapacityPreemptionPolicy"}, {"methodBody": ["METHOD_START", "{", "if    ( record . getName (  )  . endsWith (  \"  . tmp \"  )  )     {", ". LOG . error (  (  \" incomplete   rm   state   store   entry   found    :  \"     +    record )  )  ;", "fs . delete ( record ,    false )  ;", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["checkAndRemovePartialRecord"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "FileStatus [  ]    newChildNodes    =    fs . listStatus ( path ,    new   PathFilter (  )     {", "@ Override", "public   boolean   accept ( Path   path )     {", "return   path . getName (  )  . endsWith (  \"  . new \"  )  ;", "}", "}  )  ;", "for    ( FileStatus   newChildNodeStatus    :    newChildNodes )     {", "assert   newChildNodeStatus . isFile (  )  ;", "String   newChildNodeName    =    newChildNodeStatus . getPath (  )  . getName (  )  ;", "String   childNodeName    =    newChildNodeName . substring (  0  ,     (  ( newChildNodeName . length (  )  )     -     (  \"  . new \"  . length (  )  )  )  )  ;", "Path   childNodePath    =    new   Path ( newChildNodeStatus . getPath (  )  . getParent (  )  ,    childNodeName )  ;", "replaceFile ( newChildNodeStatus . getPath (  )  ,    childNodePath )  ;", "}", "}", "METHOD_END"], "methodName": ["checkAndResumeUpdateOperation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   fs . createNewFile ( newFile )  ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( fs . delete ( deletePath ,    true )  )  )     {", "throw   new   Exception (  (  \" Fad   to   delete    \"     +    deletePath )  )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   getNodePath ( root ,    appId )  ;", "}", "METHOD_END"], "methodName": ["getAppDir"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( root ,    nodeName )  ;", "}", "METHOD_END"], "methodName": ["getNodePath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "checkAndResumeUpdateOperation ( amrmTokenSecretManagerRoot )  ;", "Path   amrmTokenSecretManagerStateDataDir    =    new   Path ( amrmTokenSecretManagerRoot ,     . AMRMTOKEN _ SECRET _ MANAGER _ NODE )  ;", "FileStatus   status ;", "try    {", "status    =    fs . getFileStatus ( amrmTokenSecretManagerStateDataDir )  ;", "assert   status . isFile (  )  ;", "}    catch    ( FileNotFoundException   ex )     {", "return ;", "}", "byte [  ]    data    =    readFile ( amrmTokenSecretManagerStateDataDir ,    status . getLen (  )  )  ;", "AMRMTokenSecretManagerStatePBImpl   stateData    =    new   AMRMTokenSecretManagerStatePBImpl ( AMRMTokenSecretManagerStateProto . parseFrom ( data )  )  ;", "rmState . amrmTokenSecretManagerState    =    AMRMTokenSecretManagerState . newInstance ( stateData . getCurrentMasterKey (  )  ,    stateData . getNextMasterKey (  )  )  ;", "}", "METHOD_END"], "methodName": ["loadAMRMTokenSecretManagerState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "List < RMStateStore . ApplicationAttemptState >    attempts    =    new   ArrayList < RMStateStore . ApplicationAttemptState >  (  )  ;", "for    ( FileStatus   appDir    :    fs . listStatus ( rmAppRoot )  )     {", "checkAndResumeUpdateOperation ( appDir . getPath (  )  )  ;", "for    ( FileStatus   childNodeStatus    :    fs . listStatus ( appDir . getPath (  )  )  )     {", "assert   childNodeStatus . isFile (  )  ;", "String   childNodeName    =    childNodeStatus . getPath (  )  . getName (  )  ;", "if    ( checkAndRemovePartialRecord ( childNodeStatus . getPath (  )  )  )     {", "continue ;", "}", "byte [  ]    childData    =    readFile ( childNodeStatus . getPath (  )  ,    childNodeStatus . getLen (  )  )  ;", "if    ( childNodeName . startsWith ( appIdStrPrefix )  )     {", "if    ( FileSystemRMStateStore . LOG . isDebugEnabled (  )  )     {", "FileSystemRMStateStore . LOG . debug (  (  \" Loading   application   from   node :     \"     +    childNodeName )  )  ;", "}", "ApplicationId   appId    =    ConverterUtils . toApplicationId ( childNodeName )  ;", "ApplicationStateDataPBImpl   appStateData    =    new   ApplicationStateDataPBImpl ( ApplicationStateDataProto . parseFrom ( childData )  )  ;", "RMStateStore . ApplicationState   appState    =    new   RMStateStore . ApplicationState ( appStateData . getSubmitTime (  )  ,    appStateData . getStartTime (  )  ,    appStateData . getApplicationSubmissionContext (  )  ,    appStateData . getUser (  )  ,    appStateData . getState (  )  ,    appStateData . getDiagnostics (  )  ,    appStateData . getFinishTime (  )  )  ;", "assert   appId . equals ( appState . context . getApplicationId (  )  )  ;", "rmState . appState . put ( appId ,    appState )  ;", "} else", "if    ( childNodeName . startsWith ( ApplicationAttemptId . appAttemptIdStrPrefix )  )     {", "if    ( FileSystemRMStateStore . LOG . isDebugEnabled (  )  )     {", "FileSystemRMStateStore . LOG . debug (  (  \" Loading   application   attempt   from   node :     \"     +    childNodeName )  )  ;", "}", "ApplicationAttemptId   attemptId    =    ConverterUtils . toApplicationAttemptId ( childNodeName )  ;", "records . impl . pb . ApplicationAttemptStateDataPBImpl   attemptStateData    =    new   records . impl . pb . ApplicationAttemptStateDataPBImpl ( parseFrom ( childData )  )  ;", "Credentials   credentials    =    null ;", "if    (  ( attemptStateData . getAppAttemptTokens (  )  )     !  =    null )     {", "credentials    =    new   Credentials (  )  ;", "DataInputByteBuffer   dibb    =    new   DataInputByteBuffer (  )  ;", "dibb . reset ( attemptStateData . getAppAttemptTokens (  )  )  ;", "credentials . readTokenStorageStream ( dibb )  ;", "}", "RMStateStore . ApplicationAttemptState   attemptState    =    new   RMStateStore . ApplicationAttemptState ( attemptId ,    attemptStateData . getMasterContainer (  )  ,    credentials ,    attemptStateData . getStartTime (  )  ,    attemptStateData . getState (  )  ,    attemptStateData . getFinalTrackingUrl (  )  ,    attemptStateData . getDiagnostics (  )  ,    attemptStateData . getFinalApplicationStatus (  )  ,    attemptStateData . getAMContainerExitStatus (  )  )  ;", "assert   attemptId . equals ( attemptState . getAttemptId (  )  )  ;", "attempts . add ( attemptState )  ;", "} else    {", "FileSystemRMStateStore . LOG . info (  (  \" Unknown   child   node   with   name :     \"     +    childNodeName )  )  ;", "}", "}", "}", "for    ( RMStateStore . ApplicationAttemptState   attemptState    :    attempts )     {", "ApplicationId   appId    =    attemptState . getAttemptId (  )  . getApplicationId (  )  ;", "RMStateStore . ApplicationState   appState    =    rmState . appState . get ( appId )  ;", "assert   appState    !  =    null ;", "appState . attempts . put ( attemptState . getAttemptId (  )  ,    attemptState )  ;", "}", "FileSystemRMStateStore . LOG . info (  \" Done   Loading   applications   from   FS   state   store \"  )  ;", "}    catch    ( Exception   e )     {", "FileSystemRMStateStore . LOG . error (  \" Failed   to   load   state .  \"  ,    e )  ;", "throw   e ;", "}", "}", "METHOD_END"], "methodName": ["loadRMAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "checkAndResumeUpdateOperation ( rmDTSecretManagerRoot )  ;", "FileStatus [  ]    childNodes    =    fs . listStatus ( rmDTSecretManagerRoot )  ;", "for    ( FileStatus   childNodeStatus    :    childNodes )     {", "assert   childNodeStatus . isFile (  )  ;", "String   childNodeName    =    childNodeStatus . getPath (  )  . getName (  )  ;", "if    ( checkAndRemovePartialRecord ( childNodeStatus . getPath (  )  )  )     {", "continue ;", "}", "if    ( childNodeName . startsWith ( RMStateStore . DELEGATION _ TOKEN _ SEQUENCE _ NUMBER _ PREFIX )  )     {", "rmState . rmSecretManagerState . dtSequenceNumber    =    Integer . parseInt ( childNodeName . split (  \"  _  \"  )  [  1  ]  )  ;", "continue ;", "}", "Path   childNodePath    =    getNodePath ( rmDTSecretManagerRoot ,    childNodeName )  ;", "byte [  ]    childData    =    readFile ( childNodePath ,    childNodeStatus . getLen (  )  )  ;", "ByteArrayInputStream   is    =    new   ByteArrayInputStream ( childData )  ;", "DataInputStream   fsIn    =    new   DataInputStream ( is )  ;", "if    ( childNodeName . startsWith ( RMStateStore . DELEGATION _ KEY _ PREFIX )  )     {", "DelegationKey   key    =    new   DelegationKey (  )  ;", "key . readFields ( fsIn )  ;", "rmState . rmSecretManagerState . masterKeyState . add ( key )  ;", "} else", "if    ( childNodeName . startsWith ( RMStateStore . DELEGATION _ TOKEN _ PREFIX )  )     {", "curity . client . RMDelegationTokenIdentifier   identifier    =    new   curity . client . RMDelegationTokenIdentifier (  )  ;", "identifier . readFields ( fsIn )  ;", "long   renewDate    =    fsIn . readLong (  )  ;", "rmState . rmSecretManagerState . delegationTokenState . put ( identifier ,    renewDate )  ;", "} else    {", "FileSystemRMStateStore . LOG . warn (  \" Unknown   file   for   recovering   RMDelegationTokenSecretManager \"  )  ;", "}", "fsIn . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["loadRMDTSecretManagerState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "FSDataInputSam   fsIn    =    fs . open ( inputPath )  ;", "byte [  ]    data    =    new   byte [  (  ( int )     ( len )  )  ]  ;", "fsInadFully ( data )  ;", "fsIn . close (  )  ;", "turn   data ;", "}", "METHOD_END"], "methodName": ["readFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   fs . rename ( src ,    dst )  ;", "}", "METHOD_END"], "methodName": ["renameFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "if    ( fs . exists ( dstPath )  )     {", "deleteFile ( dstPath )  ;", "} else    {", ". LOG . info (  (  \" File   doesn ' t   exist .    Skip   deleting   the   file    \"     +    dstPath )  )  ;", "}", "fs . rename ( srcPath ,    dstPath )  ;", "}", "METHOD_END"], "methodName": ["replaceFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Path   nodeCreatePath    =    getNodePath ( rmDTSecretManagerRoot ,     (  ( RMStateStore . DELEGATION _ TOKEN _ PREFIX )     +     ( identifier . getSequenceNumber (  )  )  )  )  ;", "ByteArrayOutputStream   os    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   fsOut    =    new   DataOutputStream ( os )  ;", "identifier . write ( fsOut )  ;", "fsOut . writeLong ( renewDate )  ;", "if    ( isUpdate )     {", ". LOG . info (  (  \" Updating   RMDelegationToken _  \"     +     ( identifier . getSequenceNumber (  )  )  )  )  ;", "updateFile ( nodeCreatePath ,    os . toByteArray (  )  )  ;", "} else    {", ". LOG . info (  (  \" Storing   RMDelegationToken _  \"     +     ( identifier . getSequenceNumber (  )  )  )  )  ;", "writeFile ( nodeCreatePath ,    os . toByteArray (  )  )  ;", "}", "fsOut . close (  )  ;", "Path   latestSequenceNumberPath    =    getNodePath ( rmDTSecretManagerRoot ,     (  ( RMStateStore . DELEGATION _ TOKEN _ SEQUENCE _ NUMBER _ PREFIX )     +    latestSequenceNumber )  )  ;", ". LOG . info (  (  (  \" Storing    \"     +     ( RMStateStore . DELEGATION _ TOKEN _ SEQUENCE _ NUMBER _ PREFIX )  )     +    latestSequenceNumber )  )  ;", "if    (  ( dtSequenceNumberPath )     =  =    null )     {", "if    (  !  ( createFile ( latestSequenceNumberPath )  )  )     {", "throw   new   Exception (  (  \" Failed   to   create    \"     +    latestSequenceNumberPath )  )  ;", "}", "} else    {", "if    (  !  ( renameFile ( dtSequenceNumberPath ,    latestSequenceNumberPath )  )  )     {", "throw   new   Exception (  (  \" Failed   to   rename    \"     +     ( dtSequenceNumberPath )  )  )  ;", "}", "}", "dtSequenceNumberPath    =    latestSequenceNumberPath ;", "}", "METHOD_END"], "methodName": ["storeOrUpdateRMDelegationTokenAndSequenceNumberState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Path   newPath    =    new   Path ( outputPath . getParent (  )  ,     (  ( outputPath . getName (  )  )     +     \"  . new \"  )  )  ;", "write ( newPath ,    data )  ;", "replace ( newPath ,    outputPath )  ;", "}", "METHOD_END"], "methodName": ["updateFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Path   tempPath    =    new   Path ( outputPath . getParent (  )  ,     (  ( outputPath . getName (  )  )     +     \"  . tmp \"  )  )  ;", "FSDataOutputStream   fsOut    =    null ;", "fsOut    =    fs . create ( tempPath ,    true )  ;", "fsOut . write ( data )  ;", "fsOut . close (  )  ;", "fs . rename ( tempPath ,    outputPath )  ;", "}", "METHOD_END"], "methodName": ["writeFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   state ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Version   loadedVersion    =    loadVersion (  )  ;", ". LOG . info (  (  \" Loaded   RM   state   version   info    \"     +    loadedVersion )  )  ;", "if    (  ( loadedVersion    !  =    null )     &  &     ( loadedVersion . equals ( getCurrentVersion (  )  )  )  )     {", "return ;", "}", "if    ( loadedVersion    =  =    null )     {", "loadedVersion    =    Version . newInstance (  1  ,     0  )  ;", "}", "if    ( loadedVersion . isCompatibleTo ( getCurrentVersion (  )  )  )     {", ". LOG . info (  (  \" Storing   RM   state   version   info    \"     +     ( getCurrentVersion (  )  )  )  )  ;", "storeVersion (  )  ;", "} else    {", "throw   new   RMStateVersionIncompatibleException (  (  (  (  \" Expecting   RM   state   version    \"     +     ( getCurrentVersion (  )  )  )     +     \"  ,    but   loading   version    \"  )     +    loadedVersion )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "Credentials   credentials    =    new   Credentials (  )  ;", "SecretKey   clientTokenMasterKey    =    appAttempt . getClientTokenMasterKey (  )  ;", "if    ( clientTokenMasterKey    !  =    null )     {", "credentials . addSecretKey (  . AM _ CLIENT _ TOKEN _ MASTER _ KEY _ NAME ,    clientTokenMasterKey . getEncoded (  )  )  ;", "}", "return   credentials ;", "}", "METHOD_END"], "methodName": ["getCredentialsFromAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "this . stateMachine . doTransition ( event . getType (  )  ,    event )  ;", "}    catch    ( InvalidStateTransitonException   e )     {", ". LOG . error (  \" Can ' t   handle   this   event   at   current   state \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["handleStoreEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "rmDispatcher . getEventHandler (  )  . handle ( event )  ;", "}", "METHOD_END"], "methodName": ["notifyApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "rmDispatcher . getEventHandler (  )  . handle ( event )  ;", "}", "METHOD_END"], "methodName": ["notifyApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "RMFatalEventType   type ;", "if    ( failureCause   instanceof   FencedException )     {", "type    =    RMFatalEventType . STATE _ STORE _ FENCED ;", "} else    {", "type    =    RMFatalEventType . STATE _ STORE _ OP _ FAILED ;", "}", "rmDispatcher . getEventHandler (  )  . handle ( new   RMFatalEvent ( type ,    failureCause )  )  ;", "}", "METHOD_END"], "methodName": ["notifyStoreOperationFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "RMStateStore . ApplicationState   appState    =    new   RMStateStore . ApplicationState ( app . getSubmitTime (  )  ,    app . getStartTime (  )  ,    app . getApplicationSubmissionContext (  )  ,    app . getUser (  )  )  ;", "for    ( RMAppAttempt   appAttempt    :    app . getAppAttempts (  )  . values (  )  )     {", "Credentials   credentials    =    getCredentialsFromAppAttempt ( appAttempt )  ;", "RMStateStore . ApplicationAttemptState   attemptState    =    new   RMStateStore . ApplicationAttemptState ( appAttempt . getAppAttemptId (  )  ,    appAttempt . getMasterContainer (  )  ,    credentials ,    appAttempt . getStartTime (  )  )  ;", "appState . attempts . put ( attemptState . getAttemptId (  )  ,    attemptState )  ;", "}", "dispatcher . getEventHandler (  )  . handle ( new   RMStateStoreRemoveAppEvent ( appState )  )  ;", "}", "METHOD_END"], "methodName": ["removeApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "removeRMDTMasterKey ( delegationKey )  ;", "}    catch    ( Exception   e )     {", "notifyStoreOperationFailed ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["removeRMDTMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "removeRMDelegationToken ( rmDTIdentifier )  ;", "}    catch    ( Exception   e )     {", "notifyStoreOperationFailed ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["removeRMDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "this . rmDispatcher    =    dispatcher ;", "}", "METHOD_END"], "methodName": ["setRMDispatcher"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   context    =    app . getApplicationSubmissionContext (  )  ;", "assert   context   instanceof   ApplicationSubmissionContextPBImpl ;", ". ApplicationState   appState    =    new    . ApplicationState ( app . getSubmitTime (  )  ,    app . getStartTime (  )  ,    context ,    app . getUser (  )  )  ;", "dispatcher . getEventHandler (  )  . handle ( new   AppEvent ( appState )  )  ;", "}", "METHOD_END"], "methodName": ["storeNewApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "Credentials   credentials    =    getCredentialsFromAppAttempt ( appAttempt )  ;", ". ApplicationAttemptState   attemptState    =    new    . ApplicationAttemptState ( appAttempt . getAppAttemptId (  )  ,    appAttempt . getMasterContainer (  )  ,    credentials ,    appAttempt . getStartTime (  )  )  ;", "dispatcher . getEventHandler (  )  . handle ( new   AppAttemptEvent ( attemptState )  )  ;", "}", "METHOD_END"], "methodName": ["storeNewApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "storeRMDTMasterKey ( delegationKey )  ;", "}    catch    ( Exception   e )     {", "notifyStoreOperationFailed ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["storeRMDTMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "storeRMDelegationTokenAndSequenceNumber ( rmDTIdentifier ,    renewDate ,    latestSequenceNumber )  ;", "}    catch    ( Exception   e )     {", "notifyStoreOperationFailed ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["storeRMDelegationTokenAndSequenceNumber"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "dispatcher . getEventHandler (  )  . handle ( new   RMStateUpdateAppAttemptEvent ( attemptState )  )  ;", "}", "METHOD_END"], "methodName": ["updateApplicationAttemptState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "dispatcher . getEventHandler (  )  . handle ( new   RMStateUpdateAppEvent ( appState )  )  ;", "}", "METHOD_END"], "methodName": ["updateApplicationState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "try    {", "updateRMDelegationTokenAndSequenceNumberInternal ( rmDTIdentifier ,    renewDate ,    latestSequenceNumber )  ;", "}    catch    ( Exception   e )     {", "notifyOperationFailed ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["updateRMDelegationTokenAndSequenceNumber"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   attemptState ;", "}", "METHOD_END"], "methodName": ["getAppAttemptState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreAppAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "return   appState ;", "}", "METHOD_END"], "methodName": ["getAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreAppEvent"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    ReflectionUtils . newInstance ( conf . getClass ( RM _ STORE ,    MemoryRMStateStore . class ,    RMStateStore . class )  ,    conf )  ;", "return   store ;", "}", "METHOD_END"], "methodName": ["getStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreFactory"}, {"methodBody": ["METHOD_START", "{", "return   appState ;", "}", "METHOD_END"], "methodName": ["getAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreRemoveAppEvent"}, {"methodBody": ["METHOD_START", "{", "ArrayList < RMApp >    appList    =    new   ArrayList < RMApp >  (  )  ;", "for    ( int   i    =     0  ;    i    <    numApps ;    i +  +  )     {", "ApplicationId   appId    =    ApplicationId . newInstance (  1  3  8  3  1  8  3  3  3  8  ,    i )  ;", "RMApp   app    =    storeApp ( store ,    appId ,     1  2  3  4  5  6  7  8  9  ,     9  8  7  6  5  4  3  2  1  )  ;", "appList . add ( app )  ;", "}", "Assert . assertEquals ( numApps ,    appList . size (  )  )  ;", "for    ( RMApp   app    :    appList )     {", "while    ( true )     {", "if    ( sHelper . appExists ( app )  )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "}", "return   appList ;", "}", "METHOD_END"], "methodName": ["createAndStoreApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "Token < AMRMTokenIdentifier >    appToken    =    appTokenMgr . createAndGetAMRMToken ( attemptId )  ;", "appToketService ( new   Text (  \" appTokervice \"  )  )  ;", "return   appToken ;", "}", "METHOD_END"], "methodName": ["generateAMRMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   context    =    new   ApplicationSubmissionContextPBImpl (  )  ;", "context . setApplicationId ( appId )  ;", "RMApp   mockApp    =    mock ( RMApp . class )  ;", "when ( mockApp . getApplicationId (  )  )  . thenReturn ( appId )  ;", "when ( mockApp . getSubmitTime (  )  )  . thenReturn ( submitTime )  ;", "when ( mockApp . getStartTime (  )  )  . thenReturn ( startTime )  ;", "when ( mockApp . getApplicationSubmissionContext (  )  )  . thenReturn ( context )  ;", "when ( mockApp . getUser (  )  )  . thenReturn (  \" test \"  )  ;", "s . sNewApplication ( mockApp )  ;", "return   mockApp ;", "}", "METHOD_END"], "methodName": ["storeApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    new   ContainerPBImpl (  )  ;", "container . setId ( ConverterUtils . toContainerId ( containerIdStr )  )  ;", "RMAppAttempt   mockAttempt    =    mock ( RMAppAttempt . class )  ;", "when ( mockAttempt . getAppAttemptId (  )  )  . thenReturn ( attemptId )  ;", "when ( mockAttempt . getMasterContainer (  )  )  . thenReturn ( container )  ;", "when ( mockAttempt . getAMRMToken (  )  )  . thenReturn ( appToken )  ;", "when ( mockAttempt . getClientTokenMasterKey (  )  )  . thenReturn ( clientTokenMasterKey )  ;", "dispatcher . attemptId    =    attemptId ;", "s . sNewApplicationAttempt ( mockAttempt )  ;", "waitNotify ( dispatcher )  ;", "return   container . getId (  )  ;", "}", "METHOD_END"], "methodName": ["storeAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "System . out . println (  \" Start   testing \"  )  ;", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", ". TestDispatcher   dispatcher    =    new    . TestDispatcher (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( store )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "AMRMTokenSecretManager   appTokenMgr    =    new   AMRMTokenSecretManager ( conf ,    rmContext )  ;", "MasterKeyData   firstMasterKeyData    =    appTokenMgr . createNewMasterKey (  )  ;", "AMRMTokenSecretManagerState   state 1     =    AMRMTokenSecretManagerState . newInstance ( firstMasterKeyData . getMasterKey (  )  ,    null )  ;", "rmContext . getStateStore (  )  . storeOrUpdateAMRMTokenSecretManagerState ( state 1  ,    false )  ;", "store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMStateStore . RMState   state    =    store . loadState (  )  ;", "Assert . assertNotNull ( state . getAMRMTokenSecretManagerState (  )  )  ;", "Assert . assertEquals ( firstMasterKeyData . getMasterKey (  )  ,    state . getAMRMTokenSecretManagerState (  )  . getCurrentMasterKey (  )  )  ;", "Assert . assertNull ( state . getAMRMTokenSecretManagerState (  )  . getNextMasterKey (  )  )  ;", "MasterKeyData   secondMasterKeyData    =    appTokenMgr . createNewMasterKey (  )  ;", "AMRMTokenSecretManagerState   state 2     =    AMRMTokenSecretManagerState . newInstance ( firstMasterKeyData . getMasterKey (  )  ,    secondMasterKeyData . getMasterKey (  )  )  ;", "rmContext . getStateStore (  )  . storeOrUpdateAMRMTokenSecretManagerState ( state 2  ,    true )  ;", "store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMStateStore . RMState   state _  2     =    store . loadState (  )  ;", "Assert . assertNotNull ( state _  2  . getAMRMTokenSecretManagerState (  )  )  ;", "Assert . assertEquals ( firstMasterKeyData . getMasterKey (  )  ,    state _  2  . getAMRMTokenSecretManagerState (  )  . getCurrentMasterKey (  )  )  ;", "Assert . assertEquals ( secondMasterKeyData . getMasterKey (  )  ,    state _  2  . getAMRMTokenSecretManagerState (  )  . getNextMasterKey (  )  )  ;", "appTokenMgr . recover ( state _  2  )  ;", "Assert . assertEquals ( appTokenMgr . getCurrnetMasterKeyData (  )  . getSecretKey (  )  ,    firstMasterKeyData . getSecretKey (  )  )  ;", "Assert . assertEquals ( appTokenMgr . getNextMasterKeyData (  )  . getSecretKey (  )  ,    secondMasterKeyData . getSecretKey (  )  )  ;", "store . close (  )  ;", "}", "METHOD_END"], "methodName": ["testAMRMTokenSecretManagerStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( new    . TestDispatcher (  )  )  ;", "ArrayList < RMApp >    appList    =    createAndStoreApps ( stateStoreHelper ,    store ,     5  )  ;", "for    ( RMApp   app    :    appList )     {", "store . removeApplication ( app )  ;", "while    ( true )     {", "if    (  !  ( stateStoreHelper . appExists ( app )  )  )     {", "break ;", "} else    {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["testAppDeletion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( new    . TestDispatcher (  )  )  ;", "Version   defaultVersion    =    stateStoreHelper . getCurrentVersion (  )  ;", "store . checkVersion (  )  ;", "Assert . assertEquals ( defaultVersion ,    store . loadVersion (  )  )  ;", "Version   compatibleVersion    =    Version . newInstance ( defaultVersion . getMajorVersion (  )  ,     (  ( defaultVersion . getMinorVersion (  )  )     +     2  )  )  ;", "stateStoreHelper . writeVersion ( compatibleVersion )  ;", "Assert . assertEquals ( compatibleVersion ,    store . loadVersion (  )  )  ;", "store . checkVersion (  )  ;", "Assert . assertEquals ( defaultVersion ,    store . loadVersion (  )  )  ;", "Version   incompatibleVersion    =    Version . newInstance (  (  ( defaultVersion . getMajorVersion (  )  )     +     2  )  ,    defaultVersion . getMinorVersion (  )  )  ;", "stateStoreHelper . writeVersion ( incompatibleVersion )  ;", "try    {", "store . checkVersion (  )  ;", "Assert . fail (  \" Invalid   version ,    should   fail .  \"  )  ;", "}    catch    ( Throwable   t )     {", "Assert . assertTrue (  ( t   instanceof   RMStateVersionIncompatibleException )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCheckVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", "ArrayList < RMApp >    appList    =    createAndStoreApps ( stateStoreHelper ,    store ,     5  )  ;", "store . deleteStore (  )  ;", "for    ( RMApp   app    :    appList )     {", "Assert . assertFalse ( stateStoreHelper . appExists ( app )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDeleteStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( new    . TestDispatcher (  )  )  ;", "int   firstTimeEpoch    =    store . getAndIncrementEpoch (  )  ;", "Assert . assertEquals (  0  ,    firstTimeEpoch )  ;", "int   secondTimeEpoch    =    store . getAndIncrementEpoch (  )  ;", "Assert . assertEquals (  1  ,    secondTimeEpoch )  ;", "int   thirdTimeEpoch    =    store . getAndIncrementEpoch (  )  ;", "Assert . assertEquals (  2  ,    thirdTimeEpoch )  ;", "}", "METHOD_END"], "methodName": ["testEpoch"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "long   submitTime    =    System . currentTimeMillis (  )  ;", "long   startTime    =     ( System . currentTimeMillis (  )  )     +     1  2  3  4  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", ". TestDispatcher   dispatcher    =    new    . TestDispatcher (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getStateStore (  )  )  . thenReturn ( store )  ;", "AMRMTokenSecretManager   appTokenMgr    =    spy ( new   AMRMTokenSecretManager ( conf ,    rmContext )  )  ;", "MasterKeyData   masterKeyData    =    appTokenMgr . createNewMasterKey (  )  ;", "when ( appTokenMgr . getMasterKey (  )  )  . thenReturn ( masterKeyData )  ;", "ClientToAMTokenSecretManagerInRM   clientToAMTokenMgr    =    new   ClientToAMTokenSecretManagerInRM (  )  ;", "ApplicationAttemptId   attemptId 1     =    ConverterUtils . toApplicationAttemptId (  \" appattempt _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  1  _  0  0  0  0  0  1  \"  )  ;", "ApplicationId   appId 1     =    attemptId 1  . getApplicationId (  )  ;", "storeApp ( store ,    appId 1  ,    submitTime ,    startTime )  ;", "Token < AMRMTokenIdentifier >    appAttemptToken 1     =    generateAMRMToken ( attemptId 1  ,    appTokenMgr )  ;", "SecretKey   clientTokenKey 1     =    clientToAMTokenMgr . createMasterKey ( attemptId 1  )  ;", "ContainerId   containerId 1     =    storeAttempt ( store ,    attemptId 1  ,     \" container _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  ,    appAttemptToken 1  ,    clientTokenKey 1  ,    dispatcher )  ;", "String   appAttemptIdStr 2     =     \" appattempt _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  1  _  0  0  0  0  0  2  \"  ;", "ApplicationAttemptId   attemptId 2     =    ConverterUtils . toApplicationAttemptId ( appAttemptIdStr 2  )  ;", "Token < AMRMTokenIdentifier >    appAttemptToken 2     =    generateAMRMToken ( attemptId 2  ,    appTokenMgr )  ;", "SecretKey   clientTokenKey 2     =    clientToAMTokenMgr . createMasterKey ( attemptId 2  )  ;", "ContainerId   containerId 2     =    storeAttempt ( store ,    attemptId 2  ,     \" container _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  1  _  0  2  _  0  0  0  0  0  1  \"  ,    appAttemptToken 2  ,    clientTokenKey 2  ,    dispatcher )  ;", "ApplicationAttemptId   attemptIdRemoved    =    ConverterUtils . toApplicationAttemptId (  \" appattempt _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  2  _  0  0  0  0  0  1  \"  )  ;", "ApplicationId   appIdRemoved    =    attemptIdRemoved . getApplicationId (  )  ;", "storeApp ( store ,    appIdRemoved ,    submitTime ,    startTime )  ;", "storeAttempt ( store ,    attemptIdRemoved ,     \" container _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  2  _  0  1  _  0  0  0  0  0  1  \"  ,    null ,    null ,    dispatcher )  ;", "RMApp   mockRemovedApp    =    mock ( RMApp . class )  ;", "HashMap < ApplicationAttemptId ,    RMAppAttempt >    attempts    =    new   HashMap < ApplicationAttemptId ,    RMAppAttempt >  (  )  ;", "ApplicationSubmissionContext   context    =    new   ApplicationSubmissionContextPBImpl (  )  ;", "context . setApplicationId ( appIdRemoved )  ;", "when ( mockRemovedApp . getSubmitTime (  )  )  . thenReturn ( submitTime )  ;", "when ( mockRemovedApp . getApplicationSubmissionContext (  )  )  . thenReturn ( context )  ;", "when ( mockRemovedApp . getAppAttempts (  )  )  . thenReturn ( attempts )  ;", "RMAppAttempt   mockRemovedAttempt    =    mock ( RMAppAttempt . class )  ;", "when ( mockRemovedAttempt . getAppAttemptId (  )  )  . thenReturn ( attemptIdRemoved )  ;", "attempts . put ( attemptIdRemoved ,    mockRemovedAttempt )  ;", "store . removeApplication ( mockRemovedApp )  ;", "storeApp ( store ,    appIdRemoved ,    submitTime ,    startTime )  ;", "storeAttempt ( store ,    attemptIdRemoved ,     \" container _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  2  _  0  1  _  0  0  0  0  0  1  \"  ,    null ,    null ,    dispatcher )  ;", "store . removeApplication ( mockRemovedApp )  ;", "Thread . sleep (  1  0  0  0  )  ;", "store . close (  )  ;", "modifyAppState (  )  ;", "store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMStateStore . RMState   state    =    store . loadState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    rmAppState    =    state . getApplicationState (  )  ;", "RMStateStore . ApplicationState   appState    =    rmAppState . get ( appId 1  )  ;", "assertNotNull ( appState )  ;", "assertEquals ( submitTime ,    appState . getSubmitTime (  )  )  ;", "assertEquals ( startTime ,    appState . getStartTime (  )  )  ;", "assertEquals ( appId 1  ,    appState . getApplicationSubmissionContext (  )  . getApplicationId (  )  )  ;", "RMStateStore . ApplicationAttemptState   attemptState    =    appState . getAttempt ( attemptId 1  )  ;", "assertNotNull ( attemptState )  ;", "assertEquals ( attemptId 1  ,    attemptState . getAttemptId (  )  )  ;", "assertEquals (  (  -  1  0  0  0  )  ,    attemptState . getAMContainerExitStatus (  )  )  ;", "assertEquals ( containerId 1  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "assertArrayEquals ( clientTokenKey 1  . getEncoded (  )  ,    attemptState . getAppAttemptCredentials (  )  . getSecretKey ( RMStateStore . AM _ CLIENT _ TOKEN _ MASTER _ KEY _ NAME )  )  ;", "attemptState    =    appState . getAttempt ( attemptId 2  )  ;", "assertNotNull ( attemptState )  ;", "assertEquals ( attemptId 2  ,    attemptState . getAttemptId (  )  )  ;", "assertEquals ( containerId 2  ,    attemptState . getMasterContainer (  )  . getId (  )  )  ;", "assertArrayEquals ( clientTokenKey 2  . getEncoded (  )  ,    attemptState . getAppAttemptCredentials (  )  . getSecretKey ( RMStateStore . AM _ CLIENT _ TOKEN _ MASTER _ KEY _ NAME )  )  ;", "RMStateStore . ApplicationState   appState 2     =    new   RMStateStore . ApplicationState ( appState . submitTime ,    appState . startTime ,    appState . context ,    appState . user ,    RMAppState . FINISHED ,     \" appDiagnostics \"  ,     1  2  3  4  )  ;", "appState 2  . attempts . putAll ( appState . attempts )  ;", "store . updateApplicationState ( appState 2  )  ;", "RMStateStore . ApplicationAttemptState   oldAttemptState    =    attemptState ;", "RMStateStore . ApplicationAttemptState   newAttemptState    =    new   RMStateStore . ApplicationAttemptState ( oldAttemptState . getAttemptId (  )  ,    oldAttemptState . getMasterContainer (  )  ,    oldAttemptState . getAppAttemptCredentials (  )  ,    oldAttemptState . getStartTime (  )  ,    RMAppAttemptState . FINISHED ,     \" myTrackingUrl \"  ,     \" attemptDiagnostics \"  ,    FinalApplicationStatus . SUCCEEDED ,     1  0  0  )  ;", "store . updateApplicationAttemptState ( newAttemptState )  ;", "ApplicationId   dummyAppId    =    ApplicationId . newInstance (  1  2  3  4  ,     1  0  )  ;", "ApplicationSubmissionContext   dummyContext    =    new   ApplicationSubmissionContextPBImpl (  )  ;", "dummyContext . setApplicationId ( dummyAppId )  ;", "RMStateStore . ApplicationState   dummyApp    =    new   RMStateStore . ApplicationState ( appState . submitTime ,    appState . startTime ,    dummyContext ,    appState . user ,    RMAppState . FINISHED ,     \" appDiagnostics \"  ,     1  2  3  4  )  ;", "store . updateApplicationState ( dummyApp )  ;", "ApplicationAttemptId   dummyAttemptId    =    ApplicationAttemptId . newInstance ( dummyAppId ,     6  )  ;", "RMStateStore . ApplicationAttemptState   dummyAttempt    =    new   RMStateStore . ApplicationAttemptState ( dummyAttemptId ,    oldAttemptState . getMasterContainer (  )  ,    oldAttemptState . getAppAttemptCredentials (  )  ,    oldAttemptState . getStartTime (  )  ,    RMAppAttemptState . FINISHED ,     \" myTrackingUrl \"  ,     \" attemptDiagnostics \"  ,    FinalApplicationStatus . SUCCEEDED ,     1  1  1  )  ;", "store . updateApplicationAttemptState ( dummyAttempt )  ;", "Thread . sleep (  1  0  0  0  )  ;", "store . close (  )  ;", "store    =    stateStoreHelper . getRMStateStore (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMStateStore . RMState   newRMState    =    store . loadState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    newRMAppState    =    newRMState . getApplicationState (  )  ;", "assertNotNull ( newRMAppState . get ( dummyApp . getAppId (  )  )  )  ;", "RMStateStore . ApplicationState   updatedAppState    =    newRMAppState . get ( appId 1  )  ;", "assertEquals ( appState . getAppId (  )  ,    updatedAppState . getAppId (  )  )  ;", "assertEquals ( appState . getSubmitTime (  )  ,    updatedAppState . getSubmitTime (  )  )  ;", "assertEquals ( appState . getStartTime (  )  ,    updatedAppState . getStartTime (  )  )  ;", "assertEquals ( appState . getUser (  )  ,    updatedAppState . getUser (  )  )  ;", "assertEquals ( RMAppState . FINISHED ,    updatedAppState . getState (  )  )  ;", "assertEquals (  \" appDiagnostics \"  ,    updatedAppState . getDiagnostics (  )  )  ;", "assertEquals (  1  2  3  4  ,    updatedAppState . getFinishTime (  )  )  ;", "assertNotNull ( newRMAppState . get ( dummyApp . getAppId (  )  )  . getAttempt ( dummyAttemptId )  )  ;", "RMStateStore . ApplicationAttemptState   updatedAttemptState    =    updatedAppState . getAttempt ( newAttemptState . getAttemptId (  )  )  ;", "assertEquals ( oldAttemptState . getAttemptId (  )  ,    updatedAttemptState . getAttemptId (  )  )  ;", "assertEquals ( containerId 2  ,    updatedAttemptState . getMasterContainer (  )  . getId (  )  )  ;", "assertArrayEquals ( clientTokenKey 2  . getEncoded (  )  ,    updatedAttemptState . getAppAttemptCredentials (  )  . getSecretKey ( RMStateStore . AM _ CLIENT _ TOKEN _ MASTER _ KEY _ NAME )  )  ;", "assertEquals ( RMAppAttemptState . FINISHED ,    updatedAttemptState . getState (  )  )  ;", "assertEquals (  \" myTrackingUrl \"  ,    updatedAttemptState . getFinalTrackingUrl (  )  )  ;", "assertEquals (  \" attemptDiagnostics \"  ,    updatedAttemptState . getDiagnostics (  )  )  ;", "assertEquals (  1  0  0  ,    updatedAttemptState . getAMContainerExitStatus (  )  )  ;", "assertEquals ( SUCCEEDED ,    updatedAttemptState . getFinalApplicationStatus (  )  )  ;", "assertTrue ( stateStoreHelper . isFinalStateValid (  )  )  ;", "store . close (  )  ;", "}", "METHOD_END"], "methodName": ["testRMAppStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "RMStateStore   store    =    stateStoreHelper . getRMStateStore (  )  ;", ". TestDispatcher   dispatcher    =    new    . TestDispatcher (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "RMDelegationTokenIdentifier   dtId 1     =    new   RMDelegationTokenIdentifier ( new   Text (  \" owner 1  \"  )  ,    new   Text (  \" renewer 1  \"  )  ,    new   Text (  \" realuser 1  \"  )  )  ;", "Long   renewDate 1     =    new   Long ( System . currentTimeMillis (  )  )  ;", "int   sequenceNumber    =     1  1  1  1  ;", "store . storeRMDelegationTokenAndSequenceNumber ( dtId 1  ,    renewDate 1  ,    sequenceNumber )  ;", "modifyRMDelegationTokenState (  )  ;", "Map < RMDelegationTokenIdentifier ,    Long >    token 1     =    new   HashMap < RMDelegationTokenIdentifier ,    Long >  (  )  ;", "token 1  . put ( dtId 1  ,    renewDate 1  )  ;", "DelegationKey   key    =    new   DelegationKey (  1  2  3  4  ,     4  3  2  1  ,     \" keyBytes \"  . getBytes (  )  )  ;", "HashSet < DelegationKey >    keySet    =    new   HashSet < DelegationKey >  (  )  ;", "keySet . add ( key )  ;", "store . storeRMDTMasterKey ( key )  ;", "RMStateStore . RMDTSecretManagerState   secretManagerState    =    store . loadState (  )  . getRMDTSecretManagerState (  )  ;", "Assert . assertEquals ( token 1  ,    secretManagerState . getTokenState (  )  )  ;", "Assert . assertEquals ( keySet ,    secretManagerState . getMasterKeyState (  )  )  ;", "Assert . assertEquals ( sequenceNumber ,    secretManagerState . getDTSequenceNumber (  )  )  ;", "renewDate 1     =    new   Long ( System . currentTimeMillis (  )  )  ;", "+  + sequenceNumber ;", "store . updateRMDelegationTokenAndSequenceNumber ( dtId 1  ,    renewDate 1  ,    sequenceNumber )  ;", "token 1  . put ( dtId 1  ,    renewDate 1  )  ;", "RMStateStore . RMDTSecretManagerState   updateSecretManagerState    =    store . loadState (  )  . getRMDTSecretManagerState (  )  ;", "Assert . assertEquals ( token 1  ,    updateSecretManagerState . getTokenState (  )  )  ;", "Assert . assertEquals ( keySet ,    updateSecretManagerState . getMasterKeyState (  )  )  ;", "Assert . assertEquals ( sequenceNumber ,    updateSecretManagerState . getDTSequenceNumber (  )  )  ;", "store . removeRMDTMasterKey ( key )  ;", "keySet . clear (  )  ;", "RMStateStore . RMDTSecretManagerState   noKeySecretManagerState    =    store . loadState (  )  . getRMDTSecretManagerState (  )  ;", "Assert . assertEquals ( token 1  ,    noKeySecretManagerState . getTokenState (  )  )  ;", "Assert . assertEquals ( keySet ,    noKeySecretManagerState . getMasterKeyState (  )  )  ;", "Assert . assertEquals ( sequenceNumber ,    noKeySecretManagerState . getDTSequenceNumber (  )  )  ;", "store . removeRMDelegationToken ( dtId 1  ,    sequenceNumber )  ;", "RMStateStore . RMDTSecretManagerState   noKeyAndTokenSecretManagerState    =    store . loadState (  )  . getRMDTSecretManagerState (  )  ;", "token 1  . clear (  )  ;", "Assert . assertEquals ( token 1  ,    noKeyAndTokenSecretManagerState . getTokenState (  )  )  ;", "Assert . assertEquals ( keySet ,    noKeyAndTokenSecretManagerState . getMasterKeyState (  )  )  ;", "Assert . assertEquals ( sequenceNumber ,    noKeySecretManagerState . getDTSequenceNumber (  )  )  ;", "store . close (  )  ;", "}", "METHOD_END"], "methodName": ["testRMDTSecretManagerStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "long   startTime    =    System . currentTimeMillis (  )  ;", "while    (  !  ( dispatcher . notified )  )     {", "synchronized ( dispatcher )     {", "try    {", "dispatcher . wait (  1  0  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", "e . printStackTrace (  )  ;", "}", "}", "if    (  (  ( System . currentTimeMillis (  )  )     -    startTime )     >     (  1  0  0  0     *     6  0  )  )     {", "fail (  \" Timed   out   attempt   s   notification \"  )  ;", "}", "}", "dispatcher . notified    =    false ;", "}", "METHOD_END"], "methodName": ["waitNotify"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreTestBase"}, {"methodBody": ["METHOD_START", "{", "return   attemptState ;", "}", "METHOD_END"], "methodName": ["getAppAttemptState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateUpdateAppAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "return   appState ;", "}", "METHOD_END"], "methodName": ["getAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateUpdateAppEvent"}, {"methodBody": ["METHOD_START", "{", "HdfsConfiguration   conf    =    new   HdfsConfiguration (  )  ;", "MiniDFSCluster   cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  1  )  . build (  )  ;", "try    {", "fsTester    =    newTester ( cluster )  ;", "FSDataOutputStream   fsOut    =    null ;", "FileSystemRMStateStore   fileSystemRMStateStore    =     (  ( FileSystemRMStateStore )     ( fsTester . getRMStateStore (  )  )  )  ;", "String   appAttemptIdStr 3     =     \" appattempt _  1  3  5  2  9  9  4  1  9  3  3  4  3  _  0  0  0  1  _  0  0  0  0  0  3  \"  ;", "ApplicationAttemptId   attemptId 3     =    ConverterUtils . toApplicationAttemptId ( appAttemptIdStr 3  )  ;", "Path   appDir    =    fsTester . store . getAppDir ( attemptId 3  . getApplicationId (  )  . toString (  )  )  ;", "Path   tempAppAttemptFile    =    new   Path ( appDir ,     (  ( attemptId 3  . toString (  )  )     +     \"  . tmp \"  )  )  ;", "fsOut    =    fileSystemRMStateStore . fs . create ( tempAppAttemptFile ,    false )  ;", "fsOut . write (  \" Some   random   data    \"  . getBytes (  )  )  ;", "fsOut . close (  )  ;", "testRMAppStateStore ( fsTester )  ;", "Assert . assertFalse ( fsTester . workingDirPathURI . getFileSystem ( conf )  . exists ( tempAppAttemptFile )  )  ;", "testRMDTSecretManagerStateStore ( fsTester )  ;", "testCheckVersion ( fsTester )  ;", "testEpoch ( fsTester )  ;", "testAppDeletion ( fsTester )  ;", "testDeleteStore ( fsTester )  ;", "testAMRMTokenSecretManagerStateStore ( fsTester )  ;", "}    finally    {", "cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFSRMStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore"}, {"methodBody": ["METHOD_START", "{", "HdfsConfiguration   conf    =    new   HdfsConfiguration (  )  ;", "MiniDFSCluster   cluster    =    new   MiniDFSCluster . Builder ( conf )  . numDataNodes (  2  )  . build (  )  ;", "cluster . waitActive (  )  ;", "try    {", "Tester   fsTester    =    newTester ( cluster )  ;", "final   RMStateStore   store    =    fsTester . getRMStateStore (  )  ;", "store . setRMDispatcher ( new   RMStateStoreTestBase . TestDispatcher (  )  )  ;", "final   AtomicBoolean   assertionFailedInThread    =    new   AtomicBoolean ( false )  ;", "cluster . shutdownNameNodes (  )  ;", "Thread   clientThread    =    new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "store . storeApplicationStateInternal ( ApplicationId . newInstance (  1  0  0 L ,     1  )  ,    ApplicationStateData . newInstance (  1  1  1  ,     1  1  1  ,     \" user \"  ,    null ,    RMAppState . ACCEPTED ,     \" diagnostics \"  ,     3  3  3  )  )  ;", "}    catch    ( Exception   e )     {", "if    (  !  ( e . getMessage (  )  . contains (  (  \" could   only   be   replicated \"     +     \"    to    0    nodes   instead   of   minReplication    (  =  1  )  \"  )  )  )  )     {", "assertionFailedInThread . set ( true )  ;", "}", "e . printStackTrace (  )  ;", "}", "}", "}  ;", "Thread . sleep (  2  0  0  0  )  ;", "clientThread . start (  )  ;", "cluster . restartNameNode (  )  ;", "clientThread . join (  )  ;", "Assert . assertFalse ( assertionFailedInThread . get (  )  )  ;", "}    finally    {", "cluster . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFSRMStateStoreClientRetry"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestFSRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,    rmIds )  ;", "conf . setBoolean ( RECOVERY _ ENABLED ,    true )  ;", "conf . set ( RM _ STORE ,    ZKRMStateStore . class . getName (  )  )  ;", "conf . set ( RM _ ZK _ ADDRESS ,    hostPort )  ;", "conf . setInt ( RM _ ZK _ TIMEOUT _ MS ,     . ZK _ TIMEOUT _ MS )  ;", "conf . set ( RM _ HA _ ID ,    rmId )  ;", "conf . set ( RM _ WEBAPP _ ADDRESS ,     \" localhost :  0  \"  )  ;", "for    ( String   rpcAddress    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "for    ( String   id    :    HAUtil . getRMHAIds ( conf )  )     {", "conf . set ( HAUtil . addSuffix ( rpcAddress ,    id )  ,     \" localhost :  0  \"  )  ;", "}", "}", "conf . set ( HAUtil . addSuffix ( RM _ ADMIN _ ADDRESS ,    rmId )  ,     (  \" localhost :  \"     +    adminPort )  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createHARMConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "StateChangeRequestInfo   req    =    new   StateChangeRequestInfo ( RequestSource . REQUEST _ BY _ USER )  ;", "Configuration   conf 1     =    createHARMConf (  \" rm 1  , rm 2  \"  ,     \" rm 1  \"  ,     1  2  3  4  )  ;", "conf 1  . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "ResourceManager   rm 1     =    new   ResourceManager (  )  ;", "rm 1  . init ( conf 1  )  ;", "rm 1  . start (  )  ;", "rm 1  . getRMContext (  )  . getRMAdminService (  )  . transitionToActive ( req )  ;", "assertEquals (  \" RM   with   ZKStore   didn ' t   start \"  ,    STARTED ,    rm 1  . getServiceState (  )  )  ;", "assertEquals (  \" RM   should   be   Active \"  ,    ACTIVE ,    rm 1  . getRMContext (  )  . getRMAdminService (  )  . getServiceStatus (  )  . getState (  )  )  ;", "Configuration   conf 2     =    createHARMConf (  \" rm 1  , rm 2  \"  ,     \" rm 2  \"  ,     5  6  7  8  )  ;", "conf 2  . setBoolean ( AUTO _ FAILOVER _ ENABLED ,    false )  ;", "ResourceManager   rm 2     =    new   ResourceManager (  )  ;", "rm 2  . init ( conf 2  )  ;", "rm 2  . start (  )  ;", "rm 2  . getRMContext (  )  . getRMAdminService (  )  . transitionToActive ( req )  ;", "assertEquals (  \" RM   with   ZKStore   didn ' t   start \"  ,    STARTED ,    rm 2  . getServiceState (  )  )  ;", "assertEquals (  \" RM   should   be   Active \"  ,    ACTIVE ,    rm 2  . getRMContext (  )  . getRMAdminService (  )  . getServiceStatus (  )  . getState (  )  )  ;", "for    ( int   i    =     0  ;    i    <     (  (  . ZK _ TIMEOUT _ MS )     /     5  0  )  ;    i +  +  )     {", "if    (  ( HAServiceState . ACTIVE )     =  =     ( rm 1  . getRMContext (  )  . getRMAdminService (  )  . getServiceStatus (  )  . getState (  )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "assertEquals (  \" RM   should   have   been   fenced \"  ,    STANDBY ,    rm 1  . getRMContext (  )  . getRMAdminService (  )  . getServiceStatus (  )  . getState (  )  )  ;", "assertEquals (  \" RM   should   be   Active \"  ,    ACTIVE ,    rm 2  . getRMContext (  )  . getRMAdminService (  )  . getServiceStatus (  )  . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFencing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStore . TestZKRMStateStoreTester   zkTester    =    new   TestZKRMStateStore . TestZKRMStateStoreTester (  )  ;", "testRMAppStateStore ( zkTester )  ;", "testRMDTSecretManagerStateStore ( zkTester )  ;", "testCheckVersion ( zkTester )  ;", "testEpoch ( zkTester )  ;", "testAppDeletion ( zkTester )  ;", "testDeleteStore ( zkTester )  ;", "testAMRMTokenSecretManagerStateStore ( zkTester )  ;", "}", "METHOD_END"], "methodName": ["testZKRMStateStoreRealZK"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ ZK _ ACL ,     \" randomstring &  *  \"  )  ;", "try    {", "zkClientTester . getRMStateStore ( conf )  ;", "fail (  \" ZKRMStateStore   created   with   bad   ACL \"  )  ;", "}    catch    ( ZKUtil   bafe )     {", "}    catch    ( Exception   e )     {", "String   error    =     \" Incorrect   exception   on   BadAclFormat \"  ;", "LOG . error ( error ,    e )  ;", "fail ( error )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidZKAclConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ ZK _ ACL ,     \" world : anyone : rwca \"  )  ;", "try    {", "zkClientTester . store . zkClient . delete ( zkClientTester . store . znodeWorkingPath ,     (  -  1  )  )  ;", "fail (  \" Shouldn ' t   be   able   to   delete   path \"  )  ;", "}    catch    ( Exception   e )     {", "}", "}", "METHOD_END"], "methodName": ["testSetZKAcl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ ZK _ NUM _ RETRIES ,     1  )  ;", "conf . setInt ( RM _ ZK _ TIMEOUT _ MS ,    TestZKRMStateStoreZKClientConnections . ZK _ TIMEOUT _ MS )  ;", "conf . set ( RM _ ZK _ ACL ,    TestZKRMStateStoreZKClientConnections . TEST _ ACL )  ;", "conf . set ( RM _ ZK _ AUTH ,    TestZKRMStateStoreZKClientConnections . TEST _ AUTH _ GOOD )  ;", "zkClientTester . getRMStateStore ( conf )  ;", "}", "METHOD_END"], "methodName": ["testZKAuths"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "String   path    =     \"  / test \"  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ ZK _ TIMEOUT _ MS ,    TestZKRMStateStoreZKClientConnections . ZK _ TIMEOUT _ MS )  ;", "ZKRMStateStore   store    =     (  ( ZKRMStateStore )     ( zkClientTester . getRMStateStore ( conf )  )  )  ;", "RMStateStoreTestBase . TestDispatcher   dispatcher    =    new   RMStateStoreTestBase . TestDispatcher (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "store . createWithRetries ( path ,    null ,    OPEN _ ACL _ UNSAFE ,    PERSISTENT )  ;", "store . getDataWithRetries ( path ,    true )  ;", "store . setDataWithRetries ( path ,     \" newBytes \"  . getBytes (  )  ,     0  )  ;", "stopServer (  )  ;", "zkClientTester . watcher . waitForDisconnected ( TestZKRMStateStoreZKClientConnections . ZK _ OP _ WAIT _ TIME )  ;", "try    {", "store . getDataWithRetries ( path ,    true )  ;", "fail (  \" Expected   ZKClient   time   out   exception \"  )  ;", "}    catch    ( Exception   e )     {", "assertTrue ( e . getMessage (  )  . contains (  \" Wait   for   ZKClient   creation   timed   out \"  )  )  ;", "}", "startServer (  )  ;", "zkClientTester . watcher . waitForConnected ( TestZKRMStateStoreZKClientConnections . ZK _ OP _ WAIT _ TIME )  ;", "byte [  ]    ret    =    null ;", "try    {", "ret    =    store . getDataWithRetries ( path ,    true )  ;", "}    catch    ( Exception   e )     {", "String   error    =     \" ZKRMStateStore   Session   restore   failed \"  ;", "LOG . error ( error ,    e )  ;", "fail ( error )  ;", "}", "assertEquals (  \" newBytes \"  ,    new   String ( ret )  )  ;", "}", "METHOD_END"], "methodName": ["testZKClientDisconnectAndReconnect"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "final   String   path    =     \"  / test \"  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ ZK _ TIMEOUT _ MS ,    TestZKRMStateStoreZKClientConnections . ZK _ TIMEOUT _ MS )  ;", "conf . setLong ( RM _ ZK _ RETRY _ INTERVAL _ MS ,     1  0  0  )  ;", "final   ZKRMStateStore   store    =     (  ( ZKRMStateStore )     ( zkClientTester . getRMStateStore ( conf )  )  )  ;", "RMStateStoreTestBase . TestDispatcher   dispatcher    =    new   RMStateStoreTestBase . TestDispatcher (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "final   AtomicBoolean   assertionFailedInThread    =    new   AtomicBoolean ( false )  ;", "stopServer (  )  ;", "Thread   clientThread    =    new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "store . getDataWithRetries ( path ,    true )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "assertionFailedInThread . set ( true )  ;", "}", "}", "}  ;", "Thread . sleep (  2  0  0  0  )  ;", "startServer (  )  ;", "clientThread . join (  )  ;", "Assert . assertFalse ( assertionFailedInThread . get (  )  )  ;", "}", "METHOD_END"], "methodName": ["testZKClientRetry"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "ZKRMStateStore   store    =     (  ( ZKRMStateStore )     ( zkClientTester . getRMStateStore ( conf )  )  )  ;", "assertEquals ( DEFAULT _ RM _ ZK _ RETRY _ INTERVAL _ MS ,    store . zkRetryInterval )  ;", "store . stop (  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "store    =     (  ( ZKRMStateStore )     ( zkClientTester . getRMStateStore ( conf )  )  )  ;", "assertEquals (  (  ( YarnConfiguration . DEFAULT _ RM _ ZK _ TIMEOUT _ MS )     /     ( YarnConfiguration . DEFAULT _ ZK _ RM _ NUM _ RETRIES )  )  ,    store . zkRetryInterval )  ;", "store . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testZKRetryInterval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "TestZKRMStateStoreZKClientConnections . TestZKClient   zkClientTester    =    new   TestZKRMStateStoreZKClientConnections . TestZKClient (  )  ;", "String   path    =     \"  / test \"  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ ZK _ TIMEOUT _ MS ,    TestZKRMStateStoreZKClientConnections . ZK _ TIMEOUT _ MS )  ;", "ZKRMStateStore   store    =     (  ( ZKRMStateStore )     ( zkClientTester . getRMStateStore ( conf )  )  )  ;", "RMStateStoreTestBase . TestDispatcher   dispatcher    =    new   RMStateStoreTestBase . TestDispatcher (  )  ;", "store . setRMDispatcher ( dispatcher )  ;", "zkClientTester . forExpire    =    true ;", "store . createWithRetries ( path ,    null ,    OPEN _ ACL _ UNSAFE ,    PERSISTENT )  ;", "store . getDataWithRetries ( path ,    true )  ;", "store . setDataWithRetries ( path ,     \" bytes \"  . getBytes (  )  ,     0  )  ;", "zkClientTester . syncBarrier . await (  )  ;", "try    {", "byte [  ]    ret    =    store . getDataWithRetries ( path ,    false )  ;", "assertEquals (  \" bytes \"  ,    new   String ( ret )  )  ;", "}    catch    ( Exception   e )     {", "String   error    =     \" New   session   creation   failed \"  ;", "LOG . error ( error ,    e )  ;", "fail ( error )  ;", "}", "}", "METHOD_END"], "methodName": ["testZKSessionTimeout"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStoreZKClientConnections"}, {"methodBody": ["METHOD_START", "{", "String   nodeCreatePath    =    getNodePath ( delegationTokensRootPath ,     (  ( RMStateStore . DELEGATION _ TOKEN _ PREFIX )     +     ( rmDTIdentifier . getSequenceNumber (  )  )  )  )  ;", "ByteArrayOutputStream   tokenOs    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   tokenOut    =    new   DataOutputStream ( tokenOs )  ;", "ByteArrayOutputStream   seqOs    =    new   ByteArrayOutputStream (  )  ;", "DataOutputStream   seqOut    =    new   DataOutputStream ( seqOs )  ;", "try    {", "rmDTIdentifier . write ( tokenOut )  ;", "tokenOut . writeLong ( renewDate )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  ( isUpdate    ?     \" Storing    \"     :     \" Updating    \"  )     +     \" RMDelegationToken _  \"  )     +     ( rmDTIdentifier . getSequenceNumber (  )  )  )  )  ;", "}", "if    ( isUpdate )     {", "opList . add ( Op . setData ( nodeCreatePath ,    tokenOs . toByteArray (  )  ,     (  -  1  )  )  )  ;", "} else    {", "opList . add ( Op . create ( nodeCreatePath ,    tokenOs . toByteArray (  )  ,    zkAcl ,    PERSISTENT )  )  ;", "}", "seqOut . writeInt ( latestSequenceNumber )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  ( isUpdate    ?     \" Storing    \"     :     \" Updating    \"  )     +     ( dtSequenceNumberPath )  )     +     \"  .    SequenceNumber :     \"  )     +    latestSequenceNumber )  )  ;", "}", "opList . add ( Op . setData ( dtSequenceNumberPath ,    seqOs . toByteArray (  )  ,     (  -  1  )  )  )  ;", "}    finally    {", "tokenOs . close (  )  ;", "seqOs . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addStoreOrUpdateOps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( zkClient )     !  =    null )     {", "t    {", "zkClient . close (  )  ;", "}    catch    ( InterruptedException   e )     {", "throw   new   IOException (  \" Interrupted   while   closing   ZK \"  ,    e )  ;", "}", "zkClient    =    null ;", "}", "if    (  ( oldZkClient )     !  =    null )     {", "t    {", "oldZkClient . close (  )  ;", "}    catch    ( InterruptedException   e )     {", "throw   new   IOException (  \" Interrupted   while   closing   old   ZK \"  ,    e )  ;", "}", "oldZkClient    =    null ;", "}", "}", "METHOD_END"], "methodName": ["closeZkClients"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "List < ACL >    zkRootNodeAcl    =    new   ArrayList < ACL >  (  )  ;", "for    ( ACL   acl    :    sourceACLs )     {", "zkRootNodeAcl . add ( new   ACL ( ZKUtil . removeSpecificPerms ( acl . getPerms (  )  ,     . CREATE _ DELETE _ PERMS )  ,    acl . getId (  )  )  )  ;", "}", "zkRootNodeUsername    =    HAUtil . getConfValueForRMInstance ( RM _ ADDRESS ,    DEFAULT _ RM _ ADDRESS ,    conf )  ;", "Id   rmId    =    new   Id ( zkRootNodeAuthScheme ,    DigestAuthenticationProvider . generateDigest (  (  (  ( zkRootNodeUsername )     +     \"  :  \"  )     +     ( zkRootNodePassword )  )  )  )  ;", "zkRootNodeAcl . add ( new   ACL (  . CREATE _ DELETE _ PERMS ,    rmId )  )  ;", "return   zkRootNodeAcl ;", "}", "METHOD_END"], "methodName": ["constructZkRootNodeACL"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "closeZkClients (  )  ;", "for    ( int   retries    =     0  ;     ( retries    <     ( numRetries )  )     &  &     (  ( zkClient )     =  =    null )  ;    retries +  +  )     {", "try    {", "zkClient    =    getNewZooKeeper (  )  ;", "for    ( ZKUtil . ZKAuthInfo   zkAuth    :    zkAuths )     {", "zkClient . addAuthInfo ( zkAuth . getScheme (  )  ,    zkAuth . getAuth (  )  )  ;", "}", "if    ( useDefaultFencingScheme )     {", "zkClient . addAuthInfo ( zkRootNodeAuthScheme ,     (  (  ( zkRootNodeUsername )     +     \"  :  \"  )     +     ( zkRootNodePassword )  )  . getBytes (  )  )  ;", "}", "}    catch    ( IOException   ioe )     {", ". LOG . info (  (  \" Failed   to   connect   to   the   ZooKeeper   on   attempt    -     \"     +     ( retries    +     1  )  )  )  ;", "ioe . printStackTrace (  )  ;", "}", "}", "if    (  ( zkClient )     =  =    null )     {", ". LOG . error (  \" Unable   to   connect   to   Zookeeper \"  )  ;", "throw   new   YarnRuntimeException (  \" Unable   to   connect   to   Zookeeper \"  )  ;", "}", "this . notifyAll (  )  ;", ". LOG . info (  \" Created   new   ZK   connection \"  )  ;", "}", "METHOD_END"], "methodName": ["createConnection"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "new   ZKRMStateStore . ZKAction < String >  (  )     {", "@ Override", "public   String   run (  )    throws   InterruptedException ,    KeeperException    {", "try    {", "return   zkClient . create ( rootPath ,    null ,    zkAcl ,    PERSISTENT )  ;", "}    catch    ( KeeperException   ke )     {", "if    (  ( ke . code (  )  )     =  =     ( Code . NODEEXISTS )  )     {", "ZKRMStateStore . LOG . debug (  ( rootPath    +     \" znode   already   exists !  \"  )  )  ;", "return   null ;", "} else    {", "throw   ke ;", "}", "}", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["createRootDir"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "doMultiWithRetries ( Op . create ( path ,    data ,    acl ,    mode )  )  ;", "}", "METHOD_END"], "methodName": ["createWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "new   ZKRMStateStore . ZKAction < Void >  (  )     {", "@ Override", "Void   run (  )    throws   InterruptedException ,    KeeperException    {", "recursiveDeleteWithRetriesHelper ( path ,    watch )  ;", "return   null ;", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["deleteWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "final   List < Op >    execOpList    =    new   ArrayList < Op >  (  (  ( opList . size (  )  )     +     2  )  )  ;", "execOpList . add ( createFencingNodePathOp )  ;", "execOpList . addAll ( opList )  ;", "execOpList . add ( deleteFencingNodePathOp )  ;", "new    . ZKAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   InterruptedException ,    KeeperException    {", "zkClient . multi ( execOpList )  ;", "return   null ;", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["doMultiWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "doMultiWithRetries ( Collections . singletonList ( op )  )  ;", "}", "METHOD_END"], "methodName": ["doMultiWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ZKRMStateStore . ZKAction < Stat >  (  )     {", "@ Override", "Stat   run (  )    throws   InterruptedException ,    KeeperException    {", "return   zkClient . exists ( path ,    watch )  ;", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["existsWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "if    ( ZKRMStateStore . LOG . isTraceEnabled (  )  )     {", "logRootNodeAcls (  \" Before   fencing \\ n \"  )  ;", "}", "new   ZKRMStateStore . ZKAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   InterruptedException ,    KeeperException    {", "zkClient . setACL ( zkRootNodePath ,    zkRootNodeAcl ,     (  -  1  )  )  ;", "return   null ;", "}", "}  . runWithRetries (  )  ;", "new   ZKRMStateStore . ZKAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   InterruptedException ,    KeeperException    {", "try    {", "zkClient . multi ( Collections . singletonList ( deleteFencingNodePathOp )  )  ;", "}    catch    ( KeeperException   nne )     {", "ZKRMStateStore . LOG . info (  (  (  \" Fencing   node    \"     +     ( fencingNodePath )  )     +     \"    doesn ' t   exist   to   delete \"  )  )  ;", "}", "return   null ;", "}", "}  . runWithRetries (  )  ;", "if    ( ZKRMStateStore . LOG . isTraceEnabled (  )  )     {", "logRootNodeAcls (  \" After   fencing \\ n \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["fence"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ZKRMStateStore . ZKAction < List < ACL >  >  (  )     {", "@ Override", "public   List < ACL >    run (  )    throws   InterruptedException ,    KeeperException    {", "return   zkClient . getACL ( path ,    stat )  ;", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["getACLWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ZKRMStateStore . ZKAction < List < String >  >  (  )     {", "@ Override", "List < String >    run (  )    throws   InterruptedException ,    KeeperException    {", "return   zkClient . getChildren ( path ,    watch )  ;", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["getChildrenWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return   new   ZKRMStateStore . ZKAction < byte [  ]  >  (  )     {", "@ Override", "public   byte [  ]    run (  )    throws   InterruptedException ,    KeeperException    {", "return   zkClient . getData ( path ,    watch ,    null )  ;", "}", "}  . runWithRetries (  )  ;", "}", "METHOD_END"], "methodName": ["getDataWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "ZooKeeper   zk    =    new   ZooKeeper ( zkHostPort ,    zkSessionTimeout ,    null )  ;", "zk . register ( new    . ForwardingWatcher (  )  )  ;", "return   zk ;", "}", "METHOD_END"], "methodName": ["getNewZooKeeper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "return    ( root    +     \"  /  \"  )     +    nodeName ;", "}", "METHOD_END"], "methodName": ["getNodePath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    data    =    getDataWithRetries ( amrmTokenSecretManagerRoot ,    true )  ;", "if    ( data    =  =    null )     {", ". LOG . warn (  \" There   is   no   data   saved \"  )  ;", "return ;", "}", "AMRMTokenSecretManagerStatePBImpl   stateData    =    new   AMRMTokenSecretManagerStatePBImpl ( AMRMTokenSecretManagerStateProto . parseFrom ( data )  )  ;", "rmState . amrmTokenSecretManagerState    =    AMRMTokenSecretManagerState . newInstance ( stateData . getCurrentMasterKey (  )  ,    stateData . getNextMasterKey (  )  )  ;", "}", "METHOD_END"], "methodName": ["loadAMRMTokenSecretManagerState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "String   appPath    =    getNodePath ( rmAppRoot ,    appId . toString (  )  )  ;", "List < String >    attempts    =    getChildrenWithRetries ( appPath ,    false )  ;", "for    ( String   attemptIDStr    :    attempts )     {", "if    ( attemptIDStr . startsWith ( appAttemptIdStrPrefix )  )     {", "String   attemptPath    =    getNodePath ( appPath ,    attemptIDStr )  ;", "byte [  ]    attemptData    =    getDataWithRetries ( attemptPath ,    true )  ;", "ApplicationAttemptId   attemptId    =    ConverterUtils . toApplicationAttemptId ( attemptIDStr )  ;", "ApplicationAttemptStateDataPBImpl   attemptStateData    =    new   ApplicationAttemptStateDataPBImpl ( ApplicationAttemptStateDataProto . parseFrom ( attemptData )  )  ;", "Credentials   credentials    =    null ;", "if    (  ( attemptStateData . getAppAttemptTokens (  )  )     !  =    null )     {", "credentials    =    new   Credentials (  )  ;", "DataInputByteBuffer   dibb    =    new   DataInputByteBuffer (  )  ;", "dibb . reset ( attemptStateData . getAppAttemptTokens (  )  )  ;", "credentials . readTokenStorageStream ( dibb )  ;", "}", "RMStateStore . ApplicationAttemptState   attemptState    =    new   RMStateStore . ApplicationAttemptState ( attemptId ,    attemptStateData . getMasterContainer (  )  ,    credentials ,    attemptStateData . getStartTime (  )  ,    attemptStateData . getState (  )  ,    attemptStateData . getFinalTrackingUrl (  )  ,    attemptStateData . getDiagnostics (  )  ,    attemptStateData . getFinalApplicationStatus (  )  ,    attemptStateData . getAMContainerExitStatus (  )  )  ;", "appState . attempts . put ( attemptState . getAttemptId (  )  ,    attemptState )  ;", "}", "}", ". LOG . debug (  \" Done   Loading   applications   from   ZK   state   store \"  )  ;", "}", "METHOD_END"], "methodName": ["loadApplicationAttemptState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "List < String >    childNodes    =    getChildrenWithRetries ( rmAppRoot ,    true )  ;", "for    ( String   childNodeName    :    childNodes )     {", "String   childNodePath    =    getNodePath ( rmAppRoot ,    childNodeName )  ;", "byte [  ]    childData    =    getDataWithRetries ( childNodePath ,    true )  ;", "if    ( childNodeName . startsWith ( appIdStrPrefix )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Loading   application   from   znode :     \"     +    childNodeName )  )  ;", "}", "ApplicationId   appId    =    ConverterUtils . toApplicationId ( childNodeName )  ;", "ApplicationStateDataPBImpl   appStateData    =    new   ApplicationStateDataPBImpl ( ApplicationStateDataProto . parseFrom ( childData )  )  ;", "RMStateStore . ApplicationState   appState    =    new   RMStateStore . ApplicationState ( appStateData . getSubmitTime (  )  ,    appStateData . getStartTime (  )  ,    appStateData . getApplicationSubmissionContext (  )  ,    appStateData . getUser (  )  ,    appStateData . getState (  )  ,    appStateData . getDiagnostics (  )  ,    appStateData . getFinishTime (  )  )  ;", "if    (  !  ( appId . equals ( appState . context . getApplicationId (  )  )  )  )     {", "throw   new   YarnRuntimeException (  (  \" The   child   node   name   is   different    \"     +     \" from   the   application   id \"  )  )  ;", "}", "rmState . appState . put ( appId ,    appState )  ;", "loadApplicationAttemptState ( appState ,    appId )  ;", "} else    {", ". LOG . info (  (  \" Unknown   child   node   with   name :     \"     +    childNodeName )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["loadRMAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "loadRMDelegationKeyState ( rmState )  ;", "loadRMSequentialNumberState ( rmState )  ;", "loadRMDelegationTokenState ( rmState )  ;", "}", "METHOD_END"], "methodName": ["loadRMDTSecretManagerState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "List < String >    childNodes    =    getChildrenWithRetries ( dtMasterKeysRootPath ,    true )  ;", "for    ( String   childNodeName    :    childNodes )     {", "String   childNodePath    =    getNodePath ( dtMasterKeysRootPath ,    childNodeName )  ;", "byte [  ]    childData    =    getDataWithRetries ( childNodePath ,    true )  ;", "if    ( childData    =  =    null )     {", ". LOG . warn (  (  (  \" Content   of    \"     +    childNodePath )     +     \"    is   broken .  \"  )  )  ;", "continue ;", "}", "ByteArrayInputStream   is    =    new   ByteArrayInputStream ( childData )  ;", "DataInputStream   fsIn    =    new   DataInputStream ( is )  ;", "try    {", "if    ( childNodeName . startsWith ( RMStateStore . DELEGATION _ KEY _ PREFIX )  )     {", "DelegationKey   key    =    new   DelegationKey (  )  ;", "key . readFields ( fsIn )  ;", "rmState . rmSecretManagerState . masterKeyState . add ( key )  ;", "}", "}    finally    {", "is . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["loadRMDelegationKeyState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "List < String >    childNodes    =    getChildrenWithRetries ( delegationTokensRootPath ,    true )  ;", "for    ( String   childNodeName    :    childNodes )     {", "String   childNodePath    =    getNodePath ( delegationTokensRootPath ,    childNodeName )  ;", "byte [  ]    childData    =    getDataWithRetries ( childNodePath ,    true )  ;", "if    ( childData    =  =    null )     {", ". LOG . warn (  (  (  \" Content   of    \"     +    childNodePath )     +     \"    is   broken .  \"  )  )  ;", "continue ;", "}", "ByteArrayInputStream   is    =    new   ByteArrayInputStream ( childData )  ;", "DataInputStream   fsIn    =    new   DataInputStream ( is )  ;", "try    {", "if    ( childNodeName . startsWith ( RMStateStore . DELEGATION _ TOKEN _ PREFIX )  )     {", "RMDelegationTokenIdentifier   identifier    =    new   RMDelegationTokenIdentifier (  )  ;", "identifier . readFields ( fsIn )  ;", "long   renewDate    =    fsIn . readLong (  )  ;", "rmState . rmSecretManagerState . delegationTokenState . put ( identifier ,    renewDate )  ;", "}", "}    finally    {", "is . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["loadRMDelegationTokenState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    seqData    =    getDataWithRetries ( dtSequenceNumberPath ,    false )  ;", "if    ( seqData    !  =    null )     {", "ByteArrayInputStream   seqIs    =    new   ByteArrayInputStream ( seqData )  ;", "DataInputStream   seqIn    =    new   DataInputStream ( seqIs )  ;", "try    {", "rmState . rmSecretMState . dtSequenceNumber    =    seqIn . readInt (  )  ;", "}    finally    {", "seqIn . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["loadRMSequentialNumberState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Stat   getStat    =    new   Stat (  )  ;", "List < ACL >    getAcls    =    getACLWithRetries ( zkRootNodePath ,    getStat )  ;", "StringBuilder   builder    =    new   StringBuilder (  )  ;", "builder . append ( prefix )  ;", "for    ( ACL   acl    :    getAcls )     {", "builder . append ( acl . toString (  )  )  ;", "}", "builder . append ( getStat . toString (  )  )  ;", ". LOG . debug ( builder . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["logRootNodeAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "Event . EventType   eventType    =    event . getType (  )  ;", ". LOG . info (  (  (  (  (  (  (  (  \" Watcher   event   type :     \"     +    eventType )     +     \"    with   state :  \"  )     +     ( event . getState (  )  )  )     +     \"    for   path :  \"  )     +     ( event . getPath (  )  )  )     +     \"    for    \"  )     +     ( this )  )  )  ;", "if    ( eventType    =  =     ( EventType . None )  )     {", "switch    ( event . getState (  )  )     {", "case   SyncConnected    :", ". LOG . info (  \"    Session   connected \"  )  ;", "if    (  ( oldZkClient )     !  =    null )     {", "zkClient    =    oldZkClient ;", "oldZkClient    =    null ;", "this . notifyAll (  )  ;", ". LOG . info (  \"    Session   restored \"  )  ;", "}", "break ;", "case   Disconnected    :", ". LOG . info (  \"    Session   disconnected \"  )  ;", "oldZkClient    =    zkClient ;", "zkClient    =    null ;", "break ;", "case   Expired    :", ". LOG . info (  \"    Session   expired \"  )  ;", "createConnection (  )  ;", "break ;", "default    :", ". LOG . error (  (  (  \" Unexpected   Zookeeper \"     +     \"    watch   event   state :     \"  )     +     ( event . getState (  )  )  )  )  ;", "break ;", "}", "}", "}", "METHOD_END"], "methodName": ["processWatchEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "List < String >    children    =    zkClient . getChildren ( path ,    watch )  ;", "for    ( String   child    :    children )     {", "ursiveDeleteWithRetriesHelper (  (  ( path    +     \"  /  \"  )     +    child )  ,    false )  ;", "}", "zkClient . delete ( path ,     (  -  1  )  )  ;", "}", "METHOD_END"], "methodName": ["recursiveDeleteWithRetriesHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "doMultiWithRetries ( Op . setData ( path ,    data ,    version )  )  ;", "}", "METHOD_END"], "methodName": ["setDataWithRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"}, {"methodBody": ["METHOD_START", "{", "AMRMTokenSecretManagerState   data    =    Records . newRecord ( AMRMTokenSecretManagerState . class )  ;", "data . setCurrentMasterKey ( currentMasterKey )  ;", "data . setNextMasterKey ( nextMasterKey )  ;", "return   data ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState"}, {"methodBody": ["METHOD_START", "{", "AMRMTokenSecretManagerState   data    =    Records . newRecord ( AMRMTokenSecretManagerState . class )  ;", "data . setCurrentMasterKey ( state . getCurrentMasterKey (  )  )  ;", "data . setNextMasterKey ( state . getNextMasterKey (  )  )  ;", "return   data ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.AMRMTokenSecretManagerState"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptStateData   attemptStateData    =    Records . newRecord ( ApplicationAttemptStateData . class )  ;", "attemptStateData . setAttemptId ( attemptId )  ;", "attemptStateData . setMasterContainer ( container )  ;", "attemptStateData . setAppAttemptTokens ( attemptTokens )  ;", "attemptStateData . setState ( finalState )  ;", "attemptStateData . setFinalTrackingUrl ( finalTrackingUrl )  ;", "attemptStateData . setDiagnostics ( diagnostics )  ;", "attemptStateData . setStartTime ( startTime )  ;", "attemptStateData . setFinalApplicationStatus ( amUnregisteredFinalStatus )  ;", "attemptStateData . setAMContainerExitStatus ( exitStatus )  ;", "return   attemptStateData ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData"}, {"methodBody": ["METHOD_START", "{", "Credentials   credentials    =    attemptState . getAppAttemptCredentials (  )  ;", "ByteBuffer   appAttemptTokens    =    null ;", "if    ( credentials    !  =    null )     {", "DataOutputBuffer   dob    =    new   DataOutputBuffer (  )  ;", "credentials . writeTokenStorageToStream ( dob )  ;", "appAttemptTokens    =    ByteBuffer . wrap ( dob . getData (  )  ,     0  ,    dob . getLength (  )  )  ;", "}", "return    . newInstance ( attemptState . getAttemptId (  )  ,    attemptState . getMasterContainer (  )  ,    appAttemptTokens ,    attemptState . getStartTime (  )  ,    attemptState . getState (  )  ,    attemptState . getFinalTrackingUrl (  )  ,    attemptState . getDiagnostics (  )  ,    attemptState . getFinalApplicationStatus (  )  ,    attemptState . getAMContainerExitStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationAttemptStateData"}, {"methodBody": ["METHOD_START", "{", "ApplicationStateData   appState    =    Records . newRecord ( ApplicationStateData . class )  ;", "appState . setSubmitTime ( submitTime )  ;", "appState . setStartTime ( startTime )  ;", "appState . setUser ( user )  ;", "appState . setApplicationSubmissionContext ( submissionContext )  ;", "appState . setState ( state )  ;", "appState . setDiagnostics ( diagnostics )  ;", "appState . setFinishTime ( finishTime )  ;", "return   appState ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationStateData . newInstance ( appState . getSubmitTime (  )  ,    appState . getStartTime (  )  ,    appState . getUser (  )  ,    appState . getApplicationSubmissionContext (  )  ,    appState . getState (  )  ,    appState . getDiagnostics (  )  ,    appState . getFinishTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData"}, {"methodBody": ["METHOD_START", "{", "Epoch   epoch    =    Records . newRecord ( Epoch . class )  ;", "epoch . setEpoch ( sequenceNumber )  ;", "return   epoch ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.Epoch"}, {"methodBody": ["METHOD_START", "{", "return   String . valueOf ( getEpoch (  )  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.Epoch"}, {"methodBody": ["METHOD_START", "{", "return   new   MasterKeyPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.AMRMTokenSecretManagerStatePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( MasterKeyPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.AMRMTokenSecretManagerStatePBImpl"}, {"methodBody": ["METHOD_START", "{", "meLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    buildbuild (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.AMRMTokenSecretManagerStatePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.AMRMTokenSecretManagerStatePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . currentMasterKey )     !  =    null )     {", "builder . setCurrentMasterKey ( convertToProtoFormat ( this . currentMasterKey )  )  ;", "}", "if    (  ( this . nextMasterKey )     !  =    null )     {", "builder . setNextMasterKey ( convertToProtoFormat ( this . nextMasterKey )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.AMRMTokenSecretManagerStatePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild )  ;", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.AMRMTokenSecretManagerStatePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RMAppAttemptState . valueOf ( e . name (  )  . replace ( ApplicationAttemptStateDataPBImpl . RM _ APP _ ATTEMPT _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RMAppAttemptStateProto . valueOf (  (  ( ApplicationAttemptStateDataPBImpl . RM _ APP _ ATTEMPT _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . attemptId )     !  =    null )     {", "builder . setAttemptId (  (  ( IdPBImpl )     ( attemptId )  )  . getProto (  )  )  ;", "}", "if    (  ( this . masterContainer )     !  =    null )     {", "builder . setMasterContainer (  (  ( ContainerPBImpl )     ( masterContainer )  )  . getProto (  )  )  ;", "}", "if    (  ( this . appAttemptTokens )     !  =    null )     {", "builder . setAppAttemptTokens ( ProtoUtils . convertToProtoFormat ( this . appAttemptTokens )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild )  ;", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationAttemptStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RMAppState . valueOf ( e . name (  )  . replace ( ApplicationStateDataPBImpl . RM _ APP _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RMAppStateProto . valueOf (  (  ( ApplicationStateDataPBImpl . RM _ APP _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationSubmissionContext )     !  =    null )     {", "builder . setubmissionContext (  (  ( ubmissionContextPBImpl )     ( applicationSubmissionContext )  )  . getProto (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild )  ;", "meLocalToBuild )  ;", "proto    =    buildbuild (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.ApplicationStateDataPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.EpochPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.recovery.records.impl.pb.EpochPBImpl"}, {"methodBody": ["METHOD_START", "{", "Priority   priority    =    RecordFactoryProvider . getRecordFactory ( null )  . newRecordInstance ( class )  ;", "priority . setPriority ( prio )  ;", "return   priority ;", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.Priority"}, {"methodBody": ["METHOD_START", "{", "return   weights [ resourceType . ordinal (  )  ]  ;", "}", "METHOD_END"], "methodName": ["getWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceWeights"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( weights . length )  ;    i +  +  )     {", "weights [ i ]     =    weight ;", "}", "}", "METHOD_END"], "methodName": ["setWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceWeights"}, {"methodBody": ["METHOD_START", "{", "weights [ resourceType . ordinal (  )  ]     =    weight ;", "}", "METHOD_END"], "methodName": ["setWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceWeights"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  <  \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( Type . values (  )  . length )  ;    i +  +  )     {", "if    ( i    !  =     0  )     {", "sb . append (  \"  ,     \"  )  ;", "}", "Type   resourceType    =    Type . values (  )  [ i ]  ;", "sb . append ( resourceType . name (  )  . toLowerCase (  )  )  ;", "sb . append ( String . format (  \"    weight =  %  .  1 f \"  ,    getWeight ( resourceType )  )  )  ;", "}", "sb . append (  \"  >  \"  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.ResourceWeights"}, {"methodBody": ["METHOD_START", "{", "ResourceWeights   rw 1     =    new   ResourceWeights (  )  ;", "Assert . assertEquals (  \" Default   CPU   weight   should   be    0  .  0 f .  \"  ,     0  .  0 F ,    rw 1  . getWeight ( ResourceType . CPU )  ,     1  .  0 E -  5 F )  ;", "Assert . assertEquals (  \" Default   memory   weight   should   be    0  .  0 f \"  ,     0  .  0 F ,    rw 1  . getWeight ( ResourceType . MEMORY )  ,     1  .  0 E -  5 F )  ;", "ResourceWeights   rw 2     =    new   ResourceWeights (  2  .  0 F )  ;", "Assert . assertEquals (  \" The   CPU   weight   should   be    2  .  0 f .  \"  ,     2  .  0 F ,    rw 2  . getWeight ( ResourceType . CPU )  ,     1  .  0 E -  5 F )  ;", "Assert . assertEquals (  \" The   memory   weight   should   be    2  .  0 f \"  ,     2  .  0 F ,    rw 2  . getWeight ( ResourceType . MEMORY )  ,     1  .  0 E -  5 F )  ;", "ResourceWeights   rw 3     =    new   ResourceWeights (  1  .  5 F ,     2  .  0 F )  ;", "Assert . assertEquals (  \" The   CPU   weight   should   be    2  .  0 f \"  ,     2  .  0 F ,    rw 3  . getWeight ( ResourceType . CPU )  ,     1  .  0 E -  5 F )  ;", "Assert . assertEquals (  \" The   memory   weight   should   be    1  .  5 f \"  ,     1  .  5 F ,    rw 3  . getWeight ( ResourceType . MEMORY )  ,     1  .  0 E -  5 F )  ;", "rw 3  . setWeight ( ResourceType . CPU ,     2  .  5 F )  ;", "Assert . assertEquals (  \" The   CPU   weight   should   be   set   to    2  .  5 f .  \"  ,     2  .  5 F ,    rw 3  . getWeight ( ResourceType . CPU )  ,     1  .  0 E -  5 F )  ;", "rw 3  . setWeight ( ResourceType . MEMORY ,     4  .  0 F )  ;", "Assert . assertEquals (  \" The   memory   weight   should   be   set   to    4  .  0 f .  \"  ,     4  .  0 F ,    rw 3  . getWeight ( ResourceType . MEMORY )  ,     1  .  0 E -  5 F )  ;", "}", "METHOD_END"], "methodName": ["testWeights"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.TestResourceWeights"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( createResource (  1  ,     1  )  ,    componentwiseMin ( createResource (  1  ,     1  )  ,    createResource (  2  ,     2  )  )  )  ;", "assertEquals ( createResource (  1  ,     1  )  ,    componentwiseMin ( createResource (  2  ,     2  )  ,    createResource (  1  ,     1  )  )  )  ;", "assertEquals ( createResource (  1  ,     1  )  ,    componentwiseMin ( createResource (  1  ,     2  )  ,    createResource (  2  ,     1  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testComponentwiseMin"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.TestResources"}, {"methodBody": ["METHOD_START", "{", "assertTrue ( fitsIn ( createResource (  1  ,     1  )  ,    createResource (  2  ,     2  )  )  )  ;", "assertTrue ( fitsIn ( createResource (  2  ,     2  )  ,    createResource (  2  ,     2  )  )  )  ;", "assertFalse ( fitsIn ( createResource (  2  ,     2  )  ,    createResource (  1  ,     1  )  )  )  ;", "assertFalse ( fitsIn ( createResource (  1  ,     2  )  ,    createResource (  2  ,     1  )  )  )  ;", "assertFalse ( fitsIn ( createResource (  2  ,     1  )  ,    createResource (  1  ,     2  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testFitsIn"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resource.TestResources"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Dispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "RMContext   context    =    new   RMContextImpl ( dispatcher ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )  ;", "dispatcher . register ( SchedulerEventType . class ,    new   InlineDispatcher . EmptyEventHandler (  )  )  ;", "dispatcher . register ( RMNodeEventType . class ,    new   ResourceManager . NodeEventDispatcher ( context )  )  ;", "NMLivelinessMonitor   nmLivelinessMonitor    =    new    . TestNmLivelinessMonitor ( dispatcher )  ;", "nmLivelinessMonitor . init ( conf )  ;", "nmLivelinessMonitor . start (  )  ;", "NodesListManager   nodesListManager    =    new   NodesListManager ( context )  ;", "nodesListManager . init ( conf )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "containerTokenSecretManager . start (  )  ;", "NMTokenSecretManagerInRM   nmTokenSecretManager    =    new   NMTokenSecretManagerInRM ( conf )  ;", "nmTokenSecretManager . start (  )  ;", "resourceTrackerService    =    new   ResourceTrackerService ( context ,    nodesListManager ,    nmLivelinessMonitor ,    containerTokenSecretManager ,    nmTokenSecretManager )  ;", "resourceTrackerService . init ( conf )  ;", "resourceTrackerService . start (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry"}, {"methodBody": ["METHOD_START", "{", "String   hostname 1     =     \" localhost 1  \"  ;", "String   hostname 2     =     \" localhost 2  \"  ;", "String   hostname 3     =     \" localhost 3  \"  ;", "Resource   capability    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "RegisterNodeManagerRequest   request 1     =     . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId 1     =    NodeId . newInstance ( hostname 1  ,     0  )  ;", "request 1  . setNodeId ( nodeId 1  )  ;", "request 1  . setHttpPort (  0  )  ;", "request 1  . setResource ( capability )  ;", "resourceTrackerService . registerNodeManager ( request 1  )  ;", "RegisterNodeManagerRequest   request 2     =     . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId 2     =    NodeId . newInstance ( hostname 2  ,     0  )  ;", "request 2  . setNodeId ( nodeId 2  )  ;", "request 2  . setHttpPort (  0  )  ;", "request 2  . setResource ( capability )  ;", "resourceTrackerService . registerNodeManager ( request 2  )  ;", "int   waitCount    =     0  ;", "while    (  (  ( ClusterMetrics . getMetrics (  )  . getNumLostNMs (  )  )     !  =     2  )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", "synchronized ( this )     {", "wait (  1  0  0  )  ;", "}", "}", "Assert . assertEquals (  2  ,    ClusterMetrics . getMetrics (  )  . getNumLostNMs (  )  )  ;", "request 3     =     . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId 3     =    NodeId . newInstance ( hostname 3  ,     0  )  ;", "request 3  . setNodeId ( nodeId 3  )  ;", "request 3  . setHttpPort (  0  )  ;", "request 3  . setResource ( capability )  ;", "resourceTrackerService . registerNodeManager ( request 3  )  ;", "stopT    =    false ;", "new    . ThirdNodeHeartBeatThread (  )  . start (  )  ;", "Assert . assertEquals (  2  ,    ClusterMetrics . getMetrics (  )  . getNumLostNMs (  )  )  ;", "stopT    =    true ;", "}", "METHOD_END"], "methodName": ["testNMExpiry"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMExpiry"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Dispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "dispatcher . register ( RMNodeEventType . class ,    new    . TestRMNodeEventDispatcher (  )  )  ;", "RMContext   context    =    new   RMContextImpl ( dispatcher ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )  ;", "dispatcher . register ( SchedulerEventType . class ,    new   InlineDispatcher . EmptyEventHandler (  )  )  ;", "dispatcher . register ( RMNodeEventType . class ,    new   ResourceManager . NodeEventDispatcher ( context )  )  ;", "NMLivelinessMonitor   nmLivelinessMonitor    =    new   NMLivelinessMonitor ( dispatcher )  ;", "nmLivelinessMonitor . init ( conf )  ;", "nmLivelinessMonitor . start (  )  ;", "NodesListManager   nodesListManager    =    new   NodesListManager ( context )  ;", "nodesListManager . init ( conf )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "containerTokenSecretManager . start (  )  ;", "NMTokenSecretManagerInRM   nmTokenSecretManager    =    new   NMTokenSecretManagerInRM ( conf )  ;", "nmTokenSecretManager . start (  )  ;", "resourceTrackerService    =    new   ResourceTrackerService ( context ,    nodesListManager ,    nmLivelinessMonitor ,    containerTokenSecretManager ,    nmTokenSecretManager )  ;", "resourceTrackerService . init ( conf )  ;", "resourceTrackerService . start (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMReconnect"}, {"methodBody": ["METHOD_START", "{", "String   hostname 1     =     \" localhost 1  \"  ;", "Resource   capability    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "RegisterNodeManagerRequest   request 1     =     . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "NodeId   nodeId 1     =    NodeId . newInstance ( hostname 1  ,     0  )  ;", "request 1  . setNodeId ( nodeId 1  )  ;", "request 1  . setHttpPort (  0  )  ;", "request 1  . setResource ( capability )  ;", "resourceTrackerService . registerNodeManager ( request 1  )  ;", "Assert . assertEquals ( RMNodeEventType . STARTED ,    rmNodeEvent . getType (  )  )  ;", "rmNodeEvent    =    null ;", "resourceTrackerService . registerNodeManager ( request 1  )  ;", "Assert . assertEquals ( RMNodeEventType . RECONNECTED ,    rmNodeEvent . getType (  )  )  ;", "rmNodeEvent    =    null ;", "resourceTrackerService . registerNodeManager ( request 1  )  ;", "capability    =    BuilderUtils . newResource (  1  0  2  4  ,     2  )  ;", "request 1  . setResource ( capability )  ;", "Assert . assertEquals ( RMNodeEventType . RECONNECTED ,    rmNodeEvent . getType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReconnect"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestNMReconnect"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Dispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "dispatcher . register ( SchedulerEventType . class ,    new   event . EventHandler < Event >  (  )     {", "@ Override", "public   void   handle ( Event   event )     {", "}", "}  )  ;", "RMContext   context    =    new   RMContextImpl ( dispatcher ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    null ,    null )  ;", "dispatcher . register ( RMNodeEventType . class ,    new   ResourceManager . NodeEventDispatcher ( context )  )  ;", "NodesListManager   nodesListManager    =    new   NodesListManager ( context )  ;", "nodesListManager . init ( conf )  ;", "context . getContainerTokenSecretManager (  )  . rollMasterKey (  )  ;", "context . getNMTokenSecretManager (  )  . rollMasterKey (  )  ;", "resourceTrackerService    =    new   ResourceTrackerService ( context ,    nodesListManager ,    new   NMLivelinessMonitor ( dispatcher )  ,    context . getContainerTokenSecretManager (  )  ,    context . getNMTokenSecretManager (  )  )  ;", "resourceTrackerService . init ( conf )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestRMNMRPCResponseId"}, {"methodBody": ["METHOD_START", "{", "String   node    =     \" localhost \"  ;", "Resource   capability    =    BuilderUtils . newResource (  1  0  2  4  ,     1  )  ;", "RegisterNodeManagerRequest   request    =     . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "nodeId    =    NodeId . newInstance ( node ,     1  2  3  4  )  ;", "request . setNodeId ( nodeId )  ;", "request . setHttpPort (  0  )  ;", "request . setResource ( capability )  ;", "RegisterNodeManagerRequest   request 1     =     . recordFactory . newRecordInstance ( RegisterNodeManagerRequest . class )  ;", "request 1  . setNodeId ( nodeId )  ;", "request 1  . setHttpPort (  0  )  ;", "request 1  . setResource ( capability )  ;", "resourceTrackerService . registerNodeManager ( request 1  )  ;", "NodeStatus   nodeStatus    =     . recordFactory . newRecordInstance ( NodeStatus . class )  ;", "nodeStatus . setNodeId ( nodeId )  ;", "NodeHealthStatus   nodeHealthStatus    =     . recordFactory . newRecordInstance ( NodeHealthStatus . class )  ;", "nodeHealthStatus . setIsNodeHealthy ( true )  ;", "nodeStatus . setNodeHealthStatus ( nodeHealthStatus )  ;", "NodeHeartbeatRequest   nodeHeartBeatRequest    =     . recordFactory . newRecordInstance ( NodeHeartbeatRequest . class )  ;", "nodeHeartBeatRequest . setNodeStatus ( nodeStatus )  ;", "nodeStatus . setResponseId (  0  )  ;", "NodeHeartbeatResponse   response    =    resourceTrackerService . nodeHeartbeat ( nodeHeartBeatRequest )  ;", "Assert . assertTrue (  (  ( response . getResponseId (  )  )     =  =     1  )  )  ;", "nodeStatus . setResponseId ( response . getResponseId (  )  )  ;", "response    =    resourceTrackerService . nodeHeartbeat ( nodeHeartBeatRequest )  ;", "Assert . assertTrue (  (  ( response . getResponseId (  )  )     =  =     2  )  )  ;", "response    =    resourceTrackerService . nodeHeartbeat ( nodeHeartBeatRequest )  ;", "Assert . assertTrue (  (  ( response . getResponseId (  )  )     =  =     2  )  )  ;", "nodeStatus . setResponseId (  0  )  ;", "response    =    resourceTrackerService . nodeHeartbeat ( nodeHeartBeatRequest )  ;", "Assert . assertTrue ( NodeAction . RESYNC . equals ( response . getNodeAction (  )  )  )  ;", "Assert . assertEquals (  \" Too   far   behind   rm   response   id :  2    nm   response   id :  0  \"  ,    response . getDiagnosticsMessage (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRPCResponseId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.resourcetracker.TestRMNMRPCResponseId"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  \" Not   supported   yet .  \"  )  ;", "}", "METHOD_END"], "methodName": ["getResourcePreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . attempt    =    attempt ;", "}", "METHOD_END"], "methodName": ["setCurrentAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . diagnostics    =    new   StringBuilder ( diag )  ;", "}", "METHOD_END"], "methodName": ["setDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . finish    =    time ;", "}", "METHOD_END"], "methodName": ["setFinishTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . maxAppAttempts    =    maxAppAttempts ;", "}", "METHOD_END"], "methodName": ["setNumMaxRetries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . queue    =    queue ;", "}", "METHOD_END"], "methodName": ["setQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . start    =    time ;", "}", "METHOD_END"], "methodName": ["setStartTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . state    =    state ;", "}", "METHOD_END"], "methodName": ["setState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . url    =    url ;", "}", "METHOD_END"], "methodName": ["setTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "this . user    =    user ;", "}", "METHOD_END"], "methodName": ["setUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.MockRMApp"}, {"methodBody": ["METHOD_START", "{", "return   this . appId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnostics ;", "}", "METHOD_END"], "methodName": ["getDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "return   transferStateFromPreviousAttempt ;", "}", "METHOD_END"], "methodName": ["getTransferStateFromPreviousAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnostics ;", "}", "METHOD_END"], "methodName": ["getDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFinishedAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "createNewAttempt (  )  ;", "handler . handle ( new   StartAttemptEvent ( currentAttempt . getAppAttemptId (  )  ,    transferStateFromPreviousAttempt )  )  ;", "}", "METHOD_END"], "methodName": ["createAndStartNewAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "switch    ( state )     {", "case   NEW    :", "case   NEW _ SAVING    :", "case   SUBMITTED    :", "case   ACCEPTED    :", "case   RUNNING    :", "case   FINAL _ SAVING    :", "case   KILLING    :", "return   FinallicationStatus . UNDEFINED ;", "case   FINISHING    :", "case   FINISHED    :", "case   FAILED    :", "return   FinallicationStatus . FAILED ;", "case   KILLED    :", "return   FinallicationStatus . KILLED ;", "}", "throw   new   YarnRuntimeException (  \" Unknown   state   passed !  \"  )  ;", "}", "METHOD_END"], "methodName": ["createFinalApplicationStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     (  ( attempts . size (  )  )     +     1  )  )  ;", "Attempt   attempt    =    new   AttemptImpl ( appAttemptId ,    rmContext ,    scheduler ,    masterService ,    submissionContext ,    conf ,     (  ( maxAppAttempts )     =  =     (  ( getNumFailedAppAttempts (  )  )     +     1  )  )  )  ;", "attempts . put ( appAttemptId ,    attempt )  ;", "currentAttempt    =    attempt ;", "}", "METHOD_END"], "methodName": ["createNewAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "String   msg    =    null ;", "RMAppFailedAttemptEvent   failedEvent    =     (  ( RMAppFailedAttemptEvent )     ( event )  )  ;", "if    ( this . submissionContext . getUndAM (  )  )     {", "msg    =     (  (  (  \" Und   application    \"     +     ( this . getApplicationId (  )  )  )     +     \"    failed   due   to    \"  )     +     ( failedEvent . getDiagnostics (  )  )  )     +     \"  .    Failing   the   application .  \"  ;", "} else", "if    (  ( getNumFailedAppAttempts (  )  )     >  =     ( this . maxAppAttempts )  )     {", "msg    =     (  (  (  (  (  \" Application    \"     +     ( this . getApplicationId (  )  )  )     +     \"    failed    \"  )     +     ( this . maxAppAttempts )  )     +     \"    times   due   to    \"  )     +     ( failedEvent . getDiagnostics (  )  )  )     +     \"  .    Failing   the   application .  \"  ;", "}", "return   msg ;", "}", "METHOD_END"], "methodName": ["getAppAttemptFailedDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "return    \" Application   killed   by   user .  \"  ;", "}", "METHOD_END"], "methodName": ["getAppKilledDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "int   completedAttempts    =     0  ;", "for    ( Attempt   attempt    :    attempts . values (  )  )     {", "if    ( attempt . shouldCountTowardsMaxAttemptRetry (  )  )     {", "completedAttempts +  +  ;", "}", "}", "return   completedAttempts ;", "}", "METHOD_END"], "methodName": ["getNumFailedAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . recoveredFinalState ;", "}", "METHOD_END"], "methodName": ["getRecoveredFinalState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "RMAppState   appState    =     (  ( RMAppImpl )     ( rmApp )  )  . getRecoveredFinalState (  )  ;", "if    ( appState    =  =    null )     {", "appState    =    rmApp . getState (  )  ;", "}", "return    (  ( appState    =  =     ( RMAppState . FAILED )  )     |  |     ( appState    =  =     ( RMAppState . FINISHED )  )  )     |  |     ( appState    =  =     ( RMAppState . KILLED )  )  ;", "}", "METHOD_END"], "methodName": ["isAppInFinalState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "NodeState   nodeState    =    node . getState (  )  ;", "updatedNodes . add ( node )  ;", ". LOG . debug (  (  (  (  (  (  \" Received   node   update   event :  \"     +    type )     +     \"    for   node :  \"  )     +    node )     +     \"    with   state :  \"  )     +    nodeState )  )  ;", "}", "METHOD_END"], "methodName": ["processNodeUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "for    ( RMAppAttempt   attempt    :    getAppAttempts (  )  . values (  )  )     {", "attempt . handle ( new   RMAppAttemptEvent ( attempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . RECOVER )  )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "transitionTodo    =    transitionToDo ;", "targetedFinalState    =    targetFinalState ;", "eventCausingFinalSaving    =    event ;", "}", "METHOD_END"], "methodName": ["rememberTargetTransitions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "rememberTargetTransitions ( event ,    transitionToDo ,    targetFinalState )  ;", "this . stateBeforeFinalSaving    =    getState (  )  ;", "this . storedFinishTime    =    System . currentTimeMillis (  )  ;", ". LOG . info (  (  (  (  \" Updating   application    \"     +     ( this . applicationId )  )     +     \"    with   final   state :     \"  )     +     ( this . targetedFinalState )  )  )  ;", "String   diags    =    null ;", "switch    ( event . getType (  )  )     {", "case   APP _ REJECTED    :", "RMAppRejectedEvent   rejectedEvent    =     (  ( RMAppRejectedEvent )     ( event )  )  ;", "diags    =    rejectedEvent . getMessage (  )  ;", "break ;", "case   ATTEMPT _ FINISHED    :", "RMAppFinishedAttemptEvent   finishedEvent    =     (  ( RMAppFinishedAttemptEvent )     ( event )  )  ;", "diags    =    finishedEvent . getDiagnostics (  )  ;", "break ;", "case   ATTEMPT _ FAILED    :", "RMAppFailedAttemptEvent   failedEvent    =     (  ( RMAppFailedAttemptEvent )     ( event )  )  ;", "diags    =    getAppAttemptFailedDiagnostics ( failedEvent )  ;", "break ;", "case   ATTEMPT _ KILLED    :", "diags    =     . getAppKilledDiagnostics (  )  ;", "break ;", "default    :", "break ;", "}", "RMStateStore . ApplicationState   appState    =    new   RMStateStore . ApplicationState ( this . submitTime ,    this . startTime ,    this . submissionContext ,    this . user ,    stateToBeStored ,    diags ,    this . storedFinishTime )  ;", "this . rmContext . getStateStore (  )  . updateApplicationState ( appState )  ;", "}", "METHOD_END"], "methodName": ["rememberTargetTransitionsAndStoreState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"}, {"methodBody": ["METHOD_START", "{", "return   numAMContainersPreempted ;", "}", "METHOD_END"], "methodName": ["getNumAMContainersPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numNonAMContainersPreempted ;", "}", "METHOD_END"], "methodName": ["getNumNonAMContainersPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMetrics"}, {"methodBody": ["METHOD_START", "{", "return   resourcePreempted ;", "}", "METHOD_END"], "methodName": ["getResourcePreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMetrics"}, {"methodBody": ["METHOD_START", "{", "return   result ;", "}", "METHOD_END"], "methodName": ["getResult"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMoveEvent"}, {"methodBody": ["METHOD_START", "{", "return   targetQueue ;", "}", "METHOD_END"], "methodName": ["getTargetQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMoveEvent"}, {"methodBody": ["METHOD_START", "{", "return   node ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppNodeUpdateEvent"}, {"methodBody": ["METHOD_START", "{", "return   updateType ;", "}", "METHOD_END"], "methodName": ["getUpdateType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppNodeUpdateEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . message ;", "}", "METHOD_END"], "methodName": ["getMessage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent"}, {"methodBody": ["METHOD_START", "{", "return   node ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent"}, {"methodBody": ["METHOD_START", "{", "verify ( store ,    times (  0  )  )  . updateApplicationState ( any ( RMStateStore . ApplicationState . class )  )  ;", "}", "METHOD_END"], "methodName": ["assertAppFinalStateNotSaved"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "verify ( store ,    times (  1  )  )  . updateApplicationState ( any ( RMStateStore . ApplicationState . class )  )  ;", "}", "METHOD_END"], "methodName": ["assertAppFinalStateSaved"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals (  (  \" application   state   should   have   been    \"     +    state )  ,    state ,    application . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "assertTimesAtFinish ( application )  ;", ". assertAppState ( RMAppState . FAILED ,    application )  ;", ". assertFinalAppStatus ( FAILED ,    application )  ;", "StringBuilder   diag    =    application . getDiagnostics (  )  ;", "Assert . assertTrue (  \" application   diagnostics   is   not   correct \"  ,    diag . toString (  )  . matches ( regex )  )  ;", "}", "METHOD_END"], "methodName": ["assertFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals (  (  \" Final   application   status   should   have   been    \"     +    status )  ,    status ,    application . getFinalApplicationStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertFinalAppStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "assertTimesAtFinish ( application )  ;", ". assertAppState ( RMAppState . KILLED ,    application )  ;", ". assertFinalAppStatus ( KILLED ,    application )  ;", "StringBuilder   diag    =    application . getDiagnostics (  )  ;", "Assert . assertEquals (  \" application   diagnostics   is   not   correct \"  ,     \" Application   killed   by   user .  \"  ,    diag . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["assertKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "Assert . assertTrue (  \" application   start   time   is   not   greater   then    0  \"  ,     (  ( application . getStartTime (  )  )     >     0  )  )  ;", "Assert . assertTrue (  \" application   start   time   is   before   currentTime \"  ,     (  ( application . getStartTime (  )  )     <  =     ( System . currentTimeMillis (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["assertStartTimeSet"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . assertStartTimeSet ( application )  ;", "Assert . assertTrue (  \" application   finish   time   is   not   greater   then    0  \"  ,     (  ( application . getFinishTime (  )  )     >     0  )  )  ;", "Assert . assertTrue (  \" application   finish   time   is   not    >  =    then   start   time \"  ,     (  ( application . getFinishTime (  )  )     >  =     ( application . getStartTime (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["assertTimesAtFinish"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    MockApps . newAppID (  (  ( TestRMAppTransitions . appId )  +  +  )  )  ;", "String   user    =    MockApps . newUserName (  )  ;", "String   name    =    MockApps . newAppName (  )  ;", "String   queue    =    MockApps . newQueue (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    TestRMAppTransitions . maxAppAttempts )  ;", "scheduler    =    mock ( YarnScheduler . class )  ;", "ApplicationMasterService   masterService    =    new   ApplicationMasterService ( rmContext ,    scheduler )  ;", "if    ( submissionContext    =  =    null )     {", "submissionContext    =    new   ApplicationSubmissionContextPBImpl (  )  ;", "}", "submissionContext . setApplicationId ( applicationId )  ;", "RMApp   application    =    new   RMAppImpl ( applicationId ,    rmContext ,    conf ,    name ,    user ,    queue ,    submissionContext ,    scheduler ,    masterService ,    System . currentTimeMillis (  )  ,     \" YARN \"  ,    null )  ;", "TestRMAppTransitions . testAppStartState ( applicationId ,    user ,    name ,    queue ,    application )  ;", "this . rmContext . getRMApps (  )  . putIfAbsent ( application . getApplicationId (  )  ,    application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["createNewTestApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    createNewTestApp ( null )  ;", "RMStateStore . ApplicationState   appState    =    new   RMStateStore . ApplicationState ( app . getSubmitTime (  )  ,    app . getStartTime (  )  ,    app . getApplicationSubmissionContext (  )  ,    app . getUser (  )  ,    rmAppState ,    null ,    app . getFinishTime (  )  )  ;", "applicationState . put ( app . getApplicationId (  )  ,    appState )  ;", "}", "METHOD_END"], "methodName": ["createRMStateForApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {    Boolean . FALSE    }  ,    new   Object [  ]  {    Boolean . TRUE    }     }  )  ;", "}", "METHOD_END"], "methodName": ["getTestParameters"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ UPDATE _ SAVED )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "}", "METHOD_END"], "methodName": ["sendAppUpdateSavedEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "application . getCurrentAppAttempt (  )  . handle ( new   RMAppAttemptEvent ( application . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,    RMAppAttemptEventType . ATTEMPT _ UPDATE _ SAVED )  )  ;", "}", "METHOD_END"], "methodName": ["sendAttemptUpdateSavedEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   YarnConfiguration (  )  ;", "AuthenticationMethod   authMethod    =    AuthenticationMethod . SIMPLE ;", "if    ( isSecurityEnabled )     {", "authMethod    =    AuthenticationMethod . KERBEROS ;", "}", "SecurityUtil . setAuthenticationMethod ( authMethod ,    conf )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "rmDispatcher    =    new   DrainDispatcher (  )  ;", "ContainerAllocationExpirer   containerAllocationExpirer    =    mock ( ContainerAllocationExpirer . class )  ;", "AMLivelinessMonitor   amLivelinessMonitor    =    mock ( AMLivelinessMonitor . class )  ;", "AMLivelinessMonitor   amFinishingMonitor    =    mock ( AMLivelinessMonitor . class )  ;", "store    =    mock ( RMStateStore . class )  ;", "writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "this . rmContext    =    new   RMContextImpl ( rmDispatcher ,    containerAllocationExpirer ,    amLivelinessMonitor ,    amFinishingMonitor ,    null ,    new   AMRMTokenSecretManager ( conf ,    this . rmContext )  ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    writer )  ;", "(  ( RMContextImpl )     ( rmContext )  )  . setStateStore ( store )  ;", "rmDispatcher . register ( RMAppAttemptEventType . class ,    new    . TestApplicationAttemptEventDispatcher ( this . rmContext )  )  ;", "rmDispatcher . register ( RMAppEventType . class ,    new    . TestApplicationEventDispatcher ( rmContext )  )  ;", "rmDispatcher . register ( RMAppManagerEventType . class ,    new    . TestApplicationManagerEventDispatcher (  )  )  ;", "schedulerDispatcher    =    new    . TestSchedulerEventDispatcher (  )  ;", "rmDispatcher . register ( SchedulerEventType . class ,    schedulerDispatcher )  ;", "rmDispatcher . init ( conf )  ;", "rmDispatcher . start (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppAcceptedFailed    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppAccepted ( null )  ;", "Assert . assertTrue (  (  ( TestRMAppTransitions . maxAppAttempts )     >     1  )  )  ;", "for    ( int   i    =     1  ;    i    <     ( TestRMAppTransitions . maxAppAttempts )  ;    i +  +  )     {", "RMAppEvent   event    =    new   RMAppFailedAttemptEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ FAILED ,     \"  \"  ,    false )  ;", "application . handle ( event )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . ACCEPTED ,    application )  ;", "event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ ACCEPTED )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . ACCEPTED ,    application )  ;", "}", "String   message    =     \" Test   fail \"  ;", "RMAppEvent   event    =    new   RMAppFailedAttemptEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ FAILED ,    message ,    false )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertFailed ( application ,     (  (  \"  .  *  \"     +    message )     +     \"  .  * Failing   the   application .  *  \"  )  )  ;", "assertAppFinalStateSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppAcceptedFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppAcceptedKill    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppAccepted ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLING ,    application )  ;", "RMAppEvent   appAttemptKilled    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ KILLED )  ;", "application . handle ( appAttemptKilled )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FINAL _ SAVING ,    application )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertKilled ( application )  ;", "assertAppFinalStateSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . KILLED )  ;", "verifyAppRemovedSchedulerEvent ( RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testAppAcceptedKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppAtFinishingIgnoreKill    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppFinishing ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FINISHING ,    application )  ;", "}", "METHOD_END"], "methodName": ["testAppAtFinishingIgnoreKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppFailedFailed    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppNewSaving ( null )  ;", "RMAppEvent   event    =    new   RMAppRejectedEvent ( application . getApplicationId (  )  ,     \"  \"  )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FAILED ,    application )  ;", "event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FAILED ,    application )  ;", "verifyApplicationFinished ( RMAppState . FAILED )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FAILED ,    application )  ;", "}", "METHOD_END"], "methodName": ["testAppFailedFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppFinalSavingToFinished    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppFinalSaving ( null )  ;", "final   String   diagMsg    =     \" some   diagnostics \"  ;", "RMAppEvent   event    =    new   RMAppFinishedAttemptEvent ( application . getApplicationId (  )  ,    diagMsg )  ;", "application . handle ( event )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FINAL _ SAVING ,    application )  ;", "RMAppEvent   appUpdated    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ UPDATE _ SAVED )  ;", "application . handle ( appUpdated )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FINISHED ,    application )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertFinalAppStatus ( FAILED ,    application )  ;", "Assert . assertTrue (  \" Finished   app   missing   diagnostics \"  ,     (  ( application . getDiagnostics (  )  . indexOf ( diagMsg )  )     !  =     (  -  1  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppFinalSavingToFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppFinishedFinished    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppFinished ( null ,     \"  \"  )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . FINISHED ,    application )  ;", "StringBuilder   diag    =    application . getDiagnostics (  )  ;", "Assert . assertEquals (  \" application   diagnostics   is   not   correct \"  ,     \"  \"  ,    diag . toString (  )  )  ;", "verifyApplicationFinished ( RMAppState . FINISHED )  ;", "}", "METHOD_END"], "methodName": ["testAppFinishedFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppKilledKilled    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppRunning ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAttemptUpdateSavedEvent ( application )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLED ,    application )  ;", "event    =    new   RMAppFinishedAttemptEvent ( application . getApplicationId (  )  ,     \"  \"  )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLED ,    application )  ;", "event    =    new   RMAppFailedAttemptEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ FAILED ,     \"  \"  ,    false )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLED ,    application )  ;", "event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLED ,    application )  ;", "verifyApplicationFinished ( RMAppState . KILLED )  ;", "assertTimesAtFinish ( application )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLED ,    application )  ;", "}", "METHOD_END"], "methodName": ["testAppKilledKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppNewKill    -  -  -  \"  )  ;", "RMApp   application    =    createNewTestApp ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertKilled ( application )  ;", "assertAppFinalStateNotSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . KILLED )  ;", "verifyAppRemovedSchedulerEvent ( RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testAppNewKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppNewReject    -  -  -  \"  )  ;", "RMApp   application    =    createNewTestApp ( null )  ;", "String   rejectedText    =     \" Test   Application   Rejected \"  ;", "RMAppEvent   event    =    new   RMAppRejectedEvent ( application . getApplicationId (  )  ,    rejectedText )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertFailed ( application ,    rejectedText )  ;", "assertAppFinalStateNotSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppNewReject"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppNewSavingKill    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppNewSaving ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertKilled ( application )  ;", "verifyApplicationFinished ( RMAppState . KILLED )  ;", "verifyAppRemovedSchedulerEvent ( RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testAppNewSavingKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppNewSavingReject    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppNewSaving ( null )  ;", "String   rejectedText    =     \" Test   Application   Rejected \"  ;", "RMAppEvent   event    =    new   RMAppRejectedEvent ( application . getApplicationId (  )  ,    rejectedText )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertFailed ( application ,    rejectedText )  ;", "assertAppFinalStateSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppNewSavingReject"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppRecoverPath    -  -  -  \"  )  ;", "testCreateAppSubmittedRecovery ( null )  ;", "}", "METHOD_END"], "methodName": ["testAppRecoverPath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppRunningFailed    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppRunning ( null )  ;", "RMAppAttempt   appAttempt    =    application . getCurrentAppAttempt (  )  ;", "int   expectedAttemptId    =     1  ;", "Assert . assertEquals ( expectedAttemptId ,    appAttempt . getAppAttemptId (  )  . getAttemptId (  )  )  ;", "Assert . assertTrue (  (  ( TestRMAppTransitions . maxAppAttempts )     >     1  )  )  ;", "for    ( int   i    =     1  ;    i    <     ( TestRMAppTransitions . maxAppAttempts )  ;    i +  +  )     {", "RMAppEvent   event    =    new   RMAppFailedAttemptEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ FAILED ,     \"  \"  ,    false )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . ACCEPTED ,    application )  ;", "appAttempt    =    application . getCurrentAppAttempt (  )  ;", "Assert . assertEquals (  (  +  + expectedAttemptId )  ,    appAttempt . getAppAttemptId (  )  . getAttemptId (  )  )  ;", "event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ ACCEPTED )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . ACCEPTED ,    application )  ;", "event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ REGISTERED )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . RUNNING ,    application )  ;", "}", "RMAppEvent   event    =    new   RMAppFailedAttemptEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ FAILED ,     \"  \"  ,    false )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertFailed ( application ,     \"  .  * Failing   the   application .  *  \"  )  ;", "assertAppFinalStateSaved ( application )  ;", "event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "assertFailed ( application ,     \"  .  * Failing   the   application .  *  \"  )  ;", "assertAppFinalStateSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppRunningFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppRunningKill    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppRunning ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLING ,    application )  ;", "RMAppEvent   finishEvent    =    new   RMAppFinishedAttemptEvent ( application . getApplicationId (  )  ,    null )  ;", "application . handle ( finishEvent )  ;", "TestRMAppTransitions . assertAppState ( RMAppState . KILLING ,    application )  ;", "sendAttemptUpdateSavedEvent ( application )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertKilled ( application )  ;", "verifyApplicationFinished ( RMAppState . KILLED )  ;", "verifyAppRemovedSchedulerEvent ( RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testAppRunningKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "Assert . assertTrue (  \" application   start   time   is   not   greater   then    0  \"  ,     (  ( application . getStartTime (  )  )     >     0  )  )  ;", "Assert . assertTrue (  \" application   start   time   is   before   currentTime \"  ,     (  ( application . getStartTime (  )  )     <  =     ( System . currentTimeMillis (  )  )  )  )  ;", "Assert . assertEquals (  \" application   user   is   not   correct \"  ,    user ,    application . getUser (  )  )  ;", "Assert . assertEquals (  \" application   id   is   not   correct \"  ,    applicationId ,    application . getApplicationId (  )  )  ;", "Assert . assertEquals (  \" application   progress   is   not   correct \"  ,     (  ( float )     (  0  .  0  )  )  ,    application . getProgress (  )  ,     (  ( float )     (  0  .  0  )  )  )  ;", "Assert . assertEquals (  \" application   queue   is   not   correct \"  ,    queue ,    application . getQueue (  )  )  ;", "Assert . assertEquals (  \" application   name   is   not   correct \"  ,    name ,    application . getName (  )  )  ;", "Assert . assertEquals (  \" application   finish   time   is   not    0    and   should   be \"  ,     0  ,    application . getFinishTime (  )  )  ;", "Assert . assertEquals (  \" application   tracking   url   is   not   correct \"  ,    null ,    application . getTrackingUrl (  )  )  ;", "StringBuilder   diag    =    application . getDiagnostics (  )  ;", "Assert . assertEquals (  \" application   diagnostics   is   not   correct \"  ,     0  ,    diag . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppStartState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppSubmittedKill -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppSubmittedNoRecovery ( null )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . KILL )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertKilled ( application )  ;", "assertAppFinalStateSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . KILLED )  ;", "verifyAppRemovedSchedulerEvent ( RMAppState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testAppSubmittedKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppSubmittedRejected    -  -  -  \"  )  ;", "RMApp   application    =    testCreateAppSubmittedNoRecovery ( null )  ;", "String   rejectedText    =     \" app   rejected \"  ;", "RMAppEvent   event    =    new   RMAppRejectedEvent ( application . getApplicationId (  )  ,    rejectedText )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertFailed ( application ,    rejectedText )  ;", "assertAppFinalStateSaved ( application )  ;", "verifyApplicationFinished ( RMAppState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppSubmittedRejected"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "TestRMAppTransitions . LOG . info (  \"  -  -  -    START :    testAppSuccessPath    -  -  -  \"  )  ;", "final   String   diagMsg    =     \" some   diagnostics \"  ;", "RMApp   application    =    testCreateAppFinished ( null ,    diagMsg )  ;", "Assert . assertTrue (  \" Finished   application   missing   diagnostics \"  ,     (  ( application . getDiagnostics (  )  . indexOf ( diagMsg )  )     !  =     (  -  1  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppSuccessPath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMStateStore . RMState   state    =    new   RMStateStore . RMState (  )  ;", "Map < ApplicationId ,    RMStateStore . ApplicationState >    applicationState    =    state . getApplicationState (  )  ;", "createRMStateForApplications ( applicationState ,    State . FINISHED )  ;", "createRMStateForApplications ( applicationState ,    State . KILLED )  ;", "createRMStateForApplications ( applicationState ,    State . FAILED )  ;", "for    ( RMStateStore . ApplicationState   appState    :    applicationState . values (  )  )     {", "testRecoverApplication ( appState ,    state )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppsRecoveringStates"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    testCreateAppSubmittedNoRecovery ( submissionContext )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ ACCEPTED )  ;", "application . handle ( event )  ;", ". assertStartTimeSet ( application )  ;", ". assertAppState ( RMAppState . ACCEPTED ,    application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppAccepted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    testCreateAppRunning ( submissionContext )  ;", "RMAppEvent   finishingEvent    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ UNREGISTERED )  ;", "application . handle ( finishingEvent )  ;", ". assertAppState ( RMAppState . FINAL _ SAVING ,    application )  ;", "assertAppFinalStateSaved ( application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppFinalSaving"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    null ;", "if    (  ( submissionContext    !  =    null )     &  &     ( submissionContext . getUnmanagedAM (  )  )  )     {", "application    =    testCreateAppRunning ( submissionContext )  ;", "} else    {", "application    =    testCreateAppFinishing ( submissionContext )  ;", "}", "RMAppEvent   finishedEvent    =    new   RMAppFinishedAttemptEvent ( application . getApplicationId (  )  ,    diagnostics )  ;", "application . handle ( finishedEvent )  ;", ". assertAppState ( RMAppState . FINISHED ,    application )  ;", "assertTimesAtFinish ( application )  ;", ". assertFinalAppStatus ( FAILED ,    application )  ;", "Assert . assertTrue (  \" Finished   app   missing   diagnostics \"  ,     (  ( application . getDiagnostics (  )  . indexOf ( diagnostics )  )     !  =     (  -  1  )  )  )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "assert    ( submissionContext    =  =    null )     |  |     (  !  ( submissionContext . getUnmanagedAM (  )  )  )  ;", "RMApp   application    =    testCreateAppFinalSaving ( submissionContext )  ;", "RMAppEvent   appUpdated    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ UPDATE _ SAVED )  ;", "application . handle ( appUpdated )  ;", ". assertAppState ( RMAppState . FINISHING ,    application )  ;", "assertTimesAtFinish ( application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppFinishing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    createNewTestApp ( submissionContext )  ;", "verify ( writer )  . applicationStarted ( any ( RMApp . class )  )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . START )  ;", "application . handle ( event )  ;", ". assertStartTimeSet ( application )  ;", ". assertAppState ( RMAppState . NEW _ SAVING ,    application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppNewSaving"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    testCreateAppAccepted ( submissionContext )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ REGISTERED )  ;", "application . handle ( event )  ;", ". assertStartTimeSet ( application )  ;", ". assertAppState ( RMAppState . RUNNING ,    application )  ;", ". assertFinalAppStatus ( UNDEFINED ,    application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    testCreateAppNewSaving ( submissionContext )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . APP _ NEW _ SAVED )  ;", "application . handle ( event )  ;", ". assertStartTimeSet ( application )  ;", ". assertAppState ( RMAppState . SUBMITTED ,    application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppSubmittedNoRecovery"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   application    =    createNewTestApp ( submissionContext )  ;", "RMAppEvent   event    =    new   RMAppEvent ( application . getApplicationId (  )  ,    RMAppEventType . RECOVER )  ;", "application . handle ( event )  ;", ". assertStartTimeSet ( application )  ;", ". assertAppState ( RMAppState . SUBMITTED ,    application )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["testCreateAppSubmittedRecovery"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    createNewTestApp ( null )  ;", ". assertAppState ( RMAppState . NEW ,    app )  ;", "ApplicationReport   report    =    app . createAndGetApplicationReport ( null ,    true )  ;", "Assert . assertNotNull ( report . getApplicationResourceUsageReport (  )  )  ;", "Assert . assertEquals ( report . getApplicationResourceUsageReport (  )  ,    RMServerUtils . DUMMY _ APPLICATION _ RESOURCE _ USAGE _ REPORT )  ;", "report    =    app . createAndGetApplicationReport (  \" clientuser \"  ,    true )  ;", "Assert . assertNotNull ( report . getApplicationResourceUsageReport (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetAppReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   submissionContext    =    appState . getApplicationSubmissionContext (  )  ;", "Impl   application    =    new   Impl ( appState . getAppId (  )  ,    rmContext ,    conf ,    submissionContext . getApplicationName (  )  ,    null ,    submissionContext . getQueue (  )  ,    submissionContext ,    null ,    null ,    appState . getSubmitTime (  )  ,    submissionContext . getApplicationType (  )  ,    submissionContext . getApplicationTags (  )  )  ;", "Assert . assertEquals ( State . NEW ,    application . getState (  )  )  ;", "application . recover ( rmState )  ;", "Assert . assertTrue (  \" Application   is   not   in   recoveredFinalStatus .  \"  ,    Impl . isAppInFinalState ( application )  )  ;", "application . handle ( new   Event ( appState . getAppId (  )  ,    EventType . RECOVER )  )  ;", "rmDispatcher . await (  )  ;", "State   finalState    =    appState . getState (  )  ;", "Assert . assertEquals (  \" Application   is   not   in   finalState .  \"  ,    finalState ,    application . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRecoverApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   subContext    =    new   ApplicationSubmissionContextPBImpl (  )  ;", "subContext . setUnmanagedAM ( true )  ;", ". LOG . info (  \"  -  -  -    START :    testUnmanagedAppSuccessPath    -  -  -  \"  )  ;", "final   String   diagMsg    =     \" some   diagnostics \"  ;", "RMApp   application    =    testCreateAppFinished ( subContext ,    diagMsg )  ;", "Assert . assertTrue (  \" Finished   app   missing   diagnostics \"  ,     (  ( application . getDiagnostics (  )  . indexOf ( diagMsg )  )     !  =     (  -  1  )  )  )  ;", "reset ( writer )  ;", ". LOG . info (  \"  -  -  -    START :    testUnmanagedAppFailPath    -  -  -  \"  )  ;", "application    =    testCreateAppRunning ( subContext )  ;", "RMAppEvent   event    =    new   RMAppFailedAttemptEvent ( application . getApplicationId (  )  ,    RMAppEventType . ATTEMPT _ FAILED ,     \"  \"  ,    false )  ;", "application . handle ( event )  ;", "rmDispatcher . await (  )  ;", "RMAppAttempt   appAttempt    =    application . getCurrentAppAttempt (  )  ;", "Assert . assertEquals (  1  ,    appAttempt . getAppAttemptId (  )  . getAttemptId (  )  )  ;", "sendAppUpdateSavedEvent ( application )  ;", "assertFailed ( application ,     \"  .  * Unmanaged   application .  * Failing   the   application .  *  \"  )  ;", "assertAppFinalStateSaved ( application )  ;", "}", "METHOD_END"], "methodName": ["testUnmanagedApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( SchedulerEventType . APP _ REMOVED ,    schedulerDispatcher . lastSchedulerEvent . getType (  )  )  ;", "if    (  ( schedulerDispatcher . lastSchedulerEvent )    instanceof   AppRemovedSchedulerEvent )     {", "AppRemovedSchedulerEvent   appRemovedEvent    =     (  ( AppRemovedSchedulerEvent )     ( schedulerDispatcher . lastSchedulerEvent )  )  ;", "Assert . assertEquals ( finalState ,    appRemovedEvent . getFinalState (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyAppRemovedSchedulerEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "ArgumentCaptor < RMAppState >    finalState    =    ArgumentCaptor . forClass ( RMAppState . class )  ;", "verify ( writer )  . applicationFinished ( any ( RMApp . class )  ,    finalState . capture (  )  )  ;", "Assert . assertEquals ( state ,    finalState . getValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyApplicationFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.TestRMAppTransitions"}, {"methodBody": ["METHOD_START", "{", "super . serviceInit ( conf )  ;", "int   expireIntvl    =    conf . getInt ( RM _ AM _ EXPIRY _ INTERVAL _ MS ,    DEFAULT _ RM _ AM _ EXPIRY _ INTERVAL _ MS )  ;", "setExpireInterval ( expireIntvl )  ;", "setInterval (  ( expireIntvl    /     3  )  )  ;", "}", "METHOD_END"], "methodName": ["serviceInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "return   this . appAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "rmContext . getAMLivelinessMonitor (  )  . register ( getAppAttemptId (  )  )  ;", "}", "METHOD_END"], "methodName": ["attemptLaunched"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "final   String   scheme    =    WebAppUtils . getHttpSchemePrefix ( conf )  ;", "URI   trackingUri    =     ( StringUtils . isEmpty ( trackingUriWithoutScheme )  )     ?    null    :    ProxyUriUtils . getUriFromAMUrl ( scheme ,    trackingUriWithoutScheme )  ;", "String   proxy    =    WebAppUtils . getProxyHostAndPort ( conf )  ;", "URI   proxyUri    =    ProxyUriUtils . getUriFromAMUrl ( scheme ,    proxy )  ;", "URI   result    =    ProxyUriUtils . getProxyUri ( trackingUri ,    proxyUri ,    applicationAttemptId . getApplicationId (  )  )  ;", "return   result . toASCIIString (  )  ;", "}    catch    ( URISyntaxException   e )     {", ". LOG . warn (  (  \" Could   not   proxify    \"     +    trackingUriWithoutScheme )  ,    e )  ;", "return   trackingUriWithoutScheme ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["generateProxyUriWithScheme"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   status    =    finishEvent . getContainerStatus (  )  ;", "StringBuilder   diagnosticsBuilder    =    new   StringBuilder (  )  ;", "diagnosticsBuilder . append (  \" AM   Container   for    \"  )  . append ( finishEvent . getApplicationd (  )  )  . append (  \"    exited   with    \"  )  . append (  \"    exitCode :     \"  )  . append ( status . getExitStatus (  )  )  . append (  \"  \\ n \"  )  ;", "if    (  ( this . getTrackingUrl (  )  )     !  =    null )     {", "diagnosticsBuilder . append (  \" For   more   detailed   output ,  \"  )  . append (  \"    check   application   tracking   page :  \"  )  . append ( this . getTrackingUrl (  )  )  . append (  \" Then ,    click   on   links   to   logs   of   each   attempt .  \\ n \"  )  ;", "}", "diagnosticsBuilder . append (  \" Diagnostics :     \"  )  . append ( status . getDiagnostics (  )  )  . append (  \" Failing   this   attempt \"  )  ;", "return   diagnosticsBuilder . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getAMContainerCrashedDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return   this . amContainerExitStatus ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getAMContainerExitStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "String   diag    =     (  \" ApplicationMaster   for   attempt    \"     +     ( event . getApplicationAttemptId (  )  )  )     +     \"    timed   out \"  ;", "return   diag ;", "}", "METHOD_END"], "methodName": ["getAMExpiredDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "return    \" Unmanaged   AM   must   register   after   AM   attempt   reaches   LAUNCHED   state .  \"  ;", "}", "METHOD_END"], "methodName": ["getUnexpectedAMRegisteredDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "this . host    =     \" N / A \"  ;", "this . rpcPort    =     -  1  ;", "}", "METHOD_END"], "methodName": ["invalidateAMHostAndPort"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "eventHandler . handle ( new   AMLauncherEvent ( AMLauncherEventType . LAUNCH ,    this )  )  ;", "}", "METHOD_END"], "methodName": ["launchAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "return   maybeLastAttempt ;", "}", "METHOD_END"], "methodName": ["mayBeLastAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  ( appAttemptTokens    =  =    null )     |  |     ( state    =  =     ( RMAppAttemptState . FAILED )  )  )     |  |     ( state    =  =     ( RMAppAttemptState . FINISHED )  )  )     |  |     ( state    =  =     ( RMAppAttemptState . KILLED )  )  )     {", "return ;", "}", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "byte [  ]    clientTokenMasterKeyBytes    =    appAttemptTokens . getSecretKey ( RMStateStore . AM _ CLIENT _ TOKEN _ MASTER _ KEY _ NAME )  ;", "clientTokenMasterKey    =    rmContext . getClientToAMTokenSecretManager (  )  . registerMasterKey ( applicationAttemptId ,    clientTokenMasterKeyBytes )  ;", "}", "this . amrmToken    =    rmContext . getAMRMTokenSecretManager (  )  . createAndGetAMRMToken ( applicationAttemptId )  ;", "}", "METHOD_END"], "methodName": ["recoverAppAttemptCredentials"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "transitionTodo    =    transitionToDo ;", "targetedFinalState    =    targetFinalState ;", "eventCausingFinalSaving    =    event ;", "}", "METHOD_END"], "methodName": ["rememberTargetTransitions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "rememberTargetTransitions ( event ,    transitionToDo ,    targetFinalState )  ;", "stateBeforeFinalSaving    =    getState (  )  ;", "String   diags    =    null ;", "String   finalTrackingUrl    =    null ;", "FinalApplicationStatus   finalStatus    =    null ;", "int   exitStatus    =    ContainerExitStatus . INVALID ;", "switch    ( event . getType (  )  )     {", "case   LAUNCH _ FAILED    :", "RMAppAttemptLaunchFailedEvent   launchFaileEvent    =     (  ( RMAppAttemptLaunchFailedEvent )     ( event )  )  ;", "diags    =    launchFaileEvent . getMessage (  )  ;", "break ;", "case   REGISTERED    :", "diags    =     . getUnexpectedAMRegisteredDiagnostics (  )  ;", "break ;", "case   UNREGISTERED    :", "RMAppAttemptUnregistrationEvent   unregisterEvent    =     (  ( RMAppAttemptUnregistrationEvent )     ( event )  )  ;", "diags    =    unregisterEvent . getDiagnostics (  )  ;", "finalTrackingUrl    =     . sanitizeTrackingUrl ( unregisterEvent . getFinalTrackingUrl (  )  )  ;", "finalStatus    =    unregisterEvent . getFinalApplicationStatus (  )  ;", "break ;", "case   CONTAINER _ FINISHED    :", "RMAppAttemptContainerFinishedEvent   finishEvent    =     (  ( RMAppAttemptContainerFinishedEvent )     ( event )  )  ;", "diags    =    getAMContainerCrashedDiagnostics ( finishEvent )  ;", "exitStatus    =    finishEvent . getContainerStatus (  )  . getExitStatus (  )  ;", "break ;", "case   KILL    :", "break ;", "case   EXPIRE    :", "diags    =     . getAMExpiredDiagnostics ( event )  ;", "break ;", "default    :", "break ;", "}", "RMStateStore   rmStore    =    rmContext . getStateStore (  )  ;", "RMStateStore . ApplicationAttemptState   attemptState    =    new   RMStateStore . ApplicationAttemptState ( applicationAttemptId ,    getMasterContainer (  )  ,    rmStore . getCredentialsFromAppAttempt ( this )  ,    startTime ,    stateToBeStored ,    finalTrackingUrl ,    diags ,    finalStatus ,    exitStatus )  ;", ". LOG . info (  (  (  (  (  (  \" Updating   application   attempt    \"     +     ( applicationAttemptId )  )     +     \"    with   final   state :     \"  )     +     ( targetedFinalState )  )     +     \"  ,    and   exit   status :     \"  )     +    exitStatus )  )  ;", "rmStore . updateApplicationAttemptState ( attemptState )  ;", "}", "METHOD_END"], "methodName": ["rememberTargetTransitionsAndStoreState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "appAttempt . rmContext . getClientToAMTokenSecretManager (  )  . unRegisterApplication ( appAttempt . getd (  )  )  ;", "}", "appAttempt . rmContext . getAMRMTokenSecretManager (  )  . applicationMasterFinished ( appAttempt . getd (  )  )  ;", "}", "METHOD_END"], "methodName": ["removeCredentials"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "try    {", "Thread . sleep (  5  0  0  )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . warn (  (  \" Interrupted   while   waiting   to   resend   the \"     +     \"    ContainerAllocated   Event .  \"  )  )  ;", "}", "appAttempt . eventHandler . handle ( new   RMAppAttemptContainerAllocatedEvent ( appAttempt . applicationAttemptId )  )  ;", "}", "}  . start (  )  ;", "}", "METHOD_END"], "methodName": ["retryFetchingAMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "return    ( url    =  =    null )     |  |     ( url . trim (  )  . isEmpty (  )  )     ?     \" N / A \"     :    url ;", "}", "METHOD_END"], "methodName": ["sanitizeTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   status    =    finishEvent . getContainerStatus (  )  ;", "String   diagnostics    =    getAMContainerCrashedDiagnostics ( finishEvent )  ;", "this . diagnosticspend ( diagnostics )  ;", "this . amContainerExitStatus    =    status . getExitStatus (  )  ;", "}", "METHOD_END"], "methodName": ["setAMContainerCrashedDiagnosticsAndExitStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", "this . amrmToken    =    lastToken ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["setAMRMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "masterContainer    =    container ;", "}", "METHOD_END"], "methodName": ["setMasterContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "originalTrackingUrl    =    pjoin ( WebAppUtils . getResolvedRMWebAppURLWithScheme ( conf )  ,     \" cluster \"  ,     \" app \"  ,    getAppAttemptId (  )  . getApplicationId (  )  )  ;", "proxiedTrackingUrl    =    originalTrackingUrl ;", "}", "METHOD_END"], "methodName": ["setTrackingUrlToRMAppPage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "RMAppAttemptImpl . LOG . info (  (  (  (  (  (  \" Storing   attempt :    AppId :     \"     +     ( getAppAttemptId (  )  . getApplicationId (  )  )  )     +     \"    AttemptId :     \"  )     +     ( getAppAttemptId (  )  )  )     +     \"    MasterContainer :     \"  )     +     ( masterContainer )  )  )  ;", "rmContext . getStateStore (  )  . storeNewApplicationAttempt ( this )  ;", "}", "METHOD_END"], "methodName": ["storeAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "this . justFinishedContainers    =    attempt . getJustFinishedContainers (  )  ;", "}", "METHOD_END"], "methodName": ["transferStateFromPreviousAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "progress    =     1  .  0 F ;", "RMAppAttemptUnregistrationEvent   unregisterEvent    =     (  ( RMAppAttemptUnregistrationEvent )     ( event )  )  ;", "diagnostics . append ( unregisterEvent . getDiagnostics (  )  )  ;", "originalTrackingUrl    =     . sanitizeTrackingUrl ( unregisterEvent . getFinalTrackingUrl (  )  )  ;", "proxiedTrackingUrl    =    generateProxyUriWithScheme ( originalTrackingUrl )  ;", "finalStatus    =    unregisterEvent . getFinalApplicationStatus (  )  ;", "}", "METHOD_END"], "methodName": ["updateInfoOnAMUnregister"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . isPreempted . get (  )  ;", "}", "METHOD_END"], "methodName": ["getIsPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics"}, {"methodBody": ["METHOD_START", "{", "return   numNonAMContainersPreempted . get (  )  ;", "}", "METHOD_END"], "methodName": ["getNumNonAMContainersPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics"}, {"methodBody": ["METHOD_START", "{", "try    {", "readLock . lock (  )  ;", "return   Preempted ;", "}    finally    {", "readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getResourcePreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics"}, {"methodBody": ["METHOD_START", "{", "this . isPreempted . set ( true )  ;", "}", "METHOD_END"], "methodName": ["setIsPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics"}, {"methodBody": ["METHOD_START", "{", "try    {", "writeLock . lock (  )  ;", "resourcePreempted    =    Resources . addTo ( resourcePreempted ,    resource )  ;", "}    finally    {", "writeLock . unlock (  )  ;", "}", "if    (  !  ( container . isAMContainer (  )  )  )     {", ". LOG . info ( String . format (  (  \" Non - AM   container   preempted ,    current   appAttemptId =  % s ,     \"     +     \" containerId =  % s ,    resource =  % s \"  )  ,    attemptId ,    container . getContainerId (  )  ,    resource )  )  ;", "numNonAMContainersPreempted . incrementAndGet (  )  ;", "} else    {", ". LOG . info ( String . format (  (  \" AM   container   preempted ,     \"     +     \" current   appAttemptId =  % s ,    containerId =  % s ,    resource =  % s \"  )  ,    attemptId ,    container . getContainerId (  )  ,    resource )  )  ;", "isPreempted . set ( true )  ;", "}", "}", "METHOD_END"], "methodName": ["updatePreemptionInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics"}, {"methodBody": ["METHOD_START", "{", "return   transferStateFromPreviousAttempt ;", "}", "METHOD_END"], "methodName": ["getTransferStateFromPreviousAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppStartAttemptEvent"}, {"methodBody": ["METHOD_START", "{", "scheduleApplicationAttempt (  )  ;", "Container   container    =    mock ( Container . class )  ;", "Resource   resource    =    BuilderUtils . newResource (  2  0  4  8  ,     1  )  ;", "when ( container . getId (  )  )  . thenReturn ( BuilderUtils . newContainerId ( applicationAttempt . getAppAttemptId (  )  ,     1  )  )  ;", "when ( container . getResource (  )  )  . thenReturn ( resource )  ;", "Allocation   allocation    =    mock ( Allocation . class )  ;", "when ( allocation . getContainers (  )  )  . thenReturn ( Collections . singletonList ( container )  )  ;", "when ( scheduler . allocate ( any ( ApplicationAttemptId . class )  ,    any ( List . class )  ,    any ( List . class )  ,    any ( List . class )  ,    any ( List . class )  )  )  . thenReturn ( allocation )  ;", "RMContainer   rmContainer    =    mock ( RMContainerImpl . class )  ;", "when ( scheduler . getRMContainer ( container . getId (  )  )  )  . thenReturn ( rmContainer )  ;", "applicationAttempt . handle ( new   ContainerAllocatedEvent ( applicationAttempt . getAppAttemptId (  )  )  )  ;", "assertEquals ( State . ALLOCATED _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . ATTEMPT _ NEW _ SAVED )  )  ;", "testAppAttemptAllocatedState ( container )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["allocateApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "String   url    =    null ;", "final   String   scheme    =    WebAppUtils . getHttpSchemePrefix ( conf )  ;", "try    {", "URI   trackingUri    =     ( StringUtils . isEmpty ( appAttempt . getOriginalTrackingUrl (  )  )  )     ?    null    :    ProxyUriUtils . getUriFromAMUrl ( scheme ,    appAttempt . getOriginalTrackingUrl (  )  )  ;", "String   proxy    =    WebAppUtils . getProxyHostAndPort ( conf )  ;", "URI   proxyUri    =    ProxyUriUtils . getUriFromAMUrl ( scheme ,    proxy )  ;", "URI   result    =    ProxyUriUtils . getProxyUri ( trackingUri ,    proxyUri ,    appAttempt . getId (  )  . getApplicationId (  )  )  ;", "url    =    result . toASCIIString (  )  ;", "}    catch    ( URISyntaxException   ex )     {", "Assert . fail (  )  ;", "}", "return   url ;", "}", "METHOD_END"], "methodName": ["getProxyUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {    Boolean . FALSE    }  ,    new   Object [  ]  {    Boolean . TRUE    }     }  )  ;", "}", "METHOD_END"], "methodName": ["getTestParameters"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "assertNull ( applicationAttempt . createClientToken (  \" some   client \"  )  )  ;", "}", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . LAUNCHED )  )  ;", "testAppAttemptLaunchedState ( container )  ;", "}", "METHOD_END"], "methodName": ["launchApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "applicationAttempt . handle ( new   RMAppAttemptRegistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,    host ,    rpcPort ,    trackingUrl )  )  ;", "testAppAttemptRunningState ( container ,    host ,    rpcPort ,    trackingUrl ,    unmanagedAM )  ;", "}", "METHOD_END"], "methodName": ["runApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "submitApplicationAttempt (  )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . ATTEMPT _ ADDED )  )  ;", "if    ( unmanagedAM )     {", "assertEquals ( State . LAUNCHED _ UNMANAGED _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . ATTEMPT _ NEW _ SAVED )  )  ;", "}", "testAppAttemptScheduledState (  )  ;", "}", "METHOD_END"], "methodName": ["scheduleApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . ATTEMPT _ UPDATE _ SAVED )  )  ;", "}", "METHOD_END"], "methodName": ["sendAttemptUpdateSavedEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "AuthenticationMethod   authMethod    =    AuthenticationMethod . SIMPLE ;", "if    ( isSecurityEnabled )     {", "authMethod    =    AuthenticationMethod . KERBEROS ;", "}", "SecurityUtil . setAuthenticationMethod ( authMethod ,    conf )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "InlineDispatcher   rmDispatcher    =    new   InlineDispatcher (  )  ;", "ContainerAllocationExpirer   containerAllocationExpirer    =    mock ( ContainerAllocationExpirer . class )  ;", "amLivelinessMonitor    =    mock ( AMLivelinessMonitor . class )  ;", "amFinishingMonitor    =    mock ( AMLivelinessMonitor . class )  ;", "writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "MasterKeyData   masterKeyData    =    amRMTokenManager . createNewMasterKey (  )  ;", "when ( amRMTokenManager . getMasterKey (  )  )  . thenReturn ( masterKeyData )  ;", "rmContext    =    new   RMContextImpl ( rmDispatcher ,    containerAllocationExpirer ,    amLivelinessMonitor ,    amFinishingMonitor ,    null ,    amRMTokenManager ,    new   RMContainerTokenSecretManager ( conf )  ,    nmTokenManager ,    clientToAMTokenManager ,    writer )  ;", "store    =    mock ( RMStateStore . class )  ;", "(  ( RMContextImpl )     ( rmContext )  )  . setStateStore ( store )  ;", "scheduler    =    mock ( YarnScheduler . class )  ;", "masterService    =    mock ( ApplicationMasterService . class )  ;", "applicationMasterLauncher    =    mock ( ApplicationMasterLauncher . class )  ;", "rmDispatcher . register ( RMAppAttemptEventType . class ,    new    . TestApplicationAttemptEventDispatcher (  )  )  ;", "rmDispatcher . register ( RMAppEventType . class ,    new    . TestApplicationEventDispatcher (  )  )  ;", "rmDispatcher . register ( SchedulerEventType . class ,    new    . TestSchedulerEventDispatcher (  )  )  ;", "rmDispatcher . register ( AMLauncherEventType . class ,    new    . TestAMLauncherEventDispatcher (  )  )  ;", "rmDispatcher . init ( conf )  ;", "rmDispatcher . start (  )  ;", "ApplicationId   applicationId    =    MockApps . newAppID (  (  (  . appId )  +  +  )  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     0  )  ;", "final   String   user    =    MockApps . newUserName (  )  ;", "final   String   queue    =    MockApps . newQueue (  )  ;", "submissionContext    =    mock ( ApplicationSubmissionContext . class )  ;", "when ( submissionContext . getQueue (  )  )  . thenReturn ( queue )  ;", "Resource   resource    =    BuilderUtils . newResource (  1  5  3  6  ,     1  )  ;", "ContainerLaunchContext   amContainerSpec    =    BuilderUtils . newContainerLaunchContext ( null ,    null ,    null ,    null ,    null ,    null )  ;", "when ( submissionContext . getAMContainerSpec (  )  )  . thenReturn ( amContainerSpec )  ;", "when ( submissionContext . getResource (  )  )  . thenReturn ( resource )  ;", "unmanagedAM    =    false ;", "application    =    mock ( RMAppImpl . class )  ;", "applicationAttempt    =    new   RMAppAttemptImpl ( applicationAttemptId ,    rmContext ,    scheduler ,    masterService ,    submissionContext ,    new   Configuration (  )  ,    false )  ;", "when ( application . getCurrentAppAttempt (  )  )  . thenReturn ( applicationAttempt )  ;", "when ( application . getApplicationId (  )  )  . thenReturn ( applicationId )  ;", "testAppAttemptNewState (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   appAttemptId    =    applicationAttempt . getAppAttemptId (  )  ;", "applicationAttempt . handle ( new   Event ( appAttemptId ,    EventType . START )  )  ;", "testAppAttemptSubmittedState (  )  ;", "}", "METHOD_END"], "methodName": ["submitApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "(  ( AsyncDispatcher )     ( this . rmContext . getDispatcher (  )  )  )  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "String   containerDiagMsg    =     \" some   error \"  ;", "int   exitCode    =     1  2  3  ;", "ContainerStatus   cs    =    BuilderUtils . newContainerStatus ( amContainer . getId (  )  ,    COMPLETE ,    containerDiagMsg ,    exitCode )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    cs )  )  ;", "assertEquals ( ALLOCATED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( State . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyApplicationAttemptFinished ( State . FAILED )  ;", "boolean   shouldCheckURL    =     ( applicationAttempt . getTrackingUrl (  )  )     !  =    null ;", "verifyAMCrashAtAllocatedDiagnosticInfo ( applicationAttempt . getDiagnostics (  )  ,    exitCode ,    shouldCheckURL )  ;", "}", "METHOD_END"], "methodName": ["testAMCrashAtAllocated"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "scheduleApplicationAttempt (  )  ;", "ContainerStatus   cs    =    SchedulerUtils . createAbnormalContainerStatus ( BuilderUtils . newContainerId ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ,    SchedulerUtils . LOST _ CONTAINER )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    cs )  )  ;", "assertEquals ( SCHEDULED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( State . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "verifyApplicationAttemptFinished ( State . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAMCrashAtScheduled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "String   diagnostics    =     \" Launch   Failed \"  ;", "applicationAttempt . handle ( new   LaunchFailedEvent ( applicationAttempt . getAppAttemptId (  )  ,    diagnostics )  )  ;", "assertEquals ( ALLOCATED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "testAppAttemptFailedState ( amContainer ,    diagnostics )  ;", "}", "METHOD_END"], "methodName": ["testAllocatedToFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . KILL )  )  ;", "assertEquals ( ALLOCATED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "testAppAttemptKilledState ( amContainer ,     . EMPTY _ DIAGNOSTICS )  ;", "}", "METHOD_END"], "methodName": ["testAllocatedToKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . ALLOCATED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( amContainer ,    applicationAttempt . getMasterContainer (  )  )  ;", "verify ( applicationMasterLauncher )  . handle ( any ( AMLauncherEvent . class )  )  ;", "verify ( scheduler ,    times (  2  )  )  . allocate ( any ( ApplicationAttemptId . class )  ,    any ( List . class )  ,    any ( List . class )  ,    any ( List . class )  ,    any ( List . class )  )  ;", "verify ( nmTokenManager )  . clearNodeSetForAttempt ( applicationAttempt . getAppAttemptId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptAllocatedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( State . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( diagnostics ,    applicationAttempt . getDiagnostics (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertEquals ( container ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  .  0  ,     (  ( double )     ( applicationAttempt . getProgress (  )  )  )  ,     1  .  0 E -  4  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "verify ( application ,    times (  1  )  )  . handle ( any ( RMAppFailedAttemptEvent . class )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyAttemptFinalStateSaved (  )  ;", "verifyApplicationAttemptFinished ( State . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptFailedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . FINISHED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( diagnostics ,    applicationAttempt . getDiagnostics (  )  )  ;", "verifyUrl ( trackingUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "if    ( unmanagedAM )     {", "verifyUrl ( trackingUrl ,    applicationAttempt . getTrackingUrl (  )  )  ;", "} else    {", "assertEquals ( getProxyUrl ( applicationAttempt )  ,    applicationAttempt . getTrackingUrl (  )  )  ;", "verifyAttemptFinalStateSaved (  )  ;", "}", "assertEquals ( finishedContainerCount ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertEquals ( container ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals ( finalStatus ,    applicationAttempt . getFinalApplicationStatus (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "assertFalse ( transferStateFromPreviousAttempt )  ;", "verifyApplicationAttemptFinished ( RMAppAttemptState . FINISHED )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptFinishedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . FINISHING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( diagnostics ,    applicationAttempt . getDiagnostics (  )  )  ;", "verifyUrl ( trackingUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "assertEquals ( getProxyUrl ( applicationAttempt )  ,    applicationAttempt . getTrackingUrl (  )  )  ;", "assertEquals ( container ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals ( finalStatus ,    applicationAttempt . getFinalApplicationStatus (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     0  )  ;", "verifyAttemptFinalStateSaved (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptFinishingState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( State . KILLED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( diagnostics ,    applicationAttempt . getDiagnostics (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertEquals ( amContainer ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  .  0  ,     (  ( double )     ( applicationAttempt . getProgress (  )  )  )  ,     1  .  0 E -  4  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getFinalApplicationStatus (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyAttemptFinalStateSaved (  )  ;", "assertFalse ( transferStateFromPreviousAttempt )  ;", "verifyApplicationAttemptFinished ( State . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptKilledState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . LAUNCHED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( container ,    applicationAttempt . getMasterContainer (  )  )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "assertNotNull ( applicationAttempt . createClientToken (  \" some   client \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppAttemptLaunchedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . NEW ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getDiagnostics (  )  . length (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  .  0  ,     (  ( double )     ( applicationAttempt . getProgress (  )  )  )  ,     1  .  0 E -  4  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getFinalApplicationStatus (  )  )  ;", "assertNotNull ( applicationAttempt . getTrackingUrl (  )  )  ;", "assertFalse (  \" N / A \"  . equals ( applicationAttempt . getTrackingUrl (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptNewState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . LAUNCHED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptRecoveredState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . RUNNING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( container ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals ( host ,    applicationAttempt . getHost (  )  )  ;", "assertEquals ( rpcPort ,    applicationAttempt . getRpcPort (  )  )  ;", "verifyUrl ( trackingUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "if    ( unmanagedAM )     {", "verifyUrl ( trackingUrl ,    applicationAttempt . getTrackingUrl (  )  )  ;", "} else    {", "assertEquals ( getProxyUrl ( applicationAttempt )  ,    applicationAttempt . getTrackingUrl (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppAttemptRunningState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "RMAppAttemptState   expectedState ;", "int   expectedAllocateCount ;", "if    ( unmanagedAM )     {", "expectedState    =    RMAppAttemptState . LAUNCHED ;", "expectedAllocateCount    =     0  ;", "} else    {", "expectedState    =    RMAppAttemptState . SCHEDULED ;", "expectedAllocateCount    =     1  ;", "}", "assertEquals ( expectedState ,    applicationAttempt . getAppAttemptState (  )  )  ;", "verify ( scheduler ,    times ( expectedAllocateCount )  )  . allocate ( any ( ApplicationAttemptId . class )  ,    any ( List . class )  ,    any ( List . class )  ,    any ( List . class )  ,    any ( List . class )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  .  0  ,     (  ( double )     ( applicationAttempt . getProgress (  )  )  )  ,     1  .  0 E -  4  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getFinalApplicationStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptScheduledState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( RMAppAttemptState . SUBMITTED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getDiagnostics (  )  . length (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  .  0  ,     (  ( double )     ( applicationAttempt . getProgress (  )  )  )  ,     1  .  0 E -  4  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getFinalApplicationStatus (  )  )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "verify ( clientToAMTokenManager )  . createMasterKey ( applicationAttempt . getAppAttemptId (  )  )  ;", "assertNull ( applicationAttempt . createClientToken (  \" some   client \"  )  )  ;", "}", "assertNull ( applicationAttempt . createClientToken ( null )  )  ;", "verify ( masterService )  . registerAppAttempt ( applicationAttempt . getAppAttemptId (  )  )  ;", "verify ( scheduler )  . handle ( any ( AppAttemptAddedSchedulerEvent . class )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptSubmittedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( State . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( diagnostics ,    applicationAttempt . getDiagnostics (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  .  0  ,     (  ( double )     ( applicationAttempt . getProgress (  )  )  )  ,     1  .  0 E -  4  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "assertNull ( applicationAttempt . getFinalApplicationStatus (  )  )  ;", "verify ( masterService )  . unregisterAttempt ( applicationAttempt . getAppAttemptId (  )  )  ;", "verify ( application )  . handle ( any ( RMAppRejectedEvent . class )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyApplicationAttemptFinished ( State . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptSubmittedToFailedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "applicationAttempt    =    new   RMAppAttemptImpl ( applicationAttempt . getAppAttemptId (  )  ,    rmContext ,    scheduler ,    masterService ,    submissionContext ,    new   Configuration (  )  ,    true )  ;", "when ( submissionContext . getKeepContainersAcrossApplicationAttempts (  )  )  . thenReturn ( true )  ;", "when ( submissionContext . getMaxAppAttempts (  )  )  . thenReturn (  1  )  ;", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "ContainerStatus   cs 1     =    ContainerStatus . newInstance ( amContainer . getId (  )  ,    COMPLETE ,     \" some   error \"  ,     1  2  3  )  ;", "ApplicationAttemptId   appAttemptId    =    applicationAttempt . getAppAttemptId (  )  ;", "applicationAttempt . handle ( new   RMAppAttemptContainerFinishedEvent ( appAttemptId ,    cs 1  )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( RMAppAttemptState . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertFalse ( transferStateFromPreviousAttempt )  ;", "verifyApplicationAttemptFinished ( RMAppAttemptState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testContainersCleanupForLastAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "testUnmanagedAMSuccess (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyTrackingUrlUnmanagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "when ( submissionContext . getKeepContainersAcrossApplicationAttempts (  )  )  . thenReturn ( true )  ;", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "ContainerStatus   cs 1     =    ContainerStatus . newInstance ( amContainer . getId (  )  ,    COMPLETE ,     \" some   error \"  ,     1  2  3  )  ;", "ApplicationAttemptId   appAttemptId    =    applicationAttempt . getAppAttemptId (  )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( appAttemptId ,    cs 1  )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( State . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertTrue ( transferStateFromPreviousAttempt )  ;", "verifyApplicationAttemptFinished ( State . FAILED )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "ContainerStatus   cs 2     =    ContainerStatus . newInstance ( ContainerId . newInstance ( appAttemptId ,     2  )  ,    COMPLETE ,     \"  \"  ,     0  )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( appAttemptId ,    cs 2  )  )  ;", "assertEquals (  1  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertEquals ( cs 2  . getContainerId (  )  ,    applicationAttempt . getJustFinishedContainers (  )  . get (  0  )  . getContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFailedToFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . SUCCEEDED ;", "String   trackingUrl    =     \" mytrackingurl \"  ;", "String   diagnostics    =     \" Successful \"  ;", "applicationAttempt . handle ( new   UnregistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,    trackingUrl ,    finalStatus ,    diagnostics )  )  ;", "assertEquals ( State . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    BuilderUtils . newContainerStatus ( amContainer . getId (  )  ,    COMPLETE ,     \"  \"  ,     0  )  )  )  ;", "assertEquals ( State . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "testAppAttemptFinishedState ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics ,     0  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testFinalSavingToFinishedWithContainerFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . SUCCEEDED ;", "String   trackingUrl    =     \" mytrackingurl \"  ;", "String   diagnostics    =     \" Successssseeeful \"  ;", "applicationAttempt . handle ( new   UnregistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,    trackingUrl ,    finalStatus ,    diagnostics )  )  ;", "assertEquals ( State . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . EXPIRE )  )  ;", "assertEquals ( State . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "testAppAttemptFinishedState ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics ,     0  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testFinalSavingToFinishedWithExpire"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . SUCCEEDED ;", "String   trackingUrl    =     \" mytrackingurl \"  ;", "String   diagnostics    =     \" Successful \"  ;", "unregisterApplicationAttempt ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . EXPIRE )  )  ;", "testAppAttemptFinishedState ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics ,     0  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testFinishingExpire"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . FAILED ;", "String   trackingUrl    =     \" newtrackingurl \"  ;", "String   diagnostics    =     \" Job   failed \"  ;", "unregisterApplicationAttempt ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . KILL )  )  ;", "testAppAttemptFinishingState ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "}", "METHOD_END"], "methodName": ["testFinishingKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . SUCCEEDED ;", "String   trackingUrl    =     \" mytrackingurl \"  ;", "String   diagnostics    =     \" Successful \"  ;", "unregisterApplicationAttempt ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    BuilderUtils . newContainerStatus ( BuilderUtils . newContainerId ( applicationAttempt . getAppAttemptId (  )  ,     4  2  )  ,    COMPLETE ,     \"  \"  ,     0  )  )  )  ;", "testAppAttemptFinishingState ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "}", "METHOD_END"], "methodName": ["testFinishingToFinishing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assumeTrue ( isSecurityEnabled )  ;", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "Token < ClientToAMTokenIdentifier >    token    =    applicationAttempt . createClientToken ( null )  ;", "Assert . assertNull ( token )  ;", "token    =    applicationAttempt . createClientToken (  \" clientuser \"  )  ;", "Assert . assertNull ( token )  ;", "launchApplicationAttempt ( amContainer )  ;", "token    =    applicationAttempt . createClientToken ( null )  ;", "Assert . assertNull ( token )  ;", "token    =    applicationAttempt . createClientToken (  \" clientuser \"  )  ;", "Assert . assertNotNull ( token )  ;", "applicationAttempt . handle ( new   Event ( applicationAttempt . getAppAttemptId (  )  ,    EventType . KILL )  )  ;", "assertEquals ( LAUNCHED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "token    =    applicationAttempt . createClientToken ( null )  ;", "Assert . assertNull ( token )  ;", "token    =    applicationAttempt . createClientToken (  \" clientuser \"  )  ;", "Assert . assertNull ( token )  ;", "}", "METHOD_END"], "methodName": ["testGetClientToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . EXPIRE )  )  ;", "assertEquals ( LAUNCHED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( RMAppAttemptState . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertTrue (  \" expire   diagnostics   missing \"  ,    applicationAttempt . getDiagnostics (  )  . contains (  \" timed   out \"  )  )  ;", "String   rmAppPageUrl    =    pjoin (  . RM _ WEBAPP _ ADDR ,     \" cluster \"  ,     \" app \"  ,    applicationAttempt . getAppAttemptId (  )  . getApplicationId (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getTrackingUrl (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyApplicationAttemptFinished ( RMAppAttemptState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testLaunchedExpire"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "testTrackingUrlManagedAM (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testManagedAMWithEmptyTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "testTrackingUrlManagedAM ( null )  ;", "}", "METHOD_END"], "methodName": ["testManagedAMWithNullTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "testTrackingUrlManagedAM (  \" theTrackingUrl \"  )  ;", "}", "METHOD_END"], "methodName": ["testManagedAMWithTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . KILL )  )  ;", "assertEquals ( NEW ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "testAppAttemptKilledState ( null ,     . EMPTY _ DIAGNOSTICS )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["testNewToKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . RECOVER )  )  ;", "testAppAttemptRecoveredState (  )  ;", "}", "METHOD_END"], "methodName": ["testNewToRecovered"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "testUnmanagedAMSuccess ( null )  ;", "}", "METHOD_END"], "methodName": ["testNullTrackingUrlUnmanagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . EXPIRE )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( RMAppAttemptState . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertTrue (  \" expire   diagnostics   missing \"  ,    applicationAttempt . getDiagnostics (  )  . contains (  \" timed   out \"  )  )  ;", "String   rmAppPageUrl    =    pjoin (  . RM _ WEBAPP _ ADDR ,     \" cluster \"  ,     \" app \"  ,    applicationAttempt . getAppAttemptId (  )  . getApplicationId (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getTrackingUrl (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyAMHostAndPortInvalidated (  )  ;", "verifyApplicationAttemptFinished ( RMAppAttemptState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testRunningExpire"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "String   containerDiagMsg    =     \" some   error \"  ;", "int   exitCode    =     1  2  3  ;", "ContainerStatus   cs    =    BuilderUtils . newContainerStatus ( amContainer . getId (  )  ,    COMPLETE ,    containerDiagMsg ,    exitCode )  ;", "ApplicationAttemptId   appAttemptId    =    applicationAttempt . getAppAttemptId (  )  ;", "applicationAttempt . handle ( new   RMAppAttemptContainerFinishedEvent ( appAttemptId ,    cs )  )  ;", "assertEquals ( RMAppAttemptState . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "applicationAttempt . handle ( new   RMAppAttemptContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    BuilderUtils . newContainerStatus ( amContainer . getId (  )  ,    COMPLETE ,     \"  \"  ,     0  )  )  )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . EXPIRE )  )  ;", "assertEquals ( RMAppAttemptState . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( RMAppAttemptState . FAILED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertEquals ( amContainer ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "String   rmAppPageUrl    =    pjoin (  . RM _ WEBAPP _ ADDR ,     \" cluster \"  ,     \" app \"  ,    applicationAttempt . getAppAttemptId (  )  . getApplicationId (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getTrackingUrl (  )  )  ;", "verifyAMHostAndPortInvalidated (  )  ;", "verifyApplicationAttemptFinished ( RMAppAttemptState . FAILED )  ;", "}", "METHOD_END"], "methodName": ["testRunningToFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . KILL )  )  ;", "assertEquals ( RMAppAttemptState . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "applicationAttempt . handle ( new   RMAppAttemptContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    BuilderUtils . newContainerStatus ( amContainer . getId (  )  ,    COMPLETE ,     \"  \"  ,     0  )  )  )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . EXPIRE )  )  ;", "assertEquals ( RMAppAttemptState . FINAL _ SAVING ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals ( RUNNING ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertEquals ( RMAppAttemptState . KILLED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "assertEquals (  0  ,    applicationAttempt . getJustFinishedContainers (  )  . size (  )  )  ;", "assertEquals ( amContainer ,    applicationAttempt . getMasterContainer (  )  )  ;", "assertEquals (  0  ,    application . getRanNodes (  )  . size (  )  )  ;", "String   rmAppPageUrl    =    pjoin (  . RM _ WEBAPP _ ADDR ,     \" cluster \"  ,     \" app \"  ,    applicationAttempt . getAppAttemptId (  )  . getApplicationId (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getOriginalTrackingUrl (  )  )  ;", "assertEquals ( rmAppPageUrl ,    applicationAttempt . getTrackingUrl (  )  )  ;", "verifyTokenCount ( applicationAttempt . getAppAttemptId (  )  ,     1  )  ;", "verifyAMHostAndPortInvalidated (  )  ;", "verifyApplicationAttemptFinished ( RMAppAttemptState . KILLED )  ;", "}", "METHOD_END"], "methodName": ["testRunningToKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "scheduleApplicationAttempt (  )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . KILL )  )  ;", "assertEquals ( SCHEDULED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "testAppAttemptKilledState ( null ,     . EMPTY _ DIAGNOSTICS )  ;", "}", "METHOD_END"], "methodName": ["testScheduledToKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "submitApplicationAttempt (  )  ;", "applicationAttempt . handle ( new   RMAppAttemptEvent ( applicationAttempt . getAppAttemptId (  )  ,    RMAppAttemptEventType . KILL )  )  ;", "assertEquals ( SUBMITTED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "testAppAttemptKilledState ( null ,     . EMPTY _ DIAGNOSTICS )  ;", "}", "METHOD_END"], "methodName": ["testSubmittedToKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . SUCCEEDED ;", "String   trackingUrl    =     \" mytrackingurl \"  ;", "String   diagnostics    =     \" Successful \"  ;", "unregisterApplicationAttempt ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    BuilderUtils . newContainerStatus ( amContainer . getId (  )  ,    COMPLETE ,     \"  \"  ,     0  )  )  )  ;", "testAppAttemptFinishedState ( amContainer ,    finalStatus ,    trackingUrl ,    diagnostics ,     0  ,    false )  ;", "}", "METHOD_END"], "methodName": ["testSuccessfulFinishingToFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,    url ,    false )  ;", "unregisterApplicationAttempt ( amContainer ,    SUCCEEDED ,    url ,     \" Successful \"  )  ;", "}", "METHOD_END"], "methodName": ["testTrackingUrlManagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "testUnmanagedAMSuccess (  \" oldTrackingUrl \"  )  ;", "}", "METHOD_END"], "methodName": ["testTrackingUrlUnmanagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "unmanagedAM    =    true ;", "when ( submissionContext . getUnmanagedAM (  )  )  . thenReturn ( true )  ;", "when ( submissionContext . getKeepContainersAcrossApplicationAttempts (  )  )  . thenReturn ( true )  ;", "submitApplicationAttempt (  )  ;", "applicationAttempt . handle ( new   RegistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  )  )  ;", "assertEquals ( SUBMITTED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "assertFalse ( transferStateFromPreviousAttempt )  ;", "}", "METHOD_END"], "methodName": ["testUnmanagedAMContainersCleanup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "unmanagedAM    =    true ;", "when ( submissionContext . getUnmanagedAM (  )  )  . thenReturn ( true )  ;", "scheduleApplicationAttempt (  )  ;", "testAppAttemptLaunchedState ( null )  ;", "verify ( amLivelinessMonitor ,    times (  1  )  )  . register ( applicationAttempt . getAppAttemptId (  )  )  ;", "runApplicationAttempt ( null ,     \" host \"  ,     8  0  4  2  ,    url ,    true )  ;", "Container   container    =    mock ( Container . class )  ;", "when ( container . getNodeId (  )  )  . thenReturn ( NodeId . newInstance (  \" host \"  ,     1  2  3  4  )  )  ;", "application . handle ( new   RMAppRunningOnNodeEvent ( application . getApplicationId (  )  ,    container . getNodeId (  )  )  )  ;", "applicationAttempt . handle ( new   ContainerFinishedEvent ( applicationAttempt . getAppAttemptId (  )  ,    mock ( ContainerStatus . class )  )  )  ;", "String   diagnostics    =     \" Successful \"  ;", "FinalApplicationStatus   finalStatus    =    FinalApplicationStatus . SUCCEEDED ;", "applicationAttempt . handle ( new   UnregistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,    url ,    finalStatus ,    diagnostics )  )  ;", "testAppAttemptFinishedState ( null ,    finalStatus ,    url ,    diagnostics ,     1  ,    true )  ;", "assertFalse ( transferStateFromPreviousAttempt )  ;", "}", "METHOD_END"], "methodName": ["testUnmanagedAMSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "unmanagedAM    =    true ;", "when ( submissionContext . getUnmanagedAM (  )  )  . thenReturn ( true )  ;", "submitApplicationAttempt (  )  ;", "assertEquals ( State . SUBMITTED ,    applicationAttempt . getAppAttemptState (  )  )  ;", "applicationAttempt . handle ( new   RegistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  )  )  ;", "assertEquals ( SUBMITTED ,    applicationAttempt . createApplicationAttemptState (  )  )  ;", "testAppAttemptSubmittedToFailedState (  \" Unmanaged   AM   must   register   after   AM   attempt   reaches   LAUNCHED   state .  \"  )  ;", "}", "METHOD_END"], "methodName": ["testUnmanagedAMUnexpectedRegistration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "unregisterApplicationAttempt ( amContainer ,    KILLED ,     \" newtrackingurl \"  ,     \" Killed   by   user \"  )  ;", "}", "METHOD_END"], "methodName": ["testUnregisterToKilledFinishing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "Container   amContainer    =    allocateApplicationAttempt (  )  ;", "launchApplicationAttempt ( amContainer )  ;", "runApplicationAttempt ( amContainer ,     \" host \"  ,     8  0  4  2  ,     \" oldtrackingurl \"  ,    false )  ;", "unregisterApplicationAttempt ( amContainer ,    SUCCEEDED ,     \" mytrackingurl \"  ,     \" Successful \"  )  ;", "}", "METHOD_END"], "methodName": ["testUnregisterToSuccessfulFinishing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "applicationAttempt . handle ( new   RMAppAttemptUnregistrationEvent ( applicationAttempt . getAppAttemptId (  )  ,    trackingUrl ,    finalStatus ,    diagnostics )  )  ;", "sendAttemptUpdateSavedEvent ( applicationAttempt )  ;", "testAppAttemptFinishingState ( container ,    finalStatus ,    trackingUrl ,    diagnostics )  ;", "}", "METHOD_END"], "methodName": ["unregisterApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" Diagnostic   information   does   not   point   the   logs   to   the   users \"  ,    diagnostics . contains (  \" logs \"  )  )  ;", "assertTrue (  \" Diagnostic   information   does   not   contain   application   attempt   id \"  ,    diagnostics . contains ( applicationAttempt . getId (  )  . toString (  )  )  )  ;", "assertTrue (  \" Diagnostic   information   does   not   contain   application   exit   code \"  ,    diagnostics . contains (  (  \" exitCode :     \"     +    exitCode )  )  )  ;", "if    ( shouldCheckURL )     {", "assertTrue (  \" Diagnostic   information   does   not   contain   application   proxy   URL \"  ,    diagnostics . contains ( applicationAttempt . getWebProxyBase (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyAMCrashAtAllocatedDiagnosticInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" N / A \"  ,    applicationAttempt . getHost (  )  )  ;", "assertEquals (  (  -  1  )  ,    applicationAttempt . getRpcPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyAMHostAndPortInvalidated"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "ArgumentCaptor < RMAppAttemptState >    finalState    =    ArgumentCaptor . forClass ( RMAppAttemptState . class )  ;", "verify ( writer )  . applicationAttemptFinished ( any ( RMAppAttempt . class )  ,    finalState . capture (  )  )  ;", "Assert . assertEquals ( state ,    finalState . getValue (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyApplicationAttemptFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "verify ( store ,    times (  1  )  )  . updateApplicationAttemptState ( any ( RMStateStore . ApplicationAttemptState . class )  )  ;", "}", "METHOD_END"], "methodName": ["verifyAttemptFinalStateSaved"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "verify ( amRMTokenManager ,    times ( count )  )  . applicationMasterFinished ( appAttemptId )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "verify ( clientToAMTokenManager ,    times ( count )  )  . unRegisterApplication ( appAttemptId )  ;", "if    ( count    >     0  )     {", "assertNull ( applicationAttempt . createClientToken (  \" client \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["verifyTokenCount"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "if    (  ( url 1     =  =    null )     |  |     ( url 1  . trim (  )  . isEmpty (  )  )  )     {", "assertEquals (  \" N / A \"  ,    url 2  )  ;", "} else    {", "assertEquals ( url 1  ,    url 2  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.TestRMAppAttemptTransitions"}, {"methodBody": ["METHOD_START", "{", "return   this . containerStatus ;", "}", "METHOD_END"], "methodName": ["getContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . message ;", "}", "METHOD_END"], "methodName": ["getMessage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . rpcport ;", "}", "METHOD_END"], "methodName": ["getRpcport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . trackingurl ;", "}", "METHOD_END"], "methodName": ["getTrackingurl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . progress ;", "}", "METHOD_END"], "methodName": ["getProgress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptStatusupdateEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnostics ;", "}", "METHOD_END"], "methodName": ["getDiagnostics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . finalStatus ;", "}", "METHOD_END"], "methodName": ["getFinalApplicationStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . finalTrackingUrl ;", "}", "METHOD_END"], "methodName": ["getFinalTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent"}, {"methodBody": ["METHOD_START", "{", "int   expireIntvl    =    conf . getInt ( RM _ CONTAINER _ ALLOC _ EXPIRY _ INTERVAL _ MS ,    DEFAULT _ RM _ CONTAINER _ ALLOC _ EXPIRY _ INTERVAL _ MS )  ;", "setInterval ( expireIntvl )  ;", "setMonitorInterval (  ( expireIntvl    /     3  )  )  ;", "super . serviceInit ( conf )  ;", "}", "METHOD_END"], "methodName": ["serviceInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer"}, {"methodBody": ["METHOD_START", "{", "return   this . containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . remoteContainerStatus ;", "}", "METHOD_END"], "methodName": ["getRemoteContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerFinishedEvent"}, {"methodBody": ["METHOD_START", "{", "return   finishedStatus ;", "}", "METHOD_END"], "methodName": ["getFinishedStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"}, {"methodBody": ["METHOD_START", "{", "try    {", "writeLock . lock (  )  ;", "this . isA    =    isA ;", "}    finally    {", "writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["setAMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"}, {"methodBody": ["METHOD_START", "{", "try    {", "writeLock . lock (  )  ;", "thisRequests    =    requests ;", "}    finally    {", "writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["setResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"}, {"methodBody": ["METHOD_START", "{", "return   containerReport ;", "}", "METHOD_END"], "methodName": ["getContainerReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerRecoverEvent"}, {"methodBody": ["METHOD_START", "{", "return   reservedNode ;", "}", "METHOD_END"], "methodName": ["getReservedNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerReservedEvent"}, {"methodBody": ["METHOD_START", "{", "return   reservedPriority ;", "}", "METHOD_END"], "methodName": ["getReservedPriority"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerReservedEvent"}, {"methodBody": ["METHOD_START", "{", "return   reservedResource ;", "}", "METHOD_END"], "methodName": ["getReservedResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerReservedEvent"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    rm 1  . registerNode (  \" unknownhost :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  1  0  2  4  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "ResourceScheduler   scheduler    =    rm 1  . getResourceScheduler (  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "ContainerId   containerId 2     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . ALLOCATED )  ;", "Assert . assertNotNull ( scheduler . getRMContainer ( containerId 2  )  . getResourceRequests (  )  )  ;", "am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . ACQUIRED )  ;", "Assert . assertNull ( scheduler . getRMContainer ( containerId 2  )  . getResourceRequests (  )  )  ;", "}", "METHOD_END"], "methodName": ["testExistenceOfResourceRequestInRMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.TestRMContainerImpl"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   drainDispatcher    =    new   DrainDispatcher (  )  ;", "EventHandler < RMAppAttemptEvent >    appAttemptEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler   generic    =    mock ( EventHandler . class )  ;", "drainDispatcher . register ( RMAppAttemptEventType . class ,    appAttemptEventHandler )  ;", "drainDispatcher . register ( RMNodeEventType . class ,    generic )  ;", "drainDispatcher . init ( new   YarnConfiguration (  )  )  ;", "drainDispatcher . start (  )  ;", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" host \"  ,     3  4  2  5  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   containerId    =    BuilderUtils . newContainerId ( appAttemptId ,     1  )  ;", "ContainerAllocationExpirer   expirer    =    mock ( ContainerAllocationExpirer . class )  ;", "Resource   resource    =    BuilderUtils . newResource (  5  1  2  ,     1  )  ;", "Priority   priority    =    BuilderUtils . newPriority (  5  )  ;", "Container   container    =    BuilderUtils . newContainer ( containerId ,    nodeId ,     \" host :  3  4  6  5  \"  ,    resource ,    priority ,    null )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getDispatcher (  )  )  . thenReturn ( drainDispatcher )  ;", "when ( rmContext . getContainerAllocationExpirer (  )  )  . thenReturn ( expirer )  ;", "when ( rmContext . getRMApplicationHistoryWriter (  )  )  . thenReturn ( writer )  ;", "RMContainer   rmContainer    =    new    ( container ,    appAttemptId ,    nodeId ,     \" user \"  ,    rmContext )  ;", "assertEquals ( RMContainerState . NEW ,    rmContainer . getState (  )  )  ;", "assertEquals ( resource ,    rmContainer . getAllocatedResource (  )  )  ;", "assertEquals ( nodeId ,    rmContainer . getAllocatedNode (  )  )  ;", "assertEquals ( priority ,    rmContainer . getAllocatedPriority (  )  )  ;", "verify ( writer )  . containerStarted ( any ( RMContainer . class )  )  ;", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . START )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . ALLOCATED ,    rmContainer . getState (  )  )  ;", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . ACQUIRED )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . ACQUIRED ,    rmContainer . getState (  )  )  ;", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . LAUNCHED )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . RUNNING ,    rmContainer . getState (  )  )  ;", "assertEquals (  \"  /  / host :  3  4  6  5  / node / containerlogs / container _  1  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  / user \"  ,    rmContainer . getLogURL (  )  )  ;", "reset ( appAttemptEventHandler )  ;", "ContainerStatus   containerStatus    =    SchedulerUtils . createAbnormalContainerStatus ( containerId ,    SchedulerUtils . EXPIRED _ CONTAINER )  ;", "rmContainer . handle ( new   RMContainerFinishedEvent ( containerId ,    containerStatus ,    RMContainerEventType . EXPIRE )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . RUNNING ,    rmContainer . getState (  )  )  ;", "verify ( writer ,    never (  )  )  . containerFinished ( any ( RMContainer . class )  )  ;", "}", "METHOD_END"], "methodName": ["testExpireWhileRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.TestRMContainerImpl"}, {"methodBody": ["METHOD_START", "{", "DrainDispatcher   drainDispatcher    =    new   DrainDispatcher (  )  ;", "EventHandler < RMAppAttemptEvent >    appAttemptEventHandler    =    mock ( EventHandler . class )  ;", "EventHandler   generic    =    mock ( EventHandler . class )  ;", "drainDispatcher . register ( RMAppAttemptEventType . class ,    appAttemptEventHandler )  ;", "drainDispatcher . register ( RMNodeEventType . class ,    generic )  ;", "drainDispatcher . init ( new   YarnConfiguration (  )  )  ;", "drainDispatcher . start (  )  ;", "NodeId   nodeId    =    BuilderUtils . newNodeId (  \" host \"  ,     3  4  2  5  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "ContainerId   containerId    =    BuilderUtils . newContainerId ( appAttemptId ,     1  )  ;", "ContainerAllocationExpirer   expirer    =    mock ( ContainerAllocationExpirer . class )  ;", "Resource   resource    =    BuilderUtils . newResource (  5  1  2  ,     1  )  ;", "Priority   priority    =    BuilderUtils . newPriority (  5  )  ;", "Container   container    =    BuilderUtils . newContainer ( containerId ,    nodeId ,     \" host :  3  4  6  5  \"  ,    resource ,    priority ,    null )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getDispatcher (  )  )  . thenReturn ( drainDispatcher )  ;", "when ( rmContext . getContainerAllocationExpirer (  )  )  . thenReturn ( expirer )  ;", "when ( rmContext . getRMApplicationHistoryWriter (  )  )  . thenReturn ( writer )  ;", "RMContainer   rmContainer    =    new    ( container ,    appAttemptId ,    nodeId ,     \" user \"  ,    rmContext )  ;", "assertEquals ( RMContainerState . NEW ,    rmContainer . getState (  )  )  ;", "assertEquals ( resource ,    rmContainer . getAllocatedResource (  )  )  ;", "assertEquals ( nodeId ,    rmContainer . getAllocatedNode (  )  )  ;", "assertEquals ( priority ,    rmContainer . getAllocatedPriority (  )  )  ;", "verify ( writer )  . containerStarted ( any ( RMContainer . class )  )  ;", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . START )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . ALLOCATED ,    rmContainer . getState (  )  )  ;", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . ACQUIRED )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . ACQUIRED ,    rmContainer . getState (  )  )  ;", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . LAUNCHED )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . RUNNING ,    rmContainer . getState (  )  )  ;", "assertEquals (  \"  /  / host :  3  4  6  5  / node / containerlogs / container _  1  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  / user \"  ,    rmContainer . getLogURL (  )  )  ;", "reset ( appAttemptEventHandler )  ;", "ContainerStatus   containerStatus    =    SchedulerUtils . createAbnormalContainerStatus ( containerId ,    SchedulerUtils . RELEASED _ CONTAINER )  ;", "rmContainer . handle ( new   RMContainerFinishedEvent ( containerId ,    containerStatus ,    RMContainerEventType . RELEASED )  )  ;", "drainDispatcher . await (  )  ;", "assertEquals ( RMContainerState . RELEASED ,    rmContainer . getState (  )  )  ;", "assertEquals ( SchedulerUtils . RELEASED _ CONTAINER ,    rmContainer . getDiagnosticsInfo (  )  )  ;", "assertEquals ( ABORTED ,    rmContainer . getContainerExitStatus (  )  )  ;", "assertEquals ( COMPLETE ,    rmContainer . getContainerState (  )  )  ;", "verify ( writer )  . containerFinished ( any ( RMContainer . class )  )  ;", "ArgumentCaptor < RMAppAttemptContainerFinishedEvent >    captor    =    ArgumentCaptor . forClass ( RMAppAttemptContainerFinishedEvent . class )  ;", "verify ( appAttemptEventHandler )  . handle ( captor . capture (  )  )  ;", "RMAppAttemptContainerFinishedEvent   cfEvent    =    captor . getValue (  )  ;", "assertEquals ( appAttemptId ,    cfEvent . getApplicationAttemptId (  )  )  ;", "assertEquals ( containerStatus ,    cfEvent . getContainerStatus (  )  )  ;", "assertEquals ( RMAppAttemptEventType . CONTAINER _ FINISHED ,    cfEvent . getType (  )  )  ;", "rmContainer . handle ( new   RMContainerFinishedEvent ( containerId ,    SchedulerUtils . createAbnormalContainerStatus ( containerId ,     \" FinishedContainer \"  )  ,    RMContainerEventType . FINISHED )  )  ;", "assertEquals ( RMContainerState . RELEASED ,    rmContainer . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReleaseWhileRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.TestRMContainerImpl"}, {"methodBody": ["METHOD_START", "{", "return   this . appId ;", "}", "METHOD_END"], "methodName": ["getAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeCleanAppEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . contId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeCleanContainerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeId ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . launchedContainers ;", "}", "METHOD_END"], "methodName": ["getLaunchedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "return   nodeUpdateQueue . size (  )  ;", "}", "METHOD_END"], "methodName": ["getQueueSize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "RMNodeImpl . LOG . debug (  (  (  (  \" Processing    \"     +     ( event . getNodeId (  )  )  )     +     \"    of   type    \"  )     +     ( event . getType (  )  )  )  )  ;", "try    {", "writeLock . lock (  )  ;", "NodeState   oldState    =    getState (  )  ;", "try    {", "stateMachine . doTransition ( event . getType (  )  ,    event )  ;", "}    catch    ( InvalidStateTransitonException   e )     {", "RMNodeImpl . LOG . error (  \" Can ' t   handle   this   event   at   current   state \"  ,    e )  ;", "RMNodeImpl . LOG . error (  (  (  (  \" Invalid   event    \"     +     ( event . getType (  )  )  )     +     \"    on   Node       \"  )     +     ( this . nodeId )  )  )  ;", "}", "if    ( oldState    !  =     ( getState (  )  )  )     {", "RMNodeImpl . LOG . info (  (  (  (  (  ( nodeId )     +     \"    Node   Transitioned   from    \"  )     +    oldState )     +     \"    to    \"  )     +     ( getState (  )  )  )  )  ;", "}", "}    finally    {", "writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["handle"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    context . getRMApps (  )  . get ( appId )  ;", "if    ( null    =  =    app )     {", ". LOG . warn (  (  (  \" Cannot   get   RMApp   by   appId =  \"     +    appId )     +     \"  ,    just   added   it   to   finishedApplications   list   for   cleanup \"  )  )  ;", "rmNode . finishedApplications . add ( appId )  ;", "return ;", "}", "context . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRunningOnNodeEvent ( appId ,    nodeId )  )  ;", "}", "METHOD_END"], "methodName": ["handleRunningAppOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "is . writeLock . lock (  )  ;", "try    {", "isealReport    = ealReport ;", "}    finally    {", "is . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["setHealthReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", "this . lastHealthReptTime    =    lastHealthReptTime ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["setLastHealthReportTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "this . nextHeartBeat    =    nextHeartBeat ;", "}", "METHOD_END"], "methodName": ["setNextHeartBeat"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "ClusterMetrics   metrics    =    ClusterMetrics . getMetrics (  )  ;", "switch    ( initialState )     {", "case   RUNNING    :", "metrics . decrNumActiveNodes (  )  ;", "break ;", "case   UNHEALTHY    :", "metrics . decrNumUnhealthyNMs (  )  ;", "break ;", "}", "switch    ( finalState )     {", "case   DECOMMISSIONED    :", "Set < String >    ecludedHosts    =    context . getNodesListM (  )  . getHostsReader (  )  . getExcludedHosts (  )  ;", "if    (  (  !  ( ecludedHosts . contains ( hostName )  )  )     &  &     (  !  ( ecludedHosts . contains ( NetUtils . normalizeHostName ( hostName )  )  )  )  )     {", "metrics . incrDecommisionedNMs (  )  ;", "}", "break ;", "case   LOST    :", "metrics . incrNumLostNMs (  )  ;", "break ;", "case   REBOOTED    :", "metrics . incrNumRebootedNMs (  )  ;", "break ;", "case   UNHEALTHY    :", "metrics . incrNumUnhealthyNMs (  )  ;", "break ;", "}", "}", "METHOD_END"], "methodName": ["updateMetricsForDeactivatedNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "ClusterMetrics   metrics    =    ClusterMetrics . getMetrics (  )  ;", "metrics . incrNumActives (  )  ;", "switch    ( previousState )     {", "case   LOST    :", "metrics . decrNumLostNMs (  )  ;", "break ;", "case   REBOOTED    :", "metrics . decrNumRebootedNMs (  )  ;", "break ;", "case   DECOMMISSIONED    :", "metrics . decrDecommisionedNMs (  )  ;", "break ;", "case   UNHEALTHY    :", "metrics . decrNumUnhealthyNMs (  )  ;", "break ;", "}", "}", "METHOD_END"], "methodName": ["updateMetricsForRejoinedNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl"}, {"methodBody": ["METHOD_START", "{", "return   reconnectedNode ;", "}", "METHOD_END"], "methodName": ["getReconnectedNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeReconnectEvent"}, {"methodBody": ["METHOD_START", "{", "return   runningApplications ;", "}", "METHOD_END"], "methodName": ["getRunningApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeReconnectEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . containerStatuses ;", "}", "METHOD_END"], "methodName": ["getNMContainerStatuses"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   runningApplications ;", "}", "METHOD_END"], "methodName": ["getRunningApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStartedEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . containersCollection ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . keepAliveAppIds ;", "}", "METHOD_END"], "methodName": ["getKeepAliveAppIds"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . latestResponse ;", "}", "METHOD_END"], "methodName": ["getLatestResponse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeHealthStatus ;", "}", "METHOD_END"], "methodName": ["getNodeHealthStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeStatusEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . completedContainers ;", "}", "METHOD_END"], "methodName": ["getCompletedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.UpdatedContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . newlyLaunchedContainers ;", "}", "METHOD_END"], "methodName": ["getNewlyLaunchedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.rmnode.UpdatedContainerInfo"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplicationAttempt   application    =    getCurrentAttemptForContainer ( containerId )  ;", "if    ( application    =  =    null )     {", ". LOG . info (  (  (  (  (  (  \" Unknown   application    \"     +     ( containerId . getApplicationAttemptId (  )  . getApplicationId (  )  )  )     +     \"    launched   container    \"  )     +    containerId )     +     \"    on   node :     \"  )     +    node )  )  ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMNodeCleanContainerEvent ( node . getNodeID (  )  ,    containerId )  )  ;", "return ;", "}", "application . containerLaunchedOnNode ( containerId ,    node . getNodeID (  )  )  ;", "}", "METHOD_END"], "methodName": ["containerLaunchedOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "new   Timer (  )  . schedule ( new   TimerTask (  )     {", "@ Override", "public   void   run (  )     {", "for    ( SchedulerApplication < T >    app    :    applications . values (  )  )     {", "T   attempt    =    app . getCurrentAppAttempt (  )  ;", "synchronized ( attempt )     {", "for    ( ContainerId   containerId    :    attempt . getPendingRelease (  )  )     {", "RMAuditLogger . logFailure ( app . getUser (  )  ,    RMAuditLogger . AuditConstants . RELEASE _ CONTAINER ,     \" Unauthorized   access   or   invalid   container \"  ,     \" Scheduler \"  ,     \" Trying   to   release   container   not   owned   by   app   or   with   invalid   id .  \"  ,    attempt . getApplicationId (  )  ,    containerId )  ;", "}", "attempt . getPendingRelease (  )  . clear (  )  ;", "}", "}", ". LOG . info (  \" Release   request   cache   is   cleaned   up \"  )  ;", "}", "}  ,    nmExpireInterval )  ;", "}", "METHOD_END"], "methodName": ["createReleaseCache"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < T >    app    =    applications . get ( applicationAttemptId . getApplicationId (  )  )  ;", "return   app    =  =    null    ?    null    :    app . getCurrentAppAttempt (  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "return   getApplicationAttempt ( containerId . getApplicationAttemptId (  )  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentAttemptForContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "return   applications ;", "}", "METHOD_END"], "methodName": ["getSchedulerApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodes . get ( nodeId )  ;", "}", "METHOD_END"], "methodName": ["getSchedulerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    currentAttempt . getApplicationId (  )  ;", "Application < T >    app    =    applications . get ( appId )  ;", "List < Container >    containerList    =    new   ArrayList < Container >  (  )  ;", "RMApp   appImpl    =    this . rmContext . getRMApps (  )  . get ( appId )  ;", "if    ( appImpl . getApplicationSubmissionContext (  )  . getUnmanagedAM (  )  )     {", "return   containerList ;", "}", "Collection < RMContainer >    liveContainers    =    app . getCurrentAppAttempt (  )  . getLiveContainers (  )  ;", "ContainerId   amContainerId    =    rmContext . getRMApps (  )  . get ( appId )  . getCurrentAppAttempt (  )  . getMasterContainer (  )  . getId (  )  ;", "for    ( RMContainer   rmContainer    :    liveContainers )     {", "if    (  !  ( rmContainer . getContainerId (  )  . equals ( amContainerId )  )  )     {", "containerList . add ( rmContainer . getContainer (  )  )  ;", "}", "}", "return   containerList ;", "}", "METHOD_END"], "methodName": ["getTransferredContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( container . getContainerState (  )  . equals ( COMPLETE )  )  )     {", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMNodeCleanContainerEvent ( node . getNodeID (  )  ,    container . getContainerId (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["killOrphanContainerOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    Container . newInstance ( status . getContainerId (  )  ,    node . getNodeID (  )  ,    node . getHttpAddress (  )  ,    status . getAllocatedResource (  )  ,    status . getPriority (  )  ,    null )  ;", "ApplicationAttemptId   attemptId    =    container . getId (  )  . getApplicationAttemptId (  )  ;", "RMContainer   rmContainer    =    new   RMContainerImpl ( container ,    attemptId ,    node . getNodeID (  )  ,    applications . get ( attemptId . getApplicationId (  )  )  . getUser (  )  ,    rmContext ,    status . getCreationTime (  )  )  ;", "return   rmContainer ;", "}", "METHOD_END"], "methodName": ["recoverAndCreateContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  !  ( rmContext . isWorkPreservingRecoveryEnabled (  )  )  )     |  |     ( containerReports    =  =    null )  )     |  |     (  ( containerReports    !  =    null )     &  &     ( containerReports . isEmpty (  )  )  )  )     {", "return ;", "}", "for    ( NMContainerStatus   container    :    containerReports )     {", "ApplicationId   appId    =    container . getContainerId (  )  . getApplicationAttemptId (  )  . getApplicationId (  )  ;", "RMApp   rmApp    =    rmContext . getRMApps (  )  . get ( appId )  ;", "if    ( rmApp    =  =    null )     {", ". LOG . error (  (  (  \" Skip   recovering   container    \"     +    container )     +     \"    for   unknown   application .  \"  )  )  ;", "killOrphanContainerOnNode ( nm ,    container )  ;", "continue ;", "}", "if    ( rmApp . getApplicationSubmissionContext (  )  . getUnmanagedAM (  )  )     {", ". LOG . info (  (  (  (  \" Skip   recovering   container    \"     +    container )     +     \"    for   unmanaged   AM .  \"  )     +     ( rmApp . getApplicationId (  )  )  )  )  ;", "killOrphanContainerOnNode ( nm ,    container )  ;", "continue ;", "}", "SchedulerApplication < T >    schedulerApp    =    applications . get ( appId )  ;", "if    ( schedulerApp    =  =    null )     {", ". LOG . info (  (  (  (  \" Skip   recovering   container       \"     +    container )     +     \"    for   unknown   SchedulerApplication .    Application   current   state   is    \"  )     +     ( rmApp . getState (  )  )  )  )  ;", "killOrphanContainerOnNode ( nm ,    container )  ;", "continue ;", "}", ". LOG . info (  (  \" Recovering   container    \"     +    container )  )  ;", "SchedulerApplicationAttempt   schedulerAttempt    =    schedulerApp . getCurrentAppAttempt (  )  ;", "if    (  !  ( rmApp . getApplicationSubmissionContext (  )  . getKeepContainersAcrossApplicationAttempts (  )  )  )     {", "if    (  ( schedulerAttempt . isStopped (  )  )     |  |     (  !  ( schedulerAttempt . getApplicationAttemptId (  )  . equals ( container . getContainerId (  )  . getApplicationAttemptId (  )  )  )  )  )     {", ". LOG . info (  (  (  \" Skip   recovering   container    \"     +    container )     +     \"    for   already   stopped   attempt .  \"  )  )  ;", "killOrphanContainerOnNode ( nm ,    container )  ;", "continue ;", "}", "}", "RMContainer   rmContainer    =    recoverAndCreateContainer ( container ,    nm )  ;", "rmContainer . handle ( new   RMContainerRecoverEvent ( container . getContainerId (  )  ,    container )  )  ;", "nodes . get ( nm . getNodeID (  )  )  . recoverContainer ( rmContainer )  ;", "Queue   queue    =    schedulerAttempt . getQueue (  )  ;", "queue . recoverContainer ( clusterResource ,    schedulerAttempt ,    rmContainer )  ;", "schedulerAttempt . recoverContainer ( rmContainer )  ;", "RMAppAttempt   appAttempt    =    rmApp . getCurrentAppAttempt (  )  ;", "if    ( appAttempt    !  =    null )     {", "Container   masterContainer    =    appAttempt . getMasterContainer (  )  ;", "if    (  ( masterContainer    !  =    null )     &  &     ( masterContainer . getId (  )  . equals ( rmContainer . getContainerId (  )  )  )  )     {", "(  ( RMContainerImpl )     ( rmContainer )  )  . setAMContainer ( true )  ;", "}", "}", "synchronized ( schedulerAttempt )     {", "Set < ContainerId >    releases    =    schedulerAttempt . getPendingRelease (  )  ;", "if    ( releases . contains ( container . getContainerId (  )  )  )     {", "rmContainer . handle ( new   RMContainerFinishedEvent ( container . getContainerId (  )  ,    SchedulerUtils . createAbnormalContainerStatus ( container . getContainerId (  )  ,    SchedulerUtils . RELEASED _ CONTAINER )  ,    RMContainerEventType . RELEASED )  )  ;", "releases . remove ( container . getContainerId (  )  )  ;", ". LOG . info (  (  ( container . getContainerId (  )  )     +     \"    is   released   by   application .  \"  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["recoverContainersOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    requests    =    rmContainer . getResourceRequests (  )  ;", "if    ( requests    =  =    null )     {", "return ;", "}", "SchedulerApplicationAttempt   Attempt    =    getCurrentAttemptForContainer ( rmContainer . getContainerId (  )  )  ;", "if    ( Attempt    !  =    null )     {", "Attempt . recoverResourceRequests ( requests )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverResourceRequestForContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "for    ( ContainerId   containerId    :    containers )     {", "RMContainer   rmContainer    =    getRMContainer ( containerId )  ;", "if    ( rmContainer    =  =    null )     {", "if    (  (  ( System . currentTimeMillis (  )  )     -     ( ResourceManager . getClusterTimeStamp (  )  )  )     <     ( nmExpireInterval )  )     {", ". LOG . info (  (  ( containerId    +     \"    doesn ' t   exist .    Add   the   container \"  )     +     \"    to   the   release   request   cache   as   it   maybe   on   recovery .  \"  )  )  ;", "synchronized ( attempt )     {", "attempt . getPendingRelease (  )  . add ( containerId )  ;", "}", "} else    {", "RMAuditLogger . logFailure ( attempt . getUser (  )  ,    RMAuditLogger . AuditConstants . RELEASE _ CONTAINER ,     \" Unauthorized   access   or   invalid   container \"  ,     \" Scheduler \"  ,     \" Trying   to   release   container   not   owned   by   app   or   with   invalid   id .  \"  ,    attempt . getApplicationId (  )  ,    containerId )  ;", "}", "}", "completedContainer ( rmContainer ,    SchedulerUtils . createAbnormalContainerStatus ( containerId ,    SchedulerUtils . RELEASED _ CONTAINER )  ,    RMContainerEventType . RELEASED )  ;", "}", "}", "METHOD_END"], "methodName": ["releaseContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"}, {"methodBody": ["METHOD_START", "{", "Set < ApplicationId >    userApps    =    usersApplications . get ( user )  ;", "if    ( userApps    =  =    null )     {", "userApps    =    new   HashSet < ApplicationId >  (  )  ;", "usersApplications . put ( user ,    userApps )  ;", "+  +  ( activeUsers )  ;", "metrics . incrActiveUsers (  )  ;", ". LOG . debug (  (  (  (  \" User    \"     +    user )     +     \"    added   to   activeUsers ,    currently :     \"  )     +     ( activeUsers )  )  )  ;", "}", "if    ( userApps . add ( applicationId )  )     {", "metrics . activateApp ( user )  ;", "}", "}", "METHOD_END"], "methodName": ["activateApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager"}, {"methodBody": ["METHOD_START", "{", "Set < ApplicationId >    userApps    =    usersApplications . get ( user )  ;", "if    ( userApps    !  =    null )     {", "if    ( userApps . remove ( applicationId )  )     {", "metrics . deactivateApp ( user )  ;", "}", "if    ( userApps . isEmpty (  )  )     {", "usersApplications . remove ( user )  ;", "-  -  ( activeUsers )  ;", "metrics . decrActiveUsers (  )  ;", ". LOG . debug (  (  (  (  \" User    \"     +    user )     +     \"    removed   from   activeUsers ,    currently :     \"  )     +     ( activeUsers )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["deactivateApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager"}, {"methodBody": ["METHOD_START", "{", "return   activeUsers ;", "}", "METHOD_END"], "methodName": ["getNumActiveUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager"}, {"methodBody": ["METHOD_START", "{", "return   fungibleContainers ;", "}", "METHOD_END"], "methodName": ["getContainerPreemptions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation"}, {"methodBody": ["METHOD_START", "{", "return   containers ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation"}, {"methodBody": ["METHOD_START", "{", "return   nmTokens ;", "}", "METHOD_END"], "methodName": ["getNMTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation"}, {"methodBody": ["METHOD_START", "{", "return   resourceLimit ;", "}", "METHOD_END"], "methodName": ["getResourceLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation"}, {"methodBody": ["METHOD_START", "{", "return   fungibleResources ;", "}", "METHOD_END"], "methodName": ["getResourcePreemptions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation"}, {"methodBody": ["METHOD_START", "{", "return   strictContainers ;", "}", "METHOD_END"], "methodName": ["getStrictContainerPreemptions"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    resourceRequests    =    new   ArrayList < ResourceRequest >  (  )  ;", "if    ( type    =  =     ( NodeType . NODE _ LOCAL )  )     {", "allocateNodeLocal ( node ,    priority ,    request ,    container ,    resourceRequests )  ;", "} else", "if    ( type    =  =     ( NodeType . RACK _ LOCAL )  )     {", "allocateRackLocal ( node ,    priority ,    request ,    container ,    resourceRequests )  ;", "} else    {", "allocateOffSwitch ( node ,    priority ,    request ,    container ,    resourceRequests )  ;", "}", "QueueMetrics   metrics    =    queue . getMetrics (  )  ;", "if    ( pending )     {", "pending    =    false ;", "metrics . runAppAttempt ( applicationId ,    user )  ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  \" allocate :    applicationId =  \"     +     ( applicationId )  )     +     \"    container =  \"  )     +     ( container . getId (  )  )  )     +     \"    host =  \"  )     +     ( container . getNodeId (  )  . toString (  )  )  )     +     \"    user =  \"  )     +     ( user )  )     +     \"    resource =  \"  )     +     ( request . getCapability (  )  )  )  )  ;", "}", "metrics . allocateResources ( user ,     1  ,    request . getCapability (  )  ,    true )  ;", "return   resourceRequests ;", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "nodeLocalRequest . setNumContainers (  (  ( nodeLocalRequest . getNumContainers (  )  )     -     1  )  )  ;", "if    (  ( nodeLocalRequest . getNumContainers (  )  )     =  =     0  )     {", "this . requests . get ( priority )  . remove ( node . getNodeName (  )  )  ;", "}", "ResourceRequest   rackLocalRequest    =    requests . get ( priority )  . get ( node . getRackName (  )  )  ;", "rackLocalRequest . setNumContainers (  (  ( rackLocalRequest . getNumContainers (  )  )     -     1  )  )  ;", "if    (  ( rackLocalRequest . getNumContainers (  )  )     =  =     0  )     {", "this . requests . get ( priority )  . remove ( node . getRackName (  )  )  ;", "}", "ResourceRequest   offRackRequest    =    requests . get ( priority )  . get ( ANY )  ;", "decrementOutstanding ( offRackRequest )  ;", "Requests . add ( cloneResourceRequest ( nodeLocalRequest )  )  ;", "Requests . add ( cloneResourceRequest ( rackLocalRequest )  )  ;", "Requests . add ( cloneResourceRequest ( offRackRequest )  )  ;", "}", "METHOD_END"], "methodName": ["allocateNodeLocal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "decrementOutstanding ( offSwitchRequest )  ;", "Requests . add ( cloneResourceRequest ( offSwitchRequest )  )  ;", "}", "METHOD_END"], "methodName": ["allocateOffSwitch"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "rackLocalRequest . setNumContainers (  (  ( rackLocalRequest . getNumContainers (  )  )     -     1  )  )  ;", "if    (  ( rackLocalRequest . getNumContainers (  )  )     =  =     0  )     {", "this . requests . get ( priority )  . remove ( node . getRackName (  )  )  ;", "}", "ResourceRequest   offRackRequest    =    requests . get ( priority )  . get ( ANY )  ;", "decrementOutstanding ( offRackRequest )  ;", "Requests . add ( cloneResourceRequest ( rackLocalRequest )  )  ;", "Requests . add ( cloneResourceRequest ( offRackRequest )  )  ;", "}", "METHOD_END"], "methodName": ["allocateRackLocal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "boolean   deactivate    =    true ;", "for    ( Priority   priority    :    getPriorities (  )  )     {", "RRequest   request    =    getRRequest ( priority ,    ANY )  ;", "if    (  ( request . getNumContainers (  )  )     >     0  )     {", "deactivate    =    false ;", "break ;", "}", "}", "if    ( deactivate )     {", "activeUsersManager . deactivateApplication ( user ,    applicationId )  ;", "}", "}", "METHOD_END"], "methodName": ["checkForDeactivation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "priorities . clear (  )  ;", "requests . clear (  )  ;", ". LOG . info (  (  (  \" Application    \"     +     ( applicationId )  )     +     \"    requests   cleared \"  )  )  ;", "}", "METHOD_END"], "methodName": ["clearRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   newRequest    =    ResourceRequest . newInstance ( request . getPriority (  )  ,    request . getResourceName (  )  ,    request . getCapability (  )  ,     1  ,    request . getRelaxLocality (  )  )  ;", "return   newRequest ;", "}", "METHOD_END"], "methodName": ["cloneResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "int   numOffSwitchContainers    =     ( offSwitchRequest . getNumContainers (  )  )     -     1  ;", "offSwitchRequesttNumContainers ( numOffSwitchContainers )  ;", "if    ( numOffSwitchContainers    =  =     0  )     {", "checkForDeactivation (  )  ;", "}", "}", "METHOD_END"], "methodName": ["decrementOutstanding"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    ret    =    new   ArrayList < ResourceRequest >  (  )  ;", "for    ( Map < String ,    ResourceRequest >    r    :    requests . values (  )  )     {", "ret . addAll ( r . values (  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getAllResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . blacklist ;", "}", "METHOD_END"], "methodName": ["getBlackList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . containerIdCounter . incrementAndGet (  )  ;", "}", "METHOD_END"], "methodName": ["getNewContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   priorities ;", "}", "METHOD_END"], "methodName": ["getPriorities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   queueName ;", "}", "METHOD_END"], "methodName": ["getQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    getResourceRequest ( priority ,    ANY )  ;", "return   request . getCapability (  )  ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    ResourceRequest >    nodeRequests    =    requests . get ( priority )  ;", "return   nodeRequests    =  =    null    ?    null    :    nodeRequests . get ( Name )  ;", "}", "METHOD_END"], "methodName": ["getResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   requests . get ( priority )  ;", "}", "METHOD_END"], "methodName": ["getResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   blacklist . contains ( resourceName )  ;", "}", "METHOD_END"], "methodName": ["isBlacklisted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   pending ;", "}", "METHOD_END"], "methodName": ["isPending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   oldMetrics    =    queue . getMetrics (  )  ;", "QueueMetrics   newMetrics    =    newQueue . getMetrics (  )  ;", "for    ( Map < String ,    RRequest >    asks    :    requests . values (  )  )     {", "RRequest   request    =    asks . get ( ANY )  ;", "if    ( request    !  =    null )     {", "oldMetrics . decrPendingRs ( user ,    request . getNumContainers (  )  ,    request . getCapability (  )  )  ;", "newMetrics . incrPendingRs ( user ,    request . getNumContainers (  )  ,    request . getCapability (  )  )  ;", "}", "}", "oldMetrics . moveAppFrom ( this )  ;", "newMetrics . moveAppTo ( this )  ;", "activeUsersManager . deactivateApplication ( user ,    applicationId )  ;", "activeUsersManager    =    newQueue . getActiveUsersManager (  )  ;", "activeUsersManager . activateApplication ( user ,    applicationId )  ;", "this . queue    =    newQueue ;", "this . queueName    =    newQueue . getQueueName (  )  ;", "}", "METHOD_END"], "methodName": ["move"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   metrics    =    queue . getMetrics (  )  ;", "if    ( pending )     {", "pending    =    false ;", "metrics . runAppAttempt ( applicationId ,    user )  ;", "}", "if    ( rmContainer . getState (  )  . equals ( RMContainerState . COMPLETED )  )     {", "return ;", "}", "metrics . allocateRs ( user ,     1  ,    rmContainer . getAllocatedR (  )  ,    false )  ;", "}", "METHOD_END"], "methodName": ["recoverContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "this . queue    =    queue ;", "}", "METHOD_END"], "methodName": ["setQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   metrics    =    queue . getMetrics (  )  ;", "for    ( Map < String ,    RRequest >    asks    :    requests . values (  )  )     {", "RRequest   request    =    asks . get ( ANY )  ;", "if    ( request    !  =    null )     {", "metrics . decrPendingRs ( user ,    request . getNumContainers (  )  ,    request . getCapability (  )  )  ;", "}", "}", "metrics . finishAppAttempt ( applicationId ,    pending ,    user )  ;", "clearRequests (  )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "this . blacklist    =    appInfo . getBlackList (  )  ;", "}", "METHOD_END"], "methodName": ["transferStateFromPreviousAppSchedulingInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "if    ( blacklistAdditions    !  =    null )     {", "blacklist . addAll ( blacklistAdditions )  ;", "}", "if    ( blacklistRemovals    !  =    null )     {", "blacklistmoveAll ( blacklistRemovals )  ;", "}", "}", "METHOD_END"], "methodName": ["updateBlacklist"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   metrics    =    queue . getMetrics (  )  ;", "for    ( ResourceRequest   request    :    requests )     {", "Priority   priority    =    request . getPriority (  )  ;", "String   resourceName    =    request . getResourceName (  )  ;", "boolean   updatePendingResources    =    false ;", "ResourceRequest   lastRequest    =    null ;", "if    ( resourceName . equals ( ANY )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  \" update :  \"     +     \"    application =  \"  )     +     ( applicationId )  )     +     \"    request =  \"  )     +    request )  )  ;", "}", "updatePendingResources    =    true ;", "if    (  ( request . getNumContainers (  )  )     >     0  )     {", "activeUsersManager . activateApplication ( user ,    applicationId )  ;", "}", "}", "Map < String ,    ResourceRequest >    asks    =    this . requests . get ( priority )  ;", "if    ( asks    =  =    null )     {", "asks    =    new   HashMap < String ,    ResourceRequest >  (  )  ;", "this . requests . put ( priority ,    asks )  ;", "this . priorities . add ( priority )  ;", "}", "lastRequest    =    asks . get ( resourceName )  ;", "if    ( recoverPreemptedRequest    &  &     ( lastRequest    !  =    null )  )     {", "request . setNumContainers (  (  ( lastRequest . getNumContainers (  )  )     +     1  )  )  ;", "}", "asks . put ( resourceName ,    request )  ;", "if    ( updatePendingResources )     {", "if    (  ( request . getNumContainers (  )  )     <  =     0  )     {", ". LOG . info (  \" checking   for   deactivate .  .  .     \"  )  ;", "checkForDeactivation (  )  ;", "}", "int   lastRequestContainers    =     ( lastRequest    !  =    null )     ?    lastRequest . getNumContainers (  )     :     0  ;", "Resource   lastRequestCapability    =     ( lastRequest    !  =    null )     ?    lastRequest . getCapability (  )     :    Resources . none (  )  ;", "metrics . incrPendingResources ( user ,    request . getNumContainers (  )  ,    request . getCapability (  )  )  ;", "metrics . decrPendingResources ( user ,    lastRequestContainers ,    lastRequestCapability )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updateResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"}, {"methodBody": ["METHOD_START", "{", "return   aid ;", "}", "METHOD_END"], "methodName": ["getAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . container ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEvent"}, {"methodBody": ["METHOD_START", "{", "return   numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeReport"}, {"methodBody": ["METHOD_START", "{", "return   usedResources ;", "}", "METHOD_END"], "methodName": ["getUsedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeReport"}, {"methodBody": ["METHOD_START", "{", "return   this . completed ;", "}", "METHOD_END"], "methodName": ["getCompletedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeResponse"}, {"methodBody": ["METHOD_START", "{", "return   this . toCleanUp ;", "}", "METHOD_END"], "methodName": ["getContainersToCleanUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeResponse"}, {"methodBody": ["METHOD_START", "{", "return   this . finishedApplications ;", "}", "METHOD_END"], "methodName": ["getFinishedApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeResponse"}, {"methodBody": ["METHOD_START", "{", "pendingContainers . decr ( containers )  ;", "pendingMB . decr (  (  (  . getMemory (  )  )     *    containers )  )  ;", "pendingVCo . decr (  (  (  . getVirtualCo (  )  )     *    containers )  )  ;", "}", "METHOD_END"], "methodName": ["_decrPendingResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "pendingContainers . incr ( containers )  ;", "pendingMB . incr (  (  (  . getMemory (  )  )     *    containers )  )  ;", "pendingVCo . incr (  (  (  . getVirtualCo (  )  )     *    containers )  )  ;", "}", "METHOD_END"], "methodName": ["_incrPendingResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "activeApplications . incr (  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . activateApp ( user )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . activateApp ( user )  ;", "}", "}", "METHOD_END"], "methodName": ["activateApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "allocatedContainers . incr ( containers )  ;", "aggregateContainersAllocated . incr ( containers )  ;", "allocatedMB . incr (  (  ( res . getMemory (  )  )     *    containers )  )  ;", "allocatedVCores . incr (  (  ( res . getVirtualCores (  )  )     *    containers )  )  ;", "if    ( decrPending )     {", "_ decrPendingResources ( containers ,    res )  ;", "}", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . allocateResources ( user ,    containers ,    res ,    decrPending )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . allocateResources ( user ,    containers ,    res ,    decrPending )  ;", "}", "}", "METHOD_END"], "methodName": ["allocateResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "ArrayList < Integer >    buckets    =    parseInts ( conf . get ( RM _ METRICS _ RUNTIME _ BUCKETS ,    DEFAULT _ RM _ METRICS _ RUNTIME _ BUCKETS )  )  ;", "MutableGaugeInt [  ]    result    =    new   MutableGaugeInt [  ( buckets . size (  )  )     +     1  ]  ;", "result [  0  ]     =    registry . newGauge (  \" running _  0  \"  ,     \"  \"  ,     0  )  ;", "long [  ]    cuts    =    new   long [ buckets . size (  )  ]  ;", "for    ( int   i    =     0  ;    i    <     ( buckets . size (  )  )  ;     +  + i )     {", "result [  ( i    +     1  )  ]     =    registry . newGauge (  (  \" running _  \"     +     ( buckets . get ( i )  )  )  ,     \"  \"  ,     0  )  ;", "cuts [ i ]     =     (  ( buckets . get ( i )  )     *     1  0  0  0 L )     *     6  0  ;", "}", "this . runBuckets    =    new   TimeBucketMetrics < api . records . ApplicationId >  ( cuts )  ;", "return   result ;", "}", "METHOD_END"], "methodName": ["buildBuckets"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics . queueMetrics . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clearQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "activeApplications . decr (  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . deactivateApp ( user )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . deactivateApp ( user )  ;", "}", "}", "METHOD_END"], "methodName": ["deactivateApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "activeUsers . decr (  )  ;", "}", "METHOD_END"], "methodName": ["decrActiveUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "_ decrPendingResources ( containers ,    res )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . decrPendingResources ( user ,    containers ,    res )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . decrPendingResources ( user ,    containers ,    res )  ;", "}", "}", "METHOD_END"], "methodName": ["decrPendingResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "switch    ( rmAppFinalState )     {", "case   KILLED    :", "appsKilled . incr (  )  ;", "break ;", "case   FAILED    :", "appsFailed . incr (  )  ;", "break ;", "default    :", "appsCompleted . incr (  )  ;", "break ;", "}", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . finishApp ( user ,    rmAppFinalState )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . finishApp ( user ,    rmAppFinalState )  ;", "}", "}", "METHOD_END"], "methodName": ["finishApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "runBuckets . remove ( appId )  ;", "if    ( isPending )     {", "appsPending . decr (  )  ;", "} else    {", "appsRunning . decr (  )  ;", "}", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . finishAppAttempt ( appId ,    isPending ,    user )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . finishAppAttempt ( appId ,    isPending ,    user )  ;", "}", "}", "METHOD_END"], "methodName": ["finishAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   QueueMetrics . forQueue ( DefaultMetricsSystem . instance (  )  ,    queueName ,    parent ,    enableUserMetrics ,    conf )  ;", "}", "METHOD_END"], "methodName": ["forQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   metrics    =    QueueMetrics . queueMetrics . get ( queueName )  ;", "if    ( metrics    =  =    null )     {", "metrics    =    new   QueueMetrics ( ms ,    queueName ,    parent ,    enableUserMetrics ,    conf )  . tag ( QueueMetrics . QUEUE _ INFO ,    queueName )  ;", "if    ( ms    !  =    null )     {", "metrics    =    ms . register ( QueueMetrics . sourceName ( queueName )  . toString (  )  ,     (  \" Metrics   for   queue :     \"     +    queueName )  ,    metrics )  ;", "}", "QueueMetrics . queueMetrics . put ( queueName ,    metrics )  ;", "}", "return   metrics ;", "}", "METHOD_END"], "methodName": ["forQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   activeApplications . value (  )  ;", "}", "METHOD_END"], "methodName": ["getActiveApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   activeUsers . value (  )  ;", "}", "METHOD_END"], "methodName": ["getActiveUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   allocatedContainers . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAllocatedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   allocatedMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAllocatedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newResource ( allocatedMB . value (  )  ,    allocatedVCores . value (  )  )  ;", "}", "METHOD_END"], "methodName": ["getAllocatedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   allocatedVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAllocatedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   appsCompleted . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsCompleted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   appsFailed . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   appsKilled . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   appsPending . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsPending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   appsRunning . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   appsSubmitted . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsSubmitted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   availableMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAvailableMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   availableVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getAvailableVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "updateRunningTime (  )  ;", "gistry . snapshot ( collector . addRecord ( gistry . info (  )  )  ,    all )  ;", "}", "METHOD_END"], "methodName": ["getMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   metricsSystem ;", "}", "METHOD_END"], "methodName": ["getMetricsSystem"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   pendingContainers . value (  )  ;", "}", "METHOD_END"], "methodName": ["getPendingContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   pendingMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getPendingMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   pendingVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getPendingVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   reservedContainers . value (  )  ;", "}", "METHOD_END"], "methodName": ["getReservedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   reservedMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getReservedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   reservedVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getReservedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "if    (  ( users )     =  =    null )     {", "return   null ;", "}", "metrics    =    users . get ( userName )  ;", "if    ( metrics    =  =    null )     {", "metrics    =    new    ( metricsSystem ,    queueName ,    null ,    false ,    conf )  ;", "users . put ( userName ,    metrics )  ;", "metricsSystem . register (  . sourceName ( queueName )  . append (  \"  , user =  \"  )  . append ( userName )  . toString (  )  ,     (  (  (  (  \" Metrics   for   user    '  \"     +    userName )     +     \"  '    in   queue    '  \"  )     +     ( queueName )  )     +     \"  '  \"  )  ,    metrics . tag (  . QUEUE _ INFO ,    queueName )  . tag (  . USER _ INFO ,    userName )  )  ;", "}", "return   metrics ;", "}", "METHOD_END"], "methodName": ["getUserMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "activeUsers . incr (  )  ;", "}", "METHOD_END"], "methodName": ["incrActiveUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "_ incrPendingResources ( containers ,    res )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . incrPendingResources ( user ,    containers ,    res )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . incrPendingResources ( user ,    containers ,    res )  ;", "}", "}", "METHOD_END"], "methodName": ["incrPendingResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "if    ( app . isPending (  )  )     {", "appsPending . decr (  )  ;", "} else    {", "appsRunning . decr (  )  ;", "}", "userMetrics    =    getUserMetrics ( app . getUser (  )  )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . moveAppFrom ( app )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . moveAppFrom ( app )  ;", "}", "}", "METHOD_END"], "methodName": ["moveAppFrom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "if    ( app . isPending (  )  )     {", "appsPending . incr (  )  ;", "} else    {", "appsRunning . incr (  )  ;", "}", "userMetrics    =    getUserMetrics ( app . getUser (  )  )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . moveAppTo ( app )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . moveAppTo ( app )  ;", "}", "}", "METHOD_END"], "methodName": ["moveAppTo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "ArrayList < Inte   result    =    new   ArrayList < Inte (  )  ;", "for    ( String   s    :    value . split (  \"  ,  \"  )  )     {", "result . add ( InteparseInt ( s . trim (  )  )  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["parseInts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "allocatedContainers . decr ( containers )  ;", "aggregateContainersReleased . incr ( containers )  ;", "allocatedMB . decr (  (  ( res . getMemory (  )  )     *    containers )  )  ;", "allocatedVCores . decr (  (  ( res . getVirtualCores (  )  )     *    containers )  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . releaseResources ( user ,    containers ,    res )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . releaseResources ( user ,    containers ,    res )  ;", "}", "}", "METHOD_END"], "methodName": ["releaseResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "reservedContainers . incr (  )  ;", "reservedMB . incr ( res . getMemory (  )  )  ;", "reservedVCores . incr ( res . getVirtualCores (  )  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . reserveResource ( user ,    res )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . reserveResource ( user ,    res )  ;", "}", "}", "METHOD_END"], "methodName": ["reserveResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "runBuckets . add ( appId ,    System . currentTimeMillis (  )  )  ;", "appsRunning . incr (  )  ;", "appsPending . decr (  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . runAppAttempt ( appId ,    user )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . runAppAttempt ( appId ,    user )  ;", "}", "}", "METHOD_END"], "methodName": ["runAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "availableMB . set ( limit . getMemory (  )  )  ;", "availableVCores . set ( limit . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["setAvailableResourcesToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . setAvailableResourcesToQueue ( limit )  ;", "}", "}", "METHOD_END"], "methodName": ["setAvailableResourcesToUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder ( QueueMetrics . RECORD _ INFO . name (  )  )  ;", "int   i    =     0  ;", "for    ( String   node    :    QueueMetrics . Q _ SPLITTER . split ( queueName )  )     {", "sb . append (  \"  , q \"  )  . append (  ( i +  +  )  )  . append (  '  =  '  )  . append ( node )  ;", "}", "return   sb ;", "}", "METHOD_END"], "methodName": ["sourceName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "appsSubmitted . incr (  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . submitApp ( user )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . submitApp ( user )  ;", "}", "}", "METHOD_END"], "methodName": ["submitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "appsPending . incr (  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . submitAppAttempt ( user )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . submitAppAttempt ( user )  ;", "}", "}", "METHOD_END"], "methodName": ["submitAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "registry . tag ( info ,    value )  ;", "return   this ;", "}", "METHOD_END"], "methodName": ["tag"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "reservedContainers . decr (  )  ;", "reservedMB . decr ( res . getMemory (  )  )  ;", "reservedVCores . decr ( res . getVirtualCores (  )  )  ;", "userMetrics    =    getUserMetrics ( user )  ;", "if    ( userMetrics    !  =    null )     {", "userMetrics . unreserveResource ( user ,    res )  ;", "}", "if    (  ( parent )     !  =    null )     {", "parent . unreserveResource ( user ,    res )  ;", "}", "}", "METHOD_END"], "methodName": ["unreserveResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "int [  ]    counts    =    runBuckets . getBucketCounts ( System . currentTimeMillis (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( counts . length )  ;     +  + i )     {", "runningTime [ i ] t ( counts [ i ]  )  ;", "}", "}", "METHOD_END"], "methodName": ["updateRunningTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   live ;", "}", "METHOD_END"], "methodName": ["getLiveContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport"}, {"methodBody": ["METHOD_START", "{", "return   reserved ;", "}", "METHOD_END"], "methodName": ["getReservedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport"}, {"methodBody": ["METHOD_START", "{", "return   pending ;", "}", "METHOD_END"], "methodName": ["isPending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport"}, {"methodBody": ["METHOD_START", "{", "if    ( application . isBlacklisted ( node . getNodeName (  )  )  )     {", "if    ( LOG . isDebugEnabled (  )  )     {", "LOG . debug (  (  (  (  (  \" Skipping    ' host '     \"     +     ( node . getNodeName (  )  )  )     +     \"    for    \"  )     +     ( application . getlicationId (  )  )  )     +     \"    since   it   has   been   blacklisted \"  )  )  ;", "}", "return   true ;", "}", "if    ( application . isBlacklisted ( node . getRackName (  )  )  )     {", "if    ( LOG . isDebugEnabled (  )  )     {", "LOG . debug (  (  (  (  (  \" Skipping    ' rack '     \"     +     ( node . getRackName (  )  )  )     +     \"    for    \"  )     +     ( application . getlicationId (  )  )  )     +     \"    since   it   has   been   blacklisted \"  )  )  ;", "}", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isBlacklisted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppUtils"}, {"methodBody": ["METHOD_START", "{", "return   currentAttempt ;", "}", "METHOD_END"], "methodName": ["getCurrentAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication"}, {"methodBody": ["METHOD_START", "{", "this . currentAttempt    =    currentAttempt ;", "}", "METHOD_END"], "methodName": ["setCurrentAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication"}, {"methodBody": ["METHOD_START", "{", "this . queue    =    queue ;", "}", "METHOD_END"], "methodName": ["setQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication"}, {"methodBody": ["METHOD_START", "{", "queue . getMetrics (  )  . finishApp ( user ,    rmAppFinalState )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication"}, {"methodBody": ["METHOD_START", "{", "reReservations . add ( priority )  ;", "}", "METHOD_END"], "methodName": ["addReReservation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "schedulingOpportunities . setCount ( priority ,     (  ( schedulingOpportunities . count ( priority )  )     +     1  )  )  ;", "}", "METHOD_END"], "methodName": ["addSchedulingOpportunity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "RMContainer   rmContainer    =    getRMContainer ( containerId )  ;", "if    ( rmContainer    =  =    null )     {", "rmContext . getDispatr (  )  . getEventHandler (  )  . handle ( new   RMNodeCleanContainerEvent ( nodeId ,    containerId )  )  ;", "return ;", "}", "rmContainer . handle ( new   RMContainerEvent ( containerId ,    RMContainerEventType . LAUNCHED )  )  ;", "}", "METHOD_END"], "methodName": ["containerLaunchedOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   amResource ;", "}", "METHOD_END"], "methodName": ["getAMResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getApplicationAttemptId (  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getApplicationId (  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   currentConsumption ;", "}", "METHOD_END"], "methodName": ["getCurrentConsumption"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   currentReservation ;", "}", "METHOD_END"], "methodName": ["getCurrentReservation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceLimit . getMemory (  )  )     <     0  )     {", "resourceLimit . setMemory (  0  )  ;", "}", "return   resourceLimit ;", "}", "METHOD_END"], "methodName": ["getHeadroom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   this . lastScheduledContainer ;", "}", "METHOD_END"], "methodName": ["getLastScheduledContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   new   ArrayList < RMContainer >  ( liveContainers . values (  )  )  ;", "}", "METHOD_END"], "methodName": ["getLiveContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   this . liveContainers ;", "}", "METHOD_END"], "methodName": ["getLiveContainersMap"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getNewContainerId (  )  ;", "}", "METHOD_END"], "methodName": ["getNewContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "Map < NodeId ,    RMContainer >    reservedContainers    =    this . reservedContainers . get ( priority )  ;", "return   reservedContainers    =  =    null    ?     0     :    reservedContainers . size (  )  ;", "}", "METHOD_END"], "methodName": ["getNumReservedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   this . pendingRelease ;", "}", "METHOD_END"], "methodName": ["getPendingRelease"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getPriorities (  )  ;", "}", "METHOD_END"], "methodName": ["getPriorities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getQueueName (  )  ;", "}", "METHOD_END"], "methodName": ["getQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   liveContainers . get ( id )  ;", "}", "METHOD_END"], "methodName": ["getRMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   reReservations . count ( priority )  ;", "}", "METHOD_END"], "methodName": ["getReReservations"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "List < RMContainer >    reservedContainers    =    new   ArrayList < RMContainer >  (  )  ;", "for    ( Map . Entry < Priority ,    Map < NodeId ,    RMContainer >  >    e    :    this . reservedContainers . entrySet (  )  )     {", "reservedContainers . addAll ( e . getValue (  )  . values (  )  )  ;", "}", "return   reservedContainers ;", "}", "METHOD_END"], "methodName": ["getReservedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getResource ( priority )  ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceLimit ;", "}", "METHOD_END"], "methodName": ["getResourceLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   this . appSchedulingInfo . getResourceRequest ( priority ,    resourceName )  ;", "}", "METHOD_END"], "methodName": ["getResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getResourceRequests ( priority )  ;", "}", "METHOD_END"], "methodName": ["getResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationResourceUsageReport . newInstance ( liveContainers . size (  )  ,    reservedContainers . size (  )  ,    Resources . clone ( currentConsumption )  ,    Resources . clone ( currentReservation )  ,    Resources . add ( currentConsumption ,    currentReservation )  )  ;", "}", "METHOD_END"], "methodName": ["getResourceUsageReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   schedulingOpportunities . count ( priority )  ;", "}", "METHOD_END"], "methodName": ["getSchedulingOpportunities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   getResourceRequest ( priority ,    ANY )  . getNumContainers (  )  ;", "}", "METHOD_END"], "methodName": ["getTotalRequiredResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   unmanagedAM ;", "}", "METHOD_END"], "methodName": ["getUnmanagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . getUser (  )  ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   amRunning ;", "}", "METHOD_END"], "methodName": ["isAmRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   this . appSchedulingInfo . isBlacklisted ( resourceName )  ;", "}", "METHOD_END"], "methodName": ["isBlacklisted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   appSchedulingInfo . isPending (  )  ;", "}", "METHOD_END"], "methodName": ["isPending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "Map < NodeId ,    RMContainer >    reservedContainers    =    this . reservedContainers . get ( priority )  ;", "if    ( reservedContainers    !  =    null )     {", "return   reservedContainers . containsKey ( node . getNodeID (  )  )  ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isReserved"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "return   isStopped ;", "}", "METHOD_END"], "methodName": ["isStopped"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   oldMetrics    =    queue . getMetrics (  )  ;", "QueueMetrics   newMetrics    =    newQueue . getMetrics (  )  ;", "String   user    =    getUser (  )  ;", "for    ( RMContainer   liveContainer    :    liveContainers . values (  )  )     {", "Resource       =    liveContainer . getContainer (  )  . getResource (  )  ;", "oldMetrics . releaseResources ( user ,     1  ,     )  ;", "newMetrics . allocateResources ( user ,     1  ,     ,    false )  ;", "}", "for    ( Map < NodeId ,    RMContainer >    map    :    reservedContainers . values (  )  )     {", "for    ( RMContainer   reservedContainer    :    map . values (  )  )     {", "Resource       =    reservedContainer . getReservedResource (  )  ;", "oldMetrics . unreserveResource ( user ,     )  ;", "newMetrics . reserveResource ( user ,     )  ;", "}", "}", "appSchedulingInfo . move ( newQueue )  ;", "this . queue    =    newQueue ;", "}", "METHOD_END"], "methodName": ["move"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "List < Container >    returnContainerList    =    new   ArrayList < Container >  ( newlyAllocatedContainers . size (  )  )  ;", "List < NMToken >    nmTokens    =    new   ArrayList < NMToken >  (  )  ;", "for    ( Iterator < RMContainer >    i    =    newlyAllocatedContainers . iterator (  )  ;    i . hasNext (  )  ;  )     {", "RMContainer   rmContainer    =    i . next (  )  ;", "Container   container    =    rmContainer . getContainer (  )  ;", "try    {", "container . setContainerToken ( rmContext . getContainerTokenSecretManager (  )  . createContainerToken ( container . getId (  )  ,    container . getNodeId (  )  ,    getUser (  )  ,    container . getResource (  )  ,    container . getPriority (  )  ,    rmContainer . getCreationTime (  )  )  )  ;", "NMToken   nmToken    =    rmContext . getNMTokenSecretManager (  )  . createAndGetNMToken ( getUser (  )  ,    getApplicationAttemptId (  )  ,    container )  ;", "if    ( nmToken    !  =    null )     {", "nmTokens . add ( nmToken )  ;", "}", "}    catch    ( IllegalArgumentException   e )     {", ". LOG . error (  (  (  \" Error   trying   to   assign   container   token   and   NM   token   to \"     +     \"    an   allocated   container    \"  )     +     ( container . getId (  )  )  )  ,    e )  ;", "continue ;", "}", "returnContainerList . add ( container )  ;", "i . remove (  )  ;", "rmContainer . handle ( new   RMContainerEvent ( rmContainer . getContainerId (  )  ,    RMContainerEventType . ACQUIRED )  )  ;", "}", "return   new    . ContainersAndNMTokensAllocation ( returnContainerList ,    nmTokens )  ;", "}", "METHOD_END"], "methodName": ["pullNewlyAllocatedContainersAndNMTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "appSchedulingInfo . recoverContainer ( rmContainer )  ;", "if    ( rmContainer . getState (  )  . equals ( RMContainerState . COMPLETED )  )     {", "return ;", "}", ". LOG . info (  (  (  (  \" SchedulerAttempt    \"     +     ( getApplicationAttemptId (  )  )  )     +     \"    is   recovering   container    \"  )     +     ( rmContainer . getContainerId (  )  )  )  )  ;", "liveContainers . put ( rmContainer . getContainerId (  )  ,    rmContainer )  ;", "Resources . addTo ( currentConsumption ,    rmContainer . getContainer (  )  . getResource (  )  )  ;", "}", "METHOD_END"], "methodName": ["recoverContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isStopped )  )     {", "appingInfo . updateResourceRequests ( requests ,    true )  ;", "}", "}", "METHOD_END"], "methodName": ["recoverResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( rmContainer    =  =    null )     {", "rmContainer    =    new   RMContainerImpl ( container ,    getApplicationAttemptId (  )  ,    node . getNodeID (  )  ,    appSchedulingInfo . getUser (  )  ,    rmContext )  ;", "Resources . addTo ( currentReservation ,    container . getResource (  )  )  ;", "resetReReservations ( priority )  ;", "} else    {", "addReReservation ( priority )  ;", "}", "rmContainer . handle ( new   RMContainerReservedEvent ( container . getId (  )  ,    container . getResource (  )  ,    node . getNodeID (  )  ,    priority )  )  ;", "Map < NodeId ,    RMContainer >    reservedContainers    =    this . reservedContainers . get ( priority )  ;", "if    ( reservedContainers    =  =    null )     {", "reservedContainers    =    new   HashMap < NodeId ,    RMContainer >  (  )  ;", "this . reservedContainers . put ( priority ,    reservedContainers )  ;", "}", "reservedContainers . put ( node . getNodeID (  )  ,    rmContainer )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  \" Application   attempt    \"     +     ( getApplicationAttemptId (  )  )  )     +     \"    reserved   container    \"  )     +    rmContainer )     +     \"    on   node    \"  )     +    node )     +     \"  .    This   attempt   currently   has    \"  )     +     ( reservedContainers . size (  )  )  )     +     \"    reserved   containers   at   priority    \"  )     +    priority )     +     \"  ;    currentReservation    \"  )     +     ( currentReservation . getMemory (  )  )  )  )  ;", "}", "return   rmContainer ;", "}", "METHOD_END"], "methodName": ["reserve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "reReservations . setCount ( priority ,     0  )  ;", "}", "METHOD_END"], "methodName": ["resetReReservations"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "resetSchedulingOpportunities ( priority ,    System . currentTimeMillis (  )  )  ;", "}", "METHOD_END"], "methodName": ["resetSchedulingOpportunities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "lastScheduledContainer . put ( priority ,    currentTimeMs )  ;", "schedulingOpportunities . setCount ( priority ,     0  )  ;", "}", "METHOD_END"], "methodName": ["resetSchedulingOpportunities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "this . amResource    =    amResource ;", "}", "METHOD_END"], "methodName": ["setAMResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "amRunning    =    bool ;", "}", "METHOD_END"], "methodName": ["setAmRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "this . resourceLimit    =    globalLimit ;", "}", "METHOD_END"], "methodName": ["setHeadroom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( SchedulerApplicationAttempt . LOG . isDebugEnabled (  )  )     {", "for    ( Priority   priority    :    getPriorities (  )  )     {", "Map < String ,    ResourceRequest >    requests    =    getResourceRequests ( priority )  ;", "if    ( requests    !  =    null )     {", "SchedulerApplicationAttempt . LOG . debug (  (  (  (  (  (  (  \" showRequests :  \"     +     \"    application =  \"  )     +     ( getApplicationId (  )  )  )     +     \"    headRoom =  \"  )     +     ( getHeadroom (  )  )  )     +     \"    currentConsumption =  \"  )     +     ( currentConsumption . getMemory (  )  )  )  )  ;", "for    ( ResourceRequest   request    :    requests . values (  )  )     {", "SchedulerApplicationAttempt . LOG . debug (  (  (  (  (  \" showRequests :  \"     +     \"    application =  \"  )     +     ( getApplicationId (  )  )  )     +     \"    request =  \"  )     +    request )  )  ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["showRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "isStopped    =    true ;", "appingInfo . stop ( rmAppAttemptFinalState )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "int   count    =    ingOpportunities . count ( priority )  )     -     1  ;", "thisingOpportunities . setCount ( priority ,    Math . max ( count ,     0  )  )  ;", "}", "METHOD_END"], "methodName": ["subtractSchedulingOpportunity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "this . liveContainers    =    appAttempt . getLiveContainersMap (  )  ;", "this . currentConsumption    =    appAttempt . getCurrentConsumption (  )  ;", "thisLimit    =    appAttempt . getResourceLimit (  )  ;", "this . lastScheduledContainer    =    appAttempt . getLastScheduledContainer (  )  ;", "this . appSchedulingInfo . transferStateFromPreviousAppSchedulingInfo ( appAttempt . appSchedulingInfo )  ;", "}", "METHOD_END"], "methodName": ["transferStateFromPreviousAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isStopped )  )     {", "this . appingInfo . updateBlacklist ( blacklistAdditions ,    blacklistRemovals )  ;", "}", "}", "METHOD_END"], "methodName": ["updateBlacklist"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isStopped )  )     {", "appingInfo . updateResourceRequests ( requests ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["updateResourceRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( resource    =  =    null )     {", ". LOG . error (  (  \" Invalid   resource   addition   of   null   resource   for    \"     +     ( rmNode . getNodeAddress (  )  )  )  )  ;", "return ;", "}", "Resources . addTo ( availableResource ,    resource )  ;", "Resources . subtractFrom ( usedResource ,    resource )  ;", "}", "METHOD_END"], "methodName": ["addAvailableResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    rmContainer . getContainer (  )  ;", "deductAvailableResource ( container . getResource (  )  )  ;", "+  +  ( numContainers )  ;", "launchedContainers . put ( container . getId (  )  ,    rmContainer )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  \" Assigned   container    \"     +     ( container . getId (  )  )  )     +     \"    of   capacity    \"  )     +     ( container . getResource (  )  )  )     +     \"    on   host    \"  )     +     ( rmNode . getNodeAddress (  )  )  )     +     \"  ,    which   has    \"  )     +     ( numContainers )  )     +     \"    containers ,     \"  )     +     ( getUsedResource (  )  )  )     +     \"    used   and    \"  )     +     ( getAvailableResource (  )  )  )     +     \"    available   after   allocation \"  )  )  ;", "}", "METHOD_END"], "methodName": ["allocateContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "Resources . addTo ( this . availableResource ,    deltaResource )  ;", "}", "METHOD_END"], "methodName": ["applyDeltaOnAvailableResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "if    ( resource    =  =    null )     {", ". LOG . error (  (  \" Invalid   deduction   of   null   resource   for    \"     +     ( rmNode . getNodeAddress (  )  )  )  )  ;", "return ;", "}", "Resources . subtractFrom ( availableResource ,    resource )  ;", "Resources . addTo ( usedResource ,    resource )  ;", "}", "METHOD_END"], "methodName": ["deductAvailableResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . availableResource ;", "}", "METHOD_END"], "methodName": ["getAvailableResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . rmNode . getHttpAddress (  )  ;", "}", "METHOD_END"], "methodName": ["getHttpAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . rmNode . getNodeID (  )  ;", "}", "METHOD_END"], "methodName": ["getNodeID"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   nodeName ;", "}", "METHOD_END"], "methodName": ["getNodeName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . rmNode ;", "}", "METHOD_END"], "methodName": ["getRMNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . rmNode . getRackName (  )  ;", "}", "METHOD_END"], "methodName": ["getRackName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   reservedContainer ;", "}", "METHOD_END"], "methodName": ["getReservedContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   new   ArrayList < RMContainer >  ( launchedContainers . values (  )  )  ;", "}", "METHOD_END"], "methodName": ["getRunningContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . totalResourceCapability ;", "}", "METHOD_END"], "methodName": ["getTotalResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   this . usedResource ;", "}", "METHOD_END"], "methodName": ["getUsedResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "if    ( launchedContainers . containsKey ( containerId )  )     {", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isValidContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "if    ( rmContainer . getState (  )  . equals ( RMContainerState . COMPLETED )  )     {", "return ;", "}", "allocateContainer ( rmContainer )  ;", "}", "METHOD_END"], "methodName": ["recoverContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isValidContainer ( container . getId (  )  )  )  )     {", ". LOG . error (  (  \" Invalid   container   released    \"     +    container )  )  ;", "return ;", "}", "if    ( null    !  =     ( launchedContainers . remove ( container . getId (  )  )  )  )     {", "updateResource ( container )  ;", "}", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" Released   container    \"     +     ( container . getId (  )  )  )     +     \"    of   capacity    \"  )     +     ( container . getResource (  )  )  )     +     \"    on   host    \"  )     +     ( rmNode . getNodeAddress (  )  )  )     +     \"  ,    which   currently   has    \"  )     +     ( numContainers )  )     +     \"    containers ,     \"  )     +     ( getUsedResource (  )  )  )     +     \"    used   and    \"  )     +     ( getAvailableResource (  )  )  )     +     \"    available \"  )     +     \"  ,    release   resources =  \"  )     +    true )  )  ;", "}", "METHOD_END"], "methodName": ["releaseContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "this . reservedContainer    =    reservedContainer ;", "}", "METHOD_END"], "methodName": ["setReservedContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "addAvailableResource ( container . getResource (  )  )  ;", "-  -  ( numContainers )  ;", "}", "METHOD_END"], "methodName": ["updateResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode"}, {"methodBody": ["METHOD_START", "{", "return   avail ;", "}", "METHOD_END"], "methodName": ["getAvailableResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport"}, {"methodBody": ["METHOD_START", "{", "return   num ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport"}, {"methodBody": ["METHOD_START", "{", "return   used ;", "}", "METHOD_END"], "methodName": ["getUsedResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   containerStatus    =    SchedulerUtils . recordFactory . newRecordInstance ( ContainerStatus . class )  ;", "containerStatus . setContainerId ( containerId )  ;", "containerStatus . setDiagnostics ( diagnostics )  ;", "containerStatus . setExitStatus ( exitStatus )  ;", "containerStatus . setState ( COMPLETE )  ;", "return   containerStatus ;", "}", "METHOD_END"], "methodName": ["createAbnormalContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "return   SchedulerUtils . createAbnormalContainerStatus ( containerId ,    ABORTED ,    diagnostics )  ;", "}", "METHOD_END"], "methodName": ["createAbnormalContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "return   SchedulerUtils . createAbnormalContainerStatus ( containerId ,    PREEMPTED ,    diagnostics )  ;", "}", "METHOD_END"], "methodName": ["createPreemptedContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "Resource   normalized    =    Resources . normalize ( resourceCalculator ,    ask . getCapability (  )  ,    minimumResource ,    maximumResource ,    minimumResource )  ;", "ask . setCapability ( normalized )  ;", "}", "METHOD_END"], "methodName": ["normalizeRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "Resource   normalized    =    Resources . normalize ( resourceCalculator ,    ask . getCapability (  )  ,    minimumResource ,    maximumResource ,    incrementResource )  ;", "ask . setCapability ( normalized )  ;", "}", "METHOD_END"], "methodName": ["normalizeRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "for    ( ResourceRequest   ask    :    asks )     {", ". normalizeRequest ( ask ,    resourceCalculator ,    clusterResource ,    minimumResource ,    maximumResource ,    minimumResource )  ;", "}", "}", "METHOD_END"], "methodName": ["normalizeRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "for    ( ResourceRequest   ask    :    asks )     {", ". normalizeRequest ( ask ,    resourceCalculator ,    clusterResource ,    minimumResource ,    maximumResource ,    incrementResource )  ;", "}", "}", "METHOD_END"], "methodName": ["normalizeRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "boolean   result    =    false ;", "R   oldAvailableR    =    node . getAvailableR (  )  ;", "R   newAvailableR    =    Rs . subtract ( rmNode . getTotalCapability (  )  ,    node . getUsedR (  )  )  ;", "if    (  !  ( newAvailableR . equals ( oldAvailableR )  )  )     {", "result    =    true ;", "R   deltaR    =    Rs . subtract ( newAvailableR ,    oldAvailableR )  ;", "node . applyDeltaOnAvailableR ( deltaR )  ;", "Rs . addTo ( clusterR ,    deltaR )  ;", "log . info (  (  (  (  (  (  (  \" R   change   on   node :     \"     +     ( rmNode . getNodeAddress (  )  )  )     +     \"    with   delta :    CPU :     \"  )     +     ( deltaR . getMemory (  )  )  )     +     \" core ,    Memory :     \"  )     +     ( deltaR . getMemory (  )  )  )     +     \" MB \"  )  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["updateResourceIfChanged"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( resReq . getCapability (  )  . getMemory (  )  )     <     0  )     |  |     (  ( resReq . getCapability (  )  . getMemory (  )  )     >     ( maximumResource . getMemory (  )  )  )  )     {", "throw   new   exceptions . InvalidResourceRequestException (  (  (  (  (  \" Invalid   resource   request \"     +     (  (  \"  ,    requested   memory    <     0  \"     +     \"  ,    or   requested   memory    >    max   configured \"  )     +     \"  ,    requestedMemory =  \"  )  )     +     ( resReq . getCapability (  )  . getMemory (  )  )  )     +     \"  ,    maxMemory =  \"  )     +     ( maximumResource . getMemory (  )  )  )  )  ;", "}", "if    (  (  ( resReq . getCapability (  )  . getVirtualCores (  )  )     <     0  )     |  |     (  ( resReq . getCapability (  )  . getVirtualCores (  )  )     >     ( maximumResource . getVirtualCores (  )  )  )  )     {", "throw   new   exceptions . InvalidResourceRequestException (  (  (  (  (  \" Invalid   resource   request \"     +     (  (  \"  ,    requested   virtual   cores    <     0  \"     +     \"  ,    or   requested   virtual   cores    >    max   configured \"  )     +     \"  ,    requestedVirtualCores =  \"  )  )     +     ( resReq . getCapability (  )  . getVirtualCores (  )  )  )     +     \"  ,    maxVirtualCores =  \"  )     +     ( maximumResource . getVirtualCores (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "MetricsRecordBuilder   rb    =    getMetrics ( source ,    all )  ;", "assertCounter (  \" AppsSubmitted \"  ,    submitted ,    rb )  ;", "assertGauge (  \" AppsPending \"  ,    pending ,    rb )  ;", "assertGauge (  \" AppsRunning \"  ,    running ,    rb )  ;", "assertCounter (  \" AppsCompleted \"  ,    completed ,    rb )  ;", "assertCounter (  \" AppsFailed \"  ,    failed ,    rb )  ;", "assertCounter (  \" AppsKilled \"  ,    killed ,    rb )  ;", "}", "METHOD_END"], "methodName": ["checkApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "MetricsRecordBuilder   rb    =    getMetrics ( source )  ;", "assertGauge (  \" AllocatedMB \"  ,    allocatedMB ,    rb )  ;", "assertGauge (  \" AllocatedVCores \"  ,    allocatedCores ,    rb )  ;", "assertGauge (  \" AllocatedContainers \"  ,    allocCtnrs ,    rb )  ;", "assertCounter (  \" AggregateContainersAllocated \"  ,    aggreAllocCtnrs ,    rb )  ;", "assertCounter (  \" AggregateContainersReleased \"  ,    aggreReleasedCtnrs ,    rb )  ;", "assertGauge (  \" AvailableMB \"  ,    availableMB ,    rb )  ;", "assertGauge (  \" AvailableVCores \"  ,    availableCores ,    rb )  ;", "assertGauge (  \" PendingMB \"  ,    pendingMB ,    rb )  ;", "assertGauge (  \" PendingVCores \"  ,    pendingCores ,    rb )  ;", "assertGauge (  \" PendingContainers \"  ,    pendingCtnrs ,    rb )  ;", "assertGauge (  \" ReservedMB \"  ,    reservedMB ,    rb )  ;", "assertGauge (  \" ReservedVCores \"  ,    reservedCores ,    rb )  ;", "assertGauge (  \" ReservedContainers \"  ,    reservedCtnrs ,    rb )  ;", "}", "METHOD_END"], "methodName": ["checkResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "AppSchedulingInfo   app    =    mock ( AppSchedulingInfo . class )  ;", "when ( app . getUser (  )  )  . thenReturn ( user )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  ,     1  )  ;", "ApplicationAttemptId   id    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "when ( app . getApplicationAttemptId (  )  )  . thenReturn ( id )  ;", "return   app ;", "}", "METHOD_END"], "methodName": ["mockApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "MetricsSource   s    =    ms . getSource ( QueueMetrics . sourceName ( queue )  . toString (  )  )  ;", "return   s ;", "}", "METHOD_END"], "methodName": ["queueSource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "ms    =    new   MetricsSystemImpl (  )  ;", ". clear (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =     \" single \"  ;", "QueueMetrics . forQueue ( ms ,    queueName ,    null ,    false ,     . conf )  ;", "MetricsSource   queueSource    =     . queueSource ( ms ,    queueName )  ;", ". checkApps ( queueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "try    {", ". checkApps ( queueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,    false )  ;", "Assert . fail (  )  ;", "}    catch    ( AssertionError   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" Expected   exactly   one   metric   for   name    \"  )  )  ;", "}", ". checkApps ( queueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "}", "METHOD_END"], "methodName": ["testCollectAllMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =     \" single \"  ;", "String   user    =     \" alice \"  ;", "QueueMetrics   metrics    =    QueueMetrics . forQueue ( ms ,    queueName ,    null ,    false ,     . conf )  ;", "MetricsSource   queueSource    =     . queueSource ( ms ,    queueName )  ;", "AppSchedulingInfo   app    =     . mockApp ( user )  ;", "metrics . submitApp ( user )  ;", "MetricsSource   userSource    =     . userSource ( ms ,    queueName ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . submitAppAttempt ( user )  ;", ". checkApps ( queueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . setAvailableResourcesToQueue ( Resources . createResource (  (  1  0  0     *     (  . GB )  )  ,     1  0  0  )  )  ;", "metrics . incrPendingResources ( user ,     5  ,    Resources . createResource (  (  3     *     (  . GB )  )  ,     3  )  )  ;", ". checkResources ( queueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", "metrics . runAppAttempt ( app . getApplicationId (  )  ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . allocateResources ( user ,     3  ,    Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  ,    true )  ;", ". checkResources ( queueSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", "metrics . releaseResources ( user ,     1  ,    Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  )  ;", ". checkResources ( queueSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", "metrics . finishAppAttempt ( app . getApplicationId (  )  ,    app . isPending (  )  ,    app . getUser (  )  )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishApp ( user ,    RMAppState . FINISHED )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", "assertNull ( userSource )  ;", "}", "METHOD_END"], "methodName": ["testDefaultSingleQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "MetricsSystem   ms    =    new   MetricsSystemImpl (  \" cache \"  )  ;", "ms . start (  )  ;", "try    {", "String   p 1     =     \" root 1  \"  ;", "String   leafQueueName    =     \" root 1  . leaf \"  ;", "QueueMetrics   p 1 Metrics    =    QueueMetrics . forQueue ( ms ,    p 1  ,    null ,    true ,     . conf )  ;", "Queue   parentQueue 1     =    make ( stub ( Queue . class )  . returning ( p 1 Metrics )  . from . getMetrics (  )  )  ;", "QueueMetrics   metrics    =    QueueMetrics . forQueue ( ms ,    leafQueueName ,    parentQueue 1  ,    true ,     . conf )  ;", "Assert . assertNotNull (  \" QueueMetrics   for   A   shoudn ' t   be   null \"  ,    metrics )  ;", "QueueMetrics   alterMetrics    =    QueueMetrics . forQueue ( ms ,    leafQueueName ,    parentQueue 1  ,    true ,     . conf )  ;", "Assert . assertNotNull (  \" QueueMetrics   for   alterMetrics   shoudn ' t   be   null \"  ,    alterMetrics )  ;", "}    finally    {", "ms . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testMetricsCache"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    FifoScheduler . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "QueueMetrics   metrics    =    rm . getResourceScheduler (  )  . getRootQueueMetrics (  )  ;", ". checkApps ( metrics ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "MetricsAsserts . assertGauge (  \" ReservedContainers \"  ,     0  ,    metrics )  ;", "}", "METHOD_END"], "methodName": ["testMetricsInitializedOnRMInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =     \" single \"  ;", "String   user    =     \" alice \"  ;", "QueueMetrics   metrics    =    QueueMetrics . forQueue ( ms ,    queueName ,    null ,    false ,    new   Configuration (  )  )  ;", "MetricsSource   queueSource    =     . queueSource ( ms ,    queueName )  ;", "AppSchedulingInfo   app    =     . mockApp ( user )  ;", "metrics . submitApp ( user )  ;", "MetricsSource   userSource    =     . userSource ( ms ,    queueName ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . submitAppAttempt ( user )  ;", ". checkApps ( queueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . runAppAttempt ( app . getApplicationId (  )  ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishAppAttempt ( app . getApplicationId (  )  ,    app . isPending (  )  ,    app . getUser (  )  )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . submitAppAttempt ( user )  ;", ". checkApps ( queueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . runAppAttempt ( app . getApplicationId (  )  ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishAppAttempt ( app . getApplicationId (  )  ,    app . isPending (  )  ,    app . getUser (  )  )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . submitAppAttempt ( user )  ;", ". checkApps ( queueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . runAppAttempt ( app . getApplicationId (  )  ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishAppAttempt ( app . getApplicationId (  )  ,    app . isPending (  )  ,    app . getUser (  )  )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishApp ( user ,    RMAppState . FAILED )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     1  ,     0  ,    true )  ;", "assertNull ( userSource )  ;", "}", "METHOD_END"], "methodName": ["testQueueAppMetricsForMultipleFailures"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =     \" single 2  \"  ;", "String   user    =     \" dodo \"  ;", "QueueMetrics   metrics    =    QueueMetrics . forQueue ( ms ,    queueName ,    null ,    true ,     . conf )  ;", "MetricsSource   queueSource    =     . queueSource ( ms ,    queueName )  ;", "AppSchedulingInfo   app    =     . mockApp ( user )  ;", "metrics . submitApp ( user )  ;", "MetricsSource   userSource    =     . userSource ( ms ,    queueName ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . submitAppAttempt ( user )  ;", ". checkApps ( queueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . setAvailableResourcesToQueue ( Resources . createResource (  (  1  0  0     *     (  . GB )  )  ,     1  0  0  )  )  ;", "metrics . setAvailableResourcesToUser ( user ,    Resources . createResource (  (  1  0     *     (  . GB )  )  ,     1  0  )  )  ;", "metrics . incrPendingResources ( user ,     5  ,    Resources . createResource (  (  3     *     (  . GB )  )  ,     3  )  )  ;", ". checkResources ( queueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", ". checkResources ( userSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", "metrics . runAppAttempt ( app . getApplicationId (  )  ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . allocateResources ( user ,     3  ,    Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  ,    true )  ;", ". checkResources ( queueSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", ". checkResources ( userSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", "metrics . releaseResources ( user ,     1  ,    Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  )  ;", ". checkResources ( queueSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", ". checkResources ( userSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", "metrics . finishAppAttempt ( app . getApplicationId (  )  ,    app . isPending (  )  ,    app . getUser (  )  )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishApp ( user ,    RMAppState . FINISHED )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", "}", "METHOD_END"], "methodName": ["testSingleQueueWithUserMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "String   parentQueueName    =     \" root \"  ;", "String   leafQueueName    =     \" root . leaf \"  ;", "String   user    =     \" alice \"  ;", "QueueMetrics   parentMetrics    =    QueueMetrics . forQueue ( ms ,    parentQueueName ,    null ,    true ,     . conf )  ;", "Queue   parentQueue    =    make ( stub ( Queue . class )  . returning ( parentMetrics )  . from . getMetrics (  )  )  ;", "QueueMetrics   metrics    =    QueueMetrics . forQueue ( ms ,    leafQueueName ,    parentQueue ,    true ,     . conf )  ;", "MetricsSource   parentQueueSource    =     . queueSource ( ms ,    parentQueueName )  ;", "MetricsSource   queueSource    =     . queueSource ( ms ,    leafQueueName )  ;", "AppSchedulingInfo   app    =     . mockApp ( user )  ;", "metrics . submitApp ( user )  ;", "MetricsSource   userSource    =     . userSource ( ms ,    leafQueueName ,    user )  ;", "MetricsSource   parentUserSource    =     . userSource ( ms ,    parentQueueName ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentQueueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentUserSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . submitAppAttempt ( user )  ;", ". checkApps ( queueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentQueueSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentUserSource ,     1  ,     1  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "parentMetrics . setAvailableResourcesToQueue ( Resources . createResource (  (  1  0  0     *     (  . GB )  )  ,     1  0  0  )  )  ;", "metrics . setAvailableResourcesToQueue ( Resources . createResource (  (  1  0  0     *     (  . GB )  )  ,     1  0  0  )  )  ;", "parentMetrics . setAvailableResourcesToUser ( user ,    Resources . createResource (  (  1  0     *     (  . GB )  )  ,     1  0  )  )  ;", "metrics . setAvailableResourcesToUser ( user ,    Resources . createResource (  (  1  0     *     (  . GB )  )  ,     1  0  )  )  ;", "metrics . incrPendingResources ( user ,     5  ,    Resources . createResource (  (  3     *     (  . GB )  )  ,     3  )  )  ;", ". checkResources ( queueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", ". checkResources ( parentQueueSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", ". checkResources ( userSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", ". checkResources ( parentUserSource ,     0  ,     0  ,     0  ,     0  ,     0  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  1  5     *     (  . GB )  )  ,     1  5  ,     5  ,     0  ,     0  ,     0  )  ;", "metrics . runAppAttempt ( app . getApplicationId (  )  ,    user )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     1  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . allocateResources ( user ,     3  ,    Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  ,    true )  ;", "metrics . reserveResource ( user ,    Resources . createResource (  (  3     *     (  . GB )  )  ,     3  )  )  ;", ". checkResources ( queueSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     (  3     *     (  . GB )  )  ,     3  ,     1  )  ;", ". checkResources ( parentQueueSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     (  3     *     (  . GB )  )  ,     3  ,     1  )  ;", ". checkResources ( userSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     (  3     *     (  . GB )  )  ,     3  ,     1  )  ;", ". checkResources ( parentUserSource ,     (  6     *     (  . GB )  )  ,     6  ,     3  ,     3  ,     0  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     (  3     *     (  . GB )  )  ,     3  ,     1  )  ;", "metrics . releaseResources ( user ,     1  ,    Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  )  ;", "metrics . unreserveResource ( user ,    Resources . createResource (  (  3     *     (  . GB )  )  ,     3  )  )  ;", ". checkResources ( queueSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", ". checkResources ( parentQueueSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0  0     *     (  . GB )  )  ,     1  0  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", ". checkResources ( userSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", ". checkResources ( parentUserSource ,     (  4     *     (  . GB )  )  ,     4  ,     2  ,     3  ,     1  ,     (  1  0     *     (  . GB )  )  ,     1  0  ,     (  9     *     (  . GB )  )  ,     9  ,     2  ,     0  ,     0  ,     0  )  ;", "metrics . finishAppAttempt ( app . getApplicationId (  )  ,    app . isPending (  )  ,    app . getUser (  )  )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentQueueSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentUserSource ,     1  ,     0  ,     0  ,     0  ,     0  ,     0  ,    true )  ;", "metrics . finishApp ( user ,    RMAppState . FINISHED )  ;", ". checkApps ( queueSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentQueueSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", ". checkApps ( userSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", ". checkApps ( parentUserSource ,     1  ,     0  ,     0  ,     1  ,     0  ,     0  ,    true )  ;", "}", "METHOD_END"], "methodName": ["testTwoLevelWithUserMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "MetricsSource   s    =    ms . getSource ( QueueMetrics . sourceName ( queue )  . append (  \"  , user =  \"  )  . append ( user )  . toString (  )  )  ;", "return   s ;", "}", "METHOD_END"], "methodName": ["userSource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( activeApps ,    metrics . getActiveApps (  )  )  ;", "assertEquals ( runningApps ,    metrics . getAppsRunning (  )  )  ;", "assertEquals ( allocMb ,    metrics . getAllocatedMB (  )  )  ;", "assertEquals ( allocVcores ,    metrics . getAllocatedVirtualCores (  )  )  ;", "assertEquals ( redMb ,    metrics . getRedMB (  )  )  ;", "assertEquals ( redVcores ,    metrics . getRedVirtualCores (  )  )  ;", "assertEquals ( pendingMb ,    metrics . getPendingMB (  )  )  ;", "assertEquals ( pendingVcores ,    metrics . getPendingVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appIdImpl    =    ApplicationId . newInstance (  0  ,    appId )  ;", "Id   attId    =    Id . newInstance ( appIdImpl ,    attemptId )  ;", "return   attId ;", "}", "METHOD_END"], "methodName": ["createAppAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "SchedulerNode   node    =    mock ( SchedulerNode . class )  ;", "when ( node . getNodeName (  )  )  . thenReturn (  \" somehost \"  )  ;", "when ( node . getRackName (  )  )  . thenReturn (  \" somerack \"  )  ;", "when ( node . getNodeID (  )  )  . thenReturn (  . nodeId )  ;", "return   node ;", "}", "METHOD_END"], "methodName": ["createNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics   metrics    =    QueueMetrics . forQueue ( name ,    parent ,    false ,    conf )  ;", "ActiveUsersM   activeUsersM    =    new   ActiveUsersM ( metrics )  ;", "Queue   queue    =    mock ( Queue . class )  ;", "when ( queue . getMetrics (  )  )  . thenReturn ( metrics )  ;", "when ( queue . getActiveUsersM (  )  )  . thenReturn ( activeUsersM )  ;", "return   queue ;", "}", "METHOD_END"], "methodName": ["createQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    ContainerId . newInstance ( appAttId ,    id )  ;", "RMContainer   rmContainer    =    mock ( RMContainer . class )  ;", "Container   container    =    mock ( Container . class )  ;", "when ( container . getResource (  )  )  . thenReturn ( resource )  ;", "when ( container . getNodeId (  )  )  . thenReturn (  . nodeId )  ;", "when ( rmContainer . getContainer (  )  )  . thenReturn ( container )  ;", "when ( rmContainer . getContainerId (  )  )  . thenReturn ( containerId )  ;", "return   rmContainer ;", "}", "METHOD_END"], "methodName": ["createRMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "RMContainer   container    =    createRMContainer ( appAttId ,    id ,    resource )  ;", "when ( container . getReservedResource (  )  )  . thenReturn ( resource )  ;", "when ( container . getReservedPriority (  )  )  . thenReturn ( reservedPriority )  ;", "when ( container . getReservedNode (  )  )  . thenReturn ( nodeId )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createReservedRMContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "QueueMetrics . clearQueueMetrics (  )  ;", "DefaultMetricsSys . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "final   String   user    =     \" user 1  \"  ;", "Queue   parentQueue    =    createQueue (  \" parent \"  ,    null )  ;", "Queue   oldQueue    =    createQueue (  \" old \"  ,    parentQueue )  ;", "Queue   newQueue    =    createQueue (  \" new \"  ,    parentQueue )  ;", "QueueMetrics   parentMetrics    =    parentQueue . getMetrics (  )  ;", "QueueMetrics   oldMetrics    =    oldQueue . getMetrics (  )  ;", "QueueMetrics   newMetrics    =    newQueue . getMetrics (  )  ;", "ApplicationAttemptId   appAttId    =    createAppAttemptId (  0  ,     0  )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getEpoch (  )  )  . thenReturn (  3  )  ;", "app    =    new    ( appAttId ,    user ,    oldQueue ,    oldQueue . getActiveUsersManager (  )  ,    rmContext )  ;", "oldMetrics . submitApp ( user )  ;", "assertEquals ( app . getNewContainerId (  )  ,     1  2  5  8  2  9  1  3  )  ;", "Resource   requestedResource    =    Resource . newInstance (  1  5  3  6  ,     2  )  ;", "Priority   requestedPriority    =    Priority . newInstance (  2  )  ;", "ResourceRequest   request    =    ResourceRequest . newInstance ( requestedPriority ,    ANY ,    requestedResource ,     3  )  ;", "app . updateResourceRequests ( Arrays . asList ( request )  )  ;", "RMContainer   container 1     =    createRMContainer ( appAttId ,     1  ,    requestedResource )  ;", "app . liveContainers . put ( container 1  . getContainerId (  )  ,    container 1  )  ;", "SchedulerNode   node    =    createNode (  )  ;", "app . appSchedulingInfo . allocate ( NodeType . OFF _ SWITCH ,    node ,    requestedPriority ,    request ,    container 1  . getContainer (  )  )  ;", "Priority   prio 1     =    Priority . newInstance (  1  )  ;", "Resource   reservedResource    =    Resource . newInstance (  2  0  4  8  ,     3  )  ;", "RMContainer   container 2     =    createReservedRMContainer ( appAttId ,     1  ,    reservedResource ,    node . getNodeID (  )  ,    prio 1  )  ;", "Map < NodeId ,    RMContainer >    reservations    =    new   HashMap < NodeId ,    RMContainer >  (  )  ;", "reservations . put ( node . getNodeID (  )  ,    container 2  )  ;", "app . reservedContainers . put ( prio 1  ,    reservations )  ;", "oldMetrics . reserveResource ( user ,    reservedResource )  ;", "checkQueueMetrics ( oldMetrics ,     1  ,     1  ,     1  5  3  6  ,     2  ,     2  0  4  8  ,     3  ,     3  0  7  2  ,     4  )  ;", "checkQueueMetrics ( newMetrics ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  )  ;", "checkQueueMetrics ( parentMetrics ,     1  ,     1  ,     1  5  3  6  ,     2  ,     2  0  4  8  ,     3  ,     3  0  7  2  ,     4  )  ;", "app . move ( newQueue )  ;", "checkQueueMetrics ( oldMetrics ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  ,     0  )  ;", "checkQueueMetrics ( newMetrics ,     1  ,     1  ,     1  5  3  6  ,     2  ,     2  0  4  8  ,     3  ,     3  0  7  2  ,     4  )  ;", "checkQueueMetrics ( parentMetrics ,     1  ,     1  ,     1  5  3  6  ,     2  ,     2  0  4  8  ,     3  ,     3  0  7  2  ,     4  )  ;", "}", "METHOD_END"], "methodName": ["testMove"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerApplicationAttempt"}, {"methodBody": ["METHOD_START", "{", "Priority   high    =    Priority . newInstance (  1  )  ;", "Priority   low    =    Priority . newInstance (  2  )  ;", "astTrue (  (  ( high . compareTo ( low )  )     >     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testComparePriorities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   cd    =    SchedulerUtils . createAbnormalContainerStatus ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance ( System . currentTimeMillis (  )  ,     1  )  ,     1  )  ,     1  )  ,     \" x \"  )  ;", "Assert . assertEquals ( ABORTED ,    cd . getExitStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateAbnormalContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   cd    =    SchedulerUtils . createPreemptedContainerStatus ( ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance ( System . currentTimeMillis (  )  ,     1  )  ,     1  )  ,     1  )  ,     \" x \"  )  ;", "Assert . assertEquals ( PREEMPTED ,    cd . getExitStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreatePreemptedContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculator   resourceCalculator    =    new   DefaultResourceCalculator (  )  ;", "final   int   minMemory    =     1  0  2  4  ;", "final   int   maxMemory    =     8  1  9  2  ;", "Resource   minResource    =    Resources . createResource ( minMemory ,     0  )  ;", "Resource   maxResource    =    Resources . createResource ( maxMemory ,     0  )  ;", "ResourceRequest   ask    =    new   ResourceRequestPBImpl (  )  ;", "ask . setCapability ( Resources . createResource (  (  -  1  0  2  4  )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals ( minMemory ,    ask . getCapability (  )  . getMemory (  )  )  ;", "ask . setCapability ( Resources . createResource (  0  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals ( minMemory ,    ask . getCapability (  )  . getMemory (  )  )  ;", "ask . setCapability ( Resources . createResource (  (  2     *    minMemory )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals (  (  2     *    minMemory )  ,    ask . getCapability (  )  . getMemory (  )  )  ;", "ask . setCapability ( Resources . createResource (  ( minMemory    +     1  0  )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals (  (  2     *    minMemory )  ,    ask . getCapability (  )  . getMemory (  )  )  ;", "ask . setCapability ( Resources . createResource ( maxMemory )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals ( maxMemory ,    ask . getCapability (  )  . getMemory (  )  )  ;", "ask . setCapability ( Resources . createResource (  ( maxMemory    -     1  0  )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals ( maxMemory ,    ask . getCapability (  )  . getMemory (  )  )  ;", "maxResource    =    Resources . createResource (  ( maxMemory    -     1  0  )  ,     0  )  ;", "ask . setCapability ( Resources . createResource (  ( maxMemory    -     1  0  0  )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals ( maxResource . getMemory (  )  ,    ask . getCapability (  )  . getMemory (  )  )  ;", "maxResource    =    Resources . createResource ( maxMemory ,     0  )  ;", "ask . setCapability ( Resources . createResource (  ( maxMemory    +     1  0  0  )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    null ,    minResource ,    maxResource )  ;", "assertEquals ( maxResource . getMemory (  )  ,    ask . getCapability (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNormalizeRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculator   resourceCalculator    =    new   DominantResourceCalculator (  )  ;", "Resource   minResource    =    Resources . createResource (  1  0  2  4  ,     1  )  ;", "Resource   maxResource    =    Resources . createResource (  1  0  2  4  0  ,     1  0  )  ;", "Resource   clusterResource    =    Resources . createResource (  (  1  0     *     1  0  2  4  )  ,     1  0  )  ;", "ResourceRequest   ask    =    new   ResourceRequestPBImpl (  )  ;", "ask . setCapability ( Resources . createResource (  (  -  1  0  2  4  )  ,     (  -  1  )  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    clusterResource ,    minResource ,    maxResource )  ;", "assertEquals ( minResource ,    ask . getCapability (  )  )  ;", "ask . setCapability ( Resources . createResource (  0  ,     0  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    clusterResource ,    minResource ,    maxResource )  ;", "assertEquals ( minResource ,    ask . getCapability (  )  )  ;", "assertEquals (  1  ,    ask . getCapability (  )  . getVirtualCores (  )  )  ;", "assertEquals (  1  0  2  4  ,    ask . getCapability (  )  . getMemory (  )  )  ;", "ask . setCapability ( Resources . createResource (  1  5  3  6  ,     0  )  )  ;", ". normalizeRequest ( ask ,    resourceCalculator ,    clusterResource ,    minResource ,    maxResource )  ;", "assertEquals ( Resources . createResource (  2  0  4  8  ,     1  )  ,    ask . getCapability (  )  )  ;", "assertEquals (  1  ,    ask . getCapability (  )  . getVirtualCores (  )  )  ;", "assertEquals (  2  0  4  8  ,    ask . getCapability (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNormalizeRequestWithDominantResourceCalculator"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "TestAMAuthorization . MyContainerMcontainerM =    new   TestAMAuthorization . MyContainerM )  ;", "final   TestAMAuthorization . MockRMWithAMS   rm    =    new   TestAMAuthorization . MockRMWithAMS ( new   YarnConfiguration (  )  ,    containerM ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  2  )  ;", "acls . put ( VIEW _ APP ,     \"  *  \"  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  ,     \" appname \"  ,     \" appuser \"  ,    acls )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    attempt . getAppAttemptId (  )  ;", "waitForLaunchedState ( attempt )  ;", "final   Configuration   conf    =    rm . getConfig (  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( applicationAttemptId . toString (  )  )  ;", "Credentials   credentials    =    containerMgetContainerCredentials (  )  ;", "final   InetSocketAddress   rmBindAddress    =    rm . getApplicationMasterService (  )  . getBindAddress (  )  ;", "Token <  ?    extends   TokenIdentifier >    amRMToken    =    TestAMAuthorization . MockRMWithAMS . setupAndReturnAMRMToken ( rmBindAddress ,    credentials . getAllTokens (  )  )  ;", "currentUser . addToken ( amRMToken )  ;", "ApplicationMasterProtocol   client    =    currentUser . doAs ( new   PrivilegedAction < ApplicationMasterProtocol >  (  )     {", "@ Override", "public   ApplicationMasterProtocol   run (  )     {", "return    (  ( ApplicationMasterProtocol )     ( rpc . getProxy ( ApplicationMasterProtocol . class ,    rmBindAddress ,    conf )  )  )  ;", "}", "}  )  ;", "RegisterApplicationMasterRequest   request    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "client . registerApplicationMaster ( request )  ;", "ResourceBlacklistRequest   blacklistRequest    =    ResourceBlacklistRequest . newInstance ( Collections . singletonList ( ANY )  ,    null )  ;", "AllocateRequest   allocateRequest    =    AllocateRequest . newInstance (  0  ,     0  .  0 F ,    null ,    null ,    blacklistRequest )  ;", "boolean   error    =    false ;", "try    {", "client . allocate ( allocateRequest )  ;", "}    catch    ( InvalidResourceBlacklistRequestException   e )     {", "error    =    true ;", "}", "rm . stop (  )  ;", "Assert . assertTrue (  \" Didn ' t   not   catch   InvalidResourceBlacklistRequestException \"  ,    error )  ;", "}", "METHOD_END"], "methodName": ["testValidateResourceBlacklistRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "Resource   maxResource    =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  ;", "try    {", "Resource   resource    =    Resources . createResource (  0  ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "fail (  \" Zero   memory   should   be   accepted \"  )  ;", "}", "try    {", "Resource   resource    =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     0  )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "fail (  \" Zero   vcores   should   be   accepted \"  )  ;", "}", "try    {", "Resource   resource    =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "fail (  \" Max   memory   should   be   accepted \"  )  ;", "}", "try    {", "Resource   resource    =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "fail (  \" Max   vcores   should   not   be   accepted \"  )  ;", "}", "try    {", "Resource   resource    =    Resources . createResource (  (  -  1  )  ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "fail (  \" Negative   memory   should   not   be   accepted \"  )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "}", "try    {", "Resource   resource    =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     (  -  1  )  )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "fail (  \" Negative   vcores   should   not   be   accepted \"  )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "}", "try    {", "Resource   resource    =    Resources . createResource (  (  ( YarnConfiguration . DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )     +     1  )  ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "fail (  \" More   than   max   memory   should   not   be   accepted \"  )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "}", "try    {", "Resource   resource    =    Resources . createResource ( DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     (  ( YarnConfiguration . DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )     +     1  )  )  ;", "ResourceRequest   resReq    =    BuilderUtils . newResourceRequest ( mock ( Priority . class )  ,    ANY ,    resource ,     1  )  ;", ". validateResourceRequest ( resReq ,    maxResource )  ;", "fail (  \" More   than   max   vcores   should   not   be   accepted \"  )  ;", "}    catch    ( InvalidResourceRequestException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testValidateResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance ( System . currentTimeMillis (  )  ,     1  )  ;", "AppAddedEvent   appAddedEvent    =    new   AppAddedEvent ( appId ,    queueName ,     \" user \"  )  ;", "handler . handle ( appAddedEvent )  ;", "Application < ApplicationAttempt >    app    =    applications . get ( appId )  ;", "Assert . assertNotNull ( app )  ;", "Assert . assertEquals (  \" user \"  ,    app . getUser (  )  )  ;", "AppRemovedEvent   appRemoveEvent    =    new   AppRemovedEvent ( appId ,    RMAppState . FINISHED )  ;", "handler . handle ( appRemoveEvent )  ;", "Assert . assertNull ( applications . get ( appId )  )  ;", "return   app ;", "}", "METHOD_END"], "methodName": ["verifyAppAddedAndRemovedFromScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "int   waitCount    =     0  ;", "while    (  (  ( attempt . getAppAttemptState (  )  )     !  =     ( RMAppAttemptState . LAUNCHED )  )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", ". LOG . info (  (  (  \" Waiting   for   AppAttempt   to   reach   LAUNCHED   state .     \"     +     \" Current   state   is    \"  )     +     ( attempt . getAppAttemptState (  )  )  )  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertEquals ( attempt . getAppAttemptState (  )  ,    RMAppAttemptState . LAUNCHED )  ;", "}", "METHOD_END"], "methodName": ["waitForLaunchedState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "map . put ( key ,    time )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TimeBucketMetrics"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( cuts . length )  ;     +  + i )     {", "if    ( val    <     ( cuts [ i ]  )  )     {", "return   i ;", "}", "}", "return   cuts . length ;", "}", "METHOD_END"], "methodName": ["findBucket"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TimeBucketMetrics"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( counts . length )  ;     +  + i )     {", "counts [ i ]     =     0  ;", "}", "for    ( Long   time    :    map . values (  )  )     {", "counts [ find (  ( now    -    time )  )  ]     +  =     1  ;", "}", "return   counts ;", "}", "METHOD_END"], "methodName": ["getBucketCounts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TimeBucketMetrics"}, {"methodBody": ["METHOD_START", "{", "map . remove ( key )  ;", "}", "METHOD_END"], "methodName": ["remove"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.TimeBucketMetrics"}, {"methodBody": ["METHOD_START", "{", "return   application ;", "}", "METHOD_END"], "methodName": ["getApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment"}, {"methodBody": ["METHOD_START", "{", "return   excessReservation ;", "}", "METHOD_END"], "methodName": ["getExcessReservation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment"}, {"methodBody": ["METHOD_START", "{", "return   resource ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment"}, {"methodBody": ["METHOD_START", "{", "return   skipped ;", "}", "METHOD_END"], "methodName": ["getSkipped"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment"}, {"methodBody": ["METHOD_START", "{", "this . type    =    type ;", "}", "METHOD_END"], "methodName": ["setType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSAssignment"}, {"methodBody": ["METHOD_START", "{", "if    ( absMaxCapacity    <     ( absCapacity    -     ( CSQueueUtils . EPSILON )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  (  (  (  (  (  \" Illegal   call   to   setMaxCapacity .     \"     +     \" Queue    '  \"  )     +    queueName )     +     \"  '    has    \"  )     +     \" an   absolute   capacity    (  \"  )     +    absCapacity )     +     \"  )    greater   than    \"  )     +     \" its   absolute   maximumCapacity    (  \"  )     +    absMaxCapacity )     +     \"  )  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkAbsoluteCapacities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( maximumCapacity    <     0  .  0 F )     |  |     ( maximumCapacity    >     1  .  0 F )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Illegal   value      of   maximumCapacity    \"     +    maximumCapacity )     +     \"    used   in   call   to   setMaxCapacity   for   queue    \"  )     +    queueName )  )  ;", "}", "}", "METHOD_END"], "methodName": ["checkMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "float   parentAbsMaxCapacity    =     ( parent    =  =    null )     ?     1  .  0 F    :    parent . getAbsoluteMaximumCapacity (  )  ;", "return   parentAbsMaxCapacity    *    maximumCapacity ;", "}", "METHOD_END"], "methodName": ["computeAbsoluteMaximumCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "return   Math . max (  (  ( int )     ( Math . ceil (  (  (  ( Resources . ratio ( calculator ,    clusterResource ,    minimumAllocation )  )     *    maxAMResourcePercent )     *    absoluteMaxCapacity )  )  )  )  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["computeMaxActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "return   Math . max (  (  ( int )     ( Math . ceil (  (  ( maxActiveApplications    *     ( userLimit    /     1  0  0  .  0 F )  )     *    userLimitFactor )  )  )  )  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["computeMaxActiveApplicationsPerUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "CSQueue   parent    =    queue . getParent (  )  ;", "if    ( parent    =  =    null )     {", "return   queue . getAbsoluteMaximumCapacity (  )  ;", "}", "float   parentMaxAvail    =     . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    parent )  ;", "Resource   parentResource    =    Resources . multiply ( clusterResource ,    parentMaxAvail )  ;", "if    ( Resources . isInvalidDivisor ( resourceCalculator ,    parentResource )  )     {", "return    0  .  0 F ;", "}", "float   siblingUsedCapacity    =    Resources . ratio ( resourceCalculator ,    Resources . subtract ( parent . getUsedResources (  )  ,    queue . getUsedResources (  )  )  ,    parentResource )  ;", "float   maxAvail    =    Math . min ( queue . getMaximumCapacity (  )  ,     (  1  .  0 F    -    siblingUsedCapacity )  )  ;", "float   absoluteMaxAvail    =    maxAvail    *    parentMaxAvail ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" qpath    \"     +     ( queue . getQueuePath (  )  )  )  )  ;", ". LOG . debug (  (  \" parentMaxAvail    \"     +    parentMaxAvail )  )  ;", ". LOG . debug (  (  \" siblingUsedCapacity    \"     +    siblingUsedCapacity )  )  ;", ". LOG . debug (  (  \" getAbsoluteMaximumCapacity    \"     +     ( queue . getAbsoluteMaximumCapacity (  )  )  )  )  ;", ". LOG . debug (  (  \" maxAvail    \"     +    maxAvail )  )  ;", ". LOG . debug (  (  \" absoluteMaxAvail    \"     +    absoluteMaxAvail )  )  ;", "}", "if    ( absoluteMaxAvail    <     0  .  0 F )     {", "absoluteMaxAvail    =     0  .  0 F ;", "} else", "if    ( absoluteMaxAvail    >     1  .  0 F )     {", "absoluteMaxAvail    =     1  .  0 F ;", "}", "return   absoluteMaxAvail ;", "}", "METHOD_END"], "methodName": ["getAbsoluteMaxAvailCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "Resource   queueLimit    =    Resources . none (  )  ;", "Resource   usedResources    =    childQueue . getUsedResources (  )  ;", "float   absoluteUsedCapacity    =     0  .  0 F ;", "float   usedCapacity    =     0  .  0 F ;", "if    ( Resources . greaterThan ( calculator ,    clusterResource ,    clusterResource ,    Resources . none (  )  )  )     {", "queueLimit    =    Resources . multiply ( clusterResource ,    childQueue . getAbsoluteCapacity (  )  )  ;", "absoluteUsedCapacity    =    Resources . divide ( calculator ,    clusterResource ,    usedResources ,    clusterResource )  ;", "usedCapacity    =     ( Resources . equals ( queueLimit ,    Resources . none (  )  )  )     ?     0     :    Resources . divide ( calculator ,    clusterResource ,    usedResources ,    queueLimit )  ;", "}", "childQueue . setUsedCapacity ( usedCapacity )  ;", "childQueue . setAbsoluteUsedCapacity ( absoluteUsedCapacity )  ;", "Resource   available    =    Resources . subtract ( queueLimit ,    usedResources )  ;", "childQueue . getMetrics (  )  . setAvailableResourcesToQueue ( Resources . max ( calculator ,    clusterResource ,    available ,    Resources . none (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["updateQueueStatistics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( mappings )     !  =    null )     &  &     (  ( mappings . size (  )  )     >     0  )  )     {", "try    {", "String   mappedQueue    =    getMappedQueue ( user )  ;", "if    ( mappedQueue    !  =    null )     {", "if    (  ( queueName . equals ( DEFAULT _ QUEUE _ NAME )  )     |  |     ( overrideWithQueueMappings )  )     {", ". LOG . info (  (  (  (  (  (  (  (  (  (  \" Application    \"     +    applicationId )     +     \"    user    \"  )     +    user )     +     \"    mapping    [  \"  )     +    queueName )     +     \"  ]    to    [  \"  )     +    mappedQueue )     +     \"  ]    override    \"  )     +     ( overrideWithQueueMappings )  )  )  ;", "queueName    =    mappedQueue ;", "RMApp   rmApp    =    rmContext . getRMApps (  )  . get ( applicationId )  ;", "rmApp . setQueue ( queueName )  ;", "}", "}", "}    catch    ( IOException   ioex )     {", "String   message    =     (  (  (  (  \" Failed   to   submit   application    \"     +    applicationId )     +     \"    submitted   by   user    \"  )     +    user )     +     \"    reason :     \"  )     +     ( ioex . getMessage (  )  )  ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    message )  )  ;", "return ;", "}", "}", "CSQueue   queue    =    getQueue ( queueName )  ;", "if    ( queue    =  =    null )     {", "String   message    =     (  (  (  (  \" Application    \"     +    applicationId )     +     \"    submitted   by   user    \"  )     +    user )     +     \"    to   unknown   queue :     \"  )     +    queueName ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    message )  )  ;", "return ;", "}", "if    (  !  ( queue   instanceof   LeafQueue )  )     {", "String   message    =     (  (  (  (  \" Application    \"     +    applicationId )     +     \"    submitted   by   user    \"  )     +    user )     +     \"    to   non - leaf   queue :     \"  )     +    queueName ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    message )  )  ;", "return ;", "}", "try    {", "queue . submitApplication ( applicationId ,    user ,    queueName )  ;", "}    catch    ( AccessControlException   ace )     {", ". LOG . info (  (  (  (  (  (  \" Failed   to   submit   application    \"     +    applicationId )     +     \"    to   queue    \"  )     +    queueName )     +     \"    from   user    \"  )     +    user )  ,    ace )  ;", "this . rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    ace . toString (  )  )  )  ;", "return ;", "}", "queue . getMetrics (  )  . submitApp ( user )  ;", "SchedulerApplication < FiCaSchedulerApp >    application    =    new   SchedulerApplication < FiCaSchedulerApp >  ( queue ,    user )  ;", "applications . put ( applicationId ,    application )  ;", ". LOG . info (  (  (  (  (  (  \" Accepted   application    \"     +    applicationId )     +     \"    from   user :     \"  )     +    user )     +     \"  ,    in   queue :     \"  )     +    queueName )  )  ;", "if    ( isAppRecovering )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  ( applicationId    +     \"    is   recovering .    Skip   notifying   APP _ ACCEPTED \"  )  )  ;", "}", "} else    {", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppEvent ( applicationId ,    RMAppEventType . APP _ ACCEPTED )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FiCaSchedulerApp >    application    =    applications . get ( applicationAttemptId . getApplicationId (  )  )  ;", "CSQueue   queue    =     (  ( CSQueue )     ( application . getQueue (  )  )  )  ;", "FiCaSchedulerApp   attempt    =    new   FiCaSchedulerApp ( applicationAttemptId ,    application . getUser (  )  ,    queue ,    queue . getActiveUsersManager (  )  ,    rmContext )  ;", "if    ( transferStateFromPreviousAttempt )     {", "attempt . transferStateFromPreviousAttempt ( application . getCurrentAppAttempt (  )  )  ;", "}", "application . setCurrentAppAttempt ( attempt )  ;", "queue . submitApplicationAttempt ( attempt ,    application . getUser (  )  )  ;", ". LOG . info (  (  (  (  (  (  \" Added   Application   Attempt    \"     +    applicationAttemptId )     +     \"    to   scheduler   from   user    \"  )     +     ( application . getUser (  )  )  )     +     \"    in   queue    \"  )     +     ( queue . getQueueName (  )  )  )  )  ;", "if    ( isAttemptRecovering )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  ( applicationAttemptId    +     \"    is   recovering .    Skipping   notifying   ATTEMPT _ ADDED \"  )  )  ;", "}", "} else    {", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppAttemptEvent ( applicationAttemptId ,    RMAppAttemptEventType . ATTEMPT _ ADDED )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "for    ( Map . Entry < String ,    CSQueue >    e    :    newQueues . entrySet (  )  )     {", "String   queueName    =    e . getKey (  )  ;", "CSQueue   queue    =    e . getValue (  )  ;", "if    (  !  ( queues . containsKey ( queueName )  )  )     {", "queues . put ( queueName ,    queue )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["addNewQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "this . nodes . put ( nodeManager . getNodeID (  )  ,    new   FiCaSchedulerNode ( nodeManager ,    usePortForNodeName )  )  ;", "Resources . addTo ( clusterResource ,    nodeManager . getTotalCapability (  )  )  ;", "root . updateClusterResource ( clusterResource )  ;", "int   numNodes    =    numNodeManagers . incrementAndGet (  )  ;", ". LOG . info (  (  (  (  \" Added   node    \"     +     ( nodeManager . getNodeAddress (  )  )  )     +     \"    clusterResource :     \"  )     +     ( clusterResource )  )  )  ;", "if    (  ( scheduleAsynchronously )     &  &     ( numNodes    =  =     1  )  )     {", "asyncSchedulerThread . beginSchedule (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "RMContainer   reservedContainer    =    node . getReservedContainer (  )  ;", "if    ( reservedContainer    !  =    null )     {", "FiCaSchedulerApp   reservedApplication    =    getCurrentAttemptForContainer ( reservedContainer . getContainerId (  )  )  ;", ". LOG . info (  (  (  (  \" Trying   to   fulfill   reservation   for   application    \"     +     ( reservedApplication . getApplicationId (  )  )  )     +     \"    on   node :     \"  )     +     ( node . getNodeID (  )  )  )  )  ;", "LeafQueue   queue    =     (  ( LeafQueue )     ( reservedApplication . getQueue (  )  )  )  ;", "CSAssignment   assignment    =    queue . assignContainers ( clusterResource ,    node )  ;", "RMContainer   excessReservation    =    assignment . getExcessReservation (  )  ;", "if    ( excessReservation    !  =    null )     {", "Container   container    =    excessReservation . getContainer (  )  ;", "queue . completedContainer ( clusterResource ,    assignment . getApplication (  )  ,    node ,    excessReservation ,    SchedulerUtils . createAbnormalContainerStatus ( container . getId (  )  ,    SchedulerUtils . UNRESERVED _ CONTAINER )  ,    RMContainerEventType . RELEASED ,    null )  ;", "}", "}", "if    (  ( node . getReservedContainer (  )  )     =  =    null )     {", "if    ( Resources . greaterThanOrEqual ( calculator ,    getClusterResource (  )  ,    node . getAvailableResource (  )  ,    minimumAllocation )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Trying   to   schedule   on   node :     \"     +     ( node . getNodeName (  )  )  )     +     \"  ,    available :     \"  )     +     ( node . getAvailableResource (  )  )  )  )  ;", "}", "root . assignContainers ( clusterResource ,    node )  ;", "}", "} else    {", ". LOG . info (  (  (  (  \" Skipping   scheduling   since   node    \"     +     ( node . getNodeID (  )  )  )     +     \"    is   reserved   by   application    \"  )     +     ( node . getReservedContainer (  )  . getContainerId (  )  . getApplicationAttemptId (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["allocateContainersToNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FiCaSchedulerApp >    application    =    applications . get ( applicationId )  ;", "if    ( application    =  =    null )     {", ". LOG . warn (  (  \" Couldn ' t   find   application    \"     +    applicationId )  )  ;", "return ;", "}", "CSQueue   queue    =     (  ( CSQueue )     ( application . getQueue (  )  )  )  ;", "if    (  !  ( queue   instanceof   LeafQueue )  )     {", ". LOG . error (  (  (  \" Cannot   finish   application    \"     +     \" from   non - leaf   queue :     \"  )     +     ( queue . getQueueName (  )  )  )  )  ;", "} else    {", "queue . finishApplication ( applicationId ,    application . getUser (  )  )  ;", "}", "application . stop ( finalState )  ;", "applications . remove ( applicationId )  ;", "}", "METHOD_END"], "methodName": ["doneApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacityScheduler . LOG . info (  (  (  (  (  \" Application   Attempt    \"     +    applicationAttemptId )     +     \"    is   done .  \"  )     +     \"    finalState =  \"  )     +    rmAppAttemptFinalState )  )  ;", "FiCaSchedulerApp   attempt    =    getApplicationAttempt ( applicationAttemptId )  ;", "SchedulerApplication < FiCaSchedulerApp >    application    =    applications . get ( applicationAttemptId . getApplicationId (  )  )  ;", "if    (  ( application    =  =    null )     |  |     ( attempt    =  =    null )  )     {", "CapacityScheduler . LOG . info (  (  (  \" Unknown   application    \"     +    applicationAttemptId )     +     \"    has   completed !  \"  )  )  ;", "return ;", "}", "for    ( RMContainer   rmContainer    :    attempt . getLiveContainers (  )  )     {", "if    ( keepContainers    &  &     ( rmContainer . getState (  )  . equals ( RMContainerState . RUNNING )  )  )     {", "CapacityScheduler . LOG . info (  (  \" Skip   killing    \"     +     ( rmContainer . getContainerId (  )  )  )  )  ;", "continue ;", "}", "completedContainer ( rmContainer ,    SchedulerUtils . createAbnormalContainerStatus ( rmContainer . getContainerId (  )  ,    SchedulerUtils . COMPLETED _ APPLICATION )  ,    RMContainerEventType . KILL )  ;", "}", "for    ( RMContainer   rmContainer    :    attempt . getReservedContainers (  )  )     {", "completedContainer ( rmContainer ,    SchedulerUtils . createAbnormalContainerStatus ( rmContainer . getContainerId (  )  ,     \" Application   Complete \"  )  ,    RMContainerEventType . KILL )  ;", "}", "attempt . stop ( rmAppAttemptFinalState )  ;", "String   queueName    =    attempt . getQueue (  )  . getQueueName (  )  ;", "CSQueue   queue    =    queues . get ( queueName )  ;", "if    (  !  ( queue   instanceof   LeafQueue )  )     {", "CapacityScheduler . LOG . error (  (  (  \" Cannot   finish   application    \"     +     \" from   non - leaf   queue :     \"  )     +    queueName )  )  ;", "} else    {", "queue . finishApplicationAttempt ( attempt ,    queue . getQueueName (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["doneApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodes ;", "}", "METHOD_END"], "methodName": ["getAllNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CSQueue   ret    =    this . getQueue ( queue )  ;", "if    ( ret    =  =    null )     {", "throw   new   YException (  (  (  \" The   specified   Queue :     \"     +    queue )     +     \"    doesn ' t   exist \"  )  )  ;", "}", "if    (  !  ( ret   instanceof   LeafQueue )  )     {", "throw   new   YException (  (  (  \" The   specified   Queue :     \"     +    queue )     +     \"    is   not   a   Leaf   Queue .    Move   is   supported   only   for   Leaf   Queues .  \"  )  )  ;", "}", "return    (  ( LeafQueue )     ( ret )  )  ;", "}", "METHOD_END"], "methodName": ["getAndCheckLeafQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   asyncScheduleInterval ;", "}", "METHOD_END"], "methodName": ["getAsyncScheduleInterval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "for    ( CapacitySchedulerConfiguration . QueueMapping   mapping    :    mappings )     {", "if    (  ( mapping . type )     =  =     ( CapacitySchedulerConfiguration . QueueMapping . MappingType . USER )  )     {", "if    ( mapping . source . equals ( CapacityScheduler . CURRENT _ USER _ MAPPING )  )     {", "if    ( mapping . queue . equals ( CapacityScheduler . CURRENT _ USER _ MAPPING )  )     {", "return   user ;", "} else", "if    ( mapping . queue . equals ( CapacityScheduler . PRIMARY _ GROUP _ MAPPING )  )     {", "return   groups . getGroups ( user )  . get (  0  )  ;", "} else    {", "return   mapping . queue ;", "}", "}", "if    ( user . equals ( mapping . source )  )     {", "return   mapping . queue ;", "}", "}", "if    (  ( mapping . type )     =  =     ( CapacitySchedulerConfiguration . QueueMapping . MappingType . GROUP )  )     {", "for    ( String   userGroups    :    groups . getGroups ( user )  )     {", "if    ( userGroups . equals ( mapping . source )  )     {", "return   mapping . queue ;", "}", "}", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getMappedQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   getMappedQueue ( user )  ;", "}", "METHOD_END"], "methodName": ["getMappedQueueForTest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodes . get ( nodeId )  ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( queueName    =  =    null )     {", "return   null ;", "}", "return   queues . get ( queueName )  ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return   root ;", "}", "METHOD_END"], "methodName": ["getRootQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    loadCapacitySchedulerConfiguration ( configuration )  ;", "validateConf ( this . conf )  ;", "this . minimumAllocation    =    this . conf . getMinimumAllocation (  )  ;", "this . maximumAllocation    =    this . conf . getMaximumAllocation (  )  ;", "this . calculator    =    this . conf . getResourceCalculator (  )  ;", "this . usePortForNodeName    =    this . conf . getUsePortForNodeName (  )  ;", "this . applications    =    new   ConcurrentHashMap < ApplicationId ,    SchedulerApplication < FiCaSchedulerApp >  >  (  )  ;", "initializeQueues ( this . conf )  ;", "scheduleAsynchronously    =    this . conf . getScheduleAynschronously (  )  ;", "asyncScheduleInterval    =    this . conf . getLong ( CapacityScheduler . ASYNC _ SCHEDULER _ INTERVAL ,    CapacityScheduler . DEFAULT _ ASYNC _ SCHEDULER _ INTERVAL )  ;", "if    ( scheduleAsynchronously )     {", "asyncSchedulerThread    =    new   CapacityScheduler . AsyncScheduleThread ( this )  ;", "}", "CapacityScheduler . LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" Initialized   CapacityScheduler   with    \"     +     \" calculator =  \"  )     +     ( getResourceCalculator (  )  . getClass (  )  )  )     +     \"  ,     \"  )     +     \" minimumAllocation =  <  \"  )     +     ( getMinimumResourceCapability (  )  )  )     +     \"  >  ,     \"  )     +     \" maximumAllocation =  <  \"  )     +     ( getMaximumResourceCapability (  )  )  )     +     \"  >  ,     \"  )     +     \" asynchronousScheduling =  \"  )     +     ( scheduleAsynchronously )  )     +     \"  ,     \"  )     +     \" asyncScheduleInterval =  \"  )     +     ( asyncScheduleInterval )  )     +     \" ms \"  )  )  ;", "}", "METHOD_END"], "methodName": ["initScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "overrideWithQueueMappings    =    conf . getOverrideWithQueueMappings (  )  ;", ". LOG . info (  (  \" Initialized   queue   mappings ,    override :     \"     +     ( overrideWithQueueMappings )  )  )  ;", "List < Configuration . QueueMapping >    newMappings    =    conf . getQueueMappings (  )  ;", "for    ( Configuration . QueueMapping   mapping    :    newMappings )     {", "if    (  (  !  ( mapping . queue . equals (  . CURRENT _ USER _ MAPPING )  )  )     &  &     (  !  ( mapping . queue . equals (  . PRIMARY _ GROUP _ MAPPING )  )  )  )     {", "CSQueue   queue    =    queues . get ( mapping . queue )  ;", "if    (  ( queue    =  =    null )     |  |     (  !  ( queue   instanceof   LeafQueue )  )  )     {", "throw   new   IOException (  (  \" mapping   contains   invalid   or   non - leaf   queue    \"     +     ( mapping . queue )  )  )  ;", "}", "}", "}", "mappings    =    newMappings ;", "if    (  ( mappings . size (  )  )     >     0  )     {", "groups    =    new   Groups ( conf )  ;", "}", "}", "METHOD_END"], "methodName": ["initializeQueueMappings"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "root    =    CapacityScheduler . parseQueue ( this ,    conf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    CapacityScheduler . noop )  ;", "CapacityScheduler . LOG . info (  (  \" Initialized   root   queue    \"     +     ( root )  )  )  ;", "initializeQueueMappings (  )  ;", "}", "METHOD_END"], "methodName": ["initializeQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "try    {", "InputStream   CSInputStream    =    this . rmContext . getConfigurationProvider (  )  . getConfigurationInputStream ( configuration ,    CS _ CONFIGURATION _ FILE )  ;", "if    ( CSInputStream    !  =    null )     {", "configuration . addResource ( CSInputStream )  ;", "return   new   Configuration ( configuration ,    false )  ;", "}", "return   new   Configuration ( configuration ,    true )  ;", "}    catch    ( Exception   e )     {", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["loadCapacitySchedulerConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( CapacityScheduler . LOG . isDebugEnabled (  )  )     {", "CapacityScheduler . LOG . debug (  (  (  (  \" nodeUpdate :     \"     +    nm )     +     \"    clusterResources :     \"  )     +     ( clusterResource )  )  )  ;", "}", "FiCaSchedulerNode   node    =    getNode ( nm . getNodeID (  )  )  ;", "if    ( SchedulerUtils . updateResourceIfChanged ( node ,    nm ,    clusterResource ,    CapacityScheduler . LOG )  )     {", "root . updateClusterResource ( clusterResource )  ;", "}", "List < UpdatedContainerInfo >    containerInfoList    =    nm . pullContainerUpdates (  )  ;", "List < ContainerStatus >    newlyLaunchedContainers    =    new   ArrayList < ContainerStatus >  (  )  ;", "List < ContainerStatus >    completedContainers    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( UpdatedContainerInfo   containerInfo    :    containerInfoList )     {", "newlyLaunchedContainers . addAll ( containerInfo . getNewlyLaunchedContainers (  )  )  ;", "completedContainers . addAll ( containerInfo . getCompletedContainers (  )  )  ;", "}", "for    ( ContainerStatus   launchedContainer    :    newlyLaunchedContainers )     {", "containerLaunchedOnNode ( launchedContainer . getContainerId (  )  ,    node )  ;", "}", "for    ( ContainerStatus   completedContainer    :    completedContainers )     {", "ContainerId   containerId    =    completedContainer . getContainerId (  )  ;", "CapacityScheduler . LOG . debug (  (  \" Container   FINISHED :     \"     +    containerId )  )  ;", "completedContainer ( getRMContainer ( containerId )  ,    completedContainer ,    RMContainerEventType . FINISHED )  ;", "}", "if    ( CapacityScheduler . LOG . isDebugEnabled (  )  )     {", "CapacityScheduler . LOG . debug (  (  (  (  \" Node   being   looked   for   scheduling    \"     +    nm )     +     \"    availableResource :     \"  )     +     ( node . getAvailableResource (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["nodeUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CSQueue   queue ;", "String [  ]    childQueueNames    =    conf . getQueues (  ( parent    =  =    null    ?    queueName    :     (  ( parent . getQueuePath (  )  )     +     \"  .  \"  )     +    queueName )  )  ;", "if    (  ( childQueueNames    =  =    null )     |  |     (  ( childQueueNames . length )     =  =     0  )  )     {", "if    ( null    =  =    parent )     {", "throw   new   IllegalStateException (  (  \" Queue   configuration   missing   child   queue   names   for    \"     +    queueName )  )  ;", "}", "queue    =    new   LeafQueue ( csContext ,    queueName ,    parent ,    oldQueues . get ( queueName )  )  ;", "queue    =    hook . hook ( queue )  ;", "} else    {", "ParentQueue   parentQueue    =    new   ParentQueue ( csContext ,    queueName ,    parent ,    oldQueues . get ( queueName )  )  ;", "queue    =    hook . hook ( parentQueue )  ;", "List < CSQueue >    childQueues    =    new   ArrayList < CSQueue >  (  )  ;", "for    ( String   childQueueName    :    childQueueNames )     {", "CSQueue   childQueue    =     . parseQueue ( csContext ,    conf ,    queue ,    childQueueName ,    queues ,    oldQueues ,    hook )  ;", "childQueues . add ( childQueue )  ;", "}", "parentQueue . setChildQueues ( childQueues )  ;", "}", "if    (  (  (  ( queue   instanceof   LeafQueue )     =  =    true )     &  &     ( queues . containsKey ( queueName )  )  )     &  &     (  (  ( queues . get ( queueName )  )    instanceof   LeafQueue )     =  =    true )  )     {", "throw   new   IOException (  (  (  \" Two   leaf   queues   were   named    \"     +    queueName )     +     \"  .    Leaf   queue   names   must   be   distinct \"  )  )  ;", "}", "queues . put ( queueName ,    queue )  ;", ". LOG . info (  (  \" Initialized   queue :     \"     +    queue )  )  ;", "return   queue ;", "}", "METHOD_END"], "methodName": ["parseQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    CSQueue >    newQueues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   newRoot    =     . parseQueue ( this ,    conf ,    null ,    Configuration . ROOT ,    newQueues ,    queues ,     . noop )  ;", "validateExistingQueues ( queues ,    newQueues )  ;", "addNewQueues ( queues ,    newQueues )  ;", "root . reinitialize ( newRoot ,    clusterResource )  ;", "initializeQueueMappings (  )  ;", "}", "METHOD_END"], "methodName": ["reinitializeQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerNode   node    =    nodes . get ( nodeInfo . getNodeID (  )  )  ;", "if    ( node    =  =    null )     {", "return ;", "}", "Resources . subtractFrom ( clusterResource ,    node . getRMNode (  )  . getTotalCapability (  )  )  ;", "root . updateClusterResource ( clusterResource )  ;", "int   numNodes    =    numNodeManagers . decrementAndGet (  )  ;", "if    (  ( scheduleAsynchronously )     &  &     ( numNodes    =  =     0  )  )     {", "asyncSchedulerThread . suspendSchedule (  )  ;", "}", "List < RMContainer >    runningContainers    =    node . getRunningContainers (  )  ;", "for    ( RMContainer   container    :    runningContainers )     {", "completedContainer ( container ,    SchedulerUtils . createAbnormalContainerStatus ( container . getContainerId (  )  ,    SchedulerUtils . LOST _ CONTAINER )  ,    RMContainerEventType . KILL )  ;", "}", "RMContainer   reservedContainer    =    node . getReservedContainer (  )  ;", "if    ( reservedContainer    !  =    null )     {", "completedContainer ( reservedContainer ,    SchedulerUtils . createAbnormalContainerStatus ( reservedContainer . getContainerId (  )  ,    SchedulerUtils . LOST _ CONTAINER )  ,    RMContainerEventType . KILL )  ;", "}", "this . nodes . remove ( nodeInfo . getNodeID (  )  )  ;", ". LOG . info (  (  (  (  \" Removed   node    \"     +     ( nodeInfo . getNodeAddress (  )  )  )     +     \"    clusterResource :     \"  )     +     ( clusterResource )  )  )  ;", "}", "METHOD_END"], "methodName": ["removeNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "int   current    =     0  ;", "Collection < FiCaSchedulerNode >    nodes    =    cs . getAllNodes (  )  . values (  )  ;", "int   start    =     . random . nextInt ( nodes . size (  )  )  ;", "for    ( FiCaSchedulerNode   node    :    nodes )     {", "if    (  ( current +  +  )     >  =    start )     {", "cs . allocateContainersToNode ( node )  ;", "}", "}", "for    ( FiCaSchedulerNode   node    :    nodes )     {", "cs . allocateContainersToNode ( node )  ;", "}", "try    {", "Thread . sleep ( cs . getAsyncScheduleInterval (  )  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "METHOD_END"], "methodName": ["schedule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( scheduleAsynchronously )     {", "Preconditions . checkNotNull ( asyncThread ,     \" asyncThread   is   null \"  )  ;", "asyncThread . start (  )  ;", "}", "}", "METHOD_END"], "methodName": ["startSchedulerThreads"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "int   minMem    =    conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "int   maxMem    =    conf . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  ;", "if    (  ( minMem    <  =     0  )     |  |     ( minMem    >    maxMem )  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  (  (  (  (  (  \" Invalid   resource   scheduler   memory \"     +     (  \"    allocation   configuration \"     +     \"  ,     \"  )  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )     +     \"  =  \"  )     +    minMem )     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )     +     \"  =  \"  )     +    maxMem )     +     \"  ,    min   and   max   should   be   greater   than    0  \"  )     +     \"  ,    max   should   be   no   smaller   than   min .  \"  )  )  ;", "}", "int   minVcores    =    conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "int   maxVcores    =    conf . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  ;", "if    (  ( minVcores    <  =     0  )     |  |     ( minVcores    >    maxVcores )  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  (  (  (  (  (  \" Invalid   resource   scheduler   vcores \"     +     (  \"    allocation   configuration \"     +     \"  ,     \"  )  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  )     +     \"  =  \"  )     +    minVcores )     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  )     +     \"  =  \"  )     +    maxVcores )     +     \"  ,    min   and   max   should   be   greater   than    0  \"  )     +     \"  ,    max   should   be   no   smaller   than   min .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "for    ( String   queue    :    queues . keySet (  )  )     {", "if    (  !  ( newQueues . containsKey ( queue )  )  )     {", "throw   new   IOException (  ( queue    +     \"    cannot   be   found   during   refh !  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateExistingQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "String   queuePrefix    =    getQueuePrefix ( queue )  ;", "String   defaultAcl    =     ( queue . equals (  . ROOT )  )     ?     . ALL _ ACL    :     . NONE _ ACL ;", "String   aclString    =    get (  ( queuePrefix    +     (  . getAclKey ( acl )  )  )  ,    defaultAcl )  ;", "return   new   AccessControlList ( aclString )  ;", "}", "METHOD_END"], "methodName": ["getAcl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return    \" acl _  \"     +     ( acl . toString (  )  . toLowerCase (  )  )  ;", "}", "METHOD_END"], "methodName": ["getAclKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "Map < QueueACL ,    AccessControlList >    acls    =    new   HashMap < QueueACL ,    AccessControlList >  (  )  ;", "for    ( QueueACL   acl    :    QueueACL . values (  )  )     {", "acls . put ( acl ,    getAcl ( queue ,    acl )  )  ;", "}", "return   acls ;", "}", "METHOD_END"], "methodName": ["getAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "float   capacity    =     ( queue . equals (  \" root \"  )  )     ?     1  0  0  .  0 F    :    getFloat (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . CAPACITY )  )  ,    CapacitySchedulerConfiguration . UNDEFINED )  ;", "if    (  ( capacity    <     ( CapacitySchedulerConfiguration . MINIMUM _ CAPACITY _ VALUE )  )     |  |     ( capacity    >     ( CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY _ VALUE )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Illegal    \"     +     \" capacity   of    \"  )     +    capacity )     +     \"    for   queue    \"  )     +    queue )  )  ;", "}", "CapacitySchedulerConfiguration . LOG . debug (  (  (  (  \" CSConf    -    getCapacity :    queuePrefix =  \"     +     ( getQueuePrefix ( queue )  )  )     +     \"  ,    capacity =  \"  )     +    capacity )  )  ;", "return   capacity ;", "}", "METHOD_END"], "methodName": ["getCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( CapacitySchedulerConfiguration . ENABLE _ USER _ METRICS ,    CapacitySchedulerConfiguration . DEFAULT _ ENABLE _ USER _ METRICS )  ;", "}", "METHOD_END"], "methodName": ["getEnableUserMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   maximumMemory    =    getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  ;", "int   maximumCores    =    getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  ;", "return   Rs . createR ( maximumMemory ,    maximumCores )  ;", "}", "METHOD_END"], "methodName": ["getMaximumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getFloat (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . MAXIMUM _ AM _ RESOURCE _ SUFFIX )  )  ,    getMaximumApplicationMasterResourcePercent (  )  )  ;", "}", "METHOD_END"], "methodName": ["getMaximumApplicationMasterResourcePerQueuePercent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getFloat ( CapacitySchedulerConfiguration . MAXIMUM _ APPLICATION _ MASTERS _ RESOURCE _ PERCENT ,    CapacitySchedulerConfiguration . DEFAULT _ MAXIMUM _ APPLICATIONMASTERS _ RESOURCE _ PERCENT )  ;", "}", "METHOD_END"], "methodName": ["getMaximumApplicationMasterResourcePercent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   maxApplicationsPerQueue    =    getInt (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . MAXIMUM _ APPLICATIONS _ SUFFIX )  )  ,     (  ( int )     ( CapacitySchedulerConfiguration . UNDEFINED )  )  )  ;", "return   maxApplicationsPerQueue ;", "}", "METHOD_END"], "methodName": ["getMaximumApplicationsPerQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "float   maxCapacity    =    getFloat (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY )  )  ,    CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY _ VALUE )  ;", "maxCapacity    =     ( maxCapacity    =  =     ( CapacitySchedulerConfiguration . DEFAULT _ MAXIMUM _ CAPACITY _ VALUE )  )     ?    CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY _ VALUE    :    maxCapacity ;", "return   maxCapacity ;", "}", "METHOD_END"], "methodName": ["getMaximumCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   maxApplications    =    getInt ( CapacitySchedulerConfiguration . MAXIMUM _ SYSTEM _ APPLICATIONS ,    CapacitySchedulerConfiguration . DEFAULT _ MAXIMUM _ SYSTEM _ APPLICATIIONS )  ;", "return   maxApplications ;", "}", "METHOD_END"], "methodName": ["getMaximumSystemApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   minimumMemory    =    getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "int   minimumCores    =    getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "return   Rs . createR ( minimumMemory ,    minimumCores )  ;", "}", "METHOD_END"], "methodName": ["getMinimumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   delay    =    getInt ( CapacitySchedulerConfiguration . NODE _ LOCALITY _ DELAY ,    CapacitySchedulerConfiguration . DEFAULT _ NODE _ LOCALITY _ DELAY )  ;", "return   delay    =  =     ( CapacitySchedulerConfiguration . DEFAULT _ NODE _ LOCALITY _ DELAY )     ?     0     :    delay ;", "}", "METHOD_END"], "methodName": ["getNodeLocalityDelay"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( CapacitySchedulerConfiguration . ENABLE _ QUEUE _ MAPPING _ OVERRIDE ,    CapacitySchedulerConfiguration . DEFAULT _ ENABLE _ QUEUE _ MAPPING _ OVERRIDE )  ;", "}", "METHOD_END"], "methodName": ["getOverrideWithQueueMappings"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "List < CapacitySchedulerConfiguration . QueueMapping >    mappings    =    new   ArrayList < CapacitySchedulerConfiguration . QueueMapping >  (  )  ;", "Collection < String >    mappingsString    =    CapacitySchedulerConfiguration . getTrimmedStringCollection ( CapacitySchedulerConfiguration . QUEUE _ MAPPING )  ;", "for    ( String   mappingValue    :    mappingsString )     {", "String [  ]    mapping    =    CapacitySchedulerConfiguration . getTrimmedStringCollection ( mappingValue ,     \"  :  \"  )  . toArray ( new   String [  ]  {        }  )  ;", "if    (  (  (  ( mapping . length )     !  =     3  )     |  |     (  ( mapping [  1  ]  . length (  )  )     =  =     0  )  )     |  |     (  ( mapping [  2  ]  . length (  )  )     =  =     0  )  )     {", "throw   new   IllegalArgumentException (  (  \" Illegal   queue   mapping    \"     +    mappingValue )  )  ;", "}", "CapacitySchedulerConfiguration . QueueMapping   m ;", "try    {", "CapacitySchedulerConfiguration . QueueMapping . MappingType   mappingType ;", "if    ( mapping [  0  ]  . equals (  \" u \"  )  )     {", "mappingType    =    CapacitySchedulerConfiguration . QueueMapping . MappingType . USER ;", "} else", "if    ( mapping [  0  ]  . equals (  \" g \"  )  )     {", "mappingType    =    CapacitySchedulerConfiguration . QueueMapping . MappingType . GROUP ;", "} else    {", "throw   new   IllegalArgumentException (  (  \" unknown   mapping   prefix    \"     +     ( mapping [  0  ]  )  )  )  ;", "}", "m    =    new   CapacitySchedulerConfiguration . QueueMapping ( mappingType ,    mapping [  1  ]  ,    mapping [  2  ]  )  ;", "}    catch    ( Throwable   t )     {", "throw   new   IllegalArgumentException (  (  \" Illegal   queue   mapping    \"     +    mappingValue )  )  ;", "}", "if    ( m    !  =    null )     {", "mappings . add ( m )  ;", "}", "}", "return   mappings ;", "}", "METHOD_END"], "methodName": ["getQueueMappings"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =     (  ( CapacitySchedulerConfiguration . PREFIX )     +    queue )     +     ( CapacitySchedulerConfiguration . DOT )  ;", "return   queueName ;", "}", "METHOD_END"], "methodName": ["getQueuePrefix"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration . LOG . debug (  (  \" CSConf    -    getQueues   called   for :    queuePrefix =  \"     +     ( getQueuePrefix ( queue )  )  )  )  ;", "String [  ]    queues    =    getStrings (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . QUEUES )  )  )  ;", "CapacitySchedulerConfiguration . LOG . debug (  (  (  (  \" CSConf    -    getQueues :    queuePrefix =  \"     +     ( getQueuePrefix ( queue )  )  )     +     \"  ,    queues =  \"  )     +     ( queues    =  =    null    ?     \"  \"     :    StringUtils . arrayToString ( queues )  )  )  )  ;", "return   queues ;", "}", "METHOD_END"], "methodName": ["getQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   ReflectionUtils . newInstance ( getClass ( CapacitySchedulerConfiguration . RESOURCE _ CALCULATOR _ CLASS ,    CapacitySchedulerConfiguration . DEFAULT _ RESOURCE _ CALCULATOR _ CLASS ,    ResourceCalculator . class )  ,    this )  ;", "}", "METHOD_END"], "methodName": ["getResourceCalculator"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( CapacitySchedulerConfiguration . SCHEDULE _ ASYNCHRONOUSLY _ ENABLE ,    CapacitySchedulerConfiguration . DEFAULT _ SCHEDULE _ ASYNCHRONOUSLY _ ENABLE )  ;", "}", "METHOD_END"], "methodName": ["getScheduleAynschronously"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "String   state    =    get (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . STATE )  )  )  ;", "return   state    !  =    null    ?    QueueState . valueOf ( state . toUpperCase (  )  )     :    QueueState . RUNNING ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "List < String >    values    =    new   ArrayList < String >  (  )  ;", "if    ( str    =  =    null )", "return   values ;", "StringTokeniztokeniz =    new   StringTokenizstr ,    delim )  ;", "while    ( tokenizhasMoreTokens (  )  )     {", "String   next    =    tokeniznextToken (  )  ;", "if    (  ( next    =  =    null )     |  |     ( next . trim (  )  . isEmpty (  )  )  )     {", "continue ;", "}", "values . add ( next . trim (  )  )  ;", "}", "return   values ;", "}", "METHOD_END"], "methodName": ["getTrimmedStringCollection"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( RM _ SCHEDULER _ INCLUDE _ PORT _ IN _ NODE _ NAME ,    DEFAULT _ RM _ SCHEDULER _ USE _ PORT _ FOR _ NODE _ NAME )  ;", "}", "METHOD_END"], "methodName": ["getUsePortForNodeName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   userLimit    =    getInt (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . USER _ LIMIT )  )  ,    CapacitySchedulerConfiguration . DEFAULT _ USER _ LIMIT )  ;", "return   userLimit ;", "}", "METHOD_END"], "methodName": ["getUserLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "float   userLimitFactor    =    getFloat (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . USER _ LIMIT _ FACTOR )  )  ,    CapacitySchedulerConfiguration . DEFAULT _ USER _ LIMIT _ FACTOR )  ;", "return   userLimitFactor ;", "}", "METHOD_END"], "methodName": ["getUserLimitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "String   queuePrefix    =    getQueuePrefix ( queue )  ;", "set (  ( queuePrefix    +     (  . getAclKey ( acl )  )  )  ,    aclString )  ;", "}", "METHOD_END"], "methodName": ["setAcl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "for    ( Map . Entry < QueueACL ,    AccessControlList >    e    :    acls . entrySet (  )  )     {", "setAcl ( queue ,    e . getKey (  )  ,    e . getValue (  )  . getAclString (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "if    ( queue . equals (  \" root \"  )  )     {", "throw   new   IllegalArgumentException (  \" Cannot   set   capacity ,    root   queue   has   a   fixed   capacity   of    1  0  0  .  0 f \"  )  ;", "}", "setFloat (  (  ( getQueuePrefix ( queue )  )     +     (  . CAPACITY )  )  ,    capacity )  ;", ". LOG . debug (  (  (  (  \" CSConf    -    setCapacity :    queuePrefix =  \"     +     ( getQueuePrefix ( queue )  )  )     +     \"  ,    capacity =  \"  )     +    capacity )  )  ;", "}", "METHOD_END"], "methodName": ["setCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "if    ( maxCapacity    >     ( CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY _ VALUE )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Illegal    \"     +     \" maximum - capacity   of    \"  )     +    maxCapacity )     +     \"    for   queue    \"  )     +    queue )  )  ;", "}", "setFloat (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY )  )  ,    maxCapacity )  ;", "CapacitySchedulerConfiguration . LOG . debug (  (  (  (  \" CSConf    -    setMaxCapacity :    queuePrefix =  \"     +     ( getQueuePrefix ( queue )  )  )     +     \"  ,    maxCapacity =  \"  )     +    maxCapacity )  )  ;", "}", "METHOD_END"], "methodName": ["setMaximumCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "set (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . QUEUES )  )  ,    StringUtils . arrayToString ( subQueues )  )  ;", "CapacitySchedulerConfiguration . LOG . debug (  (  (  (  \" CSConf    -    setQueues :    qPrefix =  \"     +     ( getQueuePrefix ( queue )  )  )     +     \"  ,    queues =  \"  )     +     ( StringUtils . arrayToString ( subQueues )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "setClass ( CapacitySchedulerConfiguration . RESOURCE _ CALCULATOR _ CLASS ,    resourceCalculatorClass ,    ResourceCalculator . class )  ;", "}", "METHOD_END"], "methodName": ["setResourceComparator"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "setBoolean ( CapacitySchedulerConfiguration . SCHEDULE _ ASYNCHRONOUSLY _ ENABLE ,    async )  ;", "}", "METHOD_END"], "methodName": ["setScheduleAynschronously"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "setInt (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . USER _ LIMIT )  )  ,    userLimit )  ;", "CapacitySchedulerConfiguration . LOG . debug (  (  (  (  \" here   setUserLimit :    queuePrefix =  \"     +     ( getQueuePrefix ( queue )  )  )     +     \"  ,    userLimit =  \"  )     +     ( getUserLimit ( queue )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setUserLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "setFloat (  (  ( getQueuePrefix ( queue )  )     +     ( CapacitySchedulerConfiguration . USER _ LIMIT _ FACTOR )  )  ,    userLimitFactor )  ;", "}", "METHOD_END"], "methodName": ["setUserLimitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "for    ( Iterator < FiCaSchedulerApp >    i    =    pendingApplications . iterator (  )  ;    i . hasNext (  )  ;  )     {", "FiCaSchedulerApp   application    =    i . next (  )  ;", "if    (  ( getNumActiveApplications (  )  )     >  =     ( getMaximumActiveApplications (  )  )  )     {", "break ;", "}", ". User   user    =    getUser ( application . getUser (  )  )  ;", "if    (  ( user . getActiveApplications (  )  )     <     ( getMaximumActiveApplicationsPerUser (  )  )  )     {", "user . activateApplication (  )  ;", "activeApplications . add ( application )  ;", "i . remove (  )  ;", ". LOG . info (  (  (  (  (  (  \" Application    \"     +     ( application . getApplicationId (  )  )  )     +     \"    from   user :     \"  )     +     ( application . getUser (  )  )  )     +     \"    activated   in   queue :     \"  )     +     ( getQueueName (  )  )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["activateApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "user . submitApplication (  )  ;", "pendingApplications . add ( application )  ;", "applicationAttemptMap . put ( application . getApplicationAttemptId (  )  ,    application )  ;", "activateApplications (  )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" Application   added    -  \"     +     \"    appId :     \"  )     +     ( application . getApplicationId (  )  )  )     +     \"    user :     \"  )     +    user )     +     \"  ,  \"  )     +     \"    leaf - queue :     \"  )     +     ( getQueueName (  )  )  )     +     \"     # user - pending - applications :     \"  )     +     ( user . getPendingApplications (  )  )  )     +     \"     # user - active - applications :     \"  )     +     ( user . getActiveApplications (  )  )  )     +     \"     # queue - pending - applications :     \"  )     +     ( getNumPendingApplications (  )  )  )     +     \"     # queue - active - applications :     \"  )     +     ( getNumActiveApplications (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["addApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "Resources . addTo ( usedResources ,    resource )  ;", "CSQueueUtils . updateQueueStatistics ( resourceCalculator ,    this ,    getParent (  )  ,    clusterResource ,    minimumAllocation )  ;", "+  +  ( numContainers )  ;", "String   userName    =    application . getUser (  )  ;", ". User   user    =    getUser ( userName )  ;", "user . assignContainer ( resource )  ;", "Resources . subtractFrom ( application . getHeadroom (  )  ,    resource )  ;", "metrics . setAvailableResourcesToUser ( userName ,    application . getHeadroom (  )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  ( getQueueName (  )  )     +     \"    user =  \"  )     +    userName )     +     \"    used =  \"  )     +     ( usedResources )  )     +     \"    numContainers =  \"  )     +     ( numContainers )  )     +     \"    headroom    =     \"  )     +     ( application . getHeadroom (  )  )  )     +     \"    user - resources =  \"  )     +     ( user . getConsumedResources (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["allocateResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( LeafQueue . LOG . isDebugEnabled (  )  )     {", "LeafQueue . LOG . debug (  (  (  (  (  (  (  (  (  (  \" assignContainers :    node =  \"     +     ( node . getNodeName (  )  )  )     +     \"    application =  \"  )     +     ( application . getApplicationId (  )  )  )     +     \"    priority =  \"  )     +     ( priority . getPriority (  )  )  )     +     \"    request =  \"  )     +    request )     +     \"    type =  \"  )     +    type )  )  ;", "}", "Resource   capability    =    request . getCapability (  )  ;", "Resource   available    =    node . getAvailableResource (  )  ;", "Resource   totalResource    =    node . getTotalResource (  )  ;", "if    (  !  ( Resources . fitsIn ( capability ,    totalResource )  )  )     {", "LeafQueue . LOG . warn (  (  (  (  (  (  \" Node    :     \"     +     ( node . getNodeID (  )  )  )     +     \"    does   not   have   sufficient   resource   for   request    :     \"  )     +    request )     +     \"    node   total   capability    :     \"  )     +     ( node . getTotalResource (  )  )  )  )  ;", "return   Resources . none (  )  ;", "}", "assert   Resources . greaterThan ( resourceCalculator ,    clusterResource ,    available ,    Resources . none (  )  )  ;", "Container   container    =    getContainer ( rmContainer ,    application ,    node ,    capability ,    priority )  ;", "if    ( container    =  =    null )     {", "LeafQueue . LOG . warn (  \" Couldn ' t   get   container   for   allocation !  \"  )  ;", "return   Resources . none (  )  ;", "}", "int   availableContainers    =    resourceCalculator . computeAvailableContainers ( available ,    capability )  ;", "if    ( availableContainers    >     0  )     {", "if    ( rmContainer    !  =    null )     {", "unreserve ( application ,    priority ,    node ,    rmContainer )  ;", "}", "RMContainer   allocatedContainer    =    application . allocate ( type ,    node ,    priority ,    request ,    container )  ;", "if    ( allocatedContainer    =  =    null )     {", "return   Resources . none (  )  ;", "}", "node . allocateContainer ( allocatedContainer )  ;", "LeafQueue . LOG . info (  (  (  (  (  (  (  (  (  \" assignedContainer \"     +     \"    application   attempt =  \"  )     +     ( application . getApplicationAttemptId (  )  )  )     +     \"    container =  \"  )     +    container )     +     \"    queue =  \"  )     +     ( this )  )     +     \"    clusterResource =  \"  )     +    clusterResource )  )  ;", "return   container . getResource (  )  ;", "} else    {", "reserve ( application ,    priority ,    node ,    rmContainer ,    container )  ;", "LeafQueue . LOG . info (  (  (  (  (  (  (  (  (  (  (  \" Reserved   container    \"     +     \"    application   attempt =  \"  )     +     ( application . getApplicationAttemptId (  )  )  )     +     \"    resource =  \"  )     +     ( request . getCapability (  )  )  )     +     \"    queue =  \"  )     +     ( this . toString (  )  )  )     +     \"    node =  \"  )     +    node )     +     \"    clusterResource =  \"  )     +    clusterResource )  )  ;", "return   request . getCapability (  )  ;", "}", "}", "METHOD_END"], "methodName": ["assignContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "Resource   assigned    =    Resources . none (  )  ;", "ResourceRequest   nodeLocalResourceRequest    =    application . getResourceRequest ( priority ,    node . getNodeName (  )  )  ;", "if    ( nodeLocalResourceRequest    !  =    null )     {", "assigned    =    assignNodeLocalContainers ( clusterResource ,    nodeLocalResourceRequest ,    node ,    application ,    priority ,    reservedContainer )  ;", "if    ( Resources . greaterThan ( resourceCalculator ,    clusterResource ,    assigned ,    Resources . none (  )  )  )     {", "return   new   CSAssignment ( assigned ,    NodeType . NODE _ LOCAL )  ;", "}", "}", "ResourceRequest   rackLocalResourceRequest    =    application . getResourceRequest ( priority ,    node . getRackName (  )  )  ;", "if    ( rackLocalResourceRequest    !  =    null )     {", "if    (  !  ( rackLocalResourceRequest . getRelaxLocality (  )  )  )     {", "return    . SKIP _ ASSIGNMENT ;", "}", "assigned    =    assignRackLocalContainers ( clusterResource ,    rackLocalResourceRequest ,    node ,    application ,    priority ,    reservedContainer )  ;", "if    ( Resources . greaterThan ( resourceCalculator ,    clusterResource ,    assigned ,    Resources . none (  )  )  )     {", "return   new   CSAssignment ( assigned ,    NodeType . RACK _ LOCAL )  ;", "}", "}", "ResourceRequest   offSwitchResourceRequest    =    application . getResourceRequest ( priority ,    ANY )  ;", "if    ( offSwitchResourceRequest    !  =    null )     {", "if    (  !  ( offSwitchResourceRequest . getRelaxLocality (  )  )  )     {", "return    . SKIP _ ASSIGNMENT ;", "}", "return   new   CSAssignment ( assignOffSwitchContainers ( clusterResource ,    offSwitchResourceRequest ,    node ,    application ,    priority ,    reservedContainer )  ,    NodeType . OFF _ SWITCH )  ;", "}", "return    . SKIP _ ASSIGNMENT ;", "}", "METHOD_END"], "methodName": ["assignContainersOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( canAssign ( application ,    priority ,    node ,    NodeType . NODE _ LOCAL ,    reservedContainer )  )     {", "return   assignContainer ( clusterR ,    node ,    application ,    priority ,    nodeLocalRRequest ,    NodeType . NODE _ LOCAL ,    reservedContainer )  ;", "}", "return   Rs . none (  )  ;", "}", "METHOD_END"], "methodName": ["assignNodeLocalContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( canAssign ( application ,    priority ,    node ,    NodeType . OFF _ SWITCH ,    reservedContainer )  )     {", "return   assignContainer ( clusterR ,    node ,    application ,    priority ,    offSwitchRRequest ,    NodeType . OFF _ SWITCH ,    reservedContainer )  ;", "}", "return   Rs . none (  )  ;", "}", "METHOD_END"], "methodName": ["assignOffSwitchContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( canAssign ( application ,    priority ,    node ,    NodeType . RACK _ LOCAL ,    reservedContainer )  )     {", "return   assignContainer ( clusterR ,    node ,    application ,    priority ,    rackLocalRRequest ,    NodeType . RACK _ LOCAL ,    reservedContainer )  ;", "}", "return   Rs . none (  )  ;", "}", "METHOD_END"], "methodName": ["assignRackLocalContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "Priority   priority    =    rmContainer . getReservedPriority (  )  ;", "if    (  ( application . getTotalRequiredRs ( priority )  )     =  =     0  )     {", "return   new   CSAssignment ( application ,    rmContainer )  ;", "}", "assignContainersOnNode ( clusterR ,    node ,    application ,    priority ,    rmContainer )  ;", "return   new   CSAssignment ( Rs . none (  )  ,    NodeType . NODE _ LOCAL )  ;", "}", "METHOD_END"], "methodName": ["assignReservedContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "float   potentialNewCapacity    =    Resources . divide ( resourceCalculator ,    clusterResource ,    Resources . add ( usedResources ,    required )  ,    clusterResource )  ;", "if    ( potentialNewCapacity    >     ( absoluteMaxCapacity )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  ( getQueueName (  )  )     +     \"    usedResources :     \"  )     +     ( usedResources )  )     +     \"    clusterResources :     \"  )     +    clusterResource )     +     \"    currentCapacity    \"  )     +     ( Resources . divide ( resourceCalculator ,    clusterResource ,    usedResources ,    clusterResource )  )  )     +     \"    required    \"  )     +    required )     +     \"    potentialNewCapacity :     \"  )     +    potentialNewCapacity )     +     \"     (     \"  )     +     \"    max - capacity :     \"  )     +     ( absoluteMaxCapacity )  )     +     \"  )  \"  )  )  ;", "}", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["assignToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue . User   user    =    getUser ( userName )  ;", "if    ( Resources . greaterThan ( resourceCalculator ,    clusterResource ,    user . getConsumedResources (  )  ,    limit )  )     {", "if    ( LeafQueue . LOG . isDebugEnabled (  )  )     {", "LeafQueue . LOG . debug (  (  (  (  (  (  (  (  (  \" User    \"     +    userName )     +     \"    in   queue    \"  )     +     ( getQueueName (  )  )  )     +     \"    will   exceed   limit    -     \"  )     +     \"    consumed :     \"  )     +     ( user . getConsumedResources (  )  )  )     +     \"    limit :     \"  )     +    limit )  )  ;", "}", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["assignToUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( type    =  =     ( NodeType . OFF _ SWITCH )  )     {", "if    ( reservedContainer    !  =    null )     {", "return   true ;", "}", "ResourceRequest   offSwitchRequest    =    application . getResourceRequest ( priority ,    ANY )  ;", "long   missedOpportunities    =    application . getSchedulingOpportunities ( priority )  ;", "long   requiredContainers    =    offSwitchRequest . getNumContainers (  )  ;", "float   localityWaitFactor    =    application . getLocalityWaitFactor ( priority ,    getNumClusterNodes (  )  )  ;", "return    ( requiredContainers    *    localityWaitFactor )     <    missedOpportunities ;", "}", "ResourceRequest   rackLocalRequest    =    application . getResourceRequest ( priority ,    node . getRackName (  )  )  ;", "if    (  ( rackLocalRequest    =  =    null )     |  |     (  ( rackLocalRequest . getNumContainers (  )  )     <  =     0  )  )     {", "return   false ;", "}", "if    ( type    =  =     ( NodeType . RACK _ LOCAL )  )     {", "long   missedOpportunities    =    application . getSchedulingOpportunities ( priority )  ;", "return    ( Math . min ( getNumClusterNodes (  )  ,    getNodeLocalityDelay (  )  )  )     <    missedOpportunities ;", "}", "if    ( type    =  =     ( NodeType . NODE _ LOCAL )  )     {", "ResourceRequest   nodeLocalRequest    =    application . getResourceRequest ( priority ,    node . getNodeName (  )  )  ;", "if    ( nodeLocalRequest    !  =    null )     {", "return    ( nodeLocalRequest . getNumContainers (  )  )     >     0  ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["canAssign"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "final   Resource   queueCapacity    =    Resources . max ( resourceCalculator ,    clusterResource ,    Resources . multiplyAndNormalizeUp ( resourceCalculator ,    clusterResource ,    absoluteCapacity ,    minimumAllocation )  ,    required )  ;", "Resource   currentCapacity    =     ( Resources . lessThan ( resourceCalculator ,    clusterResource ,    usedResources ,    queueCapacity )  )     ?    queueCapacity    :    Resources . add ( usedResources ,    required )  ;", "final   int   activeUsers    =    activeUsersManager . getNumActiveUsers (  )  ;", "Resource   limit    =    Resources . roundUp ( resourceCalculator ,    Resources . min ( resourceCalculator ,    clusterResource ,    Resources . max ( resourceCalculator ,    clusterResource ,    Resources . divideAndCeil ( resourceCalculator ,    currentCapacity ,    activeUsers )  ,    Resources . divideAndCeil ( resourceCalculator ,    Resources . multiplyAndRoundDown ( currentCapacity ,    userLimit )  ,     1  0  0  )  )  ,    Resources . multiplyAndRoundDown ( queueCapacity ,    userLimitFactor )  )  ,    minimumAllocation )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", "String   userName    =    application . getUser (  )  ;", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" User   limit   computation   for    \"     +    userName )     +     \"    in   queue    \"  )     +     ( getQueueName (  )  )  )     +     \"    userLimit =  \"  )     +     ( userLimit )  )     +     \"    userLimitFactor =  \"  )     +     ( userLimitFactor )  )     +     \"    required :     \"  )     +    required )     +     \"    consumed :     \"  )     +     ( getUser ( userName )  . getConsumedResources (  )  )  )     +     \"    limit :     \"  )     +    limit )     +     \"    queueCapacity :     \"  )     +    queueCapacity )     +     \"    qconsumed :     \"  )     +     ( usedResources )  )     +     \"    currentCapacity :     \"  )     +    currentCapacity )     +     \"    activeUsers :     \"  )     +    activeUsers )     +     \"    clusterCapacity :     \"  )     +    clusterResource )  )  ;", "}", "return   limit ;", "}", "METHOD_END"], "methodName": ["computeUserLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "String   user    =    application . getUser (  )  ;", "Resource   userLimit    =    computeUserLimit ( application ,    clusterResource ,    required )  ;", "float   absoluteMaxAvailCapacity    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    this )  ;", "Resource   queueMaxCap    =    Resources . multiplyAndNormalizeDown ( resourceCalculator ,    clusterResource ,    absoluteMaxAvailCapacity ,    minimumAllocation )  ;", "Resource   userConsumed    =    getUser ( user )  . getConsumedResources (  )  ;", "Resource   headroom    =    Resources . subtract ( Resources . min ( resourceCalculator ,    clusterResource ,    userLimit ,    queueMaxCap )  ,    userConsumed )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  \" Headroom   calculation   for   user    \"     +    user )     +     \"  :     \"  )     +     \"    userLimit =  \"  )     +    userLimit )     +     \"    queueMaxCap =  \"  )     +    queueMaxCap )     +     \"    consumed =  \"  )     +    userConsumed )     +     \"    headroom =  \"  )     +    headroom )  )  ;", "}", "application . setHeadroom ( headroom )  ;", "metrics . setAvailableResourcesToUser ( user ,    headroom )  ;", "return   userLimit ;", "}", "METHOD_END"], "methodName": ["computeUserLimitAndSetHeadroom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId    =    node . getRMNode (  )  . getNodeID (  )  ;", "ContainerId   containerId    =    BuilderUtils . newContainerId ( application . getApplicationAttemptId (  )  ,    application . getNewContainerId (  )  )  ;", "Container   container    =    BuilderUtils . newContainer ( containerId ,    nodeId ,    node . getRMNode (  )  . getHttpAddress (  )  ,    bility ,    priority ,    null )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   applicationAttemptMap . get ( applicationAttemptId )  ;", "}", "METHOD_END"], "methodName": ["getApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   activeApplications ;", "}", "METHOD_END"], "methodName": ["getApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   rmContainer    !  =    null    ?    rmContainer . getContainer (  )     :    createContainer ( application ,    node ,    capability ,    priority )  ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   maxAMResourcePerQueuePercent ;", "}", "METHOD_END"], "methodName": ["getMaxAMResourcePerQueuePercent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   maxApplications ;", "}", "METHOD_END"], "methodName": ["getMaxApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   maxApplicationsPerUser ;", "}", "METHOD_END"], "methodName": ["getMaxApplicationsPerUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   maxActiveApplications ;", "}", "METHOD_END"], "methodName": ["getMaximumActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   maxActiveApplicationsPerUser ;", "}", "METHOD_END"], "methodName": ["getMaximumActiveApplicationsPerUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   maximumAllocation ;", "}", "METHOD_END"], "methodName": ["getMaximumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   minimumAllocation ;", "}", "METHOD_END"], "methodName": ["getMinimumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   minimumAllocationFactor ;", "}", "METHOD_END"], "methodName": ["getMinimumAllocationFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   nodeLocalityDelay ;", "}", "METHOD_END"], "methodName": ["getNodeLocalityDelay"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   activeApplications . size (  )  ;", "}", "METHOD_END"], "methodName": ["getNumActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   getUser ( user )  . getActiveApplications (  )  ;", "}", "METHOD_END"], "methodName": ["getNumActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   getUser ( user )  . getTotalApplications (  )  ;", "}", "METHOD_END"], "methodName": ["getNumApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   pendingApplications . size (  )  ;", "}", "METHOD_END"], "methodName": ["getNumPendingApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   getUser ( user )  . getPendingApplications (  )  ;", "}", "METHOD_END"], "methodName": ["getNumPendingApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "Resource   ret    =    BuilderUtils . newResource (  0  ,     0  )  ;", "for    ( FiCaSApp   f    :    activeApplications )     {", "Resources . addTo ( ret ,    f . getTotalPendingRequests (  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getTotalResourcePending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue . User   user    =    users . get ( userName )  ;", "if    ( user    =  =    null )     {", "user    =    new   LeafQueue . User (  )  ;", "users . put ( userName ,    user )  ;", "}", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   userLimit ;", "}", "METHOD_END"], "methodName": ["getUserLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   userLimitFactor ;", "}", "METHOD_END"], "methodName": ["getUserLimitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "ArrayList < UserInfo >    usersToReturn    =    new   ArrayList < UserInfo >  (  )  ;", "for    ( Map . Entry < String ,     . User >    entry    :    users . entrySet (  )  )     {", "usersToReturn . add ( new   UserInfo ( entry . getKey (  )  ,    Resources . clone ( entry . getValue (  )  . consumed )  ,    entry . getValue (  )  . getActiveApplications (  )  ,    entry . getValue (  )  . getPendingApplications (  )  )  )  ;", "}", "return   usersToReturn ;", "}", "METHOD_END"], "methodName": ["getUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "int   requiredContainers    =    application . getTotalRequiredResources ( priority )  ;", "int   reservedContainers    =    application . getNumReservedContainers ( priority )  ;", "int   starvation    =     0  ;", "if    ( reservedContainers    >     0  )     {", "float   nodeFactor    =    Resources . ratio ( resourceCalculator ,    required ,    getMaximumAllocation (  )  )  ;", "starvation    =     (  ( int )     (  (  ( application . getReReservations ( priority )  )     /     (  ( float )     ( reservedContainers )  )  )     *     (  1  .  0 F    -     ( Math . min ( nodeFactor ,    getMinimumAllocationFactor (  )  )  )  )  )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  \" needsContainers :  \"     +     \"    app .  # re - reserve =  \"  )     +     ( application . getReReservations ( priority )  )  )     +     \"    reserved =  \"  )     +    reservedContainers )     +     \"    nodeFactor =  \"  )     +    nodeFactor )     +     \"    minAllocFactor =  \"  )     +     ( getMinimumAllocationFactor (  )  )  )     +     \"    starvation =  \"  )     +    starvation )  )  ;", "}", "}", "return    (  ( starvation    +    requiredContainers )     -    reservedContainers )     >     0  ;", "}", "METHOD_END"], "methodName": ["needContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "Resources . subtractFrom ( usedResources ,    resource )  ;", "CSQueueUtils . updateQueueStatistics ( resourceCalculator ,    this ,    getParent (  )  ,    clusterResource ,    minimumAllocation )  ;", "-  -  ( numContainers )  ;", "String   userName    =    application . getUser (  )  ;", ". User   user    =    getUser ( userName )  ;", "user . releaseContainer ( resource )  ;", "metrics . setAvailableResourcesToUser ( userName ,    application . getHeadroom (  )  )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  ( getQueueName (  )  )     +     \"    used =  \"  )     +     ( usedResources )  )     +     \"    numContainers =  \"  )     +     ( numContainers )  )     +     \"    user =  \"  )     +    userName )     +     \"    user - resources =  \"  )     +     ( user . getConsumedResources (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["releaseResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "boolean   wasActive    =    activeApplications . remove ( application )  ;", "if    (  ! wasActive )     {", "pendingApplications . remove ( application )  ;", "}", "applicationAttemptMap . remove ( application . getApplicationAttemptId (  )  )  ;", "user . finishApplication ( wasActive )  ;", "if    (  ( user . getTotalApplications (  )  )     =  =     0  )     {", "users . remove ( application . getUser (  )  )  ;", "}", "activateApplications (  )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" Application   removed    -  \"     +     \"    appId :     \"  )     +     ( application . getApplicationId (  )  )  )     +     \"    user :     \"  )     +     ( application . getUser (  )  )  )     +     \"    queue :     \"  )     +     ( getQueueName (  )  )  )     +     \"     # user - pending - applications :     \"  )     +     ( user . getPendingApplications (  )  )  )     +     \"     # user - active - applications :     \"  )     +     ( user . getActiveApplications (  )  )  )     +     \"     # queue - pending - applications :     \"  )     +     ( getNumPendingApplications (  )  )  )     +     \"     # queue - active - applications :     \"  )     +     ( getNumActiveApplications (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["removeApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( rmContainer    =  =    null )     {", "getMetrics (  )  . reserveR ( application . getUser (  )  ,    container . getR (  )  )  ;", "}", "rmContainer    =    application . reserve ( node ,    priority ,    rmContainer ,    container )  ;", "node . reserveR ( application ,    priority ,    rmContainer )  ;", "}", "METHOD_END"], "methodName": ["reserve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "CSQueueUtils . checkMaxCapacity ( getQueueName (  )  ,    capacity ,    maximumCapacity )  ;", "float   absMaxCapacity    =    CSQueueUtils . computeAbsoluteMaximumCapacity ( maximumCapacity ,    getParent (  )  )  ;", "CSQueueUtils . checkAbsoluteCapacities ( getQueueName (  )  ,    absoluteCapacity ,    absMaxCapacity )  ;", "this . maximumCapacity    =    maximumCapacity ;", "this . absoluteMaxCapacity    =    absMaxCapacity ;", "}", "METHOD_END"], "methodName": ["setMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "this . userLimit    =    userLimit ;", "}", "METHOD_END"], "methodName": ["setUserLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "this . userLimitFactor    =    userLimitFactor ;", "}", "METHOD_END"], "methodName": ["setUserLimitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "CSQueueUtils . checkMaxCapacity ( getQueueName (  )  ,  ,    maximumCapacity )  ;", "float   absCapacity    =     ( getParent (  )  . getAbsoluteCapacity (  )  )     *  ;", "CSQueueUtils . checkAbsoluteCapacities ( getQueueName (  )  ,    absCapacity ,    absoluteMaxCapacity )  ;", "this    =  ;", "this . absoluteCapacity    =    absCapacity ;", "this . maximumCapacity    =    maximumCapacity ;", "this . absoluteMaxCapacity    =    absoluteMaxCapacity ;", "this . userLimit    =    userLimit ;", "this . userLimitFactor    =    userLimitFactor ;", "this . maxApplications    =    maxApplications ;", "this . maxAMResourcePerQueuePercent    =    maxAMResourcePerQueuePercent ;", "this . maxApplicationsPerUser    =    maxApplicationsPerUser ;", "this . maxActiveApplications    =    maxActiveApplications ;", "this . maxActiveApplicationsPerUser    =    maxActiveApplicationsPerUser ;", "this . state    =    state ;", "this . acls    =    acls ;", "this . queueInfo . setCapacity ( this )  ;", "this . queueInfo . setMaximumCapacity ( this . maximumCapacity )  ;", "this . queueInfo . setQueueState ( this . state )  ;", "this . nodeLocalityDelay    =    nodeLocalityDelay ;", "StringBuilder   aclsString    =    new   StringBuilder (  )  ;", "for    ( Map . Entry < QueueACL ,    AccessControlList >    e    :    acls . entrySet (  )  )     {", "aclsString . append (  (  (  ( e . getKey (  )  )     +     \"  :  \"  )     +     ( e . getValue (  )  . getAclString (  )  )  )  )  ;", "}", "CSQueueUtils . updateQueueStatistics ( resourceCalculator ,    this ,    getParent (  )  ,    clusterResource ,    minimumAllocation )  ;", "LeafQueue . LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \" Initializing    \"     +     ( queueName )  )     +     \"  \\ n \"  )     +        =     \"  )     +  )     +     \"     [  =     ( float )    configuredCapacity    /     1  0  0     ]  \"  )     +     \"  \\ n \"  )     +     \" asboluteCapacity    =     \"  )     +    absoluteCapacity )     +     \"     [  =    parentAbsoluteCapacity    *     ]  \"  )     +     \"  \\ n \"  )     +     \" maxCapacity    =     \"  )     +    maximumCapacity )     +     \"     [  =    configuredMaxCapacity    ]  \"  )     +     \"  \\ n \"  )     +     \" absoluteMaxCapacity    =     \"  )     +    absoluteMaxCapacity )     +     \"     [  =     1  .  0    maximumCapacity   undefined ,     \"  )     +     \"  ( parentAbsoluteMaxCapacity    *    maximumCapacity )     /     1  0  0    otherwise    ]  \"  )     +     \"  \\ n \"  )     +     \" userLimit    =     \"  )     +    userLimit )     +     \"     [  =    configuredUserLimit    ]  \"  )     +     \"  \\ n \"  )     +     \" userLimitFactor    =     \"  )     +    userLimitFactor )     +     \"     [  =    configuredUserLimitFactor    ]  \"  )     +     \"  \\ n \"  )     +     \" maxApplications    =     \"  )     +    maxApplications )     +     \"     [  =    configuredMaximumSystemApplicationsPerQueue   or \"  )     +     \"     ( int )  ( configuredMaximumSystemApplications    *    absoluteCapacity )  ]  \"  )     +     \"  \\ n \"  )     +     \" maxApplicationsPerUser    =     \"  )     +    maxApplicationsPerUser )     +     \"     [  =     ( int )  ( maxApplications    *     ( userLimit    /     1  0  0  .  0 f )     *     \"  )     +     \" userLimitFactor )     ]  \"  )     +     \"  \\ n \"  )     +     \" maxActiveApplications    =     \"  )     +    maxActiveApplications )     +     \"     [  =    max (  \"  )     +     \"  ( int ) ceil (  ( clusterResourceMemory    /    minimumAllocation )     *     \"  )     +     \" maxAMResourcePerQueuePercent    *    absoluteMaxCapacity )  ,  \"  )     +     \"  1  )     ]  \"  )     +     \"  \\ n \"  )     +     \" maxActiveAppsUsingAbsCap    =     \"  )     +     ( maxActiveAppsUsingAbsCap )  )     +     \"     [  =    max (  \"  )     +     \"  ( int ) ceil (  ( clusterResourceMemory    /    minimumAllocation )     *  \"  )     +     \" maxAMResourcePercent    *    absoluteCapacity )  ,  \"  )     +     \"  1  )     ]  \"  )     +     \"  \\ n \"  )     +     \" maxActiveApplicationsPerUser    =     \"  )     +    maxActiveApplicationsPerUser )     +     \"     [  =    max (  \"  )     +     \"  ( int )  ( maxActiveApplications    *     ( userLimit    /     1  0  0  .  0 f )     *     \"  )     +     \" userLimitFactor )  ,  \"  )     +     \"  1  )     ]  \"  )     +     \"  \\ n \"  )     +     \" usedCapacity    =     \"  )     +     ( usedCapacity )  )     +     \"     [  =    usedResourcesMemory    /     \"  )     +     \"  ( clusterResourceMemory    *    absoluteCapacity )  ]  \"  )     +     \"  \\ n \"  )     +     \" absoluteUsedCapacity    =     \"  )     +     ( absoluteUsedCapacity )  )     +     \"     [  =    usedResourcesMemory    /    clusterResourceMemory ]  \"  )     +     \"  \\ n \"  )     +     \" maxAMResourcePerQueuePercent    =     \"  )     +    maxAMResourcePerQueuePercent )     +     \"     [  =    configuredMaximumAMResourcePercent    ]  \"  )     +     \"  \\ n \"  )     +     \" minimumAllocationFactor    =     \"  )     +     ( minimumAllocationFactor )  )     +     \"     [  =     ( float )  ( maximumAllocationMemory    -    minimumAllocationMemory )     /     \"  )     +     \" maximumAllocationMemory    ]  \"  )     +     \"  \\ n \"  )     +     \" numContainers    =     \"  )     +     ( numContainers )  )     +     \"     [  =    currentNumContainers    ]  \"  )     +     \"  \\ n \"  )     +     \" state    =     \"  )     +    state )     +     \"     [  =    configuredState    ]  \"  )     +     \"  \\ n \"  )     +     \" acls    =     \"  )     +    aclsString )     +     \"     [  =    configuredAcls    ]  \"  )     +     \"  \\ n \"  )     +     \" nodeLocalityDelay    =     \"  )     +    nodeLocalityDelay )     +     \"  \\ n \"  )  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfigs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "return    (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  ( queueName )     +     \"  :     \"  )     +     \" capacity =  \"  )     +     ( capacity )  )     +     \"  ,     \"  )     +     \" absoluteCapacity =  \"  )     +     ( absoluteCapacity )  )     +     \"  ,     \"  )     +     \" usedResources =  \"  )     +     ( usedResources )  )     +     \"  ,     \"  )     +     \" usedCapacity =  \"  )     +     ( getUsedCapacity (  )  )  )     +     \"  ,     \"  )     +     \" absoluteUsedCapacity =  \"  )     +     ( getAbsoluteUsedCapacity (  )  )  )     +     \"  ,     \"  )     +     \" numApps =  \"  )     +     ( getNumApplications (  )  )  )     +     \"  ,     \"  )     +     \" numContainers =  \"  )     +     ( getNumContainers (  )  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( application . unreserve ( node ,    priority )  )     {", "node . unreserveR ( application )  ;", "getMetrics (  )  . unreserveR ( application . getUser (  )  ,    rmContainer . getContainer (  )  . getR (  )  )  ;", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["unreserve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"}, {"methodBody": ["METHOD_START", "{", "+  +  ( numApplications )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  \" Application   added    -  \"     +     \"    appId :     \"  )     +    applicationId )     +     \"    user :     \"  )     +    user )     +     \"    leaf - queue   of   parent :     \"  )     +     ( getQueueName (  )  )  )     +     \"     # applications :     \"  )     +     ( getNumApplications (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["addApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "Resources . addTo ( usedResources ,    resource )  ;", "CSQueueUtils . updateQueueStatistics ( resourceCalculator ,    this ,    parent ,    clusterResource ,    minimumAllocation )  ;", "+  +  ( numContainers )  ;", "}", "METHOD_END"], "methodName": ["allocateResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "CSAssignment   assignment    =    new   CSAssignment ( Resources . createResource (  0  ,     0  )  ,    NodeType . NODE _ LOCAL )  ;", "printChildQueues (  )  ;", "for    ( Iterator < CSQueue >    iter    =    childQueues . iterator (  )  ;    iter . hasNext (  )  ;  )     {", "CSQueue   childQueue    =    iter . next (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Trying   to   assign   to   queue :     \"     +     ( childQueue . getQueuePath (  )  )  )     +     \"    stats :     \"  )     +    childQueue )  )  ;", "}", "assignment    =    childQueue . assignContainers ( cluster ,    node )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  \" Assigned   to   queue :     \"     +     ( childQueue . getQueuePath (  )  )  )     +     \"    stats :     \"  )     +    childQueue )     +     \"     -  -  >     \"  )     +     ( assignment . getResource (  )  )  )     +     \"  ,     \"  )     +     ( assignment . getType (  )  )  )  )  ;", "}", "if    ( Resources . greaterThan ( resourceCalculator ,    cluster ,    assignment . getResource (  )  ,    Resources . none (  )  )  )     {", "iter . remove (  )  ;", ". LOG . info (  (  (  (  \" Re - sorting   assigned   queue :     \"     +     ( childQueue . getQueuePath (  )  )  )     +     \"    stats :     \"  )     +    childQueue )  )  ;", "childQueues . add ( childQueue )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", "printChildQueues (  )  ;", "}", "break ;", "}", "}", "return   assignment ;", "}", "METHOD_END"], "methodName": ["assignContainersToChildQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "float   currentCapacity    =    Resources . divide ( resourceCalculator ,    clusterResource ,    usedResources ,    clusterResource )  ;", "if    ( currentCapacity    >  =     ( absoluteMaxCapacity )  )     {", ". LOG . info (  (  (  (  (  (  (  (  (  ( getQueueName (  )  )     +     \"    used =  \"  )     +     ( usedResources )  )     +     \"    current - capacity    (  \"  )     +    currentCapacity )     +     \"  )     \"  )     +     \"     >  =    max - capacity    (  \"  )     +     ( absoluteMaxCapacity )  )     +     \"  )  \"  )  )  ;", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["assignToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "return    (  ( node . getReservedContainer (  )  )     =  =    null )     &  &     ( Resources . greaterThanOrEqual ( resourceCalculator ,    clusterResource ,    node . getAvailableResource (  )  ,    minimumAllocation )  )  ;", "}", "METHOD_END"], "methodName": ["canAssign"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    ( CSQueue   q    :    childQueues )     {", "sb . append (  (  (  (  ( q . getQueuePath (  )  )     +     \"  (  \"  )     +     ( q . getUsedC (  )  )  )     +     \"  )  ,     \"  )  )  ;", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getChildQueuesToPrint"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "return   numApplications ;", "}", "METHOD_END"], "methodName": ["getNumApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "return   numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    CSQueue >    queuesMap    =    new   HashMap < String ,    CSQueue >  (  )  ;", "for    ( CSQueue   queue    :    queues )     {", "queuesMap . put ( queue . geName (  )  ,    queue )  ;", "}", "return   queuesMap ;", "}", "METHOD_END"], "methodName": ["getQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "QueueUserACLInfo   userAclInfo    =    recordFactory . newRecordInstance ( QueueUserACLInfo . class )  ;", "List < QueueACL >    operations    =    new   ArrayList < QueueACL >  (  )  ;", "for    ( QueueACL   operation    :    QueueACL . values (  )  )     {", "if    ( hasAccess ( operation ,    user )  )     {", "operations . add ( operation )  ;", "}", "}", "userAclInfo . seName ( geName (  )  )  ;", "userAclInfo . setUserAcls ( operations )  ;", "return   userAclInfo ;", "}", "METHOD_END"], "methodName": ["getUserAclInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( ParentQueue . LOG . isDebugEnabled (  )  )     {", "ParentQueue . LOG . debug (  (  (  (  \" printChildQueues    -    queue :     \"     +     ( getQueuePath (  )  )  )     +     \"    child - queues :     \"  )     +     ( getChildQueuesToPrint (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["printChildQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "Resources . subtractFrom ( usedResources ,    resource )  ;", "CSQueueUtils . updateQueueStatistics ( resourceCalculator ,    this ,    parent ,    clusterResource ,    minimumAllocation )  ;", "-  -  ( numContainers )  ;", "}", "METHOD_END"], "methodName": ["releaseResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "-  -  ( numApplications )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  \" Application   removed    -  \"     +     \"    appId :     \"  )     +    applicationId )     +     \"    user :     \"  )     +    user )     +     \"    leaf - queue   of   parent :     \"  )     +     ( getQueueName (  )  )  )     +     \"     # applications :     \"  )     +     ( getNumApplications (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["removeApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "float   childCapacities    =     0  ;", "for    ( CSQueue   queue    :    childQueues )     {", "childCapacities    +  =    queue . getCapacity (  )  ;", "}", "float   delta    =    Math . abs (  (  1  .  0 F    -    childCapacities )  )  ;", "if    (  (  (  ( capacity )     >     0  )     &  &     ( delta    >     (  . PRECISION )  )  )     |  |     (  (  ( capacity )     =  =     0  )     &  &     ( childCapacities    >     0  )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Illegal \"     +     \"    capacity   of    \"  )     +    childCapacities )     +     \"    for   children   of   queue    \"  )     +     ( queueName )  )  )  ;", "}", "this . childQueues . clear (  )  ;", "this . childQueues . addAll ( childQueues )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" setChildQueues :     \"     +     ( getChildQueuesToPrint (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setChildQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "CSQueueUtils . checkMaxCapacity ( getQueueName (  )  ,    capacity ,    maximumCapacity )  ;", "float   absMaxCapacity    =    CSQueueUtils . computeAbsoluteMaximumCapacity ( maximumCapacity ,    parent )  ;", "CSQueueUtils . checkAbsoluteCapacities ( getQueueName (  )  ,    absoluteCapacity ,    absMaxCapacity )  ;", "this . maximumCapacity    =    maximumCapacity ;", "this . absoluteMaxCapacity    =    absMaxCapacity ;", "}", "METHOD_END"], "methodName": ["setMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "CSQueueUtils . checkMaxCapacity ( getQueueName (  )  ,    capacity ,    maximumCapacity )  ;", "CSQueueUtils . checkAbsoluteCapacities ( getQueueName (  )  ,    absoluteCapacity ,    absoluteMaxCapacity )  ;", "this . capacity    =    capacity ;", "this . absoluteCapacity    =    absoluteCapacity ;", "this . maximumCapacity    =    maximumCapacity ;", "this . absoluteMaxCapacity    =    absoluteMaxCapacity ;", "this . state    =    state ;", "this . acls    =    acls ;", "this . queueInfo . setCapacity ( this . capacity )  ;", "this . queueInfo . setMaximumCapacity ( this . maximumCapacity )  ;", "this . queueInfo . setQueueState ( this . state )  ;", "StringBuilder   aclsString    =    new   StringBuilder (  )  ;", "for    ( Map . Entry < QueueACL ,    AccessControlList >    e    :    acls . entrySet (  )  )     {", "aclsString . append (  (  (  ( e . getKey (  )  )     +     \"  :  \"  )     +     ( e . getValue (  )  . getAclString (  )  )  )  )  ;", "}", "CSQueueUtils . updateQueueStatistics ( resourceCalculator ,    this ,    parent ,    clusterResource ,    minimumAllocation )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  (  (  ( queueName )     +     \"  ,    capacity =  \"  )     +    capacity )     +     \"  ,    asboluteCapacity =  \"  )     +    absoluteCapacity )     +     \"  ,    maxCapacity =  \"  )     +    maximumCapacity )     +     \"  ,    asboluteMaxCapacity =  \"  )     +    absoluteMaxCapacity )     +     \"  ,    state =  \"  )     +    state )     +     \"  ,    acls =  \"  )     +    aclsString )  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfigs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "return    (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  ( queueName )     +     \"  :     \"  )     +     \" numChildQueue =     \"  )     +     ( childQueues . size (  )  )  )     +     \"  ,     \"  )     +     \" capacity =  \"  )     +     ( capacity )  )     +     \"  ,     \"  )     +     \" absoluteCapacity =  \"  )     +     ( absoluteCapacity )  )     +     \"  ,     \"  )     +     \" usedResources =  \"  )     +     ( usedResources )  )     +     \" usedCapacity =  \"  )     +     ( getUsedCapacity (  )  )  )     +     \"  ,     \"  )     +     \" numApps =  \"  )     +     ( getNumApplications (  )  )  )     +     \"  ,     \"  )     +     \" numContainers =  \"  )     +     ( getNumContainers (  )  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerApp   application    =    mock ( FiCaSchedulerApp . class )  ;", "ApplicationAttemptId   applicationAttemptId    =    TestUtils . getMockApplicationAttemptId ( appId ,     0  )  ;", "doReturn ( applicationAttemptId . geId (  )  )  . when ( application )  . geId (  )  ;", "doReturn ( applicationAttemptId )  . when ( application )  . geAttemptId (  )  ;", "doReturn ( user )  . when ( application )  . getUser (  )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["getMockApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  ,     3  2  )  )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( Resources . createResource (  (  (  1  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0     *     3  2  )  )  )  ;", "when ( csContext . getApplicationComparator (  )  )  . thenReturn ( CapacityScheduler . applicationComparator )  ;", "when ( csContext . getQueueComparator (  )  )  . thenReturn ( CapacityScheduler . queueComparator )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "containerTokenSecretManager . rollMasterKey (  )  ;", "when ( csContext . getContainerTokenSecretManager (  )  )  . thenReturn ( containerTokenSecretManager )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,     \" root \"  ,    queues ,    queues ,    TestUtils . spyHook )  ;", "queue    =    spy ( new   LeafQueue ( csContext ,     . A ,    root ,    null )  )  ;", "doReturn ( true )  . when ( queue )  . hasAccess ( any ( QueueACL . class )  ,    any ( UserGroupInformation . class )  )  ;", "doReturn (  1  0  0  )  . when ( queue )  . getMaxApplications (  )  ;", "doReturn (  2  5  )  . when ( queue )  . getMaxApplicationsPerUser (  )  ;", "doReturn (  1  0  )  . when ( queue )  . getMaximumActiveApplications (  )  ;", "doReturn (  2  )  . when ( queue )  . getMaximumActiveApplicationsPerUser (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    TestApplicationLimits . A ,    TestApplicationLimits . B    }  )  ;", "final   String   Q _ A    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestApplicationLimits . A )  ;", "conf . setCapacity ( Q _ A ,     1  0  )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestApplicationLimits . B )  ;", "conf . setCapacity ( Q _ B ,     9  0  )  ;", "TestApplicationLimits . LOG . info (  \" Setup   top - level   queues   a   and   b \"  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "int   APPLICATION _ ID    =     0  ;", "FiCaSchedulerApp   app _  0     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "FiCaSchedulerApp   app _  1     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "FiCaSchedulerApp   app _  2     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  2  ,    user _  0  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "queue . finishApplicationAttempt ( app _  0  ,     . A )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "FiCaSchedulerApp   app _  3     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  3  ,    user _  0  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "doReturn (  3  )  . when ( queue )  . getMaximumActiveApplications (  )  ;", "FiCaSchedulerApp   app _  4     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  1  )  ;", "queue . submitApplicationAttempt ( app _  4  ,    user _  1  )  ;", "assertEquals (  3  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications ( user _  1  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  1  )  )  ;", "FiCaSchedulerApp   app _  5     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  1  )  ;", "queue . submitApplicationAttempt ( app _  5  ,    user _  1  )  ;", "assertEquals (  3  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications ( user _  1  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  1  )  )  ;", "queue . finishApplicationAttempt ( app _  4  ,     . A )  ;", "assertEquals (  3  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications ( user _  1  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testActiveApplicationLimits"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "final   String   user _  0     =     \" user _  0  \"  ;", "int   APPLICATION _ ID    =     0  ;", "doReturn (  2  )  . when ( queue )  . getMaximumActiveApplications (  )  ;", "FiCaSchedulerApp   app _  0     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertTrue ( queue . activeApplications . contains ( app _  0  )  )  ;", "FiCaSchedulerApp   app _  1     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertTrue ( queue . activeApplications . contains ( app _  1  )  )  ;", "FiCaSchedulerApp   app _  2     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  2  ,    user _  0  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertTrue ( queue . pendingApplications . contains ( app _  2  )  )  ;", "FiCaSchedulerApp   app _  3     =    getMockApplication (  ( APPLICATION _ ID +  +  )  ,    user _  0  )  ;", "queue . submitApplicationAttempt ( app _  3  ,    user _  0  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  2  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertTrue ( queue . pendingApplications . contains ( app _  3  )  )  ;", "queue . finishApplicationAttempt ( app _  2  ,     . A )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  1  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertFalse ( queue . pendingApplications . contains ( app _  2  )  )  ;", "assertFalse ( queue . activeApplications . contains ( app _  2  )  )  ;", "queue . finishApplicationAttempt ( app _  0  ,     . A )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  2  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertTrue ( queue . activeApplications . contains ( app _  3  )  )  ;", "assertFalse ( queue . pendingApplications . contains ( app _  3  )  )  ;", "assertFalse ( queue . activeApplications . contains ( app _  0  )  )  ;", "queue . finishApplicationAttempt ( app _  1  ,     . A )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  1  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertFalse ( queue . activeApplications . contains ( app _  1  )  )  ;", "queue . finishApplicationAttempt ( app _  3  ,     . A )  ;", "assertEquals (  0  ,    queue . getNumActiveApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications (  )  )  ;", "assertEquals (  0  ,    queue . getNumActiveApplications ( user _  0  )  )  ;", "assertEquals (  0  ,    queue . getNumPendingApplications ( user _  0  )  )  ;", "assertFalse ( queue . activeApplications . contains ( app _  3  )  )  ;", "}", "METHOD_END"], "methodName": ["testActiveLimitsWithKilledApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csConf . setUserLimit (  (  (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . A )  )  ,     2  5  )  ;", "setupQueueConfiguration ( csConf )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  )  )  ;", "when ( csContext . getApplicationComparator (  )  )  . thenReturn ( CapacityScheduler . applicationComparator )  ;", "when ( csContext . getQueueComparator (  )  )  . thenReturn ( CapacityScheduler . queueComparator )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "Resource   clusterResource    =    Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( clusterResource )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,     \" root \"  ,    queues ,    queues ,    TestUtils . spyHook )  ;", "LeafQueue   queue    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get (  . A )  )  )  )  ;", "String   host _  0     =     \" host _  0  \"  ;", "String   rack _  0     =     \" rack _  0  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    rack _  0  ,     0  ,     (  1  6     *     (  . GB )  )  )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "RecordFactory   recordFactory    =    RecordFactoryProvider . getRecordFactory ( null )  ;", "RMContext   rmContext    =    TestUtils . getMockRMContext (  )  ;", "Priority   priority _  1     =    TestUtils . createMockPriority (  1  )  ;", "final   ApplicationAttemptId   appAttemptId _  0  _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0  _  0     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  0  _  0  ,    user _  0  ,    queue ,    queue . getActiveUsersManager (  )  ,    rmContext )  )  ;", "queue . submitApplicationAttempt ( app _  0  _  0  ,    user _  0  )  ;", "List < ResourceRequest >    app _  0  _  0  _ requests    =    new   ArrayList < ResourceRequest >  (  )  ;", "app _  0  _  0  _ requests . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     (  . GB )  )  ,     2  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  0  _  0  . updateResourceRequests ( app _  0  _  0  _ requests )  ;", "queue . assignContainers ( clusterResource ,    node _  0  )  ;", "Resource   expectedHeadroom    =    Resources . createResource (  (  (  1  0     *     1  6  )     *     (  . GB )  )  ,     1  )  ;", "verify ( app _  0  _  0  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "final   ApplicationAttemptId   appAttemptId _  0  _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  0  _  1     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  0  _  1  ,    user _  0  ,    queue ,    queue . getActiveUsersManager (  )  ,    rmContext )  )  ;", "queue . submitApplicationAttempt ( app _  0  _  1  ,    user _  0  )  ;", "List < ResourceRequest >    app _  0  _  1  _ requests    =    new   ArrayList < ResourceRequest >  (  )  ;", "app _  0  _  1  _ requests . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     (  . GB )  )  ,     2  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  0  _  1  . updateResourceRequests ( app _  0  _  1  _ requests )  ;", "queue . assignContainers ( clusterResource ,    node _  0  )  ;", "verify ( app _  0  _  0  ,    times (  2  )  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "verify ( app _  0  _  1  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "final   ApplicationAttemptId   appAttemptId _  1  _  0     =    TestUtils . getMockApplicationAttemptId (  2  ,     0  )  ;", "FiCaSchedulerApp   app _  1  _  0     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  1  _  0  ,    user _  1  ,    queue ,    queue . getActiveUsersManager (  )  ,    rmContext )  )  ;", "queue . submitApplicationAttempt ( app _  1  _  0  ,    user _  1  )  ;", "List < ResourceRequest >    app _  1  _  0  _ requests    =    new   ArrayList < ResourceRequest >  (  )  ;", "app _  1  _  0  _ requests . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     (  . GB )  )  ,     2  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  1  _  0  . updateResourceRequests ( app _  1  _  0  _ requests )  ;", "queue . assignContainers ( clusterResource ,    node _  0  )  ;", "expectedHeadroom    =    Resources . createResource (  (  (  (  1  0     *     1  6  )     *     (  . GB )  )     /     2  )  ,     1  )  ;", "verify ( app _  0  _  0  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "verify ( app _  0  _  1  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "verify ( app _  1  _  0  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "clusterResource    =    Resources . createResource (  (  (  9  0     *     1  6  )     *     (  . GB )  )  )  ;", "queue . assignContainers ( clusterResource ,    node _  0  )  ;", "expectedHeadroom    =    Resources . createResource (  (  (  (  9     *     1  6  )     *     (  . GB )  )     /     2  )  ,     1  )  ;", "verify ( app _  0  _  0  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "verify ( app _  0  _  1  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "verify ( app _  1  _  0  )  . setHeadroom ( eq ( expectedHeadroom )  )  ;", "}", "METHOD_END"], "methodName": ["testHeadroom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  ,     1  6  )  )  ;", "when ( csContext . getApplicationComparator (  )  )  . thenReturn ( CapacityScheduler . applicationComparator )  ;", "when ( csContext . getQueueComparator (  )  )  . thenReturn ( CapacityScheduler . queueComparator )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "Resource   clusterResource    =    Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0  0     *     1  6  )  )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( clusterResource )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,     \" root \"  ,    queues ,    queues ,    TestUtils . spyHook )  ;", "LeafQueue   queue    =     (  ( LeafQueue )     ( queues . get (  . A )  )  )  ;", ". LOG . info (  (  (  (  (  \" Queue    ' A '     -  \"     +     \"    maxActiveApplications =  \"  )     +     ( queue . getMaximumActiveApplications (  )  )  )     +     \"    maxActiveApplicationsPerUser =  \"  )     +     ( queue . getMaximumActiveApplicationsPerUser (  )  )  )  )  ;", "int   expectedMaxActiveApps    =    Math . max (  1  ,     (  ( int )     ( Math . ceil (  (  (  (  (  ( float )     ( clusterResource . getMemory (  )  )  )     /     (  1     *     (  . GB )  )  )     *     ( csConf . getMaximumApplicationMasterResourcePerQueuePercent ( queue . getQueuePath (  )  )  )  )     *     ( queue . getAbsoluteMaximumCapacity (  )  )  )  )  )  )  )  ;", "assertEquals ( expectedMaxActiveApps ,    queue . getMaximumActiveApplications (  )  )  ;", "int   expectedMaxActiveAppsUsingAbsCap    =    Math . max (  1  ,     (  ( int )     ( Math . ceil (  (  (  (  (  ( float )     ( clusterResource . getMemory (  )  )  )     /     (  1     *     (  . GB )  )  )     *     ( csConf . getMaximumApplicationMasterResourcePercent (  )  )  )     *     ( queue . getAbsoluteCapacity (  )  )  )  )  )  )  )  ;", "assertEquals (  (  ( int )     ( Math . ceil (  (  ( expectedMaxActiveAppsUsingAbsCap    *     (  ( queue . getUserLimit (  )  )     /     1  0  0  .  0 F )  )     *     ( queue . getUserLimitFactor (  )  )  )  )  )  )  ,    queue . getMaximumActiveApplicationsPerUser (  )  )  ;", "assertEquals (  (  ( int )     (  ( clusterResource . getMemory (  )  )     *     ( queue . getAbsoluteCapacity (  )  )  )  )  ,    queue . getMetrics (  )  . getAvailableMB (  )  )  ;", "clusterResource    =    Resources . createResource (  (  (  1  2  0     *     1  6  )     *     (  . GB )  )  )  ;", "root . updateClusterResource ( clusterResource )  ;", "expectedMaxActiveApps    =    Math . max (  1  ,     (  ( int )     ( Math . ceil (  (  (  (  (  ( float )     ( clusterResource . getMemory (  )  )  )     /     (  1     *     (  . GB )  )  )     *     ( csConf . getMaximumApplicationMasterResourcePerQueuePercent ( queue . getQueuePath (  )  )  )  )     *     ( queue . getAbsoluteMaximumCapacity (  )  )  )  )  )  )  )  ;", "assertEquals ( expectedMaxActiveApps ,    queue . getMaximumActiveApplications (  )  )  ;", "expectedMaxActiveAppsUsingAbsCap    =    Math . max (  1  ,     (  ( int )     ( Math . ceil (  (  (  (  (  ( float )     ( clusterResource . getMemory (  )  )  )     /     (  1     *     (  . GB )  )  )     *     ( csConf . getMaximumApplicationMasterResourcePercent (  )  )  )     *     ( queue . getAbsoluteCapacity (  )  )  )  )  )  )  )  ;", "assertEquals (  (  ( int )     ( Math . ceil (  (  ( expectedMaxActiveAppsUsingAbsCap    *     (  ( queue . getUserLimit (  )  )     /     1  0  0  .  0 F )  )     *     ( queue . getUserLimitFactor (  )  )  )  )  )  )  ,    queue . getMaximumActiveApplicationsPerUser (  )  )  ;", "assertEquals (  (  ( int )     (  ( clusterResource . getMemory (  )  )     *     ( queue . getAbsoluteCapacity (  )  )  )  )  ,    queue . getMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  (  ( int )     ( CapacitySchedulerConfiguration . UNDEFINED )  )  ,    csConf . getMaximumApplicationsPerQueue ( queue . getQueuePath (  )  )  )  ;", "int   expectedMaxApps    =     (  ( int )     (  ( CapacitySchedulerConfiguration . DEFAULT _ MAXIMUM _ SYSTEM _ APPLICATIIONS )     *     ( queue . getAbsoluteCapacity (  )  )  )  )  ;", "assertEquals ( expectedMaxApps ,    queue . getMaxApplications (  )  )  ;", "int   expectedMaxAppsPerUser    =     (  ( int )     (  ( expectedMaxApps    *     (  ( queue . getUserLimit (  )  )     /     1  0  0  .  0 F )  )     *     ( queue . getUserLimitFactor (  )  )  )  )  ;", "assertEquals ( expectedMaxAppsPerUser ,    queue . getMaxApplicationsPerUser (  )  )  ;", "assertEquals (  (  ( long )     ( CapacitySchedulerConfiguration . DEFAULT _ MAXIMUM _ APPLICATIONMASTERS _ RESOURCE _ PERCENT )  )  ,     (  ( long )     ( csConf . getMaximumApplicationMasterResourcePerQueuePercent ( queue . getQueuePath (  )  )  )  )  )  ;", "csConf . setFloat (  (  (  \" yarn . scheduler . capacity .  \"     +     ( queue . getQueuePath (  )  )  )     +     \"  . maximum - am - resource - percent \"  )  ,     0  .  5 F )  ;", "queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,     \" root \"  ,    queues ,    queues ,    TestUtils . spyHook )  ;", "clusterResource    =    Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  )  ;", "queue    =     (  ( LeafQueue )     ( queues . get (  . A )  )  )  ;", "expectedMaxActiveApps    =    Math . max (  1  ,     (  ( int )     ( Math . ceil (  (  (  (  (  ( float )     ( clusterResource . getMemory (  )  )  )     /     (  1     *     (  . GB )  )  )     *     ( csConf . getMaximumApplicationMasterResourcePerQueuePercent ( queue . getQueuePath (  )  )  )  )     *     ( queue . getAbsoluteMaximumCapacity (  )  )  )  )  )  )  )  ;", "assertEquals (  (  ( long )     (  0  .  5  )  )  ,     (  ( long )     ( csConf . getMaximumApplicationMasterResourcePerQueuePercent ( queue . getQueuePath (  )  )  )  )  )  ;", "assertEquals ( expectedMaxActiveApps ,    queue . getMaximumActiveApplications (  )  )  ;", "csConf . setInt (  (  (  \" yarn . scheduler . capacity .  \"     +     ( queue . getQueuePath (  )  )  )     +     \"  . maximum - applications \"  )  ,     9  9  9  9  )  ;", "queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,     \" root \"  ,    queues ,    queues ,    TestUtils . spyHook )  ;", "queue    =     (  ( LeafQueue )     ( queues . get (  . A )  )  )  ;", "assertEquals (  9  9  9  9  ,     (  ( int )     ( csConf . getMaximumApplicationsPerQueue ( queue . getQueuePath (  )  )  )  )  )  ;", "assertEquals (  9  9  9  9  ,    queue . getMaxApplications (  )  )  ;", "expectedMaxAppsPerUser    =     (  ( int )     (  (  9  9  9  9     *     (  ( queue . getUserLimit (  )  )     /     1  0  0  .  0 F )  )     *     ( queue . getUserLimitFactor (  )  )  )  )  ;", "assertEquals ( expectedMaxAppsPerUser ,    queue . getMaxApplicationsPerUser (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLimitsComputation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestApplicationLimits"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculator   resourceCalculator ;", "Resource   clusterResource ;", "if    ( useDominant )     {", "resourceCalculator    =    new   DominantResourceCalculator (  )  ;", "clusterResource    =    Resources . createResource (  1  0  ,     0  )  ;", "} else    {", "resourceCalculator    =    new   DefaultResourceCalculator (  )  ;", "clusterResource    =    Resources . createResource (  0  ,     9  9  )  ;", "}", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( clusterResource )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  0  ,     0  )  )  ;", "final   String   L 1 Q 1     =     \" L 1 Q 1  \"  ;", "csConf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    L 1 Q 1     }  )  ;", "final   String   L 1 Q 1 P    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +    L 1 Q 1  ;", "csConf . setCapacity ( L 1 Q 1 P ,     9  0  )  ;", "csConf . setMaximumCapacity ( L 1 Q 1 P ,     9  0  )  ;", "ParentQueue   root    =    new   ParentQueue ( csContext ,    CapacitySchedulerConfiguration . ROOT ,    null ,    null )  ;", "LeafQueue   l 1 q 1     =    new   LeafQueue ( csContext ,    L 1 Q 1  ,    root ,    null )  ;", ". LOG . info (  (  \" t 1    root    \"     +     ( CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    root )  )  )  )  ;", ". LOG . info (  (  \" t 1    l 1 q 1     \"     +     ( CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 1 q 1  )  )  )  )  ;", "assertEquals (  0  .  0 F ,    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 1 q 1  )  ,     1  .  0 E -  6 F )  ;", "}", "METHOD_END"], "methodName": ["runInvalidDivisorTest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "runInvalidDivisorTest ( false )  ;", "runInvalidDivisorTest ( true )  ;", "}", "METHOD_END"], "methodName": ["testAbsoluteMaxAvailCapacityInvalidDivisor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculator   resourceCalculator    =    new   DefaultResourceCalculator (  )  ;", "Resource   clusterResource    =    Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0  0     *     3  2  )  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( clusterResource )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  ,     3  2  )  )  ;", "final   String   L 1 Q 1     =     \" L 1 Q 1  \"  ;", "csConf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    L 1 Q 1     }  )  ;", "final   String   L 1 Q 1 P    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +    L 1 Q 1  ;", "csConf . setCapacity ( L 1 Q 1 P ,     9  0  )  ;", "csConf . setMaximumCapacity ( L 1 Q 1 P ,     9  0  )  ;", "ParentQueue   root    =    new   ParentQueue ( csContext ,    CapacitySchedulerConfiguration . ROOT ,    null ,    null )  ;", "LeafQueue   l 1 q 1     =    new   LeafQueue ( csContext ,    L 1 Q 1  ,    root ,    null )  ;", ". LOG . info (  (  \" t 1    root    \"     +     ( CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    root )  )  )  )  ;", ". LOG . info (  (  \" t 1    l 1 q 1     \"     +     ( CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 1 q 1  )  )  )  )  ;", "assertEquals (  1  .  0 F ,    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    root )  ,     1  .  0 E -  6 F )  ;", "assertEquals (  0  .  9 F ,    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 1 q 1  )  ,     1  .  0 E -  6 F )  ;", "}", "METHOD_END"], "methodName": ["testAbsoluteMaxAvailCapacityNoUse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculator   resourceCalculator    =    new   DefaultResourceCalculator (  )  ;", "Resource   clusterResource    =    Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0  0     *     3  2  )  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( clusterResource )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  ,     3  2  )  )  ;", "final   String   L 1 Q 1     =     \" L 1 Q 1  \"  ;", "final   String   L 1 Q 2     =     \" L 1 Q 2  \"  ;", "final   String   L 2 Q 1     =     \" L 2 Q 1  \"  ;", "final   String   L 2 Q 2     =     \" L 2 Q 2  \"  ;", "csConf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    L 1 Q 1  ,    L 1 Q 2  ,    L 2 Q 1  ,    L 2 Q 2     }  )  ;", "final   String   L 1 Q 1 P    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +    L 1 Q 1  ;", "csConf . setCapacity ( L 1 Q 1 P ,     8  0  )  ;", "csConf . setMaximumCapacity ( L 1 Q 1 P ,     8  0  )  ;", "final   String   L 1 Q 2 P    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +    L 1 Q 2  ;", "csConf . setCapacity ( L 1 Q 2 P ,     2  0  )  ;", "csConf . setMaximumCapacity ( L 1 Q 2 P ,     1  0  0  )  ;", "final   String   L 2 Q 1 P    =     ( L 1 Q 1 P    +     \"  .  \"  )     +    L 2 Q 1  ;", "csConf . setCapacity ( L 2 Q 1 P ,     5  0  )  ;", "csConf . setMaximumCapacity ( L 2 Q 1 P ,     5  0  )  ;", "final   String   L 2 Q 2 P    =     ( L 1 Q 1 P    +     \"  .  \"  )     +    L 2 Q 2  ;", "csConf . setCapacity ( L 2 Q 2 P ,     5  0  )  ;", "csConf . setMaximumCapacity ( L 2 Q 2 P ,     5  0  )  ;", "float   result ;", "ParentQueue   root    =    new   ParentQueue ( csContext ,    CapacitySchedulerConfiguration . ROOT ,    null ,    null )  ;", "LeafQueue   l 1 q 1     =    new   LeafQueue ( csContext ,    L 1 Q 1  ,    root ,    null )  ;", "LeafQueue   l 1 q 2     =    new   LeafQueue ( csContext ,    L 1 Q 2  ,    root ,    null )  ;", "LeafQueue   l 2 q 2     =    new   LeafQueue ( csContext ,    L 2 Q 2  ,    l 1 q 1  ,    null )  ;", "LeafQueue   l 2 q 1     =    new   LeafQueue ( csContext ,    L 2 Q 1  ,    l 1 q 1  ,    null )  ;", "result    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 2 q 2  )  ;", "assertEquals (  0  .  4 F ,    result ,     1  .  0 E -  6 F )  ;", ". LOG . info (  (  \" t 2    l 2 q 2     \"     +    result )  )  ;", "Resources . addTo ( root . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  1 F )  )  ;", "Resources . addTo ( l 1 q 2  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  1 F )  )  ;", "result    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 2 q 2  )  ;", "assertEquals (  0  .  4 F ,    result ,     1  .  0 E -  6 F )  ;", ". LOG . info (  (  \" t 2    l 2 q 2     \"     +    result )  )  ;", "Resources . addTo ( root . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  3 F )  )  ;", "Resources . addTo ( l 1 q 2  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  3 F )  )  ;", "result    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 2 q 2  )  ;", "assertEquals (  0  .  3 F ,    result ,     1  .  0 E -  6 F )  ;", ". LOG . info (  (  \" t 2    l 2 q 2     \"     +    result )  )  ;", "Resources . addTo ( root . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  1 F )  )  ;", "Resources . addTo ( l 1 q 1  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  1 F )  )  ;", "result    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 2 q 2  )  ;", "assertEquals (  0  .  3 F ,    result ,     1  .  0 E -  6 F )  ;", ". LOG . info (  (  \" t 2    l 2 q 2     \"     +    result )  )  ;", "Resources . addTo ( root . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  2 F )  )  ;", "Resources . addTo ( l 1 q 1  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  2 F )  )  ;", "Resources . addTo ( l 2 q 1  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  2 F )  )  ;", "result    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 2 q 2  )  ;", "assertEquals (  0  .  3 F ,    result ,     1  .  0 E -  6 F )  ;", ". LOG . info (  (  \" t 2    l 2 q 2     \"     +    result )  )  ;", "Resources . addTo ( root . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  2 F )  )  ;", "Resources . addTo ( l 1 q 1  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  2 F )  )  ;", "Resources . addTo ( l 2 q 1  . getUsedResources (  )  ,    Resources . multiply ( clusterResource ,     0  .  2 F )  )  ;", "result    =    CSQueueUtils . getAbsoluteMaxAvailCapacity ( resourceCalculator ,    clusterResource ,    l 2 q 2  )  ;", "assertEquals (  0  .  1 F ,    result ,     1  .  0 E -  6 F )  ;", ". LOG . info (  (  \" t 2    l 2 q 2     \"     +    result )  )  ;", "}", "METHOD_END"], "methodName": ["testAbsoluteMaxAvailCapacityWithUse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCSQueueUtils"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( expected ,    application . getUsedResources (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkApplicationResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( expected ,    node . getUsed (  )  . getMemory (  )  )  ;", "node . checkRUsage (  )  ;", "}", "METHOD_END"], "methodName": ["checkNodeResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CSQueue   rootQueue    =    cs . getRootQueue (  )  ;", "CSQueue   queueA    =    findQueue ( rootQueue ,     . A )  ;", "CSQueue   queueB    =    findQueue ( rootQueue ,     . B )  ;", "CSQueue   queueA 1     =    findQueue ( queueA ,     . A 1  )  ;", "CSQueue   queueA 2     =    findQueue ( queueA ,     . A 2  )  ;", "CSQueue   queueB 1     =    findQueue ( queueB ,     . B 1  )  ;", "CSQueue   queueB 2     =    findQueue ( queueB ,     . B 2  )  ;", "CSQueue   queueB 3     =    findQueue ( queueB ,     . B 3  )  ;", "float   capA    =    capacityA    /     1  0  0  .  0 F ;", "float   capB    =    capacityB    /     1  0  0  .  0 F ;", "checkQueueCapacity ( queueA ,    capA ,    capA ,     1  .  0 F ,     1  .  0 F )  ;", "checkQueueCapacity ( queueB ,    capB ,    capB ,     1  .  0 F ,     1  .  0 F )  ;", "checkQueueCapacity ( queueA 1  ,     (  (  . A 1  _ CAPACITY )     /     1  0  0  .  0 F )  ,     (  (  (  . A 1  _ CAPACITY )     /     1  0  0  .  0 F )     *    capA )  ,     1  .  0 F ,     1  .  0 F )  ;", "checkQueueCapacity ( queueA 2  ,     (  (  . A 2  _ CAPACITY )     /     1  0  0  .  0 F )  ,     (  (  (  . A 2  _ CAPACITY )     /     1  0  0  .  0 F )     *    capA )  ,     1  .  0 F ,     1  .  0 F )  ;", "checkQueueCapacity ( queueB 1  ,     (  (  . B 1  _ CAPACITY )     /     1  0  0  .  0 F )  ,     (  (  (  . B 1  _ CAPACITY )     /     1  0  0  .  0 F )     *    capB )  ,     1  .  0 F ,     1  .  0 F )  ;", "checkQueueCapacity ( queueB 2  ,     (  (  . B 2  _ CAPACITY )     /     1  0  0  .  0 F )  ,     (  (  (  . B 2  _ CAPACITY )     /     1  0  0  .  0 F )     *    capB )  ,     1  .  0 F ,     1  .  0 F )  ;", "checkQueueCapacity ( queueB 3  ,     (  (  . B 3  _ CAPACITY )     /     1  0  0  .  0 F )  ,     (  (  (  . B 3  _ CAPACITY )     /     1  0  0  .  0 F )     *    capB )  ,     1  .  0 F ,     1  .  0 F )  ;", "}", "METHOD_END"], "methodName": ["checkQueueCapacities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "final   float   epsilon    =     1  .  0 E -  5 F ;", "assertEquals (  \" capacity \"  ,    expectedCapacity ,    q . ge (  )  ,    epsilon )  ;", "assertEquals (  \" absolute   capacity \"  ,    expectedAbsCapacity ,    q . getAbsoluteCapacity (  )  ,    epsilon )  ;", "assertEquals (  \" maximum   capacity \"  ,    expectedMaxCapacity ,    q . getMaximumCapacity (  )  ,    epsilon )  ;", "assertEquals (  \" absolute   maximum   capacity \"  ,    expectedAbsMaxCapacity ,    q . getAbsoluteMaximumCapacity (  )  ,    epsilon )  ;", "}", "METHOD_END"], "methodName": ["checkQueueCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( root . getQueuePath (  )  . equals ( queuePath )  )     {", "return   root ;", "}", "List < CSQueue >    childQueues    =    root . getChildQueues (  )  ;", "if    ( childQueues    !  =    null )     {", "for    ( CSQueue   q    :    childQueues )     {", "if    ( queuePath . startsWith ( q . getQueuePath (  )  )  )     {", "CSQueue   ult    =    findQueue ( q ,    queuePath )  ;", "if    ( ult    !  =    null )     {", "return   ult ;", "}", "}", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["findQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "for    ( CSQueue   queue    :    queues )     {", "if    ( queue . getQueueName (  )  . equals ( name )  )     {", "return   queue . getNumApplications (  )  ;", "}", "}", "return    -  1  ;", "}", "METHOD_END"], "methodName": ["getNumAppsInQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "int   result    =     0  ;", "for    ( QueueUserACLInfo   queueUserACLInfo    :    queueInformation )     {", "if    ( queueName . equals ( queueUserACLInfo . getQueueName (  )  )  )     {", "result +  +  ;", "}", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["getQueueCount"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( info    !  =    null )     {", "for    ( QueueInfo   queueInfo    :    info . getQueueInfoList (  )  )     {", "if    ( queueInfo . getQueueName (  )  . equals ( name )  )     {", "return   queueInfo ;", "} else    {", "QueueInfo   result    =    getQueueInfo ( name ,    queueInfo . getQueues (  )  )  ;", "if    ( result    =  =    null )     {", "continue ;", "}", "return   result ;", "}", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getQueueInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "nm . nodeHeartbeat ( true )  ;", "MockAM   am    =    rm . sendAMLaun ( attempt . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . RUNNING )  ;", "return   am ;", "}", "METHOD_END"], "methodName": ["launchAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "RMNode   node    =    resourceManager . getRMContext (  )  . getRMNodes (  )  . get ( nm . getNodeId (  )  )  ;", "NodeUpdateEvent   nodeUpdate    =    new   NodeUpdateEvent ( node )  ;", "resourceManager . getResource (  )  . handle ( nodeUpdate )  ;", "}", "METHOD_END"], "methodName": ["nodeUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "NodeManager   nm    =    new   NodeManager ( hostName ,    containerManagerPort ,    httpPort ,    rackName ,    capability ,    resourceManager )  ;", "NodeAddedEvent   nodeAddEvent 1     =    new   NodeAddedEvent ( resourceManager . getRMContext (  )  . getRMNodes (  )  . get ( nm . getNodeId (  )  )  )  ;", "resourceManager . getResource (  )  . handle ( nodeAddEvent 1  )  ;", "return   nm ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "resourceManager    =    new   ResourceManager (  )  ;", "Configuration   csConf    =    new   Configuration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration ( csConf )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "resourceManager . init ( conf )  ;", "resourceManager . getRMContext (  )  . getContainerTokenSecretManager (  )  . rollMasterKey (  )  ;", "resourceManager . getRMContext (  )  . getNMTokenSecretManager (  )  . rollMasterKey (  )  ;", "(  ( AsyncDispatcher )     ( resourceManager . getRMContext (  )  . getDispatcher (  )  )  )  . start (  )  ;", "mockContext    =    mock ( RMContext . class )  ;", "when ( mockContext . getConfigurationProvider (  )  )  . thenReturn ( new   LocalConfigurationProvider (  )  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( conf )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "return   rm ;", "}", "METHOD_END"], "methodName": ["setUpMove"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {     \" a \"  ,     \" b \"     }  )  ;", "conf . setCapacity (  . A ,     . A _ CAPACITY )  ;", "conf . setCapacity (  . B ,     . B _ CAPACITY )  ;", "conf . setQueues (  . A ,    new   String [  ]  {     \" a 1  \"  ,     \" a 2  \"     }  )  ;", "conf . setCapacity (  . A 1  ,     . A 1  _ CAPACITY )  ;", "conf . setUserLimitFactor (  . A 1  ,     1  0  0  .  0 F )  ;", "conf . setCapacity (  . A 2  ,     . A 2  _ CAPACITY )  ;", "conf . setUserLimitFactor (  . A 2  ,     1  0  0  .  0 F )  ;", "conf . setQueues (  . B ,    new   String [  ]  {     \" b 1  \"  ,     \" b 2  \"  ,     \" b 3  \"     }  )  ;", "conf . setCapacity (  . B 1  ,     . B 1  _ CAPACITY )  ;", "conf . setUserLimitFactor (  . B 1  ,     1  0  0  .  0 F )  ;", "conf . setCapacity (  . B 2  ,     . B 2  _ CAPACITY )  ;", "conf . setUserLimitFactor (  . B 2  ,     1  0  0  .  0 F )  ;", "conf . setCapacity (  . B 3  ,     . B 3  _ CAPACITY )  ;", "conf . setUserLimitFactor (  . B 3  ,     1  0  0  .  0 F )  ;", ". LOG . info (  \" Setup   top - level   queues   a   and   b \"  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManager )     !  =    null )     {", "resourceManager . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( conf )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >    cs    =     (  ( AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >  )     ( rm . getResourceScheduler (  )  )  )  ;", "SchedulerApplication < SchedulerApplicationAttempt >    app    =    TestSchedulerUtils . verifyAppAddedAndRemovedFromScheduler ( cs . getSchedulerApplications (  )  ,    cs ,     \" a 1  \"  )  ;", "Assert . assertEquals (  \" a 1  \"  ,    app . getQueue (  )  . getQueueName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAddAndRemoveAppFromCapacityScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "final   YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "TestAMAuthorization . MyContainerManager   containerManager    =    new   TestAMAuthorization . MyContainerManager (  )  ;", "final   TestAMAuthorization . MockRMWithAMS   rm    =    new   TestAMAuthorization . MockRMWithAMS ( conf ,    containerManager )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "Map < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  2  )  ;", "acls . put ( VIEW _ APP ,     \"  *  \"  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  ,     \" appname \"  ,     \" appuser \"  ,    acls )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    attempt . getAppAttemptId (  )  ;", "int   msecToWait    =     1  0  0  0  0  ;", "int   msecToSleep    =     1  0  0  ;", "while    (  (  ( attempt . getAppAttemptState (  )  )     !  =     ( RMAppAttemptState . LAUNCHED )  )     &  &     ( msecToWait    >     0  )  )     {", ". LOG . info (  (  (  \" Waiting   for   AppAttempt   to   reach   LAUNCHED   state .     \"     +     \" Current   state   is    \"  )     +     ( attempt . getAppAttemptState (  )  )  )  )  ;", "Thread . sleep ( msecToSleep )  ;", "msecToWait    -  =    msecToSleep ;", "}", "Assert . assertEquals ( attempt . getAppAttemptState (  )  ,    RMAppAttemptState . LAUNCHED )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( applicationAttemptId . toString (  )  )  ;", "Credentials   credentials    =    containerManager . getContainerCredentials (  )  ;", "final   InetSocketAddress   rmBindAddress    =    rm . getApplicationMasterService (  )  . getBindAddress (  )  ;", "Token <  ?    extends   TokenIdentifier >    amRMToken    =    TestAMAuthorization . MockRMWithAMS . setupAndReturnAMRMToken ( rmBindAddress ,    credentials . getAllTokens (  )  )  ;", "currentUser . addToken ( amRMToken )  ;", "ApplicationMasterProtocol   client    =    currentUser . doAs ( new   PrivilegedAction < ApplicationMasterProtocol >  (  )     {", "@ Override", "public   ApplicationMasterProtocol   run (  )     {", "return    (  ( ApplicationMasterProtocol )     ( rpc . getProxy ( ApplicationMasterProtocol . class ,    rmBindAddress ,    conf )  )  )  ;", "}", "}  )  ;", "RegisterApplicationMasterRequest   request    =    RegisterApplicationMasterRequest . newInstance (  \" localhost \"  ,     1  2  3  4  5  ,     \"  \"  )  ;", "client . registerApplicationMaster ( request )  ;", "final   CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "final   CyclicBarrier   barrier    =    new   CyclicBarrier (  2  )  ;", "Thread   otherThread    =    new   Thread ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "synchronized ( cs )     {", "try    {", "barrier . await (  )  ;", "barrier . await (  )  ;", "}    catch    ( InterruptedException   e )     {", "e . printStackTrace (  )  ;", "}    catch    ( BrokenBarrierException   e )     {", "e . printStackTrace (  )  ;", "}", "}", "}", "}  )  ;", "otherThread . start (  )  ;", "barrier . await (  )  ;", "AllocateRequest   allocateRequest    =    AllocateRequest . newInstance (  0  ,     0  .  0 F ,    null ,    null ,    null )  ;", "client . allocate ( allocateRequest )  ;", "barrier . await (  )  ;", "otherThread . join (  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAllocateDoesNotBlockOnSchedulerLock"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "Comparator < FiCaSchedulerApp >    appComparator    =    cs . getApplicationComparator (  )  ;", "ApplicationId   id 1     =    ApplicationId . newInstance (  1  ,     1  )  ;", "ApplicationId   id 2     =    ApplicationId . newInstance (  1  ,     2  )  ;", "ApplicationId   id 3     =    ApplicationId . newInstance (  2  ,     1  )  ;", "FiCaSchedulerApp   app 1     =    Mockito . mock ( FiCaSchedulerApp . class )  ;", "when ( app 1  . getApplicationId (  )  )  . thenReturn ( id 1  )  ;", "FiCaSchedulerApp   app 2     =    Mockito . mock ( FiCaSchedulerApp . class )  ;", "when ( app 2  . getApplicationId (  )  )  . thenReturn ( id 2  )  ;", "FiCaSchedulerApp   app 3     =    Mockito . mock ( FiCaSchedulerApp . class )  ;", "when ( app 3  . getApplicationId (  )  )  . thenReturn ( id 3  )  ;", "assertTrue (  (  ( appComparator . compare ( app 1  ,    app 2  )  )     <     0  )  )  ;", "assertTrue (  (  ( appComparator . compare ( app 1  ,    app 3  )  )     <     0  )  )  ;", "assertTrue (  (  ( appComparator . compare ( app 2  ,    app 3  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationComparator"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "cs    =     (  (  )     ( rm . getResourceScheduler (  )  )  )  ;", "final   int   NODES    =     1  0  0  ;", "for    ( int   i    =     0  ;    i    <    NODES ;     +  + i )     {", "String   host    =     \"  1  9  2  .  1  6  8  .  1  .  \"     +    i ;", "RMNode   node    =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( node )  )  ;", "}", "for    ( int   i    =     0  ;    i    <    NODES ;     +  + i )     {", ". schedule ( cs )  ;", "}", "}", "METHOD_END"], "methodName": ["testAsyncScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "cs    =     (  (  )     ( rm . getResourceScheduler (  )  )  )  ;", "String   host    =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "RMNode   node    =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( node )  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  0  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "SchedulerEvent   addAppEvent    =    new   AppAddedSchedulerEvent ( appId ,     \" default \"  ,     \" user \"  )  ;", "cs . handle ( addAppEvent )  ;", "SchedulerEvent   addAttemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId ,    false )  ;", "cs . handle ( addAttemptEvent )  ;", "cs . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    Collections . singletonList ( host )  ,    null )  ;", "Assert . assertTrue ( cs . getApplicationAttempt ( appAttemptId )  . isBlacklisted ( host )  )  ;", "cs . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    null ,    Collections . singletonList ( host )  )  ;", "Assert . assertFalse ( cs . getApplicationAttempt ( appAttemptId )  . isBlacklisted ( host )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testBlackListNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "TestCapacityScheduler . LOG . info (  \"  -  -  -    START :    testCapacityScheduler    -  -  -  \"  )  ;", "String   host _  0     =     \" host _  0  \"  ;", "NodeManager   nm _  0     =    registerNode ( host _  0  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  4     *     ( GB )  )  ,     1  )  )  ;", "String   host _  1     =     \" host _  1  \"  ;", "NodeManager   nm _  1     =    registerNode ( host _  1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  )  ;", "Priority   priority _  0     =    resource . Priority . create (  0  )  ;", "Priority   priority _  1     =    resource . Priority . create (  1  )  ;", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,     \" a 1  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "application _  0  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  0  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  0  _  0     =    Resources . createResource (  (  1     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  1  ,    capability _  0  _  0  )  ;", "Resource   capability _  0  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  0  ,    capability _  0  _  1  )  ;", "Task   task _  0  _  0     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  0  )  ;", "Application   application _  1     =    new   Application (  \" user _  1  \"  ,     \" b 2  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "application _  1  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  1  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  1  _  0     =    Resources . createResource (  (  3     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  1  ,    capability _  1  _  0  )  ;", "Resource   capability _  1  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  0  ,    capability _  1  _  1  )  ;", "Task   task _  1  _  0     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  1  . addTask ( task _  1  _  0  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "TestCapacityScheduler . LOG . info (  \" Kick !  \"  )  ;", "nodeUpdate ( nm _  0  )  ;", "nodeUpdate ( nm _  1  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  1     *     ( GB )  )  ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *     ( GB )  )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  4     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  0     *     ( GB )  )  ,    nm _  1  )  ;", "TestCapacityScheduler . LOG . info (  \" Adding   new   tasks .  .  .  \"  )  ;", "Task   task _  1  _  1     =    new   Task ( application _  1  ,    priority _  0  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application _  1  . addTask ( task _  1  _  1  )  ;", "application _  1  . schedule (  )  ;", "Task   task _  0  _  1     =    new   Task ( application _  0  ,    priority _  0  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  1  )  ;", "application _  0  . schedule (  )  ;", "TestCapacityScheduler . LOG . info (  (  \" Sending   hb   from    \"     +     ( nm _  0  . getHostName (  )  )  )  )  ;", "nodeUpdate ( nm _  0  )  ;", "TestCapacityScheduler . LOG . info (  (  \" Sending   hb   from    \"     +     ( nm _  1  . getHostName (  )  )  )  )  ;", "nodeUpdate ( nm _  1  )  ;", "TestCapacityScheduler . LOG . info (  \" Trying   to   allocate .  .  .  \"  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  1     *     ( GB )  )  ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  5     *     ( GB )  )  ,    application _  1  )  ;", "nodeUpdate ( nm _  0  )  ;", "nodeUpdate ( nm _  1  )  ;", "checkNodeResourceUsage (  (  4     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  2     *     ( GB )  )  ,    nm _  1  )  ;", "TestCapacityScheduler . LOG . info (  \"  -  -  -    END :    testCapacityScheduler    -  -  -  \"  )  ;", "}", "METHOD_END"], "methodName": ["testCapacityScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "QueueInfo   queueInfo    =    resourceManager . getResourceScheduler (  )  . getQueueInfo (  \" a \"  ,    true ,    true )  ;", "Assert . assertEquals ( queueInfo . getQueueName (  )  ,     \" a \"  )  ;", "Assert . assertEquals ( queueInfo . getChildQueues (  )  . size (  )  ,     2  )  ;", "List < QueueUserACLInfo >    userACLInfo    =    resourceManager . getResourceScheduler (  )  . getQueueUserAclInfo (  )  ;", "Assert . assertNotNull ( userACLInfo )  ;", "for    ( QueueUserACLInfo   queueUserACLInfo    :    userACLInfo )     {", "Assert . assertEquals ( getQueueCount ( userACLInfo ,    queueUserACLInfo . getQueueName (  )  )  ,     1  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCapacitySchedulerInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   scheduler    =    new   CapacityScheduler (  )  ;", "scheduler . setRMContext ( resourceManager . getRMContext (  )  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     2  0  4  8  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,     1  0  2  4  )  ;", "try    {", "scheduler . reinitialize ( conf ,    mockContext )  ;", "fail (  (  \" Exception   is   expected   because   the   min   memory   allocation   is \"     +     \"    larger   than   the   max   memory   allocation .  \"  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertTrue (  \" The   thrown   exception   is   not   the   expected   one .  \"  ,    e . getMessage (  )  . startsWith (  \" Invalid   resource   scheduler   memory \"  )  )  ;", "}", "conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,     2  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,     1  )  ;", "try    {", "scheduler . reinitialize ( conf ,    mockContext )  ;", "fail (  (  \" Exception   is   expected   because   the   min   vcores   allocation   is \"     +     \"    larger   than   the   max   vcores   allocation .  \"  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertTrue (  \" The   thrown   exception   is   not   the   expected   one .  \"  ,    e . getMessage (  )  . startsWith (  \" Invalid   resource   scheduler   vcores \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testConfValidation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,     \" a 1  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "Application   application _  1     =    new   Application (  \" user _  0  \"  ,     \" a 2  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "Application   application _  2     =    new   Application (  \" user _  0  \"  ,     \" b 2  \"  ,    resourceManager )  ;", "application _  2  . submit (  )  ;", "ResourceScheduler    =    resourceManager . getResourceScheduler (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( application _  0  . getApplicationAttemptId (  )  )  )  ;", "assertTrue ( appsInA . contains ( application _  1  . getApplicationAttemptId (  )  )  )  ;", "assertEquals (  2  ,    appsInA . size (  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( application _  0  . getApplicationAttemptId (  )  )  )  ;", "assertTrue ( appsInRoot . contains ( application _  1  . getApplicationAttemptId (  )  )  )  ;", "assertTrue ( appsInRoot . contains ( application _  2  . getApplicationAttemptId (  )  )  )  ;", "assertEquals (  3  ,    appsInRoot . size (  )  )  ;", "Assert . assertNull ( getAppsInQueue (  \" nonexistentqueue \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetAppsInQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "String   queue    =    getApplicationAttempt ( appsInA 1  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" a 1  \"  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "killAllAppsInQueue (  \" a 1  \"  )  ;", "rm . waitForState ( app . getApplicationId (  )  ,    RMAppState . KILLED )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . isEmpty (  )  )  ;", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertTrue ( appsInA 1  . isEmpty (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . isEmpty (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testKillAllAppsInQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "try    {", "killAllAppsInQueue (  \" DOES _ NOT _ EXIST \"  )  ;", "Assert . fail (  )  ;", "}    catch    ( YarnException   e )     {", "}", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testKillAllAppsInvalidSource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "float   delta    =     1  .  0 E -  7 F ;", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "assertEquals ( CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY _ VALUE ,    conf . getMaximumCapacity (  . A )  ,    delta )  ;", "conf . setMaximumCapacity (  . A ,     5  0  .  0 F )  ;", "assertEquals (  5  0  .  0 F ,    conf . getMaximumCapacity (  . A )  ,    delta )  ;", "conf . setMaximumCapacity (  . A ,     (  -  1  )  )  ;", "assertEquals ( CapacitySchedulerConfiguration . MAXIMUM _ CAPACITY _ VALUE ,    conf . getMaximumCapacity (  . A )  ,    delta )  ;", "}", "METHOD_END"], "methodName": ["testMaximumCapacitySetup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "String   queue    =    getApplicationAttempt ( appsInA 1  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" a 1  \"  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "List < ApplicationAttemptId >    appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertTrue ( appsInB 1  . isEmpty (  )  )  ;", "List < ApplicationAttemptId >    appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . isEmpty (  )  )  ;", "moveAllApps (  \" a 1  \"  ,     \" b 1  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertEquals (  1  ,    appsInB 1  . size (  )  )  ;", "queue    =    getApplicationAttempt ( appsInB 1  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" b 1  \"  )  )  ;", "appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInB . size (  )  )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertTrue ( appsInA 1  . isEmpty (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . isEmpty (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAllApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "List < ApplicationAttemptId >    appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertTrue ( appsInB 1  . isEmpty (  )  )  ;", "List < ApplicationAttemptId >    appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . isEmpty (  )  )  ;", "try    {", "moveAllApps (  \" a 1  \"  ,     \" DOES _ NOT _ EXIST \"  )  ;", "Assert . fail (  )  ;", "}    catch    ( YarnException   e )     {", "}", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertTrue ( appsInB 1  . isEmpty (  )  )  ;", "appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . isEmpty (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAllAppsInvalidDestination"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "List < ApplicationAttemptId >    appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertTrue ( appsInB 1  . isEmpty (  )  )  ;", "List < ApplicationAttemptId >    appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . isEmpty (  )  )  ;", "try    {", "moveAllApps (  \" DOES _ NOT _ EXIST \"  ,     \" b 1  \"  )  ;", "Assert . fail (  )  ;", "}    catch    ( YarnException   e )     {", "}", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertTrue ( appsInB 1  . isEmpty (  )  )  ;", "appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . isEmpty (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAllAppsInvalidSource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "String   queue    =    getApplicationAttempt ( appsInA 1  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" a 1  \"  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "List < ApplicationAttemptId >    appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertTrue ( appsInB 1  . isEmpty (  )  )  ;", "List < ApplicationAttemptId >    appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . isEmpty (  )  )  ;", "moveApplication ( app . getApplicationId (  )  ,     \" b 1  \"  )  ;", "appsInB 1     =    getAppsInQueue (  \" b 1  \"  )  ;", "assertEquals (  1  ,    appsInB 1  . size (  )  )  ;", "queue    =    getApplicationAttempt ( appsInB 1  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" b 1  \"  )  )  ;", "appsInB    =    getAppsInQueue (  \" b \"  )  ;", "assertTrue ( appsInB . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInB . size (  )  )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertTrue ( appsInA 1  . isEmpty (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . isEmpty (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAppBasic"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   scheduler    =    resourceManager . getResourceScheduler (  )  ;", "String   host _  0     =     \" host _  0  \"  ;", "NodeManager   nm _  0     =    registerNode ( host _  0  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  4     *     ( GB )  )  ,     1  )  )  ;", "String   host _  1     =     \" host _  1  \"  ;", "NodeManager   nm _  1     =    registerNode ( host _  1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  )  ;", "Priority   priority _  0     =    resource . Priority . create (  0  )  ;", "Priority   priority _  1     =    resource . Priority . create (  1  )  ;", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,     \" a 1  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "application _  0  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  0  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  0  _  0     =    Resources . createResource (  (  1     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  1  ,    capability _  0  _  0  )  ;", "Resource   capability _  0  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  0  ,    capability _  0  _  1  )  ;", "Task   task _  0  _  0     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  0  )  ;", "Application   application _  1     =    new   Application (  \" user _  1  \"  ,     \" b 2  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "application _  1  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  1  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  1  _  0     =    Resources . createResource (  (  1     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  1  ,    capability _  1  _  0  )  ;", "Resource   capability _  1  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  0  ,    capability _  1  _  1  )  ;", "Task   task _  1  _  0     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  1  . addTask ( task _  1  _  0  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nodeUpdate ( nm _  0  )  ;", "nodeUpdate ( nm _  1  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  1     *     ( GB )  )  ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  1     *     ( GB )  )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  2     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  0     *     ( GB )  )  ,    nm _  1  )  ;", "scheduler . moveApplication ( application _  0  . getApplicationId (  )  ,     \" b 1  \"  )  ;", "Task   task _  1  _  1     =    new   Task ( application _  1  ,    priority _  0  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application _  1  . addTask ( task _  1  _  1  )  ;", "application _  1  . schedule (  )  ;", "Task   task _  0  _  1     =    new   Task ( application _  0  ,    priority _  0  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  1  )  ;", "application _  0  . schedule (  )  ;", "nodeUpdate ( nm _  0  )  ;", "nodeUpdate ( nm _  1  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *     ( GB )  )  ,    application _  1  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *     ( GB )  )  ,    application _  0  )  ;", "checkNodeResourceUsage (  (  4     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  2     *     ( GB )  )  ,    nm _  1  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAppForMoveToQueueWithFreeCap"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   scheduler    =    resourceManager . getResourceScheduler (  )  ;", "String   host _  0     =     \" host _  0  \"  ;", "NodeManager   nm _  0     =    registerNode ( host _  0  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  5     *     ( GB )  )  ,     1  )  )  ;", "String   host _  1     =     \" host _  1  \"  ;", "NodeManager   nm _  1     =    registerNode ( host _  1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  5     *     ( GB )  )  ,     1  )  )  ;", "Priority   priority _  0     =    resource . Priority . create (  0  )  ;", "Priority   priority _  1     =    resource . Priority . create (  1  )  ;", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,     \" a 1  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "application _  0  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  0  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  0  _  0     =    Resources . createResource (  (  3     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  1  ,    capability _  0  _  0  )  ;", "Resource   capability _  0  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  0  ,    capability _  0  _  1  )  ;", "Task   task _  0  _  0     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  0  )  ;", "Application   application _  1     =    new   Application (  \" user _  1  \"  ,     \" b 2  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "application _  1  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  1  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  1  _  0     =    Resources . createResource (  (  1     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  1  ,    capability _  1  _  0  )  ;", "Resource   capability _  1  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  0  ,    capability _  1  _  1  )  ;", "Task   task _  1  _  0     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  1  . addTask ( task _  1  _  0  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nodeUpdate ( nm _  0  )  ;", "nodeUpdate ( nm _  1  )  ;", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( resourceManager . getResourceScheduler (  )  )  )  ;", "CSQueue   origRootQ    =    cs . getRootQueue (  )  ;", "CapacitySchedulerInfo   oldInfo    =    new   CapacitySchedulerInfo ( origRootQ )  ;", "int   origNumAppsA    =    getNumAppsInQueue (  \" a \"  ,    origRootQ . getChildQueues (  )  )  ;", "int   origNumAppsRoot    =    origRootQ . getNumApplications (  )  ;", "scheduler . moveApplication ( application _  0  . getApplicationId (  )  ,     \" a 2  \"  )  ;", "CSQueue   newRootQ    =    cs . getRootQueue (  )  ;", "int   newNumAppsA    =    getNumAppsInQueue (  \" a \"  ,    newRootQ . getChildQueues (  )  )  ;", "int   newNumAppsRoot    =    newRootQ . getNumApplications (  )  ;", "CapacitySchedulerInfo   newInfo    =    new   CapacitySchedulerInfo ( newRootQ )  ;", "CapacitySchedulerLeafQueueInfo   origOldA 1     =     (  ( CapacitySchedulerLeafQueueInfo )     ( getQueueInfo (  \" a 1  \"  ,    oldInfo . getQueues (  )  )  )  )  ;", "CapacitySchedulerLeafQueueInfo   origNewA 1     =     (  ( CapacitySchedulerLeafQueueInfo )     ( getQueueInfo (  \" a 1  \"  ,    newInfo . getQueues (  )  )  )  )  ;", "CapacitySchedulerLeafQueueInfo   targetOldA 2     =     (  ( CapacitySchedulerLeafQueueInfo )     ( getQueueInfo (  \" a 2  \"  ,    oldInfo . getQueues (  )  )  )  )  ;", "CapacitySchedulerLeafQueueInfo   targetNewA 2     =     (  ( CapacitySchedulerLeafQueueInfo )     ( getQueueInfo (  \" a 2  \"  ,    newInfo . getQueues (  )  )  )  )  ;", "assertEquals (  1  ,    origOldA 1  . getNumApplications (  )  )  ;", "assertEquals (  1  ,    origNumAppsA )  ;", "assertEquals (  2  ,    origNumAppsRoot )  ;", "assertEquals (  0  ,    origNewA 1  . getNumApplications (  )  )  ;", "assertEquals (  1  ,    newNumAppsA )  ;", "assertEquals (  2  ,    newNumAppsRoot )  ;", "assertEquals (  (  3     *     ( GB )  )  ,    origOldA 1  . getResourcesUsed (  )  . getMemory (  )  )  ;", "assertEquals (  1  ,    origOldA 1  . getResourcesUsed (  )  . getvCores (  )  )  ;", "assertEquals (  0  ,    origNewA 1  . getResourcesUsed (  )  . getMemory (  )  )  ;", "assertEquals (  0  ,    origNewA 1  . getResourcesUsed (  )  . getvCores (  )  )  ;", "assertEquals (  (  3     *     ( GB )  )  ,    targetNewA 2  . getResourcesUsed (  )  . getMemory (  )  )  ;", "assertEquals (  1  ,    targetNewA 2  . getResourcesUsed (  )  . getvCores (  )  )  ;", "assertEquals (  0  ,    targetOldA 2  . getNumApplications (  )  )  ;", "assertEquals (  0  ,    targetOldA 2  . getResourcesUsed (  )  . getMemory (  )  )  ;", "assertEquals (  0  ,    targetOldA 2  . getResourcesUsed (  )  . getvCores (  )  )  ;", "assertEquals (  1  ,    targetNewA 2  . getNumApplications (  )  )  ;", "assertEquals (  1  ,    origOldA 1  . getNumContainers (  )  )  ;", "assertEquals (  0  ,    origNewA 1  . getNumContainers (  )  )  ;", "assertEquals (  1  ,    targetNewA 2  . getNumContainers (  )  )  ;", "assertEquals (  0  ,    targetOldA 2  . getNumContainers (  )  )  ;", "assertEquals (  (  3     *     ( GB )  )  ,    origOldA 1  . getUsers (  )  . getUsersList (  )  . get (  0  )  . getResourcesUsed (  )  . getMemory (  )  )  ;", "assertEquals (  1  ,    origOldA 1  . getUsers (  )  . getUsersList (  )  . get (  0  )  . getResourcesUsed (  )  . getvCores (  )  )  ;", "assertEquals (  0  ,    origNewA 1  . getUsers (  )  . getUsersList (  )  . size (  )  )  ;", "assertEquals (  (  3     *     ( GB )  )  ,    targetNewA 2  . getUsers (  )  . getUsersList (  )  . get (  0  )  . getResourcesUsed (  )  . getMemory (  )  )  ;", "assertEquals (  1  ,    targetNewA 2  . getUsers (  )  . getUsersList (  )  . get (  0  )  . getResourcesUsed (  )  . getvCores (  )  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *     ( GB )  )  ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  1     *     ( GB )  )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  4     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  0     *     ( GB )  )  ,    nm _  1  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAppQueueMetricsCheck"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    setUpMove (  )  ;", "AbstractYarnScheduler    =     (  ( AbstractYarnScheduler )     ( rm . getResourceScheduler (  )  )  )  ;", "RMApp   app    =    rm . submitApp ( GB ,     \" test - move -  1  \"  ,     \" user _  0  \"  ,    null ,     \" a 1  \"  )  ;", "ApplicationAttemptId   appAttemptId    =    rm . getApplicationReport ( app . getApplicationId (  )  )  . getCurrentApplicationAttemptId (  )  ;", "List < ApplicationAttemptId >    appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertEquals (  1  ,    appsInA 1  . size (  )  )  ;", "String   queue    =    getApplicationAttempt ( appsInA 1  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" a 1  \"  )  )  ;", "List < ApplicationAttemptId >    appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "List < ApplicationAttemptId >    appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "List < ApplicationAttemptId >    appsInA 2     =    getAppsInQueue (  \" a 2  \"  )  ;", "assertTrue ( appsInA 2  . isEmpty (  )  )  ;", "moveApplication ( app . getApplicationId (  )  ,     \" a 2  \"  )  ;", "appsInA 2     =    getAppsInQueue (  \" a 2  \"  )  ;", "assertEquals (  1  ,    appsInA 2  . size (  )  )  ;", "queue    =    getApplicationAttempt ( appsInA 2  . get (  0  )  )  . getQueue (  )  . getQueueName (  )  ;", "Assert . assertTrue ( queue . equals (  \" a 2  \"  )  )  ;", "appsInA 1     =    getAppsInQueue (  \" a 1  \"  )  ;", "assertTrue ( appsInA 1  . isEmpty (  )  )  ;", "appsInA    =    getAppsInQueue (  \" a \"  )  ;", "assertTrue ( appsInA . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInA . size (  )  )  ;", "appsInRoot    =    getAppsInQueue (  \" root \"  )  ;", "assertTrue ( appsInRoot . contains ( appAttemptId )  )  ;", "assertEquals (  1  ,    appsInRoot . size (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAppSameParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   scheduler    =    resourceManager . getResourceScheduler (  )  ;", "String   host _  0     =     \" host _  0  \"  ;", "NodeManager   nm _  0     =    registerNode ( host _  0  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  5     *     ( GB )  )  ,     1  )  )  ;", "String   host _  1     =     \" host _  1  \"  ;", "NodeManager   nm _  1     =    registerNode ( host _  1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  5     *     ( GB )  )  ,     1  )  )  ;", "Priority   priority _  0     =    resource . Priority . create (  0  )  ;", "Priority   priority _  1     =    resource . Priority . create (  1  )  ;", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,     \" a 1  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "application _  0  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  0  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  0  _  0     =    Resources . createResource (  (  3     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  1  ,    capability _  0  _  0  )  ;", "Resource   capability _  0  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  0  ,    capability _  0  _  1  )  ;", "Task   task _  0  _  0     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  0  )  ;", "Application   application _  1     =    new   Application (  \" user _  1  \"  ,     \" b 2  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "application _  1  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  1  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  1  _  0     =    Resources . createResource (  (  1     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  1  ,    capability _  1  _  0  )  ;", "Resource   capability _  1  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  1  . addResourceRequestSpec ( priority _  0  ,    capability _  1  _  1  )  ;", "Task   task _  1  _  0     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  1  . addTask ( task _  1  _  0  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "scheduler . moveApplication ( application _  0  . getApplicationId (  )  ,     \" b 2  \"  )  ;", "nodeUpdate ( nm _  0  )  ;", "nodeUpdate ( nm _  1  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  0     *     ( GB )  )  ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  1     *     ( GB )  )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  1     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  0     *     ( GB )  )  ,    nm _  1  )  ;", "scheduler . moveApplication ( application _  0  . getApplicationId (  )  ,     \" a 2  \"  )  ;", "application _  0  . schedule (  )  ;", "nodeUpdate ( nm _  1  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *     ( GB )  )  ,    application _  0  )  ;", "checkNodeResourceUsage (  (  1     *     ( GB )  )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  3     *     ( GB )  )  ,    nm _  1  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAppSuccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "resourceManager    =    new   ResourceManager (  )  ;", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "StringBuilder   qState    =    new   StringBuilder (  )  ;", "qState . append ( CapacitySchedulerConfiguration . PREFIX )  . append ( TestCapacityScheduler . B )  . append ( CapacitySchedulerConfiguration . DOT )  . append ( CapacitySchedulerConfiguration . STATE )  ;", "csConf . set ( qState . toString (  )  ,    STOPPED . name (  )  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration ( csConf )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacityScheduler . class ,    ResourceScheduler . class )  ;", "resourceManager . init ( conf )  ;", "resourceManager . getRMContext (  )  . getContainerTokenSecretManager (  )  . rollMasterKey (  )  ;", "resourceManager . getRMContext (  )  . getNMTokenSecretManager (  )  . rollMasterKey (  )  ;", "(  ( AsyncDispatcher )     ( resourceManager . getRMContext (  )  . getDispatcher (  )  )  )  . start (  )  ;", "mockContext    =    mock ( RMContext . class )  ;", "when ( mockContext . getConfigurationProvider (  )  )  . thenReturn ( new   LocalConfigurationProvider (  )  )  ;", "ResourceScheduler   scheduler    =    resourceManager . getResourceScheduler (  )  ;", "String   host _  0     =     \" host _  0  \"  ;", "NodeManager   nm _  0     =    registerNode ( host _  0  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  6     *     ( GB )  )  ,     1  )  )  ;", "Priority   priority _  0     =    resource . Priority . create (  0  )  ;", "Priority   priority _  1     =    resource . Priority . create (  1  )  ;", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,     \" a 1  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "application _  0  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "Resource   capability _  0  _  0     =    Resources . createResource (  (  3     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  1  ,    capability _  0  _  0  )  ;", "Resource   capability _  0  _  1     =    Resources . createResource (  (  2     *     ( GB )  )  ,     1  )  ;", "application _  0  . addResourceRequestSpec ( priority _  0  ,    capability _  0  _  1  )  ;", "Task   task _  0  _  0     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0     }  )  ;", "application _  0  . addTask ( task _  0  _  0  )  ;", "application _  0  . schedule (  )  ;", "nodeUpdate ( nm _  0  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *     ( GB )  )  ,    application _  0  )  ;", "checkNodeResourceUsage (  (  3     *     ( GB )  )  ,    nm _  0  )  ;", "scheduler . moveApplication ( application _  0  . getApplicationId (  )  ,     \" b 1  \"  )  ;", "}", "METHOD_END"], "methodName": ["testMoveAppViolateQueueState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "cs    =    new    (  )  ;", "cs . setConf ( conf )  ;", "RMContextImpl   rmContext    =    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  ;", "cs . setRMContext ( rmContext )  ;", "Configuration   csConf    =    new   Configuration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "cs . init ( csConf )  ;", "cs . start (  )  ;", "assertEquals (  0  ,    cs . getNumClusterNodes (  )  )  ;", "RMNode   n 1     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  )  ;", "RMNode   n 2     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  2     *     ( GB )  )  )  ,     2  )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( n 2  )  )  ;", "assertEquals (  2  ,    cs . getNumClusterNodes (  )  )  ;", "cs . handle ( new   NodeRemovedSchedulerEvent ( n 1  )  )  ;", "assertEquals (  1  ,    cs . getNumClusterNodes (  )  )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "assertEquals (  2  ,    cs . getNumClusterNodes (  )  )  ;", "cs . handle ( new   NodeRemovedSchedulerEvent ( n 2  )  )  ;", "cs . handle ( new   NodeRemovedSchedulerEvent ( n 1  )  )  ;", "assertEquals (  0  ,    cs . getNumClusterNodes (  )  )  ;", "cs . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNumClusterNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "cs . setConf ( new   YarnConfiguration (  )  )  ;", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( conf )  ;", "conf . setQueues (  (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  . a . a 1  \"  )  ,    new   String [  ]  {     \" b 1  \"     }  )  ;", "conf . setCapacity (  (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  . a . a 1  . b 1  \"  )  ,     1  0  0  .  0 F )  ;", "conf . setUserLimitFactor (  (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  . a . a 1  . b 1  \"  )  ,     1  0  0  .  0 F )  ;", "cs . reinitialize ( conf ,    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  )  ;", "}", "METHOD_END"], "methodName": ["testParseQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,     3  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "int   CONTAINER _ MEMORY    =     1  0  2  4  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "cs    =     (  (  )     ( rm 1  . getResourceScheduler (  )  )  )  ;", "MockNM   nm 1     =    new   MockNM (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     1  5  1  2  0  ,    rm 1  . getResourceTrackerService (  )  )  ;", "nm 1  . registerNode (  )  ;", "RMApp   app 0     =    rm 1  . submitApp ( CONTAINER _ MEMORY )  ;", "MockAM   am 0     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "FiCaSchedulerApp   schedulerAppAttempt    =    cs . getSchedulerApplications (  )  . get ( app 0  . getApplicationId (  )  )  . getCurrentAppAttempt (  )  ;", "List < Container >    allocatedContainers    =    am 0  . allocateAndWaitForContainers (  3  ,    CONTAINER _ MEMORY ,    nm 1  )  ;", "for    ( Container   c    :    allocatedContainers )     {", "cs . killContainer ( schedulerAppAttempt . getRMContainer ( c . getId (  )  )  )  ;", "}", "waitForAppPreemptionInfo ( app 0  ,    Resource . newInstance (  ( CONTAINER _ MEMORY    *     3  )  ,     3  )  ,     0  ,     3  ,    Resource . newInstance (  ( CONTAINER _ MEMORY    *     3  )  ,     3  )  ,    false ,     3  )  ;", "cs . killContainer ( schedulerAppAttempt . getRMContainer ( app 0  . getCurrentAppAttempt (  )  . getMasterContainer (  )  . getId (  )  )  )  ;", "waitForNewAttemptCreated ( app 0  ,    am 0  . getApplicationAttemptId (  )  )  ;", "waitForAppPreemptionInfo ( app 0  ,    Resource . newInstance (  ( CONTAINER _ MEMORY    *     4  )  ,     4  )  ,     1  ,     3  ,    Resource . newInstance (  0  ,     0  )  ,    false ,     0  )  ;", "MockAM   am 1     =    launchAM ( app 0  ,    rm 1  ,    nm 1  )  ;", "schedulerAppAttempt    =    cs . getSchedulerApplications (  )  . get ( app 0  . getApplicationId (  )  )  . getCurrentAppAttempt (  )  ;", "allocatedContainers    =    am 1  . allocateAndWaitForContainers (  3  ,    CONTAINER _ MEMORY ,    nm 1  )  ;", "for    ( Container   c    :    allocatedContainers )     {", "cs . killContainer ( schedulerAppAttempt . getRMContainer ( c . getId (  )  )  )  ;", "}", "waitForAppPreemptionInfo ( app 0  ,    Resource . newInstance (  ( CONTAINER _ MEMORY    *     7  )  ,     7  )  ,     1  ,     6  ,    Resource . newInstance (  ( CONTAINER _ MEMORY    *     3  )  ,     3  )  ,    false ,     3  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "cs . setConf ( new   YarnConfiguration (  )  )  ;", "cs . setRMContext ( resourceManager . getRMContext (  )  )  ;", "cs . init ( csConf )  ;", "cs . start (  )  ;", "cs . reinitialize ( csConf ,    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( csConf )  ,    new   NMTokenSecretManagerInRM ( csConf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  )  ;", "RMNode   n 1     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  )  ;", "RMNode   n 2     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  2     *     ( GB )  )  )  ,     2  )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( n 2  )  )  ;", "Assert . assertEquals (  (  6     *     ( GB )  )  ,    cs . getClusterResource (  )  . getMemory (  )  )  ;", "n 1     =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  2     *     ( GB )  )  )  ,     1  )  ;", "cs . handle ( new   NodeRemovedSchedulerEvent ( n 1  )  )  ;", "cs . handle ( new   NodeAddedSchedulerEvent ( n 1  )  )  ;", "Assert . assertEquals (  (  4     *     ( GB )  )  ,    cs . getClusterResource (  )  . getMemory (  )  )  ;", "cs . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testReconnectedNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    rm 1  . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  1  0  2  4  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "cs    =     (  (  )     ( rm 1  . getResourceScheduler (  )  )  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "ContainerId   containerId 1     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 1  ,    RMContainerState . ALLOCATED )  ;", "RMContainer   rmContainer    =    cs . getRMContainer ( containerId 1  )  ;", "List < ResourceRequest >    requests    =    rmContainer . getResourceRequests (  )  ;", "FiCaSchedulerApp   app    =    cs . getApplicationAttempt ( am 1  . getApplicationAttemptId (  )  )  ;", "FiCaSchedulerNode   node    =    cs . getNode ( rmContainer . getAllocatedNode (  )  )  ;", "for    ( ResourceRequest   request    :    requests )     {", "if    (  ( request . getResourceName (  )  . equals ( node . getRackName (  )  )  )     |  |     ( request . getResourceName (  )  . equals ( ANY )  )  )     {", "continue ;", "}", "Assert . assertNull ( app . getResourceRequest ( request . getPriority (  )  ,    request . getResourceName (  )  )  )  ;", "}", "cs . killContainer ( rmContainer )  ;", "Assert . assertEquals (  3  ,    requests . size (  )  )  ;", "for    ( ResourceRequest   request    :    requests )     {", "Assert . assertEquals (  1  ,    app . getResourceRequest ( request . getPriority (  )  ,    request . getResourceName (  )  )  . getNumContainers (  )  )  ;", "}", "ContainerId   containerId 2     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     3  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . ALLOCATED )  ;", "List < Container >    containers    =    am 1  . allocate ( new   ArrayList < ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "Assert . assertTrue (  (  ( containers . size (  )  )     =  =     1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRecoverRequestAfterPreemption"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "RMContextImpl   rmContext    =    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  ;", "setupQueueConfiguration ( conf )  ;", "cs . setConf ( new   YarnConfiguration (  )  )  ;", "cs . setRMContext ( resourceManager . getRMContext (  )  )  ;", "cs . init ( conf )  ;", "cs . start (  )  ;", "cs . reinitialize ( conf ,    rmContext )  ;", "checkQueueCapacities ( cs ,     . A _ CAPACITY ,     . B _ CAPACITY )  ;", "conf . setCapacity (  . A ,     8  0  .  0 F )  ;", "conf . setCapacity (  . B ,     2  0  .  0 F )  ;", "cs . reinitialize ( conf ,    mockContext )  ;", "checkQueueCapacities ( cs ,     8  0  .  0 F ,     2  0  .  0 F )  ;", "cs . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRefreshQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( conf )  ;", "cs . setConf ( new   YarnConfiguration (  )  )  ;", "cs . setRMContext ( resourceManager . getRMContext (  )  )  ;", "cs . init ( conf )  ;", "cs . start (  )  ;", "cs . reinitialize ( conf ,    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  )  ;", "checkQueueCapacities ( cs ,     . A _ CAPACITY ,     . B _ CAPACITY )  ;", "String   B 4     =     (  . B )     +     \"  . b 4  \"  ;", "float   B 4  _ CAPACITY    =     1  0  ;", ". B 3  _ CAPACITY    -  =    B 4  _ CAPACITY ;", "try    {", "conf . setCapacity (  . A ,     8  0  .  0 F )  ;", "conf . setCapacity (  . B ,     2  0  .  0 F )  ;", "conf . setQueues (  . B ,    new   String [  ]  {     \" b 1  \"  ,     \" b 2  \"  ,     \" b 3  \"  ,     \" b 4  \"     }  )  ;", "conf . setCapacity (  . B 1  ,     . B 1  _ CAPACITY )  ;", "conf . setCapacity (  . B 2  ,     . B 2  _ CAPACITY )  ;", "conf . setCapacity (  . B 3  ,     . B 3  _ CAPACITY )  ;", "conf . setCapacity ( B 4  ,    B 4  _ CAPACITY )  ;", "cs . reinitialize ( conf ,    mockContext )  ;", "checkQueueCapacities ( cs ,     8  0  .  0 F ,     2  0  .  0 F )  ;", "CSQueue   rootQueue    =    cs . getRootQueue (  )  ;", "CSQueue   queueB    =    findQueue ( rootQueue ,     . B )  ;", "CSQueue   queueB 4     =    findQueue ( queueB ,    B 4  )  ;", "assertEquals ( queueB ,    queueB 4  . getParent (  )  )  ;", "}    finally    {", ". B 3  _ CAPACITY    +  =    B 4  _ CAPACITY ;", "cs . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRefreshQueuesWithNewQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "while    ( true )     {", "RMAppMetrics   appPM    =    app . getRMAppMetrics (  )  ;", "RMAppAttemptMetrics   attemptPM    =    app . getCurrentAppAttempt (  )  . getRMAppAttemptMetrics (  )  ;", "if    (  (  (  (  (  ( appPM . getRPreempted (  )  . equals ( preempted )  )     &  &     (  ( appPM . getNumAMContainersPreempted (  )  )     =  =    numAMPreempted )  )     &  &     (  ( appPM . getNumNonAMContainersPreempted (  )  )     =  =    numTaskPreempted )  )     &  &     ( attemptPM . getRPreempted (  )  . equals ( currentAttemptPreempted )  )  )     &  &     (  ( app . getCurrentAppAttempt (  )  . getRMAppAttemptMetrics (  )  . getIsPreempted (  )  )     =  =    currentAttemptAMPreempted )  )     &  &     (  ( attemptPM . getNumNonAMContainersPreempted (  )  )     =  =    numLatestAttemptTaskPreempted )  )     {", "return ;", "}", "Thread . sleep (  5  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForAppPreemptionInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "while    ( app . getCurrentAppAttempt (  )  . equals ( previousAttemptId )  )     {", "Thread . sleep (  5  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForNewAttemptCreated"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestCapacityScheduler"}, {"methodBody": ["METHOD_START", "{", "return    (  ( float )     ( expectedMemory )  )     /     (  ( float )     ( clusterResource . getMemory (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["computeQueueAbsoluteUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "return   expectedMemory    /     (  ( clusterResource . getMemory (  )  )     *     ( queue . getAbsoluteCapacity (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["computeQueueUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerApp   application    =    mock ( FiCaSchedulerApp . class )  ;", "doReturn ( user )  . when ( application )  . getUser (  )  ;", "doReturn ( Resources . createResource (  0  ,     0  )  )  . when ( application )  . getHeadroom (  )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["getMockApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "rmContext    =    TestUtils . getMockRMContext (  )  ;", "conf    =    new   YarnConfiguration (  )  ;", "csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  ,     3  2  )  )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0  0     *     3  2  )  )  )  ;", "when ( csContext . getApplicationComparator (  )  )  . thenReturn ( CapacityScheduler . applicationComparator )  ;", "when ( csContext . getQueueComparator (  )  )  . thenReturn ( CapacityScheduler . queueComparator )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceComparator )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "csConf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    TestChildQueueOrder . A ,    TestChildQueueOrder . B ,    TestChildQueueOrder . C ,    TestChildQueueOrder . D    }  )  ;", "final   String   Q _ A    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestChildQueueOrder . A )  ;", "conf . setCapacity ( Q _ A ,     2  5  )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestChildQueueOrder . B )  ;", "conf . setCapacity ( Q _ B ,     2  5  )  ;", "final   String   Q _ C    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestChildQueueOrder . C )  ;", "conf . setCapacity ( Q _ C ,     2  5  )  ;", "final   String   Q _ D    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestChildQueueOrder . D )  ;", "conf . setCapacity ( Q _ D ,     2  5  )  ;", "}", "METHOD_END"], "methodName": ["setupSortedQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "stubQueueAllocation ( queue ,    clusterResource ,    node ,    allocation ,    NodeType . NODE _ LOCAL )  ;", "}", "METHOD_END"], "methodName": ["stubQueueAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "doAnswer ( new   Answer < CSAssignment >  (  )     {", "@ Override", "public   CSAssignment   answer ( InvocationOnMock   invocation )    throws   Throwable    {", "try    {", "throw   new   Exception (  )  ;", "}    catch    ( Exception   e )     {", ". LOG . info (  (  (  (  (  (  \" FOOBAR   q . assignContainers   q =  \"     +     ( queue . getQueueName (  )  )  )     +     \"    alloc =  \"  )     +    allocation )     +     \"    node =  \"  )     +     ( node . getNodeName (  )  )  )  )  ;", "}", "final   Resource   allocatedResource    =    Resources . createResource ( allocation )  ;", "if    ( queue   instanceof   ParentQueue )     {", "(  ( ParentQueue )     ( queue )  )  . allocateResource ( clusterResource ,    allocatedResource )  ;", "} else    {", "FiCaSchedulerApp   app 1     =    getMockApplication (  0  ,     \"  \"  )  ;", "(  ( LeafQueue )     ( queue )  )  . allocateResource ( clusterResource ,    app 1  ,    allocatedResource )  ;", "}", "if    ( allocation    >     0  )     {", "doReturn ( new   CSAssignment ( Resources . none (  )  ,    type )  )  . when ( queue )  . assignContainers ( eq ( clusterResource )  ,    eq ( node )  )  ;", "Resource   available    =    node . getAvailableResource (  )  ;", "doReturn ( Resources . subtractFrom ( available ,    allocatedResource )  )  . when ( node )  . getAvailableResource (  )  ;", "}", "return   new   CSAssignment ( allocatedResource ,    type )  ;", "}", "}  )  . when ( queue )  . assignContainers ( eq ( clusterResource )  ,    eq ( node )  )  ;", "doNothing (  )  . when ( node )  . releaseContainer ( any ( Container . class )  )  ;", "}", "METHOD_END"], "methodName": ["stubQueueAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "setupSortedQueues ( csConf )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "final   int   memoryPerNode    =     1  0  ;", "final   int   coresPerNode    =     1  6  ;", "final   int   numNodes    =     1  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "doNothing (  )  . when ( node _  0  )  . releaseContainer ( any ( Container . class )  )  ;", "final   Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     ( memoryPerNode    *     (  . GB )  )  )  ,     ( numNodes    *    coresPerNode )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "CSQueue   a    =    queues . get (  . A )  ;", "CSQueue   b    =    queues . get (  . B )  ;", "CSQueue   c    =    queues . get (  . C )  ;", "CSQueue   d    =    queues . get (  . D )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "FiCaSchedulerApp   app _  0     =    getMockApplication (  0  ,    user _  0  )  ;", "doReturn ( true )  . when ( app _  0  )  . containerCompleted ( any ( RMContainer . class )  ,    any ( ContainerStatus . class )  ,    any ( RMContainerEventType . class )  )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "ContainerAllocationExpirer   expirer    =    mock ( ContainerAllocationExpirer . class )  ;", "DrainDispatcher   drainDispatcher    =    new   DrainDispatcher (  )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getContainerAllocationExpirer (  )  )  . thenReturn ( expirer )  ;", "when ( rmContext . getDispatcher (  )  )  . thenReturn ( drainDispatcher )  ;", "when ( rmContext . getRMApplicationHistoryWriter (  )  )  . thenReturn ( writer )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( app _  0  . getApplicationId (  )  ,     1  )  ;", "ContainerId   containerId    =    BuilderUtils . newContainerId ( appAttemptId ,     1  )  ;", "Container   container    =    TestUtils . getMockContainer ( containerId ,    node _  0  . getNodeID (  )  ,    Resources . createResource (  (  1     *     (  . GB )  )  )  ,    priority )  ;", "RMContainer   rmContainer    =    new   RMContainerImpl ( container ,    appAttemptId ,    node _  0  . getNodeID (  )  ,     \" user \"  ,    rmContext )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;    i +  +  )     {", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "}", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "}", "for    ( int   i    =     0  ;    i    <     4  ;    i +  +  )     {", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "}", "verifyQueueMetrics ( a ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  4     *     (  . GB )  )  ,    clusterResource )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "d . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    null ,    RMContainerEventType . KILL ,    null )  ;", "}", "verifyQueueMetrics ( a ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     (  (  (  (  ( memoryPerNode    -     1  )     -     2  )     -     3  )     -     1  )     *     (  . GB )  )  )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;    i +  +  )     {", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "}", "verifyQueueMetrics ( a ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    null ,    RMContainerEventType . KILL ,    null )  ;", "verifyQueueMetrics ( a ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     (  (  (  (  ( memoryPerNode    -     2  )     -     2  )     -     3  )     -     1  )     *     (  . GB )  )  )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "verifyQueueMetrics ( a ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "b . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    null ,    RMContainerEventType . KILL ,    null )  ;", "verifyQueueMetrics ( a ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     (  (  (  (  ( memoryPerNode    -     2  )     -     2  )     -     3  )     -     1  )     *     (  . GB )  )  )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "verifyQueueMetrics ( a ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "InOrder   allocationOrder    =    inOrder ( d ,    b )  ;", "allocationOrder . verify ( d )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", ". LOG . info (  (  \" status   child - queues :     \"     +     (  (  ( ParentQueue )     ( root )  )  . getChildQueuesToPrint (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testSortedQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( computeQueueAbsoluteUsedCapacity ( queue ,    expectedMemory ,    clusterResource )  ,    queue . getAbsoluteUsedCapacity (  )  ,    TestChildQueueOrder . DELTA )  ;", "assertEquals ( computeQueueUsedCapacity ( queue ,    expectedMemory ,    clusterResource )  ,    queue . getUsedCapacity (  )  ,    TestChildQueueOrder . DELTA )  ;", "}", "METHOD_END"], "methodName": ["verifyQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestChildQueueOrder"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    CapacitySlass ,    ResourceSlass )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm 1     =    new   MockRM ( conf )     {", "@ Override", "protected   RMSecretManagerService   createRMSecretManagerService (  )     {", "return   new    . TestRMSecretManagerService (  . this . conf ,    rmContext )  ;", "}", "}  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    rm 1  . registerNode (  \" unknownhost :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "SecurityUtilTestHelper . setTokenServiceUseIp ( true )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "RMAppAttempt   attempt    =    app 1  . getCurrentAppAttempt (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "while    (  ( numRetries )     <  =     5  )     {", "nm 1  . nodeHeartbeat ( true )  ;", "Thread . sleep (  1  0  0  0  )  ;", "Assert . assertEquals ( RMAppAttemptState . SCHEDULED ,    attempt . getAppAttemptState (  )  )  ;", "System . out . println (  \" Waiting   for   am   container   to   be   allocated .  \"  )  ;", "}", "SecurityUtilTestHelper . setTokenServiceUseIp ( false )  ;", "rm 1  . waitForState ( attempt . getAppAttemptId (  )  ,    RMAppAttemptState . ALLOCATED )  ;", "MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "}", "METHOD_END"], "methodName": ["testAMContainerAllocationWhenDNSUnavailable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    rm 1  . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "ContainerId   containerId 2     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . ALLOCATED )  ;", "RMContainer   container    =    rm 1  . getResourceScheduler (  )  . getRMContainer ( containerId 2  )  ;", "Assert . assertEquals ( containerId 2  ,    container . getContainerId (  )  )  ;", "Assert . assertNull ( container . getContainer (  )  . getContainerToken (  )  )  ;", "List < Container >    containers    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "Assert . assertEquals ( containerId 2  ,    containers . get (  0  )  . getId (  )  )  ;", "Assert . assertNotNull ( containers . get (  0  )  . getContainerToken (  )  )  ;", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testContainerTokenGeneratedOnPullRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "MockNM   nm 1     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     (  2     *     ( GB )  )  ,     4  )  ;", "MockNM   nm 2     =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  2  2  3  4  \"  ,     (  3     *     ( GB )  )  ,     4  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "nm 2  . nodeHeartbeat ( true )  ;", "int   waitCount    =     2  0  ;", "int   size    =    rm . getRMContext (  )  . getRMNodes (  )  . size (  )  ;", "while    (  (  ( size    =    rm . getRMContext (  )  . getRMNodes (  )  . size (  )  )     !  =     2  )     &  &     (  ( waitCount -  -  )     >     0  )  )     {", ". LOG . info (  (  \" Waiting   for   node   managers   to   register    :     \"     +    size )  )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "Assert . assertEquals (  2  ,    rm . getRMContext (  )  . getRMNodes (  )  . size (  )  )  ;", "RMApp   app 1     =    rm . submitApp (  1  2  8  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "RMAppAttempt   attempt 1     =    app 1  . getCurrentAppAttempt (  )  ;", "MockAM   am 1     =    rm . sendAMLaunched ( attempt 1  . getAppAttemptId (  )  )  ;", "am 1  . registerAppAttempt (  )  ;", ". LOG . info (  \" sending   container   requests    \"  )  ;", "am 1  . addRequests ( new   String [  ]  {     \"  *  \"     }  ,     (  3     *     ( GB )  )  ,     1  ,     1  )  ;", "AllocateResponse   alloc 1 Response    =    am 1  . schedule (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "int   waitCounter    =     2  0  ;", ". LOG . info (  \" heartbeating   nm 1  \"  )  ;", "while    (  (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     <     1  )     &  &     (  ( waitCounter -  -  )     >     0  )  )     {", ". LOG . info (  \" Waiting   for   containers   to   be   created   for   app    1  .  .  .  \"  )  ;", "Thread . sleep (  5  0  0  )  ;", "alloc 1 Response    =    am 1  . schedule (  )  ;", "}", ". LOG . info (  (  \" received   container    :     \"     +     ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )  )  )  ;", "Assert . assertTrue (  (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     =  =     0  )  )  ;", ". LOG . info (  \" heartbeating   nm 2  \"  )  ;", "waitCounter    =     2  0  ;", "nm 2  . nodeHeartbeat ( true )  ;", "while    (  (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     <     1  )     &  &     (  ( waitCounter -  -  )     >     0  )  )     {", ". LOG . info (  \" Waiting   for   containers   to   be   created   for   app    1  .  .  .  \"  )  ;", "Thread . sleep (  5  0  0  )  ;", "alloc 1 Response    =    am 1  . schedule (  )  ;", "}", ". LOG . info (  (  \" received   container    :     \"     +     ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )  )  )  ;", "Assert . assertTrue (  (  ( alloc 1 Response . getAllocatedContainers (  )  . size (  )  )     =  =     1  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testExcessReservationThanNodeManagerCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm 1     =    new   MockRM ( conf )  ;", "rm 1  . start (  )  ;", "MockNM   nm 1     =    rm 1  . registerNode (  \" unknownhost :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "RMApp   app 1     =    rm 1  . submitApp (  2  0  0  )  ;", "MockAM   am 1     =    MockRM . launchAndRegisterAM ( app 1  ,    rm 1  ,    nm 1  )  ;", "am 1  . allocate (  \"  1  2  7  .  0  .  0  .  1  \"  ,     1  0  2  4  ,     1  ,    new   ArrayList < ContainerId >  (  )  )  ;", "ContainerId   containerId 2     =    ContainerId . newInstance ( am 1  . getApplicationAttemptId (  )  ,     2  )  ;", "rm 1  . waitForState ( nm 1  ,    containerId 2  ,    RMContainerState . ALLOCATED )  ;", "SecurityUtilTestHelper . setTokenServiceUseIp ( true )  ;", "List < Container >    containers    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "Assert . assertEquals (  0  ,    containers . size (  )  )  ;", "SecurityUtilTestHelper . setTokenServiceUseIp ( false )  ;", "containers    =    am 1  . allocate ( new   ArrayList < api . records . ResourceRequest >  (  )  ,    new   ArrayList < ContainerId >  (  )  )  . getAllocatedContainers (  )  ;", "Assert . assertEquals (  1  ,    containers . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNormalContainerAllocationWhenDNSUnavailable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestContainerAllocation"}, {"methodBody": ["METHOD_START", "{", "for    ( QueueUserACLInfo   aclInfo    :    aclInfos )     {", "if    ( aclInfo . getUserAcls (  )  . contains ( acl )  )     {", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["hasQueueACL"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerContext   csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( new   YarnConfiguration (  )  )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( clusterResource )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  2     *     (  . GB )  )  ,     2  )  )  ;", "return   csContext ;", "}", "METHOD_END"], "methodName": ["mockCSContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "CapacityScheduler   spyCs    =    new   CapacityScheduler (  )  ;", "cs    =    spy ( spyCs )  ;", "rmContext    =    TestUtils . getMockRMContext (  )  ;", "csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csConf . setBoolean (  \" yarnuser - metrics . enable \"  ,    true )  ;", "final   String   newRoot    =     \" root \"     +     ( System . currentTimeMillis (  )  )  ;", "setupQueueConfiguration ( csConf ,    newRoot )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "cs . setConf ( conf )  ;", "csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource ( TestLeafQueue . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     ( TestLeafQueue . GB )  )  ,     3  2  )  )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( Resources . createResource (  (  (  1  0  0     *     1  6  )     *     ( TestLeafQueue . GB )  )  ,     (  1  0  0     *     3  2  )  )  )  ;", "when ( csContext . getApplicationComparator (  )  )  . thenReturn ( CapacityScheduler . applicationComparator )  ;", "when ( csContext . getQueueComparator (  )  )  . thenReturn ( CapacityScheduler . queueComparator )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceCalculator )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "containerTokenSecretManager . rollMasterKey (  )  ;", "when ( csContext . getContainerTokenSecretManager (  )  )  . thenReturn ( containerTokenSecretManager )  ;", "root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "cs . setRMContext ( rmContext )  ;", "cs . init ( csConf )  ;", "cs . start (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    newRoot    }  )  ;", "conf . setMaximumCapacity ( CapacitySchedulerConfiguration . ROOT ,     1  0  0  )  ;", "conf . setAcl ( CapacitySchedulerConfiguration . ROOT ,    SUBMIT _ APPLICATIONS ,     \"     \"  )  ;", "final   String   Q _ newRoot    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +    newRoot ;", "conf . setQueues ( Q _ newRoot ,    new   String [  ]  {     . A ,     . B ,     . C ,     . D ,     . E    }  )  ;", "conf . setCapacity ( Q _ newRoot ,     1  0  0  )  ;", "conf . setMaximumCapacity ( Q _ newRoot ,     1  0  0  )  ;", "conf . setAcl ( Q _ newRoot ,    SUBMIT _ APPLICATIONS ,     \"     \"  )  ;", "final   String   Q _ A    =     ( Q _ newRoot    +     \"  .  \"  )     +     (  . A )  ;", "conf . setCapacity ( Q _ A ,     8  .  5 F )  ;", "conf . setMaximumCapacity ( Q _ A ,     2  0  )  ;", "conf . setAcl ( Q _ A ,    SUBMIT _ APPLICATIONS ,     \"  *  \"  )  ;", "final   String   Q _ B    =     ( Q _ newRoot    +     \"  .  \"  )     +     (  . B )  ;", "conf . setCapacity ( Q _ B ,     8  0  )  ;", "conf . setMaximumCapacity ( Q _ B ,     9  9  )  ;", "conf . setAcl ( Q _ B ,    SUBMIT _ APPLICATIONS ,     \"  *  \"  )  ;", "final   String   Q _ C    =     ( Q _ newRoot    +     \"  .  \"  )     +     (  . C )  ;", "conf . setCapacity ( Q _ C ,     1  .  5 F )  ;", "conf . setMaximumCapacity ( Q _ C ,     1  0  )  ;", "conf . setAcl ( Q _ C ,    SUBMIT _ APPLICATIONS ,     \"     \"  )  ;", "conf . setQueues ( Q _ C ,    new   String [  ]  {     . C 1     }  )  ;", "final   String   Q _ C 1     =     ( Q _ C    +     \"  .  \"  )     +     (  . C 1  )  ;", "conf . setCapacity ( Q _ C 1  ,     1  0  0  )  ;", "final   String   Q _ D    =     ( Q _ newRoot    +     \"  .  \"  )     +     (  . D )  ;", "conf . setCapacity ( Q _ D ,     9  )  ;", "conf . setMaximumCapacity ( Q _ D ,     1  1  )  ;", "conf . setAcl ( Q _ D ,    SUBMIT _ APPLICATIONS ,     \" user _ d \"  )  ;", "final   String   Q _ E    =     ( Q _ newRoot    +     \"  .  \"  )     +     (  . E )  ;", "conf . setCapacity ( Q _ E ,     1  )  ;", "conf . setMaximumCapacity ( Q _ E ,     1  )  ;", "conf . setAcl ( Q _ E ,    SUBMIT _ APPLICATIONS ,     \" user _ e \"  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "doAnswer ( new   Answer < Container >  (  )     {", "@ Override", "public   Container   answer ( InvocationOnMock   invocation )    throws   Throwable    {", "final   FiCaSApp   application    =     (  ( FiCaSApp )     ( invocation . getArguments (  )  [  0  ]  )  )  ;", "final   ContainerId   containerId    =    TestUtils . getMockContainerId ( application )  ;", "Container   container    =    TestUtils . getMockContainer ( containerId ,     (  ( FiCaSNode )     ( invocation . getArguments (  )  [  1  ]  )  )  . getNodeID (  )  ,     (  ( Resource )     ( invocation . getArguments (  )  [  2  ]  )  )  ,     (  ( Priority )     ( invocation . getArguments (  )  [  3  ]  )  )  )  ;", "return   container ;", "}", "}  )  . when ( queue )  . createContainer ( any ( FiCaSApp . class )  ,    any ( FiCaSNode . class )  ,    any ( Resource . class )  ,    any ( Priority . class )  )  ;", "CSQueue   parent    =    queue . getParent (  )  ;", "doNothing (  )  . when ( parent )  . completedContainer ( any ( Resource . class )  ,    any ( FiCaSApp . class )  ,    any ( FiCaSNode . class )  ,    any ( RMContainer . class )  ,    any ( ContainerStatus . class )  ,    any ( RMContainerEventType . class )  ,    any ( CSQueue . class )  )  ;", "return   queue ;", "}", "METHOD_END"], "methodName": ["stubLeafQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    (  ( cs )     !  =    null )     {", "cs . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   e    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . E )  )  )  )  ;", "final   String   user _ e    =     \" user _ e \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _ e ,    e ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "e . submitApplicationAttempt ( app _  0  ,    user _ e )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _ e ,    e ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "e . submitApplicationAttempt ( app _  1  ,    user _ e )  ;", "final   ApplicationAttemptId   appAttemptId _  2     =    TestUtils . getMockApplicationAttemptId (  2  ,     0  )  ;", "FiCaSchedulerApp   app _  2     =    new   FiCaSchedulerApp ( appAttemptId _  2  ,    user _ e ,    e ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "e . submitApplicationAttempt ( app _  2  ,    user _ e )  ;", "assertEquals (  2  ,    e . activeApplications . size (  )  )  ;", "assertEquals (  1  ,    e . pendingApplications . size (  )  )  ;", "csConf . setDouble ( CapacitySchedulerConfiguration . MAXIMUM _ APPLICATION _ MASTERS _ RESOURCE _ PERCENT ,     (  ( CapacitySchedulerConfiguration . DEFAULT _ MAXIMUM _ APPLICATIONMASTERS _ RESOURCE _ PERCENT )     *     2  )  )  ;", "Map < String ,    CSQueue >    newQueues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   newRoot    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    newQueues ,    queues ,    TestUtils . spyHook )  ;", "queues    =    newQueues ;", "root . reinitialize ( newRoot ,    cs . getClusterResource (  )  )  ;", "assertEquals (  3  ,    e . activeApplications . size (  )  )  ;", "assertEquals (  0  ,    e . pendingApplications . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testActivateApplicationAfterQueueRefresh"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   e    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . E )  )  )  )  ;", "final   String   user _ e    =     \" user _ e \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _ e ,    e ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "e . submitApplicationAttempt ( app _  0  ,    user _ e )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _ e ,    e ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "e . submitApplicationAttempt ( app _  1  ,    user _ e )  ;", "final   ApplicationAttemptId   appAttemptId _  2     =    TestUtils . getMockApplicationAttemptId (  2  ,     0  )  ;", "FiCaSchedulerApp   app _  2     =    new   FiCaSchedulerApp ( appAttemptId _  2  ,    user _ e ,    e ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "e . submitApplicationAttempt ( app _  2  ,    user _ e )  ;", "assertEquals (  2  ,    e . activeApplications . size (  )  )  ;", "assertEquals (  1  ,    e . pendingApplications . size (  )  )  ;", "e . updateClusterResource ( Resources . createResource (  (  (  2  0  0     *     1  6  )     *     ( TestLeafQueue . GB )  )  ,     (  1  0  0     *     3  2  )  )  )  ;", "assertEquals (  3  ,    e . activeApplications . size (  )  )  ;", "assertEquals (  0  ,    e . pendingApplications . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testActivateApplicationByUpdatingClusterResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . B )  )  )  )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     1  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Arrays . asList ( TestUtils . createResourceRequest (  \"  1  2  7  .  0  .  0  .  1  \"  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     3  ,    true ,    priority ,    recordFactory )  ,    TestUtils . createResourceRequest ( TestLeafQueue . DEFAULT _ RACK ,     (  1     *     ( TestLeafQueue . GB )  )  ,     3  ,    true ,    priority ,    recordFactory )  )  )  ;", "try    {", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "}    catch    ( NullPointerException   e )     {", "Assert . fail (  (  \" NPE   when   allocating   container   on   node   but    \"     +     \" forget   to   set   off - switch   request   should   be   handled \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAllocateContainerOnNodeWithoutOffSwitchSpecified"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . B )  )  )  )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     1  )  ;", "AppAddedSchedulerEvent   addAppEvent    =    new   AppAddedSchedulerEvent ( appAttemptId _  0  . getApplicationId (  )  ,    a . getQueueName (  )  ,    user _  0  )  ;", "cs . handle ( addAppEvent )  ;", "AppAttemptAddedSchedulerEvent   addAttemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId _  0  ,    false )  ;", "cs . handle ( addAttemptEvent )  ;", "AppAttemptRemovedSchedulerEvent   event    =    new   AppAttemptRemovedSchedulerEvent ( appAttemptId _  0  ,    RMAppAttemptState . FAILED ,    false )  ;", "cs . handle ( event )  ;", "assertEquals (  0  ,    a . getMetrics (  )  . getAppsPending (  )  )  ;", "assertEquals (  0  ,    a . getMetrics (  )  . getAppsFailed (  )  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  0  ,     2  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    null ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "assertEquals (  1  ,    a . getMetrics (  )  . getAppsSubmitted (  )  )  ;", "assertEquals (  1  ,    a . getMetrics (  )  . getAppsPending (  )  )  ;", "event    =    new   AppAttemptRemovedSchedulerEvent ( appAttemptId _  0  ,    RMAppAttemptState . FINISHED ,    false )  ;", "cs . handle ( event )  ;", "AppRemovedSchedulerEvent   rEvent    =    new   AppRemovedSchedulerEvent ( appAttemptId _  0  . getApplicationId (  )  ,    RMAppState . FINISHED )  ;", "cs . handle ( rEvent )  ;", "assertEquals (  1  ,    a . getMetrics (  )  . getAppsSubmitted (  )  )  ;", "assertEquals (  0  ,    a . getMetrics (  )  . getAppsPending (  )  )  ;", "assertEquals (  0  ,    a . getMetrics (  )  . getAppsFailed (  )  )  ;", "assertEquals (  1  ,    a . getMetrics (  )  . getAppsCompleted (  )  )  ;", "QueueMetrics   userMetrics    =    a . getMetrics (  )  . getUserMetrics ( user _  0  )  ;", "assertEquals (  1  ,    userMetrics . getAppsSubmitted (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "String   rack _  0     =     \" rack _  0  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    rack _  0  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "String   rack _  1     =     \" rack _  1  \"  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode ( host _  1  ,    rack _  1  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  2     =     \"  1  2  7  .  0  .  0  .  3  \"  ;", "String   rack _  2     =     \" rack _  2  \"  ;", "FiCaSchedulerNode   node _  2     =    TestUtils . getMockNode ( host _  2  ,    rack _  2  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     3  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     1  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "List < ResourceRequest >    app _  0  _ requests _  0     =    new   ArrayList < ResourceRequest >  (  )  ;", "Priority   priority _  1     =    TestUtils . createMockPriority (  1  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority _  1  ,    recordFactory )  )  ;", "Priority   priority _  2     =    TestUtils . createMockPriority (  2  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  2  ,     (  2     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  2  ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  2  ,     (  2     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  2  ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  2     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority _  2  ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    eq ( priority _  1  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  1  ,    app _  0  . getSchedulingOpportunities ( priority _  1  )  )  ;", "assertEquals (  2  ,    app _  0  . getTotalRequiredResources ( priority _  1  )  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    eq ( priority _  2  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority _  2  )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority _  2  )  )  ;", "a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    eq ( priority _  1  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  2  ,    app _  0  . getSchedulingOpportunities ( priority _  1  )  )  ;", "assertEquals (  2  ,    app _  0  . getTotalRequiredResources ( priority _  1  )  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    eq ( priority _  2  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority _  2  )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority _  2  )  )  ;", "a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . OFF _ SWITCH )  ,    eq ( node _  2  )  ,    eq ( priority _  1  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  3  ,    app _  0  . getSchedulingOpportunities ( priority _  1  )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority _  1  )  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    eq ( priority _  2  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority _  2  )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority _  2  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . NODE _ LOCAL )  ,    eq ( node _  0  )  ,    eq ( priority _  1  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority _  1  )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority _  1  )  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  0  )  ,    eq ( priority _  2  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority _  2  )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority _  2  )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  1  )  ,    eq ( priority _  1  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority _  1  )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority _  1  )  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . OFF _ SWITCH )  ,    eq ( node _  1  )  ,    eq ( priority _  2  )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  1  ,    app _  0  . getSchedulingOpportunities ( priority _  2  )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority _  2  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationPriorityScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  2     =    TestUtils . getMockApplicationAttemptId (  2  ,     0  )  ;", "FiCaSchedulerApp   app _  2     =    new   FiCaSchedulerApp ( appAttemptId _  2  ,    user _  1  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  2  ,    user _  1  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode ( host _  1  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     2  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     1  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  2     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . setUserLimit (  5  0  )  ;", "a . setUserLimitFactor (  2  )  ;", "assertEquals (  \" There   should   only   be    1    active   user !  \"  ,     1  ,    a . getActiveUsersManager (  )  . getNumActiveUsers (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getHeadroom (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getHeadroom (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getHeadroom (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getHeadroom (  )  . getMemory (  )  )  ;", "a . setMaxCapacity (  0  .  1 F )  ;", "app _  2  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "assertEquals (  2  ,    a . getActiveUsersManager (  )  . getNumActiveUsers (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getHeadroom (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getHeadroom (  )  . getMemory (  )  )  ;", "TestLeafQueue . LOG . info (  \" here \"  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     0  ,    true ,    priority ,    recordFactory )  )  )  ;", "assertEquals (  1  ,    a . getActiveUsersManager (  )  . getNumActiveUsers (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getHeadroom (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHeadroomWithMaxCap"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   user    =    UserGroupInformation . getCurrentUser (  )  ;", "LeafQueue   a    =     . stubLeafQueue (  (  ( LeafQueue )     ( queues . get (  . A )  )  )  )  ;", "LeafQueue   b    =     . stubLeafQueue (  (  ( LeafQueue )     ( queues . get (  . B )  )  )  )  ;", "ParentQueue   c    =     (  ( ParentQueue )     ( queues . get (  . C )  )  )  ;", "LeafQueue   c 1     =     . stubLeafQueue (  (  ( LeafQueue )     ( queues . get (  . C 1  )  )  )  )  ;", "assertFalse ( root . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertTrue ( a . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertTrue ( b . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertFalse ( c . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertFalse ( c 1  . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertTrue ( hasQueueACL ( a . getQueueUserAclInfo ( user )  ,    SUBMIT _ APPLICATIONS )  )  ;", "assertTrue ( hasQueueACL ( b . getQueueUserAclInfo ( user )  ,    SUBMIT _ APPLICATIONS )  )  ;", "assertFalse ( hasQueueACL ( c . getQueueUserAclInfo ( user )  ,    SUBMIT _ APPLICATIONS )  )  ;", "assertFalse ( hasQueueACL ( c 1  . getQueueUserAclInfo ( user )  ,    SUBMIT _ APPLICATIONS )  )  ;", "}", "METHOD_END"], "methodName": ["testInheritedQueueAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "final   float   epsilon    =     1  .  0 E -  5 F ;", "LeafQueue   a    =     . stubLeafQueue (  (  ( LeafQueue )     ( queues . get (  . A )  )  )  )  ;", "assertEquals (  0  .  0  8  5  ,    a . getCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  0  8  5  ,    a . getAbsoluteCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  2  ,    a . getMaximumCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  2  ,    a . getAbsoluteMaximumCapacity (  )  ,    epsilon )  ;", "LeafQueue   b    =     . stubLeafQueue (  (  ( LeafQueue )     ( queues . get (  . B )  )  )  )  ;", "assertEquals (  0  .  8  ,    b . getCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  8  ,    b . getAbsoluteCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  9  9  ,    b . getMaximumCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  9  9  ,    b . getAbsoluteMaximumCapacity (  )  ,    epsilon )  ;", "ParentQueue   c    =     (  ( ParentQueue )     ( queues . get (  . C )  )  )  ;", "assertEquals (  0  .  0  1  5  ,    c . getCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  0  1  5  ,    c . getAbsoluteCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  1  ,    c . getMaximumCapacity (  )  ,    epsilon )  ;", "assertEquals (  0  .  1  ,    c . getAbsoluteMaximumCapacity (  )  ,    epsilon )  ;", "}", "METHOD_END"], "methodName": ["testInitializeQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "String   host _  0  _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "String   rack _  0     =     \" rack _  0  \"  ;", "String   host _  0  _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "FiCaSchedulerNode   node _  0  _  1     =    TestUtils . getMockNode ( host _  0  _  1  ,    rack _  0  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1  _  0     =     \"  1  2  7  .  0  .  0  .  3  \"  ;", "String   rack _  1     =     \" rack _  1  \"  ;", "FiCaSchedulerNode   node _  1  _  0     =    TestUtils . getMockNode ( host _  1  _  0  ,    rack _  1  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1  _  1     =     \"  1  2  7  .  0  .  0  .  4  \"  ;", "FiCaSchedulerNode   node _  1  _  1     =    TestUtils . getMockNode ( host _  1  _  1  ,    rack _  1  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     4  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "List < ResourceRequest >    app _  0  _ requests _  0     =    new   ArrayList < ResourceRequest >  (  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  0  _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  1  _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    false ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    false ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "app _  0  . updateBlacklist ( Collections . singletonList ( host _  0  _  0  )  ,    null )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "a . assignContainers ( clusterResource ,    node _  0  _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  0  _  1  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  0  _  1  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "app _  0  . updateBlacklist ( Collections . singletonList ( host _  1  _  1  )  ,    null )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  1  _  1  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "app _  0  . updateBlacklist ( Collections . singletonList ( rack _  1  )  ,    Collections . singletonList ( host _  1  _  1  )  )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  1  _  1  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "app _  0  . updateBlacklist ( null ,    Collections . singletonList ( rack _  1  )  )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( eq ( NodeType . RACK _ LOCAL )  ,    eq ( node _  1  _  1  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    false ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    false ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  0  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . NODE _ LOCAL )  ,    eq ( node _  1  _  0  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalityConstraints"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "String   rack _  0     =     \" rack _  0  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    rack _  0  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "String   rack _  1     =     \" rack _  1  \"  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode ( host _  1  ,    rack _  1  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  2     =     \"  1  2  7  .  0  .  0  .  3  \"  ;", "String   rack _  2     =     \" rack _  2  \"  ;", "FiCaSchedulerNode   node _  2     =    TestUtils . getMockNode ( host _  2  ,    rack _  2  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     3  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "List < ResourceRequest >    app _  0  _ requests _  0     =    new   ArrayList < ResourceRequest >  (  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     3  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "CSAssignment   assignment    =    null ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  1  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  3  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    assignment . getType (  )  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  2  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  3  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    assignment . getType (  )  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  2  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  3  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  3  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    assignment . getType (  )  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  2  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . OFF _ SWITCH )  ,    eq ( node _  2  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  4  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  2  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . OFF _ SWITCH ,    assignment . getType (  )  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  0  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . NODE _ LOCAL )  ,    eq ( node _  0  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    assignment . getType (  )  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  1  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . NODE _ LOCAL )  ,    eq ( node _  1  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    assignment . getType (  )  )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "assertEquals (  2  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "String   host _  3     =     \"  1  2  7  .  0  .  0  .  4  \"  ;", "FiCaSchedulerNode   node _  3     =    TestUtils . getMockNode ( host _  3  ,    rack _  1  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "doReturn (  1  )  . when ( a )  . getNodeLocalityDelay (  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  3  )  ;", "assertEquals (  1  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  2  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    assignment . getType (  )  )  ;", "assignment    =    a . assignContainers ( clusterResource ,    node _  3  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . RACK _ LOCAL )  ,    eq ( node _  3  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "assertEquals ( NodeType . RACK _ LOCAL ,    assignment . getType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalityScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "Resource   clusterResource    =    Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0  0     *     3  2  )  )  ;", "CapacitySchedulerContext   csContext    =    mockCSContext ( csConf ,    clusterResource )  ;", "csConf . setFloat ( CapacitySchedulerConfiguration . MAXIMUM _ APPLICATION _ MASTERS _ RESOURCE _ PERCENT ,     0  .  1 F )  ;", "ParentQueue   root    =    new   ParentQueue ( csContext ,    CapacitySchedulerConfiguration . ROOT ,    null ,    null )  ;", "csConf . setCapacity (  (  (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . A )  )  ,     8  0  )  ;", "LeafQueue   a    =    new   LeafQueue ( csContext ,     . A ,    root ,    null )  ;", "assertEquals (  0  .  1 F ,    a . getMaxAMResourcePerQueuePercent (  )  ,     0  .  0  0  1 F )  ;", "assertEquals (  1  6  0  ,    a . getMaximumActiveApplications (  )  )  ;", "csConf . setFloat ( CapacitySchedulerConfiguration . MAXIMUM _ APPLICATION _ MASTERS _ RESOURCE _ PERCENT ,     0  .  2 F )  ;", "LeafQueue   newA    =    new   LeafQueue ( csContext ,     . A ,    root ,    null )  ;", "a . reinitialize ( newA ,    clusterResource )  ;", "assertEquals (  0  .  2 F ,    a . getMaxAMResourcePerQueuePercent (  )  ,     0  .  0  0  1 F )  ;", "assertEquals (  3  2  0  ,    a . getMaximumActiveApplications (  )  )  ;", "Resource   newClusterResource    =    Resources . createResource (  (  (  1  0  0     *     2  0  )     *     (  . GB )  )  ,     (  1  0  0     *     3  2  )  )  ;", "a . updateClusterResource ( newClusterResource )  ;", "assertEquals (  4  0  0  ,    a . getMaximumActiveApplications (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMaxAMResourcePerQueuePercentAfterQueueRefresh"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   e    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . E )  )  )  )  ;", "assertEquals (  4  0  ,    e . getNodeLocalityDelay (  )  )  ;", "csConf . setInt ( CapacitySchedulerConfiguration . NODE _ LOCALITY _ DELAY ,     6  0  )  ;", "Map < String ,    CSQueue >    newQueues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   newRoot    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    newQueues ,    queues ,    TestUtils . spyHook )  ;", "queues    =    newQueues ;", "root . reinitialize ( newRoot ,    cs . getClusterResource (  )  )  ;", "assertEquals (  6  0  ,    e . getNodeLocalityDelay (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeLocalityAfterQueueRefresh"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  1  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  1  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  4     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     2  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  4     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  4     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAvailableMB (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  6     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "RMContainer   rmContainer    =    app _  0  . getLiveContainers (  )  . iterator (  )  . next (  )  ;", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  5     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "rmContainer    =    app _  0  . getLiveContainers (  )  . iterator (  )  . next (  )  ;", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReservation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "a . setUserLimitFactor (  1  0  )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  1  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  1  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  4     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode ( host _  1  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  4     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     3  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  4     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  4     *     ( TestLeafQueue . GB )  )  ,     1  6  )  )  ;", "when ( a . getMaximumAllocation (  )  )  . thenReturn ( Resources . createResource (  (  4     *     ( TestLeafQueue . GB )  )  ,     1  6  )  )  ;", "when ( a . getMinimumAllocationFactor (  )  )  . thenReturn (  0  .  2  5 F )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  4     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  6     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "RMContainer   rmContainer    =    app _  0  . getLiveContainers (  )  . iterator (  )  . next (  )  ;", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  5     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  1  ,    app _  1  . getReReservations ( priority )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  5     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  2  ,    app _  1  . getReReservations ( priority )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  )  ;", "assertEquals (  (  9     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    node _  1  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  2  ,    app _  1  . getReReservations ( priority )  )  ;", "rmContainer    =    app _  0  . getLiveContainers (  )  . iterator (  )  . next (  )  ;", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "CSAssignment   assignment    =    a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  8     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    assignment . getExcessReservation (  )  . getContainer (  )  . getResource (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReservationExchange"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    spy ( new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "String   host _  0  _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "String   rack _  0     =     \" rack _  0  \"  ;", "FiCaSchedulerNode   node _  0  _  0     =    TestUtils . getMockNode ( host _  0  _  0  ,    rack _  0  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  0  _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "FiCaSchedulerNode   node _  0  _  1     =    TestUtils . getMockNode ( host _  0  _  1  ,    rack _  0  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1  _  0     =     \"  1  2  7  .  0  .  0  .  3  \"  ;", "String   rack _  1     =     \" rack _  1  \"  ;", "FiCaSchedulerNode   node _  1  _  0     =    TestUtils . getMockNode ( host _  1  _  0  ,    rack _  1  ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     3  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "List < ResourceRequest >    app _  0  _ requests _  0     =    new   ArrayList < ResourceRequest >  (  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  0  _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  0  _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( host _  1  _  0  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( rack _  1  ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "a . assignContainers ( clusterResource ,    node _  0  _  0  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . NODE _ LOCAL )  ,    eq ( node _  0  _  0  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  0  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  1  _  0  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "app _  0  _ requests _  0  . clear (  )  ;", "app _  0  _ requests _  0  . add ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "app _  0  . updateResourceRequests ( app _  0  _ requests _  0  )  ;", "a . assignContainers ( clusterResource ,    node _  0  _  1  )  ;", "verify ( app _  0  ,    never (  )  )  . allocate ( any ( NodeType . class )  ,    eq ( node _  1  _  0  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  1  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  1  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  _  0  )  ;", "verify ( app _  0  )  . allocate ( eq ( NodeType . NODE _ LOCAL )  ,    eq ( node _  1  _  0  )  ,    any ( Priority . class )  ,    any ( ResourceRequest . class )  ,    any ( Container . class )  )  ;", "assertEquals (  0  ,    app _  0  . getSchedulingOpportunities ( priority )  )  ;", "assertEquals (  0  ,    app _  0  . getTotalRequiredResources ( priority )  )  ;", "}", "METHOD_END"], "methodName": ["testSchedulingConstraints"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . B )  )  )  )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     1  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     3  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  (  ( int )     (  ( node _  0  . getTotalResource (  )  . getMemory (  )  )     *     ( a . getCapacity (  )  )  )  )     -     (  1     *     ( TestLeafQueue . GB )  )  )  ,    a . getMetrics (  )  . getAvailableMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleQueueOneUserMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "final   String   user _  2     =     \" user _  2  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  2     =    TestUtils . getMockApplicationAttemptId (  2  ,     0  )  ;", "FiCaSchedulerApp   app _  2     =    new   FiCaSchedulerApp ( appAttemptId _  2  ,    user _  1  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  2  ,    user _  1  )  ;", "final   ApplicationAttemptId   appAttemptId _  3     =    TestUtils . getMockApplicationAttemptId (  3  ,     0  )  ;", "FiCaSchedulerApp   app _  3     =    new   FiCaSchedulerApp ( appAttemptId _  3  ,    user _  2  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  3  ,    user _  2  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     1  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  0  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     1  0  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . setUserLimit (  2  5  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "app _  2  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  3     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  3  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . setUserLimitFactor (  1  0  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  5     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  6     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . setMaxCapacity (  0  .  5 F )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  6     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "a . setUserLimitFactor (  1  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  7     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  8     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "for    ( RMContainer   rmContainer    :    app _  0  . getLiveContainers (  )  )     {", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "}", "assertEquals (  (  5     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "for    ( RMContainer   rmContainer    :    app _  2  . getLiveContainers (  )  )     {", "a . completedContainer ( clusterResource ,    app _  2  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "}", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "for    ( RMContainer   rmContainer    :    app _  3  . getLiveContainers (  )  )     {", "a . completedContainer ( clusterResource ,    app _  3  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "}", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  2  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  3  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleQueueWithMultipleUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     1  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     3  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAvailableMB (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "a . setUserLimitFactor (  1  0  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "a . setMaxCapacity (  0  .  5 F )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "for    ( RMContainer   rmContainer    :    app _  0  . getLiveContainers (  )  )     {", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "}", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "for    ( RMContainer   rmContainer    :    app _  1  . getLiveContainers (  )  )     {", "a . completedContainer ( clusterResource ,    app _  1  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "}", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "assertEquals (  (  ( int )     (  ( a . getCapacity (  )  )     *     ( node _  0  . getTotalResource (  )  . getMemory (  )  )  )  )  ,    a . getMetrics (  )  . getAvailableMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleQueueWithOneUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  1  ,    a ,    mock ( ActiveUsersManager . class )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  1  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  4     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode ( host _  1  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  4     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     3  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  4     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  2     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "ArrayList < ResourceRequest >    appRequests _  1     =    new   ArrayList < ResourceRequest >  (  4  )  ;", "appRequests _  1  . add ( TestUtils . createResourceRequest ( host _  0  ,     (  4     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "appRequests _  1  . add ( TestUtils . createResourceRequest ( TestLeafQueue . DEFAULT _ RACK ,     (  4     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  ;", "appRequests _  1  . add ( TestUtils . createResourceRequest ( ANY ,     (  4     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  ;", "app _  1  . updateResourceRequests ( appRequests _  1  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAvailableMB (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  6     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "doReturn (  (  -  1  )  )  . when ( a )  . getNodeLocalityDelay (  )  ;", "a . assignContainers ( clusterResource ,    node _  1  )  ;", "assertEquals (  (  1  0     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    node _  1  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  6     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "RMContainer   rmContainer    =    app _  0  . getLiveContainers (  )  . iterator (  )  . next (  )  ;", "a . completedContainer ( clusterResource ,    app _  0  ,    node _  0  ,    rmContainer ,    ContainerStatus . newInstance ( rmContainer . getContainerId (  )  ,    COMPLETE ,     \"  \"  ,    KILLED _ BY _ RESOURCEMANAGER )  ,    RMContainerEventType . KILL ,    null )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  8     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  8     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    node _  0  . getUsedResource (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getReservedMB (  )  )  ;", "assertEquals (  (  8     *     ( TestLeafQueue . GB )  )  ,    a . getMetrics (  )  . getAllocatedMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStolenReservedContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   a    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . A )  )  )  )  ;", "a . setMaxCapacity (  1  .  0 F )  ;", "final   String   user _  0     =     \" user _  0  \"  ;", "final   String   user _  1     =     \" user _  1  \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     0  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _  0  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  0  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  1  ,     0  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _  0  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  1  ,    user _  0  )  ;", "final   ApplicationAttemptId   appAttemptId _  2     =    TestUtils . getMockApplicationAttemptId (  2  ,     0  )  ;", "FiCaSchedulerApp   app _  2     =    new   FiCaSchedulerApp ( appAttemptId _  2  ,    user _  1  ,    a ,    a . getActiveUsersManager (  )  ,    rmContext )  ;", "a . submitApplicationAttempt ( app _  2  ,    user _  1  )  ;", "String   host _  0     =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode ( host _  0  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "String   host _  1     =     \"  1  2  7  .  0  .  0  .  2  \"  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode ( host _  1  ,    TestLeafQueue . DEFAULT _ RACK ,     0  ,     (  8     *     ( TestLeafQueue . GB )  )  )  ;", "final   int   numNodes    =     2  ;", "Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     (  8     *     ( TestLeafQueue . GB )  )  )  ,     ( numNodes    *     1  6  )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "Priority   priority    =    TestUtils . createMockPriority (  1  )  ;", "app _  0  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  2     *     ( TestLeafQueue . GB )  )  ,     1  ,    true ,    priority ,    recordFactory )  )  )  ;", "app _  1  . updateResourceRequests ( Collections . singletonList ( TestUtils . createResourceRequest ( ANY ,     (  1     *     ( TestLeafQueue . GB )  )  ,     2  ,    true ,    priority ,    recordFactory )  )  )  ;", "a . setUserLimit (  5  0  )  ;", "a . setUserLimitFactor (  2  )  ;", "assertEquals (  \" There   should   only   be    1    active   user !  \"  ,     1  ,    a . getActiveUsersManager (  )  . getNumActiveUsers (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  0     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  0  )  ;", "assertEquals (  (  3     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  1     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "a . assignContainers ( clusterResource ,    node _  1  )  ;", "assertEquals (  (  4     *     ( TestLeafQueue . GB )  )  ,    a . getUsedResources (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  0  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "assertEquals (  (  2     *     ( TestLeafQueue . GB )  )  ,    app _  1  . getCurrentConsumption (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUserLimits"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "LeafQueue   d    =    TestLeafQueue . stubLeafQueue (  (  ( LeafQueue )     ( queues . get ( TestLeafQueue . D )  )  )  )  ;", "final   String   user _ d    =     \" user _ d \"  ;", "final   ApplicationAttemptId   appAttemptId _  0     =    TestUtils . getMockApplicationAttemptId (  0  ,     1  )  ;", "FiCaSchedulerApp   app _  0     =    new   FiCaSchedulerApp ( appAttemptId _  0  ,    user _ d ,    d ,    null ,    rmContext )  ;", "d . submitApplicationAttempt ( app _  0  ,    user _ d )  ;", "final   ApplicationAttemptId   appAttemptId _  1     =    TestUtils . getMockApplicationAttemptId (  0  ,     2  )  ;", "FiCaSchedulerApp   app _  1     =    new   FiCaSchedulerApp ( appAttemptId _  1  ,    user _ d ,    d ,    null ,    rmContext )  ;", "d . submitApplicationAttempt ( app _  1  ,    user _ d )  ;", "}", "METHOD_END"], "methodName": ["testUserQueueAcl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return    (  ( float )     ( expectedMemory )  )     /     (  ( float )     ( clusterResource . getMemory (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["computeQueueAbsoluteUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "return   expectedMemory    /     (  ( clusterResource . getMemory (  )  )     *     ( queue . getAbsoluteCapacity (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["computeQueueUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerApp   application    =    mock ( FiCaSchedulerApp . class )  ;", "doReturn ( user )  . when ( application )  . getUser (  )  ;", "doReturn ( Resources . createResource (  0  ,     0  )  )  . when ( application )  . getHeadroom (  )  ;", "return   application ;", "}", "METHOD_END"], "methodName": ["getMockApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "for    ( QueueUserACLInfo   aclInfo    :    aclInfos )     {", "if    ( aclInfo . geName (  )  . equals ( qName )  )     {", "if    ( aclInfo . getUserAcls (  )  . contains ( acl )  )     {", "return   true ;", "}", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["hasQueueACL"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "rmContext    =    TestUtils . getMockRMContext (  )  ;", "conf    =    new   YarnConfiguration (  )  ;", "csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "csContext    =    mock ( CapacitySchedulerContext . class )  ;", "when ( csContext . getConf (  )  )  . thenReturn ( conf )  ;", "when ( csContext . getConfiguration (  )  )  . thenReturn ( csConf )  ;", "when ( csContext . getMinimumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  . GB ,     1  )  )  ;", "when ( csContext . getMaximumResourceCapability (  )  )  . thenReturn ( Resources . createResource (  (  1  6     *     (  . GB )  )  ,     3  2  )  )  ;", "when ( csContext . getClusterResource (  )  )  . thenReturn ( Resources . createResource (  (  (  1  0  0     *     1  6  )     *     (  . GB )  )  ,     (  1  0  0     *     3  2  )  )  )  ;", "when ( csContext . getApplicationComparator (  )  )  . thenReturn ( CapacityScheduler . applicationComparator )  ;", "when ( csContext . getQueueComparator (  )  )  . thenReturn ( CapacityScheduler . queueComparator )  ;", "when ( csContext . getResourceCalculator (  )  )  . thenReturn ( resourceComparator )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "csConf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    TestParentQueue . A ,    TestParentQueue . B ,    TestParentQueue . C ,    TestParentQueue . D    }  )  ;", "final   String   Q _ A    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestParentQueue . A )  ;", "conf . setCapacity ( Q _ A ,     1  0  )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestParentQueue . B )  ;", "conf . setCapacity ( Q _ B ,     5  0  )  ;", "final   String   Q _ C    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestParentQueue . C )  ;", "conf . setCapacity ( Q _ C ,     1  9  .  5 F )  ;", "final   String   Q _ D    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestParentQueue . D )  ;", "conf . setCapacity ( Q _ D ,     2  0  .  5 F )  ;", "conf . setQueues ( Q _ A ,    new   String [  ]  {    TestParentQueue . A 1  ,    TestParentQueue . A 2     }  )  ;", "conf . setCapacity (  (  ( Q _ A    +     \"  .  \"  )     +     ( TestParentQueue . A 1  )  )  ,     5  0  )  ;", "conf . setCapacity (  (  ( Q _ A    +     \"  .  \"  )     +     ( TestParentQueue . A 2  )  )  ,     5  0  )  ;", "conf . setQueues ( Q _ B ,    new   String [  ]  {    TestParentQueue . B 1  ,    TestParentQueue . B 2  ,    TestParentQueue . B 3     }  )  ;", "conf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     ( TestParentQueue . B 1  )  )  ,     1  0  )  ;", "conf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     ( TestParentQueue . B 2  )  )  ,     2  0  )  ;", "conf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     ( TestParentQueue . B 3  )  )  ,     7  0  )  ;", "conf . setQueues ( Q _ C ,    new   String [  ]  {    TestParentQueue . C 1     }  )  ;", "final   String   Q _ C 1     =     ( Q _ C    +     \"  .  \"  )     +     ( TestParentQueue . C 1  )  ;", "conf . setCapacity ( Q _ C 1  ,     1  0  0  )  ;", "conf . setQueues ( Q _ C 1  ,    new   String [  ]  {    TestParentQueue . C 1  1     }  )  ;", "final   String   Q _ C 1  1     =     ( Q _ C 1     +     \"  .  \"  )     +     ( TestParentQueue . C 1  1  )  ;", "conf . setCapacity ( Q _ C 1  1  ,     1  0  0  )  ;", "conf . setQueues ( Q _ C 1  1  ,    new   String [  ]  {    TestParentQueue . C 1  1  1     }  )  ;", "final   String   Q _ C 1  1  1     =     ( Q _ C 1  1     +     \"  .  \"  )     +     ( TestParentQueue . C 1  1  1  )  ;", "conf . setCapacity ( Q _ C 1  1  1  ,     1  0  0  )  ;", "conf . setQueues ( Q _ C 1  1  1  ,    new   String [  ]  {    TestParentQueue . C 1  1  1  1     }  )  ;", "final   String   Q _ C 1  1  1  1     =     ( Q _ C 1  1  1     +     \"  .  \"  )     +     ( TestParentQueue . C 1  1  1  1  )  ;", "conf . setCapacity ( Q _ C 1  1  1  1  ,     1  0  0  )  ;", "}", "METHOD_END"], "methodName": ["setupMultiLevelQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    TestParentQueue . A ,    TestParentQueue . B    }  )  ;", "final   String   Q _ A    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestParentQueue . A )  ;", "conf . setCapacity ( Q _ A ,     3  0  )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( TestParentQueue . B )  ;", "conf . setCapacity ( Q _ B ,     7  0  )  ;", "TestParentQueue . LOG . info (  \" Setup   top - level   queues   a   and   b \"  )  ;", "}", "METHOD_END"], "methodName": ["setupSingleLevelQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "stubQueueAllocation ( queue ,    clusterResource ,    node ,    allocation ,    NodeType . NODE _ LOCAL )  ;", "}", "METHOD_END"], "methodName": ["stubQueueAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "doAnswer ( new   Answer < CSAssignment >  (  )     {", "@ Override", "public   CSAssignment   answer ( InvocationOnMock   invocation )    throws   Throwable    {", "try    {", "throw   new   Exception (  )  ;", "}    catch    ( Exception   e )     {", ". LOG . info (  (  (  (  (  (  \" FOOBAR   q . assignContainers   q =  \"     +     ( queue . getQueueName (  )  )  )     +     \"    alloc =  \"  )     +    allocation )     +     \"    node =  \"  )     +     ( node . getNodeName (  )  )  )  )  ;", "}", "final   Resource   allocatedResource    =    Resources . createResource ( allocation )  ;", "if    ( queue   instanceof   ParentQueue )     {", "(  ( ParentQueue )     ( queue )  )  . allocateResource ( clusterResource ,    allocatedResource )  ;", "} else    {", "FiCaSchedulerApp   app 1     =    getMockApplication (  0  ,     \"  \"  )  ;", "(  ( LeafQueue )     ( queue )  )  . allocateResource ( clusterResource ,    app 1  ,    allocatedResource )  ;", "}", "if    ( allocation    >     0  )     {", "doReturn ( new   CSAssignment ( Resources . none (  )  ,    type )  )  . when ( queue )  . assignContainers ( eq ( clusterResource )  ,    eq ( node )  )  ;", "Resource   available    =    node . getAvailableResource (  )  ;", "doReturn ( Resources . subtractFrom ( available ,    allocatedResource )  )  . when ( node )  . getAvailableResource (  )  ;", "}", "return   new   CSAssignment ( allocatedResource ,    type )  ;", "}", "}  )  . when ( queue )  . assignContainers ( eq ( clusterResource )  ,    eq ( node )  )  ;", "}", "METHOD_END"], "methodName": ["stubQueueAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupMultiLevelQueues ( csConf )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "final   int   memoryPerNode    =     1  0  ;", "final   int   coresPerNode    =     1  6  ;", "final   int   numNodes    =     3  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode (  \" host _  1  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "FiCaSchedulerNode   node _  2     =    TestUtils . getMockNode (  \" host _  2  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "final   Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     ( memoryPerNode    *     (  . GB )  )  )  ,     ( numNodes    *    coresPerNode )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "CSQueue   a    =    queues . get (  . A )  ;", "CSQueue   b    =    queues . get (  . B )  ;", "CSQueue   c    =    queues . get (  . C )  ;", "CSQueue   d    =    queues . get (  . D )  ;", "CSQueue   a 1     =    queues . get (  . A 1  )  ;", "CSQueue   a 2     =    queues . get (  . A 2  )  ;", "CSQueue   b 1     =    queues . get (  . B 1  )  ;", "CSQueue   b 2     =    queues . get (  . B 2  )  ;", "CSQueue   b 3     =    queues . get (  . B 3  )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( d ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "verifyQueueMetrics ( a ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( d ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "reset ( a )  ;", "reset ( b )  ;", "reset ( c )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  1  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b 2  ,    clusterResource ,    node _  1  ,     (  4     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  1  ,     (  0     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  1  )  ;", "verifyQueueMetrics ( a ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  4     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "reset ( a )  ;", "reset ( b )  ;", "reset ( c )  ;", "stubQueueAllocation ( a 1  ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b 3  ,    clusterResource ,    node _  0  ,     (  2     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  0  ,     (  2     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "InOrder   allocationOrder    =    inOrder ( a ,    c ,    b )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( c )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  6     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "reset ( a )  ;", "reset ( b )  ;", "reset ( c )  ;", ". LOG . info (  \" here \"  )  ;", "(  ( ParentQueue )     ( a )  )  . setMaxCapacity (  0  .  1 F )  ;", "stubQueueAllocation ( a 1  ,    clusterResource ,    node _  2  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( a 2  ,    clusterResource ,    node _  2  ,     (  2     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b 3  ,    clusterResource ,    node _  2  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b 1  ,    clusterResource ,    node _  2  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( c ,    clusterResource ,    node _  2  ,     (  1     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  2  )  ;", "allocationOrder    =    inOrder ( a ,    a 2  ,    a 1  ,    b ,    c )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( a 2  )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( c )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  8     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( c ,     (  4     *     (  . GB )  )  ,    clusterResource )  ;", "reset ( a )  ;", "reset ( b )  ;", "reset ( c )  ;", "}", "METHOD_END"], "methodName": ["testMultiLevelQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupSingleLevelQueues ( csConf )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "final   int   memoryPerNode    =     1  0  ;", "final   int   coresPerNode    =     1  6  ;", "final   int   numNodes    =     2  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode (  \" host _  1  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "final   Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     ( memoryPerNode    *     (  . GB )  )  )  ,     ( numNodes    *    coresPerNode )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "LeafQueue   a    =     (  ( LeafQueue )     ( queues . get (  . A )  )  )  ;", "LeafQueue   b    =     (  ( LeafQueue )     ( queues . get (  . B )  )  )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "verifyQueueMetrics ( a ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  1  ,     (  2     *     (  . GB )  )  ,    NodeType . RACK _ LOCAL )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  1  ,     (  1     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "root . assignContainers ( clusterResource ,    node _  1  )  ;", "InOrder   allocationOrder    =    inOrder ( a ,    b )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  ,    NodeType . NODE _ LOCAL )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  2     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "allocationOrder    =    inOrder ( b ,    a )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  4     *     (  . GB )  )  ,    clusterResource )  ;", "}", "METHOD_END"], "methodName": ["testOffSwitchScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupMultiLevelQueues ( csConf )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "final   int   memoryPerNode    =     1  0  ;", "final   int   coresPerNode    =     1  0  ;", "final   int   numNodes    =     2  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode (  \" host _  1  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "final   Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     ( memoryPerNode    *     (  . GB )  )  )  ,     ( numNodes    *    coresPerNode )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "LeafQueue   b 3     =     (  ( LeafQueue )     ( queues . get (  . B 3  )  )  )  ;", "LeafQueue   b 2     =     (  ( LeafQueue )     ( queues . get (  . B 2  )  )  )  ;", "stubQueueAllocation ( b 2  ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "stubQueueAllocation ( b 3  ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "verifyQueueMetrics ( b 2  ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b 3  ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( b 2  ,    clusterResource ,    node _  1  ,     (  1     *     (  . GB )  )  ,    NodeType . RACK _ LOCAL )  ;", "stubQueueAllocation ( b 3  ,    clusterResource ,    node _  1  ,     (  1     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "root . assignContainers ( clusterResource ,    node _  1  )  ;", "InOrder   allocationOrder    =    inOrder ( b 2  ,    b 3  )  ;", "allocationOrder . verify ( b 2  )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b 3  )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( b 2  ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b 3  ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( b 2  ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  ,    NodeType . NODE _ LOCAL )  ;", "stubQueueAllocation ( b 3  ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  ,    NodeType . OFF _ SWITCH )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "allocationOrder    =    inOrder ( b 3  ,    b 2  )  ;", "allocationOrder . verify ( b 3  )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b 2  )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( b 2  ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b 3  ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "}", "METHOD_END"], "methodName": ["testOffSwitchSchedulingMultiLevelQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupMultiLevelQueues ( csConf )  ;", "csConf . setAcl ( CapacitySchedulerConfiguration . ROOT ,    SUBMIT _ APPLICATIONS ,     \"     \"  )  ;", "csConf . setAcl ( CapacitySchedulerConfiguration . ROOT ,    ADMINISTER _ QUEUE ,     \"     \"  )  ;", "final   String   Q _ C    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . C )  ;", "csConf . setAcl ( Q _ C ,    ADMINISTER _ QUEUE ,     \"  *  \"  )  ;", "final   String   Q _ C 1  1     =     (  (  ( Q _ C    +     \"  .  \"  )     +     (  . C 1  )  )     +     \"  .  \"  )     +     (  . C 1  1  )  ;", "csConf . setAcl ( Q _ C 1  1  ,    SUBMIT _ APPLICATIONS ,     \"  *  \"  )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "UserGroupInformation   user    =    UserGroupInformation . getCurrentUser (  )  ;", "ParentQueue   c    =     (  ( ParentQueue )     ( queues . get (  . C )  )  )  ;", "ParentQueue   c 1     =     (  ( ParentQueue )     ( queues . get (  . C 1  )  )  )  ;", "ParentQueue   c 1  1     =     (  ( ParentQueue )     ( queues . get (  . C 1  1  )  )  )  ;", "ParentQueue   c 1  1  1     =     (  ( ParentQueue )     ( queues . get (  . C 1  1  1  )  )  )  ;", "assertFalse ( root . hasAccess ( ADMINISTER _ QUEUE ,    user )  )  ;", "List < QueueUserACLInfo >    aclInfos    =    root . getQueueUserAclInfo ( user )  ;", "assertFalse ( hasQueueACL ( aclInfos ,    ADMINISTER _ QUEUE ,     \" root \"  )  )  ;", "assertFalse ( root . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertFalse ( hasQueueACL ( aclInfos ,    SUBMIT _ APPLICATIONS ,     \" root \"  )  )  ;", "assertTrue ( c . hasAccess ( ADMINISTER _ QUEUE ,    user )  )  ;", "assertTrue ( hasQueueACL ( aclInfos ,    ADMINISTER _ QUEUE ,     \" c \"  )  )  ;", "assertFalse ( c . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertFalse ( hasQueueACL ( aclInfos ,    SUBMIT _ APPLICATIONS ,     \" c \"  )  )  ;", "assertTrue ( c 1  . hasAccess ( ADMINISTER _ QUEUE ,    user )  )  ;", "assertTrue ( hasQueueACL ( aclInfos ,    ADMINISTER _ QUEUE ,     \" c 1  \"  )  )  ;", "assertFalse ( c 1  . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertFalse ( hasQueueACL ( aclInfos ,    SUBMIT _ APPLICATIONS ,     \" c 1  \"  )  )  ;", "assertTrue ( c 1  1  . hasAccess ( ADMINISTER _ QUEUE ,    user )  )  ;", "assertTrue ( hasQueueACL ( aclInfos ,    ADMINISTER _ QUEUE ,     \" c 1  1  \"  )  )  ;", "assertTrue ( c 1  1  . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertTrue ( hasQueueACL ( aclInfos ,    SUBMIT _ APPLICATIONS ,     \" c 1  1  \"  )  )  ;", "assertTrue ( c 1  1  1  . hasAccess ( ADMINISTER _ QUEUE ,    user )  )  ;", "assertTrue ( hasQueueACL ( aclInfos ,    ADMINISTER _ QUEUE ,     \" c 1  1  1  \"  )  )  ;", "assertTrue ( c 1  1  1  . hasAccess ( SUBMIT _ APPLICATIONS ,    user )  )  ;", "assertTrue ( hasQueueACL ( aclInfos ,    SUBMIT _ APPLICATIONS ,     \" c 1  1  1  \"  )  )  ;", "reset ( c )  ;", "}", "METHOD_END"], "methodName": ["testQueueAcl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupMultiLevelQueues ( csConf )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . B )  ;", "csConf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     (  . B 1  )  )  ,     0  )  ;", "csConf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     (  . B 2  )  )  ,     0  )  ;", "csConf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     (  . B 3  )  )  ,     0  )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "}", "METHOD_END"], "methodName": ["testQueueCapacitySettingChildZero"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupMultiLevelQueues ( csConf )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . B )  ;", "csConf . setCapacity ( Q _ B ,     0  )  ;", "final   String   Q _ A    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . A )  ;", "csConf . setCapacity ( Q _ A ,     6  0  )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "}", "METHOD_END"], "methodName": ["testQueueCapacitySettingParentZero"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupMultiLevelQueues ( csConf )  ;", "final   String   Q _ B    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . B )  ;", "csConf . setCapacity ( Q _ B ,     0  )  ;", "csConf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     (  . B 1  )  )  ,     0  )  ;", "csConf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     (  . B 2  )  )  ,     0  )  ;", "csConf . setCapacity (  (  ( Q _ B    +     \"  .  \"  )     +     (  . B 3  )  )  ,     0  )  ;", "final   String   Q _ A    =     (  ( CapacitySchedulerConfiguration . ROOT )     +     \"  .  \"  )     +     (  . A )  ;", "csConf . setCapacity ( Q _ A ,     6  0  )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "try    {", "CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "}    catch    ( IllegalArgumentException   e )     {", "fail (  (  \" Failed   to   create   queues   with    0    capacity :     \"     +    e )  )  ;", "}", "assertTrue (  \" Failed   to   create   queues   with    0    capacity \"  ,    true )  ;", "}", "METHOD_END"], "methodName": ["testQueueCapacityZero"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupSingleLevelQueues ( csConf )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "CSQueue   root    =    CapacityScheduler . parseQueue ( csContext ,    csConf ,    null ,    CapacitySchedulerConfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "final   int   memoryPerNode    =     1  0  ;", "final   int   coresPerNode    =     1  6  ;", "final   int   numNodes    =     2  ;", "FiCaSchedulerNode   node _  0     =    TestUtils . getMockNode (  \" host _  0  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "FiCaSchedulerNode   node _  1     =    TestUtils . getMockNode (  \" host _  1  \"  ,     . DEFAULT _ RACK ,     0  ,     ( memoryPerNode    *     (  . GB )  )  )  ;", "final   Resource   clusterResource    =    Resources . createResource (  ( numNodes    *     ( memoryPerNode    *     (  . GB )  )  )  ,     ( numNodes    *    coresPerNode )  )  ;", "when ( csContext . getNumClusterNodes (  )  )  . thenReturn ( numNodes )  ;", "LeafQueue   a    =     (  ( LeafQueue )     ( queues . get (  . A )  )  )  ;", "LeafQueue   b    =     (  ( LeafQueue )     ( queues . get (  . B )  )  )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "verifyQueueMetrics ( a ,     (  0     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  1     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  1  ,     (  2     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  1  ,     (  1     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  1  )  ;", "InOrder   allocationOrder    =    inOrder ( a ,    b )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  2     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  2     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "allocationOrder    =    inOrder ( b ,    a )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  4     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  0  ,     (  0     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  0  ,     (  4     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  0  )  ;", "allocationOrder    =    inOrder ( b ,    a )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  3     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  8     *     (  . GB )  )  ,    clusterResource )  ;", "stubQueueAllocation ( a ,    clusterResource ,    node _  1  ,     (  1     *     (  . GB )  )  )  ;", "stubQueueAllocation ( b ,    clusterResource ,    node _  1  ,     (  1     *     (  . GB )  )  )  ;", "root . assignContainers ( clusterResource ,    node _  1  )  ;", "allocationOrder    =    inOrder ( a ,    b )  ;", "allocationOrder . verify ( b )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "allocationOrder . verify ( a )  . assignContainers ( eq ( clusterResource )  ,    any ( FiCaSchedulerNode . class )  )  ;", "verifyQueueMetrics ( a ,     (  4     *     (  . GB )  )  ,    clusterResource )  ;", "verifyQueueMetrics ( b ,     (  9     *     (  . GB )  )  ,    clusterResource )  ;", "}", "METHOD_END"], "methodName": ["testSingleLevelQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "setupSingleLevelQueues ( csConf )  ;", "final   String   Q _ A    =     (  ( CapacitySonfiguration . ROOT )     +     \"  .  \"  )     +     \" a \"  ;", "csConf . setCapacity ( Q _ A ,     3  0  )  ;", "final   String   Q _ B    =     (  ( CapacitySonfiguration . ROOT )     +     \"  .  \"  )     +     \" b \"  ;", "csConf . setCapacity ( Q _ B ,     7  0  .  5 F )  ;", "Map < String ,    CSQueue >    queues    =    new   HashMap < String ,    CSQueue >  (  )  ;", "boolean   exceptionOccured    =    false ;", "try    {", "CapacitySparseQueue ( csContext ,    csConf ,    null ,    CapacitySonfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "}    catch    ( IllegalArgumentException   ie )     {", "exceptionOccured    =    true ;", "}", "if    (  ! exceptionOccured )     {", "Assert . fail (  \" Capacity   is   more   then    1  0  0  %    so   should   be   failed .  \"  )  ;", "}", "csConf . setCapacity ( Q _ A ,     3  0  )  ;", "csConf . setCapacity ( Q _ B ,     7  0  )  ;", "exceptionOccured    =    false ;", "queues . clear (  )  ;", "try    {", "CapacitySparseQueue ( csContext ,    csConf ,    null ,    CapacitySonfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "}    catch    ( IllegalArgumentException   ie )     {", "exceptionOccured    =    true ;", "}", "if    ( exceptionOccured )     {", "Assert . fail (  \" Capacity   is    1  0  0  %    so   should   not   be   failed .  \"  )  ;", "}", "csConf . setCapacity ( Q _ A ,     3  0  )  ;", "csConf . setCapacity ( Q _ B ,     7  0  .  0  0  5 F )  ;", "exceptionOccured    =    false ;", "queues . clear (  )  ;", "try    {", "CapacitySparseQueue ( csContext ,    csConf ,    null ,    CapacitySonfiguration . ROOT ,    queues ,    queues ,    TestUtils . spyHook )  ;", "}    catch    ( IllegalArgumentException   ie )     {", "exceptionOccured    =    true ;", "}", "if    ( exceptionOccured )     {", "Assert . fail (  \" Capacity   is   under   PRECISION   which   is    .  0  5  %    so   should   not   be   failed .  \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSingleLevelQueuesPrecision"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( computeQueueAbsoluteUsedCapacity ( queue ,    expectedMemory ,    clusterResource )  ,    queue . getAbsoluteUsedCapacity (  )  ,    TestParentQueue . DELTA )  ;", "assertEquals ( computeQueueUsedCapacity ( queue ,    expectedMemory ,    clusterResource )  ,    queue . getUsedCapacity (  )  ,    TestParentQueue . DELTA )  ;", "}", "METHOD_END"], "methodName": ["verifyQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestParentQueue"}, {"methodBody": ["METHOD_START", "{", "RMApp   app    =    resourceManager . submitApp (  2  0  0  ,     \" name \"  ,    user ,    new   HashMap < ApplicationAccessType ,    String >  (  )  ,    false ,    submissionQueue ,     (  -  1  )  ,    null ,     \" MAPREDUCE \"  ,    false )  ;", "RMAppState   expectedState    =     ( expected . isEmpty (  )  )     ?    RMAppState . FAILED    :    RMAppState . ACCEPTED ;", "resourceManager . waitForState ( app . getApplicationId (  )  ,    expectedState )  ;", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( resourceManager . getResourceScheduler (  )  )  )  ;", "SchedulerApplication   schedulerApp    =    cs . getSchedulerApplications (  )  . get ( app . getApplicationId (  )  )  ;", "String   queue    =     \"  \"  ;", "if    ( schedulerApp    !  =    null )     {", "queue    =    schedulerApp . getQueue (  )  . getQueueName (  )  ;", "}", "Assert . assertTrue (  (  (  (  \" expected    \"     +    expected )     +     \"    actual    \"  )     +    queue )  ,    expected . equals ( queue )  )  ;", "Assert . assertEquals ( expected ,    app . getQueue (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkAppQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueMappings"}, {"methodBody": ["METHOD_START", "{", "boolean   fail    =    false ;", "try    {", "conf . set ( CapacitySConfiguration . QUEUE _ MAPPING ,    mapping )  ;", "cs . reinitialize ( conf ,    null )  ;", "}    catch    ( IOException   ex )     {", "fail    =    true ;", "}", "Assert . assertTrue (  (  \" invalid   mapping   did   not   throw   exception   for    \"     +    reason )  ,    fail )  ;", "}", "METHOD_END"], "methodName": ["checkInvalidQMapping"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueMappings"}, {"methodBody": ["METHOD_START", "{", "String   actual    =    cs . getMappedQueueForTest ( user )  ;", "Assert . assertTrue (  (  (  (  \" expected    \"     +    expected )     +     \"    actual    \"  )     +    actual )  ,    expected . equals ( actual )  )  ;", "}", "METHOD_END"], "methodName": ["checkQMapping"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueMappings"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {    TestQueueMappings . Q 1  ,    TestQueueMappings . Q 2     }  )  ;", "conf . setCapacity ( TestQueueMappings . Q 1  _ PATH ,     1  0  )  ;", "conf . setCapacity ( TestQueueMappings . Q 2  _ PATH ,     9  0  )  ;", "TestQueueMappings . LOG . info (  \" Setup   top - level   queues   q 1    and   q 2  \"  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueMappings"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManager )     !  =    null )     {", ". LOG . info (  \" Stopping   the   resource   manager \"  )  ;", "resourceManager . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueMappings"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration ( csConf )  ;", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "RMContextImpl   rmContext    =    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  ;", "cs . setConf ( conf )  ;", "cs . setRMContext ( rmContext )  ;", "cs . init ( conf )  ;", "cs . start (  )  ;", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    SimpleGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "conf . set ( CapacitySchedulerConfiguration . ENABLE _ QUEUE _ MAPPING _ OVERRIDE ,     \" true \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" x : a : b \"  ,     \" invalid   specifier \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" u : a \"  ,     \" no   queue   specified \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" g : a \"  ,     \" no   queue   specified \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" u : a : b , g : a \"  ,     \" multiple   mappings   with   invalid   mapping \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" u : a : b , g : a : d : e \"  ,     \" too   many   path   segments \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" u :  :  \"  ,     \" empty   source   and   queue \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" u :  \"  ,     \" missing   source   missing   queue \"  )  ;", "checkInvalidQMapping ( conf ,    cs ,     \" u : a :  \"  ,     \" empty   source   missing   q \"  )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" u : a :  \"     +     (  . Q 1  )  )  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     . Q 1  ,    cs )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" g : agroup :  \"     +     (  . Q 1  )  )  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     . Q 1  ,    cs )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" u :  % user :  \"     +     (  . Q 2  )  )  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     . Q 2  ,    cs )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     \" u :  % user :  % user \"  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     \" a \"  ,    cs )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     \" u :  % user :  % primary _ group \"  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     \" agroup \"  ,    cs )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" g : asubgroup 1  :  \"     +     (  . Q 1  )  )  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     . Q 1  ,    cs )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \"             u    :    a    :     \"     +     (  . Q 1  )  )  )  ;", "cs . reinitialize ( conf ,    null )  ;", "checkQMapping (  \" a \"  ,     . Q 1  ,    cs )  ;", "csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "conf    =    new   YarnConfiguration ( csConf )  ;", "resourceManager    =    new   MockRM ( csConf )  ;", "resourceManager . start (  )  ;", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    SimpleGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "conf . set ( CapacitySchedulerConfiguration . ENABLE _ QUEUE _ MAPPING _ OVERRIDE ,     \" true \"  )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" u : user :  \"     +     (  . Q 1  )  )  )  ;", "resourceManager . getResourceScheduler (  )  . reinitialize ( conf ,    null )  ;", "checkAppQueue ( resourceManager ,     \" user \"  ,     . Q 2  ,     . Q 1  )  ;", "conf . setBoolean ( CapacitySchedulerConfiguration . ENABLE _ QUEUE _ MAPPING _ OVERRIDE ,    false )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" u : user :  \"     +     (  . Q 1  )  )  )  ;", "setupQueueConfiguration ( csConf )  ;", "resourceManager . getResourceScheduler (  )  . reinitialize ( conf ,    null )  ;", "checkAppQueue ( resourceManager ,     \" user \"  ,     . Q 2  ,     . Q 2  )  ;", "checkAppQueue ( resourceManager ,     \" user \"  ,    null ,     . Q 1  )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     (  \" g : usergroup :  \"     +     (  . Q 2  )  )  )  ;", "setupQueueConfiguration ( csConf )  ;", "resourceManager . getResourceScheduler (  )  . reinitialize ( conf ,    null )  ;", "checkAppQueue ( resourceManager ,     \" user \"  ,    null ,     . Q 2  )  ;", "conf . set ( CapacitySchedulerConfiguration . QUEUE _ MAPPING ,     \" u : user : non _ existent _ queue \"  )  ;", "setupQueueConfiguration ( csConf )  ;", "boolean   fail    =    false ;", "try    {", "resourceManager . getResourceScheduler (  )  . reinitialize ( conf ,    null )  ;", "}    catch    ( IOException   ioex )     {", "fail    =    true ;", "}", "Assert . assertTrue (  \" queue   initialization   failed   for   non - existent   q \"  ,    fail )  ;", "resourceManager . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testQueueMapping"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueMappings"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {     \" a \"  ,     \" b \"  ,     \" c \"     }  )  ;", "final   String   A    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . a \"  ;", "conf . setCapacity ( A ,     1  0  )  ;", "conf . setMaximumCapacity ( A ,     1  5  )  ;", "final   String   B    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . b \"  ;", "conf . setCapacity ( B ,     2  0  )  ;", "final   String   C    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . c \"  ;", "conf . setCapacity ( C ,     7  0  )  ;", "conf . setMaximumCapacity ( C ,     7  0  )  ;", ". LOG . info (  \" Setup   top - level   queues \"  )  ;", "final   String   A 1     =    A    +     \"  . a 1  \"  ;", "final   String   A 2     =    A    +     \"  . a 2  \"  ;", "conf . setQueues ( A ,    new   String [  ]  {     \" a 1  \"  ,     \" a 2  \"     }  )  ;", "conf . setCapacity ( A 1  ,     3  0  )  ;", "conf . setMaximumCapacity ( A 1  ,     4  5  )  ;", "conf . setCapacity ( A 2  ,     7  0  )  ;", "conf . setMaximumCapacity ( A 2  ,     8  5  )  ;", "final   String   B 1     =    B    +     \"  . b 1  \"  ;", "final   String   B 2     =    B    +     \"  . b 2  \"  ;", "final   String   B 3     =    B    +     \"  . b 3  \"  ;", "conf . setQueues ( B ,    new   String [  ]  {     \" b 1  \"  ,     \" b 2  \"  ,     \" b 3  \"     }  )  ;", "conf . setCapacity ( B 1  ,     5  0  )  ;", "conf . setMaximumCapacity ( B 1  ,     8  5  )  ;", "conf . setCapacity ( B 2  ,     3  0  )  ;", "conf . setMaximumCapacity ( B 2  ,     3  5  )  ;", "conf . setCapacity ( B 3  ,     2  0  )  ;", "conf . setMaximumCapacity ( B 3  ,     3  5  )  ;", "final   String   C 1     =    C    +     \"  . c 1  \"  ;", "final   String   C 2     =    C    +     \"  . c 2  \"  ;", "final   String   C 3     =    C    +     \"  . c 3  \"  ;", "final   String   C 4     =    C    +     \"  . c 4  \"  ;", "conf . setQueues ( C ,    new   String [  ]  {     \" c 1  \"  ,     \" c 2  \"  ,     \" c 3  \"  ,     \" c 4  \"     }  )  ;", "conf . setCapacity ( C 1  ,     5  0  )  ;", "conf . setMaximumCapacity ( C 1  ,     5  5  )  ;", "conf . setCapacity ( C 2  ,     1  0  )  ;", "conf . setMaximumCapacity ( C 2  ,     2  5  )  ;", "conf . setCapacity ( C 3  ,     3  5  )  ;", "conf . setMaximumCapacity ( C 3  ,     3  8  )  ;", "conf . setCapacity ( C 4  ,     5  )  ;", "conf . setMaximumCapacity ( C 4  ,     5  )  ;", ". LOG . info (  \" Setup    2 nd - level   queues \"  )  ;", "final   String   C 1  1     =    C 1     +     \"  . c 1  1  \"  ;", "final   String   C 1  2     =    C 1     +     \"  . c 1  2  \"  ;", "final   String   C 1  3     =    C 1     +     \"  . c 1  3  \"  ;", "conf . setQueues ( C 1  ,    new   String [  ]  {     \" c 1  1  \"  ,     \" c 1  2  \"  ,     \" c 1  3  \"     }  )  ;", "conf . setCapacity ( C 1  1  ,     1  5  )  ;", "conf . setMaximumCapacity ( C 1  1  ,     3  0  )  ;", "conf . setCapacity ( C 1  2  ,     4  5  )  ;", "conf . setMaximumCapacity ( C 1  2  ,     7  0  )  ;", "conf . setCapacity ( C 1  3  ,     4  0  )  ;", "conf . setMaximumCapacity ( C 1  3  ,     4  0  )  ;", ". LOG . info (  \" Setup    3 rd - level   queues \"  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing"}, {"methodBody": ["METHOD_START", "{", "CapacitySonfiguration   conf    =    new   CapacitySonfiguration (  )  ;", "conf . setQueues ( CapacitySonfiguration . ROOT ,    new   String [  ]  {     \" a \"  ,     \" b \"  ,     \" c \"     }  )  ;", "final   String   A    =     ( CapacitySonfiguration . ROOT )     +     \"  . a \"  ;", "conf . setCapacity ( A ,     5  0  )  ;", "conf . setMaximumCapacity ( A ,     6  0  )  ;", "final   String   B    =     ( CapacitySonfiguration . ROOT )     +     \"  . b \"  ;", "conf . setCapacity ( B ,     5  0  )  ;", "conf . setMaximumCapacity ( B ,     4  5  )  ;", "boolean   fail    =    false ;", "CapacityScapacityS", "try    {", "capacityS =    new   CapacityS )  ;", "capacitySsetConf ( new   YarnConfiguration (  )  )  ;", "capacitySinit ( conf )  ;", "capacitySstart (  )  ;", "capacitySreinitialize ( conf ,    null )  ;", "}    catch    ( IllegalArgumentException   iae )     {", "fail    =    true ;", "}", "Assert . assertTrue (  \" Didn ' t   throw   IllegalArgumentException   for   wrong   maxCap \"  ,    fail )  ;", "conf . setMaximumCapacity ( B ,     6  0  )  ;", "capacityS =    new   CapacityS )  ;", "capacitySsetConf ( new   YarnConfiguration (  )  )  ;", "capacitySinit ( conf )  ;", "capacitySstart (  )  ;", "capacitySreinitialize ( conf ,    null )  ;", "fail    =    false ;", "try    {", "LeafQueue   a    =     (  ( LeafQueue )     ( capacitySgetQueue ( A )  )  )  ;", "a . setMaxCapacity (  4  5  )  ;", "}    catch    ( IllegalArgumentException   iae )     {", "fail    =    true ;", "}", "Assert . assertTrue (  (  \" Didn ' t   throw   IllegalArgumentException   for   wrong    \"     +     \" setMaxCap \"  )  ,    fail )  ;", "capacitySstop (  )  ;", "}", "METHOD_END"], "methodName": ["testMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   csConf    =    new   CapacitySchedulerConfiguration (  )  ;", "setupQueueConfiguration ( csConf )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration ( csConf )  ;", "CapacityScheduler   capacityScheduler    =    new   CapacityScheduler (  )  ;", "RMContextImpl   rmContext    =    new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  ;", "capacityScheduler . setConf ( conf )  ;", "capacityScheduler . setRMContext ( rmContext )  ;", "capacityScheduler . init ( conf )  ;", "capacityScheduler . start (  )  ;", "capacityScheduler . reinitialize ( conf ,    rmContext )  ;", "CSQueue   a    =    capacityScheduler . getQueue (  \" a \"  )  ;", "Assert . assertEquals (  0  .  1  ,    a . getAbsoluteCapacity (  )  ,     . DELTA )  ;", "Assert . assertEquals (  0  .  1  5  ,    a . getAbsoluteMaximumCapacity (  )  ,     . DELTA )  ;", "CSQueue   b 1     =    capacityScheduler . getQueue (  \" b 1  \"  )  ;", "Assert . assertEquals (  (  0  .  2     *     0  .  5  )  ,    b 1  . getAbsoluteCapacity (  )  ,     . DELTA )  ;", "Assert . assertEquals (  \" Parent   B   has   no   MAX _ CAP \"  ,     0  .  8  5  ,    b 1  . getAbsoluteMaximumCapacity (  )  ,     . DELTA )  ;", "CSQueue   c 1  2     =    capacityScheduler . getQueue (  \" c 1  2  \"  )  ;", "Assert . assertEquals (  (  (  0  .  7     *     0  .  5  )     *     0  .  4  5  )  ,    c 1  2  . getAbsoluteCapacity (  )  ,     . DELTA )  ;", "Assert . assertEquals (  (  (  0  .  7     *     0  .  5  5  )     *     0  .  7  )  ,    c 1  2  . getAbsoluteMaximumCapacity (  )  ,     . DELTA )  ;", "capacityScheduler . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testQueueParsing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing"}, {"methodBody": ["METHOD_START", "{", "CapacitySonfiguration   conf    =    new   CapacitySonfiguration (  )  ;", "conf . setCapacity ( CapacitySonfiguration . ROOT ,     9  0  )  ;", "CapacityScapacityS =    new   CapacityS )  ;", "capacitySsetConf ( new   YarnConfiguration (  )  )  ;", "capacitySinit ( conf )  ;", "capacitySstart (  )  ;", "capacitySreinitialize ( conf ,    null )  ;", "capacitySstop (  )  ;", "}", "METHOD_END"], "methodName": ["testRootQueueParsing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestQueueParsing"}, {"methodBody": ["METHOD_START", "{", "Priority   p    =    TestUtils . recordFactory . newRecordInstance ( Priority . class )  ;", "p . setPriority ( priority )  ;", "return   p ;", "}", "METHOD_END"], "methodName": ["createMockPriority"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    recordFactory . newRecordInstance ( ResourceRequest . class )  ;", "Resource   capability    =    Resources . createResource ( memory ,     1  )  ;", "request . setNumContainers ( numContainers )  ;", "request . setResourceName ( Name )  ;", "request . setCapability ( capability )  ;", "request . setRelaxLocality ( relaxLocality )  ;", "request . setPriority ( priority )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["createResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    BuilderUtils . newApplicationId (  0 L ,    appId )  ;", "ApplicationAttemptId   applicationAttemptId    =    mock ( ApplicationAttemptId . class )  ;", "when ( applicationAttemptId . getApplicationId (  )  )  . thenReturn ( applicationId )  ;", "when ( applicationAttemptId . getAttemptId (  )  )  . thenReturn ( attemptId )  ;", "return   applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getMockApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    mock ( ApplicationId . class )  ;", "when ( applicationId . getClusterTimamp (  )  )  . thenReturn (  0 L )  ;", "when ( applicationId . getId (  )  )  . thenReturn ( appId )  ;", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getMockApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    mock ( Container . class )  ;", "when ( container . getId (  )  )  . thenReturn ( containerId )  ;", "when ( container . getNodeId (  )  )  . thenReturn ( nodeId )  ;", "when ( container . getResource (  )  )  . thenReturn (  )  ;", "when ( container . getPriority (  )  )  . thenReturn ( priority )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["getMockContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    mock ( ContainerId . class )  ;", "doReturn ( application . getApplicationAttemptId (  )  )  . when ( containerId )  . getApplicationAttemptId (  )  ;", "doReturn ( application . getNewContainerId (  )  )  . when ( containerId )  . getId (  )  ;", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getMockContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId    =    mock ( NodeId . class )  ;", "when ( nodeId . getHost (  )  )  . thenReturn ( host )  ;", "when ( nodeId . getPort (  )  )  . thenReturn ( port )  ;", "RMNode   rmNode    =    mock ( RMNode . class )  ;", "when ( rmNode . getNodeID (  )  )  . thenReturn ( nodeId )  ;", "when ( rmNode . getTotalCapability (  )  )  . thenReturn ( Resources . createResource ( capability ,     1  )  )  ;", "when ( rmNode . getNodeAddress (  )  )  . thenReturn (  (  ( host    +     \"  :  \"  )     +    port )  )  ;", "when ( rmNode . getHostName (  )  )  . thenReturn ( host )  ;", "when ( rmNode . getRackName (  )  )  . thenReturn ( rack )  ;", "FiCaSchedulerNode   node    =    spy ( new   FiCaSchedulerNode ( rmNode ,    false )  )  ;", ". LOG . info (  (  (  (  \" node    =     \"     +    host )     +     \"    avail =  \"  )     +     ( node . getAvailableResource (  )  )  )  )  ;", "return   node ;", "}", "METHOD_END"], "methodName": ["getMockNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "Dispatcher   nullDispatcher    =    new   Dispatcher (  )     {", "private   final   EventHandler   handler    =    new   EventHandler (  )     {", "@ Override", "public   void   handle ( Event   event )     {", "}", "}  ;", "@ Override", "public   void   register ( Class <  ?    extends   Enum >    eventType ,    EventHandler   handler )     {", "}", "@ Override", "public   EventHandler   getEventHandler (  )     {", "return   handler ;", "}", "}  ;", "ContainerAllocationExpirer   cae    =    new   ContainerAllocationExpirer ( nullDispatcher )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContextImpl   rmContext    =    new   RMContextImpl ( nullDispatcher ,    cae ,    null ,    null ,    null ,    new   AMRMTokenSecretM ( conf ,    null )  ,    new   RMContainerTokenSecretM ( conf )  ,    new   NMTokenSecretMInRM ( conf )  ,    new   ClientToAMTokenSecretMInRM (  )  ,    writer )  ;", "return   rmContext ;", "}", "METHOD_END"], "methodName": ["getMockRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.TestUtils"}, {"methodBody": ["METHOD_START", "{", "return   numActiveApplications ;", "}", "METHOD_END"], "methodName": ["getNumActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UserInfo"}, {"methodBody": ["METHOD_START", "{", "return   numPendingApplications ;", "}", "METHOD_END"], "methodName": ["getNumPendingApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UserInfo"}, {"methodBody": ["METHOD_START", "{", "return   resourcesUsed ;", "}", "METHOD_END"], "methodName": ["getResourcesUsed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UserInfo"}, {"methodBody": ["METHOD_START", "{", "return   username ;", "}", "METHOD_END"], "methodName": ["getUsername"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.UserInfo"}, {"methodBody": ["METHOD_START", "{", "if    ( liveContainers . containsKey ( cont )  )     {", "containersToPreempt . add ( cont )  ;", "}", "}", "METHOD_END"], "methodName": ["addPreemptContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "if    ( isStopped )     {", "return   null ;", "}", "if    (  ( getTotalRequiredResources ( priority )  )     <  =     0  )     {", "return   null ;", "}", "RMContainer   rmContainer    =    new   RMContainerImpl ( container ,    this . getApplicationAttemptId (  )  ,    node . getNodeID (  )  ,    appSchedulingInfo . getUser (  )  ,    this . rmContext )  ;", "newlyAllocatedContainers . add ( rmContainer )  ;", "liveContainers . put ( container . getId (  )  ,    rmContainer )  ;", "List < ResourceRequest >    resourceRequestList    =    appSchedulingInfo . allocate ( type ,    node ,    priority ,    request ,    container )  ;", "Resources . addTo ( currentConsumption ,    container . getResource (  )  )  ;", "(  ( RMContainerImpl )     ( rmContainer )  )  . setResourceRequests ( resourceRequestList )  ;", "rmContainer . handle ( new   RMContainerEvent ( container . getId (  )  ,    RMContainerEventType . START )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  \" allocate :    applicationAttemptId =  \"     +     ( container . getId (  )  . getApplicationAttemptId (  )  )  )     +     \"    container =  \"  )     +     ( container . getId (  )  )  )     +     \"    host =  \"  )     +     ( container . getNodeId (  )  . getHost (  )  )  )     +     \"    type =  \"  )     +    type )  )  ;", "}", "RMAuditLogger . logSuccess ( getUser (  )  ,    RMAuditLogger . AuditConstants . ALLOC _ CONTAINER ,     \" SchedulerApp \"  ,    getApplicationId (  )  ,    container . getId (  )  )  ;", "return   rmContainer ;", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "if    ( null    =  =     ( liveContainers . remove ( rmContainer . getContainerId (  )  )  )  )     {", "return   false ;", "}", "newlyAllocatedContainers . remove ( rmContainer )  ;", "Container   container    =    rmContainer . getContainer (  )  ;", "ContainerId   containerId    =    container . getId (  )  ;", "rmContainer . handle ( new   RMContainerFinishedEvent ( containerId ,    containerStatus ,    event )  )  ;", ". LOG . info (  (  (  (  (  (  \" Completed   container :     \"     +     ( rmContainer . getContainerId (  )  )  )     +     \"    in   state :     \"  )     +     ( rmContainer . getState (  )  )  )     +     \"    event :  \"  )     +    event )  )  ;", "containersToPreempt . remove ( rmContainer . getContainerId (  )  )  ;", "RMAuditLogger . logSuccess ( getUser (  )  ,    RMAuditLogger . AuditConstants . RELEASE _ CONTAINER ,     \" SchedulerApp \"  ,    getApplicationId (  )  ,    containerId )  ;", "Resource   containerResource    =    rmContainer . getContainer (  )  . getResource (  )  ;", "queue . getMetrics (  )  . releaseResources ( getUser (  )  ,     1  ,    containerResource )  ;", "Resources . subtractFrom ( currentConsumption ,    containerResource )  ;", "return   true ;", "}", "METHOD_END"], "methodName": ["containerCompleted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "Set < ContainerId >    currentContPreemption    =    Collections . unmodifiableSet ( new   HashSet < ContainerId >  ( containersToPreempt )  )  ;", "containersToPreempt . clear (  )  ;", "Resource   tot    =    Resource . newInstance (  0  ,     0  )  ;", "for    ( ContainerId   c    :    currentContPreemption )     {", "Resources . addTo ( tot ,    liveContainers . get ( c )  . getContainer (  )  . getResource (  )  )  ;", "}", "int   numCont    =     (  ( int )     ( Math . ceil ( Resources . divide ( rc ,    clusterResource ,    tot ,    minimumAllocation )  )  )  )  ;", "ResourceRequest   rr    =    ResourceRequest . newInstance ( UNDEFINED ,    ANY ,    minimumAllocation ,    numCont )  ;", "licationAttempt . ContainersAndNMTokensAllocation   allocation    =    pullNewlyAllocatedContainersAndNMTokens (  )  ;", "return   new   Allocation ( allocation . getContainerList (  )  ,    getHeadroom (  )  ,    null ,    currentContPreemption ,    Collections . singletonList ( rr )  ,    allocation . getNMTokenList (  )  )  ;", "}", "METHOD_END"], "methodName": ["getAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "int   requiredResources    =    Math . max (  (  ( this . getResourceRequests ( priority )  . size (  )  )     -     1  )  ,     0  )  ;", "return   Math . min (  (  (  ( float )     ( requiredResources )  )     /    clusterNodes )  ,     1  .  0 F )  ;", "}", "METHOD_END"], "methodName": ["getLocalityWaitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "Resource   ret    =    Resource . newInstance (  0  ,     0  )  ;", "for    ( ResourceRequest   rr    :    appSchedulingInfo . getAllResourceRequests (  )  )     {", "if    ( ResourceRequest . isAnyLocation ( rr . getResourceName (  )  )  )     {", "Resources . addTo ( ret ,    Resources . multiply ( rr . getCapability (  )  ,    rr . getNumContainers (  )  )  )  ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getTotalPendingRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "Map < NodeId ,    RMContainer >    reservedContainers    =    this . reservedContainers . get ( priority )  ;", "if    ( reservedContainers    !  =    null )     {", "RMContainer   reservedContainer    =    reservedContainers . remove ( node . getNodeID (  )  )  ;", "if    (  (  ( reservedContainer    !  =    null )     &  &     (  ( reservedContainer . getContainer (  )  )     !  =    null )  )     &  &     (  ( reservedContainer . getContainer (  )  . getResource (  )  )     !  =    null )  )     {", "if    ( reservedContainers . isEmpty (  )  )     {", "this . reservedContainers . remove ( priority )  ;", "}", "resetReReservations ( priority )  ;", "Resource   resource    =    reservedContainer . getContainer (  )  . getResource (  )  ;", "Resources . subtractFrom ( currentReservation ,    resource )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  \" Application    \"     +     ( getApplicationId (  )  )  )     +     \"    unreserved    \"  )     +     \"    on   node    \"  )     +    node )     +     \"  ,    currently   has    \"  )     +     ( reservedContainers . size (  )  )  )     +     \"    at   priority    \"  )     +    priority )     +     \"  ;    currentReservation    \"  )     +     ( currentReservation )  )  )  ;", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["unreserve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"}, {"methodBody": ["METHOD_START", "{", "if    ( application . isBlacklisted ( node . getNodeName (  )  )  )     {", "if    ( LOG . isDebugEnabled (  )  )     {", "LOG . debug (  (  (  (  (  \" Skipping    ' host '     \"     +     ( node . getNodeName (  )  )  )     +     \"    for    \"  )     +     ( application . getApplicationId (  )  )  )     +     \"    since   it   has   been   blacklisted \"  )  )  ;", "}", "return   true ;", "}", "if    ( application . isBlacklisted ( node . getRackName (  )  )  )     {", "if    ( LOG . isDebugEnabled (  )  )     {", "LOG . debug (  (  (  (  (  \" Skipping    ' rack '     \"     +     ( node . getRackName (  )  )  )     +     \"    for    \"  )     +     ( application . getApplicationId (  )  )  )     +     \"    since   it   has   been   blacklisted \"  )  )  ;", "}", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isBlacklisted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerUtils"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   isAppRecovering ;", "}", "METHOD_END"], "methodName": ["getIsAppRecovering"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   isAttemptRecovering ;", "}", "METHOD_END"], "methodName": ["getIsAttemptRecovering"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   transferStateFromPreviousAttempt ;", "}", "METHOD_END"], "methodName": ["getTransferStateFromPreviousAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptID"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . finalAttemptState ;", "}", "METHOD_END"], "methodName": ["getFinalAttemptState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . keepContainersAcrossAppAttempts ;", "}", "METHOD_END"], "methodName": ["getKeepContainersAcrossAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationID"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppRemovedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   this . finalState ;", "}", "METHOD_END"], "methodName": ["getFinalState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppRemovedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.ContainerExpiredSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   rmNode ;", "}", "METHOD_END"], "methodName": ["getAddedRMNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   containerReports ;", "}", "METHOD_END"], "methodName": ["getContainerReports"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeAddedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   rmNode ;", "}", "METHOD_END"], "methodName": ["getRemovedRMNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeRemovedSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   rmNode ;", "}", "METHOD_END"], "methodName": ["getRMNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent"}, {"methodBody": ["METHOD_START", "{", "return   configuredQueues ;", "}", "METHOD_END"], "methodName": ["getConfiguredQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   defaultSchedulingPolicy ;", "}", "METHOD_END"], "methodName": ["getDefaultSchedulingPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   fairSharePreemptionTimeout ;", "}", "METHOD_END"], "methodName": ["getFairSharePreemptionTimeout"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Resource   maxQueueResource    =    maxQueueResources . get ( queueName )  ;", "return   maxQueueResource    =  =    null    ?    Resources . unbounded (  )     :    maxQueueResource ;", "}", "METHOD_END"], "methodName": ["getMaxResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Resource   minQueueResource    =    minQueueResources . get ( queue )  ;", "return   minQueueResource    =  =    null    ?    Resources . none (  )     :    minQueueResource ;", "}", "METHOD_END"], "methodName": ["getMinResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Long   minSharePreemptionTimeout    =    minSharePreemptionTimeouts . get ( queueName )  ;", "return   minSharePreemptionTimeout    =  =    null    ?    defaultMinSharePreemptionTimeout    :    minSharePreemptionTimeout ;", "}", "METHOD_END"], "methodName": ["getMinSharePreemptionTimeout"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   placementPolicy ;", "}", "METHOD_END"], "methodName": ["getPlacementPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Map < QueueACL ,    AccessControlList >    queueAcls    =    this . queueAcls . get ( queue )  ;", "if    ( queueAcls    !  =    null )     {", "AccessControlList   operationAcl    =    queueAcls . get ( operation )  ;", "if    ( operationAcl    !  =    null )     {", "return   operationAcl ;", "}", "}", "return   queue . equals (  \" root \"  )     ?     . EVERYBODY _ ACL    :     . NOBODY _ ACL ;", "}", "METHOD_END"], "methodName": ["getQueueAcl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Float   maxAMShare    =    queueMaxAMShares . get ( queue )  ;", "return   maxAMShare    =  =    null    ?    queueMaxAMShareDefault    :    maxAMShare ;", "}", "METHOD_END"], "methodName": ["getQueueMaxAMShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Integer   maxApps    =    queueMaxApps . get ( queue )  ;", "return   maxApps    =  =    null    ?    queueMaxAppsDefault    :    maxApps ;", "}", "METHOD_END"], "methodName": ["getQueueMaxApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "ResourceWeights   weight    =    queueWeights . get ( queue )  ;", "return   weight    =  =    null    ?    ResourceWeights . NEUTRAL    :    weight ;", "}", "METHOD_END"], "methodName": ["getQueueWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "SchedulingPolicy   policy    =    schedulingPolicies . get ( queueName )  ;", "return   policy    =  =    null    ?    defaultSchedulingPolicy    :    policy ;", "}", "METHOD_END"], "methodName": ["getSchedulingPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "Integer   maxApps    =    userMaxApps . get ( user )  ;", "return   maxApps    =  =    null    ?    userMaxAppsDefault    :    maxApps ;", "}", "METHOD_END"], "methodName": ["getUserMaxApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   lastPeriodIndex    =    queueName . length (  )  ;", "while    ( lastPeriodIndex    !  =     (  -  1  )  )     {", "String   queue    =    queueName . substring (  0  ,    lastPeriodIndex )  ;", "if    ( getQueueAcl ( queue ,    acl )  . isUserwed ( user )  )     {", "return   true ;", "}", "lastPeriodIndex    =    queueName . lastIndexOf (  '  .  '  ,     ( lastPeriodIndex    -     1  )  )  ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["hasAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration"}, {"methodBody": ["METHOD_START", "{", "String   allocFilePath    =    conf . get ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    FairSchedulerConfiguration . DEFAULT _ ALLOCATION _ FILE )  ;", "File   allocFile    =    new   File ( allocFilePath )  ;", "if    (  !  ( allocFile . isAbsolute (  )  )  )     {", "URL   url    =    Thread . currentThread (  )  . getContextClassLoader (  )  . getResource ( allocFilePath )  ;", "if    ( url    =  =    null )     {", ". LOG . warn (  ( allocFilePath    +     \"    not   found   on   the   classpath .  \"  )  )  ;", "allocFile    =    null ;", "} else", "if    (  !  ( url . getProtocol (  )  . equalsIgnoreCase (  \" file \"  )  )  )     {", "throw   new   RuntimeException (  (  (  \" Allocation   file    \"     +    url )     +     \"    found   on   the   classpath   is   not   on   the   local   filesystem .  \"  )  )  ;", "} else    {", "allocFile    =    new   File ( url . getPath (  )  )  ;", "}", "}", "return   allocFile ;", "}", "METHOD_END"], "methodName": ["getAllocationFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =    element . getAttribute (  \" name \"  )  ;", "if    ( parentName    !  =    null )     {", "queueName    =     ( parentName    +     \"  .  \"  )     +    queueName ;", "}", "Map < QueueACL ,    AccessControlList >    acls    =    new   HashMap < QueueACL ,    AccessControlList >  (  )  ;", "NodeList   fields    =    element . getChildNodes (  )  ;", "boolean   isLeaf    =    true ;", "for    ( int   j    =     0  ;    j    <     ( fields . getLength (  )  )  ;    j +  +  )     {", "Node   fieldNode    =    fields . item ( j )  ;", "if    (  !  ( fieldNode   instanceof   Element )  )", "continue ;", "Element   field    =     (  ( Element )     ( fieldNode )  )  ;", "if    (  \" minResources \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "Resource   val    =    FairSchedulerConfiguration . parseResourceConfigValue ( text )  ;", "minQueueResources . put ( queueName ,    val )  ;", "} else", "if    (  \" maxResources \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "Resource   val    =    FairSchedulerConfiguration . parseResourceConfigValue ( text )  ;", "maxQueueResources . put ( queueName ,    val )  ;", "} else", "if    (  \" maxRunningApps \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "int   val    =    Integer . parseInt ( text )  ;", "queueMaxApps . put ( queueName ,    val )  ;", "} else", "if    (  \" maxAMShare \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "float   val    =    Float . parseFloat ( text )  ;", "val    =    Math . min ( val ,     1  .  0 F )  ;", "queueMaxAMShares . put ( queueName ,    val )  ;", "} else", "if    (  \" weight \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "double   val    =    Double . parseDouble ( text )  ;", "queueWeights . put ( queueName ,    new   resource . ResourceWeights (  (  ( float )     ( val )  )  )  )  ;", "} else", "if    (  \" minSharePreemptionTimeout \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "long   val    =     ( Long . parseLong ( text )  )     *     1  0  0  0 L ;", "minSharePreemptionTimeouts . put ( queueName ,    val )  ;", "} else", "if    (  (  \" schedulingPolicy \"  . equals ( field . getTagName (  )  )  )     |  |     (  \" schedulingMode \"  . equals ( field . getTagName (  )  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "SchedulingPolicy   policy    =    SchedulingPolicy . parse ( text )  ;", "queuePolicies . put ( queueName ,    policy )  ;", "} else", "if    (  \" aclSubmitApps \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  ;", "acls . put ( QueueACL . SUBMIT _ APPLICATIONS ,    new   AccessControlList ( text )  )  ;", "} else", "if    (  \" aclAdministerApps \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  ;", "acls . put ( QueueACL . ADMINISTER _ QUEUE ,    new   AccessControlList ( text )  )  ;", "} else", "if    (  (  \" queue \"  . endsWith ( field . getTagName (  )  )  )     |  |     (  \" pool \"  . equals ( field . getTagName (  )  )  )  )     {", "loadQueue ( queueName ,    field ,    minQueueResources ,    maxQueueResources ,    queueMaxApps ,    userMaxApps ,    queueMaxAMShares ,    queueWeights ,    queuePolicies ,    minSharePreemptionTimeouts ,    queueAcls ,    configuredQueues )  ;", "configuredQueues . get ( FSQueueType . PARENT )  . add ( queueName )  ;", "isLeaf    =    false ;", "}", "}", "if    ( isLeaf )     {", "if    (  \" parent \"  . equals ( element . getAttribute (  \" type \"  )  )  )     {", "configuredQueues . get ( FSQueueType . PARENT )  . add ( queueName )  ;", "} else    {", "configuredQueues . get ( FSQueueType . LEAF )  . add ( queueName )  ;", "}", "}", "queueAcls . put ( queueName ,    acls )  ;", "if    (  (  ( maxQueueResources . containsKey ( queueName )  )     &  &     ( minQueueResources . containsKey ( queueName )  )  )     &  &     (  !  ( Resources . fitsIn ( minQueueResources . get ( queueName )  ,    maxQueueResources . get ( queueName )  )  )  )  )     {", "AllocationFileLoaderService . LOG . warn ( String . format (  \" Queue    % s   has   max   resources    % s   less   than   min   resources    % s \"  ,    queueName ,    maxQueueResources . get ( queueName )  ,    minQueueResources . get ( queueName )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["loadQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( allocFile )     =  =    null )     {", "return ;", "}", ". LOG . info (  (  \" Loading   allocation   file    \"     +     ( allocFile )  )  )  ;", "Map < String ,    Resource >    minQueueResources    =    new   HashMap < String ,    Resource >  (  )  ;", "Map < String ,    Resource >    maxQueueResources    =    new   HashMap < String ,    Resource >  (  )  ;", "Map < String ,    Integer >    queueMaxApps    =    new   HashMap < String ,    Integer >  (  )  ;", "Map < String ,    Integer >    userMaxApps    =    new   HashMap < String ,    Integer >  (  )  ;", "Map < String ,    Float >    queueMaxAMShares    =    new   HashMap < String ,    Float >  (  )  ;", "Map < String ,    ResourceWeights >    queueWeights    =    new   HashMap < String ,    ResourceWeights >  (  )  ;", "Map < String ,    SchedulingPolicy >    queuePolicies    =    new   HashMap < String ,    SchedulingPolicy >  (  )  ;", "Map < String ,    Long >    minSharePreemptionTimeouts    =    new   HashMap < String ,    Long >  (  )  ;", "Map < String ,    Map < QueueACL ,    AccessControlList >  >    queueAcls    =    new   HashMap < String ,    Map < QueueACL ,    AccessControlList >  >  (  )  ;", "int   userMaxAppsDefault    =    Integer . MAX _ VALUE ;", "int   queueMaxAppsDefault    =    Integer . MAX _ VALUE ;", "float   queueMaxAMShareDefault    =     -  1  .  0 F ;", "long   fairSharePreemptionTimeout    =    Long . MAX _ VALUE ;", "long   defaultMinSharePreemptionTimeout    =    Long . MAX _ VALUE ;", "SchedulingPolicy   defaultSchedPolicy    =    SchedulingPolicy . DEFAULT _ POLICY ;", "QueuePlacementPolicy   newPlacementPolicy    =    null ;", "Map < FSQueueType ,    Set < String >  >    configuredQueues    =    new   HashMap < FSQueueType ,    Set < String >  >  (  )  ;", "for    ( FSQueueType   queueType    :    FSQueueType . values (  )  )     {", "configuredQueues . put ( queueType ,    new   HashSet < String >  (  )  )  ;", "}", "DocumentBuilderFactory   docBuilderFactory    =    DocumentBuilderFactory . newInstance (  )  ;", "docBuilderFactory . setIgnoringComments ( true )  ;", "DocumentBuilder   builder    =    docBuilderFactory . newDocumentBuilder (  )  ;", "Document   doc    =    builder . parse ( allocFile )  ;", "Element   root    =    doc . getDocumentElement (  )  ;", "if    (  !  (  \" allocations \"  . equals ( root . getTagName (  )  )  )  )", "throw   new   AllocationConfigurationException (  (  \" Bad   fair   scheduler   config    \"     +     \" file :    top - level   element   not    < allocations >  \"  )  )  ;", "NodeList   elements    =    root . getChildNodes (  )  ;", "List < Element >    queueElements    =    new   ArrayList < Element >  (  )  ;", "Element   placementPolicyElement    =    null ;", "for    ( int   i    =     0  ;    i    <     ( elements . getLength (  )  )  ;    i +  +  )     {", "Node   node    =    elements . item ( i )  ;", "if    ( node   instanceof   Element )     {", "Element   element    =     (  ( Element )     ( node )  )  ;", "if    (  (  \" queue \"  . equals ( element . getTagName (  )  )  )     |  |     (  \" pool \"  . equals ( element . getTagName (  )  )  )  )     {", "queueElements . add ( element )  ;", "} else", "if    (  \" user \"  . equals ( element . getTagName (  )  )  )     {", "String   userName    =    element . getAttribute (  \" name \"  )  ;", "NodeList   fields    =    element . getChildNodes (  )  ;", "for    ( int   j    =     0  ;    j    <     ( fields . getLength (  )  )  ;    j +  +  )     {", "Node   fieldNode    =    fields . item ( j )  ;", "if    (  !  ( fieldNode   instanceof   Element )  )", "continue ;", "Element   field    =     (  ( Element )     ( fieldNode )  )  ;", "if    (  \" maxRunningApps \"  . equals ( field . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( field . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "int   val    =    Integer . parseInt ( text )  ;", "userMaxApps . put ( userName ,    val )  ;", "}", "}", "} else", "if    (  \" userMaxAppsDefault \"  . equals ( element . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( element . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "int   val    =    Integer . parseInt ( text )  ;", "userMaxAppsDefault    =    val ;", "} else", "if    (  \" fairSharePreemptionTimeout \"  . equals ( element . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( element . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "long   val    =     ( Long . parseLong ( text )  )     *     1  0  0  0 L ;", "fairSharePreemptionTimeout    =    val ;", "} else", "if    (  \" defaultMinSharePreemptionTimeout \"  . equals ( element . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( element . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "long   val    =     ( Long . parseLong ( text )  )     *     1  0  0  0 L ;", "defaultMinSharePreemptionTimeout    =    val ;", "} else", "if    (  \" queueMaxAppsDefault \"  . equals ( element . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( element . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "int   val    =    Integer . parseInt ( text )  ;", "queueMaxAppsDefault    =    val ;", "} else", "if    (  \" queueMaxAMShareDefault \"  . equals ( element . getTagName (  )  )  )     {", "String   text    =     (  ( Text )     ( element . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "float   val    =    Float . parseFloat ( text )  ;", "val    =    Math . min ( val ,     1  .  0 F )  ;", "queueMaxAMShareDefault    =    val ;", "} else", "if    (  (  \" defaultQueueSchedulingPolicy \"  . equals ( element . getTagName (  )  )  )     |  |     (  \" defaultQueueSchedulingMode \"  . equals ( element . getTagName (  )  )  )  )     {", "String   text    =     (  ( Text )     ( element . getFirstChild (  )  )  )  . getData (  )  . trim (  )  ;", "defaultSchedPolicy    =    SchedulingPolicy . parse ( text )  ;", "} else", "if    (  \" queuePlacementPolicy \"  . equals ( element . getTagName (  )  )  )     {", "placementPolicyElement    =    element ;", "} else    {", ". LOG . warn (  (  \" Bad   element   in   allocations   file :     \"     +     ( element . getTagName (  )  )  )  )  ;", "}", "}", "}", "for    ( Element   element    :    queueElements )     {", "String   parent    =     \" root \"  ;", "if    ( element . getAttribute (  \" name \"  )  . equalsIgnoreCase (  \" root \"  )  )     {", "if    (  ( queueElements . size (  )  )     >     1  )     {", "throw   new   AllocationConfigurationException (  (  \" If   configuring   root   queue ,  \"     +     \"    no   other   queues   can   be   placed   alongside   it .  \"  )  )  ;", "}", "parent    =    null ;", "}", "loadQueue ( parent ,    element ,    minQueueResources ,    maxQueueResources ,    queueMaxApps ,    userMaxApps ,    queueMaxAMShares ,    queueWeights ,    queuePolicies ,    minSharePreemptionTimeouts ,    queueAcls ,    configuredQueues )  ;", "}", "Configuration   conf    =    getConfig (  )  ;", "if    ( placementPolicyElement    !  =    null )     {", "newPlacementPolicy    =    QueuePlacementPolicy . fromXml ( placementPolicyElement ,    configuredQueues ,    conf )  ;", "} else    {", "newPlacementPolicy    =    QueuePlacementPolicy . fromConfiguration ( conf ,    configuredQueues )  ;", "}", "AllocationConfiguration   info    =    new   AllocationConfiguration ( minQueueResources ,    maxQueueResources ,    queueMaxApps ,    userMaxApps ,    queueWeights ,    queueMaxAMShares ,    userMaxAppsDefault ,    queueMaxAppsDefault ,    queueMaxAMShareDefault ,    queuePolicies ,    defaultSchedPolicy ,    minSharePreemptionTimeouts ,    queueAcls ,    fairSharePreemptionTimeout ,    defaultMinSharePreemptionTimeout ,    newPlacementPolicy ,    configuredQueues )  ;", "lastSuccessfulReload    =    clock . getTime (  )  ;", "lastReloadAttemptFailed    =    false ;", "reloadListener . onReload ( info )  ;", "}", "METHOD_END"], "methodName": ["reloadAllocations"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "this . reloadListener    =    reloadListener ;", "}", "METHOD_END"], "methodName": ["setReloadListener"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "assert    ( preemptionMap . get ( container )  )     =  =    null ;", "preemptionMap . put ( container ,    time )  ;", "Rs . addTo ( preemptedRs ,    container . getAllocatedR (  )  )  ;", "}", "METHOD_END"], "methodName": ["addPreemption"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "NodeType   allowed    =    allowedLocalityLevel . get ( priority )  ;", "if    ( allowed    !  =    null )     {", "if    (  ( allowed . equals ( NodeType . OFF _ SWITCH )  )     &  &     (  ( type . equals ( NodeType . NODE _ LOCAL )  )     |  |     ( type . equals ( NodeType . RACK _ LOCAL )  )  )  )     {", "this . resetAllowedLocalityLevel ( priority ,    type )  ;", "} else", "if    (  ( allowed . equals ( NodeType . RACK _ LOCAL )  )     &  &     ( type . equals ( NodeType . NODE _ LOCAL )  )  )     {", "this . resetAllowedLocalityLevel ( priority ,    type )  ;", "}", "}", "if    (  ( getTotalRequiredResources ( priority )  )     <  =     0  )     {", "return   null ;", "}", "RMContainer   rmContainer    =    new   RMContainerImpl ( container ,    getApplicationAttemptId (  )  ,    node . getNodeID (  )  ,    appSchedulingInfo . getUser (  )  ,    rmContext )  ;", "newlyAllocatedContainers . add ( rmContainer )  ;", "liveContainers . put ( container . getId (  )  ,    rmContainer )  ;", "List < ResourceRequest >    resourceRequestList    =    appSchedulingInfo . allocate ( type ,    node ,    priority ,    request ,    container )  ;", "Resources . addTo ( currentConsumption ,    container . getResource (  )  )  ;", "(  ( RMContainerImpl )     ( rmContainer )  )  . setResourceRequests ( resourceRequestList )  ;", "rmContainer . handle ( new   RMContainerEvent ( container . getId (  )  ,    RMContainerEventType . START )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  \" allocate :    applicationAttemptId =  \"     +     ( container . getId (  )  . getApplicationAttemptId (  )  )  )     +     \"    container =  \"  )     +     ( container . getId (  )  )  )     +     \"    host =  \"  )     +     ( container . getNodeId (  )  . getHost (  )  )  )     +     \"    type =  \"  )     +    type )  )  ;", "}", "RMAuditLogger . logSuccess ( getUser (  )  ,    RMAuditLogger . AuditConstants . ALLOC _ CONTAINER ,     \" SchedulerApp \"  ,    getApplicationId (  )  ,    container . getId (  )  )  ;", "return   rmContainer ;", "}", "METHOD_END"], "methodName": ["allocate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( FSAppAttempt . LOG . isDebugEnabled (  )  )     {", "FSAppAttempt . LOG . debug (  (  (  (  \" Node   offered   to   app :     \"     +     ( getName (  )  )  )     +     \"    reserved :     \"  )     +    reserved )  )  ;", "}", "Collection < Priority >    prioritiesToTry    =     ( reserved )     ?    Arrays . asList ( node . getReservedContainer (  )  . getReservedPriority (  )  )     :    getPriorities (  )  ;", "synchronized ( this )     {", "for    ( Priority   priority    :    prioritiesToTry )     {", "if    (  (  ( getTotalRequiredResources ( priority )  )     <  =     0  )     |  |     (  !  ( hasContainerForNode ( priority ,    node )  )  )  )     {", "continue ;", "}", "addSchedulingOpportunity ( priority )  ;", "if    (  (  ( getLiveContainers (  )  . size (  )  )     =  =     0  )     &  &     (  !  ( getUnmanagedAM (  )  )  )  )     {", "if    (  !  ( getQueue (  )  . canRunAppAM ( getAMResource (  )  )  )  )     {", "return   Resources . none (  )  ;", "}", "}", "ResourceRequest   rackLocalRequest    =    getResourceRequest ( priority ,    node . getRackName (  )  )  ;", "ResourceRequest   localRequest    =    getResourceRequest ( priority ,    node . getNodeName (  )  )  ;", "if    (  ( localRequest    !  =    null )     &  &     (  !  ( localRequest . getRelaxLocality (  )  )  )  )     {", "FSAppAttempt . LOG . warn (  (  \" Relax   locality   off   is   not   supported   on   local   request :     \"     +    localRequest )  )  ;", "}", "NodeType   allowedLocality ;", "if    ( scheduler . isContinuousSchedulingEnabled (  )  )     {", "allowedLocality    =    getAllowedLocalityLevelByTime ( priority ,    scheduler . getNodeLocalityDelayMs (  )  ,    scheduler . getRackLocalityDelayMs (  )  ,    scheduler . getClock (  )  . getTime (  )  )  ;", "} else    {", "allowedLocality    =    getAllowedLocalityLevel ( priority ,    scheduler . getNumClusterNodes (  )  ,    scheduler . getNodeLocalityThreshold (  )  ,    scheduler . getRackLocalityThreshold (  )  )  ;", "}", "if    (  (  (  ( rackLocalRequest    !  =    null )     &  &     (  ( rackLocalRequest . getNumContainers (  )  )     !  =     0  )  )     &  &     ( localRequest    !  =    null )  )     &  &     (  ( localRequest . getNumContainers (  )  )     !  =     0  )  )     {", "return   assignContainer ( node ,    localRequest ,    NodeType . NODE _ LOCAL ,    reserved )  ;", "}", "if    (  ( rackLocalRequest    !  =    null )     &  &     (  !  ( rackLocalRequest . getRelaxLocality (  )  )  )  )     {", "continue ;", "}", "if    (  (  ( rackLocalRequest    !  =    null )     &  &     (  ( rackLocalRequest . getNumContainers (  )  )     !  =     0  )  )     &  &     (  ( allowedLocality . equals ( NodeType . RACK _ LOCAL )  )     |  |     ( allowedLocality . equals ( NodeType . OFF _ SWITCH )  )  )  )     {", "return   assignContainer ( node ,    rackLocalRequest ,    NodeType . RACK _ LOCAL ,    reserved )  ;", "}", "ResourceRequest   offSwitchRequest    =    getResourceRequest ( priority ,    ANY )  ;", "if    (  ( offSwitchRequest    !  =    null )     &  &     (  !  ( offSwitchRequest . getRelaxLocality (  )  )  )  )     {", "continue ;", "}", "if    (  (  ( offSwitchRequest    !  =    null )     &  &     (  ( offSwitchRequest . getNumContainers (  )  )     !  =     0  )  )     &  &     ( allowedLocality . equals ( NodeType . OFF _ SWITCH )  )  )     {", "return   assignContainer ( node ,    offSwitchRequest ,    NodeType . OFF _ SWITCH ,    reserved )  ;", "}", "}", "}", "return   Resources . none (  )  ;", "}", "METHOD_END"], "methodName": ["assignContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "Resource   capability    =    request . getCapability (  )  ;", "Resource   available    =    node . getAvailableResource (  )  ;", "Container   container    =    null ;", "if    ( reserved )     {", "container    =    node . getReservedContainer (  )  . getContainer (  )  ;", "} else    {", "container    =    createContainer ( node ,    capability ,    request . getPriority (  )  )  ;", "}", "if    ( Resources . fitsIn ( capability ,    available )  )     {", "RMContainer   allocatedContainer    =    allocate ( type ,    node ,    request . getPriority (  )  ,    request ,    container )  ;", "if    ( allocatedContainer    =  =    null )     {", "if    ( reserved )     {", "unreserve ( request . getPriority (  )  ,    node )  ;", "}", "return   Resources . none (  )  ;", "}", "if    ( reserved )     {", "unreserve ( request . getPriority (  )  ,    node )  ;", "}", "node . allocateContainer ( allocatedContainer )  ;", "if    (  (  ( getLiveContainers (  )  . size (  )  )     =  =     1  )     &  &     (  !  ( getUnmanagedAM (  )  )  )  )     {", "getQueue (  )  . addAMResourceUsage ( container . getResource (  )  )  ;", "setAmRunning ( true )  ;", "}", "return   container . getResource (  )  ;", "} else    {", "reserve ( request . getPriority (  )  ,    node ,    container ,    reserved )  ;", "return   FairSCONTAINER _ RESERVED ;", "}", "}", "METHOD_END"], "methodName": ["assignContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "RMContainer   rmContainer    =    node . getReservedContainer (  )  ;", "Priority   priority    =    rmContainer . getReservedPriority (  )  ;", "if    (  ( getTotalRequiredRs ( priority )  )     =  =     0  )     {", "unreserve ( priority ,    node )  ;", "return   Rs . none (  )  ;", "}", "if    (  !  ( Rs . fitsIn ( node . getReservedContainer (  )  . getReservedR (  )  ,    node . getAvailableR (  )  )  )  )     {", "return   Rs . none (  )  ;", "}", "return   assignContainer ( node ,    true )  ;", "}", "METHOD_END"], "methodName": ["assignReservedContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "preemptedResources . setMemory (  0  )  ;", "preemptedResources . setVirtualCores (  0  )  ;", "}", "METHOD_END"], "methodName": ["clearPreemptedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    rmContainer . getContainer (  )  ;", "ContainerId   containerId    =    container . getId (  )  ;", "newlyAllocatedContainers . remove ( rmContainer )  ;", "rmContainer . handle ( new   RMContainerFinishedEvent ( containerId ,    containerStatus ,    event )  )  ;", ". LOG . info (  (  (  (  (  (  \" Completed   container :     \"     +     ( rmContainer . getContainerId (  )  )  )     +     \"    in   state :     \"  )     +     ( rmContainer . getState (  )  )  )     +     \"    event :  \"  )     +    event )  )  ;", "liveContainers . remove ( rmContainer . getContainerId (  )  )  ;", "RMAuditLogger . logSuccess ( getUser (  )  ,    RMAuditLogger . AuditConstants . RELEASE _ CONTAINER ,     \" SchedulerApp \"  ,    getApplicationId (  )  ,    containerId )  ;", "Resource   containerResource    =    rmContainer . getContainer (  )  . getResource (  )  ;", "queue . getMetrics (  )  . releaseResources ( getUser (  )  ,     1  ,    containerResource )  ;", "Resources . subtractFrom ( currentConsumption ,    containerResource )  ;", "preemptionMap . remove ( rmContainer )  ;", "}", "METHOD_END"], "methodName": ["containerCompleted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId    =    node . getRMNode (  )  . getNodeID (  )  ;", "ContainerId   containerId    =    BuilderUtils . newContainerId ( getApplicationId (  )  ,    getNewContainerId (  )  )  ;", "Container   container    =    BuilderUtils . newContainer ( containerId ,    nodeId ,    node . getRMNode (  )  . getHttpAddress (  )  ,    capability ,    priority ,    null )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["createContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( nodeLocalityThreshold    >     1  .  0  )     {", "nodeLocalityThreshold    =     1  .  0  ;", "}", "if    ( rackLocalityThreshold    >     1  .  0  )     {", "rackLocalityThreshold    =     1  .  0  ;", "}", "if    (  ( nodeLocalityThreshold    <     0  .  0  )     |  |     ( rackLocalityThreshold    <     0  .  0  )  )     {", "return   NodeType . OFF _ SWITCH ;", "}", "if    (  !  ( allowedLocalityLevel . containsKey ( priority )  )  )     {", "allowedLocalityLevel . put ( priority ,    NodeType . NODE _ LOCAL )  ;", "return   NodeType . NODE _ LOCAL ;", "}", "NodeType   allowed    =    allowedLocalityLevel . get ( priority )  ;", "if    ( allowed . equals ( NodeType . OFF _ SWITCH )  )", "return   NodeType . OFF _ SWITCH ;", "double   threshold    =     ( allowed . equals ( NodeType . NODE _ LOCAL )  )     ?    nodeLocalityThreshold    :    rackLocalityThreshold ;", "if    (  ( getSingOpportunities ( priority )  )     >     ( numNodes    *    threshold )  )     {", "if    ( allowed . equals ( NodeType . NODE _ LOCAL )  )     {", "allowedLocalityLevel . put ( priority ,    NodeType . RACK _ LOCAL )  ;", "resetSingOpportunities ( priority )  ;", "} else", "if    ( allowed . equals ( NodeType . RACK _ LOCAL )  )     {", "allowedLocalityLevel . put ( priority ,    NodeType . OFF _ SWITCH )  ;", "resetSingOpportunities ( priority )  ;", "}", "}", "return   allowedLocalityLevel . get ( priority )  ;", "}", "METHOD_END"], "methodName": ["getAllowedLocalityLevel"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "if    (  ( nodeLocalityDelayMs    <     0  )     |  |     ( rackLocalityDelayMs    <     0  )  )     {", "return   NodeType . OFF _ SWITCH ;", "}", "if    (  !  ( allowedLocalityLevel . containsKey ( priority )  )  )     {", "allowedLocalityLevel . put ( priority ,    NodeType . NODE _ LOCAL )  ;", "return   NodeType . NODE _ LOCAL ;", "}", "NodeType   allowed    =    allowedLocalityLevel . get ( priority )  ;", "if    ( allowed . equals ( NodeType . OFF _ SWITCH )  )     {", "return   NodeType . OFF _ SWITCH ;", "}", "long   waitTime    =    currentTimeMs ;", "if    ( lastSdContainer . containsKey ( priority )  )     {", "waitTime    -  =    lastSdContainer . get ( priority )  ;", "} else    {", "waitTime    -  =    getStartTime (  )  ;", "}", "long   thresholdTime    =     ( allowed . equals ( NodeType . NODE _ LOCAL )  )     ?    nodeLocalityDelayMs    :    rackLocalityDelayMs ;", "if    ( waitTime    >    thresholdTime )     {", "if    ( allowed . equals ( NodeType . NODE _ LOCAL )  )     {", "allowedLocalityLevel . put ( priority ,    NodeType . RACK _ LOCAL )  ;", "resetSchedulingOpportunities ( priority ,    currentTimeMs )  ;", "} else", "if    ( allowed . equals ( NodeType . RACK _ LOCAL )  )     {", "allowedLocalityLevel . put ( priority ,    NodeType . OFF _ SWITCH )  ;", "resetSchedulingOpportunities ( priority ,    currentTimeMs )  ;", "}", "}", "return   allowedLocalityLevel . get ( priority )  ;", "}", "METHOD_END"], "methodName": ["getAllowedLocalityLevelByTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "return   preemptionMap . get ( container )  ;", "}", "METHOD_END"], "methodName": ["getContainerPreemptionTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "int   requiredResources    =    Math . max (  (  ( this . getResourceRequests ( priority )  . size (  )  )     -     1  )  ,     0  )  ;", "return   Math . min (  (  (  ( float )     ( requiredResources )  )     /    clusterNodes )  ,     1  .  0 F )  ;", "}", "METHOD_END"], "methodName": ["getLocalityWaitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "return   queue . getMetrics (  )  ;", "}", "METHOD_END"], "methodName": ["getMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "return   preemptedResources ;", "}", "METHOD_END"], "methodName": ["getPreemptedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "return   preemptionMap . keySet (  )  ;", "}", "METHOD_END"], "methodName": ["getPreemptionContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "return   resourceWeights ;", "}", "METHOD_END"], "methodName": ["getResourceWeights"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   anyRequest    =    getResourceRequest ( prio ,    ANY )  ;", "ResourceRequest   rackRequest    =    getResourceRequest ( prio ,    node . getRackName (  )  )  ;", "ResourceRequest   nodeRequest    =    getResourceRequest ( prio ,    node . getNodeName (  )  )  ;", "return    (  (  (  ( anyRequest    !  =    null )     &  &     (  ( anyRequest . getNumContainers (  )  )     >     0  )  )     &  &     (  ( anyRequest . getRelaxLocality (  )  )     |  |     (  ( rackRequest    !  =    null )     &  &     (  ( rackRequest . getNumContainers (  )  )     >     0  )  )  )  )     &  &     (  (  ( rackRequest    =  =    null )     |  |     ( rackRequest . getRelaxLocality (  )  )  )     |  |     (  ( nodeRequest    !  =    null )     &  &     (  ( nodeRequest . getNumContainers (  )  )     >     0  )  )  )  )     &  &     ( Resources . lessThanOrEqual (  . RESOURCE _ CALCULATOR ,    null ,    anyRequest . getCapability (  )  ,    node . getRMNode (  )  . getTotalCapability (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["hasContainerForNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "FSAppAttempt . LOG . info (  (  (  (  \" Making   reservation :    node =  \"     +     ( node . getNodeName (  )  )  )     +     \"    app _ id =  \"  )     +     ( getApplicationId (  )  )  )  )  ;", "if    (  ! alreadyReserved )     {", "getMetrics (  )  . reserveResource ( getUser (  )  ,    container . getResource (  )  )  ;", "RMContainer   rmContainer    =    super . reserve ( node ,    priority ,    null ,    container )  ;", "node . reserveResource ( this ,    priority ,    rmContainer )  ;", "} else    {", "RMContainer   rmContainer    =    node . getReservedContainer (  )  ;", "super . reserve ( node ,    priority ,    rmContainer ,    container )  ;", "node . reserveResource ( this ,    priority ,    rmContainer )  ;", "}", "}", "METHOD_END"], "methodName": ["reserve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "NodeType   old    =    allowedLocalityLevel . get ( priority )  ;", ". LOG . info (  (  (  (  (  (  (  \" Raising   locality   level   from    \"     +    old )     +     \"    to    \"  )     +    level )     +     \"    at    \"  )     +     \"    priority    \"  )     +    priority )  )  ;", "allowedLocalityLevel . put ( priority ,    level )  ;", "}", "METHOD_END"], "methodName": ["resetAllowedLocalityLevel"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "preemptedResources    =    Resources . createResource (  0  )  ;", "for    ( RMContainer   container    :    getPreemptionContainers (  )  )     {", "Resources . addTo ( preemptedResources ,    container . getAllocatedResource (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["resetPreemptedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "RMContainer   rmContainer    =    node . getReservedContainer (  )  ;", "unreserveInternal ( priority ,    node )  ;", "node . unreserveR ( this )  ;", "getMetrics (  )  . unreserveR ( getUser (  )  ,    rmContainer . getContainer (  )  . getR (  )  )  ;", "}", "METHOD_END"], "methodName": ["unreserve"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "Map < NodeId ,    RMContainer >    reservedContainers    =    this . reservedContainers . get ( priority )  ;", "RMContainer   reservedContainer    =    reservedContainers . remove ( node . getNodeID (  )  )  ;", "if    ( reservedContainers . isEmpty (  )  )     {", "this . reservedContainers . remove ( priority )  ;", "}", "resetReReservations ( priority )  ;", "Resource   resource    =    reservedContainer . getContainer (  )  . getResource (  )  ;", "Resources . subtractFrom ( currentReservation ,    resource )  ;", ". LOG . info (  (  (  (  (  (  (  (  (  (  (  \" Application    \"     +     ( getApplicationId (  )  )  )     +     \"    unreserved    \"  )     +     \"    on   node    \"  )     +    node )     +     \"  ,    currently   has    \"  )     +     ( reservedContainers . size (  )  )  )     +     \"    at   priority    \"  )     +    priority )     +     \"  ;    currentReservation    \"  )     +     ( currentReservation )  )  )  ;", "}", "METHOD_END"], "methodName": ["unreserveInternal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "if    ( amResource    !  =    null )     {", "Resources . addTo ( amResourceUsage ,    amResource )  ;", "}", "}", "METHOD_END"], "methodName": ["addAMResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( runnable )     {", "runnableApps . add ( app )  ;", "} else    {", "nonRunnableApps . add ( app )  ;", "}", "}", "METHOD_END"], "methodName": ["addApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "runnableApps . add ( appSched )  ;", "}", "METHOD_END"], "methodName": ["addAppSchedulable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "float   maxAMShare    =    scheduler . getAllocationConfiguration (  )  . getQueueMaxAMShare ( getName (  )  )  ;", "if    (  ( Math . abs (  ( maxAMShare    -     (  -  1  .  0 F )  )  )  )     <     1  .  0 E -  4  )     {", "return   true ;", "}", "Resource   maxAMResource    =    Resources . multiply ( getFairShare (  )  ,    maxAMShare )  ;", "Resource   ifRunAMResource    =    Resources . add ( amResourceUsage ,    amResource )  ;", "return    !  ( policy . checkIfAMResourceUsageOverLimit ( ifRunAMResource ,    maxAMResource )  )  ;", "}", "METHOD_END"], "methodName": ["canRunAppAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   amResourceUsage ;", "}", "METHOD_END"], "methodName": ["getAmResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   lastTimeAtHalfFairShare ;", "}", "METHOD_END"], "methodName": ["getLastTimeAtHalfFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   lastTimeAtMinShare ;", "}", "METHOD_END"], "methodName": ["getLastTimeAtMinShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   nonRunnableApps ;", "}", "METHOD_END"], "methodName": ["getNonRunnableAppSchedulables"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   runnableApps ;", "}", "METHOD_END"], "methodName": ["getRunnableAppSchedulables"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "return   parent . getPolicy (  )  . checkIfUsageOverFairShare ( getResourceUsage (  )  ,    getFairShare (  )  )  ;", "}", "METHOD_END"], "methodName": ["preemptContainerPreCheck"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "if    ( runnableApps . remove ( app )  )     {", "if    (  ( app . isAmRunning (  )  )     &  &     (  ( app . getAMR (  )  )     !  =    null )  )     {", "Rs . subtractFrom ( amRUsage ,    app . getAMR (  )  )  ;", "}", "return   true ;", "} else", "if    ( nonRunnableApps . remove ( app )  )     {", "return   false ;", "} else    {", "throw   new   IllegalStateException (  (  (  (  \" Given   app   to   remove    \"     +    app )     +     \"    does   not   exist   in   queue    \"  )     +     ( this )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["removeApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "this . lastTimeAtHalfFairShare    =    lastTimeAtHalfFairShare ;", "}", "METHOD_END"], "methodName": ["setLastTimeAtHalfFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "this . lastTimeAtMinShare    =    lastTimeAtMinShare ;", "}", "METHOD_END"], "methodName": ["setLastTimeAtMinShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "sched . updateDemand (  )  ;", "Resource   toAdd    =    sched . getDemand (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  \" Counting   resource   from    \"     +     ( sched . getName (  )  )  )     +     \"     \"  )     +    toAdd )     +     \"  ;    Total   resource   consumption   for    \"  )     +     ( getName (  )  )  )     +     \"    now    \"  )     +     ( demand )  )  )  ;", "}", "demand    =    Resources . add ( demand ,    toAdd )  ;", "demand    =    Resources . componentwiseMin ( demand ,    maxRes )  ;", "}", "METHOD_END"], "methodName": ["updateDemandForApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "continuousSchedulingRun . add ( value )  ;", "}", "METHOD_END"], "methodName": ["addContinuousSchedulingRunDuration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "nodeUpdateCall . add ( value )  ;", "}", "METHOD_END"], "methodName": ["addNodeUpdateDuration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "preemptCall . add ( value )  ;", "}", "METHOD_END"], "methodName": ["addPreemptCallDuration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "updateCall . add ( value )  ;", "}", "METHOD_END"], "methodName": ["addUpdateCallDuration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "updateThreadRun . add ( value )  ;", "}", "METHOD_END"], "methodName": ["addUpdateThreadRunDuration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "FSOpDurations . INSTANCE . setExtended ( isExtended )  ;", "return   FSOpDurations . INSTANCE ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "if    ( isExtended    =  =     ( FSOpDurations . INSTANCE . isExtended )  )", "return ;", "continuousSchedulingRun . setExtended ( isExtended )  ;", "nodeUpdateCall . setExtended ( isExtended )  ;", "updateThreadRun . setExtended ( isExtended )  ;", "updateCall . setExtended ( isExtended )  ;", "preemptCall . setExtended ( isExtended )  ;", "FSOpDurations . INSTANCE . isExtended    =    isExtended ;", "}", "METHOD_END"], "methodName": ["setExtended"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSOpDurations"}, {"methodBody": ["METHOD_START", "{", "childQueues . add ( child )  ;", "}", "METHOD_END"], "methodName": ["addChildQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue"}, {"methodBody": ["METHOD_START", "{", "( runnableApps )  -  -  ;", "}", "METHOD_END"], "methodName": ["decrementRunnableApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue"}, {"methodBody": ["METHOD_START", "{", "QueueUserACLInfo   userAclInfo    =    recordFactory . newRecordInstance ( QueueUserACLInfo . class )  ;", "List < QueueACL >    operations    =    new   ArrayList < QueueACL >  (  )  ;", "for    ( QueueACL   operation    :    QueueACL . values (  )  )     {", "if    ( hasAccess ( operation ,    user )  )     {", "operations . add ( operation )  ;", "}", "}", "userAclInfo . seName ( geName (  )  )  ;", "userAclInfo . setUserAcls ( operations )  ;", "return   userAclInfo ;", "}", "METHOD_END"], "methodName": ["getUserAclInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue"}, {"methodBody": ["METHOD_START", "{", "( runnableApps )  +  +  ;", "}", "METHOD_END"], "methodName": ["incrementRunnableApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue"}, {"methodBody": ["METHOD_START", "{", "policy . computeSteadyShares ( childQueues ,    getSteadyFairShare (  )  )  ;", "for    ( FSQueue   childQueue    :    childQueues )     {", "childQueue . getMetrics (  )  . setSteadyFairShare ( childQueue . getSteadyFairShare (  )  )  ;", "if    ( childQueue   instanceof    )     {", "(  (  )     ( childQueue )  )  . recomputeSteadyShares (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["recomputeSteadyShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue"}, {"methodBody": ["METHOD_START", "{", "if    (  (  !  ( Resources . fitsIn ( getResourceUsage (  )  ,    scheduler . getAllocationConfiguration (  )  . getMaxResources ( getName (  )  )  )  )  )     |  |     (  ( node . getReservedContainer (  )  )     !  =    null )  )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["assignContainerPreCheck"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return   fairShare ;", "}", "METHOD_END"], "methodName": ["getFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return   parent ;", "}", "METHOD_END"], "methodName": ["getParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return   policy ;", "}", "METHOD_END"], "methodName": ["getPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return   steadyFairShare ;", "}", "METHOD_END"], "methodName": ["getSteadyFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return   scheduler . getAllocationConfiguration (  )  . hasAccess ( name ,    acl ,    user )  ;", "}", "METHOD_END"], "methodName": ["hasAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "return    ( getNumRunnableApps (  )  )     >     0  ;", "}", "METHOD_END"], "methodName": ["isActive"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "this . steadyFairShare    =    steadyFairShare ;", "metrics . setSteadyFairShare ( steadyFairShare )  ;", "}", "METHOD_END"], "methodName": ["setSteadyFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "throw   new   AllocationConfigurationException (  (  (  (  \" SchedulingPolicy    \"     +    policy )     +     \"    does   not   apply   to   queue    \"  )     +     ( getName (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["throwPolicyDoesnotApplyException"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue"}, {"methodBody": ["METHOD_START", "{", "MetricsSystem   ms    =    DefaultMetricsSystem . instance (  )  ;", "QueueMetrics   metrics    =    QueueMetrics . queueMetrics . get ( queueName )  ;", "if    ( metrics    =  =    null )     {", "metrics    =    new    ( ms ,    queueName ,    parent ,    enableUserMetrics ,    conf )  . tag ( QueueMetrics . QUEUE _ INFO ,    queueName )  ;", "if    ( ms    !  =    null )     {", "metrics    =    ms . register ( QueueMetrics . sourceName ( queueName )  . toString (  )  ,     (  \" Metrics   for   queue :     \"     +    queueName )  ,    metrics )  ;", "}", "QueueMetrics . queueMetrics . put ( queueName ,    metrics )  ;", "}", "return    (  (  )     ( metrics )  )  ;", "}", "METHOD_END"], "methodName": ["forQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   fairShareMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getFairShareMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   fairShareVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getFairShareVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   maxShareMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getMaxShareMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   maxShareVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getMaxShareVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   minShareMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getMinShareMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   minShareVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getMinShareVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   steadyFairShareMB . value (  )  ;", "}", "METHOD_END"], "methodName": ["getSteadyFairShareMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   steadyFairShareVCores . value (  )  ;", "}", "METHOD_END"], "methodName": ["getSteadyFairShareVCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "fairShareMB . set ( resource . getMemory (  )  )  ;", "fairShareVCores . set ( resource . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["setFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "maxShareMB . set ( resource . getMemory (  )  )  ;", "maxShareVCores . set ( resource . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["setMaxShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "minShareMB . set ( resource . getMemory (  )  )  ;", "minShareVCores . set ( resource . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["setMinShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "steadyFairShareMB . set ( resource . getMemory (  )  )  ;", "steadyFairShareVCores . set ( resource . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["setSteadyFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueMetrics"}, {"methodBody": ["METHOD_START", "{", "return   reservedAppSchedulable ;", "}", "METHOD_END"], "methodName": ["getReservedAppSchedulable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerNode"}, {"methodBody": ["METHOD_START", "{", "if    (  ( queueName    =  =    null )     |  |     ( queueName . isEmpty (  )  )  )     {", "String   message    =     (  (  (  \" Reject   application    \"     +    applicationId )     +     \"    submitted   by   user    \"  )     +    user )     +     \"    with   an   empty   queue   name .  \"  ;", ". LOG . info ( message )  ;", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    message )  )  ;", "return ;", "}", "RMApp   rmApp    =    rmContext . getRMApps (  )  . get ( applicationId )  ;", "FSLeafQueue   queue    =    assignToQueue ( rmApp ,    queueName ,    user )  ;", "if    ( queue    =  =    null )     {", "return ;", "}", "UserGroupInformation   userUgi    =    UserGroupInformation . createRemoteUser ( user )  ;", "if    (  (  !  ( queue . hasAccess ( SUBMIT _ APPLICATIONS ,    userUgi )  )  )     &  &     (  !  ( queue . hasAccess ( ADMINISTER _ QUEUE ,    userUgi )  )  )  )     {", "String   msg    =     (  (  \" User    \"     +     ( userUgi . getUserName (  )  )  )     +     \"    cannot   submit   applications   to   queue    \"  )     +     ( queue . getName (  )  )  ;", ". LOG . info ( msg )  ;", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( applicationId ,    msg )  )  ;", "return ;", "}", "SchedulerApplication < FSAppAttempt >    application    =    new   SchedulerApplication < FSAppAttempt >  ( queue ,    user )  ;", "applications . put ( applicationId ,    application )  ;", "queue . getMetrics (  )  . submitApp ( user )  ;", ". LOG . info (  (  (  (  (  (  (  (  \" Accepted   application    \"     +    applicationId )     +     \"    from   user :     \"  )     +    user )     +     \"  ,    in   queue :     \"  )     +    queueName )     +     \"  ,    currently   num   of   applications :     \"  )     +     ( applications . size (  )  )  )  )  ;", "if    ( isAppRecovering )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  ( applicationId    +     \"    is   recovering .    Skip   notifying   APP _ ACCEPTED \"  )  )  ;", "}", "} else    {", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppEvent ( applicationId ,    RMAppEventType . APP _ ACCEPTED )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FSAppAttempt >    application    =    applications . get ( applicationAttemptId . getApplicationId (  )  )  ;", "String   user    =    application . getUser (  )  ;", "FSLeafQueue   queue    =     (  ( FSLeafQueue )     ( application . getQueue (  )  )  )  ;", "FSAppAttempt   attempt    =    new   FSAppAttempt ( this ,    applicationAttemptId ,    user ,    queue ,    new   ActiveUsersManager ( getRootQueueMetrics (  )  )  ,    rmContext )  ;", "if    ( transferStateFromPreviousAttempt )     {", "attempt . transferStateFromPreviousAttempt ( application . getCurrentAppAttempt (  )  )  ;", "}", "application . setCurrentAppAttempt ( attempt )  ;", "boolean   runnable    =    maxRunningEnforcer . canAppBeRunnable ( queue ,    user )  ;", "queue . addApp ( attempt ,    runnable )  ;", "if    ( runnable )     {", "maxRunningEnforcer . trackRunnableApp ( attempt )  ;", "} else    {", "maxRunningEnforcer . trackNonRunnableApp ( attempt )  ;", "}", "queue . getMetrics (  )  . submitAppAttempt ( user )  ;", ". LOG . info (  (  (  (  \" Added   Application   Attempt    \"     +    applicationAttemptId )     +     \"    to   scheduler   from   user :     \"  )     +    user )  )  ;", "if    ( isAttemptRecovering )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  ( applicationAttemptId    +     \"    is   recovering .    Skipping   notifying   ATTEMPT _ ADDED \"  )  )  ;", "}", "} else    {", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppAttemptEvent ( applicationAttemptId ,    RMAppAttemptEventType . ATTEMPT _ ADDED )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "nodes . put ( node . getNodeID (  )  ,    new   FSSchedulerNode ( node ,    usePortForNodeName )  )  ;", "Resources . addTo ( clusterResource ,    node . getTotalCapability (  )  )  ;", "updateRootQueueMetrics (  )  ;", "queueMgr . getRootQueue (  )  . setSteadyFairShare ( clusterResource )  ;", "queueMgr . getRootQueue (  )  . recomputeSteadyShares (  )  ;", ". LOG . info (  (  (  (  \" Added   node    \"     +     ( node . getNodeAddress (  )  )  )     +     \"    cluster   capacity :     \"  )     +     ( clusterResource )  )  )  ;", "}", "METHOD_END"], "methodName": ["addNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   queue    =    null ;", "String   appRejectMsg    =    null ;", "try    {", "QueuePlacementPolicy   placementPolicy    =    allocConf . getPlacementPolicy (  )  ;", "queueName    =    placementPolicy . assignAppToQueue ( queueName ,    user )  ;", "if    ( queueName    =  =    null )     {", "appRejectMsg    =     \" Application   rejected   by   queue   placement   policy \"  ;", "} else    {", "queue    =    queueMgr . getLeafQueue ( queueName ,    true )  ;", "if    ( queue    =  =    null )     {", "appRejectMsg    =    queueName    +     \"    is   not   a   leaf   queue \"  ;", "}", "}", "}    catch    ( IOException   ioe )     {", "appRejectMsg    =     \" Error   assigning   app   to   queue    \"     +    queueName ;", "}", "if    (  ( appRejectMsg    !  =    null )     &  &     ( rmApp    !  =    null )  )     {", ". LOG . error ( appRejectMsg )  ;", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppRejectedEvent ( rmApp . getApplicationId (  )  ,    appRejectMsg )  )  ;", "return   null ;", "}", "if    ( rmApp    !  =    null )     {", "rmApp . setQueue ( queue . getName (  )  )  ;", "} else    {", ". LOG . error (  \" Couldn ' t   find   RM   app   to   set   queue   name   on \"  )  ;", "}", "return   queue ;", "}", "METHOD_END"], "methodName": ["assignToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "FSAppAttempt   reservedAppSchedulable    =    node . getReservedAppSchedulable (  )  ;", "if    ( reservedAppSchedulable    !  =    null )     {", "Priority   reservedPriority    =    node . getReservedContainer (  )  . getReservedPriority (  )  ;", "if    (  !  ( reservedAppSchedulable . hasContainerForNode ( reservedPriority ,    node )  )  )     {", ". LOG . info (  (  (  (  \" Releasing   reservation   that   cannot   be   satisfied   for   application    \"     +     ( reservedAppSchedulable . getApplicationAttemptId (  )  )  )     +     \"    on   node    \"  )     +    node )  )  ;", "reservedAppSchedulable . unreserve ( reservedPriority ,    node )  ;", "reservedAppSchedulable    =    null ;", "} else    {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Trying   to   fulfill   reservation   for   application    \"     +     ( reservedAppSchedulable . getApplicationAttemptId (  )  )  )     +     \"    on   node :     \"  )     +    node )  )  ;", "}", "node . getReservedAppSchedulable (  )  . assignReservedContainer ( node )  ;", "}", "}", "if    ( reservedAppSchedulable    =  =    null )     {", "int   assignedContainers    =     0  ;", "while    (  ( node . getReservedContainer (  )  )     =  =    null )     {", "boolean   assignedContainer    =    false ;", "if    (  !  ( queueMgr . getRootQueue (  )  . assignContainer ( node )  . equals ( Resources . none (  )  )  )  )     {", "assignedContainers +  +  ;", "assignedContainer    =    true ;", "}", "if    (  ! assignedContainer )     {", "break ;", "}", "if    (  !  ( assignMultiple )  )     {", "break ;", "}", "if    (  ( assignedContainers    >  =     ( maxAssign )  )     &  &     (  ( maxAssign )     >     0  )  )     {", "break ;", "}", "}", "}", "updateRootQueueMetrics (  )  ;", "}", "METHOD_END"], "methodName": ["attemptScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "long   start    =    getClock (  )  . getTime (  )  ;", "List < NodeId >    nodeIdList    =    new   ArrayList < NodeId >  ( nodes . keySet (  )  )  ;", "synchronized ( this )     {", "Collections . sort ( nodeIdList ,    nodeAvailableResourceComparator )  ;", "}", "for    ( NodeId   nodeId    :    nodeIdList )     {", "FSSchedulerNode   node    =    getFSSchedulerNode ( nodeId )  ;", "try    {", "if    (  ( node    !  =    null )     &  &     ( Resources . fitsIn ( minimumAllocation ,    node . getAvailableResource (  )  )  )  )     {", "attemptScheduling ( node )  ;", "}", "}    catch    ( Throwable   ex )     {", ". LOG . error (  (  (  (  \" Error   while   attempting   scheduling   for   node    \"     +    node )     +     \"  :     \"  )     +     ( ex . toString (  )  )  )  ,    ex )  ;", "}", "}", "long   duration    =     ( getClock (  )  . getTime (  )  )     -    start ;", "fsOpDurations . addContinuousSchedulingRunDuration ( duration )  ;", "}", "METHOD_END"], "methodName": ["continuousSchedulingAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "boolean   wasRunnable    =    oldQueue . removeApp ( attempt )  ;", "boolean   nowRunnable    =    maxRunningEnforcer . canAppBeRunnable ( newQueue ,    attempt . getUser (  )  )  ;", "if    ( wasRunnable    &  &     (  ! nowRunnable )  )     {", "throw   new   IllegalStateException (  (  (  \" Should   have   already   verified   that   app    \"     +     ( attempt . getApplicationId (  )  )  )     +     \"    would   be   runnable   in   new   queue \"  )  )  ;", "}", "if    ( wasRunnable )     {", "maxRunningEnforcer . untrackRunnableApp ( attempt )  ;", "} else", "if    ( nowRunnable )     {", "maxRunningEnforcer . untrackNonRunnableApp ( attempt )  ;", "}", "attempt . move ( newQueue )  ;", "app . setQueue ( newQueue )  ;", "newQueue . addApp ( attempt ,    nowRunnable )  ;", "if    ( nowRunnable )     {", "maxRunningEnforcer . trackRunnableApp ( attempt )  ;", "}", "if    ( wasRunnable )     {", "maxRunningEnforcer . updateRunnabilityOnAppRemoval ( attempt ,    oldQueue )  ;", "}", "}", "METHOD_END"], "methodName": ["executeMove"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "String   name 1     =    queue 1  . getName (  )  ;", "String   name 2     =    queue 2  . getName (  )  ;", "int   lastPeriodIndex    =     -  1  ;", "for    ( int   i    =     0  ;    i    <     ( Math . max ( name 1  . length (  )  ,    name 2  . length (  )  )  )  ;    i +  +  )     {", "if    (  (  (  ( name 1  . length (  )  )     <  =    i )     |  |     (  ( name 2  . length (  )  )     <  =    i )  )     |  |     (  ( name 1  . charAt ( i )  )     !  =     ( name 2  . charAt ( i )  )  )  )     {", "return   queueMgr . getQueue ( name 1  . substring (  0  ,    lastPeriodIndex )  )  ;", "} else", "if    (  ( name 1  . charAt ( i )  )     =  =     '  .  '  )     {", "lastPeriodIndex    =    i ;", "}", "}", "return   queue 1  ;", "}", "METHOD_END"], "methodName": ["findLowestCommonAncestorQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   allocConf ;", "}", "METHOD_END"], "methodName": ["getAllocationConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "double   weight    =     1  .  0  ;", "if    ( sizeBasedWeight )     {", "weight    =     ( Math . log 1 p ( app . getDemand (  )  . getMemory (  )  )  )     /     ( Math . log (  2  )  )  ;", "}", "weight    *  =    app . getPriority (  )  . getPriority (  )  ;", "if    (  ( weightAdjuster )     !  =    null )     {", "weight    =    weightAdjuster . adjustWeight ( app ,    weight )  ;", "}", "ResourceWeights   Weights    =    app . getResourceWeights (  )  ;", "Weights . setWeight (  (  ( float )     ( weight )  )  )  ;", "return   Weights ;", "}", "METHOD_END"], "methodName": ["getAppWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   clock ;", "}", "METHOD_END"], "methodName": ["getClock"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["getConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   rmContext . getContainerTokenSecretManager (  )  ;", "}", "METHOD_END"], "methodName": ["getContainerTokenSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   continuousSchedulingSleepMs ;", "}", "METHOD_END"], "methodName": ["getContinuousSchedulingSleepMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   eventLog ;", "}", "METHOD_END"], "methodName": ["getEventLog"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodes . get ( nodeId )  ;", "}", "METHOD_END"], "methodName": ["getFSSchedulerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   incrAllocation ;", "}", "METHOD_END"], "methodName": ["getIncrementResourceCapability"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodeLocalityDelayMs ;", "}", "METHOD_END"], "methodName": ["getNodeLocalityDelayMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodeLocalityThreshold ;", "}", "METHOD_END"], "methodName": ["getNodeLocalityThreshold"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   queueMgr ;", "}", "METHOD_END"], "methodName": ["getQueueManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   rackLocalityDelayMs ;", "}", "METHOD_END"], "methodName": ["getRackLocalityDelayMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   rackLocalityThreshold ;", "}", "METHOD_END"], "methodName": ["getRackLocalityThreshold"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   super . getApplicationAttempt ( appAttemptId )  ;", "}", "METHOD_END"], "methodName": ["getSchedulerApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    new   FairSchedulerConfiguration ( conf )  ;", "validateConf ( this . conf )  ;", "minimumAllocation    =    this . conf . getMinimumAllocation (  )  ;", "maximumAllocation    =    this . conf . getMaximumAllocation (  )  ;", "incrAllocation    =    this . conf . getIncrementAllocation (  )  ;", "continuousSchedulingEnabled    =    this . conf . isContinuousSchedulingEnabled (  )  ;", "continuousSchedulingSleepMs    =    this . conf . getContinuousSchedulingSleepMs (  )  ;", "nodeLocalityThreshold    =    this . conf . getLocalityThresholdNode (  )  ;", "rackLocalityThreshold    =    this . conf . getLocalityThresholdRack (  )  ;", "nodeLocalityDelayMs    =    this . conf . getLocalityDelayNodeMs (  )  ;", "rackLocalityDelayMs    =    this . conf . getLocalityDelayRackMs (  )  ;", "preemptionEnabled    =    this . conf . getPreemptionEnabled (  )  ;", "preemptionUtilizationThreshold    =    this . conf . getPreemptionUtilizationThreshold (  )  ;", "assignMultiple    =    this . conf . getAssignMultiple (  )  ;", "maxAssign    =    this . conf . getMaxAssign (  )  ;", "sizeBasedWeight    =    this . conf . getSizeBasedWeight (  )  ;", "preemptionInterval    =    this . conf . getPreemptionInterval (  )  ;", "waitTimeBeforeKill    =    this . conf . getWaitTimeBeforeKill (  )  ;", "usePortForNodeName    =    this . conf . getUsePortForNodeName (  )  ;", "updateInterval    =    this . conf . getUpdateInterval (  )  ;", "if    (  ( updateInterval )     <     0  )     {", "updateInterval    =    FairSchedulerConfiguration . DEFAULT _ UPDATE _ INTERVAL _ MS ;", "FairScheduler . LOG . warn (  (  (  (  ( FairSchedulerConfiguration . UPDATE _ INTERVAL _ MS )     +     \"    is   invalid ,    so   using   default   value    \"  )     +     (  +  ( FairSchedulerConfiguration . DEFAULT _ UPDATE _ INTERVAL _ MS )  )  )     +     \"    ms   instead \"  )  )  ;", "}", "rootMetrics    =    FSQueueMetrics . forQueue (  \" root \"  ,    null ,    true ,    conf )  ;", "fsOpDurations    =    FSOpDurations . getInstance ( true )  ;", "this . applications    =    new   ConcurrentHashMap < ApplicationId ,    SchedulerApplication < FSAppAttempt >  >  (  )  ;", "this . eventLog    =    new   FairSchedulerEventLog (  )  ;", "eventLog . init ( this . conf )  ;", "allocConf    =    new   AllocationConfiguration ( conf )  ;", "try    {", "queueMgr . initialize ( conf )  ;", "}    catch    ( Exception   e )     {", "throw   new   IOException (  \" Failed   to   start   FairScheduler \"  ,    e )  ;", "}", "updateThread    =    new   FairScheduler . UpdateThread (  )  ;", "updateThread . setName (  \" FairSchedulerUpdateThread \"  )  ;", "updateThread . setDaemon ( true )  ;", "if    ( continuousSchedulingEnabled )     {", "schedulingThread    =    new   FairScheduler . ContinuousSchedulingThread (  )  ;", "schedulingThread . setName (  \" FairSchedulerContinuousScheduling \"  )  ;", "schedulingThread . setDaemon ( true )  ;", "}", "allocsLoader . init ( conf )  ;", "allocsLoader . setReloadListener ( new   FairScheduler . AllocationReloadListener (  )  )  ;", "try    {", "allocsLoader . reloadAllocations (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   IOException (  \" Failed   to   initialize   FairScheduler \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["initScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "return   continuousSchedulingEnabled ;", "}", "METHOD_END"], "methodName": ["isContinuousSchedulingEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "Resource   desiredFairShare    =    Resources . min ( FairScheduler . RESOURCE _ CALCULATOR ,    clusterResource ,    Resources . multiply ( sched . getFairShare (  )  ,     0  .  5  )  ,    sched . getDemand (  )  )  ;", "return   Resources . lessThan ( FairScheduler . RESOURCE _ CALCULATOR ,    clusterResource ,    sched . getResourceUsage (  )  ,    desiredFairShare )  ;", "}", "METHOD_END"], "methodName": ["isStarvedForFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "Resource   desiredShare    =    Resources . min ( FairScheduler . RESOURCE _ CALCULATOR ,    clusterResource ,    sched . getMinShare (  )  ,    sched . getDemand (  )  )  ;", "return   Resources . lessThan ( FairScheduler . RESOURCE _ CALCULATOR ,    clusterResource ,    sched . getResourceUsage (  )  ,    desiredShare )  ;", "}", "METHOD_END"], "methodName": ["isStarvedForMinShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "long   start    =    getClock (  )  . getTime (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" nodeUpdate :     \"     +    nm )     +     \"    cluster   capacity :     \"  )     +     ( clusterResource )  )  )  ;", "}", "eventLog . log (  \" HEARTBEAT \"  ,    nm . getHostName (  )  )  ;", "FSSchedulerNode   node    =    getFSSchedulerNode ( nm . getNodeID (  )  )  ;", "SchedulerUtils . updateResourceIfChanged ( node ,    nm ,    clusterResource ,     . LOG )  ;", "List < UpdatedContainerInfo >    containerInfoList    =    nm . pullContainerUpdates (  )  ;", "List < ContainerStatus >    newlyLaunchedContainers    =    new   ArrayList < ContainerStatus >  (  )  ;", "List < ContainerStatus >    completedContainers    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( UpdatedContainerInfo   containerInfo    :    containerInfoList )     {", "newlyLaunchedContainers . addAll ( containerInfo . getNewlyLaunchedContainers (  )  )  ;", "completedContainers . addAll ( containerInfo . getCompletedContainers (  )  )  ;", "}", "for    ( ContainerStatus   launchedContainer    :    newlyLaunchedContainers )     {", "containerLaunchedOnNode ( launchedContainer . getContainerId (  )  ,    node )  ;", "}", "for    ( ContainerStatus   completedContainer    :    completedContainers )     {", "ContainerId   containerId    =    completedContainer . getContainerId (  )  ;", ". LOG . debug (  (  \" Container   FINISHED :     \"     +    containerId )  )  ;", "completedContainer ( getRMContainer ( containerId )  ,    completedContainer ,    RMContainerEventType . FINISHED )  ;", "}", "if    ( continuousSchedulingEnabled )     {", "if    (  !  ( completedContainers . isEmpty (  )  )  )     {", "attemptScheduling ( node )  ;", "}", "} else    {", "attemptScheduling ( node )  ;", "}", "long   duration    =     ( getClock (  )  . getTime (  )  )     -    start ;", "fsOpDurations . addNodeUpdateDuration ( duration )  ;", "}", "METHOD_END"], "methodName": ["nodeUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "long   start    =    getClock (  )  . getTime (  )  ;", "if    ( Resources . equals ( toPreempt ,    Resources . none (  )  )  )     {", "return ;", "}", "Iterator < RMContainer >    warnedIter    =    warnedContainers . iterator (  )  ;", "while    ( warnedIter . hasNext (  )  )     {", "RMContainer   container    =    warnedIter . next (  )  ;", "if    (  (  (  ( container . getState (  )  )     =  =     ( RMContainerState . RUNNING )  )     |  |     (  ( container . getState (  )  )     =  =     ( RMContainerState . ALLOCATED )  )  )     &  &     ( Resources . greaterThan (  . RESOURCE _ CALCULATOR ,    clusterResource ,    toPreempt ,    Resources . none (  )  )  )  )     {", "warnOrKillContainer ( container )  ;", "Resources . subtractFrom ( toPreempt ,    container . getContainer (  )  . getResource (  )  )  ;", "} else    {", "warnedIter . remove (  )  ;", "}", "}", "try    {", "for    ( FSLeafQueue   queue    :    getQueueManager (  )  . getLeafQueues (  )  )     {", "for    ( FSAppAttempt   app    :    queue . getRunnableAppSchedulables (  )  )     {", "app . resetPreemptedResources (  )  ;", "}", "}", "while    ( Resources . greaterThan (  . RESOURCE _ CALCULATOR ,    clusterResource ,    toPreempt ,    Resources . none (  )  )  )     {", "RMContainer   container    =    getQueueManager (  )  . getRootQueue (  )  . preemptContainer (  )  ;", "if    ( container    =  =    null )     {", "break ;", "} else    {", "warnOrKillContainer ( container )  ;", "warnedContainers . add ( container )  ;", "Resources . subtractFrom ( toPreempt ,    container . getContainer (  )  . getResource (  )  )  ;", "}", "}", "}    finally    {", "for    ( FSLeafQueue   queue    :    getQueueManager (  )  . getLeafQueues (  )  )     {", "for    ( FSAppAttempt   app    :    queue . getRunnableAppSchedulables (  )  )     {", "app . clearPreemptedResources (  )  ;", "}", "}", "}", "long   duration    =     ( getClock (  )  . getTime (  )  )     -    start ;", "fsOpDurations . addPreemptCallDuration ( duration )  ;", "}", "METHOD_END"], "methodName": ["preemptResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( shouldAttemptPreemption (  )  )  )     {", "return ;", "}", "long   curTime    =    getClock (  )  . getTime (  )  ;", "if    (  ( curTime    -     ( lastPreemptCheckTime )  )     <     ( preemptionInterval )  )     {", "return ;", "}", "lastPreemptCheckTime    =    curTime ;", "Resource   resToPreempt    =    Resources . clone ( Resources . none (  )  )  ;", "for    ( FSLeafQueue   sched    :    queueMgr . getLeafQueues (  )  )     {", "Resources . addTo ( resToPreempt ,    resToPreempt ( sched ,    curTime )  )  ;", "}", "if    ( Resources . greaterThan (  . RESOURCE _ CALCULATOR ,    clusterResource ,    resToPreempt ,    Resources . none (  )  )  )     {", "preemptResources ( resToPreempt )  ;", "}", "}", "METHOD_END"], "methodName": ["preemptTasksIfNecessary"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FSAppAttempt >    application    =    applications . get ( applicationId )  ;", "if    ( application    =  =    null )     {", ". LOG . warn (  (  \" Couldn ' t   find   application    \"     +    applicationId )  )  ;", "return ;", "}", "application . stop ( finalState )  ;", "applications . remove ( applicationId )  ;", "}", "METHOD_END"], "methodName": ["removeApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "FairScheduler . LOG . info (  (  (  (  (  \" Application    \"     +    applicationAttemptId )     +     \"    is   done .  \"  )     +     \"    finalState =  \"  )     +    rmAppAttemptFinalState )  )  ;", "SchedulerApplication < FSAppAttempt >    application    =    applications . get ( applicationAttemptId . getApplicationId (  )  )  ;", "FSAppAttempt   attempt    =    getSchedulerApp ( applicationAttemptId )  ;", "if    (  ( attempt    =  =    null )     |  |     ( application    =  =    null )  )     {", "FairScheduler . LOG . info (  (  (  \" Unknown   application    \"     +    applicationAttemptId )     +     \"    has   completed !  \"  )  )  ;", "return ;", "}", "for    ( RMContainer   rmContainer    :    attempt . getLiveContainers (  )  )     {", "if    ( keepContainers    &  &     ( rmContainer . getState (  )  . equals ( RMContainerState . RUNNING )  )  )     {", "FairScheduler . LOG . info (  (  \" Skip   killing    \"     +     ( rmContainer . getContainerId (  )  )  )  )  ;", "continue ;", "}", "completedContainer ( rmContainer ,    SchedulerUtils . createAbnormalContainerStatus ( rmContainer . getContainerId (  )  ,    SchedulerUtils . COMPLETED _ APPLICATION )  ,    RMContainerEventType . KILL )  ;", "}", "for    ( RMContainer   rmContainer    :    attempt . getReservedContainers (  )  )     {", "completedContainer ( rmContainer ,    SchedulerUtils . createAbnormalContainerStatus ( rmContainer . getContainerId (  )  ,     \" Application   Complete \"  )  ,    RMContainerEventType . KILL )  ;", "}", "attempt . stop ( rmAppAttemptFinalState )  ;", "FSLeafQueue   queue    =    queueMgr . getLeafQueue ( attempt . getQueue (  )  . getQueueName (  )  ,    false )  ;", "boolean   wasRunnable    =    queue . removeApp ( attempt )  ;", "if    ( wasRunnable )     {", "maxRunningEnforcer . untrackRunnableApp ( attempt )  ;", "maxRunningEnforcer . updateRunnabilityOnAppRemoval ( attempt ,    attempt . getQueue (  )  )  ;", "} else    {", "maxRunningEnforcer . untrackNonRunnableApp ( attempt )  ;", "}", "}", "METHOD_END"], "methodName": ["removeApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "FSSchedulerNode   node    =    getFSSchedulerNode ( rmNode . getNodeID (  )  )  ;", "if    ( node    =  =    null )     {", "return ;", "}", "Resources . subtractFrom ( clusterResource ,    rmNode . getTotalCapability (  )  )  ;", "updateRootQueueMetrics (  )  ;", "List < RMContainer >    runningContainers    =    node . getRunningContainers (  )  ;", "for    ( RMContainer   container    :    runningContainers )     {", "completedContainer ( container ,    SchedulerUtils . createAbnormalContainerStatus ( container . getContainerId (  )  ,    SchedulerUtils . LOST _ CONTAINER )  ,    RMContainerEventType . KILL )  ;", "}", "RMContainer   reservedContainer    =    node . getReservedContainer (  )  ;", "if    ( reservedContainer    !  =    null )     {", "completedContainer ( reservedContainer ,    SchedulerUtils . createAbnormalContainerStatus ( reservedContainer . getContainerId (  )  ,    SchedulerUtils . LOST _ CONTAINER )  ,    RMContainerEventType . KILL )  ;", "}", "nodes . remove ( rmNode . getNodeID (  )  )  ;", "queueMgr . getRootQueue (  )  . setSteadyFairShare ( clusterResource )  ;", "queueMgr . getRootQueue (  )  . recomputeSteadyShares (  )  ;", ". LOG . info (  (  (  (  \" Removed   node    \"     +     ( rmNode . getNodeAddress (  )  )  )     +     \"    cluster   capacity :     \"  )     +     ( clusterResource )  )  )  ;", "}", "METHOD_END"], "methodName": ["removeNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "String   queue    =    sched . getName (  )  ;", "long   minShareTimeout    =    allocConf . getMinSharePreemptionTimeout ( queue )  ;", "long   fairShareTimeout    =    allocConf . getFairSharePreemptionTimeout (  )  ;", "Resource   resDueToMinShare    =    Resources . none (  )  ;", "Resource   resDueToFairShare    =    Resources . none (  )  ;", "if    (  ( curTime    -     ( sched . getLastTimeAtMinShare (  )  )  )     >    minShareTimeout )     {", "Resource   target    =    Resources . min (  . RESOURCE _ CALCULATOR ,    clusterResource ,    sched . getMinShare (  )  ,    sched . getDemand (  )  )  ;", "resDueToMinShare    =    Resources . max (  . RESOURCE _ CALCULATOR ,    clusterResource ,    Resources . none (  )  ,    Resources . subtract ( target ,    sched . getResourceUsage (  )  )  )  ;", "}", "if    (  ( curTime    -     ( sched . getLastTimeAtHalfFairShare (  )  )  )     >    fairShareTimeout )     {", "Resource   target    =    Resources . min (  . RESOURCE _ CALCULATOR ,    clusterResource ,    sched . getFairShare (  )  ,    sched . getDemand (  )  )  ;", "resDueToFairShare    =    Resources . max (  . RESOURCE _ CALCULATOR ,    clusterResource ,    Resources . none (  )  ,    Resources . subtract ( target ,    sched . getResourceUsage (  )  )  )  ;", "}", "Resource   resToPreempt    =    Resources . max (  . RESOURCE _ CALCULATOR ,    clusterResource ,    resDueToMinShare ,    resDueToFairShare )  ;", "if    ( Resources . greaterThan (  . RESOURCE _ CALCULATOR ,    clusterResource ,    resToPreempt ,    Resources . none (  )  )  )     {", "String   message    =     (  (  (  (  (  (  \" Should   preempt    \"     +    resToPreempt )     +     \"    res   for   queue    \"  )     +     ( sched . getName (  )  )  )     +     \"  :    resDueToMinShare    =     \"  )     +    resDueToMinShare )     +     \"  ,    resDueToFairShare    =     \"  )     +    resDueToFairShare ;", ". LOG . info ( message )  ;", "}", "return   resToPreempt ;", "}", "METHOD_END"], "methodName": ["resToPreempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "this . clock    =    clock ;", "}", "METHOD_END"], "methodName": ["setClock"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "this . rmContext    =    rmContext ;", "}", "METHOD_END"], "methodName": ["setRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "if    ( preemptionEnabled )     {", "return    ( preemptionUtilizationThreshold )     <     ( Math . max (  (  (  ( float )     ( rootMetrics . getAllocatedMB (  )  )  )     /     ( clusterR . getMemory (  )  )  )  ,     (  (  ( float )     ( rootMetrics . getAllocatedVirtualCores (  )  )  )     /     ( clusterR . getVirtualCores (  )  )  )  )  )  ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["shouldAttemptPreemption"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "Preconditions . checkNotNull ( updateThread ,     \" updateThread   is   null \"  )  ;", "Preconditions . checkNotNull ( allocsLoader ,     \" allocsLoader   is   null \"  )  ;", "updateThread . start (  )  ;", "if    ( continuousingEnabled )     {", "Preconditions . checkNotNull ( schedulingThread ,     \" schedulingThread   is   null \"  )  ;", "schedulingThread . start (  )  ;", "}", "allocsLoader . start (  )  ;", "}", "METHOD_END"], "methodName": ["startSchedulerThreads"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "long   start    =    getClock (  )  . getTime (  )  ;", "updatePreemptionVariables (  )  ;", "FSQueue   rootQueue    =    queueMgr . getRootQueue (  )  ;", "rootQueue . updateDemand (  )  ;", "rootQueue . setFairShare ( clusterResource )  ;", "rootQueue . recomputeShares (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", "if    (  (  -  -  ( updatesToSkipForDebug )  )     <     0  )     {", "updatesToSkipForDebug    =    UPDATE _ DEBUG _ FREQUENCY ;", ". LOG . debug (  (  (  (  (  (  (  (  \" Cluster   Capacity :     \"     +     ( clusterResource )  )     +     \"       Allocations :     \"  )     +     ( rootMetrics . getAllocatedResources (  )  )  )     +     \"       Availability :     \"  )     +     ( Resource . newInstance ( rootMetrics . getAvailableMB (  )  ,    rootMetrics . getAvailableVirtualCores (  )  )  )  )     +     \"       Demand :     \"  )     +     ( rootQueue . getDemand (  )  )  )  )  ;", "}", "}", "long   duration    =     ( getClock (  )  . getTime (  )  )     -    start ;", "fsOpDurations . addUpdateCallDuration ( duration )  ;", "}", "METHOD_END"], "methodName": ["update"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "long   now    =    getClock (  )  . getTime (  )  ;", "lastPreemptionUpdateTime    =    now ;", "for    ( FSLeafQueue       :    queueMgr . getLeafQueues (  )  )     {", "if    (  !  ( isStarvedForMinShare (  )  )  )     {", ". setLastTimeAtMinShare ( now )  ;", "}", "if    (  !  ( isStarvedForFairShare (  )  )  )     {", ". setLastTimeAtHalfFairShare ( now )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updatePreemptionVariables"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "rootMetrics . setAvailableResourcesToQueue ( Resources . subtract ( clusterResource ,    rootMetrics . getAllocatedResources (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["updateRootQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "int   minMem    =    conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "int   maxMem    =    conf . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  ;", "if    (  ( minMem    <     0  )     |  |     ( minMem    >    maxMem )  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  (  (  (  (  (  \" Invalid   resource   scheduler   memory \"     +     (  \"    allocation   configuration \"     +     \"  ,     \"  )  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )     +     \"  =  \"  )     +    minMem )     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )     +     \"  =  \"  )     +    maxMem )     +     \"  ,    min   should   equal   greater   than    0  \"  )     +     \"  ,    max   should   be   no   smaller   than   min .  \"  )  )  ;", "}", "int   minVcores    =    conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "int   maxVcores    =    conf . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  ;", "if    (  ( minVcores    <     0  )     |  |     ( minVcores    >    maxVcores )  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  (  (  (  (  (  \" Invalid   resource   scheduler   vcores \"     +     (  \"    allocation   configuration \"     +     \"  ,     \"  )  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  )     +     \"  =  \"  )     +    minVcores )     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  )     +     \"  =  \"  )     +    maxVcores )     +     \"  ,    min   should   equal   greater   than    0  \"  )     +     \"  ,    max   should   be   no   smaller   than   min .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "String   queueName    =    targetQueue . getQueueName (  )  ;", "ApplicationAttemptId   appAttId    =    app . getApplicationAttemptId (  )  ;", "FSQueue   lowestCommonAncestor    =    findLowestCommonAncestorQueue ( oldQueue ,    targetQueue )  ;", "Resource   consumption    =    app . getCurrentConsumption (  )  ;", "FSQueue   cur    =    targetQueue ;", "while    ( cur    !  =    lowestCommonAncestor )     {", "if    (  ( cur . getNumRunnableApps (  )  )     =  =     ( allocConf . getQueueMaxApps ( cur . getQueueName (  )  )  )  )     {", "throw   new   exceptions . YarnException (  (  (  (  (  (  (  \" Moving   app   attempt    \"     +    appAttId )     +     \"    to   queue    \"  )     +    queueName )     +     \"    would   violate   queue   maxRunningApps   constraints   on \"  )     +     \"    queue    \"  )     +     ( cur . getQueueName (  )  )  )  )  ;", "}", "if    (  !  ( Resources . fitsIn ( Resources . add ( cur . getResourceUsage (  )  ,    consumption )  ,    cur . getMaxShare (  )  )  )  )     {", "throw   new   exceptions . YarnException (  (  (  (  (  (  (  \" Moving   app   attempt    \"     +    appAttId )     +     \"    to   queue    \"  )     +    queueName )     +     \"    would   violate   queue   maxShare   constraints   on \"  )     +     \"    queue    \"  )     +     ( cur . getQueueName (  )  )  )  )  ;", "}", "cur    =    cur . getParent (  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyMoveDoesNotViolateConstraints"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   appAttemptId    =    container . getApplicationAttemptId (  )  ;", "FSAppAttempt   app    =    getSchedulerApp ( appAttemptId )  ;", "FSLeafQueue   queue    =    app . getQueue (  )  ;", ". LOG . info (  (  (  (  (  (  \" Preempting   container    ( prio =  \"     +     ( container . getContainer (  )  . getPriority (  )  )  )     +     \" res =  \"  )     +     ( container . getContainer (  )  . getResource (  )  )  )     +     \"  )    from   queue    \"  )     +     ( queue . getName (  )  )  )  )  ;", "Long   time    =    app . getContainerPreemptionTime ( container )  ;", "if    ( time    !  =    null )     {", "if    (  ( time    +     ( waitTimeBeforeKill )  )     <     ( getClock (  )  . getTime (  )  )  )     {", "ContainerStatus   status    =    SchedulerUtils . createPreemptedContainerStatus ( container . getContainerId (  )  ,    SchedulerUtils . PREEMPTED _ CONTAINER )  ;", "recoverResourceRequestForContainer ( container )  ;", "completedContainer ( container ,    status ,    RMContainerEventType . KILL )  ;", ". LOG . info (  (  (  (  (  \" Killing   container \"     +    container )     +     \"     ( after   waiting   for   premption   for    \"  )     +     (  ( getClock (  )  . getTime (  )  )     -    time )  )     +     \" ms )  \"  )  )  ;", "}", "} else    {", "app . addPreemption ( container ,    getClock (  )  . getTime (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["warnOrKillContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"}, {"methodBody": ["METHOD_START", "{", "Pattern   pattern    =    Pattern . compile (  (  \"  (  \\  \\ d +  )     ?  \"     +    units )  )  ;", "Matcher   matcher    =    pattern . matcher ( val )  ;", "if    (  !  ( matcher . find (  )  )  )     {", "throw   new   AllocationException (  (  \" Missing   resource :     \"     +    units )  )  ;", "}", "return   Integer . parseInt ( matcher . group (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["findResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( FairSchedulerConfiguration . ASSIGN _ MULTIPLE ,    FairSchedulerConfiguration . DEFAULT _ ASSIGN _ MULTIPLE )  ;", "}", "METHOD_END"], "methodName": ["getAssignMultiple"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getInt ( FairSchedulerConfiguration . CONTINUOUS _ SCHEDULING _ SLEEP _ MS ,    FairSchedulerConfiguration . DEFAULT _ CONTINUOUS _ SCHEDULING _ SLEEP _ MS )  ;", "}", "METHOD_END"], "methodName": ["getContinuousSchedulingSleepMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   get ( FairSchedulerConfiguration . EVENT _ LOG _ DIR ,     (  (  ( new   File ( System . getProperty (  \" hadoop . log . dir \"  ,     \"  / tmp /  \"  )  )  . getAbsolutePath (  )  )     +     ( File . separator )  )     +     \" fairscheduler \"  )  )  ;", "}", "METHOD_END"], "methodName": ["getEventlogDir"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   incrementMemory    =    getInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ,    FairSchedulerConfiguration . DEFAULT _ RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB )  ;", "int   incrementCores    =    getInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ VCORES ,    FairSchedulerConfiguration . DEFAULT _ RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ VCORES )  ;", "return   Resources . createResource ( incrementMemory ,    incrementCores )  ;", "}", "METHOD_END"], "methodName": ["getIncrementAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getLong ( FairSchedulerConfiguration . LOCALITY _ DELAY _ NODE _ MS ,    FairSchedulerConfiguration . DEFAULT _ LOCALITY _ DELAY _ NODE _ MS )  ;", "}", "METHOD_END"], "methodName": ["getLocalityDelayNodeMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getLong ( FairSchedulerConfiguration . LOCALITY _ DELAY _ RACK _ MS ,    FairSchedulerConfiguration . DEFAULT _ LOCALITY _ DELAY _ RACK _ MS )  ;", "}", "METHOD_END"], "methodName": ["getLocalityDelayRackMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getFloat ( FairSchedulerConfiguration . LOCALITY _ THRESHOLD _ NODE ,    FairSchedulerConfiguration . DEFAULT _ LOCALITY _ THRESHOLD _ NODE )  ;", "}", "METHOD_END"], "methodName": ["getLocalityThresholdNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getFloat ( FairSchedulerConfiguration . LOCALITY _ THRESHOLD _ RACK ,    FairSchedulerConfiguration . DEFAULT _ LOCALITY _ THRESHOLD _ RACK )  ;", "}", "METHOD_END"], "methodName": ["getLocalityThresholdRack"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getInt ( FairSchedulerConfiguration . MAX _ ASSIGN ,    FairSchedulerConfiguration . DEFAULT _ MAX _ ASSIGN )  ;", "}", "METHOD_END"], "methodName": ["getMaxAssign"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   mem    =    getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  ;", "int   cpu    =    getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  ;", "return   Rs . createR ( mem ,    cpu )  ;", "}", "METHOD_END"], "methodName": ["getMaximumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "int   mem    =    getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "int   cpu    =    getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES )  ;", "return   Rs . createR ( mem ,    cpu )  ;", "}", "METHOD_END"], "methodName": ["getMinimumAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( FairSchedulerConfiguration . PREEMPTION ,    FairSchedulerConfiguration . DEFAULT _ PREEMPTION )  ;", "}", "METHOD_END"], "methodName": ["getPreemptionEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getInt ( FairSchedulerConfiguration . PREEMPTION _ INTERVAL ,    FairSchedulerConfiguration . DEFAULT _ PREEMPTION _ INTERVAL )  ;", "}", "METHOD_END"], "methodName": ["getPreemptionInterval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getFloat ( FairSchedulerConfiguration . PREEMPTION _ THRESHOLD ,    FairSchedulerConfiguration . DEFAULT _ PREEMPTION _ THRESHOLD )  ;", "}", "METHOD_END"], "methodName": ["getPreemptionUtilizationThreshold"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( FairSchedulerConfiguration . SIZE _ BASED _ WEIGHT ,    FairSchedulerConfiguration . DEFAULT _ SIZE _ BASED _ WEIGHT )  ;", "}", "METHOD_END"], "methodName": ["getSizeBasedWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getLong ( FairSchedulerConfiguration . UPDATE _ INTERVAL _ MS ,    FairSchedulerConfiguration . DEFAULT _ UPDATE _ INTERVAL _ MS )  ;", "}", "METHOD_END"], "methodName": ["getUpdateInterval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( RM _ SCHEDULER _ INCLUDE _ PORT _ IN _ NODE _ NAME ,    DEFAULT _ RM _ SCHEDULER _ USE _ PORT _ FOR _ NODE _ NAME )  ;", "}", "METHOD_END"], "methodName": ["getUsePortForNodeName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getInt ( FairSchedulerConfiguration . WAIT _ TIME _ BEFORE _ KILL ,    FairSchedulerConfiguration . DEFAULT _ WAIT _ TIME _ BEFORE _ KILL )  ;", "}", "METHOD_END"], "methodName": ["getWaitTimeBeforeKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( FairSchedulerConfiguration . CONTINUOUS _ SCHEDULING _ ENABLED ,    FairSchedulerConfiguration . DEFAULT _ CONTINUOUS _ SCHEDULING _ ENABLED )  ;", "}", "METHOD_END"], "methodName": ["isContinuousSchedulingEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   getBoolean ( FairSchedulerConfiguration . EVENT _ LOG _ ENABLED ,    FairSchedulerConfiguration . DEFAULT _ EVENT _ LOG _ ENABLED )  ;", "}", "METHOD_END"], "methodName": ["isEventLogEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "try    {", "int   memory    =     . findResource ( val ,     \" mb \"  )  ;", "int   vcores    =     . findResource ( val ,     \" vcores \"  )  ;", "return   BuilderUtils . newResource ( memory ,    vcores )  ;", "}    catch    ( AllocationConfigurationException   ex )     {", "throw   ex ;", "}    catch    ( Exception   ex )     {", "throw   new   AllocationConfigurationException (  \" Error   reading   resource   config \"  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["parseResourceConfigValue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   logFile ;", "}", "METHOD_END"], "methodName": ["getLogFile"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "if    ( conf . isEventLogEnabled (  )  )     {", "try    {", "logDir    =    conf . getEventlogDir (  )  ;", "File   logDirFile    =    new   File ( logDir )  ;", "if    (  !  ( logDirFile . exists (  )  )  )     {", "if    (  !  ( logDirFile . mkdirs (  )  )  )     {", "throw   new   IOException (  (  \" Mkdirs   failed   to   create    \"     +     ( logDirFile . toString (  )  )  )  )  ;", "}", "}", "String   username    =    System . getProperty (  \" user . name \"  )  ;", "logFile    =    String . format (  \"  % s % shadoop -  % s - fairscheduler . log \"  ,    logDir ,    File . separator ,    username )  ;", "logDisabled    =    false ;", "PatternLayout   layout    =    new   PatternLayout (  \"  % d { ISO 8  6  0  1  }  \\ t % m % n \"  )  ;", "appender    =    new   DailyRollingFileAppender ( layout ,    logFile ,     \"  '  .  ' yyyy - MM - dd \"  )  ;", "appender . activateOptions (  )  ;", ". LOG . info (  (  \" Initialized   fair   scheduler   event   log ,    logging   to    \"     +     ( logFile )  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error (  \" Failed   to   initialize   fair   scheduler   event   log .    Disabling   it .  \"  ,    e )  ;", "logDisabled    =    true ;", "}", "} else    {", "logDisabled    =    true ;", "}", "return    !  ( logDisabled )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "return    !  ( logDisabled )  ;", "}", "METHOD_END"], "methodName": ["isEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    ( logDisabled )", "return ;", "StringBuffer   buffer    =    new   StringBuffer (  )  ;", "buffer . append ( eventType )  ;", "for    ( Object   param    :    params )     {", "buffer . append (  \"  \\ t \"  )  ;", "buffer . append ( param )  ;", "}", "String   message    =    buffer . toString (  )  ;", "Logger   logger    =    Logger . getLogger ( getClass (  )  )  ;", "appender . append ( new   LoggingEvent (  \"  \"  ,    logger ,    Level . INFO ,    message ,    null )  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Failed   to   append   to   fair   scheduler   event   log \"  ,    e )  ;", "logDisabled    =    true ;", "}", "}", "METHOD_END"], "methodName": ["log"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  ( appender )     !  =    null )", "appender . close (  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Failed   to   close   fair   scheduler   event   log \"  ,    e )  ;", "logDisabled    =    true ;", "}", "}", "METHOD_END"], "methodName": ["shutdown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appIdImpl    =    ApplicationId . newInstance (  0  ,    appId )  ;", "return   ApplicationAttemptId . newInstance ( appIdImpl ,    attemptId )  ;", "}", "METHOD_END"], "methodName": ["createAppAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    resourceManager . getRMContext (  )  ;", "RMApp   rmApp    =    new   RMAppImpl ( attId . getApplicationId (  )  ,    rmContext ,    conf ,    null ,    null ,    null ,    ApplicationSubmissionContext . newInstance ( null ,    null ,    null ,    null ,    null ,    false ,    false ,     0  ,    amResource ,    null )  ,    null ,    null ,     0  ,    null ,    null )  ;", "rmContext . getRMApps (  )  . put ( attId . getApplicationId (  )  ,    rmApp )  ;", "AppAddedSchedulerEvent   appAddedEvent    =    new   AppAddedSchedulerEvent ( attId . getApplicationId (  )  ,    queue ,    user )  ;", "handle ( appAddedEvent )  ;", "AppAttemptAddedSchedulerEvent   attempAddedEvent    =    new   AppAttemptAddedSchedulerEvent ( attId ,    false )  ;", "handle ( attempAddedEvent )  ;", "}", "METHOD_END"], "methodName": ["createApplicationWithAMResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     0  )  ;", "conf . setInt ( Configuration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ,     1  0  2  4  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,     1  0  2  4  0  )  ;", "conf . setBoolean ( Configuration . ASSIGN _ MULTIPLE ,    false )  ;", "conf . setFloat ( Configuration . PREEMPTION _ THRESHOLD ,     0  .  0 F )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    FairSchedulerTestBase . recordFactory . newRecordInstance ( ResourceRequest . class )  ;", "request . setCapability ( BuilderUtils . newResource ( memory ,    vcores )  )  ;", "request . setResourceName ( host )  ;", "request . setNumContainers ( numContainers )  ;", "Priority   prio    =    FairSchedulerTestBase . recordFactory . newRecordInstance ( Priority . class )  ;", "prio . setPriority ( priority )  ;", "request . setPriority ( prio )  ;", "request . setRelaxLocality ( relaxLocality )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["createResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "return   createResourceRequest ( memory ,     1  ,    host ,    priority ,    numContainers ,    relaxLocality )  ;", "}", "METHOD_END"], "methodName": ["createResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulingRequest ( memory ,    vcores ,    queueId ,    userId ,     1  )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulingRequest ( memory ,    vcores ,    queueId ,    userId ,    numContainers ,     1  )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   id    =    createAppAttemptId (  (  ( this . APP _ ID )  +  +  )  ,     (  ( this . ATTEMPT _ ID )  +  +  )  )  ;", "scheduler . addApplication ( id . getApplicationId (  )  ,    queueId ,    userId ,    false )  ;", "if    ( scheduler . getSchedulerApplications (  )  . containsKey ( id . getApplicationId (  )  )  )     {", "scheduler . addApplicationAttempt ( id ,    false ,    false )  ;", "}", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   request    =    createResourceRequest ( memory ,    vcores ,    ANY ,    priority ,    numContainers ,    true )  ;", "ask . add ( request )  ;", "scheduler . allocate ( id ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "RMApp   rmApp    =    mock ( RMApp . class )  ;", "RMAppAttempt   rmAppAttempt    =    mock ( RMAppAttempt . class )  ;", "when ( rmApp . getCurrentAppAttempt (  )  )  . thenReturn ( rmAppAttempt )  ;", "when ( rmAppAttempt . getRMAppAttemptMetrics (  )  )  . thenReturn ( new   RMAppAttemptMetrics ( id )  )  ;", "resourceManager . getRMContext (  )  . getRMApps (  )  . put ( id . getApplicationId (  )  ,    rmApp )  ;", "return   id ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulingRequest ( memory ,    queueId ,    userId ,     1  )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulingRequest ( memory ,    queueId ,    userId ,    numContainers ,     1  )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulingRequest ( memory ,     1  ,    queueId ,    userId ,    numContainers ,    priority )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   id    =    createAppAttemptId (  (  ( this . APP _ ID )  +  +  )  ,     (  ( this . ATTEMPT _ ID )  +  +  )  )  ;", "scheduler . addApplication ( id . getApplicationId (  )  ,    queueId ,    userId ,    false )  ;", "if    ( scheduler . getSchedulerApplications (  )  . containsKey ( id . getApplicationId (  )  )  )     {", "scheduler . addApplicationAttempt ( id ,    false ,    false )  ;", "}", "scheduler . allocate ( id ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "RMApp   rmApp    =    mock ( RMApp . class )  ;", "RMAppAttempt   rmAppAttempt    =    mock ( RMAppAttempt . class )  ;", "when ( rmApp . getCurrentAppAttempt (  )  )  . thenReturn ( rmAppAttempt )  ;", "when ( rmAppAttempt . getRMAppAttemptMetrics (  )  )  . thenReturn ( new   RMAppAttemptMetrics ( id )  )  ;", "resourceManager . getRMContext (  )  . getRMApps (  )  . put ( id . getApplicationId (  )  ,    rmApp )  ;", "return   id ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    createResourceRequest ( memory ,    vcores ,    ANY ,    priority ,     1  ,    true )  ;", "createSchedulingRequestExistingApplication ( request ,    attId )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequestExistingApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    createResourceRequest ( memory ,    ANY ,    priority ,     1  ,    true )  ;", "createSchedulingRequestExistingApplication ( request ,    attId )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequestExistingApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ask . add ( request )  ;", "scheduler . allocate ( attId ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["createSchedulingRequestExistingApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerTestBase"}, {"methodBody": ["METHOD_START", "{", "int   res    =    a 1  . getPriority (  )  . compareTo ( a 2  . getPriority (  )  )  ;", "if    ( res    =  =     0  )     {", "if    (  ( a 1  . getStartTime (  )  )     <     ( a 2  . getStartTime (  )  )  )     {", "res    =     -  1  ;", "} else    {", "res    =     (  ( a 1  . getStartTime (  )  )     =  =     ( a 2  . getStartTime (  )  )  )     ?     0     :     1  ;", "}", "}", "if    ( res    =  =     0  )     {", "res    =    a 1  . getApplicationId (  )  . compareTo ( a 2  . getApplicationId (  )  )  ;", "}", "return   res ;", "}", "METHOD_END"], "methodName": ["compare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FifoAppComparator"}, {"methodBody": ["METHOD_START", "{", "AllocationConfiguration   allocConf    =    scheduler . getAllocationConfiguration (  )  ;", "Integer   userNumRunnable    =    usersNumRunnableApps . get ( user )  ;", "if    ( userNumRunnable    =  =    null )     {", "userNumRunnable    =     0  ;", "}", "if    ( userNumRunnable    >  =     ( allocConf . getUserMaxApps ( user )  )  )     {", "return   false ;", "}", "while    ( queue    !  =    null )     {", "int   queueMaxApps    =    allocConf . getQueueMaxApps ( queue . getName (  )  )  ;", "if    (  ( queue . getNumRunnableApps (  )  )     >  =    queueMaxApps )     {", "return   false ;", "}", "queue    =    queue . getParent (  )  ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["canAppBeRunnable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "if    (  ( queue . getNumRunnableApps (  )  )     <     ( scheduler . getAllocationConfiguration (  )  . getQueueMaxApps ( queue . getName (  )  )  )  )     {", "if    ( queue   instanceof   FSLeafQueue )     {", "appLists . add (  (  ( FSLeafQueue )     ( queue )  )  . getNonRunnableAppSchedulables (  )  )  ;", "} else    {", "for    ( FSQueue   child    :    queue . getChildQueues (  )  )     {", "gatherPossiblyRunnableAppLists ( child ,    appLists )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["gatherPossiblyRunnableAppLists"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "String   user    =    app . getUser (  )  ;", "usersNonableApps . put ( user ,    app )  ;", "}", "METHOD_END"], "methodName": ["trackNonRunnableApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "String   user    =    app . getUser (  )  ;", "FSLeafQueue   queue    =    app . getQueue (  )  ;", "FSParentQueue   parent    =    queue . getParent (  )  ;", "while    ( parent    !  =    null )     {", "parent . incrementableApps (  )  ;", "parent    =    parent . getParent (  )  ;", "}", "Integer   userNumable    =    usersNumableApps . get ( user )  ;", "usersNumableApps . put ( user ,     (  ( userNumable    =  =    null    ?     0     :    userNumable )     +     1  )  )  ;", "}", "METHOD_END"], "methodName": ["trackRunnableApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "usersNonRunnableApps . remove ( app . getUser (  )  ,    app )  ;", "}", "METHOD_END"], "methodName": ["untrackNonRunnableApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "String   user    =    app . getUser (  )  ;", "int   newUserNum    =     ( usersNumRunnableApps . get ( user )  )     -     1  ;", "if    ( newUserNum    =  =     0  )     {", "usersNumRunnableApps . remove ( user )  ;", "} else    {", "usersNumRunnableApps . put ( user ,    newUserNum )  ;", "}", "FSLeafQueue   queue    =    app . getQueue (  )  ;", "FSParentQueue   parent    =    queue . getParent (  )  ;", "while    ( parent    !  =    null )     {", "parent . decrementRunnableApps (  )  ;", "parent    =    parent . getParent (  )  ;", "}", "}", "METHOD_END"], "methodName": ["untrackRunnableApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "AllocationConfiguration   allocConf    =    scheduler . getAllocationConfiguration (  )  ;", "FSQueue   highestQueueWithAppsNowRunnable    =     (  ( queue . getNumRunnableApps (  )  )     =  =     (  ( allocConf . getQueueMaxApps ( queue . getName (  )  )  )     -     1  )  )     ?    queue    :    null ;", "FSParentQueue   parent    =    queue . getParent (  )  ;", "while    ( parent    !  =    null )     {", "if    (  ( parent . getNumRunnableApps (  )  )     =  =     (  ( allocConf . getQueueMaxApps ( parent . getName (  )  )  )     -     1  )  )     {", "highestQueueWithAppsNowRunnable    =    parent ;", "}", "parent    =    parent . getParent (  )  ;", "}", "List < List < FSAppAttempt >  >    appsNowMaybeRunnable    =    new   ArrayList < List < FSAppAttempt >  >  (  )  ;", "if    ( highestQueueWithAppsNowRunnable    !  =    null )     {", "gatherPossiblyRunnableAppLists ( highestQueueWithAppsNowRunnable ,    appsNowMaybeRunnable )  ;", "}", "String   user    =    app . getUser (  )  ;", "Integer   userNumRunning    =    usersNumRunnableApps . get ( user )  ;", "if    ( userNumRunning    =  =    null )     {", "userNumRunning    =     0  ;", "}", "if    ( userNumRunning    =  =     (  ( allocConf . getUserMaxApps ( user )  )     -     1  )  )     {", "List < FSAppAttempt >    userWaitingApps    =    usersNonRunnableApps . get ( user )  ;", "if    ( userWaitingApps    !  =    null )     {", "appsNowMaybeRunnable . add ( userWaitingApps )  ;", "}", "}", "Iterator < FSAppAttempt >    iter    =    new    . MultiListStartTimeIterator ( appsNowMaybeRunnable )  ;", "FSAppAttempt   prev    =    null ;", "List < FSAppAttempt >    noLongerPendingApps    =    new   ArrayList < FSAppAttempt >  (  )  ;", "while    ( iter . hasNext (  )  )     {", "FSAppAttempt   next    =    iter . next (  )  ;", "if    ( next    =  =    prev )     {", "continue ;", "}", "if    ( canAppBeRunnable ( next . getQueue (  )  ,    next . getUser (  )  )  )     {", "trackRunnableApp ( next )  ;", "FSAppAttempt   appSched    =    next ;", "next . getQueue (  )  . getRunnableAppSchedulables (  )  . add ( appSched )  ;", "noLongerPendingApps . add ( appSched )  ;", "if    (  ( noLongerPendingApps . size (  )  )     >  =     ( appsNowMaybeRunnable . size (  )  )  )     {", "break ;", "}", "}", "prev    =    next ;", "}", "for    ( FSAppAttempt   appSched    :    noLongerPendingApps )     {", "if    (  !  ( appSched . getQueue (  )  . getNonRunnableAppSchedulables (  )  . remove ( appSched )  )  )     {", ". LOG . error (  (  (  (  \" Can ' t   make   app   runnable   that   does   not   already   exist   in   queue \"     +     \"    as   non - runnable :     \"  )     +    appSched )     +     \"  .    This   should   never   happen .  \"  )  )  ;", "}", "if    (  !  ( usersNonRunnableApps . remove ( appSched . getUser (  )  ,    appSched )  )  )     {", ". LOG . error (  (  (  (  \" Waiting   app    \"     +    appSched )     +     \"    expected   to   be   in    \"  )     +     \" usersNonRunnableApps ,    but   was   not .    This   should   never   happen .  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updateRunnabilityOnAppRemoval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.MaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "long   start    =    app . getStartTime (  )  ;", "long   now    =    System . currentTimeMillis (  )  ;", "if    (  ( now    -    start )     <     ( duration )  )     {", "return   cur    *     ( factor )  ;", "} else    {", "return   cur ;", "}", "}", "METHOD_END"], "methodName": ["adjustWeight"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.NewAppWeightBooster"}, {"methodBody": ["METHOD_START", "{", "if    ( conf    !  =    null )     {", "factor    =    conf . getFloat (  \" mapred . newjobweightbooster . factor \"  ,     . DEFAULT _ FACTOR )  ;", "duration    =    conf . getLong (  \" mapred . newjobweightbooster . duration \"  ,     . DEFAULT _ DURATION )  ;", "}", "super . setConf ( conf )  ;", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.NewAppWeightBooster"}, {"methodBody": ["METHOD_START", "{", "List < String >    newQueueNames    =    new   ArrayList < String >  (  )  ;", "newQueueNames . add ( name )  ;", "int   sepIndex    =    name . length (  )  ;", "FSParentQueue   parent    =    null ;", "while    ( sepIndex    !  =     (  -  1  )  )     {", "sepIndex    =    name . lastIndexOf (  '  .  '  ,     ( sepIndex    -     1  )  )  ;", "FSQueue   queue ;", "String   curName    =    null ;", "curName    =    name . substring (  0  ,    sepIndex )  ;", "queue    =    queues . get ( curName )  ;", "if    ( queue    =  =    null )     {", "newQueueNames . add ( curName )  ;", "} else    {", "if    ( queue   instanceof   FSParentQueue )     {", "parent    =     (  ( FSParentQueue )     ( queue )  )  ;", "break ;", "} else    {", "return   null ;", "}", "}", "}", "AllocationConfiguration   queueConf    =    scheduler . getAllocationConfiguration (  )  ;", "FSLeafQueue   leafQueue    =    null ;", "for    ( int   i    =     ( newQueueNames . size (  )  )     -     1  ;    i    >  =     0  ;    i -  -  )     {", "String   queueName    =    newQueueNames . get ( i )  ;", "if    (  ( i    =  =     0  )     &  &     ( queueType    !  =     ( FSQueueType . PARENT )  )  )     {", "leafQueue    =    new   FSLeafQueue ( name ,    scheduler ,    parent )  ;", "try    {", "leafQueue . setPolicy ( queueConf . getDefaultSchedulingPolicy (  )  )  ;", "}    catch    ( AllocationConfigurationException   ex )     {", ". LOG . warn (  (  (  \" Failed   to   set   default   scheduling   policy    \"     +     ( queueConf . getDefaultSchedulingPolicy (  )  )  )     +     \"    on   new   leaf   queue .  \"  )  ,    ex )  ;", "}", "parent . addChildQueue ( leafQueue )  ;", "queues . put ( leafQueue . getName (  )  ,    leafQueue )  ;", "leafQueues . add ( leafQueue )  ;", "return   leafQueue ;", "} else    {", "FSParentQueue   newParent    =    new   FSParentQueue ( queueName ,    scheduler ,    parent )  ;", "try    {", "newParent . setPolicy ( queueConf . getDefaultSchedulingPolicy (  )  )  ;", "}    catch    ( AllocationConfigurationException   ex )     {", ". LOG . warn (  (  (  \" Failed   to   set   default   scheduling   policy    \"     +     ( queueConf . getDefaultSchedulingPolicy (  )  )  )     +     \"    on   new   parent   queue .  \"  )  ,    ex )  ;", "}", "parent . addChildQueue ( newParent )  ;", "queues . put ( newParent . getName (  )  ,    newParent )  ;", "parent    =    newParent ;", "}", "}", "return   parent ;", "}", "METHOD_END"], "methodName": ["createQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "if    (  (  !  ( name . startsWith (  (  ( QueueManager . ROOT _ QUEUE )     +     \"  .  \"  )  )  )  )     &  &     (  !  ( name . equals ( QueueManager . ROOT _ QUEUE )  )  )  )     {", "name    =     (  ( QueueManager . ROOT _ QUEUE )     +     \"  .  \"  )     +    name ;", "}", "return   name ;", "}", "METHOD_END"], "methodName": ["ensureRootPrefix"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "name    =    ensureRootPrefix ( name )  ;", "synchronized ( qs )     {", "return   qs . containsKey ( name )  ;", "}", "}", "METHOD_END"], "methodName": ["exists"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "FSQueue   queue    =    getQueue ( name ,    create ,    FSQueueType . LEAF )  ;", "if    ( queue   instanceof   FSParentQueue )     {", "return   null ;", "}", "return    (  ( FSLeafQueue )     ( queue )  )  ;", "}", "METHOD_END"], "methodName": ["getLeafQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "synchronized ( queues )     {", "return   leafs ;", "}", "}", "METHOD_END"], "methodName": ["getLeafQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "FSQueue   queue    =    getQueue ( name ,    create ,    FSQueueType . PARENT )  ;", "if    ( queue   instanceof   FSLeafQueue )     {", "return   null ;", "}", "return    (  ( FSParentQueue )     ( queue )  )  ;", "}", "METHOD_END"], "methodName": ["getParentQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "name    =    ensureRootPrefix ( name )  ;", "synchronized ( qs )     {", "return   qs . get ( name )  ;", "}", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "name    =    ensureRootPrefix ( name )  ;", "synchronized ( queues )     {", "FS   queue    =    queues . get ( name )  ;", "if    (  ( queue    =  =    null )     &  &    create )     {", "queue    =    create ( name ,    queueType )  ;", "if    ( queue    !  =    null )     {", "root . recomputeSteadyShares (  )  ;", "}", "}", "return   queue ;", "}", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "return   queues . values (  )  ;", "}", "METHOD_END"], "methodName": ["getQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "return   rootQueue ;", "}", "METHOD_END"], "methodName": ["getRootQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "rootQueue    =    new   FSParentQueue (  \" root \"  ,    scheduler ,    null )  ;", "queues . put ( rootQueue . getName (  )  ,    rootQueue )  ;", "getLeafQueue ( DEFAULT _ QUEUE _ NAME ,    true )  ;", "}", "METHOD_END"], "methodName": ["initialize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "if    ( queue   instanceof   FSLeafQueue )     {", "FSLeafQueue   leafQueue    =     (  ( FSLeafQueue )     ( queue )  )  ;", "return    (  ( queue . getNumRunnableApps (  )  )     =  =     0  )     &  &     ( leafQueue . getNonRunnableAppSables (  )  . isEmpty (  )  )  ;", "} else    {", "for    ( FSQueue   child    :    queue . getChildQueues (  )  )     {", "if    (  !  ( isEmpty ( child )  )  )     {", "return   false ;", "}", "}", "return   true ;", "}", "}", "METHOD_END"], "methodName": ["isEmpty"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "queueToCreate    =    ensureRootPrefix ( queueToCreate )  ;", "if    (  ( queueToCreate . equals (  . ROOT _ QUEUE )  )     |  |     ( queueToCreate . startsWith (  (  (  (  (  . ROOT _ QUEUE )     +     \"  .  \"  )     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )     +     \"  .  \"  )  )  )  )     {", "return   false ;", "}", "FSQueue   queue    =    queues . get ( queueToCreate )  ;", "if    ( queue    !  =    null )     {", "if    ( queue   instanceof   FSLeafQueue )     {", "if    ( queueType    =  =     ( FSQueueType . LEAF )  )     {", "return   true ;", "}", "return   removeQueueIfEmpty ( queue )  ;", "} else    {", "if    ( queueType    =  =     ( FSQueueType . PARENT )  )     {", "return   true ;", "}", "return   removeQueueIfEmpty ( queue )  ;", "}", "}", "int   sepIndex    =    queueToCreate . length (  )  ;", "sepIndex    =    queueToCreate . lastIndexOf (  '  .  '  ,     ( sepIndex    -     1  )  )  ;", "while    ( sepIndex    !  =     (  -  1  )  )     {", "String   prefixString    =    queueToCreate . substring (  0  ,    sepIndex )  ;", "FSQueue   prefixQueue    =    queues . get ( prefixString )  ;", "if    (  ( prefixQueue    !  =    null )     &  &     ( prefixQueue   instanceof   FSLeafQueue )  )     {", "return   removeQueueIfEmpty ( prefixQueue )  ;", "}", "sepIndex    =    queueToCreate . lastIndexOf (  '  .  '  ,     ( sepIndex    -     1  )  )  ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["removeEmptyIncompatibleQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "if    ( queue   instanceof   FSLeafQueue )     {", "leafQueues . remove ( queue )  ;", "} else    {", "List < FSQueue >    childQueues    =    queue . getChildQueues (  )  ;", "while    (  !  ( childQueues . isEmpty (  )  )  )     {", "removeQueue ( childQueues . get (  0  )  )  ;", "}", "}", "queues . remove ( queue . getName (  )  )  ;", "queue . getParent (  )  . getChildQueues (  )  . remove ( queue )  ;", "}", "METHOD_END"], "methodName": ["removeQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "if    ( isEmpty ( queue )  )     {", "remove ( queue )  ;", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["removeQueueIfEmpty"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "for    ( String   name    :    queueConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  )     {", "if    ( removeEmptyIncompatibleQueues ( name ,    FSQueueType . LEAF )  )     {", "getLeafQueue ( name ,    true )  ;", "}", "}", "for    ( String   name    :    queueConf . getConfiguredQueues (  )  . get ( FSQueueType . PARENT )  )     {", "if    ( removeEmptyIncompatibleQueues ( name ,    FSQueueType . PARENT )  )     {", "getParentQueue ( name ,    true )  ;", "}", "}", "for    ( FSQueue   queue    :    queues . values (  )  )     {", "FSQueueMetrics   queueMetrics    =    queue . getMetrics (  )  ;", "queueMetrics . setMinShare ( queue . getMinShare (  )  )  ;", "queueMetrics . setMaxShare ( queue . getMaxShare (  )  )  ;", "try    {", "SchedulingPolicy   policy    =    queueConf . getSchedulingPolicy ( queue . getName (  )  )  ;", "policy . initialize ( scheduler . getClusterResource (  )  )  ;", "queue . setPolicy ( policy )  ;", "}    catch    ( AllocationConfigurationException   ex )     {", ". LOG . warn (  (  \" Cannot   apply   configured   scheduling   policy   to   queue    \"     +     ( queue . getName (  )  )  )  ,    ex )  ;", "}", "}", "rootQueue . recomputeSteadyShares (  )  ;", "}", "METHOD_END"], "methodName": ["updateAllocationConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager"}, {"methodBody": ["METHOD_START", "{", "for    ( QueuePlacementRule   rule    :    rules )     {", "String   queue    =    rule . assignAppToQueue ( requestedQueue ,    user ,    groups ,    configuredQueues )  ;", "if    (  ( queue    =  =    null )     |  |     (  !  ( queue . isEmpty (  )  )  )  )     {", "return   queue ;", "}", "}", "throw   new   IllegalStateException (  (  \" Should   have   applied   a   rule   before    \"     +     \" reaching   here \"  )  )  ;", "}", "METHOD_END"], "methodName": ["assignAppToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "Element   element    =     (  ( Element )     ( node )  )  ;", "String   ruleName    =    element . getAttribute (  \" name \"  )  ;", "if    (  \"  \"  . equals ( ruleName )  )     {", "throw   new   AllocationConfigurationException (  (  \" No   name   provided   for   a    \"     +     \" rule   element \"  )  )  ;", "}", "Class <  ?    extends   QueuePlacementRule >    clazz    =     . ruleClasses . get ( ruleName )  ;", "if    ( clazz    =  =    null )     {", "throw   new   AllocationConfigurationException (  (  \" No   rule   class   found   for    \"     +    ruleName )  )  ;", "}", "QueuePlacementRule   rule    =    ReflectionUtils . newInstance ( clazz ,    null )  ;", "rule . initializeFromXml ( element )  ;", "return   rule ;", "}", "METHOD_END"], "methodName": ["createAndInitializeRule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "boolean   create    =    conf . getBoolean ( FairSchedulerConfiguration . ALLOW _ UNDECLARED _ POOLS ,    FairSchedulerConfiguration . DEFAULT _ ALLOW _ UNDECLARED _ POOLS )  ;", "boolean   userAsDefaultQueue    =    conf . getBoolean ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,    FairSchedulerConfiguration . DEFAULT _ USER _ AS _ DEFAULT _ QUEUE )  ;", "List < QueuePlacementRule >    rules    =    new   ArrayList < QueuePlacementRule >  (  )  ;", "rules . add ( new   QueuePlacementRule . Specified (  )  . initialize ( create ,    null )  )  ;", "if    ( userAsDefaultQueue )     {", "rules . add ( new   QueuePlacementRule . User (  )  . initialize ( create ,    null )  )  ;", "}", "if    (  (  ! userAsDefaultQueue )     |  |     (  ! create )  )     {", "rules . add ( new   QueuePlacementRule . Default (  )  . initialize ( true ,    null )  )  ;", "}", "try    {", "return   new    ( rules ,    configuredQueues ,    conf )  ;", "}    catch    ( AllocationConfigurationException   ex )     {", "throw   new   RuntimeException (  (  \" Should   never   hit   exception   when   loading \"     +     \" placement   policy   from   conf \"  )  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["fromConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "List < QueuePlacementRule >    rules    =    new   ArrayList < QueuePlacementRule >  (  )  ;", "NodeList   elements    =    el . getChildNodes (  )  ;", "for    ( int   i    =     0  ;    i    <     ( elements . getLength (  )  )  ;    i +  +  )     {", "Node   node    =    elements . item ( i )  ;", "if    ( node   instanceof   Element )     {", "QueuePlacementRule   rule    =     . createAndInitializeRule ( node )  ;", "rules . add ( rule )  ;", "}", "}", "return   new    ( rules ,    configuredQueues ,    conf )  ;", "}", "METHOD_END"], "methodName": ["fromXml"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "return   rules ;", "}", "METHOD_END"], "methodName": ["getRules"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "String   queue    =    getQueueForApp ( requestedQueue ,    user ,    groups ,    configuredQueues )  ;", "if    (  (  ( create )     |  |     ( configuredQueues . get ( FSQueueType . LEAF )  . contains ( queue )  )  )     |  |     ( configuredQueues . get ( FSQueueType . PARENT )  . contains ( queue )  )  )     {", "return   queue ;", "} else    {", "return    \"  \"  ;", "}", "}", "METHOD_END"], "methodName": ["assignAppToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule"}, {"methodBody": ["METHOD_START", "{", "this . create    =    create ;", "return   this ;", "}", "METHOD_END"], "methodName": ["initialize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule"}, {"methodBody": ["METHOD_START", "{", "boolean   create    =    true ;", "NamedNodeMap   attributes    =    el . getAttributes (  )  ;", "Map < String ,    String >    args    =    new   HashMap < String ,    String >  (  )  ;", "for    ( int   i    =     0  ;    i    <     ( attributes . getLength (  )  )  ;    i +  +  )     {", "Node   node    =    attributes . item ( i )  ;", "String   key    =    node . getNodeName (  )  ;", "String   value    =    node . getNodeValue (  )  ;", "if    ( key . equals (  \" create \"  )  )     {", "create    =    Boolean . parseBoolean ( value )  ;", "} else    {", "args . put ( key ,    value )  ;", "}", "}", "initialize ( create ,    args )  ;", "}", "METHOD_END"], "methodName": ["initializeFromXml"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule"}, {"methodBody": ["METHOD_START", "{", "SchedulingPolicy   policy    =    SchedulingPolicy . instances . get ( clazz )  ;", "if    ( policy    =  =    null )     {", "policy    =    ReflectionUtils . newInstance ( clazz ,    null )  ;", "SchedulingPolicy . instances . put ( clazz ,    policy )  ;", "}", "return   policy ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy"}, {"methodBody": ["METHOD_START", "{", "return    (  ( policy . getApplicableDepth (  )  )     &    depth )     =  =    depth    ?    true    :    false ;", "}", "METHOD_END"], "methodName": ["isApplicableTo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy"}, {"methodBody": ["METHOD_START", "{", "@ SuppressWarnings (  \" rawtypes \"  )", "Class   clazz ;", "String   text    =    policy . toLowerCase (  )  ;", "if    ( text . equalsIgnoreCase ( FairSharePolicy . NAME )  )     {", "clazz    =    FairSharePolicy . class ;", "} else", "if    ( text . equalsIgnoreCase ( policies . FifoPolicy . NAME )  )     {", "clazz    =    policies . FifoPolicy . class ;", "} else", "if    ( text . equalsIgnoreCase ( policies . DominantResourceFairnessPolicy . NAME )  )     {", "clazz    =    policies . DominantResourceFairnessPolicy . class ;", "} else    {", "try    {", "clazz    =    Class . forName ( policy )  ;", "}    catch    ( ClassNotFoundException   cnfe )     {", "throw   new   AllocationConfigurationException (  ( policy    +     \"    SchedulingPolicy   class   not   found !  \"  )  )  ;", "}", "}", "if    (  !  ( SchedulingPolicy . class . isAssignableFrom ( clazz )  )  )     {", "throw   new   AllocationConfigurationException (  ( policy    +     \"    does   not   extend   SchedulingPolicy \"  )  )  ;", "}", "return   SchedulingPolicy . getInstance ( clazz )  ;", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.SchedulingPolicy"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "AllocationFileLoaderService   allocLoader    =    new   AllocationFileLoaderService (  )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  < aclAdministerApps > alice , bob   admins <  / aclAdministerApps >  \"  )  ;", "out . println (  \"  < schedulingPolicy > fair <  / schedulingPolicy >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueC \\  \"  >  \"  )  ;", "out . println (  \"  < aclSubmitApps > alice , bob   admins <  / aclSubmitApps >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueD \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  3  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  < maxAMShare >  0  .  4  <  / maxAMShare >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueE \\  \"  >  \"  )  ;", "out . println (  \"  < minSharePreemptionTimeout >  6  0  <  / minSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueF \\  \"    type =  \\  \" parent \\  \"     >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueG \\  \"  >  \"  )  ;", "out . println (  \"           < queue   name =  \\  \" queueH \\  \"  >  \"  )  ;", "out . println (  \"           <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queueMaxAppsDefault >  1  5  <  / queueMaxAppsDefault >  \"  )  ;", "out . println (  \"  < userMaxAppsDefault >  5  <  / userMaxAppsDefault >  \"  )  ;", "out . println (  \"  < queueMaxAMShareDefault >  0  .  5 f <  / queueMaxAMShareDefault >  \"  )  ;", "out . println (  \"  < user   name =  \\  \" user 1  \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  1  0  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  <  / user >  \"  )  ;", "out . println (  (  \"  < defaultMinSharePreemptionTimeout >  1  2  0  \"     +     \"  <  / defaultMinSharePreemptionTimeout >  \"  )  )  ;", "out . println (  \"  < fairSharePreemptionTimeout >  3  0  0  <  / fairSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  < defaultQueueSchedulingPolicy > drf <  / defaultQueueSchedulingPolicy >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "allocLoader . init ( conf )  ;", ". ReloadListener   confHolder    =    new    . ReloadListener (  )  ;", "allocLoader . setReloadListener ( confHolder )  ;", "allocLoader . reloadAllocations (  )  ;", "AllocationConfiguration   queueConf    =    confHolder . allocConf ;", "assertEquals (  6  ,    queueConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . size (  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals ( Resources . createResource (  1  0  2  4  ,     0  )  ,    queueConf . getMinResources (  \" root . queueA \"  )  )  ;", "assertEquals ( Resources . createResource (  2  0  4  8  ,     0  )  ,    queueConf . getMinResources (  \" root . queueB \"  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  \" root . queueC \"  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  \" root . queueD \"  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  \" root . queueE \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueA \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueB \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueC \"  )  )  ;", "assertEquals (  3  ,    queueConf . getQueueMaxApps (  \" root . queueD \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueE \"  )  )  ;", "assertEquals (  1  0  ,    queueConf . getUserMaxApps (  \" user 1  \"  )  )  ;", "assertEquals (  5  ,    queueConf . getUserMaxApps (  \" user 2  \"  )  )  ;", "assertEquals (  0  .  5 F ,    queueConf . getQueueMaxAMShare (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  ,     0  .  0  1  )  ;", "assertEquals (  0  .  5 F ,    queueConf . getQueueMaxAMShare (  \" root . queueA \"  )  ,     0  .  0  1  )  ;", "assertEquals (  0  .  5 F ,    queueConf . getQueueMaxAMShare (  \" root . queueB \"  )  ,     0  .  0  1  )  ;", "assertEquals (  0  .  5 F ,    queueConf . getQueueMaxAMShare (  \" root . queueC \"  )  ,     0  .  0  1  )  ;", "assertEquals (  0  .  4 F ,    queueConf . getQueueMaxAMShare (  \" root . queueD \"  )  ,     0  .  0  1  )  ;", "assertEquals (  0  .  5 F ,    queueConf . getQueueMaxAMShare (  \" root . queueE \"  )  ,     0  .  0  1  )  ;", "assertEquals (  \"  *  \"  ,    queueConf . getQueueAcl (  \" root \"  ,    ADMINISTER _ QUEUE )  . getAclString (  )  )  ;", "assertEquals (  \"  *  \"  ,    queueConf . getQueueAcl (  \" root \"  ,    SUBMIT _ APPLICATIONS )  . getAclString (  )  )  ;", "assertEquals (  \"     \"  ,    queueConf . getQueueAcl (  \" root . queueA \"  ,    ADMINISTER _ QUEUE )  . getAclString (  )  )  ;", "assertEquals (  \"     \"  ,    queueConf . getQueueAcl (  \" root . queueA \"  ,    SUBMIT _ APPLICATIONS )  . getAclString (  )  )  ;", "assertEquals (  \" alice , bob   admins \"  ,    queueConf . getQueueAcl (  \" root . queueB \"  ,    ADMINISTER _ QUEUE )  . getAclString (  )  )  ;", "assertEquals (  \" alice , bob   admins \"  ,    queueConf . getQueueAcl (  \" root . queueC \"  ,    SUBMIT _ APPLICATIONS )  . getAclString (  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueA \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueB \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueC \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueD \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueA \"  )  )  ;", "assertEquals (  6  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueE \"  )  )  ;", "assertEquals (  3  0  0  0  0  0  ,    queueConf . getFairSharePreemptionTimeout (  )  )  ;", "assertTrue ( queueConf . getConfiguredQueues (  )  . get ( FSQueueType . PARENT )  . contains (  \" root . queueF \"  )  )  ;", "assertTrue ( queueConf . getConfiguredQueues (  )  . get ( FSQueueType . PARENT )  . contains (  \" root . queueG \"  )  )  ;", "assertTrue ( queueConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . contains (  \" root . queueG . queueH \"  )  )  ;", "assertEquals ( DominantResourceFairnessPolicy . NAME ,    queueConf . getSchedulingPolicy (  \" root \"  )  . getName (  )  )  ;", "assertEquals ( DominantResourceFairnessPolicy . NAME ,    queueConf . getSchedulingPolicy (  \" root . queueA \"  )  . getName (  )  )  ;", "assertEquals ( FairSharePolicy . NAME ,    queueConf . getSchedulingPolicy (  \" root . queueB \"  )  . getName (  )  )  ;", "assertEquals ( DominantResourceFairnessPolicy . NAME ,    queueConf . getSchedulingPolicy (  \" root . newqueue \"  )  . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAllocationFileParsing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "AllocationFileLoaderService   allocLoader    =    new   AllocationFileLoaderService (  )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < pool   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / pool >  \"  )  ;", "out . println (  \"  < pool   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  < aclAdministerApps > alice , bob   admins <  / aclAdministerApps >  \"  )  ;", "out . println (  \"  <  / pool >  \"  )  ;", "out . println (  \"  < pool   name =  \\  \" queueC \\  \"  >  \"  )  ;", "out . println (  \"  < aclSubmitApps > alice , bob   admins <  / aclSubmitApps >  \"  )  ;", "out . println (  \"  <  / pool >  \"  )  ;", "out . println (  \"  < pool   name =  \\  \" queueD \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  3  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  <  / pool >  \"  )  ;", "out . println (  \"  < pool   name =  \\  \" queueE \\  \"  >  \"  )  ;", "out . println (  \"  < minSharePreemptionTimeout >  6  0  <  / minSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  <  / pool >  \"  )  ;", "out . println (  \"  < queueMaxAppsDefault >  1  5  <  / queueMaxAppsDefault >  \"  )  ;", "out . println (  \"  < userMaxAppsDefault >  5  <  / userMaxAppsDefault >  \"  )  ;", "out . println (  \"  < user   name =  \\  \" user 1  \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  1  0  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  <  / user >  \"  )  ;", "out . println (  (  \"  < defaultMinSharePreemptionTimeout >  1  2  0  \"     +     \"  <  / defaultMinSharePreemptionTimeout >  \"  )  )  ;", "out . println (  \"  < fairSharePreemptionTimeout >  3  0  0  <  / fairSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "allocLoader . init ( conf )  ;", ". ReloadListener   confHolder    =    new    . ReloadListener (  )  ;", "allocLoader . setReloadListener ( confHolder )  ;", "allocLoader . reloadAllocations (  )  ;", "AllocationConfiguration   queueConf    =    confHolder . allocConf ;", "assertEquals (  5  ,    queueConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . size (  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals ( Resources . createResource (  1  0  2  4  ,     0  )  ,    queueConf . getMinResources (  \" root . queueA \"  )  )  ;", "assertEquals ( Resources . createResource (  2  0  4  8  ,     0  )  ,    queueConf . getMinResources (  \" root . queueB \"  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  \" root . queueC \"  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  \" root . queueD \"  )  )  ;", "assertEquals ( Resources . createResource (  0  )  ,    queueConf . getMinResources (  \" root . queueE \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueA \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueB \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueC \"  )  )  ;", "assertEquals (  3  ,    queueConf . getQueueMaxApps (  \" root . queueD \"  )  )  ;", "assertEquals (  1  5  ,    queueConf . getQueueMaxApps (  \" root . queueE \"  )  )  ;", "assertEquals (  1  0  ,    queueConf . getUserMaxApps (  \" user 1  \"  )  )  ;", "assertEquals (  5  ,    queueConf . getUserMaxApps (  \" user 2  \"  )  )  ;", "assertEquals (  \"     \"  ,    queueConf . getQueueAcl (  \" root . queueA \"  ,    ADMINISTER _ QUEUE )  . getAclString (  )  )  ;", "assertEquals (  \"     \"  ,    queueConf . getQueueAcl (  \" root . queueA \"  ,    SUBMIT _ APPLICATIONS )  . getAclString (  )  )  ;", "assertEquals (  \" alice , bob   admins \"  ,    queueConf . getQueueAcl (  \" root . queueB \"  ,    ADMINISTER _ QUEUE )  . getAclString (  )  )  ;", "assertEquals (  \" alice , bob   admins \"  ,    queueConf . getQueueAcl (  \" root . queueC \"  ,    SUBMIT _ APPLICATIONS )  . getAclString (  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  (  \" root .  \"     +     ( YarnConfiguration . DEFAULT _ QUEUE _ NAME )  )  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueA \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueB \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueC \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueD \"  )  )  ;", "assertEquals (  1  2  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueA \"  )  )  ;", "assertEquals (  6  0  0  0  0  ,    queueConf . getMinSharePreemptionTimeout (  \" root . queueE \"  )  )  ;", "assertEquals (  3  0  0  0  0  0  ,    queueConf . getFairSharePreemptionTimeout (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBackwardsCompatibleAllocationFileParsing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     \" test - fair - scheduler . xml \"  )  ;", "allocLoader    =    new    (  )  ;", "File   allocationFile    =    allocLoader . getAllocationFile ( conf )  ;", "assertEquals (  \" test - fair - scheduler . xml \"  ,    allocationFile . getName (  )  )  ;", "assertTrue ( allocationFile . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetAllocationFileFromClasspath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" other \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "AllocationFileLoaderService   allocLoader    =    new   AllocationFileLoaderService (  )  ;", "allocLoader . init ( conf )  ;", ". ReloadListener   confHolder    =    new    . ReloadListener (  )  ;", "allocLoader . setReloadListener ( confHolder )  ;", "allocLoader . reloadAllocations (  )  ;", "}", "METHOD_END"], "methodName": ["testQueueAlongsideRoot"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestAllocationFileLoaderService . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"              < maxRunningApps >  1  <  / maxRunningApps >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" queueB \\  \"     /  >  \"  )  ;", "out . println (  \"        < queuePlacementPolicy >  \"  )  ;", "out . println (  \"              < rule   name =  ' default '     /  >  \"  )  ;", "out . println (  \"        <  / queuePlacementPolicy >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "TestAllocationFileLoaderService . MockClock   clock    =    new   TestAllocationFileLoaderService . MockClock (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestAllocationFileLoaderService . ALLOC _ FILE )  ;", "AllocationFileLoaderService   allocLoader    =    new   AllocationFileLoaderService ( clock )  ;", "allocLoader . reloadIntervalMs    =     5  ;", "allocLoader . init ( conf )  ;", "TestAllocationFileLoaderService . ReloadListener   confHolder    =    new   TestAllocationFileLoaderService . ReloadListener (  )  ;", "allocLoader . setReloadListener ( confHolder )  ;", "allocLoader . reloadAllocations (  )  ;", "AllocationConfiguration   allocConf    =    confHolder . allocConf ;", "QueuePlacementPolicy   policy    =    allocConf . getPlacementPolicy (  )  ;", "List < QueuePlacementRule >    rules    =    policy . getRules (  )  ;", "assertEquals (  1  ,    rules . size (  )  )  ;", "assertEquals ( QueuePlacementRule . Default . class ,    rules . get (  0  )  . getClass (  )  )  ;", "assertEquals (  1  ,    allocConf . getQueueMaxApps (  \" root . queueA \"  )  )  ;", "assertEquals (  2  ,    allocConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . size (  )  )  ;", "assertTrue ( allocConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . contains (  \" root . queueA \"  )  )  ;", "assertTrue ( allocConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . contains (  \" root . queueB \"  )  )  ;", "confHolder . allocConf    =    null ;", "out    =    new   PrintWriter ( new   FileWriter ( TestAllocationFileLoaderService . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"              < maxRunningApps >  3  <  / maxRunningApps >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"        < queuePlacementPolicy >  \"  )  ;", "out . println (  \"              < rule   name =  ' specified '     /  >  \"  )  ;", "out . println (  \"              < rule   name =  ' nestedUserQueue '     >  \"  )  ;", "out . println (  \"                             < rule   name =  ' primaryGroup '     /  >  \"  )  ;", "out . println (  \"              <  / rule >  \"  )  ;", "out . println (  \"              < rule   name =  ' default '     /  >  \"  )  ;", "out . println (  \"        <  / queuePlacementPolicy >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "clock . tick (  (  (  ( System . currentTimeMillis (  )  )     +     ( AllocationFileLoaderService . ALLOC _ RELOAD _ WAIT _ MS )  )     +     1  0  0  0  0  )  )  ;", "allocLoader . start (  )  ;", "while    (  ( confHolder . allocConf )     =  =    null )     {", "Thread . sleep (  2  0  )  ;", "}", "allocConf    =    confHolder . allocConf ;", "policy    =    allocConf . getPlacementPolicy (  )  ;", "rules    =    policy . getRules (  )  ;", "assertEquals (  3  ,    rules . size (  )  )  ;", "assertEquals ( QueuePlacementRule . Specified . class ,    rules . get (  0  )  . getClass (  )  )  ;", "assertEquals ( QueuePlacementRule . NestedUserQueue . class ,    rules . get (  1  )  . getClass (  )  )  ;", "assertEquals ( QueuePlacementRule . PrimaryGroup . class ,     (  ( QueuePlacementRule . NestedUserQueue )     ( rules . get (  1  )  )  )  . nestedRule . getClass (  )  )  ;", "assertEquals ( QueuePlacementRule . Default . class ,    rules . get (  2  )  . getClass (  )  )  ;", "assertEquals (  3  ,    allocConf . getQueueMaxApps (  \" root . queueB \"  )  )  ;", "assertEquals (  1  ,    allocConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . size (  )  )  ;", "assertTrue ( allocConf . getConfiguredQueues (  )  . get ( FSQueueType . LEAF )  . contains (  \" root . queueB \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testReload"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "conf . setBoolean ( FairSchedulerConfiguration . ALLOW _ UNDECLARED _ POOLS ,    false )  ;", "conf . setBoolean ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,    false )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "AllocationFileLoaderService   allocLoader    =    new   AllocationFileLoaderService (  )  ;", "allocLoader . init ( conf )  ;", ". ReloadListener   confHolder    =    new    . ReloadListener (  )  ;", "allocLoader . setReloadListener ( confHolder )  ;", "allocLoader . reloadAllocations (  )  ;", "AllocationConfiguration   allocConf    =    confHolder . allocConf ;", "QueuePlacementPolicy   placementPolicy    =    allocConf . getPlacementPolicy (  )  ;", "List < QueuePlacementRule >    rules    =    placementPolicy . getRules (  )  ;", "assertEquals (  2  ,    rules . size (  )  )  ;", "assertEquals ( QueuePlacementRule . Specified . class ,    rules . get (  0  )  . getClass (  )  )  ;", "assertEquals ( false ,    rules . get (  0  )  . create )  ;", "assertEquals ( QueuePlacementRule . Default . class ,    rules . get (  1  )  . getClass (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimplePlacementPolicyFromConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestAllocationFileLoaderService"}, {"methodBody": ["METHOD_START", "{", "scheds    =    new   ArrayList < Schedulable >  (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable ( Resources . createResource (  0  ,     2  0  )  ,    new   ResourceWeights (  2  .  0 F )  )  )  ;", "scheds . add ( new   FakeSchedulable ( Resources . createResource (  0  ,     0  )  ,    new   ResourceWeights (  1  .  0 F )  )  )  ;", "scheds . add ( new   FakeSchedulable ( Resources . createResource (  0  ,     5  )  ,    new   ResourceWeights (  1  .  0 F )  )  )  ;", "scheds . add ( new   FakeSchedulable ( Resources . createResource (  0  ,     1  5  )  ,    new   ResourceWeights (  0  .  5 F )  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  0  ,     4  5  )  ,    ResourceType . CPU )  ;", "verifyCPUShares (  2  0  ,     5  ,     5  ,     1  5  )  ;", "}", "METHOD_END"], "methodName": ["testCPU"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "ComputeFairShares . computeShares ( scheds ,    Resources . createResource (  4  0  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable (  )  )  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  4  0  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  1  0  ,     1  0  ,     1  0  ,     1  0  )  ;", "}", "METHOD_END"], "methodName": ["testEqualSharing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "int   million    =     1  0  0  0     *     1  0  0  0  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", "scheds . add ( new   FakeSchedulable (  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  (  4  0     *    million )  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  (  1  0     *    million )  ,     (  1  0     *    million )  ,     (  1  0     *    million )  ,     (  1  0     *    million )  )  ;", "}", "METHOD_END"], "methodName": ["testLargeShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable (  0  ,     1  0  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     5  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     1  1  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     3  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  4  0  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  1  3  ,     1  3  ,     1  1  ,     3  )  ;", "}", "METHOD_END"], "methodName": ["testLowMaxShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable (  2  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  1  8  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  2  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  4  0  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  2  0  ,     1  8  ,     0  ,     2  )  ;", "}", "METHOD_END"], "methodName": ["testMinShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable (  0  ,     2  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     1  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     1  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     0  .  5  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  4  5  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  2  0  ,     1  0  ,     1  0  ,     5  )  ;", "}", "METHOD_END"], "methodName": ["testWeightedSharing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable (  0  ,     1  0  ,     2  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     1  1  ,     1  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     3  0  ,     1  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     2  0  ,     0  .  5  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  4  5  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  1  0  ,     1  1  ,     1  6  ,     8  )  ;", "}", "METHOD_END"], "methodName": ["testWeightedSharingWithMaxShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "scheds . add ( new   FakeSchedulable (  2  0  ,     2  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  0  ,     1  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  5  ,     1  .  0  )  )  ;", "scheds . add ( new   FakeSchedulable (  1  5  ,     0  .  5  )  )  ;", ". computeShares ( scheds ,    Resources . createResource (  4  5  )  ,    ResourceType . MEMORY )  ;", "verifyMemoryShares (  2  0  ,     5  ,     5  ,     1  5  )  ;", "}", "METHOD_END"], "methodName": ["testWeightedSharingWithMinShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( scheds . size (  )  ,    shares . length )  ;", "for    ( int   i    =     0  ;    i    <     ( shares . length )  ;    i +  +  )     {", "Assert . assertEquals ( shares [ i ]  ,    scheds . get ( i )  . get (  )  . getVirtualCores (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyCPUShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( scheds . size (  )  ,    shares . length )  ;", "for    ( int   i    =     0  ;    i    <     ( shares . length )  ;    i +  +  )     {", "Assert . assertEquals ( shares [ i ]  ,    scheds . get ( i )  . get (  )  . getMemory (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyMemoryShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    createConfiguration (  )  ;", "resourceManager    =    new   MockRM ( conf )  ;", "resourceManager . start (  )  ;", "=     (  ( FairScheduler )     ( resourceManager . getResourceScheduler (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   queue    =    Mockito . mock ( FSLeafQueue . class )  ;", "Priority   prio    =    Mockito . mock ( Priority . class )  ;", "Mockito . when ( prio . getPriority (  )  )  . thenReturn (  1  )  ;", "double   nodeLocalityThreshold    =     0  .  5  ;", "double   rackLocalityThreshold    =     0  .  6  ;", "ApplicationAttemptId   applicationAttemptId    =    createAppAttemptId (  1  ,     1  )  ;", "RMContext   rmContext    =    resourceManager . getRMContext (  )  ;", "schedulerApp    =    new    ( scheduler ,    applicationAttemptId ,     \" user 1  \"  ,    queue ,    null ,    rmContext )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "for    ( int   i    =     0  ;    i    <     5  ;    i +  +  )     {", "schedulerApp . addSchedulingOpportunity ( prio )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "}", "schedulerApp . addSchedulingOpportunity ( prio )  ;", "assertEquals ( NodeType . RACK _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "schedulerApp . resetAllowedLocalityLevel ( prio ,    NodeType . NODE _ LOCAL )  ;", "schedulerApp . resetSchedulingOpportunities ( prio )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "for    ( int   i    =     0  ;    i    <     5  ;    i +  +  )     {", "schedulerApp . addSchedulingOpportunity ( prio )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "}", "schedulerApp . addSchedulingOpportunity ( prio )  ;", "assertEquals ( NodeType . RACK _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "for    ( int   i    =     0  ;    i    <     6  ;    i +  +  )     {", "schedulerApp . addSchedulingOpportunity ( prio )  ;", "assertEquals ( NodeType . RACK _ LOCAL ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "}", "schedulerApp . addSchedulingOpportunity ( prio )  ;", "assertEquals ( NodeType . OFF _ SWITCH ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,    nodeLocalityThreshold ,    rackLocalityThreshold )  )  ;", "}", "METHOD_END"], "methodName": ["testDelayScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   queue    =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue \"  ,    true )  ;", "Priority   prio    =    Mockito . mock ( Priority . class )  ;", "Mockito . when ( prio . getPriority (  )  )  . thenReturn (  1  )  ;", ". MockClock   clock    =    new    . MockClock (  )  ;", "scheduler . setClock ( clock )  ;", "long   nodeLocalityDelayMs    =     5     *     1  0  0  0 L ;", "long   rackLocalityDelayMs    =     6     *     1  0  0  0 L ;", "RMContext   rmContext    =    resourceManager . getRMContext (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    createAppAttemptId (  1  ,     1  )  ;", "FSAppAttempt   schedulerApp    =    new   FSAppAttempt ( scheduler ,    applicationAttemptId ,     \" user 1  \"  ,    queue ,    null ,    rmContext )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevelByTime ( prio ,    nodeLocalityDelayMs ,    rackLocalityDelayMs ,    clock . getTime (  )  )  )  ;", "clock . tick (  4  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevelByTime ( prio ,    nodeLocalityDelayMs ,    rackLocalityDelayMs ,    clock . getTime (  )  )  )  ;", "clock . tick (  2  )  ;", "assertEquals ( NodeType . RACK _ LOCAL ,    schedulerApp . getAllowedLocalityLevelByTime ( prio ,    nodeLocalityDelayMs ,    rackLocalityDelayMs ,    clock . getTime (  )  )  )  ;", "schedulerApp . resetAllowedLocalityLevel ( prio ,    NodeType . NODE _ LOCAL )  ;", "schedulerApp . resetSchedulingOpportunities ( prio ,    clock . getTime (  )  )  ;", "assertEquals ( NodeType . NODE _ LOCAL ,    schedulerApp . getAllowedLocalityLevelByTime ( prio ,    nodeLocalityDelayMs ,    rackLocalityDelayMs ,    clock . getTime (  )  )  )  ;", "clock . tick (  6  )  ;", "assertEquals ( NodeType . RACK _ LOCAL ,    schedulerApp . getAllowedLocalityLevelByTime ( prio ,    nodeLocalityDelayMs ,    rackLocalityDelayMs ,    clock . getTime (  )  )  )  ;", "clock . tick (  7  )  ;", "assertEquals ( NodeType . OFF _ SWITCH ,    schedulerApp . getAllowedLocalityLevelByTime ( prio ,    nodeLocalityDelayMs ,    rackLocalityDelayMs ,    clock . getTime (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testDelaySchedulingForContinuousScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   queue    =    Mockito . mock ( FSLeafQueue . class )  ;", "Priority   prio    =    Mockito . mock ( Priority . class )  ;", "Mockito . when ( prio . getPriority (  )  )  . thenReturn (  1  )  ;", "RMContext   rmContext    =    resourceManager . getRMContext (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    createAppAttemptId (  1  ,     1  )  ;", "schedulerApp    =    new    ( scheduler ,    applicationAttemptId ,     \" user 1  \"  ,    queue ,    null ,    rmContext )  ;", "assertEquals ( NodeType . OFF _ SWITCH ,    schedulerApp . getAllowedLocalityLevel ( prio ,     1  0  ,     (  -  1  .  0  )  ,     (  -  1  .  0  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testLocalityLevelWithoutDelays"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSAppAttempt"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    FairSclass ,    ResourceSclass )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["createConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "FairScheduler   scheduler    =    new   FairScheduler (  )  ;", "Configuration   conf    =    createConfiguration (  )  ;", "conf . set ( FairSchedulerConfiguration . ASSIGN _ MULTIPLE ,     \" false \"  )  ;", "ResourceManager   resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( conf )  ;", "(  ( AsyncDispatcher )     ( resourceManager . getRMContext (  )  . getDispatcher (  )  )  )  . start (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "String   queueName    =     \" root . queue 1  \"  ;", "scheduler . allocConf    =    mock ( AllocationConfiguration . class )  ;", "when ( scheduler . allocConf . getMaxResources ( queueName )  )  . thenReturn ( maxResource )  ;", "when ( scheduler . allocConf . getMinResources ( queueName )  )  . thenReturn ( Resources . none (  )  )  ;", "schedulable    =    new    ( queueName ,    scheduler ,    null )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "FSAppAttempt   app    =    mock ( FSAppAttempt . class )  ;", "Mockito . when ( app . getDemand (  )  )  . thenReturn ( maxR )  ;", "schedulable . addAppSchedulable ( app )  ;", "schedulable . addAppSchedulable ( app )  ;", "schedulable . updateDemand (  )  ;", "assertTrue (  \" Demand   is   greater   than   max   allowed    \"  ,    Rs . equals ( schedulable . getDemand (  )  ,    maxR )  )  ;", "}", "METHOD_END"], "methodName": ["testUpdateDemand"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFSLeafQueue"}, {"methodBody": ["METHOD_START", "{", "scheduler    =    new   FairScheduler (  )  ;", "conf    =    createConfiguration (  )  ;", "resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( conf )  ;", "(  ( AsyncDispatcher )     ( resourceManager . getRMContext (  )  . getDispatcher (  )  )  )  . start (  )  ;", "resourceManager . getRMContext (  )  . getStateStore (  )  . start (  )  ;", "resourceManager . getRMContext (  )  . getContainerTokenSecretManager (  )  . rollMasterKey (  )  ;", "scheduler . setRMContext ( resourceManager . getRMContext (  )  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler    =    null ;", "resourceManager    =    null ;", "QueueMetrics . clearQueueMetrics (  )  ;", "DefaultMetricsSystem . shutdown (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"  >  \"  )  ;", "out . println (  \"        < aclSubmitApps >     <  / aclSubmitApps >  \"  )  ;", "out . println (  \"        < aclAdministerApps >     <  / aclAdministerApps >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" queue 1  \\  \"  >  \"  )  ;", "out . println (  \"              < aclSubmitApps > norealuserhasthisname <  / aclSubmitApps >  \"  )  ;", "out . println (  \"              < aclAdministerApps > norealuserhasthisname <  / aclAdministerApps >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" norealuserhasthisname \"  ,     1  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" norealuserhasthisname 2  \"  ,     1  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( attId 1  )  ;", "assertNotNull (  \" The   application   was   not   allowed \"  ,    app 1  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( attId 2  )  ;", "assertNull (  \" The   application   was   allowed \"  ,    app 2  )  ;", "}", "METHOD_END"], "methodName": ["testAclSubmitApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >     =     (  ( AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >  )     ( resourceManager . getResourceScheduler (  )  )  )  ;", "TestSchedulerUtils . verifyAppAddedAndRemovedFromScheduler ( getSchedulerApplications (  )  ,        \" default \"  )  ;", "}", "METHOD_END"], "methodName": ["testAddAndRemoveAppFromFairScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getClusterResource (  )  . getMemory (  )  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  5  1  2  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "assertEquals (  1  5  3  6  ,    scheduler . getClusterResource (  )  . getMemory (  )  )  ;", "NodeRemovedSchedulerEvent   nodeEvent 3     =    new   NodeRemovedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 3  )  ;", "assertEquals (  5  1  2  ,    scheduler . getClusterResource (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAggregateCapacityTracking"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   attemptId    =    createAppAttemptId (  1  ,     1  )  ;", "AppAddedSchedulerEvent   appAddedEvent    =    new   AppAddedSchedulerEvent ( attemptId . getApplicationId (  )  ,     \" default \"  ,     \" user 1  \"  )  ;", "scheduler . handle ( appAddedEvent )  ;", "AppAttemptAddedSchedulerEvent   attemptAddedEvent    =    new   AppAttemptAddedSchedulerEvent ( createAppAttemptId (  1  ,     1  )  ,    false )  ;", "scheduler . handle ( attemptAddedEvent )  ;", "assertEquals (  2  ,    scheduler . getQueueManager (  )  . getLeafQueues (  )  . size (  )  )  ;", "assertEquals (  1  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" user 1  \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 1     =    new   AppAttemptRemovedSchedulerEvent ( createAppAttemptId (  1  ,     1  )  ,    RMAppAttemptState . FINISHED ,    false )  ;", "scheduler . handle ( appRemovedEvent 1  )  ;", "assertEquals (  0  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" user 1  \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAppAdditionAndRemoval"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "final   String   user    =     \" user 1  \"  ;", "final   String   fifoQueue    =     \" fifo \"  ;", "final   String   fairParent    =     \" fairParent \"  ;", "final   String   fairChild 1     =    fairParent    +     \"  . fairChild 1  \"  ;", "final   String   fairChild 2     =    fairParent    +     \"  . fairChild 2  \"  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  8  1  9  2  ,     8  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  8  1  9  2  ,     8  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,    fifoQueue ,    user ,     4  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,    fairChild 1  ,    user ,     4  )  ;", "ApplicationAttemptId   attId 3     =    createSchedulingRequest (  1  0  2  4  ,    fairChild 2  ,    user ,     4  )  ;", "ApplicationAttemptId   attId 4     =    createSchedulingRequest (  1  0  2  4  ,    fifoQueue ,    user ,     4  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( attId 1  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( attId 2  )  ;", "FSAppAttempt   app 3     =    scheduler . getSchedulerApp ( attId 3  )  ;", "FSAppAttempt   app 4     =    scheduler . getSchedulerApp ( attId 4  )  ;", "scheduler . getQueueManager (  )  . getLeafQueue ( fifoQueue ,    true )  . setPolicy ( SchedulingPolicy . parse (  \" fifo \"  )  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent 1     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "NodeUpdateSchedulerEvent   updateEvent 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "for    ( int   i    =     0  ;    i    <     8  ;    i +  +  )     {", "scheduler . handle ( updateEvent 1  )  ;", "scheduler . handle ( updateEvent 2  )  ;", "if    (  (  ( i    +     1  )     %     2  )     =  =     0  )     {", "String   ERR    =     (  \" Wrong   number   of   assigned   containers   after    \"     +     ( i    +     1  )  )     +     \"    updates \"  ;", "if    ( i    <     4  )     {", "assertEquals ( ERR ,     ( i    +     1  )  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals ( ERR ,     0  ,    app 4  . getLiveContainers (  )  . size (  )  )  ;", "} else    {", "assertEquals ( ERR ,     4  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals ( ERR ,     ( i    -     3  )  ,    app 4  . getLiveContainers (  )  . size (  )  )  ;", "}", "assertEquals ( ERR ,     (  ( i    +     1  )     /     2  )  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals ( ERR ,     (  ( i    +     1  )     /     2  )  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testAssignContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,     \" true \"  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "scheduler . getQueueManager (  )  . getLeafQueue (  \" root . child 1  . granchild \"  ,    true )  ;", "scheduler . getQueueManager (  )  . getLeafQueue (  \" root . child 2  \"  ,    true )  ;", "RMApp   rmApp 1     =    new   MockRMApp (  0  ,     0  ,    RMAppState . NEW )  ;", "RMApp   rmApp 2     =    new   MockRMApp (  1  ,     1  ,    RMAppState . NEW )  ;", "assertNull ( scheduler . assignToQueue ( rmApp 1  ,     \" root . child 1  \"  ,     \" tintin \"  )  )  ;", "assertNotNull ( scheduler . assignToQueue ( rmApp 2  ,     \" root . child 2  \"  ,     \" snowy \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testAssignToNonLeafQueueReturnsNull"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,     \" true \"  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMApp   rmApp 1     =    new   MockRMApp (  0  ,     0  ,    RMAppState . NEW )  ;", "RMApp   rmApp 2     =    new   MockRMApp (  1  ,     1  ,    RMAppState . NEW )  ;", "FSLeafQueue   queue 1     =    scheduler . assignToQueue ( rmApp 1  ,     \" default \"  ,     \" asterix \"  )  ;", "FSLeafQueue   queue 2     =    scheduler . assignToQueue ( rmApp 2  ,     \" notdefault \"  ,     \" obelix \"  )  ;", "assertEquals ( rmApp 1  . getQueue (  )  ,    queue 1  . getName (  )  )  ;", "assertEquals (  \" root . asterix \"  ,    rmApp 1  . getQueue (  )  )  ;", "assertEquals ( rmApp 2  . getQueue (  )  ,    queue 2  . getName (  )  )  ;", "assertEquals (  \" root . notdefault \"  ,    rmApp 2  . getQueue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAssignToQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    BuilderUtils . newResource (  8  1  9  2  ,     5  )  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "ApplicationAttemptId   appAttId 1     =    createSchedulingRequest (  2  0  4  8  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( appAttId 1  )  ;", "ApplicationAttemptId   appAttId 2     =    createSchedulingRequest (  1  0  2  4  ,     2  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( appAttId 2  )  ;", "DominantResourceFairnessPolicy   drfPolicy    =    new   DominantResourceFairnessPolicy (  )  ;", "drfPolicy . initialize ( scheduler . getClusterResource (  )  )  ;", "scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . setPolicy ( drfPolicy )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  2  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBasicDRFAssignment"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    BuilderUtils . newResource (  8  1  9  2  ,     7  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "ApplicationAttemptId   appAttId 1     =    createSchedulingRequest (  3  0  7  2  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( appAttId 1  )  ;", "ApplicationAttemptId   appAttId 2     =    createSchedulingRequest (  2  0  4  8  ,     2  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( appAttId 2  )  ;", "ApplicationAttemptId   appAttId 3     =    createSchedulingRequest (  1  0  2  4  ,     2  ,     \" queue 2  \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app 3     =    scheduler . getSchedulerApp ( appAttId 3  )  ;", "DominantResourceFairnessPolicy   drfPolicy    =    new   DominantResourceFairnessPolicy (  )  ;", "drfPolicy . initialize ( scheduler . getClusterResource (  )  )  ;", "scheduler . getQueueManager (  )  . getQueue (  \" root \"  )  . setPolicy ( drfPolicy )  ;", "scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . setPolicy ( drfPolicy )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  2  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBasicDRFWithQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "final   int   GB    =     1  0  2  4  ;", "String   host    =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  1  6     *    GB )  ,     1  6  )  ,     0  ,    host )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "ApplicationAttemptId   appAttemptId    =    createSchedulingRequest ( GB ,     \" root . default \"  ,     \" user \"  ,     1  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( appAttemptId )  ;", "scheduler . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    Collections . singletonList ( host )  ,    null )  ;", "assertTrue ( app . isBlacklisted ( host )  )  ;", "scheduler . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    null ,    Collections . singletonList ( host )  )  ;", "assertFalse ( scheduler . getSchedulerApp ( appAttemptId )  . isBlacklisted ( host )  )  ;", "List < ResourceRequest >    update    =    Arrays . asList ( createResourceRequest ( GB ,    node . getHostName (  )  ,     1  ,     0  ,    true )  )  ;", "scheduler . allocate ( appAttemptId ,    update ,    Collections .  < ContainerId > emptyList (  )  ,    Collections . singletonList ( host )  ,    null )  ;", "assertTrue ( app . isBlacklisted ( host )  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Incorrect   number   of   containers   allocated \"  ,     0  ,    app . getLiveContainers (  )  . size (  )  )  ;", "scheduler . allocate ( appAttemptId ,    update ,    Collections .  < ContainerId > emptyList (  )  ,    null ,    Collections . singletonList ( host )  )  ;", "assertFalse ( app . isBlacklisted ( host )  )  ;", "createSchedulingRequest ( GB ,     \" root . default \"  ,     \" user \"  ,     1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Incorrect   number   of   containers   allocated \"  ,     1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBlacklistNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     0  )  ;", "ResourceRequest   nodeRequest    =    createResourceRequest (  1  0  2  4  ,    node 1  . getHostName (  )  ,     1  ,     1  ,    true )  ;", "ResourceRequest   rackRequest    =    createResourceRequest (  1  0  2  4  ,     \" rack 1  \"  ,     1  ,     1  ,    false )  ;", "ResourceRequest   anyRequest    =    createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     1  ,    false )  ;", "createSchedulingRequestExistingApplication ( nodeRequest ,    attId 1  )  ;", "createSchedulingRequestExistingApplication ( rackRequest ,    attId 1  )  ;", "createSchedulingRequestExistingApplication ( anyRequest ,    attId 1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   node 2 UpdateEvent    =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId 1  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "scheduler . handle ( node 2 UpdateEvent )  ;", "assertEquals (  0  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "List < ResourceRequest >    update    =    Arrays . asList ( createResourceRequest (  1  0  2  4  ,    node 1  . getHostName (  )  ,     1  ,     0  ,    true )  ,    createResourceRequest (  1  0  2  4  ,     \" rack 1  \"  ,     1  ,     0  ,    true )  ,    createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     1  ,    true )  )  ;", "scheduler . allocate ( attId 1  ,    update ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "scheduler . handle ( node 2 UpdateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCancelStrictLocality"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setLong ( FairSchedulerConfiguration . PREEMPTION _ INTERVAL ,     5  0  0  0  )  ;", "conf . setLong ( FairSchedulerConfiguration . WAIT _ TIME _ BEFORE _ KILL ,     1  0  0  0  0  )  ;", "conf . set (  (  ( FairSchedulerConfiguration . ALLOCATION _ FILE )     +     \"  . allocation . file \"  )  ,     . ALLOC _ FILE )  ;", "conf . set ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,     \" false \"  )  ;", "FairSchedulerTestBase . MockClock   clock    =    new   FairSchedulerTestBase . MockClock (  )  ;", "scheduler . setClock ( clock )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueC \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" default \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  4     *     1  0  2  4  )  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  4     *     1  0  2  4  )  ,     4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   app 1     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueA \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "createSchedulingRequestExistingApplication (  (  1     *     1  0  2  4  )  ,     1  ,     2  ,    app 1  )  ;", "ApplicationAttemptId   app 2     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueA \"  ,     \" user 1  \"  ,     1  ,     3  )  ;", "createSchedulingRequestExistingApplication (  (  1     *     1  0  2  4  )  ,     1  ,     4  ,    app 2  )  ;", "ApplicationAttemptId   app 3     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "createSchedulingRequestExistingApplication (  (  1     *     1  0  2  4  )  ,     1  ,     2  ,    app 3  )  ;", "ApplicationAttemptId   app 4     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     3  )  ;", "createSchedulingRequestExistingApplication (  (  1     *     1  0  2  4  )  ,     1  ,     4  ,    app 4  )  ;", "scheduler . update (  )  ;", "scheduler . getQueueManager (  )  . getLeafQueue (  \" queueA \"  ,    true )  . setPolicy ( SchedulingPolicy . parse (  \" fifo \"  )  )  ;", "scheduler . getQueueManager (  )  . getLeafQueue (  \" queueB \"  ,    true )  . setPolicy ( SchedulingPolicy . parse (  \" fair \"  )  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate 1     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "for    ( int   i    =     0  ;    i    <     4  ;    i +  +  )     {", "scheduler . handle ( nodeUpdate 1  )  ;", "scheduler . handle ( nodeUpdate 2  )  ;", "}", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 1  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 3  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 4  )  . getLiveContainers (  )  . size (  )  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueC \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueC \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" default \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" default \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "scheduler . update (  )  ;", "scheduler . preemptResources ( Resources . createResource (  (  2     *     1  0  2  4  )  )  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 1  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 3  )  . getLiveContainers (  )  . size (  )  )  ;", "assertTrue (  \" App 2    should   have   container   to   be   preempted \"  ,     (  !  ( Collections . disjoint ( scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  ,    scheduler . getSchedulerApp ( app 2  )  . getPreemptionContainers (  )  )  )  )  )  ;", "assertTrue (  \" App 4    should   have   container   to   be   preempted \"  ,     (  !  ( Collections . disjoint ( scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  ,    scheduler . getSchedulerApp ( app 2  )  . getPreemptionContainers (  )  )  )  )  )  ;", "clock . tick (  1  5  )  ;", "scheduler . preemptResources ( Resources . createResource (  (  2     *     1  0  2  4  )  )  )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( app 4  )  . getLiveContainers (  )  . size (  )  )  ;", "Set < RMContainer >    set    =    new   HashSet < RMContainer >  (  )  ;", "for    ( RMContainer   container    :    scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  )     {", "if    (  ( container . getAllocatedPriority (  )  . getPriority (  )  )     =  =     4  )     {", "set . add ( container )  ;", "}", "}", "for    ( RMContainer   container    :    scheduler . getSchedulerApp ( app 4  )  . getLiveContainers (  )  )     {", "if    (  ( container . getAllocatedPriority (  )  . getPriority (  )  )     =  =     4  )     {", "set . add ( container )  ;", "}", "}", "assertTrue (  (  \" Containers   with   priority =  4    in   app 2    and   app 4    should   be    \"     +     \" preempted .  \"  )  ,    set . isEmpty (  )  )  ;", "scheduler . preemptResources ( Resources . createResource (  (  2     *     1  0  2  4  )  )  )  ;", "clock . tick (  1  5  )  ;", "scheduler . preemptResources ( Resources . createResource (  (  2     *     1  0  2  4  )  )  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( app 1  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( app 3  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( app 4  )  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . preemptResources ( Resources . createResource (  (  2     *     1  0  2  4  )  )  )  ;", "assertTrue (  \" App 1    should   have   no   container   to   be   preempted \"  ,    scheduler . getSchedulerApp ( app 1  )  . getPreemptionContainers (  )  . isEmpty (  )  )  ;", "assertTrue (  \" App 2    should   have   no   container   to   be   preempted \"  ,    scheduler . getSchedulerApp ( app 2  )  . getPreemptionContainers (  )  . isEmpty (  )  )  ;", "assertTrue (  \" App 3    should   have   no   container   to   be   preempted \"  ,    scheduler . getSchedulerApp ( app 3  )  . getPreemptionContainers (  )  . isEmpty (  )  )  ;", "assertTrue (  \" App 4    should   have   no   container   to   be   preempted \"  ,    scheduler . getSchedulerApp ( app 4  )  . getPreemptionContainers (  )  . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testChoiceOfPreemptedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FairScheduler   scheduler    =    new   FairScheduler (  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     2  0  4  8  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,     1  0  2  4  )  ;", "try    {", "scheduler . serviceInit ( conf )  ;", "fail (  (  \" Exception   is   expected   because   the   min   memory   allocation   is \"     +     \"    larger   than   the   max   memory   allocation .  \"  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertTrue (  \" The   thrown   exception   is   not   the   expected   one .  \"  ,    e . getMessage (  )  . startsWith (  \" Invalid   resource   scheduler   memory \"  )  )  ;", "}", "conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,     2  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,     1  )  ;", "try    {", "scheduler . serviceInit ( conf )  ;", "fail (  (  \" Exception   is   expected   because   the   min   vcores   allocation   is \"     +     \"    larger   than   the   max   vcores   allocation .  \"  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertTrue (  \" The   thrown   exception   is   not   the   expected   one .  \"  ,    e . getMessage (  )  . startsWith (  \" Invalid   resource   scheduler   vcores \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testConfValidation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < defaultQueueSchedulingPolicy > fair <  / defaultQueueSchedulingPolicy >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"  >  \"  )  ;", "out . println (  \"        < schedulingPolicy > drf <  / schedulingPolicy >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 1  \\  \"  >  \"  )  ;", "out . println (  \"              < minResources >  1  0  2  4 mb ,  1 vcores <  / minResources >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 2  \\  \"  >  \"  )  ;", "out . println (  \"              < minResources >  1  0  2  4 mb ,  4 vcores <  / minResources >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "FSQueue   root    =    queueManager . getRootQueue (  )  ;", "assertTrue (  (  ( root . getPolicy (  )  )    instanceof   DominantResourceFairnessPolicy )  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" child 1  \"  ,    false )  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" child 2  \"  ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["testConfigureRootQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FairScheduler   fs    =    new   FairScheduler (  )  ;", "Configuration   conf    =    createConfiguration (  )  ;", "conf . setBoolean ( FairSchedulerConfiguration . CONTINUOUS _ SCHEDULING _ ENABLED ,    true )  ;", "fs . setRMContext ( resourceManager . getRMContext (  )  )  ;", "fs . init ( conf )  ;", "fs . start (  )  ;", "fs . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "Assert . assertTrue (  \" Continuous   scheduling   should   be   enabled .  \"  ,    fs . isContinuousSchedulingEnabled (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  8     *     1  0  2  4  )  ,     8  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "fs . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  8     *     1  0  2  4  )  ,     8  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "fs . handle ( nodeEvent 2  )  ;", "Assert . assertEquals ( fs . getClusterResource (  )  . getMemory (  )  ,     (  1  6     *     1  0  2  4  )  )  ;", "Assert . assertEquals ( fs . getClusterResource (  )  . getVirtualCores (  )  ,     1  6  )  ;", "ApplicationAttemptId   appAttemptId    =    createAppAttemptId (  (  ( this . APP _ ID )  +  +  )  ,     (  ( this . ATTEMPT _ ID )  +  +  )  )  ;", "fs . addApplication ( appAttemptId . getApplicationId (  )  ,     \" queue 1  1  \"  ,     \" user 1  1  \"  ,    false )  ;", "fs . addApplicationAttempt ( appAttemptId ,    false ,    false )  ;", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   request    =    createResourceRequest (  1  0  2  4  ,     1  ,    ANY ,     1  ,     1  ,    true )  ;", "ask . add ( request )  ;", "fs . allocate ( appAttemptId ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "Thread . sleep (  (  ( fs . getConf (  )  . getContinuousSchedulingSleepMs (  )  )     +     5  0  0  )  )  ;", "FSAppAttempt   app    =    fs . getSchedulerApp ( appAttemptId )  ;", "while    ( app . getCurrentConsumption (  )  . equals ( Resources . none (  )  )  )     {", "}", "Assert . assertEquals (  1  0  2  4  ,    app . getCurrentConsumption (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  1  ,    app . getCurrentConsumption (  )  . getVirtualCores (  )  )  ;", "request    =    createResourceRequest (  1  0  2  4  ,     1  ,    ANY ,     2  ,     1  ,    true )  ;", "ask . clear (  )  ;", "ask . add ( request )  ;", "fs . allocate ( appAttemptId ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "while    ( app . getCurrentConsumption (  )  . equals ( Resources . createResource (  1  0  2  4  ,     1  )  )  )     {", "}", "Assert . assertEquals (  2  0  4  8  ,    app . getCurrentConsumption (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  2  ,    app . getCurrentConsumption (  )  . getVirtualCores (  )  )  ;", "Set < NodeId >    nodes    =    new   HashSet < NodeId >  (  )  ;", "Iterator < RMContainer >    it    =    app . getLiveContainers (  )  . iterator (  )  ;", "while    ( it . hasNext (  )  )     {", "nodes . add ( it . next (  )  . getContainer (  )  . getNodeId (  )  )  ;", "}", "Assert . assertEquals (  2  ,    nodes . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContinuousScheduling"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "Assert . assertTrue (  \" Continuous   scheduling   should   be   disabled .  \"  ,     (  !  ( scheduler . isContinuousSchedulingEnabled (  )  )  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  8     *     1  0  2  4  )  ,     8  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  8     *     1  0  2  4  )  ,     8  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "Assert . assertEquals (  \" We   should   have   two   alive   nodes .  \"  ,     2  ,    scheduler . getNumClusterNodes (  )  )  ;", "NodeRemovedSchedulerEvent   removeNode 1     =    new   NodeRemovedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( removeNode 1  )  ;", "Assert . assertEquals (  \" We   should   only   have   one   alive   node .  \"  ,     1  ,    scheduler . getNumClusterNodes (  )  )  ;", "try    {", "scheduler . continuousSchedulingAttempt (  )  ;", "}    catch    ( Exception   e )     {", "fail (  (  \" Exception   happened   when   doing   continuous   scheduling .     \"     +     ( e . toString (  )  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testContinuousSchedulingWithNodeRemoved"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    BuilderUtils . newResource (  1  2  2  8  8  ,     1  2  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "ApplicationAttemptId   appAttId 1     =    createSchedulingRequest (  3  0  7  4  ,     1  ,     \" queue 1  . subqueue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "Thread . sleep (  3  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( appAttId 1  )  ;", "ApplicationAttemptId   appAttId 2     =    createSchedulingRequest (  1  0  2  4  ,     3  ,     \" queue 1  . subqueue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "Thread . sleep (  3  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( appAttId 2  )  ;", "ApplicationAttemptId   appAttId 3     =    createSchedulingRequest (  2  0  4  8  ,     2  ,     \" queue 1  . subqueue 2  \"  ,     \" user 1  \"  ,     2  )  ;", "Thread . sleep (  3  )  ;", "FSAppAttempt   app 3     =    scheduler . getSchedulerApp ( appAttId 3  )  ;", "ApplicationAttemptId   appAttId 4     =    createSchedulingRequest (  1  0  2  4  ,     2  ,     \" queue 2  \"  ,     \" user 1  \"  ,     2  )  ;", "Thread . sleep (  3  )  ;", "FSAppAttempt   app 4     =    scheduler . getSchedulerApp ( appAttId 4  )  ;", "DominantResourceFairnessPolicy   drfPolicy    =    new   DominantResourceFairnessPolicy (  )  ;", "drfPolicy . initialize ( scheduler . getClusterResource (  )  )  ;", "scheduler . getQueueManager (  )  . getQueue (  \" root \"  )  . setPolicy ( drfPolicy )  ;", "scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . setPolicy ( drfPolicy )  ;", "scheduler . getQueueManager (  )  . getQueue (  \" queue 1  . subqueue 1  \"  )  . setPolicy ( drfPolicy )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 4  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  2  ,    app 4  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  2  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "Assert . assertEquals (  1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    app 4  . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDRFHierarchicalQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "conf . setBoolean ( FairSchedulerConfiguration . ALLOW _ UNDECLARED _ POOLS ,    false )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "List < QueuePlacementRule >    rules    =    scheduler . allocConf . placementPolicy . getRules (  )  ;", "for    ( QueuePlacementRule   rule    :    rules )     {", "if    ( rule   instanceof   QueuePlacementRule . Default )     {", "QueuePlacementRule . Default   defaultRule    =     (  ( QueuePlacementRule . Default )     ( rule )  )  ;", "assertNotNull ( defaultRule . defaultQueueName )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testDefaultRuleInitializesProperlyWhenPolicyNotConfigured"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( FairSchedulerConfiguration . ALLOW _ UNDECLARED _ POOLS ,    false )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" jerry \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "FSLeafQueue   jerryQueue    =    queueManager . getLeafQueue (  \" jerry \"  ,    false )  ;", "FSLeafQueue   defaultQueue    =    queueManager . getLeafQueue (  \" default \"  ,    false )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" jerry \"  ,     \" someuser \"  )  ;", "assertEquals (  1  ,    jerryQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" newqueue \"  ,     \" someuser \"  )  ;", "assertEquals (  1  ,    jerryQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    defaultQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" someuser \"  )  ;", "assertEquals (  1  ,    jerryQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  2  ,    defaultQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" jerry \"  )  ;", "assertEquals (  2  ,    jerryQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  2  ,    defaultQueue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDontAllowUndeclaredPools"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "assertEquals (  1  ,    scheduler . getQueueManager (  )  . getLeafQueues (  )  . size (  )  )  ;", "ApplicationAttemptId   appAttemptId    =    createAppAttemptId (  1  ,     1  )  ;", "AppAddedSchedulerEvent   appAddedEvent    =    new   AppAddedSchedulerEvent ( appAttemptId . getApplicationId (  )  ,     \"  \"  ,     \" user 1  \"  )  ;", "scheduler . handle ( appAddedEvent )  ;", "assertEquals (  1  ,    scheduler . getQueueManager (  )  . getLeafQueues (  )  . size (  )  )  ;", "assertNull ( scheduler . getSchedulerApp ( appAttemptId )  )  ;", "assertEquals (  0  ,    resourceManager . getRMContext (  )  . getRMApps (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testEmptyQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" parentq \\  \"    type =  \\  \" parent \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queuePlacementPolicy >  \"  )  ;", "out . println (  \"  < rule   name =  \\  \" nestedUserQueue \\  \"  >  \"  )  ;", "out . println (  \"                 < rule   name =  \\  \" specified \\  \"    create =  \\  \" false \\  \"     /  >  \"  )  ;", "out . println (  \"  <  / rule >  \"  )  ;", "out . println (  \"  < rule   name =  \\  \" default \\  \"     /  >  \"  )  ;", "out . println (  \"  <  / queuePlacementPolicy >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "RMApp   rmApp 1     =    new   MockRMApp (  0  ,     0  ,    RMAppState . NEW )  ;", "RMApp   rmApp 2     =    new   MockRMApp (  1  ,     1  ,    RMAppState . NEW )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "int   capacity    =     1  6     *     1  0  2  4  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource ( capacity )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" root . parentq \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" root . parentq \"  ,     \" user 2  \"  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" root . default \"  ,     \" user 3  \"  )  ;", "scheduler . update (  )  ;", "scheduler . getQueueManager (  )  . getRootQueue (  )  . setSteadyFairShare ( scheduler . getClusterResource (  )  )  ;", "scheduler . getQueueManager (  )  . getRootQueue (  )  . recomputeSteadyShares (  )  ;", "Collection < FSLeafQueue >    leafQueues    =    scheduler . getQueueManager (  )  . getLeafQueues (  )  ;", "for    ( FSLeafQueue   leaf    :    leafQueues )     {", "if    (  ( leaf . getName (  )  . equals (  \" root . parentq . user 1  \"  )  )     |  |     ( leaf . getName (  )  . equals (  \" root . parentq . user 2  \"  )  )  )     {", "assertEquals (  ( capacity    /     4  )  ,    leaf . getFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    leaf . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  1  .  0  ,    leaf . getWeights (  )  . getWeight ( ResourceType . MEMORY )  ,     0  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFairShareAndWeightsInNestedUserQueueRule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  3     *     1  0  2  4  )  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" queueA \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" queueB \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "Collection < FSLeafQueue >    queues    =    scheduler . getQueueManager (  )  . getLeafQueues (  )  ;", "assertEquals (  3  ,    queues . size (  )  )  ;", "for    ( FSLeafQueue   p    :    queues )     {", "if    ( p . getName (  )  . equals (  \" root . queueA \"  )  )     {", "assertEquals (  1  0  2  4  ,    p . getFairShare (  )  . getMemory (  )  )  ;", "} else", "if    ( p . getName (  )  . equals (  \" root . queueB \"  )  )     {", "assertEquals (  2  0  4  8  ,    p . getFairShare (  )  . getMemory (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFairShareWithMinAlloc"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  3  0  7  2  ,     3  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( attId 1  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( attId 2  )  ;", "FSLeafQueue   queue 1     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "queue 1  . setPolicy ( new   FifoPolicy (  )  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  2  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  2  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFifoWithinQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   appAttId 1     =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  . subqueue 1  \"  ,     \" user 1  \"  )  ;", "ApplicationAttemptId   appAttId 2     =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  . subqueue 2  \"  ,     \" user 1  \"  )  ;", "ApplicationAttemptId   appAttId 3     =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" default \"  ,     \" user 1  \"  )  ;", "List < ApplicationAttemptId >    apps    =    scheduler . getAppsInQueue (  \" queue 1  . subqueue 1  \"  )  ;", "assertEquals (  1  ,    apps . size (  )  )  ;", "assertEquals ( appAttId 1  ,    apps . get (  0  )  )  ;", "apps    =    scheduler . getAppsInQueue (  \" root . queue 1  . subqueue 1  \"  )  ;", "assertEquals (  1  ,    apps . size (  )  )  ;", "assertEquals ( appAttId 1  ,    apps . get (  0  )  )  ;", "apps    =    scheduler . getAppsInQueue (  \" user 1  \"  )  ;", "assertEquals (  1  ,    apps . size (  )  )  ;", "assertEquals ( appAttId 3  ,    apps . get (  0  )  )  ;", "apps    =    scheduler . getAppsInQueue (  \" root . user 1  \"  )  ;", "assertEquals (  1  ,    apps . size (  )  )  ;", "assertEquals ( appAttId 3  ,    apps . get (  0  )  )  ;", "apps    =    scheduler . getAppsInQueue (  \" queue 1  \"  )  ;", "Assert . assertEquals (  2  ,    apps . size (  )  )  ;", "Set < ApplicationAttemptId >    appAttIds    =    Sets . newHashSet ( apps . get (  0  )  ,    apps . get (  1  )  )  ;", "assertTrue ( appAttIds . contains ( appAttId 1  )  )  ;", "assertTrue ( appAttIds . contains ( appAttId 2  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetAppsInQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueC \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueD \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "Collection < FSLeafQueue >    leafQueues    =    queueManager . getLeafQueues (  )  ;", "Assert . assertEquals (  4  ,    leafQueues . size (  )  )  ;", "Assert . assertNotNull ( queueManager . getLeafQueue (  \" queueA \"  ,    false )  )  ;", "Assert . assertNotNull ( queueManager . getLeafQueue (  \" queueB . queueC \"  ,    false )  )  ;", "Assert . assertNotNull ( queueManager . getLeafQueue (  \" queueB . queueD \"  ,    false )  )  ;", "Assert . assertNotNull ( queueManager . getLeafQueue (  \" default \"  ,    false )  )  ;", "Assert . assertEquals (  4  ,    leafQueues . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHierarchicalQueueAllocationFileParsing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "FSLeafQueue   leafQueue    =    queueManager . getLeafQueue (  \" parent . child \"  ,    true )  ;", "Assert . assertEquals (  2  ,    queueManager . getLeafQueues (  )  . size (  )  )  ;", "Assert . assertNotNull ( leafQueue )  ;", "Assert . assertEquals (  \" root . parent . child \"  ,    leafQueue . getName (  )  )  ;", "FSLeafQueue   leafQueue 2     =    queueManager . getLeafQueue (  \" parent \"  ,    true )  ;", "Assert . assertNull ( leafQueue 2  )  ;", "Assert . assertEquals (  2  ,    queueManager . getLeafQueues (  )  . size (  )  )  ;", "FSLeafQueue   leafQueue 3     =    queueManager . getLeafQueue (  \" parent . child . grandchild \"  ,    true )  ;", "Assert . assertNull ( leafQueue 3  )  ;", "Assert . assertEquals (  2  ,    queueManager . getLeafQueues (  )  . size (  )  )  ;", "FSLeafQueue   leafQueue 4     =    queueManager . getLeafQueue (  \" parent . sister \"  ,    true )  ;", "Assert . assertNotNull ( leafQueue 4  )  ;", "Assert . assertEquals (  \" root . parent . sister \"  ,    leafQueue 4  . getName (  )  )  ;", "Assert . assertEquals (  3  ,    queueManager . getLeafQueues (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHierarchicalQueuesSimilarParents"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( RM _ SCHEDULER _ INCLUDE _ PORT _ IN _ NODE _ NAME ,    true )  ;", "init ( conf )  ;", "start (  )  ;", "reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  ,     1  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  1  \"  ,     2  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     0  )  ;", "ResourceRequest   nodeRequest    =    createResourceRequest (  1  0  2  4  ,     (  (  ( node 1  . getNodeID (  )  . getHost (  )  )     +     \"  :  \"  )     +     ( node 1  . getNodeID (  )  . getPort (  )  )  )  ,     1  ,     1  ,    true )  ;", "ResourceRequest   rackRequest    =    createResourceRequest (  1  0  2  4  ,    node 1  . getRackName (  )  ,     1  ,     1  ,    false )  ;", "ResourceRequest   anyRequest    =    createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     1  ,    false )  ;", "createSchedulingRequestExistingApplication ( nodeRequest ,    attId 1  )  ;", "createSchedulingRequestExistingApplication ( rackRequest ,    attId 1  )  ;", "createSchedulingRequestExistingApplication ( anyRequest ,    attId 1  )  ;", "update (  )  ;", "NodeUpdateSchedulerEvent   node 1 UpdateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "NodeUpdateSchedulerEvent   node 2 UpdateEvent    =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "FSAppAttempt   app    =    getSchedulerApp ( attId 1  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "handle ( node 2 UpdateEvent )  ;", "assertEquals (  0  ,    app . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    app . getReservedContainers (  )  . size (  )  )  ;", "}", "handle ( node 1 UpdateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHostPortNodeName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  7  5  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  4     *     1  0  2  4  )  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  (  3     *     1  0  2  4  )  ,     \" queueA \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   nodeEvent 2     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueB \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "Collection < FSLeafQueue >    queues    =    scheduler . getQueueManager (  )  . getLeafQueues (  )  ;", "assertEquals (  3  ,    queues . size (  )  )  ;", "for    ( FSLeafQueue   p    :    queues )     {", "if    ( p . getName (  )  . equals (  \" root . queueA \"  )  )     {", "assertEquals ( false ,    scheduler . isStarvedForFairShare ( p )  )  ;", "} else", "if    ( p . getName (  )  . equals (  \" root . queueB \"  )  )     {", "assertEquals ( true ,    scheduler . isStarvedForFairShare ( p )  )  ;", "}", "}", "scheduler . handle ( nodeEvent 2  )  ;", "for    ( FSLeafQueue   p    :    queues )     {", "if    ( p . getName (  )  . equals (  \" root . queueB \"  )  )     {", "assertEquals ( false ,    scheduler . isStarvedForFairShare ( p )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testIsStarvedForFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  2  0  4  8 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  4     *     1  0  2  4  )  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  (  3     *     1  0  2  4  )  ,     \" queueA \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   nodeEvent 2     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueB \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "Collection < FSLeafQueue >    queues    =    scheduler . getQueueManager (  )  . getLeafQueues (  )  ;", "assertEquals (  3  ,    queues . size (  )  )  ;", "for    ( FSLeafQueue   p    :    queues )     {", "if    ( p . getName (  )  . equals (  \" root . queueA \"  )  )     {", "assertEquals ( false ,    scheduler . isStarvedForMinShare ( p )  )  ;", "} else", "if    ( p . getName (  )  . equals (  \" root . queueB \"  )  )     {", "assertEquals ( true ,    scheduler . isStarvedForMinShare ( p )  )  ;", "}", "}", "scheduler . handle ( nodeEvent 2  )  ;", "for    ( FSLeafQueue   p    :    queues )     {", "if    ( p . getName (  )  . equals (  \" root . queueB \"  )  )     {", "assertEquals ( false ,    scheduler . isStarvedForMinShare ( p )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testIsStarvedForMinShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( FairSchedulerConfiguration . ASSIGN _ MULTIPLE ,    true )  ;", "conf . setInt ( FairSchedulerConfiguration . MAX _ ASSIGN ,     3  )  ;", "conf . setBoolean ( FairSchedulerConfiguration . SIZE _ BASED _ WEIGHT ,    true )  ;", "conf . setDouble ( FairSchedulerConfiguration . LOCALITY _ THRESHOLD _ NODE ,     0  .  5  )  ;", "conf . setDouble ( FairSchedulerConfiguration . LOCALITY _ THRESHOLD _ RACK ,     0  .  7  )  ;", "conf . setBoolean ( FairSchedulerConfiguration . CONTINUOUS _ SCHEDULING _ ENABLED ,    true )  ;", "conf . setInt ( FairSchedulerConfiguration . CONTINUOUS _ SCHEDULING _ SLEEP _ MS ,     1  0  )  ;", "conf . setInt ( FairSchedulerConfiguration . LOCALITY _ DELAY _ RACK _ MS ,     5  0  0  0  )  ;", "conf . setInt ( FairSchedulerConfiguration . LOCALITY _ DELAY _ NODE _ MS ,     5  0  0  0  )  ;", "conf . setInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,     1  0  2  4  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     5  1  2  )  ;", "conf . setInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ,     1  2  8  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "Assert . assertEquals ( true ,    scheduler . assignMultiple )  ;", "Assert . assertEquals (  3  ,    scheduler . maxAssign )  ;", "Assert . assertEquals ( true ,    scheduler . sizeBasedWeight )  ;", "Assert . assertEquals (  0  .  5  ,    scheduler . nodeLocalityThreshold ,     0  .  0  1  )  ;", "Assert . assertEquals (  0  .  7  ,    scheduler . rackLocalityThreshold ,     0  .  0  1  )  ;", "Assert . assertTrue (  \" The   continuous   scheduling   should   be   enabled \"  ,    scheduler . continuousSchedulingEnabled )  ;", "Assert . assertEquals (  1  0  ,    scheduler . continuousSchedulingSleepMs )  ;", "Assert . assertEquals (  5  0  0  0  ,    scheduler . nodeLocalityDelayMs )  ;", "Assert . assertEquals (  5  0  0  0  ,    scheduler . rackLocalityDelayMs )  ;", "Assert . assertEquals (  1  0  2  4  ,    scheduler . getMaximumResourceCapability (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  5  1  2  ,    scheduler . getMinimumResourceCapability (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  1  2  8  ,    scheduler . getIncrementResourceCapability (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testLoadConfigurationOnInitialize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "FSQueue   aQueue    =    mock ( FSLeafQueue . class )  ;", "FSQueue   bQueue    =    mock ( FSLeafQueue . class )  ;", "FSQueue   a 1 Queue    =    mock ( FSLeafQueue . class )  ;", "FSQueue   b 1 Queue    =    mock ( FSLeafQueue . class )  ;", "when ( a 1 Queue . getName (  )  )  . thenReturn (  \" root . queue 1  . a . a 1  \"  )  ;", "when ( b 1 Queue . getName (  )  )  . thenReturn (  \" root . queue 1  . b . b 1  \"  )  ;", "when ( aQueue . getChildQueues (  )  )  . thenReturn ( Arrays . asList ( a 1 Queue )  )  ;", "when ( bQueue . getChildQueues (  )  )  . thenReturn ( Arrays . asList ( b 1 Queue )  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "FSParentQueue   queue 1     =    queueManager . getParentQueue (  \" queue 1  \"  ,    true )  ;", "queue 1  . addChildQueue ( aQueue )  ;", "queue 1  . addChildQueue ( bQueue )  ;", "FSQueue   ancestorQueue    =    indLowestCommonAncestorQueue ( a 1 Queue ,    b 1 Queue )  ;", "assertEquals ( ancestorQueue ,    queue 1  )  ;", "}", "METHOD_END"], "methodName": ["testLowestCommonAncestorDeeperHierarchy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "FSLeafQueue   aQueue    =    mock ( FSLeafQueue . class )  ;", "FSLeafQueue   bQueue    =    mock ( FSLeafQueue . class )  ;", "when ( aQueue . getName (  )  )  . thenReturn (  \" root . queue 1  . a \"  )  ;", "when ( bQueue . getName (  )  )  . thenReturn (  \" root . queue 1  . b \"  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "FSParentQueue   queue 1     =    queueManager . getParentQueue (  \" queue 1  \"  ,    true )  ;", "queue 1  . addChildQueue ( aQueue )  ;", "queue 1  . addChildQueue ( bQueue )  ;", "FSQueue   ancestorQueue    =    indLowestCommonAncestorQueue ( aQueue ,    bQueue )  ;", "assertEquals ( ancestorQueue ,    queue 1  )  ;", "}", "METHOD_END"], "methodName": ["testLowestCommonAncestorForNonRootParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "FSLeafQueue   aQueue    =    mock ( FSLeafQueue . class )  ;", "FSLeafQueue   bQueue    =    mock ( FSLeafQueue . class )  ;", "when ( aQueue . getName (  )  )  . thenReturn (  \" root . a \"  )  ;", "when ( bQueue . getName (  )  )  . thenReturn (  \" root . b \"  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "FSParentQueue   queue 1     =    queueManager . getParentQueue (  \" root \"  ,    false )  ;", "queue 1  . addChildQueue ( aQueue )  ;", "queue 1  . addChildQueue ( bQueue )  ;", "FSQueue   ancestorQueue    =    indLowestCommonAncestorQueue ( aQueue ,    bQueue )  ;", "assertEquals ( ancestorQueue ,    queue 1  )  ;", "}", "METHOD_END"], "methodName": ["testLowestCommonAncestorRootParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( FairSchedulerConfiguration . ASSIGN _ MULTIPLE ,    true )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  6  3  8  4  ,     1  6  )  ,     0  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  1  0  2  4  ,     \" root . default \"  ,     \" user \"  ,     8  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "scheduler . maxAssign    =     2  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Incorrect   number   of   containers   allocated \"  ,     2  ,    app . getLiveContainers (  )  . size (  )  )  ;", "scheduler . maxAssign    =     -  1  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Incorrect   number   of   containers   allocated \"  ,     8  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMaxAssign"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( FairSchedulerConfiguration . ASSIGN _ MULTIPLE ,    true )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     0  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  6  3  8  4  ,     1  6  )  ,     0  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  0  ,     1  ,     \" root . default \"  ,     \" user \"  ,     8  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "scheduler . maxAssign    =     2  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Incorrect   number   of   containers   allocated \"  ,     2  ,    app . getLiveContainers (  )  . size (  )  )  ;", "scheduler . maxAssign    =     -  1  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Incorrect   number   of   containers   allocated \"  ,     8  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMaxAssignWithZeroMemoryContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "FairSchedulerTestBase . MockClock   clock    =    new   FairSchedulerTestBase . MockClock (  )  ;", "scheduler . setClock ( clock )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 1  \\  \"  >  \"  )  ;", "out . println (  \"        < maxRunningApps >  3  <  / maxRunningApps >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" sub 1  \\  \"  >  <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" sub 2  \\  \"  >  <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" sub 3  \\  \"  >  \"  )  ;", "out . println (  \"              < maxRunningApps >  1  <  / maxRunningApps >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  . sub 1  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 1  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 1  \"  ,     1  ,     0  )  ;", "clock . tick (  1  0  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  . sub 3  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 2  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 3  \"  ,     1  ,     0  )  ;", "clock . tick (  1  0  )  ;", "ApplicationAttemptId   attId 3     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  . sub 2  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 3  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 2  \"  ,     1  ,     0  )  ;", "clock . tick (  1  0  )  ;", "ApplicationAttemptId   attId 4     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  . sub 2  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 4  ,    false )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 2  \"  ,     1  ,     1  )  ;", "clock . tick (  1  0  )  ;", "ApplicationAttemptId   attId 5     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  . sub 3  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 5  ,    false )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 3  \"  ,     1  ,     1  )  ;", "clock . tick (  1  0  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 1     =    new   AppAttemptRemovedSchedulerEvent ( attId 2  ,    RMAppAttemptState . FINISHED ,    false )  ;", "scheduler . handle ( appRemovedEvent 1  )  ;", "verifyAppRunnable ( attId 4  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 2  \"  ,     2  ,     0  )  ;", "verifyAppRunnable ( attId 5  ,    false )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 3  \"  ,     0  ,     1  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 2     =    new   AppAttemptRemovedSchedulerEvent ( attId 5  ,    RMAppAttemptState . KILLED ,    true )  ;", "scheduler . handle ( appRemovedEvent 2  )  ;", "assertEquals (  0  ,    scheduler . maxRunningEnforcer . usersNonRunnableApps . get (  \" user 1  \"  )  . size (  )  )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 3  \"  ,     0  ,     0  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 3     =    new   AppAttemptRemovedSchedulerEvent ( attId 4  ,    RMAppAttemptState . FINISHED ,    true )  ;", "scheduler . handle ( appRemovedEvent 3  )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 2  \"  ,     1  ,     0  )  ;", "verifyQueueNumRunnable (  \" queue 1  . sub 3  \"  ,     0  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["testMaxRunningAppsHierarchicalQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FairScheduler   fs    =    new   FairScheduler (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     0  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,     0  )  ;", "conf . setInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ,     5  1  2  )  ;", "conf . setInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ VCORES ,     2  )  ;", "fs . init ( conf )  ;", "fs . reinitialize ( conf ,    null )  ;", "Assert . assertEquals (  0  ,    fs . getMinimumResourceCapability (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  0  ,    fs . getMinimumResourceCapability (  )  . getVirtualCores (  )  )  ;", "Assert . assertEquals (  5  1  2  ,    fs . getIncrementResourceCapability (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  2  ,    fs . getIncrementResourceCapability (  )  . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMinZeroResourcesSettings"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueMgr    =    scheduler . getQueueManager (  )  ;", "FSLeafQueue   oldQueue    =    queueMgr . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "FSLeafQueue   targetQueue    =    queueMgr . getLeafQueue (  \" queue 2  \"  ,    true )  ;", "scheduler . getAllocationConfiguration (  )  . queueMaxApps . put (  \" root . queue 1  \"  ,     0  )  ;", "ApplicationAttemptId   appAttId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     3  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( appAttId )  ;", "assertTrue ( oldQueue . getNonRunnableAppSchedulables (  )  . contains ( app )  )  ;", "scheduler . moveApplication ( appAttId . getApplicationId (  )  ,     \" queue 2  \"  )  ;", "assertFalse ( oldQueue . getNonRunnableAppSchedulables (  )  . contains ( app )  )  ;", "assertFalse ( targetQueue . getNonRunnableAppSchedulables (  )  . contains ( app )  )  ;", "assertTrue ( targetQueue . getRunnableAppSchedulables (  )  . contains ( app )  )  ;", "assertEquals (  1  ,    targetQueue . getNumRunnableApps (  )  )  ;", "assertEquals (  1  ,    queueMgr . getRootQueue (  )  . getNumRunnableApps (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMoveMakesAppRunnable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueMgr    =    scheduler . getQueueManager (  )  ;", "FSLeafQueue   oldQueue    =    queueMgr . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "FSLeafQueue   targetQueue    =    queueMgr . getLeafQueue (  \" queue 2  \"  ,    true )  ;", "scheduler . getAllocationConfiguration (  )  . queueMaxApps . put (  \" root . queue 1  \"  ,     0  )  ;", "scheduler . getAllocationConfiguration (  )  . queueMaxApps . put (  \" root . queue 2  \"  ,     0  )  ;", "ApplicationAttemptId   appAttId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     3  )  ;", "assertEquals (  0  ,    oldQueue . getNumRunnableApps (  )  )  ;", "scheduler . moveApplication ( appAttId . getApplicationId (  )  ,     \" queue 2  \"  )  ;", "assertEquals (  0  ,    oldQueue . getNumRunnableApps (  )  )  ;", "assertEquals (  0  ,    targetQueue . getNumRunnableApps (  )  )  ;", "assertEquals (  0  ,    queueMgr . getRootQueue (  )  . getNumRunnableApps (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMoveNonRunnableApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueMgr    =    scheduler . getQueueManager (  )  ;", "FSLeafQueue   oldQueue    =    queueMgr . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "FSLeafQueue   targetQueue    =    queueMgr . getLeafQueue (  \" queue 2  \"  ,    true )  ;", "ApplicationAttemptId   appAttId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     3  )  ;", "ApplicationId   appId    =    appAttId . getApplicationId (  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals ( Resource . newInstance (  1  0  2  4  ,     1  )  ,    oldQueue . getResourceUsage (  )  )  ;", "scheduler . update (  )  ;", "assertEquals ( Resource . newInstance (  3  0  7  2  ,     3  )  ,    oldQueue . getDemand (  )  )  ;", "scheduler . moveApplication ( appId ,     \" queue 2  \"  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( appAttId )  ;", "assertSame ( targetQueue ,    app . getQueue (  )  )  ;", "assertFalse ( oldQueue . getRunnableAppSchedulables (  )  . contains ( app )  )  ;", "assertTrue ( targetQueue . getRunnableAppSchedulables (  )  . contains ( app )  )  ;", "assertEquals ( Resource . newInstance (  0  ,     0  )  ,    oldQueue . getResourceUsage (  )  )  ;", "assertEquals ( Resource . newInstance (  1  0  2  4  ,     1  )  ,    targetQueue . getResourceUsage (  )  )  ;", "assertEquals (  0  ,    oldQueue . getNumRunnableApps (  )  )  ;", "assertEquals (  1  ,    targetQueue . getNumRunnableApps (  )  )  ;", "assertEquals (  1  ,    queueMgr . getRootQueue (  )  . getNumRunnableApps (  )  )  ;", "scheduler . update (  )  ;", "assertEquals ( Resource . newInstance (  0  ,     0  )  ,    oldQueue . getDemand (  )  )  ;", "assertEquals ( Resource . newInstance (  3  0  7  2  ,     3  )  ,    targetQueue . getDemand (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMoveRunnableApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "ApplicationAttemptId   appAttId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     3  )  ;", "scheduler . moveApplication ( appAttId . getApplicationId (  )  ,     \" queue 2  \"  )  ;", "}", "METHOD_END"], "methodName": ["testMoveToNonexistentQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueMgr    =    scheduler . getQueueManager (  )  ;", "queueMgr . getLeafQueue (  \" queue 2  \"  ,    true )  ;", "scheduler . getAllocationConfiguration (  )  . queueMaxApps . put (  \" root . queue 2  \"  ,     0  )  ;", "ApplicationAttemptId   appAttId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     3  )  ;", "scheduler . moveApplication ( appAttId . getApplicationId (  )  ,     \" queue 2  \"  )  ;", "}", "METHOD_END"], "methodName": ["testMoveWouldViolateMaxAppsConstraints"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueMgr    =    scheduler . getQueueManager (  )  ;", "FSLeafQueue   oldQueue    =    queueMgr . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "queueMgr . getLeafQueue (  \" queue 2  \"  ,    true )  ;", "scheduler . getAllocationConfiguration (  )  . maxQueueResources . put (  \" root . queue 2  \"  ,    Resource . newInstance (  1  0  2  4  ,     1  )  )  ;", "ApplicationAttemptId   appAttId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" queue 1  \"  ,     \" user 1  \"  ,     3  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  2  0  4  8  ,     2  )  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "scheduler . handle ( updateEvent )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals ( Resource . newInstance (  2  0  4  8  ,     2  )  ,    oldQueue . getResourceUsage (  )  )  ;", "scheduler . moveApplication ( appAttId . getApplicationId (  )  ,     \" queue 2  \"  )  ;", "}", "METHOD_END"], "methodName": ["testMoveWouldViolateMaxResourcesConstraints"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 2  \"  ,     \" user 2  \"  ,     1  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 3  \"  ,     \" user 3  \"  ,     1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getSchedulerApp ( attId 1  )  . getCurrentReservation (  )  . getMemory (  )  )  ;", "assertEquals (  0  ,    scheduler . getSchedulerApp ( attId 2  )  . getCurrentReservation (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleContainersWaitingForReservation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "RMNode   node 3     =    MockNodes . newNodeInfo (  2  ,    Resources . createResource (  1  0  2  4  )  ,     3  ,     \"  1  2  7  .  0  .  0  .  3  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   appId    =    createAppAttemptId (  (  ( this . APP _ ID )  +  +  )  ,     (  ( this . ATTEMPT _ ID )  +  +  )  )  ;", "scheduler . addApplication ( appId . getApplicationId (  )  ,     \" queue 1  \"  ,     \" user 1  \"  ,    false )  ;", "scheduler . addApplicationAttempt ( appId ,    false ,    false )  ;", "List < ResourceRequest >    asks    =    new   ArrayList < ResourceRequest >  (  )  ;", "asks . add ( createResourceRequest (  1  0  2  4  ,    node 1  . getHostName (  )  ,     1  ,     1  ,    true )  )  ;", "asks . add ( createResourceRequest (  1  0  2  4  ,    node 2  . getHostName (  )  ,     1  ,     1  ,    true )  )  ;", "asks . add ( createResourceRequest (  1  0  2  4  ,    node 3  . getHostName (  )  ,     1  ,     1  ,    true )  )  ;", "asks . add ( createResourceRequest (  1  0  2  4  ,    node 1  . getRackName (  )  ,     1  ,     1  ,    true )  )  ;", "asks . add ( createResourceRequest (  1  0  2  4  ,    node 3  . getRackName (  )  ,     1  ,     1  ,    true )  )  ;", "asks . add ( createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     2  ,    true )  )  ;", "scheduler . allocate ( appId ,    asks ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent 1     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent 1  )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( appId )  . getLiveContainers (  )  . size (  )  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "scheduler . handle ( updateEvent 2  )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( appId )  . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleNodesSingleRackRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    SimpleGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" user 1 group \\  \"    type =  \\  \" parent \\  \"  >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queuePlacementPolicy >  \"  )  ;", "out . println (  \"  < rule   name =  \\  \" specified \\  \"    create =  \\  \" false \\  \"     /  >  \"  )  ;", "out . println (  \"  < rule   name =  \\  \" nestedUserQueue \\  \"  >  \"  )  ;", "out . println (  \"                 < rule   name =  \\  \" primaryGroup \\  \"    create =  \\  \" false \\  \"     /  >  \"  )  ;", "out . println (  \"  <  / rule >  \"  )  ;", "out . println (  \"  < rule   name =  \\  \" default \\  \"     /  >  \"  )  ;", "out . println (  \"  <  / queuePlacementPolicy >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMApp   rmApp 1     =    new   MockRMApp (  0  ,     0  ,    RMAppState . NEW )  ;", "FSLeafQueue   user 1 Leaf    =    scheduler . assignToQueue ( rmApp 1  ,     \" root . default \"  ,     \" user 1  \"  )  ;", "assertEquals (  \" root . user 1 group . user 1  \"  ,    user 1 Leaf . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  2  0  4  8  ,     1  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  1  0  2  4  ,     1  ,     \" default \"  ,     \" user 1  \"  ,     2  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNoMoreCpuOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FairScheduler   fs    =    new   FairScheduler (  )  ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,     2  5  6  )  ;", "conf . setInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ VCORES ,     1  )  ;", "conf . setInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ,     5  1  2  )  ;", "conf . setInt ( FairSchedulerConfiguration . RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ VCORES ,     2  )  ;", "fs . init ( conf )  ;", "fs . reinitialize ( conf ,    null )  ;", "Assert . assertEquals (  2  5  6  ,    fs . getMinimumResourceCapability (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  1  ,    fs . getMinimumResourceCapability (  )  . getVirtualCores (  )  )  ;", "Assert . assertEquals (  5  1  2  ,    fs . getIncrementResourceCapability (  )  . getMemory (  )  )  ;", "Assert . assertEquals (  2  ,    fs . getIncrementResourceCapability (  )  . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNonMinZeroResourcesSettings"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"  >  \"  )  ;", "out . println (  \"        < aclSubmitApps >     <  / aclSubmitApps >  \"  )  ;", "out . println (  \"        < aclAdministerApps >     <  / aclAdministerApps >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" queue 1  \\  \"  >  \"  )  ;", "out . println (  \"              < aclSubmitApps > userallow <  / aclSubmitApps >  \"  )  ;", "out . println (  \"              < aclAdministerApps > userallow <  / aclAdministerApps >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "int   appId    =     ( this . APP _ ID )  +  +  ;", "String   user    =     \" usernotallow \"  ;", "String   queue    =     \" queue 1  \"  ;", "ApplicationId   applicationId    =    MockApps . newAppID ( appId )  ;", "String   name    =    MockApps . newAppName (  )  ;", "ApplicationMasterService   masterService    =    new   ApplicationMasterService ( resourceManager . getRMContext (  )  ,    scheduler )  ;", "ApplicationSubmissionContext   submissionContext    =    new   ApplicationSubmissionContextPBImpl (  )  ;", "ContainerLaunchContext   clc    =    BuilderUtils . newContainerLaunchContext ( null ,    null ,    null ,    null ,    null ,    null )  ;", "submissionContext . setApplicationId ( applicationId )  ;", "submissionContext . setAMContainerSpec ( clc )  ;", "RMApp   application    =    new   RMAppImpl ( applicationId ,    resourceManager . getRMContext (  )  ,    conf ,    name ,    user ,    queue ,    submissionContext ,    scheduler ,    masterService ,    System . currentTimeMillis (  )  ,     \" YARN \"  ,    null )  ;", "resourceManager . getRMContext (  )  . getRMApps (  )  . putIfAbsent ( applicationId ,    application )  ;", "application . handle ( new   RMAppEvent ( applicationId ,    RMAppEventType . START )  )  ;", "final   int   MAX _ TRIES    =     2  0  ;", "int   numTries    =     0  ;", "while    (  (  !  ( application . getState (  )  . equals ( RMAppState . SUBMITTED )  )  )     &  &     ( numTries    <    MAX _ TRIES )  )     {", "try    {", "Thread . sleep (  1  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "ex . printStackTrace (  )  ;", "}", "numTries +  +  ;", "}", "assertEquals (  \" The   application   doesn ' t   reach   SUBMITTED .  \"  ,    RMAppState . SUBMITTED ,    application . getState (  )  )  ;", "ApplicationAttemptId   attId    =    ApplicationAttemptId . newInstance ( applicationId ,     (  ( this . ATTEMPT _ ID )  +  +  )  )  ;", "scheduler . addApplication ( attId . getApplicationId (  )  ,    queue ,    user ,    false )  ;", "numTries    =     0  ;", "while    (  (  ( application . getFinishTime (  )  )     =  =     0  )     &  &     ( numTries    <    MAX _ TRIES )  )     {", "try    {", "Thread . sleep (  1  0  0  )  ;", "}    catch    ( InterruptedException   ex )     {", "ex . printStackTrace (  )  ;", "}", "numTries +  +  ;", "}", "assertEquals ( FAILED ,    application . getFinalApplicationStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNotAllowSubmitApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,     \" false \"  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   appAttemptId    =    createAppAttemptId (  1  ,     1  )  ;", "createApplicationWithAMResource ( appAttemptId ,     \" default \"  ,     \" user 2  \"  ,    null )  ;", "assertEquals (  0  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" user 1  \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" default \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  0  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" user 2  \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNotUserAsDefaultQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "MetricsCollectorImpl   collector    =    new   MetricsCollectorImpl (  )  ;", "sOpDurations . getMetrics ( collector ,    true )  ;", "assertEquals (  \" Incorrect   number   of   perf   metrics \"  ,     1  ,    collector . getRecords (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPerfMetricsInited"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "FairSchedulerTestBase . MockClock   clock    =    new   FairSchedulerTestBase . MockClock (  )  ;", "scheduler . setClock ( clock )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" default \\  \"  >  \"  )  ;", "out . println (  \"  < maxResources >  0 mb ,  0 vcores <  / maxResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueC \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueD \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  .  2  5  <  / weight >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . print (  \"  < defaultMinSharePreemptionTimeout >  5  <  / defaultMinSharePreemptionTimeout >  \"  )  ;", "out . print (  \"  < fairSharePreemptionTimeout >  1  0  <  / fairSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  2     *     1  0  2  4  )  ,     2  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  2     *     1  0  2  4  )  ,     2  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "RMNode   node 3     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  2     *     1  0  2  4  )  ,     2  )  ,     3  ,     \"  1  2  7  .  0  .  0  .  3  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 3     =    new   NodeAddedSchedulerEvent ( node 3  )  ;", "scheduler . handle ( nodeEvent 3  )  ;", "ApplicationAttemptId   app 1     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueA \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "ApplicationAttemptId   app 2     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueA \"  ,     \" user 1  \"  ,     1  ,     2  )  ;", "ApplicationAttemptId   app 3     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueA \"  ,     \" user 1  \"  ,     1  ,     3  )  ;", "ApplicationAttemptId   app 4     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "ApplicationAttemptId   app 5     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     2  )  ;", "ApplicationAttemptId   app 6     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     3  )  ;", "scheduler . update (  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;    i +  +  )     {", "NodeUpdateSchedulerEvent   nodeUpdate 1     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeUpdate 1  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeUpdate 2  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate 3     =    new   NodeUpdateSchedulerEvent ( node 3  )  ;", "scheduler . handle ( nodeUpdate 3  )  ;", "}", "ApplicationAttemptId   app 7     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueC \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "ApplicationAttemptId   app 8     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueC \"  ,     \" user 1  \"  ,     1  ,     2  )  ;", "ApplicationAttemptId   app 9     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueC \"  ,     \" user 1  \"  ,     1  ,     3  )  ;", "ApplicationAttemptId   app 1  0     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueD \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "ApplicationAttemptId   app 1  1     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueD \"  ,     \" user 1  \"  ,     1  ,     2  )  ;", "ApplicationAttemptId   app 1  2     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" queueD \"  ,     \" user 1  \"  ,     1  ,     3  )  ;", "scheduler . update (  )  ;", "FSLeafQueue   schedC    =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queueC \"  ,    true )  ;", "FSLeafQueue   schedD    =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queueD \"  ,    true )  ;", "assertTrue ( Resources . equals ( Resources . none (  )  ,    scheduler . resToPreempt ( schedC ,    clock . getTime (  )  )  )  )  ;", "assertTrue ( Resources . equals ( Resources . none (  )  ,    scheduler . resToPreempt ( schedD ,    clock . getTime (  )  )  )  )  ;", "clock . tick (  6  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . resToPreempt ( schedC ,    clock . getTime (  )  )  . getMemory (  )  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . resToPreempt ( schedD ,    clock . getTime (  )  )  . getMemory (  )  )  ;", "scheduler . update (  )  ;", "clock . tick (  6  )  ;", "assertEquals (  1  5  3  6  ,    scheduler . resToPreempt ( schedC ,    clock . getTime (  )  )  . getMemory (  )  )  ;", "assertEquals (  1  5  3  6  ,    scheduler . resToPreempt ( schedD ,    clock . getTime (  )  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionDecision"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setLong ( FairSchedulerConfiguration . PREEMPTION _ INTERVAL ,     5  0  0  0  )  ;", "conf . setLong ( FairSchedulerConfiguration . WAIT _ TIME _ BEFORE _ KILL ,     1  0  0  0  0  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "conf . set ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,     \" false \"  )  ;", "FairSchedulerTestBase . MockClock   clock    =    new   FairSchedulerTestBase . MockClock (  )  ;", "scheduler . setClock ( clock )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter (  . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  8  <  / weight >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA 1  \\  \"     /  >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA 2  \\  \"     /  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  2  <  / weight >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . print (  \"  < fairSharePreemptionTimeout >  1  0  <  / fairSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  8     *     1  0  2  4  )  ,     8  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "ApplicationAttemptId   app 1     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueA . queueA 1  \"  ,     \" user 1  \"  ,     7  ,     1  )  ;", "ApplicationAttemptId   app 2     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueB \"  ,     \" user 2  \"  ,     1  ,     1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate 1     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "for    ( int   i    =     0  ;    i    <     8  ;    i +  +  )     {", "scheduler . handle ( nodeUpdate 1  )  ;", "}", "assertEquals (  7  ,    scheduler . getSchedulerApp ( app 1  )  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( app 2  )  . getLiveContainers (  )  . size (  )  )  ;", "ApplicationAttemptId   app 3     =    createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     1  ,     \" queueA . queueA 2  \"  ,     \" user 3  \"  ,     7  ,     1  )  ;", "scheduler . update (  )  ;", "clock . tick (  1  1  )  ;", "scheduler . update (  )  ;", "Resource   toPreempt    =    scheduler . resToPreempt ( scheduler . getQueueManager (  )  . getLeafQueue (  \" queueA . queueA 2  \"  ,    false )  ,    clock . getTime (  )  )  ;", "assertEquals (  3  2  7  7  ,    toPreempt . getMemory (  )  )  ;", "scheduler . preemptResources ( toPreempt )  ;", "assertEquals (  3  ,    scheduler . getSchedulerApp ( app 1  )  . getPreemptionContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionIsNotDelayedToNextRound"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   id 1  1     =    createAppAttemptId (  1  ,     1  )  ;", "scheduler . addApplication ( id 1  1  . getApplicationId (  )  ,     \" root . queue 1  \"  ,     \" user 1  \"  ,    false )  ;", "scheduler . addApplicationAttempt ( id 1  1  ,    false ,    false )  ;", "ApplicationAttemptId   id 2  1     =    createAppAttemptId (  2  ,     1  )  ;", "scheduler . addApplication ( id 2  1  . getApplicationId (  )  ,     \" root . queue 2  \"  ,     \" user 1  \"  ,    false )  ;", "scheduler . addApplicationAttempt ( id 2  1  ,    false ,    false )  ;", "ApplicationAttemptId   id 2  2     =    createAppAttemptId (  2  ,     2  )  ;", "scheduler . addApplication ( id 2  2  . getApplicationId (  )  ,     \" root . queue 2  \"  ,     \" user 1  \"  ,    false )  ;", "scheduler . addApplicationAttempt ( id 2  2  ,    false ,    false )  ;", "int   minReqSize    =    FairSchedulerConfiguration . DEFAULT _ RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ;", "List < ResourceRequest >    ask 1     =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   request 1     =    createResourceRequest (  ( minReqSize    *     2  )  ,    ANY ,     1  ,     1  ,    true )  ;", "ask 1  . add ( request 1  )  ;", "scheduler . allocate ( id 1  1  ,    ask 1  ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "List < ResourceRequest >    ask 2     =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   request 2     =    createResourceRequest (  (  2     *    minReqSize )  ,     \" foo \"  ,     1  ,     1  ,    false )  ;", "ResourceRequest   request 3     =    createResourceRequest ( minReqSize ,     \" bar \"  ,     1  ,     2  ,    false )  ;", "ask 2  . add ( request 2  )  ;", "ask 2  . add ( request 3  )  ;", "scheduler . allocate ( id 2  1  ,    ask 2  ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "List < ResourceRequest >    ask 3     =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   request 4     =    createResourceRequest (  (  2     *    minReqSize )  ,    ANY ,     1  ,     1  ,    true )  ;", "ask 3  . add ( request 4  )  ;", "scheduler . allocate ( id 2  2  ,    ask 3  ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "scheduler . update (  )  ;", "assertEquals (  (  2     *    minReqSize )  ,    scheduler . getQueueManager (  )  . getQueue (  \" root . queue 1  \"  )  . getDemand (  )  . getMemory (  )  )  ;", "assertEquals (  (  (  (  2     *    minReqSize )     +     (  2     *    minReqSize )  )     +     (  2     *    minReqSize )  )  ,    scheduler . getQueueManager (  )  . getQueue (  \" root . queue 2  \"  )  . getDemand (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testQueueDemandCalculation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 1  \\  \"  >  \"  )  ;", "out . println (  \"  < maxAMShare >  0  .  2  <  / maxAMShare >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  2  0  4  8  0  ,     2  0  )  ,     0  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "scheduler . update (  )  ;", "FSLeafQueue   queue 1     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "assertEquals (  \" Queue   queue 1  ' s   fair   share   should   be    0  \"  ,     0  ,    queue 1  . getFairShare (  )  . getMemory (  )  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" root . default \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "Resource   amResource 1     =    Resource . newInstance (  1  0  2  4  ,     1  )  ;", "Resource   amResource 2     =    Resource . newInstance (  2  0  4  8  ,     2  )  ;", "Resource   amResource 3     =    Resource . newInstance (  1  8  6  0  ,     2  )  ;", "int   amPriority    =    RMAppAttemptImpl . AM _ CONTAINER _ PRIORITY . getPriority (  )  ;", "ApplicationAttemptId   attId 1     =    createAppAttemptId (  1  ,     1  )  ;", "createApplicationWithAMResource ( attId 1  ,     \" queue 1  \"  ,     \" user 1  \"  ,    amResource 1  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     1  ,    amPriority ,    attId 1  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( attId 1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 1  ' s   AM   requests    1  0  2  4    MB   memory \"  ,     1  0  2  4  ,    app 1  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 1  ' s   AM   should   be   running \"  ,     1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    1  0  2  4    MB   memory \"  ,     1  0  2  4  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId 2     =    createAppAttemptId (  2  ,     1  )  ;", "createApplicationWithAMResource ( attId 2  ,     \" queue 1  \"  ,     \" user 1  \"  ,    amResource 1  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     1  ,    amPriority ,    attId 2  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( attId 2  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 2  ' s   AM   requests    1  0  2  4    MB   memory \"  ,     1  0  2  4  ,    app 2  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 2  ' s   AM   should   be   running \"  ,     1  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId 3     =    createAppAttemptId (  3  ,     1  )  ;", "createApplicationWithAMResource ( attId 3  ,     \" queue 1  \"  ,     \" user 1  \"  ,    amResource 1  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     1  ,    amPriority ,    attId 3  )  ;", "FSAppAttempt   app 3     =    scheduler . getSchedulerApp ( attId 3  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 3  ' s   AM   requests    1  0  2  4    MB   memory \"  ,     1  0  2  4  ,    app 3  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 3  ' s   AM   should   not   be   running \"  ,     0  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     1  ,    attId 1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 1    should   have   two   running   containers \"  ,     2  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 1     =    new   AppAttemptRemovedSchedulerEvent ( attId 1  ,    RMAppAttemptState . FINISHED ,    false )  ;", "scheduler . update (  )  ;", "scheduler . handle ( appRemovedEvent 1  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 1  ' s   AM   should   be   finished \"  ,     0  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Application 3  ' s   AM   should   be   running \"  ,     1  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId 4     =    createAppAttemptId (  4  ,     1  )  ;", "createApplicationWithAMResource ( attId 4  ,     \" queue 1  \"  ,     \" user 1  \"  ,    amResource 2  )  ;", "createSchedulingRequestExistingApplication (  2  0  4  8  ,     2  ,    amPriority ,    attId 4  )  ;", "FSAppAttempt   app 4     =    scheduler . getSchedulerApp ( attId 4  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 4  ' s   AM   requests    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    app 4  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 4  ' s   AM   should   not   be   running \"  ,     0  ,    app 4  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId 5     =    createAppAttemptId (  5  ,     1  )  ;", "createApplicationWithAMResource ( attId 5  ,     \" queue 1  \"  ,     \" user 1  \"  ,    amResource 2  )  ;", "createSchedulingRequestExistingApplication (  2  0  4  8  ,     2  ,    amPriority ,    attId 5  )  ;", "FSAppAttempt   app 5     =    scheduler . getSchedulerApp ( attId 5  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 5  ' s   AM   requests    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    app 5  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 5  ' s   AM   should   not   be   running \"  ,     0  ,    app 5  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 4     =    new   AppAttemptRemovedSchedulerEvent ( attId 4  ,    RMAppAttemptState . KILLED ,    false )  ;", "scheduler . handle ( appRemovedEvent 4  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 5  ' s   AM   should   not   be   running \"  ,     0  ,    app 5  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 2     =    new   AppAttemptRemovedSchedulerEvent ( attId 2  ,    RMAppAttemptState . FINISHED ,    false )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 3     =    new   AppAttemptRemovedSchedulerEvent ( attId 3  ,    RMAppAttemptState . FINISHED ,    false )  ;", "scheduler . handle ( appRemovedEvent 2  )  ;", "scheduler . handle ( appRemovedEvent 3  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 2  ' s   AM   should   be   finished \"  ,     0  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Application 3  ' s   AM   should   be   finished \"  ,     0  ,    app 3  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Application 5  ' s   AM   should   be   running \"  ,     1  ,    app 5  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId 6     =    createAppAttemptId (  6  ,     1  )  ;", "createApplicationWithAMResource ( attId 6  ,     \" queue 1  \"  ,     \" user 1  \"  ,    amResource 3  )  ;", "createSchedulingRequestExistingApplication (  1  8  6  0  ,     2  ,    amPriority ,    attId 6  )  ;", "FSAppAttempt   app 6     =    scheduler . getSchedulerApp ( attId 6  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 6  ' s   AM   should   not   be   running \"  ,     0  ,    app 6  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Application 6  ' s   AM   requests    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    app 6  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 5     =    new   AppAttemptRemovedSchedulerEvent ( attId 5  ,    RMAppAttemptState . FINISHED ,    false )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 6     =    new   AppAttemptRemovedSchedulerEvent ( attId 6  ,    RMAppAttemptState . FINISHED ,    false )  ;", "scheduler . handle ( appRemovedEvent 5  )  ;", "scheduler . handle ( appRemovedEvent 6  )  ;", "scheduler . update (  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    0  \"  ,     0  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testQueueMaxAMShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 1  \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 2  \\  \"  >  \"  )  ;", "out . println (  \"  < maxAMShare >  1  .  0  <  / maxAMShare >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 3  \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 4  \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 5  \\  \"  >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  8  1  9  2  ,     2  0  )  ,     0  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "scheduler . update (  )  ;", "FSLeafQueue   queue 1     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 1  \"  ,    true )  ;", "assertEquals (  \" Queue   queue 1  ' s   fair   share   should   be    0  \"  ,     0  ,    queue 1  . getFairShare (  )  . getMemory (  )  )  ;", "FSLeafQueue   queue 2     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 2  \"  ,    true )  ;", "assertEquals (  \" Queue   queue 2  ' s   fair   share   should   be    0  \"  ,     0  ,    queue 2  . getFairShare (  )  . getMemory (  )  )  ;", "FSLeafQueue   queue 3     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 3  \"  ,    true )  ;", "assertEquals (  \" Queue   queue 3  ' s   fair   share   should   be    0  \"  ,     0  ,    queue 3  . getFairShare (  )  . getMemory (  )  )  ;", "FSLeafQueue   queue 4     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 4  \"  ,    true )  ;", "assertEquals (  \" Queue   queue 4  ' s   fair   share   should   be    0  \"  ,     0  ,    queue 4  . getFairShare (  )  . getMemory (  )  )  ;", "FSLeafQueue   queue 5     =    scheduler . getQueueManager (  )  . getLeafQueue (  \" queue 5  \"  ,    true )  ;", "assertEquals (  \" Queue   queue 5  ' s   fair   share   should   be    0  \"  ,     0  ,    queue 5  . getFairShare (  )  . getMemory (  )  )  ;", "List < String >    queues    =    Arrays . asList (  \" root . default \"  ,     \" root . queue 3  \"  ,     \" root . queue 4  \"  ,     \" root . queue 5  \"  )  ;", "for    ( String   queue    :    queues )     {", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,    queue ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "}", "Resource   amResource 1     =    Resource . newInstance (  2  0  4  8  ,     1  )  ;", "int   amPriority    =    RMAppAttemptImpl . AM _ CONTAINER _ PRIORITY . getPriority (  )  ;", "ApplicationAttemptId   attId 1     =    createAppAttemptId (  1  ,     1  )  ;", "createApplicationWithAMResource ( attId 1  ,     \" queue 1  \"  ,     \" test 1  \"  ,    amResource 1  )  ;", "createSchedulingRequestExistingApplication (  2  0  4  8  ,     1  ,    amPriority ,    attId 1  )  ;", "FSAppAttempt   app 1     =    scheduler . getSchedulerApp ( attId 1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 1  ' s   AM   requests    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    app 1  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 1  ' s   AM   should   be   running \"  ,     1  ,    app 1  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 1  ' s   AM   resource   usage   should   be    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    queue 1  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId 2     =    createAppAttemptId (  2  ,     1  )  ;", "createApplicationWithAMResource ( attId 2  ,     \" queue 2  \"  ,     \" test 1  \"  ,    amResource 1  )  ;", "createSchedulingRequestExistingApplication (  2  0  4  8  ,     1  ,    amPriority ,    attId 2  )  ;", "FSAppAttempt   app 2     =    scheduler . getSchedulerApp ( attId 2  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  \" Application 2  ' s   AM   requests    2  0  4  8    MB   memory \"  ,     2  0  4  8  ,    app 2  . getAMResource (  )  . getMemory (  )  )  ;", "assertEquals (  \" Application 2  ' s   AM   should   not   be   running \"  ,     0  ,    app 2  . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  \" Queue 2  ' s   AM   resource   usage   should   be    0    MB   memory \"  ,     0  ,    queue 2  . getAmResourceUsage (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testQueueMaxAMShareDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    SimpleGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "init ( conf )  ;", "start (  )  ;", "reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   appId ;", "List < QueuePlacementRule >    rules    =    new   ArrayList < QueuePlacementRule >  (  )  ;", "rules . add ( new   QueuePlacementRule . Specified (  )  . initialize ( true ,    null )  )  ;", "rules . add ( new   QueuePlacementRule . User (  )  . initialize ( false ,    null )  )  ;", "rules . add ( new   QueuePlacementRule . PrimaryGroup (  )  . initialize ( false ,    null )  )  ;", "rules . add ( new   QueuePlacementRule . SecondaryGroupExistingQueue (  )  . initialize ( false ,    null )  )  ;", "rules . add ( new   QueuePlacementRule . Default (  )  . initialize ( true ,    null )  )  ;", "Set < String >    queues    =    Sets . newHashSet (  \" root . user 1  \"  ,     \" root . user 3 group \"  ,     \" root . user 4 subgroup 1  \"  ,     \" root . user 4 subgroup 2  \"  ,     \" root . user 5 subgroup 2  \"  )  ;", "Map < FSQueueType ,    Set < String >  >    configuredQueues    =    new   HashMap < FSQueueType ,    Set < String >  >  (  )  ;", "configuredQueues . put ( FSQueueType . LEAF ,    queues )  ;", "configuredQueues . put ( FSQueueType . PARENT ,    new   HashSet < String >  (  )  )  ;", "getAllocationConfiguration (  )  . placementPolicy    =    new   QueuePlacementPolicy ( rules ,    configuredQueues ,    conf )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" somequeue \"  ,     \" user 1  \"  )  ;", "assertEquals (  \" root . somequeue \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" user 1  \"  )  ;", "assertEquals (  \" root . user 1  \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" user 3  \"  )  ;", "assertEquals (  \" root . user 3 group \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" user 4  \"  )  ;", "assertEquals (  \" root . user 4 subgroup 1  \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" user 5  \"  )  ;", "assertEquals (  \" root . user 5 subgroup 2  \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" otheruser \"  )  ;", "assertEquals (  \" root . default \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "rules    =    new   ArrayList < QueuePlacementRule >  (  )  ;", "rules . add ( new   QueuePlacementRule . User (  )  . initialize ( false ,    null )  )  ;", "rules . add ( new   QueuePlacementRule . Specified (  )  . initialize ( true ,    null )  )  ;", "rules . add ( new   QueuePlacementRule . Default (  )  . initialize ( true ,    null )  )  ;", "getAllocationConfiguration (  )  . placementPolicy    =    new   QueuePlacementPolicy ( rules ,    configuredQueues ,    conf )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" somequeue \"  ,     \" user 1  \"  )  ;", "assertEquals (  \" root . user 1  \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" somequeue \"  ,     \" otheruser \"  )  ;", "assertEquals (  \" root . somequeue \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "appId    =    createSchedulingRequest (  1  0  2  4  ,     \" default \"  ,     \" otheruser \"  )  ;", "assertEquals (  \" root . default \"  ,    getSchedulerApp ( appId )  . getQueueName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testQueuePlacementWithPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setLong ( FairSchedulerConfiguration . WAIT _ TIME _ BEFORE _ KILL ,     1  0  )  ;", "FairSchedulerTestBase . MockClock   clock    =    new   FairSchedulerTestBase . MockClock (  )  ;", "scheduler . setClock ( clock )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "Priority   priority    =    Priority . newInstance (  2  0  )  ;", "String   host    =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "int   GB    =     1  0  2  4  ;", "RMNode   node    =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  1  6     *     1  0  2  4  )  ,     4  )  ,     0  ,    host )  ;", "NodeAddedSchedulerEvent   nodeEvent    =    new   NodeAddedSchedulerEvent ( node )  ;", "scheduler . handle ( nodeEvent )  ;", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   nodeLocalRequest    =    createResourceRequest ( GB ,     1  ,    host ,    priority . getPriority (  )  ,     1  ,    true )  ;", "ResourceRequest   rackLocalRequest    =    createResourceRequest ( GB ,     1  ,    node . getRackName (  )  ,    priority . getPriority (  )  ,     1  ,    true )  ;", "ResourceRequest   offRackRequest    =    createResourceRequest ( GB ,     1  ,    ANY ,    priority . getPriority (  )  ,     1  ,    true )  ;", "ask . add ( nodeLocalRequest )  ;", "ask . add ( rackLocalRequest )  ;", "ask . add ( offRackRequest )  ;", "ApplicationAttemptId   appAttemptId    =    createSchedulingRequest (  \" queueA \"  ,     \" user 1  \"  ,    ask )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   nodeUpdate    =    new   NodeUpdateSchedulerEvent ( node )  ;", "scheduler . handle ( nodeUpdate )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( appAttemptId )  . getLiveContainers (  )  . size (  )  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( appAttemptId )  ;", "Assert . assertNull ( app . getResourceRequest ( priority ,    host )  )  ;", "ContainerId   containerId 1     =    ContainerId . newInstance ( appAttemptId ,     1  )  ;", "RMContainer   rmContainer    =    app . getRMContainer ( containerId 1  )  ;", "scheduler . warnOrKillContainer ( rmContainer )  ;", "clock . tick (  5  )  ;", "scheduler . warnOrKillContainer ( rmContainer )  ;", "List < ResourceRequest >    requests    =    rmContainer . getResourceRequests (  )  ;", "Assert . assertEquals (  3  ,    requests . size (  )  )  ;", "for    ( ResourceRequest   request    :    requests )     {", "Assert . assertEquals (  1  ,    app . getResourceRequest ( priority ,    request . getResourceName (  )  )  . getNumContainers (  )  )  ;", "}", "scheduler . update (  )  ;", "scheduler . handle ( nodeUpdate )  ;", "List < Container >    containers    =    scheduler . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    null ,    null )  . getContainers (  )  ;", "Assert . assertTrue (  (  ( containers . size (  )  )     =  =     1  )  )  ;", "}", "METHOD_END"], "methodName": ["testRecoverRequestAfterPreemption"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   addEvent    =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( addEvent )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "scheduler . update (  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "NodeRemovedSchedulerEvent   removeEvent    =    new   NodeRemovedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( removeEvent )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "scheduler . update (  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoveNodeUpdatesRootQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  2  0  4  8  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "assertEquals (  0  ,    app . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    app . getReservedContainers (  )  . size (  )  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     2  ,    attId )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    app . getReservedContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReservationThatDoesntFit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  1  0  2  4  ,     4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  ,     2  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "ContainerId   containerId    =    scheduler . getSchedulerApp ( attId )  . getLiveContainers (  )  . iterator (  )  . next (  )  . getContainerId (  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     4  ,     2  ,    attId )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     4  ,     1  ,    attId )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "for    ( RMContainer   container    :    app . getReservedContainers (  )  )     {", "assertEquals (  2  ,    container . getReservedPriority (  )  . getPriority (  )  )  ;", "}", "scheduler . allocate ( attId ,    new   ArrayList < api . records . ResourceRequest >  (  )  ,    Arrays . asList ( containerId )  ,    null ,    null )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "Collection < RMContainer >    liveContainers    =    app . getLiveContainers (  )  ;", "assertEquals (  1  ,    liveContainers . size (  )  )  ;", "for    ( RMContainer   liveContainer    :    liveContainers )     {", "Assert . assertEquals (  2  ,    liveContainer . getContainer (  )  . getPriority (  )  . getPriority (  )  )  ;", "}", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  0  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReservationWhileMultiplePriorities"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     0  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "ResourceRequest   nodeRequest    =    createResourceRequest (  1  0  2  4  ,    node 2  . getHostName (  )  ,     1  ,     2  ,    true )  ;", "ResourceRequest   rackRequest    =    createResourceRequest (  1  0  2  4  ,     \" rack 1  \"  ,     1  ,     2  ,    true )  ;", "ResourceRequest   anyRequest    =    createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     2  ,    false )  ;", "createSchedulingRequestExistingApplication ( nodeRequest ,    attId )  ;", "createSchedulingRequestExistingApplication ( rackRequest ,    attId )  ;", "createSchedulingRequestExistingApplication ( anyRequest ,    attId )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   nodeUpdateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeUpdateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "scheduler . handle ( nodeUpdateEvent )  ;", "assertEquals (  1  ,    app . getReservedContainers (  )  . size (  )  )  ;", "rackRequest    =    createResourceRequest (  1  0  2  4  ,     \" rack 1  \"  ,     1  ,     1  ,    false )  ;", "anyRequest    =    createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     1  ,    false )  ;", "scheduler . allocate ( attId ,    Arrays . asList ( rackRequest ,    anyRequest )  ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "scheduler . handle ( nodeUpdateEvent )  ;", "assertEquals (  0  ,    app . getReservedContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReservationsStrictLocality"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queue 2  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  0  2  4  ,    scheduler . rootMetrics . getReservedMB (  )  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "NodeUpdateSchedulerEvent   updateEvent 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "scheduler . handle ( updateEvent 2  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . rootMetrics . getReservedMB (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  0  ,    scheduler . rootMetrics . getReservedMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSchedulerRootQueueMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  5  1  2  ,     2  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "createSchedulingRequest (  5  1  2  ,     2  ,     \" queue 1  \"  ,     \" user 1  \"  ,     2  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals ( Configuration . DEFAULT _ RM _ SCHEDULER _ INCREMENT _ ALLOCATION _ MB ,    scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . getResourceUsage (  )  . getMemory (  )  )  ;", "NodeUpdateSchedulerEvent   updateEvent 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "scheduler . handle ( updateEvent 2  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . getResourceUsage (  )  . getMemory (  )  )  ;", "assertEquals (  2  ,    scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . getResourceUsage (  )  . getVirtualCores (  )  )  ;", "QueueMetrics   queue 1 Metrics    =    scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . getMetrics (  )  ;", "assertEquals (  1  0  2  4  ,    queue 1 Metrics . getAllocatedMB (  )  )  ;", "assertEquals (  2  ,    queue 1 Metrics . getAllocatedVirtualCores (  )  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getRootQueueMetrics (  )  . getAllocatedMB (  )  )  ;", "assertEquals (  2  ,    scheduler . getRootQueueMetrics (  )  . getAllocatedVirtualCores (  )  )  ;", "assertEquals (  5  1  2  ,    scheduler . getRootQueueMetrics (  )  . getAvailableMB (  )  )  ;", "assertEquals (  4  ,    scheduler . getRootQueueMetrics (  )  . getAvailableVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleContainerAllocation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getQueueManager (  )  . getQueue (  \" queue 1  \"  )  . getResourceUsage (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   attId    =    createSchedulingRequest (  1  0  2  4  ,     \" queue 2  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  0  ,    scheduler . getQueueManager (  )  . getQueue (  \" queue 2  \"  )  . getResourceUsage (  )  . getMemory (  )  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getSchedulerApp ( attId )  . getCurrentReservation (  )  . getMemory (  )  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "NodeUpdateSchedulerEvent   updateEvent 2     =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "scheduler . handle ( updateEvent 2  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getQueueManager (  )  . getQueue (  \" queue 2  \"  )  . getResourceUsage (  )  . getMemory (  )  )  ;", "assertEquals (  1  0  2  4  ,    scheduler . getSchedulerApp ( attId )  . getCurrentReservation (  )  . getMemory (  )  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  0  ,    scheduler . getSchedulerApp ( attId )  . getCurrentReservation (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleContainerReservation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  1  0     *     1  0  2  4  )  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" queue 1  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" queue 2  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" root . default \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "scheduler . getQueueManager (  )  . getRootQueue (  )  . setSteadyFairShare ( scheduler . getClusterResource (  )  )  ;", "scheduler . getQueueManager (  )  . getRootQueue (  )  . recomputeSteadyShares (  )  ;", "Collection < FSLeafQueue >    queues    =    scheduler . getQueueManager (  )  . getLeafQueues (  )  ;", "assertEquals (  3  ,    queues . size (  )  )  ;", "for    ( FSLeafQueue   p    :    queues )     {", "assertEquals (  3  4  1  4  ,    p . getFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  3  4  1  4  ,    p . getMetrics (  )  . getFairShareMB (  )  )  ;", "assertEquals (  3  4  1  4  ,    p . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  3  4  1  4  ,    p . getMetrics (  )  . getSteadyFairShareMB (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testSimpleFairShareCalculation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "int   capacity    =     1  0     *     2  4  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource ( capacity )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" parent . queue 2  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" parent . queue 3  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  1  0     *     1  0  2  4  )  ,     \" root . default \"  ,     \" user 1  \"  )  ;", "scheduler . update (  )  ;", "scheduler . getQueueManager (  )  . getRootQueue (  )  . setSteadyFairShare ( scheduler . getClusterResource (  )  )  ;", "scheduler . getQueueManager (  )  . getRootQueue (  )  . recomputeSteadyShares (  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "Collection < FSLeafQueue >    queues    =    queueManager . getLeafQueues (  )  ;", "assertEquals (  3  ,    queues . size (  )  )  ;", "FSLeafQueue   queue 1     =    queueManager . getLeafQueue (  \" default \"  ,    true )  ;", "FSLeafQueue   queue 2     =    queueManager . getLeafQueue (  \" parent . queue 2  \"  ,    true )  ;", "FSLeafQueue   queue 3     =    queueManager . getLeafQueue (  \" parent . queue 3  \"  ,    true )  ;", "assertEquals (  ( capacity    /     2  )  ,    queue 1  . getFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     2  )  ,    queue 1  . getMetrics (  )  . getFairShareMB (  )  )  ;", "assertEquals (  ( capacity    /     2  )  ,    queue 1  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     2  )  ,    queue 1  . getMetrics (  )  . getSteadyFairShareMB (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 2  . getFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 2  . getMetrics (  )  . getFairShareMB (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 2  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 2  . getMetrics (  )  . getSteadyFairShareMB (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 3  . getFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 3  . getMetrics (  )  . getFairShareMB (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 3  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  ( capacity    /     4  )  ,    queue 3  . getMetrics (  )  . getSteadyFairShareMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleHierarchicalFairShareCalculation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    SimpleGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "conf . set ( Configuration . USER _ AS _ DEFAULT _ QUEUE ,     \" true \"  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  6  1  4  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "assertEquals (  6  1  4  4  ,    scheduler . getClusterResource (  )  . getMemory (  )  )  ;", "assertEquals (  6  1  4  4  ,    scheduler . getQueueManager (  )  . getRootQueue (  )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  6  1  4  4  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" default \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "ApplicationAttemptId   appAttemptId 1     =    createAppAttemptId (  1  ,     1  )  ;", "createApplicationWithAMResource ( appAttemptId 1  ,     \" default \"  ,     \" user 1  \"  ,    null )  ;", "assertEquals (  3  0  7  2  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" default \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  3  0  7  2  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" user 1  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSteadyFairShareWithQueueCreatedRuntime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < defaultQueueSchedulingPolicy > fair <  / defaultQueueSchedulingPolicy >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"  >  \"  )  ;", "out . println (  \"        < schedulingPolicy > drf <  / schedulingPolicy >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 1  \\  \"  >  \"  )  ;", "out . println (  \"              < weight >  1  <  / weight >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 2  \\  \"  >  \"  )  ;", "out . println (  \"              < weight >  1  <  / weight >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "QueueManager   queueManager    =    scheduler . getQueueManager (  )  ;", "assertEquals (  0  ,    queueManager . getLeafQueue (  \" child 1  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  0  ,    queueManager . getLeafQueue (  \" child 2  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  6  1  4  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "assertEquals (  6  1  4  4  ,    scheduler . getClusterResource (  )  . getMemory (  )  )  ;", "assertEquals (  2  0  4  8  ,    queueManager . getLeafQueue (  \" child 1  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  2  0  4  8  ,    queueManager . getLeafQueue (  \" child 2  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < defaultQueueSchedulingPolicy > fair <  / defaultQueueSchedulingPolicy >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"  >  \"  )  ;", "out . println (  \"        < schedulingPolicy > drf <  / schedulingPolicy >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 1  \\  \"  >  \"  )  ;", "out . println (  \"              < weight >  1  <  / weight >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 2  \\  \"  >  \"  )  ;", "out . println (  \"              < weight >  2  <  / weight >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"        < queue   name =  \\  \" child 3  \\  \"  >  \"  )  ;", "out . println (  \"              < weight >  2  <  / weight >  \"  )  ;", "out . println (  \"        <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "assertEquals (  1  0  2  4  ,    queueManager . getLeafQueue (  \" child 1  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  2  0  4  8  ,    queueManager . getLeafQueue (  \" child 2  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  2  0  4  8  ,    queueManager . getLeafQueue (  \" child 3  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "NodeRemovedSchedulerEvent   nodeEvent 2     =    new   NodeRemovedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "assertEquals (  0  ,    scheduler . getClusterResource (  )  . getMemory (  )  )  ;", "assertEquals (  0  ,    queueManager . getLeafQueue (  \" child 1  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "assertEquals (  0  ,    queueManager . getLeafQueue (  \" child 2  \"  ,    false )  . getSteadyFairShare (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSteadyFairShareWithReloadAndNodeAddRemove"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "RMNode   node 2     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  1  0  2  4  )  ,     2  ,     \"  1  2  7  .  0  .  0  .  2  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 2     =    new   NodeAddedSchedulerEvent ( node 2  )  ;", "scheduler . handle ( nodeEvent 2  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     0  )  ;", "ResourceRequest   nodeRequest    =    createResourceRequest (  1  0  2  4  ,    node 1  . getHostName (  )  ,     1  ,     1  ,    true )  ;", "ResourceRequest   rackRequest    =    createResourceRequest (  1  0  2  4  ,    node 1  . getRackName (  )  ,     1  ,     1  ,    false )  ;", "ResourceRequest   anyRequest    =    createResourceRequest (  1  0  2  4  ,    ANY ,     1  ,     1  ,    false )  ;", "createSchedulingRequestExistingApplication ( nodeRequest ,    attId 1  )  ;", "createSchedulingRequestExistingApplication ( rackRequest ,    attId 1  )  ;", "createSchedulingRequestExistingApplication ( anyRequest ,    attId 1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   node 1 UpdateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "NodeUpdateSchedulerEvent   node 2 UpdateEvent    =    new   NodeUpdateSchedulerEvent ( node 2  )  ;", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId 1  )  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;    i +  +  )     {", "scheduler . handle ( node 2 UpdateEvent )  ;", "assertEquals (  0  ,    app . getLiveContainers (  )  . size (  )  )  ;", "assertEquals (  0  ,    app . getReservedContainers (  )  . size (  )  )  ;", "}", "scheduler . handle ( node 1 UpdateEvent )  ;", "assertEquals (  1  ,    app . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStrictLocality"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . setBoolean ( FairSchedulerConfiguration . CONTINUOUS _ SCHEDULING _ ENABLED ,    true )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "Thread   updateThread    =    scheduler . updateThread ;", "Thread   schedulingThread    =    scheduler . schedulingThread ;", "assertTrue ( updateThread . isAlive (  )  )  ;", "assertTrue ( schedulingThread . isAlive (  )  )  ;", "scheduler . stop (  )  ;", "int   numRetries    =     1  0  0  ;", "while    (  (  ( numRetries -  -  )     >     0  )     &  &     (  ( updateThread . isAlive (  )  )     |  |     ( schedulingThread . isAlive (  )  )  )  )     {", "Thread . sleep (  5  0  )  ;", "}", "assertNotEquals (  \" One   of   the   threads   is   still   alive \"  ,     0  ,    numRetries )  ;", "}", "METHOD_END"], "methodName": ["testThreadLifeCycle"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queue 1  \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  2  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < user   name =  \\  \" user 1  \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  1  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  <  / user >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 1  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  \"  ,     1  ,     0  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 2  \"  ,     \" user 1  \"  )  ;", "verifyAppRunnable ( attId 2  ,    false )  ;", "verifyQueueNumRunnable (  \" queue 2  \"  ,     0  ,     1  )  ;", "ApplicationAttemptId   attId 3     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 2  \"  )  ;", "verifyAppRunnable ( attId 3  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  \"  ,     2  ,     0  )  ;", "ApplicationAttemptId   attId 4     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 2  \"  )  ;", "verifyAppRunnable ( attId 4  ,    false )  ;", "verifyQueueNumRunnable (  \" queue 1  \"  ,     2  ,     1  )  ;", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 1     =    new   AppAttemptRemovedSchedulerEvent ( attId 1  ,    RMAppAttemptState . FINISHED ,    false )  ;", "scheduler . handle ( appRemovedEvent 1  )  ;", "verifyAppRunnable ( attId 2  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 2  \"  ,     1  ,     0  )  ;", "verifyAppRunnable ( attId 4  ,    true )  ;", "verifyQueueNumRunnable (  \" queue 1  \"  ,     2  ,     0  )  ;", "ApplicationAttemptId   attId 5     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 2  \"  )  ;", "verifyAppRunnable ( attId 5  ,    false )  ;", "verifyQueueNumRunnable (  \" queue 1  \"  ,     2  ,     1  )  ;", "}", "METHOD_END"], "methodName": ["testUserAndQueueMaxRunningApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . USER _ AS _ DEFAULT _ QUEUE ,     \" true \"  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "ApplicationAttemptId   appAttemptId    =    createAppAttemptId (  1  ,     1  )  ;", "createApplicationWithAMResource ( appAttemptId ,     \" default \"  ,     \" user 1  \"  ,    null )  ;", "assertEquals (  1  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" user 1  \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  0  ,    scheduler . getQueueManager (  )  . getLeafQueue (  \" default \"  ,    true )  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  \" root . user 1  \"  ,    resourceManager . getRMContext (  )  . getRMApps (  )  . get ( appAttemptId . getApplicationId (  )  )  . getQueue (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUserAsDefaultQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,    TestFairScheduler . ALLOC _ FILE )  ;", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairScheduler . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < user   name =  \\  \" user 1  \\  \"  >  \"  )  ;", "out . println (  \"  < maxRunningApps >  1  <  / maxRunningApps >  \"  )  ;", "out . println (  \"  <  / user >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  8  1  9  2  ,     8  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "ApplicationAttemptId   attId 1     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "NodeUpdateSchedulerEvent   updateEvent    =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  1  ,    scheduler . getSchedulerApp ( attId 1  )  . getLiveContainers (  )  . size (  )  )  ;", "ApplicationAttemptId   attId 2     =    createSchedulingRequest (  1  0  2  4  ,     \" queue 1  \"  ,     \" user 1  \"  ,     1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  0  ,    scheduler . getSchedulerApp ( attId 2  )  . getLiveContainers (  )  . size (  )  )  ;", "createSchedulingRequestExistingApplication (  1  0  2  4  ,     1  ,    attId 1  )  ;", "scheduler . update (  )  ;", "scheduler . handle ( updateEvent )  ;", "assertEquals (  2  ,    scheduler . getSchedulerApp ( attId 1  )  . getLiveContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUserMaxRunningApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FSAppAttempt   app    =    scheduler . getSchedulerApp ( attId )  ;", "FSLeafQueue   queue    =    app . getQueue (  )  ;", "Collection < FSAppAttempt >    runnableApps    =    queue . getRunnableAppSchedulables (  )  ;", "Collection < FSAppAttempt >    nonRunnableApps    =    queue . getNonRunnableAppSchedulables (  )  ;", "assertEquals ( runnable ,    runnableApps . contains ( app )  )  ;", "assertEquals (  (  ! runnable )  ,    nonRunnableApps . contains ( app )  )  ;", "}", "METHOD_END"], "methodName": ["verifyAppRunnable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   queue    =    scheduler . getQueueManager (  )  . getLeafQueue ( queueName ,    false )  ;", "assertEquals ( numRunnableInQueue ,    queue . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals ( numNonRunnableInQueue ,    queue . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyQueueNumRunnable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairScheduler"}, {"methodBody": ["METHOD_START", "{", "FairSchedulerConfiguration . parseResourceConfigValue (  \"  1 o 2  4 vc 0 res \"  )  ;", "}", "METHOD_END"], "methodName": ["testGibberish"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "FairSchedulerConfiguration . parseResourceConfigValue (  \"  1  0  2  4  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNoUnits"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "FairSchedulerConfiguration . parseResourceConfigValue (  \"  1  0  2  4 vcores \"  )  ;", "}", "METHOD_END"], "methodName": ["testOnlyCPU"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "FairSchedulerConfiguration . parseResourceConfigValue (  \"  1  0  2  4 mb \"  )  ;", "}", "METHOD_END"], "methodName": ["testOnlyMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( BuilderUtils . newResource (  1  0  2  4  ,     2  )  ,    FairSchedulerConfiguration . parseResourceConfigValue (  \"  2    vcores ,     1  0  2  4    mb \"  )  )  ;", "assertEquals ( BuilderUtils . newResource (  1  0  2  4  ,     2  )  ,    FairSchedulerConfiguration . parseResourceConfigValue (  \"  1  0  2  4    mb ,     2    vcores \"  )  )  ;", "assertEquals ( BuilderUtils . newResource (  1  0  2  4  ,     2  )  ,    FairSchedulerConfiguration . parseResourceConfigValue (  \"  2 vcores ,  1  0  2  4 mb \"  )  )  ;", "assertEquals ( BuilderUtils . newResource (  1  0  2  4  ,     2  )  ,    FairSchedulerConfiguration . parseResourceConfigValue (  \"  1  0  2  4 mb ,  2 vcores \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testParseResourceConfigValue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerConfiguration"}, {"methodBody": ["METHOD_START", "{", "scheduler    =    new   FairScheduler (  )  ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,    FairScheduler . class ,    ResourceScheduler . class )  ;", "conf . set (  \" yarnevent - log - enabled \"  ,     \" true \"  )  ;", "conf . set ( FairSchedulerConfiguration . ASSIGN _ MULTIPLE ,     \" false \"  )  ;", "resourceManager    =    new   ResourceManager (  )  ;", "resourceManager . init ( conf )  ;", "(  ( AsyncDispatcher )     ( resourceManager . getRMContext (  )  . getDispatcher (  )  )  )  . start (  )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    resourceManager . getRMContext (  )  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "logFile . delete (  )  ;", "logFile . getParentFile (  )  . delete (  )  ;", "if    (  (     !  =    null )     {", "stop (  )  ;", "=    null ;", "}", "if    (  ( resourceManager )     !  =    null )     {", "resourceManager . stop (  )  ;", "resourceManager    =    null ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "FairSchedulerEventLog   eventLog    =    scheduler . getEventLog (  )  ;", "logFile    =    new   File ( eventLog . getLogFile (  )  )  ;", "Assert . assertTrue ( logFile . exists (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateEventLog"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerEventLog"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairSchedulerFairShare . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" root \\  \"     >  \"  )  ;", "out . println (  \"           < queue   name =  \\  \" parentA \\  \"     >  \"  )  ;", "out . println (  \"                       < weight >  8  <  / weight >  \"  )  ;", "out . println (  \"                       < queue   name =  \\  \" childA 1  \\  \"     /  >  \"  )  ;", "out . println (  \"                       < queue   name =  \\  \" childA 2  \\  \"     /  >  \"  )  ;", "out . println (  \"                       < queue   name =  \\  \" childA 3  \\  \"     /  >  \"  )  ;", "out . println (  \"                       < queue   name =  \\  \" childA 4  \\  \"     /  >  \"  )  ;", "out . println (  \"           <  / queue >  \"  )  ;", "out . println (  \"           < queue   name =  \\  \" parentB \\  \"     >  \"  )  ;", "out . println (  \"                       < weight >  1  <  / weight >  \"  )  ;", "out . println (  \"                       < queue   name =  \\  \" childB 1  \\  \"     /  >  \"  )  ;", "out . println (  \"                       < queue   name =  \\  \" childB 2  \\  \"     /  >  \"  )  ;", "out . println (  \"           <  / queue >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  (  (  \"  < defaultQueueSchedulingPolicy >  \"     +    policy )     +     \"  <  / defaultQueueSchedulingPolicy >  \"  )  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "resourceManager    =    new   MockRM ( conf )  ;", "resourceManager . start (  )  ;", "scheduler    =     (  ( FairScheduler )     ( resourceManager . getResourceScheduler (  )  )  )  ;", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource ( mem ,    vCores )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "}", "METHOD_END"], "methodName": ["createClusterWithQueuesAndOneNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "createClusterWithQueuesAndOneNode ( mem ,     0  ,    policy )  ;", "}", "METHOD_END"], "methodName": ["createClusterWithQueuesAndOneNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "conf    =    createConfiguration (  )  ;", "conf . set ( FairSchedulerConfiguration . ALLOCATION _ FILE ,     . ALLOC _ FILE )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManager )     !  =    null )     {", "resourceManager . stop (  )  ;", "resourceManager    =    null ;", "}", "conf    =    null ;", "}", "METHOD_END"], "methodName": ["teardown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "int   nodeCapacity    =     1  6     *     1  0  2  4  ;", "createClusterWithQueuesAndOneNode ( nodeCapacity ,     \" fair \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 1  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  3     *     1  0  2  4  )  ,     \" root . parentA . childA 2  \"  ,     \" user 2  \"  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" root . parentB . childB 1  \"  ,     \" user 3  \"  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" root . default \"  ,     \" user 4  \"  )  ;", "update (  )  ;", "for    ( int   i    =     1  ;    i    <  =     2  ;    i +  +  )     {", "assertEquals (  4  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  (  \" root . parentA . childA \"     +    i )  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  .  9  )  ;", "}", "assertEquals (  1  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentB . childB 1  \"  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  .  9  )  ;", "verifySteadyFairShareMemory ( getQueueManager (  )  . getLeafQueues (  )  ,    nodeCapacity )  ;", "}", "METHOD_END"], "methodName": ["testFairShareMultipleActiveQueuesUnderDifferentParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "int   nodeCapacity    =     1  6     *     1  0  2  4  ;", "createClusterWithQueuesAndOneNode ( nodeCapacity ,     \" fair \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 1  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 2  \"  ,     \" user 2  \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 3  \"  ,     \" user 3  \"  )  ;", "update (  )  ;", "for    ( int   i    =     1  ;    i    <  =     3  ;    i +  +  )     {", "assertEquals (  3  3  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  (  \" root . parentA . childA \"     +    i )  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  .  9  )  ;", "}", "verifySteadyFairShareMemory ( getQueueManager (  )  . getLeafQueues (  )  ,    nodeCapacity )  ;", "}", "METHOD_END"], "methodName": ["testFairShareMultipleActiveQueuesUnderSameParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "int   nodeCapacity    =     1  6     *     1  0  2  4  ;", "createClusterWithQueuesAndOneNode ( nodeCapacity ,     \" fair \"  )  ;", "update (  )  ;", "Collection < FSLeafQueue >    leafQueues    =    getQueueManager (  )  . getLeafQueues (  )  ;", "for    ( FSLeafQueue   leaf    :    leafQueues )     {", "if    ( leaf . getName (  )  . startsWith (  \" root . parentA \"  )  )     {", "assertEquals (  0  ,     (  (  ( double )     ( leaf . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )  ,     0  )  ;", "} else", "if    ( leaf . getName (  )  . startsWith (  \" root . parentB \"  )  )     {", "assertEquals (  0  ,     (  (  ( double )     ( leaf . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )  ,     0  )  ;", "}", "}", "verifySteadyFairShareMemory ( leafQueues ,    nodeCapacity )  ;", "}", "METHOD_END"], "methodName": ["testFairShareNoAppsRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "int   nodeCapacity    =     1  6     *     1  0  2  4  ;", "createClusterWithQueuesAndOneNode ( nodeCapacity ,     \" fair \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 1  \"  ,     \" user 1  \"  )  ;", "update (  )  ;", "assertEquals (  1  0  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentA . childA 1  \"  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  .  1  )  ;", "assertEquals (  0  ,     (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentA . childA 2  \"  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )  ,     0  .  1  )  ;", "verifySteadyFairShareMemory ( getQueueManager (  )  . getLeafQueues (  )  ,    nodeCapacity )  ;", "}", "METHOD_END"], "methodName": ["testFairShareOneAppRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "int   nodeCapacity    =     1  6     *     1  0  2  4  ;", "createClusterWithQueuesAndOneNode ( nodeCapacity ,     \" fair \"  )  ;", "ApplicationAttemptId   app 1     =    createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 1  \"  ,     \" user 1  \"  )  ;", "ApplicationAttemptId   app 2     =    createSchedulingRequest (  (  3     *     1  0  2  4  )  ,     \" root . parentA . childA 2  \"  ,     \" user 2  \"  )  ;", "update (  )  ;", "for    ( int   i    =     1  ;    i    <  =     2  ;    i +  +  )     {", "assertEquals (  5  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  (  \" root . parentA . childA \"     +    i )  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  .  9  )  ;", "}", "AppAttemptRemovedSchedulerEvent   appRemovedEvent 1     =    new   AppAttemptRemovedSchedulerEvent ( app 1  ,    RMAppAttemptState . FINISHED ,    false )  ;", "handle ( appRemovedEvent 1  )  ;", "update (  )  ;", "assertEquals (  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentA . childA 1  \"  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  )  ;", "assertEquals (  1  0  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentA . childA 2  \"  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeCapacity )     *     1  0  0  )  ,     0  .  1  )  ;", "verifySteadyFairShareMemory ( getQueueManager (  )  . getLeafQueues (  )  ,    nodeCapacity )  ;", "}", "METHOD_END"], "methodName": ["testFairShareResetsToZeroWhenAppsComplete"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "int   nodeMem    =     1  6     *     1  0  2  4  ;", "int   nodeVCores    =     1  0  ;", "createClusterWithQueuesAndOneNode ( nodeMem ,    nodeVCores ,     \" drf \"  )  ;", "createSchedulingRequest (  (  2     *     1  0  2  4  )  ,     \" root . parentA . childA 1  \"  ,     \" user 1  \"  )  ;", "createSchedulingRequest (  (  3     *     1  0  2  4  )  ,     \" root . parentA . childA 2  \"  ,     \" user 2  \"  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" root . parentB . childB 1  \"  ,     \" user 3  \"  )  ;", "createSchedulingRequest (  (  1     *     1  0  2  4  )  ,     \" root . default \"  ,     \" user 4  \"  )  ;", "update (  )  ;", "for    ( int   i    =     1  ;    i    <  =     2  ;    i +  +  )     {", "assertEquals (  4  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  (  \" root . parentA . childA \"     +    i )  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeMem )     *     1  0  0  )  ,     0  .  9  )  ;", "assertEquals (  4  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  (  \" root . parentA . childA \"     +    i )  ,    false )  . getFairShare (  )  . getVirtualCores (  )  )  )     /    nodeVCores )     *     1  0  0  )  ,     0  .  9  )  ;", "}", "assertEquals (  1  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentB . childB 1  \"  ,    false )  . getFairShare (  )  . getMemory (  )  )  )     /    nodeMem )     *     1  0  0  )  ,     0  .  9  )  ;", "assertEquals (  1  0  ,     (  (  (  ( double )     ( getQueueManager (  )  . getLeafQueue (  \" root . parentB . childB 1  \"  ,    false )  . getFairShare (  )  . getVirtualCores (  )  )  )     /    nodeVCores )     *     1  0  0  )  ,     0  .  9  )  ;", "Collection < FSLeafQueue >    leafQueues    =    getQueueManager (  )  . getLeafQueues (  )  ;", "for    ( FSLeafQueue   leaf    :    leafQueues )     {", "if    ( leaf . getName (  )  . startsWith (  \" root . parentA \"  )  )     {", "assertEquals (  0  .  2  ,     (  (  ( double )     ( leaf . getSteadyFairShare (  )  . getMemory (  )  )  )     /    nodeMem )  ,     0  .  0  0  1  )  ;", "assertEquals (  0  .  2  ,     (  (  ( double )     ( leaf . getSteadyFairShare (  )  . getVirtualCores (  )  )  )     /    nodeVCores )  ,     0  .  0  0  1  )  ;", "} else", "if    ( leaf . getName (  )  . startsWith (  \" root . parentB \"  )  )     {", "assertEquals (  0  .  0  5  ,     (  (  ( double )     ( leaf . getSteadyFairShare (  )  . getMemory (  )  )  )     /    nodeMem )  ,     0  .  0  0  1  )  ;", "assertEquals (  0  .  1  ,     (  (  ( double )     ( leaf . getSteadyFairShare (  )  . getVirtualCores (  )  )  )     /    nodeVCores )  ,     0  .  0  0  1  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testFairShareWithDRFMultipleActiveQueuesUnderDifferentParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "for    ( FSLeafQueue   leaf    :    leafQueues )     {", "if    ( leaf . getName (  )  . startsWith (  \" root . parentA \"  )  )     {", "assertEquals (  0  .  2  ,     (  (  ( double )     ( leaf . getSteady (  )  . getMemory (  )  )  )     /    nodeCapacity )  ,     0  .  0  0  1  )  ;", "} else", "if    ( leaf . getName (  )  . startsWith (  \" root . parentB \"  )  )     {", "assertEquals (  0  .  0  5  ,     (  (  ( double )     ( leaf . getSteady (  )  . getMemory (  )  )  )     /    nodeCapacity )  ,     0  .  0  0  1  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["verifySteadyFairShareMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerFairShare"}, {"methodBody": ["METHOD_START", "{", "RMNode   node 1     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource ( memory ,    vcores )  ,     1  ,     \" node 1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 1  )  ;", "handle ( nodeEvent 1  )  ;", "assertEquals (  \" Incorrect   amount   of   resources   in   the   cluster \"  ,    memory ,    rootMetrics . getAvailableMB (  )  )  ;", "assertEquals (  \" Incorrect   amount   of   resources   in   the   cluster \"  ,    vcores ,    rootMetrics . getAvailableVirtualCores (  )  )  ;", "createSchedulingRequest ( appMemory ,     \" queueA \"  ,     \" user 1  \"  ,    appContainers )  ;", "update (  )  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "NodeUpdateSchedulerEvent   nodeUpdate 1     =    new   NodeUpdateSchedulerEvent ( node 1  )  ;", "handle ( nodeUpdate 1  )  ;", "}", "assertEquals (  \" app 1  ' s   request   is   not   met \"  ,     ( memory    -     ( appContainers    *    appMemory )  )  ,    rootMetrics . getAvailableMB (  )  )  ;", "}", "METHOD_END"], "methodName": ["registerNodeAndSubmitApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption"}, {"methodBody": ["METHOD_START", "{", "conf    =    createConfiguration (  )  ;", "clock    =    new   TestBase . MockClock (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption"}, {"methodBody": ["METHOD_START", "{", "conf . setFloat ( FairSchedulerConfiguration . PREEMPTION _ THRESHOLD ,    utilizationThreshold )  ;", "resourceManager    =    new   MockRM ( conf )  ;", "resourceManager . start (  )  ;", "assertTrue (  (  ( resourceManager . getResourceScheduler (  )  )    instanceof    . StubbedFairScheduler )  )  ;", "scheduler    =     (  ( FairScheduler )     ( resourceManager . getResourceScheduler (  )  )  )  ;", "scheduler . setClock ( clock )  ;", "scheduler . updateInterval    =     6  0     *     1  0  0  0  ;", "}", "METHOD_END"], "methodName": ["startResourceManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resourceManager )     !  =    null )     {", "resourceManager . stop (  )  ;", "resourceManager    =    null ;", "}", "conf    =    null ;", "}", "METHOD_END"], "methodName": ["teardown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   out    =    new   PrintWriter ( new   FileWriter ( TestFairSchedulerPreemption . ALLOC _ FILE )  )  ;", "out . println (  \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"  ?  >  \"  )  ;", "out . println (  \"  < allocations >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" default \\  \"  >  \"  )  ;", "out . println (  \"  < maxResources >  0 mb ,  0 vcores <  / maxResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueA \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  1  <  / weight >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . println (  \"  < queue   name =  \\  \" queueB \\  \"  >  \"  )  ;", "out . println (  \"  < weight >  1  <  / weight >  \"  )  ;", "out . println (  \"  < minResources >  1  0  2  4 mb ,  0 vcores <  / minResources >  \"  )  ;", "out . println (  \"  <  / queue >  \"  )  ;", "out . print (  \"  < defaultMinSharePreemptionTimeout >  5  <  / defaultMinSharePreemptionTimeout >  \"  )  ;", "out . print (  \"  < fairSharePreemptionTimeout >  1  0  <  / fairSharePreemptionTimeout >  \"  )  ;", "out . println (  \"  <  / allocations >  \"  )  ;", "out . close (  )  ;", "startResourceManager (  0  .  0 F )  ;", "registerNodeAndSubmitApp (  (  4     *     1  0  2  4  )  ,     4  ,     2  ,     1  0  2  4  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "scheduler . update (  )  ;", "clock . tick (  6  )  ;", "(  ( TestFairSchedulerPreemption . StubbedFairScheduler )     ( scheduler )  )  . resetLastPreemptResources (  )  ;", "scheduler . preemptTasksIfNecessary (  )  ;", "assertEquals (  \" preemptResources (  )    should   have   been   called \"  ,     1  0  2  4  ,     (  ( TestFairSchedulerPreemption . StubbedFairScheduler )     ( scheduler )  )  . lastPreemptMemory )  ;", "resourceManager . stop (  )  ;", "startResourceManager (  0  .  8 F )  ;", "registerNodeAndSubmitApp (  (  4     *     1  0  2  4  )  ,     4  ,     3  ,     1  0  2  4  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "scheduler . update (  )  ;", "clock . tick (  6  )  ;", "(  ( TestFairSchedulerPreemption . StubbedFairScheduler )     ( scheduler )  )  . resetLastPreemptResources (  )  ;", "scheduler . preemptTasksIfNecessary (  )  ;", "assertEquals (  \" preemptResources (  )    should   not   have   been   called \"  ,     (  -  1  )  ,     (  ( TestFairSchedulerPreemption . StubbedFairScheduler )     ( scheduler )  )  . lastPreemptMemory )  ;", "resourceManager . stop (  )  ;", "startResourceManager (  0  .  7 F )  ;", "registerNodeAndSubmitApp (  (  4     *     1  0  2  4  )  ,     4  ,     3  ,     1  0  2  4  )  ;", "createSchedulingRequest (  1  0  2  4  ,     \" queueB \"  ,     \" user 1  \"  ,     1  ,     1  )  ;", "scheduler . update (  )  ;", "clock . tick (  6  )  ;", "(  ( TestFairSchedulerPreemption . StubbedFairScheduler )     ( scheduler )  )  . resetLastPreemptResources (  )  ;", "scheduler . preemptTasksIfNecessary (  )  ;", "assertEquals (  \" preemptResources (  )    should   have   been   called \"  ,     1  0  2  4  ,     (  ( TestFairSchedulerPreemption . StubbedFairScheduler )     ( scheduler )  )  . lastPreemptMemory )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionWithFreeResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestFairSchedulerPreemption"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance (  0 L ,     (  ( appNum )  +  +  )  )  ;", "ApplicationAttemptId   attId    =    ApplicationAttemptId . newInstance ( appId ,     0  )  ;", "boolean   runnable    =    max . canAppBeRunnable ( queue ,    user )  ;", "FSAppAttempt   app    =    new   FSAppAttempt ( scheduler ,    attId ,    user ,    queue ,    null ,    rmContext )  ;", "queue . addApp ( app ,    runnable )  ;", "if    ( runnable )     {", "max . trackRunnableApp ( app )  ;", "} else    {", "max . trackNonRunnableApp ( app )  ;", "}", "return   app ;", "}", "METHOD_END"], "methodName": ["addApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "FSAppAttempt   schedApp    =    mock ( FSAppAttempt . class )  ;", "when ( schedApp . getStartTime (  )  )  . thenReturn ( startTime )  ;", "return   schedApp ;", "}", "METHOD_END"], "methodName": ["mockAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "app . getQueue (  )  . removeApp ( app )  ;", "max . untrackRunnableApp ( app )  ;", "max . updateRunnabilityOnAppRemoval ( app ,    app . getQueue (  )  )  ;", "}", "METHOD_END"], "methodName": ["removeApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "clock    =    new   FairSchedulerTestBase . MockClock (  )  ;", "scheduler    =    mock ( FairScheduler . class )  ;", "when ( scheduler . getConf (  )  )  . thenReturn ( new   FairSchedulerConfiguration ( conf )  )  ;", "when ( scheduler . getClock (  )  )  . thenReturn ( clock )  ;", "AllocationConfiguration   allocConf    =    new   AllocationConfiguration ( conf )  ;", "when ( scheduler . getAllocationConfiguration (  )  )  . thenReturn ( allocConf )  ;", "queueManager    =    new   QueueManager ( scheduler )  ;", "queueManager . initialize ( conf )  ;", "queueMaxApps    =    allocConf . queueMaxApps ;", "userMaxApps    =    allocConf . userMaxApps ;", "maxAppsEnforcer    =    new    ( scheduler )  ;", "appNum    =     0  ;", "rmContext    =    mock ( RMContext . class )  ;", "when ( rmContext . getEpoch (  )  )  . thenReturn (  0  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "List < List < FSAppAttempt >  >    lists    =    new   ArrayList < List < FSAppAttempt >  >  (  )  ;", "lists . add ( Arrays . asList ( mockAppAttempt (  1  )  )  )  ;", "lists . add ( Arrays . asList ( mockAppAttempt (  2  )  )  )  ;", "Iterator < FSAppAttempt >    iter    =    new    . MultiListStartTimeIterator ( lists )  ;", "assertEquals (  1  ,    iter . next (  )  . getStartTime (  )  )  ;", "assertEquals (  2  ,    iter . next (  )  . getStartTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultiListStartTimeIteratorEmptyAppLists"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   leaf 1     =    queueManager . getLeafQueue (  \" root . queue 1  . subqueue 1  . leaf 1  \"  ,    true )  ;", "FSLeafQueue   leaf 2     =    queueManager . getLeafQueue (  \" root . queue 1  . subqueue 2  . leaf 2  \"  ,    true )  ;", "queueMaxApps . put (  \" root . queue 1  \"  ,     2  )  ;", "FSAppAttempt   app 1     =    addApp ( leaf 1  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "assertEquals (  1  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  2  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "removeApp ( app 1  )  ;", "assertEquals (  0  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  2  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleAppsWaitingOnCousinQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   leaf 1     =    queueManager . getLeafQueue (  \" root . queue 1  \"  ,    true )  ;", "FSLeafQueue   leaf 2     =    queueManager . getLeafQueue (  \" root . queue 2  \"  ,    true )  ;", "queueMaxApps . put (  \" root \"  ,     2  )  ;", "queueMaxApps . put (  \" root . queue 1  \"  ,     1  )  ;", "queueMaxApps . put (  \" root . queue 2  \"  ,     1  )  ;", "FSAppAttempt   app 1     =    addApp ( leaf 1  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "assertEquals (  1  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "removeApp ( app 1  )  ;", "assertEquals (  0  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoveDoesNotEnableAnyApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   leaf 1     =    queueManager . getLeafQueue (  \" root . queue 1  . subqueue 1  . leaf 1  \"  ,    true )  ;", "FSLeafQueue   leaf 2     =    queueManager . getLeafQueue (  \" root . queue 1  . subqueue 2  . leaf 2  \"  ,    true )  ;", "queueMaxApps . put (  \" root . queue 1  \"  ,     2  )  ;", "FSAppAttempt   app 1     =    addApp ( leaf 1  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "assertEquals (  1  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "removeApp ( app 1  )  ;", "assertEquals (  0  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  2  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  0  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoveEnablesAppOnCousinQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   leaf 1     =    queueManager . getLeafQueue (  \" root . queue 1  . leaf 1  \"  ,    true )  ;", "FSLeafQueue   leaf 2     =    queueManager . getLeafQueue (  \" root . queue 1  . leaf 2  \"  ,    true )  ;", "queueMaxApps . put (  \" root . queue 1  . leaf 1  \"  ,     2  )  ;", "userMaxApps . put (  \" user 1  \"  ,     1  )  ;", "FSAppAttempt   app 1     =    addApp ( leaf 1  ,     \" user 1  \"  )  ;", "addApp ( leaf 1  ,     \" user 2  \"  )  ;", "addApp ( leaf 1  ,     \" user 3  \"  )  ;", "addApp ( leaf 2  ,     \" user 1  \"  )  ;", "assertEquals (  2  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 1  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "removeApp ( app 1  )  ;", "assertEquals (  2  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  0  ,    leaf 1  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  0  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoveEnablesOneByQueueOneByUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "FSLeafQueue   leaf 1     =    queueManager . getLeafQueue (  \" root . queue 1  . subqueue 1  . leaf 1  \"  ,    true )  ;", "FSLeafQueue   leaf 2     =    queueManager . getLeafQueue (  \" root . queue 1  . subqueue 2  . leaf 2  \"  ,    true )  ;", "queueMaxApps . put (  \" root . queue 1  \"  ,     2  )  ;", "FSAppAttempt   app 1     =    addApp ( leaf 1  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "addApp ( leaf 2  ,     \" user \"  )  ;", "clock . tick (  2  0  )  ;", "addApp ( leaf 1  ,     \" user \"  )  ;", "assertEquals (  1  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 1  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  1  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "removeApp ( app 1  )  ;", "assertEquals (  0  ,    leaf 1  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  2  ,    leaf 2  . getRunnableAppSchedulables (  )  . size (  )  )  ;", "assertEquals (  0  ,    leaf 2  . getNonRunnableAppSchedulables (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoveEnablingOrderedByStartTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestMaxRunningAppsEnforcer"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   FairSchedulerConfiguration (  )  ;", "FairScheduler   scheduler    =    mock ( FairScheduler . class )  ;", "AllocationConfiguration   allocConf    =    new   AllocationConfiguration ( conf )  ;", "when ( scheduler . getAllocationConfiguration (  )  )  . thenReturn ( allocConf )  ;", "when ( scheduler . getConf (  )  )  . thenReturn ( conf )  ;", "SystemClock   clock    =    new   SystemClock (  )  ;", "when ( scheduler . getClock (  )  )  . thenReturn ( clock )  ;", "notEmptyQueues    =    new   HashSet < FSQueue >  (  )  ;", "queueManager    =    new    ( scheduler )     {", "@ Override", "public   boolean   isEmpty ( FSQueue   queue )     {", "return    !  ( notEmptyQueues . contains ( queue )  )  ;", "}", "}  ;", "FSQueueMetrics . forQueue (  \" root \"  ,    null ,    true ,    conf )  ;", "queueManager . initialize ( conf )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueueManager"}, {"methodBody": ["METHOD_START", "{", "updateConfiguredLeafQueues ( queueManager ,     \" queue 1  \"  )  ;", "updateConfiguredLeafQueues ( queueManager ,     \" queue 1  . queue 2  \"  )  ;", "assertNull ( queueManager . getLeafQueue (  \" queue 1  \"  ,    false )  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" queue 1  . queue 2  \"  ,    false )  )  ;", "updateConfiguredLeafQueues ( queueManager ,     \" queue 1  \"  )  ;", "assertNull ( queueManager . getLeafQueue (  \" queue 1  . queue 2  \"  ,    false )  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" queue 1  \"  ,    false )  )  ;", "notEmptyQueues . add ( queueManager . getLeafQueue (  \" queue 1  \"  ,    false )  )  ;", "updateConfiguredLeafQueues ( queueManager ,     \" queue 1  . queue 2  \"  )  ;", "assertNull ( queueManager . getLeafQueue (  \" queue 1  . queue 2  \"  ,    false )  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" queue 1  \"  ,    false )  )  ;", "notEmptyQueues . clear (  )  ;", "updateConfiguredLeafQueues ( queueManager ,     \" queue 1  . queue 2  \"  )  ;", "notEmptyQueues . add ( queueManager . getQueue (  \" root . queue 1  \"  )  )  ;", "updateConfiguredLeafQueues ( queueManager ,     \" queue 1  \"  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" queue 1  . queue 2  \"  ,    false )  )  ;", "assertNull ( queueManager . getLeafQueue (  \" queue 1  \"  ,    false )  )  ;", "updateConfiguredLeafQueues ( queueManager ,     \" default . queue 3  \"  )  ;", "assertNull ( queueManager . getLeafQueue (  \" default . queue 3  \"  ,    false )  )  ;", "assertNotNull ( queueManager . getLeafQueue (  \" default \"  ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["testReloadTurnsLeafQueueIntoParent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueueManager"}, {"methodBody": ["METHOD_START", "{", "AllocationConfiguration   allocConf    =    new   AllocationConfiguration ( conf )  ;", "allocConf . configuredQueues . get ( FSQueueType . LEAF )  . add (  \" root . queue 1  \"  )  ;", "q . updateAllocationConfiguration ( allocConf )  ;", "assertNotNull ( q . getLeafQueue (  \" root . queue 1  \"  ,    false )  )  ;", "notEmptyQueues . add ( q . getLeafQueue (  \" root . queue 1  \"  ,    false )  )  ;", "allocConf    =    new   AllocationConfiguration ( conf )  ;", "allocConf . configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . queue 1  \"  )  ;", "q . updateAllocationConfiguration ( allocConf )  ;", "assertNotNull ( q . getLeafQueue (  \" root . queue 1  \"  ,    false )  )  ;", "assertNull ( q . getParentQueue (  \" root . queue 1  \"  ,    false )  )  ;", "notEmptyQueues . clear (  )  ;", "q . updateAllocationConfiguration ( allocConf )  ;", "assertNull ( q . getLeafQueue (  \" root . queue 1  \"  ,    false )  )  ;", "assertNotNull ( q . getParentQueue (  \" root . queue 1  \"  ,    false )  )  ;", "assertTrue ( q . getParentQueue (  \" root . queue 1  \"  ,    false )  . getChildQueues (  )  . isEmpty (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReloadTurnsLeafToParentWithNoLeaf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueueManager"}, {"methodBody": ["METHOD_START", "{", "AllocationConfiguration   allocConf    =    new   AllocationConfiguration ( conf )  ;", "allocConf . configureds . get ( FSType . LEAF )  . addAll ( Sets . newHashSet ( confLeafs )  )  ;", "queueMgr . updateAllocationConfiguration ( allocConf )  ;", "}", "METHOD_END"], "methodName": ["updateConfiguredLeafQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueueManager"}, {"methodBody": ["METHOD_START", "{", "Throwable   th    =    null ;", "try    {", "parse ( sb . toString (  )  )  ;", "}    catch    ( Exception   e )     {", "th    =    e ;", "}", "astTrue (  ( th   instanceof   AllocationConfigurationException )  )  ;", "}", "METHOD_END"], "methodName": ["assertIfExceptionThrown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "configuredQueues    =    new   HashMap < FSQueueType ,    Set < String >  >  (  )  ;", "for    ( FSQueueType   type    :    FSQueueType . values (  )  )     {", "configuredQueues . put ( type ,    new   HashSet < String >  (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initTest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "DocumentBuilderFactory   docBuilderFactory    =    DocumentBuilderFactory . newInstance (  )  ;", "docBuilderFactory . setIgnoringComments ( true )  ;", "DocumentBuilder   builder    =    docBuilderFactory . newDocumentBuilder (  )  ;", "Document   doc    =    builder . parse ( IOUtils . toInputStream ( str )  )  ;", "Element   root    =    doc . getDocumentElement (  )  ;", "return   QueuePlacementPolicy . fromXml ( root ,    configuredQueues ,     . conf )  ;", "}", "METHOD_END"], "methodName": ["parse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "TestQueuePlacementPolicy . conf . setClass ( HADOOP _ SECURITY _ GROUP _ MAPPING ,    SimpleGroupsMapping . class ,    GroupMappingServiceProvider . class )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "configuredQueues . get ( FSQueueType . LEAF )  . add (  \" root . someDefaultQueue \"  )  ;", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '    create =  ' false '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '    queue =  ' root . someDefaultQueue '  /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . someDefaultQueue \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultRuleWithQueueAttribute"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . parentq \"  )  ;", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '    create =  ' false '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' default '    queue =  ' root . parentq '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . parentq . user 1  \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueueDefaultRule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < q >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' primaryGroup '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / q >  \"  )  ;", "Throwable   th    =    null ;", "try    {", "parse ( sb . toString (  )  )  ;", "}    catch    ( Exception   e )     {", "th    =    e ;", "}", "assertNull ( th )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueueParsing"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < q >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / q >  \"  )  ;", "assertIfExceptionThrown ( sb )  ;", "sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < q >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' unknownRule '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / q >  \"  )  ;", "assertIfExceptionThrown ( sb )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueueParsingErrors"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '    create =  ' false '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' primaryGroup '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . user 1 group . user 1  \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "configuredQueues . get ( FSQueueType . LEAF )  . add (  \" root . specifiedq \"  )  ;", "assertEquals (  \" root . specifiedq \"  ,    policy . assignAppToQueue (  \" root . specifiedq \"  ,     \" user 2  \"  )  )  ;", "configuredQueues . get ( FSQueueType . LEAF )  . add (  \" root . user 3 group \"  )  ;", "assertEquals (  \" root . default \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 3  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueuePrimaryGroup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' primaryGroup '    create =  ' false '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . default \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . user 1 group \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . user 1 group . user 1  \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '    create =  ' false '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' primaryGroup '    create =  ' false '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "assertEquals (  \" root . default \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 2  \"  )  )  ;", "configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . user 2 group \"  )  ;", "configuredQueues . get ( FSQueueType . LEAF )  . add (  \" root . user 2 group . user 2  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . user 2 group . user 2  \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueuePrimaryGroupNoCreate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' secondaryGroupExistingQueue '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . default \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . user 1 subgroup 1  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . user 1 subgroup 1  . user 1  \"  ,    policy . assignAppToQueue (  \" root . default \"  ,     \" user 1  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueueSecondaryGroup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' nestedUserQueue '  >  \"  )  ;", "sb . append (  \"                       < rule   name =  ' specified '    create =  ' false '  /  >  \"  )  ;", "sb . append (  \"        <  / rule >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . parent 1  \"  )  ;", "configuredQueues . get ( FSQueueType . PARENT )  . add (  \" root . parent 2  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . parent 1  . user 1  \"  ,    policy . assignAppToQueue (  \" root . parent 1  \"  ,     \" user 1  \"  )  )  ;", "assertEquals (  \" root . parent 2  . user 2  \"  ,    policy . assignAppToQueue (  \" root . parent 2  \"  ,     \" user 2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNestedUserQueueSpecificRule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  \\  ' user \\  '    create =  \\  \" false \\  \"     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "configuredQueues . get ( FSQueueType . LEAF )  . add (  \" root . someuser \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . specifiedq \"  ,    policy . assignAppToQueue (  \" specifiedq \"  ,     \" someuser \"  )  )  ;", "assertEquals (  \" root . someuser \"  ,    policy . assignAppToQueue (  \" default \"  ,     \" someuser \"  )  )  ;", "assertEquals (  \" root . specifiedq \"  ,    policy . assignAppToQueue (  \" specifiedq \"  ,     \" otheruser \"  )  )  ;", "assertEquals (  \" root . default \"  ,    policy . assignAppToQueue (  \" default \"  ,     \" otheruser \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNoCreate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < q >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  \\  ' user \\  '    create =  \\  \" false \\  \"     /  >  \"  )  ;", "sb . append (  \"  <  / q >  \"  )  ;", "parse ( sb . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testOmittedTerminalRule"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' reject '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . specifiedq \"  ,    policy . assignAppToQueue (  \" specifiedq \"  ,     \" someuser \"  )  )  ;", "assertEquals ( null ,    policy . assignAppToQueue (  \" default \"  ,     \" someuser \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSpecifiedThenReject"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < queuePlacementPolicy >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' user '     /  >  \"  )  ;", "sb . append (  \"  <  / queuePlacementPolicy >  \"  )  ;", "policy    =    parse ( sb . toString (  )  )  ;", "assertEquals (  \" root . specifiedq \"  ,    policy . assignAppToQueue (  \" specifiedq \"  ,     \" someuser \"  )  )  ;", "assertEquals (  \" root . someuser \"  ,    policy . assignAppToQueue (  \" default \"  ,     \" someuser \"  )  )  ;", "assertEquals (  \" root . otheruser \"  ,    policy . assignAppToQueue (  \" default \"  ,     \" otheruser \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSpecifiedUserPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < q >  \"  )  ;", "sb . append (  \"        < rule   name =  ' specified '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '     /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' user '     /  >  \"  )  ;", "sb . append (  \"  <  / q >  \"  )  ;", "parse ( sb . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testTerminalRuleInMiddle"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "sb . append (  \"  < q >  \"  )  ;", "sb . append (  \"        < rule   name =  ' secondaryGroupExistingQueue '    create =  ' true '  /  >  \"  )  ;", "sb . append (  \"        < rule   name =  ' default '    create =  ' false '  /  >  \"  )  ;", "sb . append (  \"  <  / q >  \"  )  ;", "parse ( sb . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testTerminals"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestQueuePlacementPolicy"}, {"methodBody": ["METHOD_START", "{", "final   String   ERR    =     \" Broken   SchedulingPolicy # isApplicableTo \"  ;", "SchedulingPolicy   policy    =    SchedulingPolicy . parse (  \" fifo \"  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ LEAF )  )  ;", "assertFalse ( ERR ,    SchedulingPolicy . isApplicableTo ( SchedulingPolicy . parse (  \" fifo \"  )  ,    SchedulingPolicy . DEPTH _ INTERMEDIATE )  )  ;", "assertFalse ( ERR ,    SchedulingPolicy . isApplicableTo ( SchedulingPolicy . parse (  \" fifo \"  )  ,    SchedulingPolicy . DEPTH _ ROOT )  )  ;", "policy    =    SchedulingPolicy . parse (  \" fair \"  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ LEAF )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ INTERMEDIATE )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ ROOT )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ PARENT )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ ANY )  )  ;", "policy    =    SchedulingPolicy . parse (  \" drf \"  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ LEAF )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ INTERMEDIATE )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ ROOT )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ PARENT )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ ANY )  )  ;", "policy    =    Mockito . mock ( SchedulingPolicy . class )  ;", "Mockito . when ( policy . getApplicableDepth (  )  )  . thenReturn ( SchedulingPolicy . DEPTH _ PARENT )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ INTERMEDIATE )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ ROOT )  )  ;", "assertTrue ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ PARENT )  )  ;", "assertFalse ( ERR ,    SchedulingPolicy . isApplicableTo ( policy ,    SchedulingPolicy . DEPTH _ ANY )  )  ;", "}", "METHOD_END"], "methodName": ["testIsApplicableTo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestSchedulingPolicy"}, {"methodBody": ["METHOD_START", "{", "SchedulingPolicy   sm    =    SchedulingPolicy . parse ( FairSharePolicy . class . getName (  )  )  ;", "assertTrue (  \" Invalid   scheduler   name \"  ,    sm . getName (  )  . equals ( FairSharePolicy . NAME )  )  ;", "sm    =    SchedulingPolicy . parse ( FairSharePolicy . class . getCanonicalName (  )  )  ;", "assertTrue (  \" Invalid   scheduler   name \"  ,    sm . getName (  )  . equals ( FairSharePolicy . NAME )  )  ;", "sm    =    SchedulingPolicy . getInstance ( FairSharePolicy . class )  ;", "assertTrue (  \" Invalid   scheduler   name \"  ,    sm . getName (  )  . equals ( FairSharePolicy . NAME )  )  ;", "sm    =    SchedulingPolicy . parse (  \" drf \"  )  ;", "assertTrue (  \" Invalid   scheduler   name \"  ,    sm . getName (  )  . equals ( DominantResourceFairnessPolicy . NAME )  )  ;", "sm    =    SchedulingPolicy . parse (  \" fair \"  )  ;", "assertTrue (  \" Invalid   scheduler   name \"  ,    sm . getName (  )  . equals ( FairSharePolicy . NAME )  )  ;", "sm    =    SchedulingPolicy . parse (  \" fifo \"  )  ;", "assertTrue (  \" Invalid   scheduler   name \"  ,    sm . getName (  )  . equals ( FifoPolicy . NAME )  )  ;", "}", "METHOD_END"], "methodName": ["testParseSchedulingPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.TestSchedulingPolicy"}, {"methodBody": ["METHOD_START", "{", "double   share    =     ( sched . getWeights (  )  . getWeight ( type )  )     *    w 2 rRatio ;", "share    =    Math . max ( share ,     . getResourceValue ( sched . getMinShare (  )  ,    type )  )  ;", "share    =    Math . min ( share ,     . getResourceValue ( sched . getMaxShare (  )  ,    type )  )  ;", "return    (  ( int )     ( share )  )  ;", "}", "METHOD_END"], "methodName": ["computeShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "Collection < Schedulable >    activeSchedulables    =    new   ArrayList < Schedulable >  (  )  ;", "for    ( Schedulable   sched    :    schedulables )     {", "if    (  ( sched   instanceof   FSQueue )     &  &     (  !  (  (  ( FSQueue )     ( sched )  )  . isActive (  )  )  )  )     {", ". setResourceValue (  0  ,    sched . getFairShare (  )  ,    type )  ;", "} else    {", "activeSchedulables . add ( sched )  ;", "}", "}", ". computeSharesInternal ( activeSchedulables ,    totalResources ,    type ,    false )  ;", "}", "METHOD_END"], "methodName": ["computeShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "if    ( schedulables . isEmpty (  )  )     {", "return ;", "}", "int   totalMaxShare    =     0  ;", "for    ( Schedulable   sched    :    schedulables )     {", "int   maxShare    =     . getResourceValue ( sched . getMaxShare (  )  ,    type )  ;", "if    ( maxShare    =  =     ( Integer . MAX _ VALUE )  )     {", "totalMaxShare    =    Integer . MAX _ VALUE ;", "break ;", "} else    {", "totalMaxShare    +  =    maxShare ;", "}", "}", "int   totalResource    =    Math . min ( totalMaxShare ,     . getResourceValue ( totalResources ,    type )  )  ;", "double   rMax    =     1  .  0  ;", "while    (  (  . resourceUsedWithWeightToResourceRatio ( rMax ,    schedulables ,    type )  )     <    totalResource )     {", "rMax    *  =     2  .  0  ;", "}", "double   left    =     0  ;", "double   right    =    rMax ;", "for    ( int   i    =     0  ;    i    <     (  . COMPUTE _ FAIR _ SHARES _ ITERATIONS )  ;    i +  +  )     {", "double   mid    =     ( left    +    right )     /     2  .  0  ;", "int   plannedResourceUsed    =     . resourceUsedWithWeightToResourceRatio ( mid ,    schedulables ,    type )  ;", "if    ( plannedResourceUsed    =  =    totalResource )     {", "right    =    mid ;", "break ;", "} else", "if    ( plannedResourceUsed    <    totalResource )     {", "left    =    mid ;", "} else    {", "right    =    mid ;", "}", "}", "for    ( Schedulable   sched    :    schedulables )     {", "if    ( isSteadyShare )     {", ". setResourceValue (  . computeShare ( sched ,    right ,    type )  ,     (  ( FSQueue )     ( sched )  )  . getSteadyFairShare (  )  ,    type )  ;", "} else    {", ". setResourceValue (  . computeShare ( sched ,    right ,    type )  ,    sched . getFairShare (  )  ,    type )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["computeSharesInternal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "ComputeFairShares . computeSharesInternal ( queues ,    totalResources ,    type ,    true )  ;", "}", "METHOD_END"], "methodName": ["computeSteadyShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "switch    ( type )     {", "case   MEMORY    :", "return    . getMemory (  )  ;", "case   CPU    :", "return    . getVirtualCores (  )  ;", "default    :", "throw   new   IllegalArgumentException (  \" Invalid    \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["getResourceValue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "int   resourcesTaken    =     0  ;", "for    ( Schedulable   sched    :    schedulables )     {", "int   share    =     . computeShare ( sched ,    w 2 rRatio ,    type )  ;", "resourcesTaken    +  =    share ;", "}", "return   resourcesTaken ;", "}", "METHOD_END"], "methodName": ["resourceUsedWithWeightToResourceRatio"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "switch    ( type )     {", "case   MEMORY    :", ". setMemory ( val )  ;", "break ;", "case   CPU    :", ". setVirtualCores ( val )  ;", "break ;", "default    :", "throw   new   IllegalArgumentException (  \" Invalid    \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["setResourceValue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares"}, {"methodBody": ["METHOD_START", "{", "DominantResourceFairnessPolicy   policy    =    new   DominantResourceFairnessPolicy (  )  ;", "policy . initialize ( BuilderUtils . newResource ( clusterMem ,    clusterCpu )  )  ;", "return   policy . getComparator (  )  ;", "}", "METHOD_END"], "methodName": ["createComparator"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulable ( memUsage ,    cpuUsage ,    ResourceWeights . NEUTRAL ,     0  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["createSchedulable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulable ( memUsage ,    cpuUsage ,    ResourceWeights . NEUTRAL ,    minMemShare ,    minCpuShare )  ;", "}", "METHOD_END"], "methodName": ["createSchedulable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "return   createSchedulable ( memUsage ,    cpuUsage ,    weights ,     0  ,     0  )  ;", "}", "METHOD_END"], "methodName": ["createSchedulable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "Resource   usage    =    BuilderUtils . newResource ( memUsage ,    cpuUsage )  ;", "Resource   minShare    =    BuilderUtils . newResource ( minMemShare ,    minCpuShare )  ;", "return   new   FakeSchedulable ( minShare ,    Resources . createResource ( Integer . MAX _ VALUE ,    Integer . MAX _ VALUE )  ,    weights ,    Resources . none (  )  ,    usage ,     0 L )  ;", "}", "METHOD_END"], "methodName": ["createSchedulable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     1  0  0  )  . compare ( createSchedulable (  2  0  0  0  ,     5  )  ,    createSchedulable (  4  0  0  0  ,     3  )  )  )     <     0  )  )  ;", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     1  0  0  )  . compare ( createSchedulable (  2  0  0  0  ,     5  ,     3  0  0  0  ,     6  )  ,    createSchedulable (  4  0  0  0  ,     3  ,     5  0  0  0  ,     4  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testBothAreNeedy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "Resource   used    =    Resources . createResource (  1  0  ,     5  )  ;", "Resource   capacity    =    Resources . createResource (  1  0  0  ,     1  0  )  ;", "ResourceType [  ]    resourceOrder    =    new   ResourceType [  2  ]  ;", "ResourceWeights   shares    =    new   ResourceWeights (  )  ;", ". DominantResourceFairnessComparator   comparator    =    new    . DominantResourceFairnessComparator (  )  ;", "comparator . calculateShares ( used ,    capacity ,    shares ,    resourceOrder ,    ResourceWeights . NEUTRAL )  ;", "assertEquals (  0  .  1  ,    shares . getWeight ( ResourceType . MEMORY )  ,     1  .  0 E -  5  )  ;", "assertEquals (  0  .  5  ,    shares . getWeight ( ResourceType . CPU )  ,     1  .  0 E -  5  )  ;", "assertEquals ( ResourceType . CPU ,    resourceOrder [  0  ]  )  ;", "assertEquals ( ResourceType . MEMORY ,    resourceOrder [  1  ]  )  ;", "}", "METHOD_END"], "methodName": ["testCalculateShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  4  0  0  0  ,     3  )  ,    createSchedulable (  2  0  0  0  ,     5  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testDifferentDominantResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  1  0  0  0  ,     3  ,    new   ResourceWeights (  2  .  0 F )  )  ,    createSchedulable (  2  0  0  0  ,     1  )  )  )     <     0  )  )  ;", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  3  0  0  0  ,     1  ,    new   ResourceWeights (  2  .  0 F )  )  ,    createSchedulable (  1  0  0  0  ,     2  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testEvenWeightsDifferentDominantResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  3  0  0  0  ,     1  ,    new   ResourceWeights (  2  .  0 F )  )  ,    createSchedulable (  2  0  0  0  ,     1  )  )  )     <     0  )  )  ;", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  1  0  0  0  ,     3  ,    new   ResourceWeights (  2  .  0 F )  )  ,    createSchedulable (  1  0  0  0  ,     2  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testEvenWeightsSameDominantResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  2  0  0  0  ,     5  ,     0  ,     6  )  ,    createSchedulable (  4  0  0  0  ,     3  ,     0  ,     0  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testOneIsNeedy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     4  )  . compare ( createSchedulable (  1  0  0  0  ,     1  )  ,    createSchedulable (  2  0  0  0  ,     1  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testSameDominantResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  1  0  0  0  ,     3  ,    new   ResourceWeights (  1  .  0 F ,     2  .  0 F )  )  ,    createSchedulable (  2  0  0  0  ,     1  )  )  )     <     0  )  )  ;", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  3  0  0  0  ,     1  ,    new   ResourceWeights (  2  .  0 F ,     1  .  0 F )  )  ,    createSchedulable (  1  0  0  0  ,     2  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnevenWeightsDifferentDominantResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  3  0  0  0  ,     1  ,    new   ResourceWeights (  2  .  0 F ,     1  .  0 F )  )  ,    createSchedulable (  2  0  0  0  ,     1  )  )  )     <     0  )  )  ;", "assertTrue (  (  ( createComparator (  8  0  0  0  ,     8  )  . compare ( createSchedulable (  1  0  0  0  ,     3  ,    new   ResourceWeights (  1  .  0 F ,     2  .  0 F )  )  ,    createSchedulable (  1  0  0  0  ,     2  )  )  )     <     0  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnevenWeightsSameDominantResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestDominantResourceFairnessPolicy"}, {"methodBody": ["METHOD_START", "{", "schedulables    =    new   ArrayList < Schedulable >  (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues"}, {"methodBody": ["METHOD_START", "{", "policy . computeShares ( schedulables ,    Resources . none (  )  )  ;", "}", "METHOD_END"], "methodName": ["testComputeShares"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues"}, {"methodBody": ["METHOD_START", "{", "testComputeShares ( SchedulingPolicy . getInstance ( DominantResourceFairnessPolicy . class )  )  ;", "}", "METHOD_END"], "methodName": ["testDRFPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues"}, {"methodBody": ["METHOD_START", "{", "testComputeShares ( SchedulingPolicy . getInstance ( FairSharePolicy . class )  )  ;", "}", "METHOD_END"], "methodName": ["testFairSharePolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues"}, {"methodBody": ["METHOD_START", "{", "testComputeShares ( SchedulingPolicy . getInstance ( FifoPolicy . class )  )  ;", "}", "METHOD_END"], "methodName": ["testFifoPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.TestEmptyQueues"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FiCaSchedulerApp >    application    =    new   SchedulerApplication < FiCaSchedulerApp >  ( DEFAULT _ QUEUE ,    user )  ;", "applications . put ( applicationId ,    application )  ;", "metrics . submitApp ( user )  ;", ". LOG . info (  (  (  (  (  (  \" Accepted   application    \"     +    applicationId )     +     \"    from   user :     \"  )     +    user )     +     \"  ,    currently   num   of   applications :     \"  )     +     ( applications . size (  )  )  )  )  ;", "if    ( isAppRecovering )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  ( applicationId    +     \"    is   recovering .    Skip   notifying   APP _ ACCEPTED \"  )  )  ;", "}", "} else    {", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppEvent ( applicationId ,    RMAppEventType . APP _ ACCEPTED )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FiCaSchedulerApp >    application    =    applications . get ( appAttemptId . getApplicationId (  )  )  ;", "String   user    =    application . getUser (  )  ;", "FiCaSchedulerApp   schedulerApp    =    new   FiCaSchedulerApp ( appAttemptId ,    user ,    DEFAULT _ QUEUE ,    activeUsersManager ,    this . rmContext )  ;", "if    ( transferStateFromPreviousAttempt )     {", "schedulerApp . transferStateFromPreviousAttempt ( application . getCurrentAppAttempt (  )  )  ;", "}", "application . setCurrentAppAttempt ( schedulerApp )  ;", "metrics . submitAppAttempt ( user )  ;", ". LOG . info (  (  (  (  \" Added   Application   Attempt    \"     +    appAttemptId )     +     \"    to   scheduler   from   user    \"  )     +     ( application . getUser (  )  )  )  )  ;", "if    ( isAttemptRecovering )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  ( appAttemptId    +     \"    is   recovering .    Skipping   notifying   ATTEMPT _ ADDED \"  )  )  ;", "}", "} else    {", "rmContext . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppAttemptEvent ( appAttemptId ,    RMAppAttemptEventType . ATTEMPT _ ADDED )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "this . nodes . put ( nodeManager . getNodeID (  )  ,    new   FiCaSchedulerNode ( nodeManager ,    usePortForNodeName )  )  ;", "Resources . addTo ( clusterResource ,    nodeManager . getTotalCapability (  )  )  ;", "}", "METHOD_END"], "methodName": ["addNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FifoScheduler . LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  (  \" assignContainers :  \"     +     \"    node =  \"  )     +     ( node . getRMNode (  )  . getNodeAddress (  )  )  )     +     \"    application =  \"  )     +     ( application . getApplicationId (  )  . getId (  )  )  )     +     \"    priority =  \"  )     +     ( priority . getPriority (  )  )  )     +     \"    assignableContainers =  \"  )     +    assignableContainers )     +     \"    request =  \"  )     +    request )     +     \"    type =  \"  )     +    type )  )  ;", "Resource   capability    =    request . getCapability (  )  ;", "int   availableContainers    =     ( node . getAvailableResource (  )  . getMemory (  )  )     /     ( capability . getMemory (  )  )  ;", "int   assignedContainers    =    Math . min ( assignableContainers ,    availableContainers )  ;", "if    ( assignedContainers    >     0  )     {", "for    ( int   i    =     0  ;    i    <    assignedContainers ;     +  + i )     {", "NodeId   nodeId    =    node . getRMNode (  )  . getNodeID (  )  ;", "ContainerId   containerId    =    BuilderUtils . newContainerId ( application . getApplicationAttemptId (  )  ,    application . getNewContainerId (  )  )  ;", "Container   container    =    BuilderUtils . newContainer ( containerId ,    nodeId ,    node . getRMNode (  )  . getHttpAddress (  )  ,    capability ,    priority ,    null )  ;", "RMContainer   rmContainer    =    application . allocate ( type ,    node ,    priority ,    request ,    container )  ;", "node . allocateContainer ( rmContainer )  ;", "increaseUsedResources ( rmContainer )  ;", "}", "}", "return   assignedContainers ;", "}", "METHOD_END"], "methodName": ["assignContainer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FifoScheduler . LOG . debug (  (  (  (  (  \" assignContainers :  \"     +     \"    node =  \"  )     +     ( node . getRMNode (  )  . getNodeAddress (  )  )  )     +     \"     # applications =  \"  )     +     ( applications . size (  )  )  )  )  ;", "for    ( Map . Entry < ApplicationId ,    SchedulerApplication < FiCaSchedulerApp >  >    e    :    applications . entrySet (  )  )     {", "FiCaSchedulerApp   application    =    e . getValue (  )  . getCurrentAppAttempt (  )  ;", "if    ( application    =  =    null )     {", "continue ;", "}", "FifoScheduler . LOG . debug (  \" pre - assignContainers \"  )  ;", "application . showRequests (  )  ;", "synchronized ( application )     {", "if    ( SchedulerAppUtils . isBlacklisted ( application ,    node ,    FifoScheduler . LOG )  )     {", "continue ;", "}", "for    ( Priority   priority    :    application . getPriorities (  )  )     {", "int   maxContainers    =    getMaxAllocatableContainers ( application ,    priority ,    node ,    NodeType . OFF _ SWITCH )  ;", "if    ( maxContainers    >     0  )     {", "int   assignedContainers    =    assignContainersOnNode ( node ,    application ,    priority )  ;", "if    ( assignedContainers    =  =     0  )     {", "break ;", "}", "}", "}", "}", "FifoScheduler . LOG . debug (  \" post - assignContainers \"  )  ;", "application . showRequests (  )  ;", "if    ( Resources . lessThan ( resourceCalculator ,    clusterResource ,    node . getAvailableResource (  )  ,    minimumAllocation )  )     {", "break ;", "}", "}", "for    ( SchedulerApplication < FiCaSchedulerApp >    application    :    applications . values (  )  )     {", "FiCaSchedulerApp   attempt    =     (  ( FiCaSchedulerApp )     ( application . getCurrentAppAttempt (  )  )  )  ;", "if    ( attempt    =  =    null )     {", "continue ;", "}", "updateAppHeadRoom ( attempt )  ;", "}", "}", "METHOD_END"], "methodName": ["assignContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   nodeLocalContainers    =    assignNodeLocalContainers ( node ,    application ,    priority )  ;", "int   rackLocalContainers    =    assignRackLocalContainers ( node ,    application ,    priority )  ;", "int   offSwitchContainers    =    assignOffSwitchContainers ( node ,    application ,    priority )  ;", ". LOG . debug (  (  (  (  (  (  (  (  (  \" assignContainersOnNode :  \"     +     \"    node =  \"  )     +     ( node . getRMNode (  )  . getNodeAddress (  )  )  )     +     \"    application =  \"  )     +     ( application . getApplicationId (  )  . getId (  )  )  )     +     \"    priority =  \"  )     +     ( priority . getPriority (  )  )  )     +     \"     # assigned =  \"  )     +     (  ( nodeLocalContainers    +    rackLocalContainers )     +    offSwitchContainers )  )  )  ;", "return    ( nodeLocalContainers    +    rackLocalContainers )     +    offSwitchContainers ;", "}", "METHOD_END"], "methodName": ["assignContainersOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   assignedContainers    =     0  ;", "RRequest   request    =    application . getRRequest ( priority ,    node . getNodeName (  )  )  ;", "if    ( request    !  =    null )     {", "RRequest   rackRequest    =    application . getRRequest ( priority ,    node . getRMNode (  )  . getRackName (  )  )  ;", "if    (  ( rackRequest    =  =    null )     |  |     (  ( rackRequest . getNumContainers (  )  )     <  =     0  )  )     {", "return    0  ;", "}", "int   assignableContainers    =    Math . min ( getMaxAllocatableContainers ( application ,    priority ,    node ,    NodeType . NODE _ LOCAL )  ,    request . getNumContainers (  )  )  ;", "assignedContainers    =    assignContainer ( node ,    application ,    priority ,    assignableContainers ,    request ,    NodeType . NODE _ LOCAL )  ;", "}", "return   assignedContainers ;", "}", "METHOD_END"], "methodName": ["assignNodeLocalContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   assignedContainers    =     0  ;", "RRequest   request    =    application . getRRequest ( priority ,    ANY )  ;", "if    ( request    !  =    null )     {", "assignedContainers    =    assignContainer ( node ,    application ,    priority ,    request . getNumContainers (  )  ,    request ,    NodeType . OFF _ SWITCH )  ;", "}", "return   assignedContainers ;", "}", "METHOD_END"], "methodName": ["assignOffSwitchContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   assignedContainers    =     0  ;", "RRequest   request    =    application . getRRequest ( priority ,    node . getRMNode (  )  . getRackName (  )  )  ;", "if    ( request    !  =    null )     {", "RRequest   offSwitchRequest    =    application . getRRequest ( priority ,    ANY )  ;", "if    (  ( offSwitchRequest . getNumContainers (  )  )     <  =     0  )     {", "return    0  ;", "}", "int   assignableContainers    =    Math . min ( getMaxAllocatableContainers ( application ,    priority ,    node ,    NodeType . RACK _ LOCAL )  ,    request . getNumContainers (  )  )  ;", "assignedContainers    =    assignContainer ( node ,    application ,    priority ,    assignableContainers ,    request ,    NodeType . RACK _ LOCAL )  ;", "}", "return   assignedContainers ;", "}", "METHOD_END"], "methodName": ["assignRackLocalContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "SchedulerApplication < FiCaSchedulerApp >    application    =    applications . get ( applicationId )  ;", "if    ( application    =  =    null )     {", ". LOG . warn (  (  \" Couldn ' t   find   application    \"     +    applicationId )  )  ;", "return ;", "}", "activeUsersManager . deactivateApplication ( application . getUser (  )  ,    applicationId )  ;", "application . stop ( finalState )  ;", "applications . remove ( applicationId )  ;", "}", "METHOD_END"], "methodName": ["doneApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerApp   attempt    =    getApplicationAttempt ( applicationAttemptId )  ;", "SchedulerApplication < FiCaSchedulerApp >    application    =    applications . get ( applicationAttemptId . getApplicationId (  )  )  ;", "if    (  ( application    =  =    null )     |  |     ( attempt    =  =    null )  )     {", "throw   new   IOException (  (  (  \" Unknown   application    \"     +    applicationAttemptId )     +     \"    has   completed !  \"  )  )  ;", "}", "for    ( RMContainer   container    :    attempt . getLiveContainers (  )  )     {", "if    ( keepContainers    &  &     ( container . getState (  )  . equals ( RMContainerState . RUNNING )  )  )     {", ". LOG . info (  (  \" Skip   killing    \"     +     ( container . getContainerId (  )  )  )  )  ;", "continue ;", "}", "completedContainer ( container ,    SchedulerUtils . createAbnormalContainerStatus ( container . getContainerId (  )  ,    SchedulerUtils . COMPLETED _ APPLICATION )  ,    RMContainerEventType . KILL )  ;", "}", "attempt . stop ( rmAppAttemptFinalState )  ;", "}", "METHOD_END"], "methodName": ["doneApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   maxContainers    =     0  ;", "RRequest   offSwitchRequest    =    application . getRRequest ( priority ,    ANY )  ;", "if    ( offSwitchRequest    !  =    null )     {", "maxContainers    =    offSwitchRequest . getNumContainers (  )  ;", "}", "if    ( type    =  =     ( NodeType . OFF _ SWITCH )  )     {", "return   maxContainers ;", "}", "if    ( type    =  =     ( NodeType . RACK _ LOCAL )  )     {", "RRequest   rackLocalRequest    =    application . getRRequest ( priority ,    node . getRMNode (  )  . getRackName (  )  )  ;", "if    ( rackLocalRequest    =  =    null )     {", "return   maxContainers ;", "}", "maxContainers    =    Math . min ( maxContainers ,    rackLocalRequest . getNumContainers (  )  )  ;", "}", "if    ( type    =  =     ( NodeType . NODE _ LOCAL )  )     {", "RRequest   nodeLocalRequest    =    application . getRRequest ( priority ,    node . getRMNode (  )  . getNodeAddress (  )  )  ;", "if    ( nodeLocalRequest    !  =    null )     {", "maxContainers    =    Math . min ( maxContainers ,    nodeLocalRequest . getNumContainers (  )  )  ;", "}", "}", "return   maxContainers ;", "}", "METHOD_END"], "methodName": ["getMaxAllocatableContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "return   nodes . get ( nodeId )  ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "return   usedResource ;", "}", "METHOD_END"], "methodName": ["getUsedResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Resources . addTo ( usedResource ,    rmContainer . getAllocatedResource (  )  )  ;", "}", "METHOD_END"], "methodName": ["increaseUsedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "validateConf ( conf )  ;", "this . applications    =    new   ConcurrentSkipListMap < ApplicationId ,    SchedulerApplication < FiCaSchedulerApp >  >  (  )  ;", "this . minimumAllocation    =    Resources . createResource ( conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )  ;", "this . maximumAllocation    =    Resources . createResource ( conf . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )  ;", "this . usePortForNodeName    =    conf . getBoolean ( RM _ SCHEDULER _ INCLUDE _ PORT _ IN _ NODE _ NAME ,    DEFAULT _ RM _ SCHEDULER _ USE _ PORT _ FOR _ NODE _ NAME )  ;", "this . metrics    =    QueueMetrics . forQueue ( FifoScheduler . DEFAULT _ QUEUE _ NAME ,    null ,    false ,    conf )  ;", "this . activeUsersManager    =    new   ActiveUsersManager ( metrics )  ;", "}", "METHOD_END"], "methodName": ["initScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerNode   node    =    getNode ( rmNode . getNodeID (  )  )  ;", "SchedulerUtils . updateResourceIfChanged ( node ,    rmNode ,    clusterResource ,     . LOG )  ;", "List < UpdatedContainerInfo >    containerInfoList    =    rmNode . pullContainerUpdates (  )  ;", "List < ContainerStatus >    newlyLaunchedContainers    =    new   ArrayList < ContainerStatus >  (  )  ;", "List < ContainerStatus >    completedContainers    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( UpdatedContainerInfo   containerInfo    :    containerInfoList )     {", "newlyLaunchedContainers . addAll ( containerInfo . getNewlyLaunchedContainers (  )  )  ;", "completedContainers . addAll ( containerInfo . getCompletedContainers (  )  )  ;", "}", "for    ( ContainerStatus   launchedContainer    :    newlyLaunchedContainers )     {", "containerLaunchedOnNode ( launchedContainer . getContainerId (  )  ,    node )  ;", "}", "for    ( ContainerStatus   completedContainer    :    completedContainers )     {", "ContainerId   containerId    =    completedContainer . getContainerId (  )  ;", ". LOG . debug (  (  \" Container   FINISHED :     \"     +    containerId )  )  ;", "completedContainer ( getRMContainer ( containerId )  ,    completedContainer ,    RMContainerEventType . FINISHED )  ;", "}", "if    ( Resources . greaterThanOrEqual ( resourceCalculator ,    clusterResource ,    node . getAvailableResource (  )  ,    minimumAllocation )  )     {", ". LOG . debug (  (  (  (  \" Node   heartbeat    \"     +     ( rmNode . getNodeID (  )  )  )     +     \"    available   resource    =     \"  )     +     ( node . getAvailableResource (  )  )  )  )  ;", "assignContainers ( node )  ;", ". LOG . debug (  (  (  (  \" Node   after   allocation    \"     +     ( rmNode . getNodeID (  )  )  )     +     \"    resource    =     \"  )     +     ( node . getAvailableResource (  )  )  )  )  ;", "}", "updateAvailableResourcesMetrics (  )  ;", "}", "METHOD_END"], "methodName": ["nodeUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FiCaSchedulerNode   node    =    getNode ( nodeInfo . getNodeID (  )  )  ;", "if    ( node    =  =    null )     {", "return ;", "}", "for    ( RMContainer   container    :    node . getRunningContainers (  )  )     {", "completedContainer ( container ,    SchedulerUtils . createAbnormalContainerStatus ( container . getContainerId (  )  ,    SchedulerUtils . LOST _ CONTAINER )  ,    RMContainerEventType . KILL )  ;", "}", "this . nodes . remove ( nodeInfo . getNodeID (  )  )  ;", "Resources . subtractFrom ( clusterResource ,    node . getRMNode (  )  . getTotalCapability (  )  )  ;", "}", "METHOD_END"], "methodName": ["removeNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "schedulerAttempt . setHeadroom ( Resources . subtract ( clusterResource ,    usedResource )  )  ;", "}", "METHOD_END"], "methodName": ["updateAppHeadRoom"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "metrics . setAvailableResourcesToQueue ( Resources . subtract ( clusterResource ,    usedResource )  )  ;", "}", "METHOD_END"], "methodName": ["updateAvailableResourcesMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "int   minMem    =    conf . getInt ( RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  ;", "int   maxMem    =    conf . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  ;", "if    (  ( minMem    <  =     0  )     |  |     ( minMem    >    maxMem )  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  (  (  (  (  (  (  (  (  \" Invalid   resource   scheduler   memory \"     +     (  \"    allocation   configuration \"     +     \"  ,     \"  )  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB )  )     +     \"  =  \"  )     +    minMem )     +     \"  ,     \"  )     +     ( YarnConfiguration . RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )     +     \"  =  \"  )     +    maxMem )     +     \"  ,    min   and   max   should   be   greater   than    0  \"  )     +     \"  ,    max   should   be   no   smaller   than   min .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateConf"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( expected ,    application . getUsedResources (  )  . getMemory (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkApplicationResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Assert . assertEquals ( expected ,    node . getUsed (  )  . getMemory (  )  )  ;", "node . checkRUsage (  )  ;", "}", "METHOD_END"], "methodName": ["checkNodeResourceUsage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appIdImpl    =    ApplicationId . newInstance (  0  ,    appId )  ;", "ApplicationAttemptId   attId    =    ApplicationAttemptId . newInstance ( appIdImpl ,    attemptId )  ;", "return   attId ;", "}", "METHOD_END"], "methodName": ["createAppAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    TestFifoScheduler . recordFactory . newRecordInstance ( ResourceRequest . class )  ;", "request . setCapability ( Resources . createResource ( memory )  )  ;", "request . setResourceName ( host )  ;", "request . setNumContainers ( numContainers )  ;", "Priority   prio    =    TestFifoScheduler . recordFactory . newRecordInstance ( Priority . class )  ;", "prio . setPriority ( priority )  ;", "request . setPriority ( prio )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["createResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "TestFifoScheduler   t    =    new   TestFifoScheduler (  )  ;", "t . setUp (  )  ;", "t . testFifoScheduler (  )  ;", "t . tearDown (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeManager ( hostName ,    containerManagerPort ,    nmHttpPort ,    rackName ,    capability ,    resourceManager )  ;", "}", "METHOD_END"], "methodName": ["registerNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "resourceManager    =    new   ResourceManager (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "resourceManager . init ( conf )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "resourceManager . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >    fs    =     (  ( AbstractYarnScheduler < SchedulerApplicationAttempt ,    SchedulerNode >  )     ( rm . getResourceScheduler (  )  )  )  ;", "TestSchedulerUtils . verifyAppAddedAndRemovedFromScheduler ( fs . getSchedulerApplications (  )  ,    fs ,     \" queue \"  )  ;", "}", "METHOD_END"], "methodName": ["testAddAndRemoveAppFromFiFoScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "AsyncDispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   rmContext    =    new   RMContextImpl ( dispatcher ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    writer )  ;", "scheduler    =    new    (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "scheduler . setRMContext ( rmContext )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( conf ,    rmContext )  ;", "QueueMetrics   metrics    =    scheduler . getRootQueueMetrics (  )  ;", "int   beforeAppsSubmitted    =    metrics . getAppsSubmitted (  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  2  0  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "SchedulerEvent   appEvent    =    new   AppAddedSchedulerEvent ( appId ,     \" queue \"  ,     \" user \"  )  ;", "scheduler . handle ( appEvent )  ;", "SchedulerEvent   attemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId ,    false )  ;", "scheduler . handle ( attemptEvent )  ;", "appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     2  )  ;", "SchedulerEvent   attemptEvent 2     =    new   AppAttemptAddedSchedulerEvent ( appAttemptId ,    false )  ;", "scheduler . handle ( attemptEvent 2  )  ;", "int   afterAppsSubmitted    =    metrics . getAppsSubmitted (  )  ;", "Assert . assertEquals (  1  ,     ( afterAppsSubmitted    -    beforeAppsSubmitted )  )  ;", "scheduler . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( RM _ SCHEDULER ,     . class ,    ResourceScheduler . class )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "rm . start (  )  ;", "fs    =     (  (  )     ( rm . getResourceScheduler (  )  )  )  ;", "String   host    =     \"  1  2  7  .  0  .  0  .  1  \"  ;", "RMNode   node    =    MockNodes . newNodeInfo (  0  ,    MockNodes . newResource (  (  4     *     ( GB )  )  )  ,     1  ,    host )  ;", "fs . handle ( new   NodeAddedSchedulerEvent ( node )  )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  1  0  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    BuilderUtils . newApplicationAttemptId ( appId ,     1  )  ;", "SchedulerEvent   appEvent    =    new   AppAddedSchedulerEvent ( appId ,     \" default \"  ,     \" user \"  )  ;", "fs . handle ( appEvent )  ;", "SchedulerEvent   attemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId ,    false )  ;", "fs . handle ( attemptEvent )  ;", "fs . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    Collections . singletonList ( host )  ,    null )  ;", "Assert . assertTrue ( fs . getApplicationAttempt ( appAttemptId )  . isBlacklisted ( host )  )  ;", "fs . allocate ( appAttemptId ,    Collections .  < ResourceRequest > emptyList (  )  ,    Collections .  < ContainerId > emptyList (  )  ,    null ,    Collections . singletonList ( host )  )  ;", "Assert . assertFalse ( fs . getApplicationAttempt ( appAttemptId )  . isBlacklisted ( host )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testBlackListNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "TestFifoScheduler . LOG . info (  \"  -  -  -    START :    testFifoScheduler    -  -  -  \"  )  ;", "final   int   GB    =     1  0  2  4  ;", "String   host _  0     =     \" host _  0  \"  ;", "NodeManager   nm _  0     =    registerNode ( host _  0  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  4     *    GB )  ,     1  )  )  ;", "nm _  0  . heartbeat (  )  ;", "String   host _  1     =     \" host _  1  \"  ;", "NodeManager   nm _  1     =    registerNode ( host _  1  ,     1  2  3  4  ,     2  3  4  5  ,    DEFAULT _ RACK ,    Resources . createResource (  (  2     *    GB )  ,     1  )  )  ;", "nm _  1  . heartbeat (  )  ;", "Priority   priority _  0     =    resource . Priority . create (  0  )  ;", "Priority   priority _  1     =    resource . Priority . create (  1  )  ;", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "application _  0  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  0  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  0  _  0     =    Resources . createResource ( GB )  ;", "application _  0  . addResourceRequestSpec ( priority _  1  ,    capability _  0  _  0  )  ;", "Resource   capability _  0  _  1     =    Resources . createResource (  (  2     *    GB )  )  ;", "application _  0  . addResourceRequestSpec ( priority _  0  ,    capability _  0  _  1  )  ;", "Task   task _  0  _  0     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  0  )  ;", "Application   application _  1     =    new   Application (  \" user _  1  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "application _  1  . addNodeManager ( host _  0  ,     1  2  3  4  ,    nm _  0  )  ;", "application _  1  . addNodeManager ( host _  1  ,     1  2  3  4  ,    nm _  1  )  ;", "Resource   capability _  1  _  0     =    Resources . createResource (  (  3     *    GB )  )  ;", "application _  1  . addResourceRequestSpec ( priority _  1  ,    capability _  1  _  0  )  ;", "Resource   capability _  1  _  1     =    Resources . createResource (  (  4     *    GB )  )  ;", "application _  1  . addResourceRequestSpec ( priority _  0  ,    capability _  1  _  1  )  ;", "Task   task _  1  _  0     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  1  . addTask ( task _  1  _  0  )  ;", "TestFifoScheduler . LOG . info (  \" Send   resource   requests   to   the   scheduler \"  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "TestFifoScheduler . LOG . info (  (  \" Send   a   heartbeat   to   kick   the   tires   on   the   Scheduler .  .  .     \"     +     (  \" nm 0     -  >    task _  0  _  0    and   task _  1  _  0    allocated ,    used =  4 G    \"     +     \" nm 1     -  >    nothing   allocated \"  )  )  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage ( GB ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  1  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkNodeResourceUsage (  (  4     *    GB )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  0     *    GB )  ,    nm _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Adding   new   tasks .  .  .  \"  )  ;", "Task   task _  1  _  1     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application _  1  . addTask ( task _  1  _  1  )  ;", "Task   task _  1  _  2     =    new   Task ( application _  1  ,    priority _  1  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application _  1  . addTask ( task _  1  _  2  )  ;", "Task   task _  1  _  3     =    new   Task ( application _  1  ,    priority _  0  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application _  1  . addTask ( task _  1  _  3  )  ;", "application _  1  . schedule (  )  ;", "Task   task _  0  _  1     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  1  )  ;", "Task   task _  0  _  2     =    new   Task ( application _  0  ,    priority _  1  ,    new   String [  ]  {    host _  0  ,    host _  1     }  )  ;", "application _  0  . addTask ( task _  0  _  2  )  ;", "Task   task _  0  _  3     =    new   Task ( application _  0  ,    priority _  0  ,    new   String [  ]  {    ResourceRequest . ANY    }  )  ;", "application _  0  . addTask ( task _  0  _  3  )  ;", "application _  0  . schedule (  )  ;", "TestFifoScheduler . LOG . info (  (  \" Sending   hb   from    \"     +     ( nm _  0  . getHostName (  )  )  )  )  ;", "nm _  0  . heartbeat (  )  ;", "TestFifoScheduler . LOG . info (  (  \" Sending   hb   from    \"     +     ( nm _  1  . getHostName (  )  )  )  )  ;", "nm _  1  . heartbeat (  )  ;", "TestFifoScheduler . LOG . info (  \" Trying   to   allocate .  .  .  \"  )  ;", "application _  0  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  0  )  ;", "application _  1  . schedule (  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  1  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkNodeResourceUsage (  (  4     *    GB )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  2     *    GB )  ,    nm _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  0  _  0  \"  )  ;", "application _  0  . finishTask ( task _  0  _  0  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  4     *    GB )  ,    nm _  0  )  ;", "checkNodeResourceUsage (  (  2     *    GB )  ,    nm _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  1  _  0  \"  )  ;", "application _  1  . finishTask ( task _  1  _  0  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  4     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  0     *    GB )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  2     *    GB )  ,    nm _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  0  _  3  \"  )  ;", "application _  0  . finishTask ( task _  0  _  3  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  2     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  0     *    GB )  ,    application _  1  )  ;", "checkNodeResourceUsage (  (  0     *    GB )  ,    nm _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  0  _  1  \"  )  ;", "application _  0  . finishTask ( task _  0  _  1  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  1     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  0     *    GB )  ,    application _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  0  _  2  \"  )  ;", "application _  0  . finishTask ( task _  0  _  2  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  0     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  4     *    GB )  ,    application _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  1  _  3  \"  )  ;", "application _  1  . finishTask ( task _  1  _  3  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  0     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  1  )  ;", "TestFifoScheduler . LOG . info (  \" Finishing   up   task _  1  _  1  \"  )  ;", "application _  1  . finishTask ( task _  1  _  1  )  ;", "application _  0  . schedule (  )  ;", "application _  1  . schedule (  )  ;", "nm _  0  . heartbeat (  )  ;", "nm _  1  . heartbeat (  )  ;", "checkApplicationResourceUsage (  (  0     *    GB )  ,    application _  0  )  ;", "checkApplicationResourceUsage (  (  3     *    GB )  ,    application _  1  )  ;", "TestFifoScheduler . LOG . info (  \"  -  -  -    END :    testFifoScheduler    -  -  -  \"  )  ;", "}", "METHOD_END"], "methodName": ["testFifoScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "FifoScheduler   scheduler    =    new   FifoScheduler (  )  ;", "QueueInfo   queueInfo    =    scheduler . getQueueInfo ( null ,    false ,    false )  ;", "Assert . assertEquals (  0  .  0 F ,    queueInfo . getCurrentCapacity (  )  ,     0  .  0 F )  ;", "}", "METHOD_END"], "methodName": ["testFifoSchedulerCapacityWhenNoNMs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "Application   application _  0     =    new   Application (  \" user _  0  \"  ,    resourceManager )  ;", "application _  0  . submit (  )  ;", "Application   application _  1     =    new   Application (  \" user _  0  \"  ,    resourceManager )  ;", "application _  1  . submit (  )  ;", "ResourceScheduler    =    resourceManager . getResourceScheduler (  )  ;", "List < ApplicationAttemptId >    appsInDefault    =    getAppsInQueue (  \" default \"  )  ;", "assertTrue ( appsInDefault . contains ( application _  0  . getApplicationAttemptId (  )  )  )  ;", "assertTrue ( appsInDefault . contains ( application _  1  . getApplicationAttemptId (  )  )  )  ;", "assertEquals (  2  ,    appsInDefault . size (  )  )  ;", "Assert . assertNull ( getAppsInQueue (  \" someotherqueue \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetAppsInQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "AsyncDispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "containerTokenSecretManager . rollMasterKey (  )  ;", "NMTokenSecretManagerInRM   nmTokenSecretManager    =    new   NMTokenSecretManagerInRM ( conf )  ;", "nmTokenSecretManager . rollMasterKey (  )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   rmContext    =    new   RMContextImpl ( dispatcher ,    null ,    null ,    null ,    null ,    null ,    containerTokenSecretManager ,    nmTokenSecretManager ,    null ,    writer )  ;", "FifoScheduler   scheduler    =    new   FifoScheduler (  )  ;", "scheduler . setRMContext ( rmContext )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( new   Configuration (  )  ,    rmContext )  ;", "RMNode   node 0     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  (  1  0  2  4     *     6  4  )  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 0  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "int    _ appId    =     1  ;", "int    _ appAttemptId    =     1  ;", "ApplicationAttemptId   appAttemptId    =    createAppAttemptId (  _ appId ,     _ appAttemptId )  ;", "AppAddedSchedulerEvent   appEvent    =    new   AppAddedSchedulerEvent ( appAttemptId . getApplicationId (  )  ,     \" queue 1  \"  ,     \" user 1  \"  )  ;", "scheduler . handle ( appEvent )  ;", "AppAttemptAddedSchedulerEvent   attemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId ,    false )  ;", "scheduler . handle ( attemptEvent )  ;", "int   memory    =     6  4  ;", "int   nConts    =     3  ;", "int   priority    =     2  0  ;", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   nodeLocal    =    createResourceRequest ( memory ,    node 0  . getHostName (  )  ,    priority ,    nConts )  ;", "ResourceRequest   rackLocal    =    createResourceRequest ( memory ,    node 0  . getRackName (  )  ,    priority ,    nConts )  ;", "ResourceRequest   any    =    createResourceRequest ( memory ,    ANY ,    priority ,    nConts )  ;", "ask . add ( nodeLocal )  ;", "ask . add ( rackLocal )  ;", "ask . add ( any )  ;", "scheduler . allocate ( appAttemptId ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "NodeUpdateSchedulerEvent   node 0 Update    =    new   NodeUpdateSchedulerEvent ( node 0  )  ;", "Assert . assertEquals (  3  ,    nodeLocal . getNumContainers (  )  )  ;", "scheduler . handle ( node 0 Update )  ;", "Assert . assertEquals (  0  ,    nodeLocal . getNumContainers (  )  )  ;", "SchedulerAppReport   info    =    scheduler . getSchedulerAppInfo ( appAttemptId )  ;", "Assert . assertEquals (  3  ,    info . getLiveContainers (  )  . size (  )  )  ;", "scheduler . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNodeLocalAssignment"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "AsyncDispatcher   dispatcher    =    new   InlineDispatcher (  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "RMContainerTokenSecretManager   containerTokenSecretManager    =    new   RMContainerTokenSecretManager ( conf )  ;", "containerTokenSecretManager . rollMasterKey (  )  ;", "NMTokenSecretManagerInRM   nmTokenSecretManager    =    new   NMTokenSecretManagerInRM ( conf )  ;", "nmTokenSecretManager . rollMasterKey (  )  ;", "RMApplicationHistoryWriter   writer    =    mock ( RMApplicationHistoryWriter . class )  ;", "RMContext   rmContext    =    new   RMContextImpl ( dispatcher ,    null ,    null ,    null ,    null ,    null ,    containerTokenSecretManager ,    nmTokenSecretManager ,    null ,    writer )  ;", "FifoScheduler   scheduler    =    new   FifoScheduler (  )     {", "@ SuppressWarnings (  \" unused \"  )", "public   Map < NodeId ,    FiCaSchedulerNode >    getNodes (  )     {", "return   nodes ;", "}", "}  ;", "scheduler . setRMContext ( rmContext )  ;", "scheduler . init ( conf )  ;", "scheduler . start (  )  ;", "scheduler . reinitialize ( new   Configuration (  )  ,    rmContext )  ;", "RMNode   node 0     =    MockNodes . newNodeInfo (  1  ,    Resources . createResource (  2  0  4  8  ,     4  )  ,     1  ,     \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "NodeAddedSchedulerEvent   nodeEvent 1     =    new   NodeAddedSchedulerEvent ( node 0  )  ;", "scheduler . handle ( nodeEvent 1  )  ;", "Method   method    =    scheduler . getClass (  )  . getDeclaredMethod (  \" getNodes \"  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Map < NodeId ,    FiCaSchedulerNode >    schedulerNodes    =     (  ( Map < NodeId ,    FiCaSchedulerNode >  )     ( method . invoke ( scheduler )  )  )  ;", "assertEquals ( schedulerNodes . values (  )  . size (  )  ,     1  )  ;", "node 0  . setResourceOption ( ResourceOption . newInstance ( Resources . createResource (  1  0  2  4  ,     4  )  ,    RMNode . OVER _ COMMIT _ TIMEOUT _ MILLIS _ DEFAULT )  )  ;", "assertEquals ( node 0  . getTotalCapability (  )  . getMemory (  )  ,     1  0  2  4  )  ;", "assertEquals ( schedulerNodes . get ( node 0  . getNodeID (  )  )  . getAvailableResource (  )  . getMemory (  )  ,     2  0  4  8  )  ;", "NodeUpdateSchedulerEvent   node 0 Update    =    new   NodeUpdateSchedulerEvent ( node 0  )  ;", "scheduler . handle ( node 0 Update )  ;", "assertEquals ( schedulerNodes . get ( node 0  . getNodeID (  )  )  . getAvailableResource (  )  . getMemory (  )  ,     1  0  2  4  )  ;", "QueueInfo   queueInfo    =    scheduler . getQueueInfo ( null ,    false ,    false )  ;", "Assert . assertEquals (  0  .  0 F ,    queueInfo . getCurrentCapacity (  )  ,     0  .  0 F )  ;", "int    _ appId    =     1  ;", "int    _ appAttemptId    =     1  ;", "ApplicationAttemptId   appAttemptId    =    createAppAttemptId (  _ appId ,     _ appAttemptId )  ;", "AppAddedSchedulerEvent   appEvent    =    new   AppAddedSchedulerEvent ( appAttemptId . getApplicationId (  )  ,     \" queue 1  \"  ,     \" user 1  \"  )  ;", "scheduler . handle ( appEvent )  ;", "AppAttemptAddedSchedulerEvent   attemptEvent    =    new   AppAttemptAddedSchedulerEvent ( appAttemptId ,    false )  ;", "scheduler . handle ( attemptEvent )  ;", "int   memory    =     1  0  2  4  ;", "int   priority    =     1  ;", "List < ResourceRequest >    ask    =    new   ArrayList < ResourceRequest >  (  )  ;", "ResourceRequest   nodeLocal    =    createResourceRequest ( memory ,    node 0  . getHostName (  )  ,    priority ,     1  )  ;", "ResourceRequest   rackLocal    =    createResourceRequest ( memory ,    node 0  . getRackName (  )  ,    priority ,     1  )  ;", "ResourceRequest   any    =    createResourceRequest ( memory ,    ANY ,    priority ,     1  )  ;", "ask . add ( nodeLocal )  ;", "ask . add ( rackLocal )  ;", "ask . add ( any )  ;", "scheduler . allocate ( appAttemptId ,    ask ,    new   ArrayList < api . records . ContainerId >  (  )  ,    null ,    null )  ;", "Assert . assertEquals (  1  ,    nodeLocal . getNumContainers (  )  )  ;", "scheduler . handle ( node 0 Update )  ;", "Assert . assertEquals (  0  ,    nodeLocal . getNumContainers (  )  )  ;", "SchedulerAppReport   info    =    scheduler . getSchedulerAppInfo ( appAttemptId )  ;", "Assert . assertEquals (  1  ,    info . getLiveContainers (  )  . size (  )  )  ;", "queueInfo    =    scheduler . getQueueInfo ( null ,    false ,    false )  ;", "Assert . assertEquals (  1  .  0 F ,    queueInfo . getCurrentCapacity (  )  ,     0  .  0 F )  ;", "}", "METHOD_END"], "methodName": ["testUpdateResourceOnNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.TestFifoScheduler"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", ". LOG . info (  (  \" Activating   next   master   key   with   id :     \"     +     ( this . nextMasterKey . getMasterKey (  )  . getKeyId (  )  )  )  )  ;", "this . currentMasterKey    =    this . nextMasterKey ;", "this . nextMasterKey    =    null ;", "State   state    =    State . newInstance ( this . currentMasterKey . getMasterKey (  )  ,    null )  ;", "rmContext . getStateStore (  )  . storeOrUpdateState ( state ,    true )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["activateNextMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", "AMRMTokenIdentifier   identifier    =    token . decodeIdentifier (  )  ;", ". LOG . debug (  (  \" Adding   password   for    \"     +     ( identifier . getApplicationAttemptId (  )  )  )  )  ;", "appAttemptSet . add ( identifier . getApplicationAttemptId (  )  )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["addPersistedPassword"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", ". LOG . info (  (  \" Application   finished ,    removing   password   for    \"     +    appAttemptId )  )  ;", "this . appAttemptSet . remove ( appAttemptId )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["applicationMasterFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", ". LOG . info (  (  \" Create   AMRMToken   for   ApplicationAttempt :     \"     +    appAttemptId )  )  ;", "AMRMTokenIdentifier   identifier    =    new   AMRMTokenIdentifier ( appAttemptId ,    getMasterKey (  )  . getMasterKey (  )  . getKeyId (  )  )  ;", "byte [  ]    password    =    this . createPassword ( identifier )  ;", "appAttemptSet . add ( appAttemptId )  ;", "return   new   Token < AMRMTokenIdentifier >  ( identifier . getBytes (  )  ,    password ,    identifier . getKind (  )  ,    new   Text (  )  )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createAndGetAMRMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", "return   new   MasterKeyData (  (  ( serialNo )  +  +  )  ,    generate (  )  )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createNewMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return   this . currentMasterKey ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getCurrnetMasterKeyData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return    ( nextMasterKey )     =  =    null    ?    currentMasterKey    :    nextMasterKey ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return   this . nextMasterKey ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getNextMasterKeyData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "if    (  ( state . getAMRMTokenSecretManagerState (  )  )     !  =    null )     {", "MasterKey   currentKey    =    state . getAMRMTokenSecretManagerState (  )  . getCurrentMasterKey (  )  ;", "this . currentMasterKey    =    new   MasterKeyData ( currentKey ,    createSecretKey ( currentKey . getBytes (  )  . array (  )  )  )  ;", "MasterKey   nextKey    =    state . getAMRMTokenSecretManagerState (  )  . getNextMasterKey (  )  ;", "if    ( nextKey    !  =    null )     {", "this . nextMasterKey    =    new   MasterKeyData ( nextKey ,    createSecretKey ( nextKey . getBytes (  )  . array (  )  )  )  ;", "this . timer . schedule ( new   AMRMTokenSecretManager . NextKeyActivator (  )  ,    this . activationDelay )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["recover"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", ". LOG . info (  \" Rolling   master - key   for   amrm - tokens \"  )  ;", "this . nextMasterKey    =    createNewMasterKey (  )  ;", "State   state    =    State . newInstance ( this . currentMasterKey . getMasterKey (  )  ,    this . nextMasterKey . getMasterKey (  )  )  ;", "rmContext . getStateStore (  )  . storeOrUpdateState ( state ,    true )  ;", "this . timer . schedule ( new    . NextKeyActivator (  )  ,    this . activationDelay )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["rollMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . currentMasterKey )     =  =    null )     {", "this . currentMasterKey    =    createNewMasterKey (  )  ;", "State   state    =    State . newInstance ( this . currentMasterKey . getMasterKey (  )  ,    null )  ;", "rmContext . getStateStore (  )  . storeOrUpdateState ( state ,    false )  ;", "}", "this . timer . scheduleAtFixedRate ( new    . MasterKeyRoller (  )  ,    rollingInterval ,    rollingInterval )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . timer . cancel (  )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "return   generateSecret (  )  ;", "}", "METHOD_END"], "methodName": ["createMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "this . masterKeys . put ( applicationAttemptID ,    key )  ;", "}", "METHOD_END"], "methodName": ["registerApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "SecretKey   key    =    createSecretKey ( keyData )  ;", "registerApplication ( applicationAttemptID ,    key )  ;", "return   key ;", "}", "METHOD_END"], "methodName": ["registerMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "this . masterKeys . remove ( applicationAttemptID )  ;", "}", "METHOD_END"], "methodName": ["unRegisterApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "processDelegationTokenRenewerEvent ( new   DelegationTokenRenewer . DelegationTokenRenewerAppSubmitEvent ( applicationId ,    ts ,    shouldCancelAtEnd )  )  ;", "}", "METHOD_END"], "methodName": ["addApplicationAsync"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "handleAppSubmitEvent ( new   DelegationTokenRenewer . DelegationTokenRenewerAppSubmitEvent ( applicationId ,    ts ,    shouldCancelAtEnd )  )  ;", "}", "METHOD_END"], "methodName": ["addApplicationSync"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "delegationTokens . add ( t )  ;", "}", "METHOD_END"], "methodName": ["addTokenToList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "processDelegationTokenRenewerEvent ( new   DelegationTokenRenewer . DelegationTokenRenewerEvent ( applicationId ,    DelegationTokenRenewer . DelegationTokenRenewerEventType . FINISH _ APPLICATION )  )  ;", "}", "METHOD_END"], "methodName": ["applicationFinished"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "if    ( t . shouldCancelAtEnd )     {", "dtCancelThread . cancelToken ( t . token ,    t . conf )  ;", "} else    {", ". LOG . info (  (  \" Did   not   cancel    \"     +    t )  )  ;", "}", "}", "METHOD_END"], "methodName": ["cancelToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "int   nThreads    =    conf . getInt ( RM _ DELEGATION _ TOKEN _ RENEWER _ THREAD _ COUNT ,    DEFAULT _ RM _ DELEGATION _ TOKEN _ RENEWER _ THREAD _ COUNT )  ;", "ThreadFactory   tf    =    new   ThreadFactoryBuilder (  )  . setNameFormat (  \"     #  % d \"  )  . build (  )  ;", "ThreadPoolExecutor   pool    =    new   ThreadPoolExecutor (  (  5     <    nThreads    ?     5     :    nThreads )  ,    nThreads ,     3 L ,    TimeUnit . SECONDS ,    new   LinkedBlockingQueue < Runnable >  (  )  )  ;", "pool . setThreadFactory ( tf )  ;", "pool . allowCoreThreadTimeOut ( true )  ;", "return   pool ;", "}", "METHOD_END"], "methodName": ["createNewThreadPoolService"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Set < Token <  ?  >  >    tokens    =    new   HashSet < Token <  ?  >  >  (  )  ;", "for    (  . DelegationTokenToRenew   delegationToken    :    delegationTokens )     {", "tokens . add ( delegationToken . token )  ;", "}", "return   tokens ;", "}", "METHOD_END"], "methodName": ["getDelegationTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( tokenKeepAliveEnabled )  )     {", "removeApplicationFromRenewal ( evt . getApplicationId (  )  )  ;", "} else    {", "delayedRemovalMap . put ( evt . getApplicationId (  )  ,     (  ( System . currentTimeMillis (  )  )     +     ( tmovalDelayMs )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["handleAppFinishEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    evt . getApplicationId (  )  ;", "Credentials   ts    =    evt . getCredentials (  )  ;", "boolean   shouldCancelAtEnd    =    evt . shouldCancelAtEnd (  )  ;", "if    ( ts    =  =    null )     {", "return ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  \" Registering   tokens   for   renewal   for :  \"     +     \"    appId    =     \"  )     +    applicationId )  )  ;", "}", "Collection < Token <  ?  >  >    tokens    =    ts . getAllTokens (  )  ;", "long   now    =    System . currentTimeMillis (  )  ;", "List <  . DelegationTokenToRenew >    tokenList    =    new   ArrayList <  . DelegationTokenToRenew >  (  )  ;", "for    ( Token <  ?  >    token    :    tokens )     {", "if    ( token . isManaged (  )  )     {", "tokenList . add ( new    . DelegationTokenToRenew ( applicationId ,    token ,    getConfig (  )  ,    now ,    shouldCancelAtEnd )  )  ;", "}", "}", "if    (  !  ( tokenList . isEmpty (  )  )  )     {", "for    (  . DelegationTokenToRenew   dtr    :    tokenList )     {", "try    {", "renewToken ( dtr )  ;", "}    catch    ( IOException   ioe )     {", "throw   new   IOException (  (  \" Failed   to   renew   token :     \"     +     ( dtr . token )  )  ,    ioe )  ;", "}", "}", "for    (  . DelegationTokenToRenew   dtr    :    tokenList )     {", "addTokenToList ( dtr )  ;", "setTimerForTokenRenewal ( dtr )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  \" Registering   token   for   renewal   for :  \"     +     \"    service    =     \"  )     +     ( dtr . token . getService (  )  )  )     +     \"    for   appId    =     \"  )     +     ( dtr . applicationId )  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["handleAppSubmitEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "serviceStateLock . readLock (  )  . lock (  )  ;", "try    {", "if    ( isServiceStarted )     {", "renewerService . execute ( newRunnable ( evt )  )  ;", "} else    {", "pendingEventQueue . add ( evt )  ;", "}", "}    finally    {", "serviceStateLock . readLock (  )  . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["processDelegationTokenRenewerEvent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "synchronized ( delegationTokens )     {", "Iterator <  . DelegationTokenToRenew >    it    =    delegationTokens . iterator (  )  ;", "while    ( it . hasNext (  )  )     {", ". DelegationTokenToRenew   dttr    =    it . next (  )  ;", "if    ( dttr . applicationId . equals ( applicationId )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Removing   delegation   token   for   appId =  \"     +    applicationId )     +     \"  ;    token =  \"  )     +     ( dttr . token . getService (  )  )  )  )  ;", "}", "if    (  ( dttr . timerTask )     !  =    null )", "dttr . timerTask . cancel (  )  ;", "cancelToken ( dttr )  ;", "it . remove (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["removeApplicationFromRenewal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    t . applicationId ;", "if    (  . LOG . isDebugEnabled (  )  )", ". LOG . debug (  (  (  (  \" removing   failed   delegation   token   for   appid =  \"     +    applicationId )     +     \"  ; t =  \"  )     +     ( t . token . getService (  )  )  )  )  ;", "delegationTokens . remove ( t )  ;", "if    (  ( t . timerTask )     !  =    null )", "t . timerTask . cancel (  )  ;", "}", "METHOD_END"], "methodName": ["removeFailedDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "try    {", "dttr . expirDate    =    UserGroupInform . getLoginUser (  )  . doAs ( new   PrivilegedExceptionAction < Long >  (  )     {", "@ Override", "public   Long   run (  )    throws   Exception    {", "return   dttr . token . renew ( dttr . conf )  ;", "}", "}  )  ;", "}    catch    ( InterruptedException   e )     {", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["renewToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Renewer . setSecretManager ( rmContext . getRMDelegationTokenSecretManager (  )  ,    rmContext . getClientRMService (  )  . getBindAddress (  )  )  ;", "}", "METHOD_END"], "methodName": ["setLocalSecretManagerAndServiceAddr"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "this . rmContext    =    rmContext ;", "}", "METHOD_END"], "methodName": ["setRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "long   expiresIn    =     ( token . expirationDate )     -     ( System . currentTimeMillis (  )  )  ;", "long   renewIn    =     ( token . expirationDate )     -     ( expiresIn    /     1  0  )  ;", "TimerTask   tTask    =    new    . RenewalTimerTask ( token )  ;", "token . setTimerTask ( tTask )  ;", "renewalTimer . schedule ( token . timerTask ,    new   Date ( renewIn )  )  ;", "}", "METHOD_END"], "methodName": ["setTimerForTokenRenewal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( tokenKeepAliveEnabled )     &  &     ( appIds    !  =    null )  )     &  &     (  ( appIds . size (  )  )     >     0  )  )     {", "for    ( ApplicationId   appId    :    appIds )     {", "delayedRemovalMap . put ( appId ,     (  ( System . currentTimeMillis (  )  )     +     ( tmovalDelayMs )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["updateKeepAliveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "super . writeLock . lock (  )  ;", "try    {", ". LOG . info (  (  \" Activating   next   master   key   with   id :     \"     +     ( this . nextMasterKey . getMasterKey (  )  . getKeyId (  )  )  )  )  ;", "this . currentMasterKey    =    this . nextMasterKey ;", "this . nextMasterKey    =    null ;", "clearApplicationNMTokenKeys (  )  ;", "}    finally    {", "super . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["activateNextMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "Iterator < HashSet < NodeId >  >    nodeSetI    =    this . appAttemptToNodeKeyMap . values (  )  . iterator (  )  ;", "while    ( nodeSetI . hasNext (  )  )     {", "nodeSetI . next (  )  . clear (  )  ;", "}", "}", "METHOD_END"], "methodName": ["clearApplicationNMTokenKeys"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "super . writeLock . lock (  )  ;", "try    {", "HashSet < NodeId >    nodeSet    =    this . appAttemptToNodeKeyMap . get ( attemptId )  ;", "if    ( nodeSet    !  =    null )     {", ". LOG . info (  (  \" Clear   node   set   for    \"     +    attemptId )  )  ;", "nodeSet . clear (  )  ;", "}", "}    finally    {", "super . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["clearNodeSetForAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "try    {", "this . readLock . lock (  )  ;", "HashSet < NodeId >    nodeSet    =    this . appAttemptToNodeKeyMap . get ( appAttemptId )  ;", "NMToken   nmToken    =    null ;", "if    ( nodeSet    !  =    null )     {", "if    (  !  ( nodeSet . contains ( container . getNodeId (  )  )  )  )     {", ". LOG . info (  (  (  (  \" Sending   NMToken   for   nodeId    :     \"     +     ( container . getNodeId (  )  )  )     +     \"    for   container    :     \"  )     +     ( container . getId (  )  )  )  )  ;", "Token   token    =    createNMToken ( container . getId (  )  . getApplicationAttemptId (  )  ,    container . getNodeId (  )  ,    applicationSubmitter )  ;", "nmToken    =    NMToken . newInstance ( container . getNodeId (  )  ,    token )  ;", "nodeSet . add ( container . getNodeId (  )  )  ;", "}", "}", "return   nmToken ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createAndGetNMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "super . readLock . lock (  )  ;", "try    {", "if    (  ( this . nextMasterKey )     =  =    null )     {", "return   null ;", "} else    {", "return   this . nextMasterKey . getMasterKey (  )  ;", "}", "}    finally    {", "super . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getNextKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "try    {", "thisadLock . lock (  )  ;", "HashSet < NodeId >    nodes    =    this . appAttemptToNodeKeyMap . get ( appAttemptId )  ;", "if    (  ( nodes    !  =    null )     &  &     ( nodes . contains ( nodeId )  )  )     {", "turn   true ;", "} else    {", "turn   false ;", "}", "}    finally    {", "thisadLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["isApplicationAttemptNMTokenPresent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "try    {", "thisadLock . lock (  )  ;", "turn   this . appAttemptToNodeKeyMap . containsKey ( appAttemptId )  ;", "}    finally    {", "thisadLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["isApplicationAttemptRegistered"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "try    {", "this . writeLock . lock (  )  ;", "this . appAttemptToNodeKeyMap . put ( appAttemptId ,    new   HashSet < api . records . NodeId >  (  )  )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["registerApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "try    {", "this . weLock . lock (  )  ;", "Iterator < HashSet < NodeId >  >    appNodeKeySetIterator    =    this . appAttemptToNodeKeyMap . values (  )  . iterator (  )  ;", "while    ( appNodeKeySetIterator . hasNext (  )  )     {", "appNodeKeySetIterator . next (  )  . remove ( nodeId )  ;", "}", "}    finally    {", "this . weLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["removeNodeKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "super . writeLock . lock (  )  ;", "try    {", ". LOG . info (  \" Rolling   master - key   for   nm - tokens \"  )  ;", "if    (  ( this . currentMasterKey )     =  =    null )     {", "this . currentMasterKey    =    createNewMasterKey (  )  ;", "} else    {", "this . nextMasterKey    =    createNewMasterKey (  )  ;", ". LOG . info (  (  (  (  (  \" Going   to   activate   master - key   with   key - id    \"     +     ( this . nextMasterKey . getMasterKey (  )  . getKeyId (  )  )  )     +     \"    in    \"  )     +     ( this . activationDelay )  )     +     \" ms \"  )  )  ;", "this . timer . schedule ( new    . NextKeyActivator (  )  ,    this . activationDelay )  ;", "}", "}    finally    {", "super . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["rollMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "rollMasterKey (  )  ;", "this . timer . scheduleAtFixedRate ( new    . MasterKeyRoller (  )  ,    rollingInterval ,    rollingInterval )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "this . timer . cancel (  )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "try    {", "this . weLock . lock (  )  ;", "this . appAttemptToNodeKeyMap . remove ( appAttemptId )  ;", "}    finally    {", "this . weLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["unregisterApplicationAttempt"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isACLsEnable )  )     {", "return   true ;", "}", "return   scheduler . checkAccess ( callerUGI ,    acl ,    queueName )  ;", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager"}, {"methodBody": ["METHOD_START", "{", "String   header    =    req . getHeader ( RMAuthenticationHandler . HEADER )  ;", "return   header ;", "}", "METHOD_END"], "methodName": ["getEncodedDelegationTokenFromRequest"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMAuthenticationHandler"}, {"methodBody": ["METHOD_START", "{", "RMAuthenticationHandler . secretManager    =    manager ;", "RMAuthenticationHandler . secretManagerInitialized    =    true ;", "}", "METHOD_END"], "methodName": ["setSecretManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMAuthenticationHandler"}, {"methodBody": ["METHOD_START", "{", "if    (  ( RMAuthenticationHandler . secretManagerInitialized )     =  =    false )     {", "throw   new   IllegalStateException (  \" Secret   manager   not   initialized \"  )  ;", "}", "ByteArrayInputStream   buf    =    new   ByteArrayInputStream ( token . getIdentifier (  )  )  ;", "DataInputStream   dis    =    new   DataInputStream ( buf )  ;", "RMDelegationTokenIdentifier   id    =    RMAuthenticationHandler . secretManager . createIdentifier (  )  ;", "try    {", "id . readFields ( dis )  ;", "RMAuthenticationHandler . secretManager . verifyToken ( id ,    token . getPassword (  )  )  ;", "}    catch    ( Throwable   t )     {", "return   null ;", "}    finally    {", "dis . close (  )  ;", "}", "return   id . getUser (  )  ;", "}", "METHOD_END"], "methodName": ["verifyToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMAuthenticationHandler"}, {"methodBody": ["METHOD_START", "{", "super . writeLock . lock (  )  ;", "try    {", ". LOG . info (  (  \" Activating   next   master   key   with   id :     \"     +     ( this . nextMasterKey . getMasterKey (  )  . getKeyId (  )  )  )  )  ;", "this . currentMasterKey    =    this . nextMasterKey ;", "this . nextMasterKey    =    null ;", "}    finally    {", "super . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["activateNextMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    password ;", "Identifier   tokenIdentifier ;", "long   expiryTimeStamp    =     ( System . currentTimeMillis (  )  )     +     ( containerTokenExpiryInterval )  ;", "this . readLock . lock (  )  ;", "try    {", "tokenIdentifier    =    new   Identifier ( containerId ,    nodeId . toString (  )  ,    appSubmitter ,    capability ,    expiryTimeStamp ,    this . currentMasterKey . getMasterKey (  )  . getKeyId (  )  ,    ResourceManager . getClusterTimeStamp (  )  ,    priority ,    createTime )  ;", "password    =    this . createPassword ( tokenIdentifier )  ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "return   BuilderUtils . new ( nodeId ,    password ,    tokenIdentifier )  ;", "}", "METHOD_END"], "methodName": ["createContainerToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "super . readLock . lock (  )  ;", "try    {", "if    (  ( this . nextMasterKey )     =  =    null )     {", "return   null ;", "} else    {", "return   this . nextMasterKey . getMasterKey (  )  ;", "}", "}    finally    {", "super . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getNextKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "super . writeLock . lock (  )  ;", "try    {", ". LOG . info (  \" Rolling   master - key   for   container - tokens \"  )  ;", "if    (  ( this . currentMasterKey )     =  =    null )     {", "this . currentMasterKey    =    createNewMasterKey (  )  ;", "} else    {", "this . nextMasterKey    =    createNewMasterKey (  )  ;", ". LOG . info (  (  (  (  (  \" Going   to   activate   master - key   with   key - id    \"     +     ( this . nextMasterKey . getMasterKey (  )  . getKeyId (  )  )  )     +     \"    in    \"  )     +     ( this . activationDelay )  )     +     \" ms \"  )  )  ;", "this . timer . schedule ( new    . NextKeyActivator (  )  ,    this . activationDelay )  ;", "}", "}    finally    {", "super . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["rollMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "rollMasterKey (  )  ;", "this . timer . scheduleAtFixedRate ( new    . MasterKeyRoller (  )  ,    rollingInterval ,    rollingInterval )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . timer . cancel (  )  ;", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "HashSet < DelegationKey >    keySet    =    new   HashSet < DelegationKey >  (  )  ;", "keySet . addAll ( allKeys . values (  )  )  ;", "return   keySet ;", "}", "METHOD_END"], "methodName": ["getAllMasterKeys"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "Map < RMDelegationTokenIdentifier ,    Long >    allTokens    =    new   HashMap < RMDelegationTokenIdentifier ,    Long >  (  )  ;", "for    ( Map . Entry < RMDelegationTokenIdentifier ,    DelegationTokenInformation >    entry    :    currentTokens . entrySet (  )  )     {", "allTokens . put ( entry . getKey (  )  ,    entry . getValue (  )  . getRenewDate (  )  )  ;", "}", "return   allTokens ;", "}", "METHOD_END"], "methodName": ["getAllTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "return   delegationTokenSequenceNumber ;", "}", "METHOD_END"], "methodName": ["getLatestDTSequenceNumber"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "DelegationTokenInformation   info    =    currentTokens . get ( ident )  ;", "if    ( info    =  =    null )     {", "throw   new   security . token . SecretManager . InvalidToken (  (  (  \" token    (  \"     +     ( ident . toString (  )  )  )     +     \"  )    can ' t   be   found   in   cache \"  )  )  ;", "}", "return   info . getRenewDate (  )  ;", "}", "METHOD_END"], "methodName": ["getRenewDate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "Configuration   confWithS =    new   Configuration (  )  ;", "confWithSset ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {    conf    }  ,    new   Object [  ]  {    confWithS }     }  )  ;", "}", "METHOD_END"], "methodName": ["configs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens"}, {"methodBody": ["METHOD_START", "{", "return   currentUser . doAs ( new   PrivilegedAction < ApplicationMasterProtocol >  (  )     {", "@ Override", "public   ApplicationMasterProtocol   run (  )     {", "return    (  ( ApplicationMasterProtocol )     ( rpc . getProxy ( ApplicationMasterProtocol . class ,    rm . getApplicationMasterService (  )  . getBindAddress (  )  ,    conf )  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["createRMClient"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens"}, {"methodBody": ["METHOD_START", "{", "MockRM   rm    =    new   MockRM ( conf )     {", "@ Override", "protected   void   doSecureLogin (  )    throws   IOException    {", "}", "}  ;", "rm . start (  )  ;", "MockNM   nm    =    rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  0  0  0  )  ;", "RMApp   app    =    rm . submitApp (  2  0  0  )  ;", "MockAM   am    =    MockRM . launchAndRegisterAM ( app ,    rm ,    nm )  ;", "AllocateResponse   response    =    am . allocate ( Records . newRecord ( AllocateRequest . class )  )  ;", "Assert . assertNull ( response . getAMRMToken (  )  )  ;", "rm . getRMContext (  )  . getAMRMTokenSecretManager (  )  . rollMasterKey (  )  ;", "response    =    am . allocate ( Records . newRecord ( AllocateRequest . class )  )  ;", "Assert . assertNotNull ( response . getAMRMToken (  )  )  ;", "Token < AMRMTokenIdentifier >    amrmToken    =    ConverterUtils . convertFromYarn ( response . getAMRMToken (  )  ,    new   io . Text ( response . getAMRMToken (  )  . getService (  )  )  )  ;", "Assert . assertEquals ( amrmToken . decodeIdentifier (  )  . getKeyId (  )  ,    rm . getRMContext (  )  . getAMRMTokenSecretManager (  )  . getMasterKey (  )  . getMasterKey (  )  . getKeyId (  )  )  ;", "response    =    am . allocate ( Records . newRecord ( AllocateRequest . class )  )  ;", "Assert . assertNull ( response . getAMRMToken (  )  )  ;", "rm . getRMContext (  )  . getAMRMTokenSecretManager (  )  . activateNextMasterKey (  )  ;", "response    =    am . allocate ( Records . newRecord ( AllocateRequest . class )  )  ;", "Assert . assertNull ( response . getAMRMToken (  )  )  ;", "rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAMRMMasterKeysUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens"}, {"methodBody": ["METHOD_START", "{", "conf . setLong ( RM _ AMRM _ TOKEN _ MASTER _ KEY _ ROLLING _ INTERVAL _ SECS ,    TestAMRMTokens . rolling _ interval _ sec )  ;", "conf . setLong ( RM _ AM _ EXPIRY _ INTERVAL _ MS ,    TestAMRMTokens . am _ expire _ ms )  ;", "TestAMAuthorization . MyContainerManager   containerManager    =    new   TestAMAuthorization . MyContainerManager (  )  ;", "final   TestAMAuthorization . MockRMWithAMS   rm    =    new   TestAMAuthorization . MockRMWithAMS ( conf ,    containerManager )  ;", "rm . start (  )  ;", "Long   startTime    =    System . currentTimeMillis (  )  ;", "final   Configuration   conf    =    rm . getConfig (  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "ApplicationMasterProtocol   rmClient    =    null ;", "AMRMTokenSecretManager   appTokenSecretManager    =    rm . getRMContext (  )  . getAMRMTokenSecretManager (  )  ;", "MasterKeyData   oldKey    =    appTokenSecretManager . getMasterKey (  )  ;", "Assert . assertNotNull ( oldKey )  ;", "try    {", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "int   waitCount    =     0  ;", "while    (  (  ( containerManager . containerTokens )     =  =    null )     &  &     (  ( waitCount +  +  )     <     ( TestAMRMTokens . maxWaitAttempts )  )  )     {", "TestAMRMTokens . LOG . info (  \" Waiting   for   AM   Launch   to   happen .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertNotNull ( containerManager . containerTokens )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    attempt . getAppAttemptId (  )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( applicationAttemptId . toString (  )  )  ;", "Credentials   credentials    =    containerManager . getContainerCredentials (  )  ;", "final   InetSocketAddress   rmBindAddress    =    rm . getApplicationMasterService (  )  . getBindAddress (  )  ;", "Token <  ?    extends   TokenIdentifier >    amRMToken    =    TestAMAuthorization . MockRMWithAMS . setupAndReturnAMRMToken ( rmBindAddress ,    credentials . getAllTokens (  )  )  ;", "currentUser . addToken ( amRMToken )  ;", "rmClient    =    createRMClient ( rm ,    conf ,    rpc ,    currentUser )  ;", "RegisterApplicationMasterRequest   request    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "rmClient . registerApplicationMaster ( request )  ;", "AllocateRequest   allocateRequest    =    Records . newRecord ( AllocateRequest . class )  ;", "Assert . assertTrue (  (  ( rmClient . allocate ( allocateRequest )  . getAMCommand (  )  )     =  =    null )  )  ;", "while    (  (  ( System . currentTimeMillis (  )  )     -    startTime )     <     (  ( TestAMRMTokens . rolling _ interval _ sec )     *     1  0  0  0  )  )     {", "rmClient . allocate ( allocateRequest )  ;", "Thread . sleep (  5  0  0  )  ;", "}", "MasterKeyData   newKey    =    appTokenSecretManager . getMasterKey (  )  ;", "Assert . assertNotNull ( newKey )  ;", "Assert . assertFalse (  \" Master   key   should   have   changed !  \"  ,    oldKey . equals ( newKey )  )  ;", "rpc . stopProxy ( rmClient ,    conf )  ;", "rmClient    =    createRMClient ( rm ,    conf ,    rpc ,    currentUser )  ;", "Assert . assertTrue (  (  ( rmClient . allocate ( allocateRequest )  . getAMCommand (  )  )     =  =    null )  )  ;", "waitCount    =     0  ;", "while    (  ( waitCount +  +  )     <  =     ( TestAMRMTokens . maxWaitAttempts )  )     {", "if    (  ( appTokenSecretManager . getCurrnetMasterKeyData (  )  )     !  =    oldKey )     {", "break ;", "}", "try    {", "rmClient . allocate ( allocateRequest )  ;", "}    catch    ( Exception   ex )     {", "break ;", "}", "Thread . sleep (  2  0  0  )  ;", "}", "Assert . assertTrue ( appTokenSecretManager . getCurrnetMasterKeyData (  )  . equals ( newKey )  )  ;", "Assert . assertTrue ( appTokenSecretManager . getMasterKey (  )  . equals ( newKey )  )  ;", "Assert . assertTrue (  (  ( appTokenSecretManager . getNextMasterKeyData (  )  )     =  =    null )  )  ;", "Token < AMRMTokenIdentifier >    newToken    =    appTokenSecretManager . createAndGetAMRMToken ( applicationAttemptId )  ;", "SecurityUtil . setTokenService ( newToken ,    rmBindAddress )  ;", "currentUser . addToken ( newToken )  ;", "rpc . stopProxy ( rmClient ,    conf )  ;", "rmClient    =    createRMClient ( rm ,    conf ,    rpc ,    currentUser )  ;", "allocateRequest    =    Records . newRecord ( AllocateRequest . class )  ;", "Assert . assertTrue (  (  ( rmClient . allocate ( allocateRequest )  . getAMCommand (  )  )     =  =    null )  )  ;", "rpc . stopProxy ( rmClient ,    conf )  ;", "try    {", "currentUser . addToken ( amRMToken )  ;", "rmClient    =    createRMClient ( rm ,    conf ,    rpc ,    currentUser )  ;", "allocateRequest    =    Records . newRecord ( AllocateRequest . class )  ;", "Assert . assertTrue (  (  ( rmClient . allocate ( allocateRequest )  . getAMCommand (  )  )     =  =    null )  )  ;", "Assert . fail (  \" The   old   Token   should   not   work \"  )  ;", "}    catch    ( Exception   ex )     {", "}", "}    finally    {", "rm . stop (  )  ;", "if    ( rmClient    !  =    null )     {", "rpc . stopProxy ( rmClient ,    conf )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testMasterKeyRollOver"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens"}, {"methodBody": ["METHOD_START", "{", "TestAMAuthorization . MyContainerManager   containerManager    =    new   TestAMAuthorization . MyContainerManager (  )  ;", "final   TestAMAuthorization . MockRMWithAMS   rm    =    new   TestAMAuthorization . MockRMWithAMS ( conf ,    containerManager )  ;", "rm . start (  )  ;", "final   Configuration   conf    =    rm . getConfig (  )  ;", "final   YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "ApplicationMasterProtocol   rmClient    =    null ;", "try    {", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "int   waitCount    =     0  ;", "while    (  (  ( containerManager . containerTokens )     =  =    null )     &  &     (  ( waitCount +  +  )     <     2  0  )  )     {", ". LOG . info (  \" Waiting   for   AM   Launch   to   happen .  .  \"  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "}", "Assert . assertNotNull ( containerManager . containerTokens )  ;", "RMAppAttempt   attempt    =    app . getCurrentAppAttempt (  )  ;", "ApplicationAttemptId   applicationAttemptId    =    attempt . getAppAttemptId (  )  ;", "UserGroupInformation   currentUser    =    UserGroupInformation . createRemoteUser ( applicationAttemptId . toString (  )  )  ;", "Credentials   credentials    =    containerManager . getContainerCredentials (  )  ;", "final   InetSocketAddress   rmBindAddress    =    rm . getApplicationMasterService (  )  . getBindAddress (  )  ;", "Token <  ?    extends   TokenIdentifier >    amRMToken    =    TestAMAuthorization . MockRMWithAMS . setupAndReturnAMRMToken ( rmBindAddress ,    credentials . getAllTokens (  )  )  ;", "currentUser . addToken ( amRMToken )  ;", "rmClient    =    createRMClient ( rm ,    conf ,    rpc ,    currentUser )  ;", "RegisterApplicationMasterRequest   request    =    Records . newRecord ( RegisterApplicationMasterRequest . class )  ;", "rmClient . registerApplicationMaster ( request )  ;", "FinishApplicationMasterRequest   finishAMRequest    =    Records . newRecord ( FinishApplicationMasterRequest . class )  ;", "finishAMRequest . setFinalApplicationStatus ( SUCCEEDED )  ;", "finishAMRequest . setDiagnostics (  \" diagnostics \"  )  ;", "finishAMRequest . setTrackingUrl (  \" url \"  )  ;", "rmClient . finishApplicationMaster ( finishAMRequest )  ;", "ContainerStatus   containerStatus    =    BuilderUtils . newContainerStatus ( attempt . getMasterContainer (  )  . getId (  )  ,    COMPLETE ,     \" AM   Container   Finished \"  ,     0  )  ;", "rm . getRMContext (  )  . getDispatcher (  )  . getEventHandler (  )  . handle ( new   RMAppAttemptContainerFinishedEvent ( applicationAttemptId ,    containerStatus )  )  ;", "int   count    =     0  ;", "while    (  (  ( attempt . getState (  )  )     !  =     ( RMAppAttemptState . FINISHED )  )     &  &     ( count    <     (  . maxWaitAttempts )  )  )     {", "Thread . sleep (  1  0  0  )  ;", "count +  +  ;", "}", "Assert . assertTrue (  (  ( attempt . getState (  )  )     =  =     ( RMAppAttemptState . FINISHED )  )  )  ;", "rpc . stopProxy ( rmClient ,    conf )  ;", "rmClient    =    createRMClient ( rm ,    conf ,    rpc ,    currentUser )  ;", "AllocateRequest   allocateRequest    =    Records . newRecord ( AllocateRequest . class )  ;", "try    {", "rmClient . allocate ( allocateRequest )  ;", "Assert . fail (  (  \" You   got   to   be   kidding   me !     \"     +     \" Using   App   tokens   after   app - finish   should   fail !  \"  )  )  ;", "}    catch    ( Throwable   t )     {", ". LOG . info (  \" Exception   found   is    \"  ,    t )  ;", "Assert . assertTrue ( t . getCause (  )  . getMessage (  )  . contains (  (  ( applicationAttemptId . toString (  )  )     +     \"    not   found   in   AMRMTokenSecretManager .  \"  )  )  )  ;", "}", "}    finally    {", "rm . stop (  )  ;", "if    ( rmClient    !  =    null )     {", "rpc . stopProxy ( rmClient ,    conf )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testTokenExpiry"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestAMRMTokens"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "ContainerManagementProtocol   containerManager    =    mock ( ContainerManagementProtocol . class )  ;", "StartContainersResponse   mockResponse    =    mock ( StartContainersResponse . class )  ;", "when ( containerManager . startContainers (  (  ( StartContainersRequest )     ( any (  )  )  )  )  )  . thenReturn ( mockResponse )  ;", "final   DrainDispatcher   dispatcher    =    new   DrainDispatcher (  )  ;", "MockRM   rm    =    new   MockRMWithCustomAMLauncher ( conf ,    containerManager )     {", "protected   ClientRMService   createClientRMService (  )     {", "return   new   ClientRMService ( this . rmContext ,    scheduler ,    this . rmAppManager ,    this . applicationACLsManager ,    this . queueACLsManager ,    getRMContext (  )  . getRMDelegationTokenSecretManager (  )  )  ;", "}", "@ Override", "protected   Dispatcher   createDispatcher (  )     {", "return   dispatcher ;", "}", "@ Override", "protected   void   doSecureLogin (  )    throws   IOException    {", "}", "}  ;", "rm . start (  )  ;", "RMApp   app    =    rm . submitApp (  1  0  2  4  )  ;", "MockNM   nm 1     =    rm . registerNode (  \" localhost :  1  2  3  4  \"  ,     3  0  7  2  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "dispatcher . await (  )  ;", "nm 1  . nodeHeartbeat ( true )  ;", "dispatcher . await (  )  ;", "ApplicationAttemptId   appAttempt    =    app . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ;", "final   MockAM   mockAM    =    new   MockAM ( rm . getRMContext (  )  ,    rm . getApplicationMasterService (  )  ,    app . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  ;", "UserGroupInformation   appUgi    =    UserGroupInformation . createRemoteUser ( appAttempt . toString (  )  )  ;", "RegisterApplicationMasterResponse   response    =    appUgi . doAs ( new   PrivilegedAction < RegisterApplicationMasterResponse >  (  )     {", "@ Override", "public   RegisterApplicationMasterResponse   run (  )     {", "RegisterApplicationMasterResponse   response    =    null ;", "try    {", "response    =    mockAM . registerAppAttempt (  )  ;", "}    catch    ( Exception   e )     {", "Assert . fail (  \" Exception   was   not   expected \"  )  ;", "}", "return   response ;", "}", "}  )  ;", "GetApplicationReportRequest   request    =    Records . newRecord ( GetApplicationReportRequest . class )  ;", "request . setApplicationId ( app . getApplicationId (  )  )  ;", "GetApplicationReportResponse   reportResponse    =    rm . getClientRMService (  )  . getApplicationReport ( request )  ;", "ApplicationReport   appReport    =    reportResponse . getApplicationReport (  )  ;", "Token   originalClientToAMToken    =    appReport . getClientToAMToken (  )  ;", "Assert . assertNotNull ( response . getClientToAMTokenMasterKey (  )  )  ;", "Assert . assertTrue (  (  ( response . getClientToAMTokenMasterKey (  )  . array (  )  . length )     >     0  )  )  ;", "ApplicationAttemptId   appAttemptId    =    app . getAppAttempts (  )  . keySet (  )  . iterator (  )  . next (  )  ;", "Assert . assertNotNull ( appAttemptId )  ;", "final    . CustomAM   am    =    new    . CustomAM ( appAttemptId ,    response . getClientToAMTokenMasterKey (  )  . array (  )  )  ;", "am . init ( conf )  ;", "am . start (  )  ;", "SecurityUtil . setSecurityInfoProviders ( new    . CustomSecurityInfo (  )  )  ;", "try    {", ". CustomProtocol   client    =     (  (  . CustomProtocol )     ( RPC . getProxy (  . CustomProtocol . class ,     1 L ,    am . address ,    conf )  )  )  ;", "client . ping (  )  ;", "fail (  \" Access   by   unauthenticated   user   should   fail !  !  \"  )  ;", "}    catch    ( Exception   e )     {", "Assert . assertFalse ( am . pinged )  ;", "}", "Token < ClientToAMTokenIdentifier >    token    =    ConverterUtils . convertFromYarn ( originalClientToAMToken ,    am . address )  ;", "verifyTokenWithTamperedID ( conf ,    am ,    token )  ;", "verifyTokenWithTamperedUserName ( conf ,    am ,    token )  ;", "verifyValidToken ( conf ,    am ,    token )  ;", "}", "METHOD_END"], "methodName": ["testClientToAMTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens"}, {"methodBody": ["METHOD_START", "{", "Token < ClientToAMTokenIdentifier >    maliciousToken    =    new   Token < ClientToAMTokenIdentifier >  ( maliciousID . getBytes (  )  ,    token . getPassword (  )  ,    token . getKind (  )  ,    token . getService (  )  )  ;", "ugi . addToken ( maliciousToken )  ;", "try    {", "ugi . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", "try    {", ". CustomProtocol   client    =     (  (  . CustomProtocol )     ( RPC . getProxy (  . CustomProtocol . class ,     1 L ,    am . address ,    conf )  )  )  ;", "client . ping (  )  ;", "fail (  (  \" Connection   initiation   with   illegally   modified    \"     +     \" tokens   is   expected   to   fail .  \"  )  )  ;", "return   null ;", "}    catch    ( YarnException   ex )     {", "fail (  (  \" Cannot   get   a   YARN   remote   exception   as    \"     +     \" it   will   indicate   RPC   success \"  )  )  ;", "throw   ex ;", "}", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "Assert . assertEquals ( RemoteException . class . getName (  )  ,    e . getClass (  )  . getName (  )  )  ;", "e    =     (  ( RemoteException )     ( e )  )  . unwrapRemoteException (  )  ;", "Assert . assertEquals ( SaslException . class . getCanonicalName (  )  ,    e . getClass (  )  . getCanonicalName (  )  )  ;", "Assert . assertTrue ( e . getMessage (  )  . contains (  (  \" DIGEST - MD 5  :    digest   response   format   violation .     \"     +     \" Mismatched   response .  \"  )  )  )  ;", "Assert . assertFalse ( am . pinged )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyTamperedToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser (  \" me \"  )  ;", "Identifier   maliciousID    =    new   Identifier ( BuilderUtils . newApplicationAttemptId ( BuilderUtils . newApplicationId ( am . appAttemptId . getApplicationId (  )  . getClusterTimestamp (  )  ,     4  2  )  ,     4  3  )  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  ;", "verifyTamperedToken ( conf ,    am ,    token ,    ugi ,    maliciousID )  ;", "}", "METHOD_END"], "methodName": ["verifyTokenWithTamperedID"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi    =    UserGroupInformation . createRemoteUser (  \" me \"  )  ;", "Identifier   maliciousID    =    new   Identifier ( am . appAttemptId ,     \" evilOrc \"  )  ;", "verifyTamperedToken ( conf ,    am ,    token ,    ugi ,    maliciousID )  ;", "}", "METHOD_END"], "methodName": ["verifyTokenWithTamperedUserName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   ugi ;", "ugi    =    UserGroupInformation . createRemoteUser (  \" me \"  )  ;", "ugi . addToken ( token )  ;", "ugi . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "@ Override", "public   Void   run (  )    throws   Exception    {", ". CustomProtocol   client    =     (  (  . CustomProtocol )     ( RPC . getProxy (  . CustomProtocol . class ,     1 L ,    am . address ,    conf )  )  )  ;", "client . ping (  )  ;", "Assert . assertTrue ( am . pinged )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["verifyValidToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestClientToAMTokens"}, {"methodBody": ["METHOD_START", "{", "return   new   DelegationTokenRenewer (  )     {", "@ Override", "protected   ThreadPoolExecutor   createNewThreadPoolService ( Configuration   conf )     {", "ThreadPoolExecutor   pool    =    new   ThreadPoolExecutor (  5  ,     5  ,     3 L ,    TimeUnit . SECONDS ,    new   LinkedBlockingQueue < Runnable >  (  )  )     {", "@ Override", "protected   void   afterExecute ( Runnable   r ,    Throwable   t )     {", "counter . decrementAndGet (  )  ;", "super . afterExecute ( r ,    t )  ;", "}", "@ Override", "public   void   execute ( Runnable   command )     {", "counter . incrementAndGet (  )  ;", "super . execute ( command )  ;", "}", "}  ;", "return   pool ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createNewDelegationTokenRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Text   user 1     =    new   Text (  \" user 1  \"  )  ;", ". MyDelegationTokenSecretManager   sm    =    new    . MyDelegationTokenSecretManager ( DFSConfigKeys . DFS _ NAMENODE _ DELEGATION _ KEY _ UPDATE _ INTERVAL _ DEFAULT ,    DFSConfigKeys . DFS _ NAMENODE _ DELEGATION _ KEY _ UPDATE _ INTERVAL _ DEFAULT ,    DFSConfigKeys . DFS _ NAMENODE _ DELEGATION _ TOKEN _ MAX _ LIFETIME _ DEFAULT ,     3  6  0  0  0  0  0  ,    null )  ;", "sm . startThreads (  )  ;", "DelegationTokenIdentifier   dtId 1     =    new   DelegationTokenIdentifier ( user 1  ,    renewer ,    user 1  )  ;", ". MyToken   token 1     =    new    . MyToken ( dtId 1  ,    sm )  ;", "token 1  . setService ( new   Text (  \" localhost :  0  \"  )  )  ;", "return   token 1  ;", "}", "METHOD_END"], "methodName": ["createTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "TestDelegationTokenRenewer . counter    =    new   AtomicInteger (  0  )  ;", "TestDelegationTokenRenewer . conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( TestDelegationTokenRenewer . conf )  ;", "TestDelegationTokenRenewer . eventQueue    =    new   LinkedBlockingQueue < Event >  (  )  ;", "TestDelegationTokenRenewer . dispatcher    =    new   AsyncDispatcher ( TestDelegationTokenRenewer . eventQueue )  ;", "TestDelegationTokenRenewer . Renewer . reset (  )  ;", "delegationTokenRenewer    =    createNewDelegationTokenRenewer ( TestDelegationTokenRenewer . conf ,    TestDelegationTokenRenewer . counter )  ;", "RMContext   mockContext    =    mock ( RMContext . class )  ;", "ClientRMService   mockClientRMService    =    mock ( ClientRMService . class )  ;", "when ( mockContext . getDelegationTokenRenewer (  )  )  . thenReturn ( delegationTokenRenewer )  ;", "when ( mockContext . getDispatcher (  )  )  . thenReturn ( TestDelegationTokenRenewer . dispatcher )  ;", "when ( mockContext . getClientRMService (  )  )  . thenReturn ( mockClientRMService )  ;", "InetSocketAddress   sockAddr    =    InetSocketAddress . createUnresolved (  \" localhost \"  ,     1  2  3  4  )  ;", "when ( mockClientRMService . getBindAddress (  )  )  . thenReturn ( sockAddr )  ;", "delegationTokenRenewer . setRMContext ( mockContext )  ;", "delegationTokenRenewer . init ( TestDelegationTokenRenewer . conf )  ;", "delegationTokenRenewer . start (  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "TestDelegationTokenRenewer . conf    =    new   Configuration (  )  ;", "URI   uri    =    new   URI (  (  ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / localhost :  0  \"  )  )  ;", "System . out . println (  (  \" scheme   is    :     \"     +     ( uri . getScheme (  )  )  )  )  ;", "TestDelegationTokenRenewer . conf . setClass (  (  (  \" fs .  \"     +     ( uri . getScheme (  )  )  )     +     \"  . impl \"  )  ,    TestDelegationTokenRenewer . MyFS . class ,    DistributedFileSystem . class )  ;", "FileSystem . setDefaultUri ( TestDelegationTokenRenewer . conf ,    uri )  ;", "TestDelegationTokenRenewer . LOG . info (  (  \" filesystem   uri    =     \"     +     ( FileSystem . getDefaultUri ( TestDelegationTokenRenewer . conf )  . toString (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["setUpClass"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "delegationTokenRenewer . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "TestDelegationTokenRenewer . MyFS   dfs    =     (  ( TestDelegationTokenRenewer . MyFS )     ( FileSystem . get ( TestDelegationTokenRenewer . conf )  )  )  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" dfs =  \"     +     (  ( Object )     ( dfs . hashCode (  )  )  )  )     +     \"  ; conf =  \"  )     +     ( TestDelegationTokenRenewer . conf . hashCode (  )  )  )  )  ;", "TestDelegationTokenRenewer . MyToken   token    =    dfs . getDelegationToken (  \" user 1  \"  )  ;", "token . cancelToken (  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "ts . addToken ( token . getKind (  )  ,    token )  ;", "ApplicationId   appId    =    BuilderUtils . newApplicationId (  0  ,     0  )  ;", "delegationTokenRenewer . addApplicationAsync ( appId ,    ts ,    true )  ;", "int   waitCnt    =     2  0  ;", "while    (  ( waitCnt -  -  )     >     0  )     {", "if    (  !  ( TestDelegationTokenRenewer . eventQueue . isEmpty (  )  )  )     {", "Event   evt    =    TestDelegationTokenRenewer . eventQueue . take (  )  ;", "if    (  ( evt . getType (  )  )     =  =     ( RMAppEventType . APP _ REJECTED )  )     {", "Assert . assertTrue (  (  ( RMAppEvent )     ( evt )  )  . getApplicationId (  )  . equals ( appId )  )  ;", "return ;", "}", "} else    {", "Thread . sleep (  5  0  0  )  ;", "}", "}", "fail (  \" App   submission   with   a   cancelled   token   should   have   failed \"  )  ;", "}", "METHOD_END"], "methodName": ["testAppRejectionWithCancelledDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "MockRM   rm    =    new   MockRM ( conf )  ;", "ByteBuffer   tokens    =    ByteBuffer . wrap (  \" BOGUS \"  . getBytes (  )  )  ;", "ContainerLaunchContext   amContainer    =    ContainerLaunchContext . newInstance ( new   HashMap < String ,    api . records . LocalResource >  (  )  ,    new   HashMap < String ,    String >  (  )  ,    new   ArrayList < String >  (  )  ,    new   HashMap < String ,    ByteBuffer >  (  )  ,    tokens ,    new   HashMap < api . records . ApplicationAccessType ,    String >  (  )  )  ;", "ApplicationSubmissionContext   appSubContext    =    ApplicationSubmissionContext . newInstance ( ApplicationId . newInstance (  1  2  3  4  1  2  1  ,     0  )  ,     \" BOGUS \"  ,     \" default \"  ,    UNDEFINED ,    amContainer ,    false ,    true ,     1  ,    Resource . newInstance (  1  0  2  4  ,     1  )  ,     \" BOGUS \"  )  ;", "SubmitApplicationRequest   request    =    SubmitApplicationRequest . newInstance ( appSubContext )  ;", "try    {", "rm . getClientRMService (  )  . submitApplication ( request )  ;", "fail (  \" Error   was   excepted .  \"  )  ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" Bad   header   found   in   token   storage \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppSubmissionWithInvalidDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "final   CyclicBarrier   startBarrier    =    new   CyclicBarrier (  2  )  ;", "final   CyclicBarrier   endBarrier    =    new   CyclicBarrier (  2  )  ;", "final   Credentials   creds 1     =    new   Credentials (  )  ;", "final   Token <  ?  >    token 1     =    mock ( Token . class )  ;", "creds 1  . addToken ( new   Text (  \" token \"  )  ,    token 1  )  ;", "doReturn ( true )  . when ( token 1  )  . isManaged (  )  ;", "doAnswer ( new   Answer < Long >  (  )     {", "public   Long   answer ( InvocationOnMock   invocation )    throws   InterruptedException ,    BrokenBarrierException    {", "startBarrier . await (  )  ;", "endBarrier . await (  )  ;", "return   Long . MAX _ VALUE ;", "}", "}  )  . when ( token 1  )  . renew ( any ( Configuration . class )  )  ;", "final   Credentials   creds 2     =    new   Credentials (  )  ;", "final   Token <  ?  >    token 2     =    mock ( Token . class )  ;", "creds 2  . addToken ( new   Text (  \" token \"  )  ,    token 2  )  ;", "doReturn ( true )  . when ( token 2  )  . isManaged (  )  ;", "doReturn ( Long . MAX _ VALUE )  . when ( token 2  )  . renew ( any ( Configuration . class )  )  ;", "final   DelegationTokenRenewer   dtr    =    createNewDelegationTokenRenewer (  . conf ,     . counter )  ;", "RMContext   mockContext    =    mock ( RMContext . class )  ;", "ClientRMService   mockClientRMService    =    mock ( ClientRMService . class )  ;", "when ( mockContext . getClientRMService (  )  )  . thenReturn ( mockClientRMService )  ;", "InetSocketAddress   sockAddr    =    InetSocketAddress . createUnresolved (  \" localhost \"  ,     1  2  3  4  )  ;", "when ( mockClientRMService . getBindAddress (  )  )  . thenReturn ( sockAddr )  ;", "dtr . setRMContext ( mockContext )  ;", "when ( mockContext . getDelegationTokenRenewer (  )  )  . thenReturn ( dtr )  ;", "dtr . init (  . conf )  ;", "dtr . start (  )  ;", "Thread   submitThread    =    new   Thread (  )     {", "@ Override", "public   void   run (  )     {", "dtr . addApplicationAsync ( mock ( ApplicationId . class )  ,    creds 1  ,    false )  ;", "}", "}  ;", "submitThread . start (  )  ;", "startBarrier . await (  )  ;", "dtr . addApplicationAsync ( mock ( ApplicationId . class )  ,    creds 2  ,    false )  ;", "endBarrier . await (  )  ;", "submitThread . join (  )  ;", "}", "METHOD_END"], "methodName": ["testConcurrentAddApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Configuration   lconf    =    new   Configuration ( TestDelegationTokenRenewer . conf )  ;", "lconf . setBoolean ( LOG _ AGGREGATION _ ENABLED ,    true )  ;", "lconf . setLong ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,     6  0  0  0 L )  ;", "lconf . setLong ( RM _ DELAYED _ DELEGATION _ TOKEN _ REMOVAL _ INTERVAL _ MS ,     1  0  0  0 L )  ;", "DelegationTokenRenewer   localDtr    =    createNewDelegationTokenRenewer ( lconf ,    TestDelegationTokenRenewer . counter )  ;", "RMContext   mockContext    =    mock ( RMContext . class )  ;", "ClientRMService   mockClientRMService    =    mock ( ClientRMService . class )  ;", "when ( mockContext . getClientRMService (  )  )  . thenReturn ( mockClientRMService )  ;", "when ( mockContext . getDelegationTokenRenewer (  )  )  . thenReturn ( localDtr )  ;", "when ( mockContext . getDispatcher (  )  )  . thenReturn ( TestDelegationTokenRenewer . dispatcher )  ;", "InetSocketAddress   sockAddr    =    InetSocketAddress . createUnresolved (  \" localhost \"  ,     1  2  3  4  )  ;", "when ( mockClientRMService . getBindAddress (  )  )  . thenReturn ( sockAddr )  ;", "localDtr . setRMContext ( mockContext )  ;", "localDtr . init ( lconf )  ;", "localDtr . start (  )  ;", "TestDelegationTokenRenewer . MyFS   dfs    =     (  ( TestDelegationTokenRenewer . MyFS )     ( FileSystem . get ( lconf )  )  )  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" dfs =  \"     +     (  ( Object )     ( dfs . hashCode (  )  )  )  )     +     \"  ; conf =  \"  )     +     ( lconf . hashCode (  )  )  )  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "TestDelegationTokenRenewer . MyToken   token 1     =    dfs . getDelegationToken (  \" user 1  \"  )  ;", "String   nn 1     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 1  :  0  \"  ;", "ts . addToken ( new   Text ( nn 1  )  ,    token 1  )  ;", "ApplicationId   applicationId _  0     =    BuilderUtils . newApplicationId (  0  ,     0  )  ;", "localDtr . addApplicationAsync ( applicationId _  0  ,    ts ,    true )  ;", "waitForEventsToGetProcessed ( localDtr )  ;", "if    (  !  ( TestDelegationTokenRenewer . eventQueue . isEmpty (  )  )  )     {", "Event   evt    =    TestDelegationTokenRenewer . eventQueue . take (  )  ;", "if    ( evt   instanceof   RMAppEvent )     {", "Assert . assertEquals (  (  ( RMAppEvent )     ( evt )  )  . getType (  )  ,    RMAppEventType . START )  ;", "} else    {", "fail (  \" RMAppEvent . START   was   expected !  !  \"  )  ;", "}", "}", "localDtr . applicationFinished ( applicationId _  0  )  ;", "waitForEventsToGetProcessed ( localDtr )  ;", "token 1  . renew ( lconf )  ;", "Thread . sleep (  1  0  0  0  0 L )  ;", "try    {", "token 1  . renew ( lconf )  ;", "fail (  \" Renewal   of   cancelled   token   should   have   failed \"  )  ;", "}    catch    ( InvalidToken   ite )     {", "}", "}", "METHOD_END"], "methodName": ["testDTKeepAlive1"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Configuration   lconf    =    new   Configuration ( TestDelegationTokenRenewer . conf )  ;", "lconf . setBoolean ( LOG _ AGGREGATION _ ENABLED ,    true )  ;", "lconf . setLong ( RM _ NM _ EXPIRY _ INTERVAL _ MS ,     6  0  0  0 L )  ;", "lconf . setLong ( RM _ DELAYED _ DELEGATION _ TOKEN _ REMOVAL _ INTERVAL _ MS ,     1  0  0  0 L )  ;", "DelegationTokenRenewer   localDtr    =    createNewDelegationTokenRenewer ( TestDelegationTokenRenewer . conf ,    TestDelegationTokenRenewer . counter )  ;", "RMContext   mockContext    =    mock ( RMContext . class )  ;", "ClientRMService   mockClientRMService    =    mock ( ClientRMService . class )  ;", "when ( mockContext . getClientRMService (  )  )  . thenReturn ( mockClientRMService )  ;", "when ( mockContext . getDelegationTokenRenewer (  )  )  . thenReturn ( localDtr )  ;", "when ( mockContext . getDispatcher (  )  )  . thenReturn ( TestDelegationTokenRenewer . dispatcher )  ;", "InetSocketAddress   sockAddr    =    InetSocketAddress . createUnresolved (  \" localhost \"  ,     1  2  3  4  )  ;", "when ( mockClientRMService . getBindAddress (  )  )  . thenReturn ( sockAddr )  ;", "localDtr . setRMContext ( mockContext )  ;", "localDtr . init ( lconf )  ;", "localDtr . start (  )  ;", "TestDelegationTokenRenewer . MyFS   dfs    =     (  ( TestDelegationTokenRenewer . MyFS )     ( FileSystem . get ( lconf )  )  )  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" dfs =  \"     +     (  ( Object )     ( dfs . hashCode (  )  )  )  )     +     \"  ; conf =  \"  )     +     ( lconf . hashCode (  )  )  )  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "TestDelegationTokenRenewer . MyToken   token 1     =    dfs . getDelegationToken (  \" user 1  \"  )  ;", "String   nn 1     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 1  :  0  \"  ;", "ts . addToken ( new   Text ( nn 1  )  ,    token 1  )  ;", "ApplicationId   applicationId _  0     =    BuilderUtils . newApplicationId (  0  ,     0  )  ;", "localDtr . addApplicationAsync ( applicationId _  0  ,    ts ,    true )  ;", "localDtr . applicationFinished ( applicationId _  0  )  ;", "waitForEventsToGetProcessed ( delegationTokenRenewer )  ;", "localDtr . updateKeepAliveApplications ( Collections . singletonList ( applicationId _  0  )  )  ;", "token 1  . renew ( lconf )  ;", "Thread . sleep (  4  5  0  0 L )  ;", "token 1  . renew ( lconf )  ;", "Thread . sleep (  3  0  0  0 L )  ;", "try    {", "token 1  . renew ( lconf )  ;", "fail (  \" Renewal   of   cancelled   token   should   have   failed \"  )  ;", "}    catch    ( InvalidToken   ite )     {", "}", "}", "METHOD_END"], "methodName": ["testDTKeepAlive2"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "TestDelegationTokenRenewer . MyFS   dfs    =     (  ( TestDelegationTokenRenewer . MyFS )     ( FileSystem . get ( TestDelegationTokenRenewer . conf )  )  )  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" dfs =  \"     +     (  ( Object )     ( dfs . hashCode (  )  )  )  )     +     \"  ; conf =  \"  )     +     ( TestDelegationTokenRenewer . conf . hashCode (  )  )  )  )  ;", "TestDelegationTokenRenewer . MyToken   token 1  ;", "TestDelegationTokenRenewer . MyToken   token 2  ;", "TestDelegationTokenRenewer . MyToken   token 3  ;", "token 1     =    dfs . getDelegationToken (  \" user 1  \"  )  ;", "token 2     =    dfs . getDelegationToken (  \" user 2  \"  )  ;", "token 3     =    dfs . getDelegationToken (  \" user 3  \"  )  ;", "TestDelegationTokenRenewer . Renewer . tokenToRenewIn 2 Sec    =    token 1  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  \" token =  \"     +    token 1  )     +     \"    should   be   renewed   for    2    secs \"  )  )  ;", "String   nn 1     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 1  :  0  \"  ;", "String   nn 2     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 2  :  0  \"  ;", "String   nn 3     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 3  :  0  \"  ;", "Credentials   ts    =    new   Credentials (  )  ;", "ts . addToken ( new   Text ( nn 1  )  ,    token 1  )  ;", "ts . addToken ( new   Text ( nn 2  )  ,    token 2  )  ;", "ts . addToken ( new   Text ( nn 3  )  ,    token 3  )  ;", "ApplicationId   applicationId _  0     =    BuilderUtils . newApplicationId (  0  ,     0  )  ;", "delegationTokenRenewer . addApplicationAsync ( applicationId _  0  ,    ts ,    true )  ;", "waitForEventsToGetProcessed ( delegationTokenRenewer )  ;", "int   numberOfExpectedRenewals    =     3     +     1  ;", "int   attempts    =     1  0  ;", "while    (  ( attempts -  -  )     >     0  )     {", "try    {", "Thread . sleep (  (  3     *     1  0  0  0  )  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "if    (  ( TestDelegationTokenRenewer . Renewer . counter )     =  =    numberOfExpectedRenewals )", "break ;", "}", "TestDelegationTokenRenewer . LOG . info (  (  (  (  (  (  \" dfs =  \"     +     ( dfs . hashCode (  )  )  )     +     \"  ; Counter    =     \"  )     +     ( TestDelegationTokenRenewer . Renewer . counter )  )     +     \"  ; t =  \"  )     +     ( TestDelegationTokenRenewer . Renewer . lastRenewed )  )  )  ;", "assertEquals (  \" renew   wasn ' t   called   as   many   times   as   expected (  4  )  :  \"  ,    numberOfExpectedRenewals ,    TestDelegationTokenRenewer . Renewer . counter )  ;", "assertEquals (  \" most   recently   renewed   token   mismatch \"  ,    TestDelegationTokenRenewer . Renewer . lastRenewed ,    token 1  )  ;", "ts    =    new   Credentials (  )  ;", "TestDelegationTokenRenewer . MyToken   token 4     =    dfs . getDelegationToken (  \" user 4  \"  )  ;", "TestDelegationTokenRenewer . Renewer . tokenToRenewIn 2 Sec    =    token 4  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  \" token =  \"     +    token 4  )     +     \"    should   be   renewed   for    2    secs \"  )  )  ;", "String   nn 4     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 4  :  0  \"  ;", "ts . addToken ( new   Text ( nn 4  )  ,    token 4  )  ;", "ApplicationId   applicationId _  1     =    BuilderUtils . newApplicationId (  0  ,     1  )  ;", "delegationTokenRenewer . addApplicationAsync ( applicationId _  1  ,    ts ,    true )  ;", "waitForEventsToGetProcessed ( delegationTokenRenewer )  ;", "delegationTokenRenewer . applicationFinished ( applicationId _  1  )  ;", "waitForEventsToGetProcessed ( delegationTokenRenewer )  ;", "numberOfExpectedRenewals    =    TestDelegationTokenRenewer . Renewer . counter ;", "try    {", "Thread . sleep (  (  6     *     1  0  0  0  )  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" Counter    =     \"     +     ( TestDelegationTokenRenewer . Renewer . counter )  )     +     \"  ; t =  \"  )     +     ( TestDelegationTokenRenewer . Renewer . lastRenewed )  )  )  ;", "assertEquals (  \" renew   wasn ' t   called   as   many   times   as   expected \"  ,    numberOfExpectedRenewals ,    TestDelegationTokenRenewer . Renewer . counter )  ;", "try    {", "token 4  . renew ( TestDelegationTokenRenewer . conf )  ;", "fail (  \" Renewal   of   cancelled   token   should   have   failed \"  )  ;", "}    catch    ( InvalidToken   ite )     {", "}", "}", "METHOD_END"], "methodName": ["testDTRenewal"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "TestDelegationTokenRenewer . MyFS   dfs    =     (  ( TestDelegationTokenRenewer . MyFS )     ( FileSystem . get ( TestDelegationTokenRenewer . conf )  )  )  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" dfs =  \"     +     (  ( Object )     ( dfs . hashCode (  )  )  )  )     +     \"  ; conf =  \"  )     +     ( TestDelegationTokenRenewer . conf . hashCode (  )  )  )  )  ;", "Credentials   ts    =    new   Credentials (  )  ;", "TestDelegationTokenRenewer . MyToken   token 1     =    dfs . getDelegationToken (  \" user 1  \"  )  ;", "TestDelegationTokenRenewer . Renewer . tokenToRenewIn 2 Sec    =    token 1  ;", "TestDelegationTokenRenewer . LOG . info (  (  (  \" token =  \"     +    token 1  )     +     \"    should   be   renewed   for    2    secs \"  )  )  ;", "String   nn 1     =     ( DelegationTokenRenewer . SCHEME )     +     \"  :  /  / host 1  :  0  \"  ;", "ts . addToken ( new   Text ( nn 1  )  ,    token 1  )  ;", "ApplicationId   applicationId _  1     =    BuilderUtils . newApplicationId (  0  ,     1  )  ;", "delegationTokenRenewer . addApplicationAsync ( applicationId _  1  ,    ts ,    false )  ;", "waitForEventsToGetProcessed ( delegationTokenRenewer )  ;", "delegationTokenRenewer . applicationFinished ( applicationId _  1  )  ;", "waitForEventsToGetProcessed ( delegationTokenRenewer )  ;", "int   numberOfExpectedRenewals    =    TestDelegationTokenRenewer . Renewer . counter ;", "try    {", "Thread . sleep (  (  6     *     1  0  0  0  )  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "TestDelegationTokenRenewer . LOG . info (  (  (  (  \" Counter    =     \"     +     ( TestDelegationTokenRenewer . Renewer . counter )  )     +     \"  ; t =  \"  )     +     ( TestDelegationTokenRenewer . Renewer . lastRenewed )  )  )  ;", "assertEquals (  \" renew   wasn ' t   called   as   many   times   as   expected \"  ,    numberOfExpectedRenewals ,    TestDelegationTokenRenewer . Renewer . counter )  ;", "token 1  . renew ( TestDelegationTokenRenewer . conf )  ;", "}", "METHOD_END"], "methodName": ["testDTRenewalWithNoCancel"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "final   Credentials   credsx    =    new   Credentials (  )  ;", "final   Token <  ?  >    tokenx    =    mock ( Token . class )  ;", "credsx . addToken ( new   Text (  \" token \"  )  ,    tokenx )  ;", "doReturn ( true )  . when ( tokenx )  . isManaged (  )  ;", "doThrow ( new   IOException (  \" boom \"  )  )  . when ( tokenx )  . renew ( any ( Configuration . class )  )  ;", "final   DelegationTokenRenewer   dtr    =    createNewDelegationTokenRenewer (  . conf ,     . counter )  ;", "RMContext   mockContext    =    mock ( RMContext . class )  ;", "ClientRMService   mockClientRMService    =    mock ( ClientRMService . class )  ;", "when ( mockContext . getClientRMService (  )  )  . thenReturn ( mockClientRMService )  ;", "InetSocketAddress   sockAddr    =    InetSocketAddress . createUnresolved (  \" localhost \"  ,     1  2  3  4  )  ;", "when ( mockClientRMService . getBindAddress (  )  )  . thenReturn ( sockAddr )  ;", "dtr . setRMContext ( mockContext )  ;", "when ( mockContext . getDelegationTokenRenewer (  )  )  . thenReturn ( dtr )  ;", "dtr . init (  . conf )  ;", "dtr . start (  )  ;", "try    {", "dtr . addApplicationSync ( mock ( ApplicationId . class )  ,    credsx ,    false )  ;", "fail (  \" Catch   IOException   on   app   submission \"  )  ;", "}    catch    ( IOException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains ( tokenx . toString (  )  )  )  ;", "Assert . assertTrue ( e . getCause (  )  . toString (  )  . contains (  \" boom \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDTRonAppSubmission"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "int   wait    =     4  0  ;", "while    (  (  ( wait -  -  )     >     0  )     &  &     (  (  . counter . get (  )  )     >     0  )  )     {", "Thread . sleep (  2  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["waitForEventsToGetProcessed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "delegationTokenRenewer    =    new    (  )  ;", "RMContext   mockContext    =    mock ( RMContext . class )  ;", "ClientRMService   mockClientRMService    =    mock ( ClientRMService . class )  ;", "when ( mockContext . getClientRMService (  )  )  . thenReturn ( mockClientRMService )  ;", "delegationTokenRenewer . setRMContext ( mockContext )  ;", "delegationTokenRenewer . init ( conf )  ;", "delegationTokenRenewer . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testStartupFailure"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewerLifecycle"}, {"methodBody": ["METHOD_START", "{", "Logger   rootLogger    =    LogManager . getRootLogger (  )  ;", "rootLogger . setLevel ( Level . DEBUG )  ;", "ExitUtil . disableSystemExit (  )  ;", "conf    =    new   YarnConfiguration (  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "conf . set ( RM _ STORE ,    MemoryRMStateStore . class . getName (  )  )  ;", "conf . set ( RM _ SCHEDULER ,    FairScheduler . class . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Map < RMDelegationTokenIdentifier ,    Long >    rmDTState    =    rmState . getRMDTSecretManagerState (  )  . getTokenState (  )  ;", "Set < DelegationKey >    rmDTMasterKeyState    =    rmState . getRMDTSecretManagerState (  )  . getMasterKeyState (  )  ;", "MockRM   rm 1     =    new    . MyMockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "RMDelegationTokenSecretManager   dtSecretManager    =    rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  ;", "Assert . assertEquals ( dtSecretManager . getAllMasterKeys (  )  ,    rmDTMasterKeyState )  ;", "Set < DelegationKey >    expiringKeys    =    new   HashSet < DelegationKey >  (  )  ;", "expiringKeys . addAll ( dtSecretManager . getAllMasterKeys (  )  )  ;", "GetDelegationTokenRequest   request    =    mock ( GetDelegationTokenRequest . class )  ;", "when ( request . getRenewer (  )  )  . thenReturn (  \" renewer 1  \"  )  ;", "GetDelegationTokenResponse   response    =    rm 1  . getClientRMService (  )  . getDelegationToken ( request )  ;", "Token   delegationToken    =    response . getRMDelegationToken (  )  ;", "Token < RMDelegationTokenIdentifier >    token 1     =    ConverterUtils . convertFromYarn ( delegationToken ,     (  ( Text )     ( null )  )  )  ;", "RMDelegationTokenIdentifier   dtId 1     =    token 1  . decodeIdentifier (  )  ;", "while    (  (  (  (  . TestRMDelegationTokenSecretManager )     ( dtSecretManager )  )  . numUpdatedKeys . get (  )  )     <     3  )     {", "(  (  . TestRMDelegationTokenSecretManager )     ( dtSecretManager )  )  . checkCurrentKeyInStateStore ( rmDTMasterKeyState )  ;", "Thread . sleep (  1  0  0  )  ;", "}", "int   count    =     0  ;", "while    (  ( rmDTState . containsKey ( dtId 1  )  )     &  &     ( count    <     1  0  0  )  )     {", "Thread . sleep (  1  0  0  )  ;", "count +  +  ;", "}", "rm 1  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRMDTMasterKeyStateOnRollingMasterKey"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "MemoryRMStateStore   memStore    =    new   MemoryRMStateStore (  )  ;", "memStore . init ( conf )  ;", "RMStateStore . RMState   rmState    =    memStore . getState (  )  ;", "Set < DelegationKey >    rmDTMasterKeyState    =    rmState . getRMDTSecretManagerState (  )  . getMasterKeyState (  )  ;", "MockRM   rm 1     =    new    . MyMockRM ( conf ,    memStore )  ;", "rm 1  . start (  )  ;", "RMDelegationTokenSecretManager   dtSecretManager    =    rm 1  . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  ;", "Assert . assertEquals ( dtSecretManager . getAllMasterKeys (  )  ,    rmDTMasterKeyState )  ;", "Set < DelegationKey >    expiringKeys    =    new   HashSet < DelegationKey >  (  )  ;", "expiringKeys . addAll ( dtSecretManager . getAllMasterKeys (  )  )  ;", "while    ( true )     {", "boolean   allExpired    =    true ;", "for    ( DelegationKey   key    :    expiringKeys )     {", "if    ( rmDTMasterKeyState . contains ( key )  )     {", "allExpired    =    false ;", "}", "}", "if    ( allExpired )", "break ;", "Thread . sleep (  5  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["testRemoveExpiredMasterKeyInRMStateStore"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.TestRMDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "if    (  ( RMPolicyProvider . rmPolicyProvider )     =  =    null )     {", "synchronized ( RMPolicyProvider . class )     {", "if    (  ( RMPolicyProvider . rmPolicyProvider )     =  =    null )     {", "RMPolicyProvider . rmPolicyProvider    =    new   RMPolicyProvider (  )  ;", "}", "}", "}", "return   RMPolicyProvider . rmPolicyProvider ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.security.authorize.RMPolicyProvider"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" left :  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["left"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \"  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["percent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" width :  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["width"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.CapacitySchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" left :  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["left"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \"  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["percent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" width :  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["width"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.DefaultSchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" left :  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["left"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \"  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["percent"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \" width :  %  .  1 f %  %  \"  ,     ( f    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["width"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerPage"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   b    =    tableInit (  )  . append (  \"  ,    aoColumnDefs :     [  \"  )  ;", "b . append (  \"  {  ' bSearchable '  :    false ,     ' aTargets '  :     [     6     ]  }  \"  )  ;", "b . append (  (  \"  ,     {  ' sType '  :     ' title - numeric '  ,     ' bSearchable '  :    false ,     \"     +     \"  ' aTargets '  :     [     7  ,     8     ]     }  \"  )  )  ;", "b . append (  \"  ,     {  ' sType '  :     ' title - numeric '  ,     ' aTargets '  :     [     4     ]  }  \"  )  ;", "b . append (  \"  ]  }  \"  )  ;", "return   b . toString (  )  ;", "}", "METHOD_END"], "methodName": ["nodesTableInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.NodesPage"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   yarnConf    =    new   YarnConfiguration ( rm . getConfig (  )  )  ;", "String   activeRMHAId    =    RMHAUtils . findActiveRMHAId ( yarnConf )  ;", "String   path    =     \"  \"  ;", "if    ( activeRMHAId    !  =    null )     {", "yarnConf . set ( RM _ HA _ ID ,    activeRMHAId )  ;", "InetSocketAddress   sock    =     ( YarnConfiguration . useHttps ( yarnConf )  )     ?    yarnConf . getSocketAddr ( RM _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ HTTPS _ PORT )     :    yarnConf . getSocketAddr ( RM _ WEBAPP _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ PORT )  ;", "path    =     (  ( sock . getHostName (  )  )     +     \"  :  \"  )     +     ( Integer . toString ( sock . getPort (  )  )  )  ;", "path    =     ( YarnConfiguration . useHttps ( yarnConf )  )     ?     \" https :  /  /  \"     +    path    :     \" http :  /  /  \"     +    path ;", "}", "return   path ;", "}", "METHOD_END"], "methodName": ["buildRedirectPath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp"}, {"methodBody": ["METHOD_START", "{", "standby    =     ( rm . getRMContext (  )  . getHAServiceState (  )  )     =  =     ( HAServiceState . STANDBY )  ;", "}", "METHOD_END"], "methodName": ["checkIfStandbyRM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp"}, {"methodBody": ["METHOD_START", "{", "return   standby ;", "}", "METHOD_END"], "methodName": ["isStandby"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp"}, {"methodBody": ["METHOD_START", "{", "return    (  (  !  ( uri . equals (  (  (  \"  /  \"     +     ( rmWebApp . wsName (  )  )  )     +     \"  / v 1  / cluster / info \"  )  )  )  )     &  &     (  !  ( uri . equals (  (  (  \"  /  \"     +     ( rmWebApp . name (  )  )  )     +     \"  / cluster \"  )  )  )  )  )     &  &     (  !  ( RMWebAppFilter . NON _ REDIRECTED _ URIS . contains ( uri )  )  )  ;", "}", "METHOD_END"], "methodName": ["shouldRedirect"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter"}, {"methodBody": ["METHOD_START", "{", "Map < YarnApplicationState ,    Map < String ,    Long >  >    scoreboard    =    new   HashMap < YarnApplicationState ,    Map < String ,    Long >  >  (  )  ;", "assert    !  ( states . isEmpty (  )  )  ;", "for    ( String   state    :    states )     {", "Map < String ,    Long >    partScoreboard    =    new   HashMap < String ,    Long >  (  )  ;", "scoreboard . put ( YarnApplicationState . valueOf ( state . toUpperCase (  )  )  ,    partScoreboard )  ;", "for    ( String   type    :    types )     {", "partScoreboard . put ( type ,     0 L )  ;", "}", "}", "return   scoreboard ;", "}", "METHOD_END"], "methodName": ["buildScoreboard"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UserGroupInformation   callerUGI ;", "try    {", "callerUGI    =    createKerberosUserGroupInformation ( hsr )  ;", "}    catch    ( YarnException   ye )     {", "return   Response . status ( FORBIDDEN )  . entity ( ye . getMessage (  )  )  . build (  )  ;", "}", "Token < RMDelegationTokenIdentifier >    token    =    extractToken ( hsr )  ;", "api . records . Token   dToken    =    BuilderUtils . newDelegationToken ( token . getIdentifier (  )  ,    token . getKind (  )  . toString (  )  ,    token . getPassword (  )  ,    token . getService (  )  . toString (  )  )  ;", "final   CancelDelegationTokenRequest   req    =    CancelDelegationTokenRequest . newInstance ( dToken )  ;", "try    {", "callerUGI . doAs ( new   PrivilegedExceptionAction < CancelDelegationTokenResponse >  (  )     {", "@ Override", "public   CancelDelegationTokenResponse   run (  )    throws   IOException ,    YarnException    {", "return   rm . getClientRMService (  )  . cancelDelegationToken ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   ue )     {", "if    (  ( ue . getCause (  )  )    instanceof   YarnException )     {", "if    (  ( ue . getCause (  )  . getCause (  )  )    instanceof   InvalidToken )     {", "throw   new   BadRequestException ( ue . getCause (  )  . getCause (  )  . getMessage (  )  )  ;", "} else", "if    (  ( ue . getCause (  )  . getCause (  )  )    instanceof   AccessControlException )     {", "return   Response . status ( FORBIDDEN )  . entity ( ue . getCause (  )  . getCause (  )  . getMessage (  )  )  . build (  )  ;", "}", "RMWebServices . LOG . info (  \" Renew   delegation   token   request   failed \"  ,    ue )  ;", "throw   ue ;", "}", "RMWebServices . LOG . info (  \" Renew   delegation   token   request   failed \"  ,    ue )  ;", "throw   ue ;", "}    catch    ( Exception   e )     {", "RMWebServices . LOG . info (  \" Renew   delegation   token   request   failed \"  ,    e )  ;", "throw   e ;", "}", "return   Response . status ( OK )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["cancelDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Long >    partScoreboard    =    scoreboard . get ( state )  ;", "Long   count    =    partScoreboard . get ( type )  ;", "partScoreboard . put ( type ,     ( count    +     1 L )  )  ;", "}", "METHOD_END"], "methodName": ["countApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appid ;", "String   error    =     \" Could   not   parse   application   id    \"     +     ( newApp . getApplicationId (  )  )  ;", "try    {", "appid    =    ConverterUtils . toApplicationId (  . recordFactory ,    newApp . getApplicationId (  )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   BadRequestException ( error )  ;", "}", "ApplicationSubmissionContext   appContext    =    ApplicationSubmissionContext . newInstance ( appid ,    newApp . getApplicationName (  )  ,    newApp . getQueue (  )  ,    Priority . newInstance ( newApp . getPriority (  )  )  ,    createContainerLaunchContext ( newApp )  ,    newApp . getUnmanagedAM (  )  ,    newApp . getCancelTokensWhenComplete (  )  ,    newApp . getMaxAppAttempts (  )  ,    createAppSubmissionContextResource ( newApp )  ,    newApp . getApplicationType (  )  ,    newApp . getKeepContainersAcrossApplicationAttempts (  )  )  ;", "appContext . setApplicationTags ( newApp . getApplicationTags (  )  )  ;", "return   appContext ;", "}", "METHOD_END"], "methodName": ["createAppSubmissionContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( newApp . getResource (  )  . getvCores (  )  )     >     ( rm . getConfig (  )  . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  )  )     {", "String   msg    =     \" Requested   more   cores   than   configured   max \"  ;", "throw   new   BadRequestException ( msg )  ;", "}", "if    (  ( newApp . getResource (  )  . getMemory (  )  )     >     ( rm . getConfig (  )  . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )  )     {", "String   msg    =     \" Requested   more   memory   than   configured   max \"  ;", "throw   new   BadRequestException ( msg )  ;", "}", "Resource   r    =    Resource . newInstance ( newApp . getResource (  )  . getMemory (  )  ,    newApp . getResource (  )  . getvCores (  )  )  ;", "return   r ;", "}", "METHOD_END"], "methodName": ["createAppSubmissionContextResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "HashMap < String ,    ByteBuffer >    hmap    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "for    ( Map . Entry < String ,    String >    entry    :    newApp . getContainerLaunchContextInfo (  )  . getAuxillaryData (  )  . entrySet (  )  )     {", "if    (  ( entry . getValue (  )  . isEmpty (  )  )     =  =    false )     {", "Base 6  4    decoder    =    new   Base 6  4  (  0  ,    null ,    true )  ;", "byte [  ]    data    =    decoder . decode ( entry . getValue (  )  )  ;", "hmap . put ( entry . getKey (  )  ,    ByteBuffer . wrap ( data )  )  ;", "}", "}", "HashMap < String ,    LocalResource >    hlr    =    new   HashMap < String ,    LocalResource >  (  )  ;", "for    ( Map . Entry < String ,    LocalResourceInfo >    entry    :    newApp . getContainerLaunchContextInfo (  )  . getResources (  )  . entrySet (  )  )     {", "LocalResourceInfo   l    =    entry . getValue (  )  ;", "LocalResource   lr    =    LocalResource . newInstance ( ConverterUtils . getYarnUrlFromURI ( l . getUrl (  )  )  ,    l . getType (  )  ,    l . getVisibility (  )  ,    l . getSize (  )  ,    l . getTimestamp (  )  )  ;", "hlr . put ( entry . getKey (  )  ,    lr )  ;", "}", "DataOutputBuffer   out    =    new   DataOutputBuffer (  )  ;", "Credentials   cs    =    createCredentials ( newApp . getContainerLaunchContextInfo (  )  . getCredentials (  )  )  ;", "cs . writeTokenStorageToStream ( out )  ;", "ByteBuffer   tokens    =    ByteBuffer . wrap ( out . getData (  )  )  ;", "ContainerLaunchContext   ctx    =    ContainerLaunchContext . newInstance ( hlr ,    newApp . getContainerLaunchContextInfo (  )  . getEnvironment (  )  ,    newApp . getContainerLaunchContextInfo (  )  . getCommands (  )  ,    hmap ,    tokens ,    newApp . getContainerLaunchContextInfo (  )  . getAcls (  )  )  ;", "return   ctx ;", "}", "METHOD_END"], "methodName": ["createContainerLaunchContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "Credentials   ret    =    new   Credentials (  )  ;", "try    {", "for    ( Map . Entry < String ,    String >    entry    :    credentials . getTokens (  )  . entrySet (  )  )     {", "Text   alias    =    new   Text ( entry . getKey (  )  )  ;", "Token < TokenIdentifi   token    =    new   Token < TokenIdentifi (  )  ;", "token . decodeFromUrlString ( entry . getValue (  )  )  ;", "ret . addToken ( alias ,    token )  ;", "}", "for    ( Map . Entry < String ,    String >    entry    :    credentials . getTokens (  )  . entrySet (  )  )     {", "Text   alias    =    new   Text ( entry . getKey (  )  )  ;", "Base 6  4    decod =    new   Base 6  4  (  0  ,    null ,    true )  ;", "byte [  ]    secret    =    decoddecode ( entry . getValue (  )  )  ;", "ret . addSecretKey ( alias ,    secret )  ;", "}", "}    catch    ( IOException   ie )     {", "throw   new   BadRequestException (  (  \" Could   not   parse   credentials   data ;    exception   message    =     \"     +     ( ie . getMessage (  )  )  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["createCredentials"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "final   String   renewer    =    tokenData . getRenewer (  )  ;", "GetDelegationTokenResponse   resp ;", "try    {", "resp    =    callerUGI . doAs ( new   PrivilegedExceptionAction < GetDelegationTokenResponse >  (  )     {", "@ Override", "public   GetDelegationTokenResponse   run (  )    throws   IOException ,    YarnException    {", "GetDelegationTokenRequest   createReq    =    GetDelegationTokenRequest . newInstance ( renewer )  ;", "return   rm . getClientRMService (  )  . getDelegationToken ( createReq )  ;", "}", "}  )  ;", "}    catch    ( Exception   e )     {", "RMWebServices . LOG . info (  \" Create   delegation   token   request   failed \"  ,    e )  ;", "throw   e ;", "}", "Token < RMDelegationTokenIdentifier >    tk    =    new   Token < RMDelegationTokenIdentifier >  ( resp . getRMDelegationToken (  )  . getIdentifier (  )  . array (  )  ,    resp . getRMDelegationToken (  )  . getPassword (  )  . array (  )  ,    new   io . Text ( resp . getRMDelegationToken (  )  . getKind (  )  )  ,    new   io . Text ( resp . getRMDelegationToken (  )  . getService (  )  )  )  ;", "RMDelegationTokenIdentifier   identifier    =    tk . decodeIdentifier (  )  ;", "long   currentExpiration    =    rm . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getRenewDate ( identifier )  ;", "DelegationToken   respToken    =    new   DelegationToken ( tk . encodeToUrlString (  )  ,    renewer ,    identifier . getOwner (  )  . toString (  )  ,    tk . getKind (  )  . toString (  )  ,    currentExpiration ,    identifier . getMaxDate (  )  )  ;", "return   Response . status ( OK )  . entity ( respToken )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["createDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   callerUGI    =    getCallerUserGroupInformation ( hsr ,    true )  ;", "if    ( callerUGI    =  =    null )     {", "String   msg    =     \" Unable   to   obtain   user   name ,    user   not   authenticated \"  ;", "throw   new   AuthorizationException ( msg )  ;", "}", "String   authType    =    hsr . getAuthType (  )  ;", "if    (  !  ( TYPE . equals ( authType )  )  )     {", "String   msg    =     \" Delegation   token   operations   can   only   be   carried   out   on   a    \"     +     \" Kerberos   authenticated   channel \"  ;", "throw   new   YarnException ( msg )  ;", "}", "callerUGI . setAuthenticationMethod ( KERBEROS )  ;", "return   callerUGI ;", "}", "METHOD_END"], "methodName": ["createKerberosUserGroupInformation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "GetNewApplicationRequest   req    =    RMWebServices . recordFactory . newRecordInstance ( GetNewApplicationRequest . class )  ;", "GetNewApplicationResponse   resp ;", "try    {", "resp    =    rm . getClientRMService (  )  . getNewApplication ( req )  ;", "}    catch    ( YarnException   e )     {", "String   msg    =     \" Unable   to   create   new   app   from   RM   web   service \"  ;", "RMWebServices . LOG . error ( msg ,    e )  ;", "throw   new   exceptions . YarnRuntimeException ( msg ,    e )  ;", "}", "NewApplication   appId    =    new   NewApplication ( resp . getApplicationId (  )  . toString (  )  ,    new   ResourceInfo ( resp . getMaximumResourceCapability (  )  )  )  ;", "return   appId ;", "}", "METHOD_END"], "methodName": ["createNewApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UGroupInformation   callerUGI    =    getCallerUGroupInformation ( hsr ,    true )  ;", "if    ( callerUGI    =  =    null )     {", "throw   new   AuthorizationException (  (  \" Unable   to   obtain   u   name ,     \"     +     \" u   not   authenticated \"  )  )  ;", "}", "if    (  ( UGroupInformation . isSecurityEnabled (  )  )     &  &     ( isStaticU ( callerUGI )  )  )     {", "String   msg    =     \" The   default   static   u   cannot   carry   out   this   operation .  \"  ;", "return   Response . status ( FORBIDDEN )  . entity ( msg )  . build (  )  ;", "}", "NewApplication   appId    =    createNewApplication (  )  ;", "return   Response . status ( OK )  . entity ( appId )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["createNewApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "Token < RMDelegationTokenIdentifier >    token    =    new   Token < RMDelegationTokenIdentifier >  (  )  ;", "try    {", "token . decodeFromUrlString ( encodedToken )  ;", "}    catch    ( Exception   ie )     {", "String   msg    =     \" Could   not   decode   encoded   token \"  ;", "throw   new   BadRequestException ( msg )  ;", "}", "return   token ;", "}", "METHOD_END"], "methodName": ["extractToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "String   encodedToken    =    request . getHeader ( RMWebServices . DELEGATION _ TOKEN _ HEADER )  ;", "if    ( encodedToken    =  =    null )     {", "String   msg    =     (  \" Header    '  \"     +     ( RMWebServices . DELEGATION _ TOKEN _ HEADER )  )     +     \"  '    containing   encoded   token   not   found \"  ;", "throw   new   BadRequestException ( msg )  ;", "}", "return   extractToken ( encodedToken )  ;", "}", "METHOD_END"], "methodName": ["extractToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "return   getClusterInfo (  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "if    (  ( appId    =  =    null )     |  |     ( appId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" appId ,     \"     +    appId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ApplicationId   id ;", "id    =    ConverterUtils . toApplicationId (  . recordFactory ,    appId )  ;", "if    ( id    =  =    null )     {", "throw   new   NotFoundException (  \" appId   is   null \"  )  ;", "}", "RMApp   app    =    rm . getRMContext (  )  . getRMApps (  )  . get ( id )  ;", "if    ( app    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   with   id :     \"     +    appId )     +     \"    not   found \"  )  )  ;", "}", "return   new   AppInfo ( app ,    hasAccess ( app ,    hsr )  ,     (  ( hsr . getScheme (  )  )     +     \"  :  /  /  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["getApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "if    (  ( appId    =  =    null )     |  |     ( appId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" appId ,     \"     +    appId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ApplicationId   id ;", "id    =    ConverterUtils . toApplicationId (  . recordFactory ,    appId )  ;", "if    ( id    =  =    null )     {", "throw   new   NotFoundException (  \" appId   is   null \"  )  ;", "}", "RMApp   app    =    rm . getRMContext (  )  . getRMApps (  )  . get ( id )  ;", "if    ( app    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   with   id :     \"     +    appId )     +     \"    not   found \"  )  )  ;", "}", "AppAttemptsInfo   appAttemptsInfo    =    new   AppAttemptsInfo (  )  ;", "for    ( RMAppAttempt   attempt    :    app . getAppAttempts (  )  . values (  )  )     {", "AppAttemptInfo   attemptInfo    =    new   AppAttemptInfo ( attempt ,    app . getUser (  )  )  ;", "appAttemptsInfo . add ( attemptInfo )  ;", "}", "return   appAttemptsInfo ;", "}", "METHOD_END"], "methodName": ["getAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UserGroupInformation   callerUGI    =    getCallerUserGroupInformation ( hsr ,    true )  ;", "String   userName    =     \"  \"  ;", "if    ( callerUGI    !  =    null )     {", "userName    =    callerUGI . getUserName (  )  ;", "}", "RMApp   app    =    null ;", "try    {", "app    =    getRMAppForAppId ( appId )  ;", "}    catch    ( NotFoundException   e )     {", "RMAuditLogger . logFailure ( userName ,    RMAuditLogger . AuditConstants . KILL _ APP _ REQUEST ,     \" UNKNOWN \"  ,     \"  \"  ,     (  \" Trying   to   get   state   of   an   absent   application    \"     +    appId )  )  ;", "throw   e ;", "}", "AppState   ret    =    new   AppState (  )  ;", "ret . setState ( app . getState (  )  . toString (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["getAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "Set < String >    states    =    RMWebServices . parseQueries ( stateQueries ,    true )  ;", "Set < String >    types    =    RMWebServices . parseQueries ( typeQueries ,    false )  ;", "if    (  ( types . size (  )  )     =  =     0  )     {", "types . add ( RMWebServices . ANY )  ;", "} else", "if    (  ( types . size (  )  )     !  =     1  )     {", "throw   new   webapp . BadRequestException (  (  (  \"  #    of   applicationTypes    =     \"     +     ( types . size (  )  )  )     +     \"  ,    we   temporarily   support   at   most   one   applicationType \"  )  )  ;", "}", "if    (  ( states . size (  )  )     =  =     0  )     {", "for    ( YarnApplicationState   state    :    YarnApplicationState . values (  )  )     {", "states . add ( state . toString (  )  . toLowerCase (  )  )  ;", "}", "}", "Map < YarnApplicationState ,    Map < String ,    Long >  >    scoreboard    =    RMWebServices . buildScoreboard ( states ,    types )  ;", "ConcurrentMap < ApplicationId ,    RMApp >    apps    =    rm . getRMContext (  )  . getRMApps (  )  ;", "for    ( RMApp   rmapp    :    apps . values (  )  )     {", "YarnApplicationState   state    =    rmapp . createApplicationState (  )  ;", "String   type    =    rmapp . getApplicationType (  )  . trim (  )  . toLowerCase (  )  ;", "if    ( states . contains ( state . toString (  )  . toLowerCase (  )  )  )     {", "if    ( types . contains ( RMWebServices . ANY )  )     {", "RMWebServices . countApp ( scoreboard ,    state ,    RMWebServices . ANY )  ;", "} else", "if    ( types . contains ( type )  )     {", "RMWebServices . countApp ( scoreboard ,    state ,    type )  ;", "}", "}", "}", "ApplicationStatisticsInfo   appStatInfo    =    new   ApplicationStatisticsInfo (  )  ;", "for    ( Map . Entry < YarnApplicationState ,    Map < String ,    Long >  >    partScoreboard    :    scoreboard . entrySet (  )  )     {", "for    ( Map . Entry < String ,    Long >    statEntry    :    partScoreboard . getValue (  )  . entrySet (  )  )     {", "StatisticsItemInfo   statItem    =    new   StatisticsItemInfo ( partScoreboard . getKey (  )  ,    statEntry . getKey (  )  ,    statEntry . getValue (  )  )  ;", "appStatInfo . add ( statItem )  ;", "}", "}", "return   appStatInfo ;", "}", "METHOD_END"], "methodName": ["getAppStatistics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "boolean   checkCount    =    false ;", "boolean   checkStart    =    false ;", "boolean   checkEnd    =    false ;", "boolean   checkAppTypes    =    false ;", "boolean   checkAppStates    =    false ;", "boolean   checkAppTags    =    false ;", "long   countNum    =     0  ;", "long   sBegin    =     0  ;", "long   sEnd    =    Long . MAX _ VALUE ;", "long   fBegin    =     0  ;", "long   fEnd    =    Long . MAX _ VALUE ;", "init (  )  ;", "if    (  ( count    !  =    null )     &  &     (  !  ( count . isEmpty (  )  )  )  )     {", "checkCount    =    true ;", "countNum    =    Long . parseLong ( count )  ;", "if    ( countNum    <  =     0  )     {", "throw   new   BadRequestException (  \" limit   value   must   be   greater   then    0  \"  )  ;", "}", "}", "if    (  ( startedBegin    !  =    null )     &  &     (  !  ( startedBegin . isEmpty (  )  )  )  )     {", "checkStart    =    true ;", "sBegin    =    Long . parseLong ( startedBegin )  ;", "if    ( sBegin    <     0  )     {", "throw   new   BadRequestException (  \" startedTimeBegin   must   be   greater   than    0  \"  )  ;", "}", "}", "if    (  ( startedEnd    !  =    null )     &  &     (  !  ( startedEnd . isEmpty (  )  )  )  )     {", "checkStart    =    true ;", "sEnd    =    Long . parseLong ( startedEnd )  ;", "if    ( sEnd    <     0  )     {", "throw   new   BadRequestException (  \" startedTimeEnd   must   be   greater   than    0  \"  )  ;", "}", "}", "if    ( sBegin    >    sEnd )     {", "throw   new   BadRequestException (  \" startedTimeEnd   must   be   greater   than   startTimeBegin \"  )  ;", "}", "if    (  ( finishBegin    !  =    null )     &  &     (  !  ( finishBegin . isEmpty (  )  )  )  )     {", "checkEnd    =    true ;", "fBegin    =    Long . parseLong ( finishBegin )  ;", "if    ( fBegin    <     0  )     {", "throw   new   BadRequestException (  \" finishTimeBegin   must   be   greater   than    0  \"  )  ;", "}", "}", "if    (  ( finishEnd    !  =    null )     &  &     (  !  ( finishEnd . isEmpty (  )  )  )  )     {", "checkEnd    =    true ;", "fEnd    =    Long . parseLong ( finishEnd )  ;", "if    ( fEnd    <     0  )     {", "throw   new   BadRequestException (  \" finishTimeEnd   must   be   greater   than    0  \"  )  ;", "}", "}", "if    ( fBegin    >    fEnd )     {", "throw   new   BadRequestException (  \" finishTimeEnd   must   be   greater   than   finishTimeBegin \"  )  ;", "}", "Set < String >    appTypes    =    RMWebServices . parseQueries ( applicationTypes ,    false )  ;", "if    (  !  ( appTypes . isEmpty (  )  )  )     {", "checkAppTypes    =    true ;", "}", "Set < String >    appTags    =    RMWebServices . parseQueries ( applicationTags ,    false )  ;", "if    (  !  ( appTags . isEmpty (  )  )  )     {", "checkAppTags    =    true ;", "}", "if    (  ( stateQuery    !  =    null )     &  &     (  !  ( stateQuery . isEmpty (  )  )  )  )     {", "statesQuery . add ( stateQuery )  ;", "}", "Set < String >    appStates    =    RMWebServices . parseQueries ( statesQuery ,    true )  ;", "if    (  !  ( appStates . isEmpty (  )  )  )     {", "checkAppStates    =    true ;", "}", "GetApplicationsRequest   request    =    GetApplicationsRequest . newInstance (  )  ;", "if    ( checkStart )     {", "request . setStartRange ( sBegin ,    sEnd )  ;", "}", "if    ( checkEnd )     {", "request . setFinishRange ( fBegin ,    fEnd )  ;", "}", "if    ( checkCount )     {", "request . setLimit ( countNum )  ;", "}", "if    ( checkAppTypes )     {", "request . setApplicationTypes ( appTypes )  ;", "}", "if    ( checkAppTags )     {", "request . setApplicationTags ( appTags )  ;", "}", "if    ( checkAppStates )     {", "request . setApplicationStates ( appStates )  ;", "}", "if    (  ( queueQuery    !  =    null )     &  &     (  !  ( queueQuery . isEmpty (  )  )  )  )     {", "ResourceScheduler   rs    =    rm . getResourceScheduler (  )  ;", "if    ( rs   instanceof   CapacityScheduler )     {", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( rs )  )  ;", "try    {", "cs . getQueueInfo ( queueQuery ,    false ,    false )  ;", "}    catch    ( IOException   e )     {", "throw   new   BadRequestException ( e . getMessage (  )  )  ;", "}", "}", "Set < String >    queues    =    new   HashSet < String >  (  1  )  ;", "queues . add ( queueQuery )  ;", "request . setQueues ( queues )  ;", "}", "if    (  ( userQuery    !  =    null )     &  &     (  !  ( userQuery . isEmpty (  )  )  )  )     {", "Set < String >    users    =    new   HashSet < String >  (  1  )  ;", "users . add ( userQuery )  ;", "request . setUsers ( users )  ;", "}", "List < ApplicationReport >    appReports    =    null ;", "try    {", "appReports    =    rm . getClientRMService (  )  . getApplications ( request ,    false )  . getApplicationList (  )  ;", "}    catch    ( YarnException   e )     {", "RMWebServices . LOG . error (  \" Unable   to   retrieve   apps   from   ClientRMService \"  ,    e )  ;", "throw   new   exceptions . YarnRuntimeException (  \" Unable   to   retrieve   apps   from   ClientRMService \"  ,    e )  ;", "}", "final   ConcurrentMap < ApplicationId ,    RMApp >    apps    =    rm . getRMContext (  )  . getRMApps (  )  ;", "AppsInfo   allApps    =    new   AppsInfo (  )  ;", "for    ( ApplicationReport   report    :    appReports )     {", "RMApp   rmapp    =    apps . get ( report . getApplicationId (  )  )  ;", "if    (  ( finalStatusQuery    !  =    null )     &  &     (  !  ( finalStatusQuery . isEmpty (  )  )  )  )     {", "FinalApplicationStatus . valueOf ( finalStatusQuery )  ;", "if    (  !  ( rmapp . getFinalApplicationStatus (  )  . toString (  )  . equalsIgnoreCase ( finalStatusQuery )  )  )     {", "continue ;", "}", "}", "AppInfo   app    =    new   AppInfo ( rmapp ,    hasAccess ( rmapp ,    hsr )  ,    WebAppUtils . getHttpSchemePrefix ( conf )  )  ;", "allApps . add ( app )  ;", "}", "return   allApps ;", "}", "METHOD_END"], "methodName": ["getApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "String   remoteUser    =    hsr . getRemoteUser (  )  ;", "if    ( usePrincipal )     {", "Principal   princ    =    hsr . getUserPrincipal (  )  ;", "remoteUser    =     ( princ    =  =    null )     ?    null    :    princ . getName (  )  ;", "}", "UserGroupInformation   callerUGI    =    null ;", "if    ( remoteUser    !  =    null )     {", "callerUGI    =    UserGroupInformation . createRemoteUser ( remoteUser )  ;", "}", "return   callerUGI ;", "}", "METHOD_END"], "methodName": ["getCallerUserGroupInformation"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "turn   new   ClusterInfo ( this . rm )  ;", "}", "METHOD_END"], "methodName": ["getClusterInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "retu   new   ClusterMetricsInfo ( this . rm ,    this . rm . getRMContext (  )  )  ;", "}", "METHOD_END"], "methodName": ["getClusterMetricsInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "if    (  ( nodeId    =  =    null )     |  |     ( nodeId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" nodeId ,     \"     +    nodeId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ResourceScheduler   sched    =    this . rm . getResourceScheduler (  )  ;", "if    ( sched    =  =    null )     {", "throw   new   NotFoundException (  \" Null   ResourceScheduler   instance \"  )  ;", "}", "NodeId   nid    =    ConverterUtils . toNodeId ( nodeId )  ;", "RMNode   ni    =    this . rm . getRMContext (  )  . getRMNodes (  )  . get ( nid )  ;", "boolean   isInactive    =    false ;", "if    ( ni    =  =    null )     {", "ni    =    this . rm . getRMContext (  )  . getInactiveRMNodes (  )  . get ( nid . getHost (  )  )  ;", "if    ( ni    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" nodeId ,     \"     +    nodeId )     +     \"  ,    is   not   found \"  )  )  ;", "}", "isInactive    =    true ;", "}", "NodeInfo   nodeInfo    =    new   NodeInfo ( ni ,    sched )  ;", "if    ( isInactive )     {", "nodeInfo . setNodeHTTPAddress (  . EMPTY )  ;", "}", "return   nodeInfo ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "ResourceScheduler   sched    =    this . rm . getResourceScheduler (  )  ;", "if    ( sched    =  =    null )     {", "throw   new   NotFoundException (  \" Null   ResourceScheduler   instance \"  )  ;", "}", "EnumSet < NodeState >    acceptedStates ;", "if    ( states    =  =    null )     {", "acceptedStates    =    EnumSet . allOf ( NodeState . class )  ;", "} else    {", "acceptedStates    =    EnumSet . noneOf ( NodeState . class )  ;", "for    ( String   stateStr    :    states . split (  \"  ,  \"  )  )     {", "acceptedStates . add ( NodeState . valueOf ( stateStr . toUpperCase (  )  )  )  ;", "}", "}", "Collection < RMNode >    rmNodes    =    RMServerUtils . queryRMNodes ( this . rm . getRMContext (  )  ,    acceptedStates )  ;", "NodesInfo   nodesInfo    =    new   NodesInfo (  )  ;", "for    ( RMNode   rmNode    :    rmNodes )     {", "NodeInfo   nodeInfo    =    new   NodeInfo ( rmNode ,    sched )  ;", "if    ( EnumSet . of ( LOST ,    DECOMMISSIONED ,    REBOOTED )  . contains ( rmNode . getState (  )  )  )     {", "nodeInfo . setNodeHTTPAddress (  . EMPTY )  ;", "}", "nodesInfo . add ( nodeInfo )  ;", "}", "return   nodesInfo ;", "}", "METHOD_END"], "methodName": ["getNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( appId    =  =    null )     |  |     ( appId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" appId ,     \"     +    appId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ApplicationId   id ;", "try    {", "id    =    ConverterUtils . toApplicationId (  . recordFactory ,    appId )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   NotFoundException (  \" appId   is   invalid \"  )  ;", "}", "if    ( id    =  =    null )     {", "throw   new   NotFoundException (  \" appId   is   invalid \"  )  ;", "}", "RMApp   app    =    rm . getRMContext (  )  . getRMApps (  )  . get ( id )  ;", "if    ( app    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   with   id :     \"     +    appId )     +     \"    not   found \"  )  )  ;", "}", "return   app ;", "}", "METHOD_END"], "methodName": ["getRMAppForAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "ResourceScheduler   rs    =    rm . getResourceScheduler (  )  ;", "SchedulerInfo   sinfo ;", "if    ( rs   instanceof   CapacityScheduler )     {", "CapacityScheduler   cs    =     (  ( CapacityScheduler )     ( rs )  )  ;", "CSQueue   root    =    cs . getRootQueue (  )  ;", "sinfo    =    new   CapacitySchedulerInfo ( root )  ;", "} else", "if    ( rs   instanceof   FairScheduler )     {", "FairScheduler   fs    =     (  ( FairScheduler )     ( rs )  )  ;", "sinfo    =    new   dao . FairSchedulerInfo ( fs )  ;", "} else", "if    ( rs   instanceof   FifoScheduler )     {", "sinfo    =    new   dao . FifoSchedulerInfo ( this . rm )  ;", "} else    {", "throw   new   NotFoundException (  \" Unknown   scheduler   configured \"  )  ;", "}", "return   new   SchedulerTypeInfo ( sinfo )  ;", "}", "METHOD_END"], "methodName": ["getSchedulerInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation   callerUGI    =    getCallerUserGroupInformation ( hsr ,    true )  ;", "if    (  ( callerUGI    !  =    null )     &  &     (  !  (  ( this . rm . getApplicationACLsM (  )  . checkAccess ( callerUGI ,    VIEW _ APP ,    app . getUser (  )  ,    app . getApplicationId (  )  )  )     |  |     ( this . rm . getQueueACLsM (  )  . checkAccess ( callerUGI ,    ADMINISTER _ QUEUE ,    app . getQueue (  )  )  )  )  )  )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["hasAccess"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "response . setContentType ( null )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "String   staticUser    =    conf . get ( HADOOP _ HTTP _ STATIC _ USER ,    DEFAULT _ HADOOP _ HTTP _ STATIC _ USER )  ;", "return   staticUser . equals ( callerUGI . getUserName (  )  )  ;", "}", "METHOD_END"], "methodName": ["isStaticUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "if    ( app    =  =    null )     {", "throw   new   IllegalArgumentException (  \" app   cannot   be   null \"  )  ;", "}", "String   userName    =    callerUGI . getUserName (  )  ;", "final   ApplicationId   appid    =    app . getApplicationId (  )  ;", "KillApplicationResponse   resp    =    null ;", "try    {", "resp    =    callerUGI . doAs ( new   PrivilegedExceptionAction < KillApplicationResponse >  (  )     {", "@ Override", "public   KillApplicationResponse   run (  )    throws   IOException ,    YarnException    {", "KillApplicationRequest   req    =    KillApplicationRequest . newInstance ( appid )  ;", "return   rm . getClientRMService (  )  . forceKillApplication ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   ue )     {", "if    (  ( ue . getCause (  )  )    instanceof   YarnException )     {", "YarnException   ye    =     (  ( YarnException )     ( ue . getCause (  )  )  )  ;", "if    (  ( ye . getCause (  )  )    instanceof   AccessControlException )     {", "String   appId    =    app . getApplicationId (  )  . toString (  )  ;", "String   msg    =     (  (  \" Unauthorized   attempt   to   kill   appid    \"     +    appId )     +     \"    by   remote   user    \"  )     +    userName ;", "return   Response . status ( FORBIDDEN )  . entity ( msg )  . build (  )  ;", "} else    {", "throw   ue ;", "}", "} else    {", "throw   ue ;", "}", "}", "AppState   ret    =    new   AppState (  )  ;", "ret . setState ( app . getState (  )  . toString (  )  )  ;", "if    ( resp . getIsKillCompleted (  )  )     {", "RMAuditLogger . logSuccess ( userName ,    RMAuditLogger . AuditConstants . KILL _ APP _ REQUEST ,     \"  \"  ,    app . getApplicationId (  )  )  ;", "} else    {", "return   Response . status ( ACCEPTED )  . entity ( ret )  . header ( LOCATION ,    hsr . getRequestURL (  )  )  . build (  )  ;", "}", "return   Response . status ( OK )  . entity ( ret )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["killApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "Set < String >    params    =    new   HashSet < String >  (  )  ;", "if    (  !  ( queries . isEmpty (  )  )  )     {", "for    ( String   query    :    queries )     {", "if    (  ( query    !  =    null )     &  &     (  !  ( query . trim (  )  . isEmpty (  )  )  )  )     {", "String [  ]    paramStrs    =    query . split (  \"  ,  \"  )  ;", "for    ( String   paramStr    :    paramStrs )     {", "if    (  ( paramStr    !  =    null )     &  &     (  !  ( paramStr . trim (  )  . isEmpty (  )  )  )  )     {", "if    ( isState )     {", "try    {", "YApplicationState . valueOf ( paramStr . trim (  )  . toUpperCase (  )  )  ;", "}    catch    ( RuntimeException   e )     {", "YApplicationState [  ]    stateArray    =    YApplicationState . values (  )  ;", "String   allAppStates    =    Arrays . toString ( stateArray )  ;", "throw   new   BadRequestException (  (  (  (  \" Invalid   application - state    \"     +     ( paramStr . trim (  )  )  )     +     \"    specified .    It   should   be   one   of    \"  )     +    allAppStates )  )  ;", "}", "}", "params . add ( paramStr . trim (  )  . toLowerCase (  )  )  ;", "}", "}", "}", "}", "}", "return   params ;", "}", "METHOD_END"], "methodName": ["parseQueries"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UGroupInformation   callerUGI ;", "try    {", "callerUGI    =    createKerberosUGroupInformation ( hsr )  ;", "}    catch    ( YarnException   ye )     {", "return   Response . status ( FORBIDDEN )  . entity ( ye . getMessage (  )  )  . build (  )  ;", "}", "return   createDelegationToken ( tokenData ,    hsr ,    callerUGI )  ;", "}", "METHOD_END"], "methodName": ["postDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UserGroupInformation   callerUGI ;", "try    {", "callerUGI    =    createKerberosUserGroupInformation ( hsr )  ;", "}    catch    ( YarnException   ye )     {", "return   Response . status ( FORBIDDEN )  . entity ( ye . getMessage (  )  )  . build (  )  ;", "}", "DelegationToken   requestToken    =    new   DelegationToken (  )  ;", "requestToketToken ( extractToken ( hsr )  . encodeToUrlString (  )  )  ;", "return   renewDelegationToken ( requestToken ,    hsr ,    callerUGI )  ;", "}", "METHOD_END"], "methodName": ["postDelegationTokenExpiration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "Token < RMDelegationTokenIdentifier >    token    =    extractToken ( tokenData . getToken (  )  )  ;", "api . records . Token   dToken    =    BuilderUtils . newDelegationToken ( token . getIdentifier (  )  ,    token . getKind (  )  . toString (  )  ,    token . getPassword (  )  ,    token . getService (  )  . toString (  )  )  ;", "final   RenewDelegationTokenRequest   req    =    RenewDelegationTokenRequest . newInstance ( dToken )  ;", "RenewDelegationTokenResponse   resp ;", "try    {", "resp    =    callerUGI . doAs ( new   PrivilegedExceptionAction < RenewDelegationTokenResponse >  (  )     {", "@ Override", "public   RenewDelegationTokenResponse   run (  )    throws   IOException ,    YarnException    {", "return   rm . getClientRMService (  )  . renewDelegationToken ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   ue )     {", "if    (  ( ue . getCause (  )  )    instanceof   YarnException )     {", "if    (  ( ue . getCause (  )  . getCause (  )  )    instanceof   InvalidToken )     {", "throw   new   BadRequestException ( ue . getCause (  )  . getCause (  )  . getMessage (  )  )  ;", "} else", "if    (  ( ue . getCause (  )  . getCause (  )  )    instanceof   AccessControlException )     {", "return   Response . status ( Status . FORBIDDEN )  . entity ( ue . getCause (  )  . getCause (  )  . getMessage (  )  )  . build (  )  ;", "}", "RMWebServices . LOG . info (  \" Renew   delegation   token   request   failed \"  ,    ue )  ;", "throw   ue ;", "}", "RMWebServices . LOG . info (  \" Renew   delegation   token   request   failed \"  ,    ue )  ;", "throw   ue ;", "}    catch    ( Exception   e )     {", "RMWebServices . LOG . info (  \" Renew   delegation   token   request   failed \"  ,    e )  ;", "throw   e ;", "}", "long   renewTime    =    resp . getNextExpirationTime (  )  ;", "DelegationToken   respToken    =    new   DelegationToken (  )  ;", "respToken . setNextExpirationTime ( renewTime )  ;", "return   Response . status ( OK )  . entity ( respToken )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["renewDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UserGroupInformation   callerUGI    =    getCallerUserGroupInformation ( hsr ,    true )  ;", "if    ( callerUGI    =  =    null )     {", "throw   new   AuthorizationException (  (  \" Unable   to   obtain   user   name ,     \"     +     \" user   not   authenticated \"  )  )  ;", "}", "if    (  ( UserGroupInformation . isSecurityEnabled (  )  )     &  &     ( isStaticUser ( callerUGI )  )  )     {", "String   msg    =     \" The   default   static   user   cannot   carry   out   this   operation .  \"  ;", "return   Response . status ( FORBIDDEN )  . entity ( msg )  . build (  )  ;", "}", "ApplicationSubmissionContext   appContext    =    createAppSubmissionContext ( newApp )  ;", "final   SubmitApplicationRequest   req    =    SubmitApplicationRequest . newInstance ( appContext )  ;", "try    {", "callerUGI . doAs ( new   PrivilegedExceptionAction < SubmitApplicationResponse >  (  )     {", "@ Override", "public   SubmitApplicationResponse   run (  )    throws   IOException ,    YarnException    {", "return   rm . getClientRMService (  )  . submitApplication ( req )  ;", "}", "}  )  ;", "}    catch    ( UndeclaredThrowableException   ue )     {", "if    (  ( ue . getCause (  )  )    instanceof   YarnException )     {", "throw   new   BadRequestException ( ue . getCause (  )  . getMessage (  )  )  ;", "}", ". LOG . info (  \" Submit   app   request   failed \"  ,    ue )  ;", "throw   ue ;", "}", "String   url    =     (  ( hsr . getRequestURL (  )  )     +     \"  /  \"  )     +     ( newApp . getApplicationId (  )  )  ;", "return   Response . status ( ACCEPTED )  . header ( LOCATION ,    url )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["submitApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "init (  )  ;", "UserGroupInformation   callerUGI    =    getCallerUserGroupInformation ( hsr ,    true )  ;", "if    ( callerUGI    =  =    null )     {", "String   msg    =     \" Unable   to   obtain   user   name ,    user   not   authenticated \"  ;", "throw   new   AuthorizationException ( msg )  ;", "}", "if    (  ( UserGroupInformation . isSecurityEnabled (  )  )     &  &     ( isStaticUser ( callerUGI )  )  )     {", "String   msg    =     \" The   default   static   user   cannot   carry   out   this   operation .  \"  ;", "return   Response . status ( FORBIDDEN )  . entity ( msg )  . build (  )  ;", "}", "String   userName    =    callerUGI . getUserName (  )  ;", "RMApp   app    =    null ;", "try    {", "app    =    getRMAppForAppId ( appId )  ;", "}    catch    ( NotFoundException   e )     {", "RMAuditLogger . logFailure ( userName ,    RMAuditLogger . AuditConstants . KILL _ APP _ REQUEST ,     \" UNKNOWN \"  ,     \" RMWebService \"  ,     (  \" Trying   to   kill / move   an   absent   application    \"     +    appId )  )  ;", "throw   e ;", "}", "if    (  !  ( app . getState (  )  . toString (  )  . equals ( targetState . getState (  )  )  )  )     {", "if    ( targetState . getState (  )  . equals ( KILLED . toString (  )  )  )     {", "return   killApp ( app ,    callerUGI ,    hsr )  ;", "}", "throw   new   webapp . BadRequestException (  (  (  \" Only    '  \"     +     ( KILLED . toString (  )  )  )     +     \"  '    is   allowed   as   a   target   state .  \"  )  )  ;", "}", "AppState   ret    =    new   AppState (  )  ;", "ret . setState ( app . getState (  )  . toString (  )  )  ;", "return   Response . status ( OK )  . entity ( ret )  . build (  )  ;", "}", "METHOD_END"], "methodName": ["updateAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices"}, {"methodBody": ["METHOD_START", "{", "setTitle (  \" About   the   Cluster \"  )  ;", "render ( AboutP . class )  ;", "}", "METHOD_END"], "methodName": ["about"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"}, {"methodBody": ["METHOD_START", "{", "render ( AppPage . class )  ;", "}", "METHOD_END"], "methodName": ["app"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"}, {"methodBody": ["METHOD_START", "{", "render ( NodesPage . class )  ;", "}", "METHOD_END"], "methodName": ["nodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"}, {"methodBody": ["METHOD_START", "{", "setTitle ( join (  \" Queue    \"  ,    get ( QUEUE _ NAME ,     \" unknown \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["queue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"}, {"methodBody": ["METHOD_START", "{", "set ( APP _ STATE ,    StringHelper . cjoin ( NEW . toString (  )  ,    NEW _ SAVING . toString (  )  ,    SUBMITTED . toString (  )  ,    ACCEPTED . toString (  )  ,    RUNNING . toString (  )  )  )  ;", "RManager   rm    =    getInstance ( RManager . class )  ;", "RScheduler   rs    =    rm . getRScheduler (  )  ;", "if    (  ( rs    =  =    null )     |  |     ( rs   instanceof   CapacityScheduler )  )     {", "setTitle (  \" Capacity   Scheduler \"  )  ;", "render ( CapacitySchedulerPage . class )  ;", "return ;", "}", "if    ( rs   instanceof   FairScheduler )     {", "setTitle (  \" Fair   Scheduler \"  )  ;", "render ( FairSchedulerPage . class )  ;", "return ;", "}", "setTitle (  \" Default   Scheduler \"  )  ;", "render ( DefaultSchedulerPage . class )  ;", "}", "METHOD_END"], "methodName": ["scheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"}, {"methodBody": ["METHOD_START", "{", "setTitle (  \" Application   Submission   Not   Allowed \"  )  ;", "}", "METHOD_END"], "methodName": ["submit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController"}, {"methodBody": ["METHOD_START", "{", "return   tableInit (  )  . append (  \"  ,     ' aaData '  :    appsTableData \"  )  . append (  \"  ,    bDeferRender :    true \"  )  . append (  \"  ,    bProcessing :    true \"  )  . append (  \"  \\ n ,    aoColumnDefs :     \"  )  . append ( getAppsTableColumnDefs (  )  )  . append (  \"  ,    aaSorting :     [  [  0  ,     ' desc '  ]  ]  }  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["appsTableInit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmView"}, {"methodBody": ["METHOD_START", "{", "set ( ACCORDION _ ID ,     \" nav \"  )  ;", "set ( initID ( ACCORDION ,     \" nav \"  )  ,     \"  { autoHeight : false ,    active :  0  }  \"  )  ;", "}", "METHOD_END"], "methodName": ["commonPreHead"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmView"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "return   sb . append (  \"  [  \\ n \"  )  . append (  \"  {  ' sType '  :  ' string '  ,     ' aTargets '  :     [  0  ]  \"  )  . append (  \"  ,     ' mRender '  :    parseHID    }  \"  )  . append (  \"  \\ n ,     {  \\  ' sType \\  '  :  \\  ' numeric \\  '  ,     \\  ' aTargets \\  '  :     [  5  ,     6  ]  \"  )  . append (  \"  ,     ' mRender '  :    renderHDate    }  \"  )  . append (  \"  \\ n ,     {  \\  ' sType \\  '  :  \\  ' numeric \\  '  ,    bSearchable : false ,     \\  ' aTargets \\  '  :     [  9  ]  \"  )  . append (  \"  ,     ' mRender '  :    parseHProgress    }  ]  \"  )  . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getAppsTableColumnDefs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.RmView"}, {"methodBody": ["METHOD_START", "{", "final   RMContext   mockRMContext    =    TestRMWebApp . mockRMContext (  3  ,    numberOfRacks ,    numberOfNodesPerRack ,     (  8     *     ( TestRMWebApp . GiB )  )  )  ;", "injector    =    WebAppTests . createMockInjector ( RMContext . class ,    mockRMContext ,    new   Module (  )     {", "@ Override", "public   void   configure ( Binder   binder )     {", "try    {", "binder . bind ( RManager . class )  . toInstance ( TestRMWebApp . mockRm ( mockRMContext )  )  ;", "}    catch    ( IOException   e )     {", "throw   new   IllegalStateException ( e )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage"}, {"methodBody": ["METHOD_START", "{", "injector . getInstance ( NodesPage . NodesBlock . class )  . render (  )  ;", "PrintWriter   writer    =    injector . getInstance ( PrintWriter . class )  ;", "WebAppTests . flushOutput ( injector )  ;", "Mockito . verify ( writer ,    Mockito . times (  (  ( numberOfActualTableHeaders )     +     ( numberOfThInMetricsTable )  )  )  )  . print (  \"  < th \"  )  ;", "Mockito . verify ( writer ,    Mockito . times (  (  (  (  ( numberOfRacks )     *     ( numberOfNodesPerRack )  )     *     ( numberOfActualTableHeaders )  )     +     ( numberOfThInMetricsTable )  )  )  )  . print (  \"  < td \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodesBlockRender"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage"}, {"methodBody": ["METHOD_START", "{", "NodesPage . NodesBlock   nodesBlock    =    injector . getInstance ( NodesPage . NodesBlock . class )  ;", "nodesBlock . set (  \" node . state \"  ,     \" lost \"  )  ;", "nodesBlock . render (  )  ;", "PrintWriter   writer    =    injector . getInstance ( PrintWriter . class )  ;", "WebAppTests . flushOutput ( injector )  ;", "Mockito . verify ( writer ,    Mockito . times (  (  ( numberOfActualTableHeaders )     +     ( numberOfThInMetricsTable )  )  )  )  . print (  \"  < th \"  )  ;", "Mockito . verify ( writer ,    Mockito . times (  (  (  (  ( numberOfRacks )     *     ( numberOfLostNodesPerRack )  )     *     ( numberOfActualTableHeaders )  )     +     ( numberOfThInMetricsTable )  )  )  )  . print (  \"  < td \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodesBlockRenderForLostNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestNodesPage"}, {"methodBody": ["METHOD_START", "{", "WebApps .  $ for (  \" yarn \"  ,    new   TestRMWebApp (  )  )  . at (  8  8  8  8  )  . inDevMode (  )  . start ( new   RMWebApp ( TestRMWebApp . mockRm (  2  5  0  0  ,     8  ,     8  ,     (  8     *     ( TestRMWebApp . GiB )  )  )  )  )  . joinThread (  )  ;", "WebApps .  $ for (  \" yarn \"  ,    new   TestRMWebApp (  )  )  . at (  8  8  8  8  )  . inDevMode (  )  . start ( new   RMWebApp ( TestRMWebApp . mockFifoRm (  1  0  ,     1  ,     4  ,     (  8     *     ( TestRMWebApp . GiB )  )  )  )  )  . joinThread (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "return   new   security . ApplicationACLsManager ( conf )  ;", "}", "METHOD_END"], "methodName": ["mockAppACLsManager"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", ". setupQueueConfiguration ( conf )  ;", "CapacityScheduler   cs    =    new   CapacityScheduler (  )  ;", "cs . setConf ( new   YarnConfiguration (  )  )  ;", "cs . setRMContext ( new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  )  ;", "cs . init ( conf )  ;", "return   cs ;", "}", "METHOD_END"], "methodName": ["mockCapacityScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "ResourceManager   rm    =    mock ( ResourceManager . class )  ;", "RMContext   rmContext    =     . mockRMContext ( apps ,    racks ,    nodes ,    mbsPerNode )  ;", "ResourceScheduler   rs    =     . mockFifoScheduler ( rmContext )  ;", "when ( rm . getResourceScheduler (  )  )  . thenReturn ( rs )  ;", "when ( rm . getRMContext (  )  )  . thenReturn ( rmContext )  ;", "return   rm ;", "}", "METHOD_END"], "methodName": ["mockFifoRm"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "CapacitySchedulerConfiguration   conf    =    new   CapacitySchedulerConfiguration (  )  ;", ". setupFifoQueueConfiguration ( conf )  ;", "FifoScheduler   fs    =    new   FifoScheduler (  )  ;", "fs . setConf ( new   YarnConfiguration (  )  )  ;", "fs . setRMContext ( rmContext )  ;", "fs . init ( conf )  ;", "return   fs ;", "}", "METHOD_END"], "methodName": ["mockFifoScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "final   List < RMApp >    apps    =    MockAsm . newApplications ( numApps )  ;", "final   ConcurrentMap < ApplicationId ,    RMApp >    applicationsMaps    =    Maps . newConcurrentMap (  )  ;", "for    ( RMApp   app    :    apps )     {", "applicationsMaps . put ( app . getApplicationId (  )  ,    app )  ;", "}", "final   List < RMNode >    nodes    =    MockNodes . newNodes ( racks ,    numNodes ,    MockNodes . newR ( mbsPerNode )  )  ;", "final   ConcurrentMap < NodeId ,    RMNode >    nodesMap    =    Maps . newConcurrentMap (  )  ;", "for    ( RMNode   node    :    nodes )     {", "nodesMap . put ( node . getNodeID (  )  ,    node )  ;", "}", "final   List < RMNode >    deactivatedNodes    =    MockNodes . deactivatedNodes ( racks ,    numNodes ,    MockNodes . newR ( mbsPerNode )  )  ;", "final   ConcurrentMap < String ,    RMNode >    deactivatedNodesMap    =    Maps . newConcurrentMap (  )  ;", "for    ( RMNode   node    :    deactivatedNodes )     {", "deactivatedNodesMap . put ( node . getHostName (  )  ,    node )  ;", "}", "return   new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )     {", "@ Override", "public   ConcurrentMap < ApplicationId ,    RMApp >    getRMApps (  )     {", "return   applicationsMaps ;", "}", "@ Override", "public   ConcurrentMap < String ,    RMNode >    getInactiveRMNodes (  )     {", "return   deactivatedNodesMap ;", "}", "@ Override", "public   ConcurrentMap < NodeId ,    RMNode >    getRMNodes (  )     {", "return   nodesMap ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["mockRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "RMContext   rmContext    =    TestRMWebApp . mockRMContext ( apps ,    racks ,    nodes ,    mbsPerNode )  ;", "return   TestRMWebApp . mockRm ( rmContext )  ;", "}", "METHOD_END"], "methodName": ["mockRm"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "ResourceManager   rm    =    mock ( ResourceManager . class )  ;", "ResourceScheduler   rs    =     . mockCapacityScheduler (  )  ;", "ApplicationACLsManager   aclMgr    =     . mockAppACLsManager (  )  ;", "when ( rm . getResourceScheduler (  )  )  . thenReturn ( rs )  ;", "when ( rm . getRMContext (  )  )  . thenReturn ( rmContext )  ;", "when ( rm . getApplicationACLsManager (  )  )  . thenReturn ( aclMgr )  ;", "return   rm ;", "}", "METHOD_END"], "methodName": ["mockRm"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues (  \" default \"  ,    new   String [  ]  {     \" default \"     }  )  ;", "conf . setCity (  \" default \"  ,     1  0  0  )  ;", "}", "METHOD_END"], "methodName": ["setupFifoQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {     \" a \"  ,     \" b \"  ,     \" c \"     }  )  ;", "final   String   A    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . a \"  ;", "conf . setCapacity ( A ,     1  0  )  ;", "final   String   B    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . b \"  ;", "conf . setCapacity ( B ,     2  0  )  ;", "final   String   C    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . c \"  ;", "conf . setCapacity ( C ,     7  0  )  ;", "final   String   A 1     =    A    +     \"  . a 1  \"  ;", "final   String   A 2     =    A    +     \"  . a 2  \"  ;", "conf . setQueues ( A ,    new   String [  ]  {     \" a 1  \"  ,     \" a 2  \"     }  )  ;", "conf . setCapacity ( A 1  ,     3  0  )  ;", "conf . setCapacity ( A 2  ,     7  0  )  ;", "final   String   B 1     =    B    +     \"  . b 1  \"  ;", "final   String   B 2     =    B    +     \"  . b 2  \"  ;", "final   String   B 3     =    B    +     \"  . b 3  \"  ;", "conf . setQueues ( B ,    new   String [  ]  {     \" b 1  \"  ,     \" b 2  \"  ,     \" b 3  \"     }  )  ;", "conf . setCapacity ( B 1  ,     5  0  )  ;", "conf . setCapacity ( B 2  ,     3  0  )  ;", "conf . setCapacity ( B 3  ,     2  0  )  ;", "final   String   C 1     =    C    +     \"  . c 1  \"  ;", "final   String   C 2     =    C    +     \"  . c 2  \"  ;", "final   String   C 3     =    C    +     \"  . c 3  \"  ;", "final   String   C 4     =    C    +     \"  . c 4  \"  ;", "conf . setQueues ( C ,    new   String [  ]  {     \" c 1  \"  ,     \" c 2  \"  ,     \" c 3  \"  ,     \" c 4  \"     }  )  ;", "conf . setCapacity ( C 1  ,     5  0  )  ;", "conf . setCapacity ( C 2  ,     1  0  )  ;", "conf . setCapacity ( C 3  ,     3  5  )  ;", "conf . setCapacity ( C 4  ,     5  )  ;", "final   String   C 1  1     =    C 1     +     \"  . c 1  1  \"  ;", "final   String   C 1  2     =    C 1     +     \"  . c 1  2  \"  ;", "final   String   C 1  3     =    C 1     +     \"  . c 1  3  \"  ;", "conf . setQueues ( C 1  ,    new   String [  ]  {     \" c 1  1  \"  ,     \" c 1  2  \"  ,     \" c 1  3  \"     }  )  ;", "conf . setCapacity ( C 1  1  ,     1  5  )  ;", "conf . setCapacity ( C 1  2  ,     4  5  )  ;", "conf . setCapacity ( C 1  3  ,     4  0  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( TestRMWebApp . class ,    this ,    new   Module (  )     {", "@ Override", "public   void   configure ( Binder   binder )     {", "binder . bind ( ApplicationACLsManager . class )  . toInstance ( new   ApplicationACLsManager ( new   Configuration (  )  )  )  ;", "}", "}  )  ;", "RmController   c    =    injector . getInstance ( RmController . class )  ;", "c . index (  )  ;", "assertEquals (  \" Applications \"  ,    c . get ( TITLE ,     \" unknown \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testControllerIndex"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "final   RMContext   rmContext    =    TestRMWebApp . mockRMContext (  3  ,     2  ,     1  2  ,     (  8     *     ( TestRMWebApp . GiB )  )  )  ;", "Injector   injector    =    WebAppTests . createMockInjector ( RMContext . class ,    rmContext ,    new   Module (  )     {", "@ Override", "public   void   configure ( Binder   binder )     {", "try    {", "binder . bind ( ResourceManager . class )  . toInstance ( TestRMWebApp . mockRm ( rmContext )  )  ;", "}    catch    ( IOException   e )     {", "throw   new   IllegalStateException ( e )  ;", "}", "}", "}  )  ;", "NodesPage   instance    =    injector . getInstance ( NodesPage . class )  ;", "instance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "instance . moreParams (  )  . put ( NODE _ STATE ,    UNHEALTHY . toString (  )  )  ;", "instance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "instance . moreParams (  )  . put ( NODE _ STATE ,    LOST . toString (  )  )  ;", "instance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testNodesPage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( RMContext . class ,    TestRMWebApp . mockRMContext (  1  5  ,     1  ,     2  ,     (  8     *     ( TestRMWebApp . GiB )  )  )  ,    new   Module (  )     {", "@ Override", "public   void   configure ( Binder   binder )     {", "try    {", "binder . bind ( ResourceManager . class )  . toInstance ( TestRMWebApp . mockRm (  3  ,     1  ,     2  ,     (  8     *     ( TestRMWebApp . GiB )  )  )  )  ;", "}    catch    ( IOException   e )     {", "throw   new   IllegalStateException ( e )  ;", "}", "}", "}  )  ;", "RmView   rmViewInstance    =    injector . getInstance ( RmView . class )  ;", "rmViewInstance . set ( APP _ STATE ,    RUNNING . toString (  )  )  ;", "rmViewInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "rmViewInstance . set ( APP _ STATE ,    StringHelper . cjoin ( ACCEPTED . toString (  )  ,    RUNNING . toString (  )  )  )  ;", "rmViewInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testView"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebApp"}, {"methodBody": ["METHOD_START", "{", "FairScheduler   fs    =    new   FairScheduler (  )  ;", "FairSchedulerConfiguration   conf    =    new   FairSchedulerConfiguration (  )  ;", "fs . setRMContext ( new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    new   RMContainerTokenSecretManager ( conf )  ,    new   NMTokenSecretManagerInRM ( conf )  ,    new   ClientToAMTokenSecretManagerInRM (  )  ,    null )  )  ;", "fs . init ( conf )  ;", "return   fs ;", "}", "METHOD_END"], "methodName": ["mockFairScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebAppFairScheduler"}, {"methodBody": ["METHOD_START", "{", "final   ConcurrentMap < ApplicationId ,    RMApp >    icationsMaps    =    Maps . newConcurrentMap (  )  ;", "int   i    =     0  ;", "for    ( RMAppState   state    :    states )     {", "MockRMApp    =    new   MockRMApp ( i ,    i ,    state )  ;", "icationsMaps . put ( getApplicationId (  )  ,     ;", "i +  +  ;", "}", "return   new   RMContextImpl ( null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )     {", "@ Override", "public   ConcurrentMap < ApplicationId ,    RMApp >    getRMApps (  )     {", "return   icationsMaps ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["mockRMContext"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebAppFairScheduler"}, {"methodBody": ["METHOD_START", "{", "ResourceManager   rm    =    mock ( ResourceManager . class )  ;", "ResourceScheduler   rs    =     . mockFairScheduler (  )  ;", "when ( rm . getResourceScheduler (  )  )  . thenReturn ( rs )  ;", "when ( rm . getRMContext (  )  )  . thenReturn ( rmContext )  ;", "return   rm ;", "}", "METHOD_END"], "methodName": ["mockRm"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebAppFairScheduler"}, {"methodBody": ["METHOD_START", "{", "List < RMAppState >    appStates    =    Arrays . asList ( RMAppState . NEW ,    RMAppState . NEW _ SAVING ,    RMAppState . SUBMITTED )  ;", "final   RMContext   rmContext    =     . mockRMContext ( appStates )  ;", "Injector   injector    =    WebAppTests . createMockInjector ( RMContext . class ,    rmContext ,    new   Module (  )     {", "@ Override", "public   void   configure ( Binder   binder )     {", "try    {", "ResourceManager   mockRmWithFairScheduler    =     . mockRm ( rmContext )  ;", "binder . bind ( ResourceManager . class )  . toInstance ( mockRmWithFairScheduler )  ;", "}    catch    ( IOException   e )     {", "throw   new   IllegalStateException ( e )  ;", "}", "}", "}  )  ;", "FairSchedulerPage   fsViewInstance    =    injector . getInstance ( FairSchedulerPage . class )  ;", "fsViewInstance . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "}", "METHOD_END"], "methodName": ["testFairSchedulerWebAppPage"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebAppFairScheduler"}, {"methodBody": ["METHOD_START", "{", "ClusterMetrics   clusterMetrics    =    ClusterMetrics . getMetrics (  )  ;", "clusterMetrics . incrDecommisionedNMs (  )  ;", "clusterMetrics . incrNumActiveNodes (  )  ;", "clusterMetrics . incrNumLostNMs (  )  ;", "clusterMetrics . incrNumRebootedNMs (  )  ;", "clusterMetrics . incrNumUnhealthyNMs (  )  ;", "}", "METHOD_END"], "methodName": ["initClusterMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testCluster"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" metrics \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterMetricsJSON ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" metrics \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterMetricsJSON ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterMetricsDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" metrics /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterMetricsJSON ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterMetricsSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" metrics \"  )  . accept (  \" application / xml \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "verifyClusterMetricsXML ( xml )  ;", "}", "METHOD_END"], "methodName": ["testClusterMetricsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterSchedulerFifo ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerFifo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterSchedulerFifo ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerFifoDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterSchedulerFifo ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerFifoSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "verifySchedulerFifoXML ( xml )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerFifoXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" info \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" info \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testInfoDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" info /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterInfo ( json )  ;", "}", "METHOD_END"], "methodName": ["testInfoSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" info \"  )  . accept (  \" application / xml \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "verifyClusterInfoXML ( xml )  ;", "}", "METHOD_END"], "methodName": ["testInfoXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . accept ( TEXT _ PLAIN )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( INTERNAL _ SERVER _ ERROR ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidAccept"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" bogus \"  )  . accept ( APPLICATION _ JSON )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidUri"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "String   responseStr    =     \"  \"  ;", "try    {", "responseStr    =    r . accept ( APPLICATION _ JSON )  . get ( String . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   uri \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "TestUtils . checkStringMatch (  \" error   string   exists   and   shouldn ' t \"  ,     \"  \"  ,    responseStr )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidUri2"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" clusterId   doesn ' t   match :     \"  ,    ResourceManager . getClusterTimeStamp (  )  ,    clusterid )  ;", "assertEquals (  \" startedOn   doesn ' t   match :     \"  ,    ResourceManager . getClusterTimeStamp (  )  ,    startedon )  ;", "assertTrue (  (  \" stated   doesn ' t   match :     \"     +    state )  ,    state . matches ( INITED . toString (  )  )  )  ;", "assertTrue (  (  \" HA   state   doesn ' t   match :     \"     +    haState )  ,    haState . matches (  \" INITIALIZING \"  )  )  ;", "TestUtils . checkStringMatch (  \" hadoopVersionBuiltOn \"  ,    VersionInfo . getDate (  )  ,    hadoopVersionBuiltOn )  ;", "TestUtils . checkStringEqual (  \" hadoopBuildVersion \"  ,    VersionInfo . getBuildVersion (  )  ,    hadoopBuildVersion )  ;", "TestUtils . checkStringMatch (  \" hadoopVersion \"  ,    VersionInfo . getVersion (  )  ,    hadoopVersion )  ;", "TestUtils . checkStringMatch (  \" resourceManagerVersionBuiltOn \"  ,    YarnVersionInfo . getDate (  )  ,    resourceManagerVersionBuiltOn )  ;", "TestUtils . checkStringEqual (  \" resourceManagerBuildVersion \"  ,    YarnVersionInfo . getBuildVersion (  )  ,    resourceManagerBuildVersion )  ;", "TestUtils . checkStringMatch (  \" resourceManagerVersion \"  ,    YarnVersionInfo . getVersion (  )  ,    resourceManagerVersion )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   info    =    json . getJSONObject (  \" clusterInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  0  ,    info . length (  )  )  ;", "verifyClusterGeneric ( info . getLong (  \" id \"  )  ,    info . getLong (  \" startedOn \"  )  ,    info . getString (  \" state \"  )  ,    info . getString (  \" haState \"  )  ,    info . getString (  \" hadoopVersionBuiltOn \"  )  ,    info . getString (  \" hadoopBuildVersion \"  )  ,    info . getString (  \" hadoopVersion \"  )  ,    info . getString (  \" ManagerVersionBuiltOn \"  )  ,    info . getString (  \" ManagerBuildVersion \"  )  ,    info . getString (  \" ManagerVersion \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" clusterInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyClusterGeneric ( TestUtils . getXmlLong ( element ,     \" id \"  )  ,    TestUtils . getXmlLong ( element ,     \" startedOn \"  )  ,    TestUtils . getXmlString ( element ,     \" state \"  )  ,    TestUtils . getXmlString ( element ,     \" haState \"  )  ,    TestUtils . getXmlString ( element ,     \" hadoopVersionBuiltOn \"  )  ,    TestUtils . getXmlString ( element ,     \" hadoopBuildVersion \"  )  ,    TestUtils . getXmlString ( element ,     \" hadoopVersion \"  )  ,    TestUtils . getXmlString ( element ,     \" resourceManagerVersionBuiltOn \"  )  ,    TestUtils . getXmlString ( element ,     \" resourceManagerBuildVersion \"  )  ,    TestUtils . getXmlString ( element ,     \" resourceManagerVersion \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyClusterInfoXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "ResourceScheduler   rs    =    TestRMWebServices . rm . getResourceScheduler (  )  ;", "QueueMetrics   metrics    =    rs . getRootQueueMetrics (  )  ;", "ClusterMetrics   clusterMetrics    =    ClusterMetrics . getMetrics (  )  ;", "long   totalMBExpect    =     ( metrics . getAvailableMB (  )  )     +     ( metrics . getAllocatedMB (  )  )  ;", "long   totalVirtualCoresExpect    =     ( metrics . getAvailableVirtualCores (  )  )     +     ( metrics . getAllocatedVirtualCores (  )  )  ;", "assertEquals (  \" appsSubmitted   doesn ' t   match \"  ,    metrics . getAppsSubmitted (  )  ,    submittedApps )  ;", "assertEquals (  \" appsCompleted   doesn ' t   match \"  ,    metrics . getAppsCompleted (  )  ,    completedApps )  ;", "assertEquals (  \" reservedMB   doesn ' t   match \"  ,    metrics . getReservedMB (  )  ,    reservedMB )  ;", "assertEquals (  \" availableMB   doesn ' t   match \"  ,    metrics . getAvailableMB (  )  ,    availableMB )  ;", "assertEquals (  \" allocatedMB   doesn ' t   match \"  ,    metrics . getAllocatedMB (  )  ,    allocMB )  ;", "assertEquals (  \" reservedVirtualCores   doesn ' t   match \"  ,    metrics . getReservedVirtualCores (  )  ,    reservedVirtualCores )  ;", "assertEquals (  \" availableVirtualCores   doesn ' t   match \"  ,    metrics . getAvailableVirtualCores (  )  ,    availableVirtualCores )  ;", "assertEquals (  \" allocatedVirtualCores   doesn ' t   match \"  ,    totalVirtualCoresExpect ,    allocVirtualCores )  ;", "assertEquals (  \" containersAllocated   doesn ' t   match \"  ,     0  ,    containersAlloc )  ;", "assertEquals (  \" totalMB   doesn ' t   match \"  ,    totalMBExpect ,    totalMB )  ;", "assertEquals (  \" totalNodes   doesn ' t   match \"  ,     (  (  (  (  ( clusterMetrics . getNumActiveNMs (  )  )     +     ( clusterMetrics . getNumLostNMs (  )  )  )     +     ( clusterMetrics . getNumDecommisionedNMs (  )  )  )     +     ( clusterMetrics . getNumRebootedNMs (  )  )  )     +     ( clusterMetrics . getUnhealthyNMs (  )  )  )  ,    totalNodes )  ;", "assertEquals (  \" lostNodes   doesn ' t   match \"  ,    clusterMetrics . getNumLostNMs (  )  ,    lostNodes )  ;", "assertEquals (  \" unhealthyNodes   doesn ' t   match \"  ,    clusterMetrics . getUnhealthyNMs (  )  ,    unhealthyNodes )  ;", "assertEquals (  \" decommissionedNodes   doesn ' t   match \"  ,    clusterMetrics . getNumDecommisionedNMs (  )  ,    decommissionedNodes )  ;", "assertEquals (  \" rebootedNodes   doesn ' t   match \"  ,    clusterMetrics . getNumRebootedNMs (  )  ,    rebootedNodes )  ;", "assertEquals (  \" activeNodes   doesn ' t   match \"  ,    clusterMetrics . getNumActiveNMs (  )  ,    activeNodes )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterMetrics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   clusterinfo    =    json . getJSONObject (  \" clusterMetrics \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  3  ,    clusterinfo . length (  )  )  ;", "verifyClusterMetrics ( clusterinfo . getInt (  \" appsSubmitted \"  )  ,    clusterinfo . getInt (  \" appsCompleted \"  )  ,    clusterinfo . getInt (  \" redMB \"  )  ,    clusterinfo . getInt (  \" availableMB \"  )  ,    clusterinfo . getInt (  \" allocatedMB \"  )  ,    clusterinfo . getInt (  \" redVirtualCores \"  )  ,    clusterinfo . getInt (  \" availableVirtualCores \"  )  ,    clusterinfo . getInt (  \" allocatedVirtualCores \"  )  ,    clusterinfo . getInt (  \" totalVirtualCores \"  )  ,    clusterinfo . getInt (  \" containersAllocated \"  )  ,    clusterinfo . getInt (  \" totalMB \"  )  ,    clusterinfo . getInt (  \" totalNodes \"  )  ,    clusterinfo . getInt (  \" lostNodes \"  )  ,    clusterinfo . getInt (  \" unhealthyNodes \"  )  ,    clusterinfo . getInt (  \" decommissionedNodes \"  )  ,    clusterinfo . getInt (  \" rebootedNodes \"  )  ,    clusterinfo . getInt (  \" activeNodes \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterMetricsJSON"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" clusterMetrics \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyClusterMetrics ( TestUtils . getXmlInt ( element ,     \" appsSubmitted \"  )  ,    TestUtils . getXmlInt ( element ,     \" appsCompleted \"  )  ,    TestUtils . getXmlInt ( element ,     \" reservedMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" availableMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" allocatedMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" reservedVirtualCores \"  )  ,    TestUtils . getXmlInt ( element ,     \" availableVirtualCores \"  )  ,    TestUtils . getXmlInt ( element ,     \" allocatedVirtualCores \"  )  ,    TestUtils . getXmlInt ( element ,     \" totalVirtualCores \"  )  ,    TestUtils . getXmlInt ( element ,     \" containersAllocated \"  )  ,    TestUtils . getXmlInt ( element ,     \" totalMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" totalNodes \"  )  ,    TestUtils . getXmlInt ( element ,     \" lostNodes \"  )  ,    TestUtils . getXmlInt ( element ,     \" unhealthyNodes \"  )  ,    TestUtils . getXmlInt ( element ,     \" decommissionedNodes \"  )  ,    TestUtils . getXmlInt ( element ,     \" rebootedNodes \"  )  ,    TestUtils . getXmlInt ( element ,     \" activeNodes \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyClusterMetricsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   info    =    json . getJSONObject (  \" scheduler \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "info    =    info . getJSONObject (  \" schedulerInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  1  ,    info . length (  )  )  ;", "verifyClusterSchedulerFifoGeneric ( info . getString (  \" type \"  )  ,    info . getString (  \" qstate \"  )  ,     (  ( float )     ( info . getDouble (  \" city \"  )  )  )  ,     (  ( float )     ( info . getDouble (  \" usedCity \"  )  )  )  ,    info . getInt (  \" minQueueMemoryCity \"  )  ,    info . getInt (  \" maxQueueMemoryCity \"  )  ,    info . getInt (  \" numNodes \"  )  ,    info . getInt (  \" usedNodeCity \"  )  ,    info . getInt (  \" availNodeCity \"  )  ,    info . getInt (  \" totalNodeCity \"  )  ,    info . getInt (  \" numContainers \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterSchedulerFifo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" type   doesn ' t   match \"  ,     \" fifoScheduler \"  ,    type )  ;", "assertEquals (  \" qstate   doesn ' t   match \"  ,    RUNNING . toString (  )  ,    state )  ;", "assertEquals (  \" city   doesn ' t   match \"  ,     1  .  0  ,    city ,     0  .  0  )  ;", "assertEquals (  \" usedCity   doesn ' t   match \"  ,     0  .  0  ,    usedCity ,     0  .  0  )  ;", "assertEquals (  \" minQueueMemoryCity   doesn ' t   match \"  ,    DEFAULT _ RM _ SCHEDULER _ MINIMUM _ ALLOCATION _ MB ,    minQueueCity )  ;", "assertEquals (  \" maxQueueMemoryCity   doesn ' t   match \"  ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    maxQueueCity )  ;", "assertEquals (  \" numNodes   doesn ' t   match \"  ,     0  ,    numNodes )  ;", "assertEquals (  \" usedNodeCity   doesn ' t   match \"  ,     0  ,    usedNodeCity )  ;", "assertEquals (  \" availNodeCity   doesn ' t   match \"  ,     0  ,    availNodeCity )  ;", "assertEquals (  \" totalNodeCity   doesn ' t   match \"  ,     0  ,    totalNodeCity )  ;", "assertEquals (  \" numContainers   doesn ' t   match \"  ,     0  ,    numContainers )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterSchedulerFifoGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodesSched    =    dom . getElementsByTagName (  \" scheduler \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodesSched . getLength (  )  )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" schedulerInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyClusterSchedulerFifoGeneric ( TestUtils . getXmlAttrString ( element ,     \" xsi : type \"  )  ,    TestUtils . getXmlString ( element ,     \" qstate \"  )  ,    TestUtils . getXmlFloat ( element ,     \" capacity \"  )  ,    TestUtils . getXmlFloat ( element ,     \" usedCapacity \"  )  ,    TestUtils . getXmlInt ( element ,     \" minQueueMemoryCapacity \"  )  ,    TestUtils . getXmlInt ( element ,     \" maxQueueMemoryCapacity \"  )  ,    TestUtils . getXmlInt ( element ,     \" numNodes \"  )  ,    TestUtils . getXmlInt ( element ,     \" usedNodeCapacity \"  )  ,    TestUtils . getXmlInt ( element ,     \" availNodeCapacity \"  )  ,    TestUtils . getXmlInt ( element ,     \" totalNodeCapacity \"  )  ,    TestUtils . getXmlInt ( element ,     \" numContainers \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifySchedulerFifoXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServices"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testAppAttemptsHelper ( app 1  . getApplicationId (  )  . toString (  )  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path ( path )  . path (  \" appattempts \"  )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   jsonAppAttempts    =    json . getJSONObject (  \" appAttempts \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    jsonAppAttempts . length (  )  )  ;", "JSONArray   jsonArray    =    jsonAppAttempts . getJSONArray (  \" appAttempt \"  )  ;", "Collection < RMAppAttempt >    attempts    =    app . getAppAttempts (  )  . values (  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,    attempts . size (  )  ,    jsonArray . length (  )  )  ;", "int   i    =     0  ;", "for    ( RMAppAttempt   attempt    :    attempts )     {", "verifyAppAttemptsInfo ( jsonArray . getJSONObject ( i )  ,    attempt ,    app . getUser (  )  )  ;", "+  + i ;", "}", "}", "METHOD_END"], "methodName": ["testAppAttemptsHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testAppAttemptsHelper (  (  ( app 1  . getApplicationId (  )  . toString (  )  )     +     \"  /  \"  )  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptsSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "String   user    =     \" user 1  \"  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,    user )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path ( app 1  . getApplicationId (  )  . toString (  )  )  . path (  \" appattempts \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" appAttempts \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "NodeList   attempt    =    dom . getElementsByTagName (  \" appAttempt \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    attempt . getLength (  )  )  ;", "verifyAppAttemptsXML ( attempt ,    app 1  . getCurrentAppAttempt (  )  ,    user )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemptsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testAppAttemptsHelper (  (  ( app 1  . getApplicationId (  )  . toString (  )  )     +     \"  /  \"  )  ,    app 1  ,     \"  \"  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppAttemtpsDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "try    {", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     4  0  9  6  )  ;", "Thread . sleep (  1  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,     2  ,    null ,     \" MAPREDUCE \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "MockAM   am    =    TestRMWebServicesApps . rm . sendAMLaunched ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "amNodeManager . nodeHeartbeat ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,     2  ,    null ,     \" MAPREDUCE \"  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,     2  ,    null ,     \" OTHER \"  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" appstatistics \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   appsStatInfo    =    json . getJSONObject (  \" appStatInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appsStatInfo . length (  )  )  ;", "JSONArray   statItems    =    appsStatInfo . getJSONArray (  \" statItem \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,    YarnApplicationState . values (  )  . length ,    statItems . length (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( YarnApplicationState . values (  )  . length )  ;     +  + i )     {", "assertEquals (  \"  *  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" type \"  )  )  ;", "if    ( statItems . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )     {", "assertEquals (  \"  2  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "} else", "if    ( statItems . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" FINISHED \"  )  )     {", "assertEquals (  \"  1  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "} else    {", "assertEquals (  \"  0  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "}", "}", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" appstatistics \"  )  . queryParam (  \" states \"  ,    ACCEPTED . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "appsStatInfo    =    json . getJSONObject (  \" appStatInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appsStatInfo . length (  )  )  ;", "statItems    =    appsStatInfo . getJSONArray (  \" statItem \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    statItems . length (  )  )  ;", "assertEquals (  \" ACCEPTED \"  ,    statItems . getJSONObject (  0  )  . getString (  \" state \"  )  )  ;", "assertEquals (  \"  *  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" type \"  )  )  ;", "assertEquals (  \"  2  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" appstatistics \"  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "appsStatInfo    =    json . getJSONObject (  \" appStatInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appsStatInfo . length (  )  )  ;", "statItems    =    appsStatInfo . getJSONArray (  \" statItem \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,    YarnApplicationState . values (  )  . length ,    statItems . length (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( YarnApplicationState . values (  )  . length )  ;     +  + i )     {", "assertEquals (  \" mapreduce \"  ,    statItems . getJSONObject (  0  )  . getString (  \" type \"  )  )  ;", "if    ( statItems . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )     {", "assertEquals (  \"  1  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "} else", "if    ( statItems . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" FINISHED \"  )  )     {", "assertEquals (  \"  1  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "} else    {", "assertEquals (  \"  0  \"  ,    statItems . getJSONObject (  0  )  . getString (  \" count \"  )  )  ;", "}", "}", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" appstatistics \"  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE , OTHER \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   exception    =    json . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   className    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" we   temporarily   support   at   most   one   applicationType \"  ,    message )  ;", "WebServicesTestUtils . checkStringEqual (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringEqual (  \" exception   className \"  ,     \" webapp . BadRequestException \"  ,    className )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" appstatistics \"  )  . queryParam (  \" states \"  ,     (  (  ( FINISHED . toString (  )  )     +     \"  ,  \"  )     +     ( ACCEPTED . toString (  )  )  )  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "appsStatInfo    =    json . getJSONObject (  \" appStatInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    appsStatInfo . length (  )  )  ;", "statItems    =    appsStatInfo . getJSONArray (  \" statItem \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    statItems . length (  )  )  ;", "JSONObject   statItem 1     =    statItems . getJSONObject (  0  )  ;", "JSONObject   statItem 2     =    statItems . getJSONObject (  1  )  ;", "assertTrue (  (  (  ( statItem 1  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )     &  &     ( statItem 2  . getString (  \" state \"  )  . equals (  \" FINISHED \"  )  )  )     |  |     (  ( statItem 2  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )     &  &     ( statItem 1  . getString (  \" state \"  )  . equals (  \" FINISHED \"  )  )  )  )  )  ;", "assertEquals (  \" mapreduce \"  ,    statItem 1  . getString (  \" type \"  )  )  ;", "assertEquals (  \"  1  \"  ,    statItem 1  . getString (  \" count \"  )  )  ;", "assertEquals (  \" mapreduce \"  ,    statItem 2  . getString (  \" type \"  )  )  ;", "assertEquals (  \"  1  \"  ,    statItem 2  . getString (  \" count \"  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" appstatistics \"  )  . queryParam (  \" states \"  ,     \" wrong _ state \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "exception    =    json . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "message    =    exception . getString (  \" message \"  )  ;", "type    =    exception . getString (  \" exception \"  )  ;", "className    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" Invalid   application - state   wrong _ state \"  ,    message )  ;", "WebServicesTestUtils . checkStringEqual (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringEqual (  \" exception   className \"  ,     \" webapp . BadRequestException \"  ,    className )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppStatistics"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testAppsHelper (  \" apps \"  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testAppsHelper (  \" apps /  \"  ,    app 1  ,     \"  \"  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path ( path )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "verifyAppInfo ( array . getJSONObject (  0  )  ,    app )  ;", "}", "METHOD_END"], "methodName": ["testAppsHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "Thread . sleep (  1  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "MockAM   am    =    TestRMWebServicesApps . rm . sendAMLaunched ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "amNodeManager . nodeHeartbeat ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,     2  ,    null ,     \" MAPREDUCE \"  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \"  \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  ,    null ,    false ,    null ,     2  ,    null ,     \" NON - YARN \"  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "assertEquals (  \" MAPREDUCE \"  ,    array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \" YARN \"  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "assertTrue (  (  (  ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" MAPREDUCE \"  )  )  )     |  |     (  ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" MAPREDUCE \"  )  )  )  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \" YARN , NON - YARN \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "assertTrue (  (  (  ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" NON - YARN \"  )  )  )     |  |     (  ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" NON - YARN \"  )  )  )  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \"  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    array . length (  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \" YARN , NON - YARN \"  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    array . length (  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \" YARN \"  )  . queryParam (  \" applicationTypes \"  ,     \"  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "assertEquals (  \" YARN \"  ,    array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \"  ,  ,  ,     ,  ,    YARN    ,  ,     ,  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "assertEquals (  \" YARN \"  ,    array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \"  ,  ,  ,     ,  ,        ,  ,     ,  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    array . length (  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \" YARN ,     , NON - YARN ,     ,  ,  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "assertTrue (  (  (  ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" NON - YARN \"  )  )  )     |  |     (  ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" NON - YARN \"  )  )  )  )  )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" applicationTypes \"  ,     \"    YARN ,     ,        ,  ,  ,  \"  )  . queryParam (  \" applicationTypes \"  ,     \" MAPREDUCE    ,     ,  ,     ,  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "assertTrue (  (  (  ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" MAPREDUCE \"  )  )  )     |  |     (  ( array . getJSONObject (  1  )  . getString (  \" applicationType \"  )  . equals (  \" YARN \"  )  )     &  &     ( array . getJSONObject (  0  )  . getString (  \" applicationType \"  )  . equals (  \" MAPREDUCE \"  )  )  )  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryAppTypes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" finalStatus \"  ,    UNDEFINED . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "System . out . println ( json . toString (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "verifyAppInfo ( array . getJSONObject (  0  )  ,    app 1  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryFinalStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" finalStatus \"  ,     \" INVALID _ test \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   state   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" api . records . FinalApplicationStatus . INVALID _ test \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" IllegalArgumentException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" IllegalArgumentException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppsQueryFinalStatusInvalid"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" finalStatus \"  ,    KILLED . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "assertEquals (  \" apps   is   not   null \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryFinalStatusNone"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "MockAM   am    =    TestRMWebServicesApps . rm . sendAMLaunched ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "amNodeManager . nodeHeartbeat ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" finishedTimeBegin \"  ,    String . valueOf ( start )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryFinishBegin"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "MockAM   am    =    TestRMWebServicesApps . rm . sendAMLaunched ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "amNodeManager . nodeHeartbeat ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "long   end    =    System . currentTimeMillis (  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" finishedTimeBegin \"  ,    String . valueOf ( start )  )  . queryParam (  \" finishedTimeEnd \"  ,    String . valueOf ( end )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryFinishBeginEnd"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "MockAM   am    =    TestRMWebServicesApps . rm . sendAMLaunched ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  )  ;", "am . registerAppAttempt (  )  ;", "am . unregisterAppAttempt (  )  ;", "amNodeManager . nodeHeartbeat ( app 1  . getCurrentAppAttempt (  )  . getAppAttemptId (  )  ,     1  ,    COMPLETE )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "long   end    =    System . currentTimeMillis (  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" finishedTimeEnd \"  ,    String . valueOf ( end )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryFinishEnd"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" limit \"  ,     \"  2  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" queue \"  ,     \" default \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" startedTimeBegin \"  ,    String . valueOf ( start )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStartBegin"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "long   end    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" startedTimeBegin \"  ,    String . valueOf ( start )  )  . queryParam (  \" startedTimeEnd \"  ,    String . valueOf ( end )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStartBeginEnd"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "long   start    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" startedTimeBegin \"  ,    String . valueOf ( start )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStartBeginSome"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "long   end    =    System . currentTimeMillis (  )  ;", "Thread . sleep (  1  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" startedTimeEnd \"  ,    String . valueOf ( end )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "assertEquals (  \" apps   is   not   null \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStartEnd"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,    ACCEPTED . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "verifyAppInfo ( array . getJSONObject (  0  )  ,    app 1  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,     \" INVALID _ test \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   state   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" Invalid   application - state   INVALID _ test \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" webapp . BadRequestException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppsQueryStateInvalid"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" state \"  ,    RUNNING . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "assertEquals (  \" apps   is   not   null \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStateNone"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "RMApp   killedApp    =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . killApp ( killedApp . getApplicationId (  )  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "MultivaluedMapImpl   params    =    new   MultivaluedMapImpl (  )  ;", "params . add (  \" states \"  ,    ACCEPTED . toString (  )  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParams ( params )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "assertEquals (  \" state   not   equal   to   ACCEPTED \"  ,     \" ACCEPTED \"  ,    array . getJSONObject (  0  )  . getString (  \" state \"  )  )  ;", "r    =    resource (  )  ;", "params    =    new   MultivaluedMapImpl (  )  ;", "params . add (  \" states \"  ,    ACCEPTED . toString (  )  )  ;", "params . add (  \" states \"  ,    KILLED . toString (  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParams ( params )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "assertTrue (  \" both   app   states   of   ACCEPTED   and   KILLED   are   not   present \"  ,     (  (  ( array . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" state \"  )  . equals (  \" KILLED \"  )  )  )     |  |     (  ( array . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" KILLED \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )  )  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStates"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "RMApp   killedApp    =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . killApp ( killedApp . getApplicationId (  )  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "MultivaluedMapImpl   params    =    new   MultivaluedMapImpl (  )  ;", "params . add (  \" states \"  ,    ACCEPTED . toString (  )  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParams ( params )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    array . length (  )  )  ;", "assertEquals (  \" state   not   equal   to   ACCEPTED \"  ,     \" ACCEPTED \"  ,    array . getJSONObject (  0  )  . getString (  \" state \"  )  )  ;", "r    =    resource (  )  ;", "params    =    new   MultivaluedMapImpl (  )  ;", "params . add (  \" states \"  ,     (  (  ( ACCEPTED . toString (  )  )     +     \"  ,  \"  )     +     ( KILLED . toString (  )  )  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParams ( params )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "assertTrue (  \" both   app   states   of   ACCEPTED   and   KILLED   are   not   present \"  ,     (  (  ( array . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" state \"  )  . equals (  \" KILLED \"  )  )  )     |  |     (  ( array . getJSONObject (  0  )  . getString (  \" state \"  )  . equals (  \" KILLED \"  )  )     &  &     ( array . getJSONObject (  1  )  . getString (  \" state \"  )  . equals (  \" ACCEPTED \"  )  )  )  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStatesComma"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" states \"  ,     \" INVALID _ test \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   state   query \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" Invalid   application - state   INVALID _ test \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" BadRequestException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" webapp . BadRequestException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAppsQueryStatesInvalid"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" states \"  ,    RUNNING . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "assertEquals (  \" apps   is   not   null \"  ,    NULL ,    json . get (  \" apps \"  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryStatesNone"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . queryParam (  \" user \"  ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   apps    =    json . getJSONObject (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    apps . length (  )  )  ;", "JSONArray   array    =    apps . getJSONArray (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    array . length (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsQueryUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testAppsHelper (  \" apps /  \"  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodesApps    =    dom . getElementsByTagName (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodesApps . getLength (  )  )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "verifyAppsXML ( nodes ,    app 1  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "TestRMWebServicesApps . rm . submitApp (  2  0  4  8  ,     \" testwordcount 2  \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodesApps    =    dom . getElementsByTagName (  \" apps \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodesApps . getLength (  )  )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    nodes . getLength (  )  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppsXMLMulti"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path (  \" application _ invalid _  1  2  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   appid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" For   input   string :     \\  \" invalid \\  \"  \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NumberFormatException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" NumberFormatException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path (  \" application _ invalid _  1  2  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   appid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" For   input   string :     \\  \" invalid \\  \"  \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NumberFormatException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" NumberFormatException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     8  1  9  2  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "MockAM   am    =    MockRM . launchAndRegisterAM ( app 1  ,    TestRMWebServicesApps . rm ,    amNodeManager )  ;", "int   maxAppAttempts    =    TestRMWebServicesApps . rm . getConfig (  )  . getInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "assertTrue (  ( maxAppAttempts    >     1  )  )  ;", "int   numAttempt    =     1  ;", "while    ( true )     {", "amNodeManager . nodeHeartbeat ( am . getApplicationAttemptId (  )  ,     1  ,    COMPLETE )  ;", "am . waitForState ( RMAppAttemptState . FAILED )  ;", "if    ( numAttempt    =  =    maxAppAttempts )     {", "TestRMWebServicesApps . rm . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . FAILED )  ;", "break ;", "}", "TestRMWebServicesApps . rm . waitForState ( app 1  . getApplicationId (  )  ,    RMAppState . ACCEPTED )  ;", "am    =    MockRM . launchAndRegisterAM ( app 1  ,    TestRMWebServicesApps . rm ,    amNodeManager )  ;", "numAttempt +  +  ;", "}", "assertEquals (  \" incorrect   number   of   attempts \"  ,    maxAppAttempts ,    app 1  . getAppAttempts (  )  . values (  )  . size (  )  )  ;", "testAppAttemptsHelper ( app 1  . getApplicationId (  )  . toString (  )  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testMultipleAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path (  \" application _  0  0  0  0  0  _  0  0  9  9  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   appid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    app   with   id :    application _  0  0  0  0  0  _  0  0  9  9    not   found \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NotFoundException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" webapp . NotFoundException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonexistApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path (  \" application _  0  0  0  0  0  _  0  0  9  9  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   invalid   appid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Exception :    app   with   id :    application _  0  0  0  0  0  _  0  0  9  9    not   found \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" NotFoundException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" webapp . NotFoundException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonexistAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testSingleAppsHelper ( app 1  . getApplicationId (  )  . toString (  )  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleApp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testSingleAppsHelper (  (  ( app 1  . getApplicationId (  )  . toString (  )  )     +     \"  /  \"  )  ,    app 1  ,     \"  \"  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleAppsDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path ( path )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "verifyAppInfo ( json . getJSONObject (  \" app \"  )  ,    app )  ;", "}", "METHOD_END"], "methodName": ["testSingleAppsHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "testSingleAppsHelper (  (  ( app 1  . getApplicationId (  )  . toString (  )  )     +     \"  /  \"  )  ,    app 1  ,    APPLICATION _ JSON )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleAppsSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesApps . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesApps . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "RMApp   app 1     =    TestRMWebServicesApps . rm . submitApp ( TestRMWebServicesApps . CONTAINER _ MB ,     \" testwordcount \"  ,     \" user 1  \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" apps \"  )  . path ( app 1  . getApplicationId (  )  . toString (  )  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" app \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "verifyAppsXML ( nodes ,    app 1  )  ;", "TestRMWebServicesApps . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleAppsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" id   doesn ' t   match \"  ,    appAttempt . getAppAttemptId (  )  . getAttemptId (  )  ,    id )  ;", "assertEquals (  \" startedTime   doesn ' t   match \"  ,    appAttempt . getStartTime (  )  ,    startTime )  ;", "TestUtils . checkStringMatch (  \" containerId \"  ,    appAttempt . getMasterContainer (  )  . getId (  )  . toString (  )  ,    containerId )  ;", "TestUtils . checkStringMatch (  \" nodeHttpAddress \"  ,    appAttempt . getMasterContainer (  )  . getNodeHttpAddress (  )  ,    nodeHttpAddress )  ;", "TestUtils . checkStringMatch (  \" nodeId \"  ,    appAttempt . getMasterContainer (  )  . getNodeId (  )  . toString (  )  ,    nodeId )  ;", "assertTrue (  \" logsLink   doesn ' t   match \"  ,    logsLink . startsWith (  \"  /  /  \"  )  )  ;", "assertTrue (  \" logsLink   doesn ' t   contain   user   info \"  ,    logsLink . endsWith (  (  \"  /  \"     +    user )  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyAppAttemptInfoGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     6  ,    info . length (  )  )  ;", "verifyAppAttemptInfoGeneric ( appAttempt ,    info . getInt (  \" id \"  )  ,    info . getLong (  \" startTime \"  )  ,    info . getString (  \" containerId \"  )  ,    info . getString (  \" nodeHttpAddress \"  )  ,    info . getString (  \" nodeId \"  )  ,    info . getString (  \" logsLink \"  )  ,    user )  ;", "}", "METHOD_END"], "methodName": ["verifyAppAttemptsInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyAppAttemptInfoGeneric ( appAttempt ,    TestUtils . getXmlInt ( element ,     \" id \"  )  ,    TestUtils . getXmlLong ( element ,     \" startTime \"  )  ,    TestUtils . getXmlString ( element ,     \" containerId \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeHttpAddress \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeId \"  )  ,    TestUtils . getXmlString ( element ,     \" logsLink \"  )  ,    user )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyAppAttemptsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  4  ,    info . length (  )  )  ;", "verifyAppInfoGeneric ( app ,    info . getString (  \" id \"  )  ,    info . getString (  \" user \"  )  ,    info . getString (  \" name \"  )  ,    info . getString (  \" applicationType \"  )  ,    info . getString (  \" queue \"  )  ,    info . getString (  \" state \"  )  ,    info . getString (  \" finalStatus \"  )  ,     (  ( float )     ( info . getDouble (  \" progress \"  )  )  )  ,    info . getString (  \" trackingUI \"  )  ,    info . getString (  \" diagnostics \"  )  ,    info . getLong (  \" clusterId \"  )  ,    info . getLong (  \" startedTime \"  )  ,    info . getLong (  \" finishedTime \"  )  ,    info . getLong (  \" elapsedTime \"  )  ,    info . getString (  \" amHostHttpAddress \"  )  ,    info . getString (  \" amContainerLogs \"  )  ,    info . getInt (  \" allocatedMB \"  )  ,    info . getInt (  \" allocatedVCores \"  )  ,    info . getInt (  \" runningContainers \"  )  ,    info . getInt (  \" preemptedRMB \"  )  ,    info . getInt (  \" preemptedRVCores \"  )  ,    info . getInt (  \" numNonAMContainerPreempted \"  )  ,    info . getInt (  \" numAMContainerPreempted \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyAppInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "WebServicesTestUtils . checkStringMatch (  \" id \"  ,    app . getApplicationId (  )  . toString (  )  ,    id )  ;", "WebServicesTestUtils . checkStringMatch (  \" user \"  ,    app . getUser (  )  ,    user )  ;", "WebServicesTestUtils . checkStringMatch (  \" name \"  ,    app . getName (  )  ,    name )  ;", "WebServicesTestUtils . checkStringMatch (  \" applicationType \"  ,    app . getApplicationType (  )  ,    applicationType )  ;", "WebServicesTestUtils . checkStringMatch (  \" queue \"  ,    app . getQueue (  )  ,    queue )  ;", "WebServicesTestUtils . checkStringMatch (  \" state \"  ,    app . getState (  )  . toString (  )  ,    state )  ;", "WebServicesTestUtils . checkStringMatch (  \" finalStatus \"  ,    app . getFinalApplicationStatus (  )  . toString (  )  ,    finalStatus )  ;", "assertEquals (  \" progress   doesn ' t   match \"  ,     0  ,    progress ,     0  .  0  )  ;", "WebServicesTestUtils . checkStringMatch (  \" trackingUI \"  ,     \" UNASSIGNED \"  ,    trackingUI )  ;", "WebServicesTestUtils . checkStringMatch (  \" diagnostics \"  ,    app . getDiagnostics (  )  . toString (  )  ,    diagnostics )  ;", "assertEquals (  \" clusterId   doesn ' t   match \"  ,    ResourceManager . getClusterTimeStamp (  )  ,    clusterId )  ;", "assertEquals (  \" startedTime   doesn ' t   match \"  ,    app . getStartTime (  )  ,    startedTime )  ;", "assertEquals (  \" finishedTime   doesn ' t   match \"  ,    app . getFinishTime (  )  ,    finishedTime )  ;", "assertTrue (  \" elapsed   time   not   greater   than    0  \"  ,     ( elapsedTime    >     0  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" amHostHttpAddress \"  ,    app . getCurrentAppAttempt (  )  . getMasterContainer (  )  . getNodeHttpAddress (  )  ,    amHostHttpAddress )  ;", "assertTrue (  \" amContainerLogs   doesn ' t   match \"  ,    amContainerLogs . startsWith (  \" http :  /  /  \"  )  )  ;", "assertTrue (  \" amContainerLogs   doesn ' t   contain   user   info \"  ,    amContainerLogs . endsWith (  (  \"  /  \"     +     ( app . getUser (  )  )  )  )  )  ;", "assertEquals (  \" allocatedMB   doesn ' t   match \"  ,     1  0  2  4  ,    allocatedMB )  ;", "assertEquals (  \" allocatedVCores   doesn ' t   match \"  ,     1  ,    allocatedVCores )  ;", "assertEquals (  \" numContainers   doesn ' t   match \"  ,     1  ,    numContainers )  ;", "assertEquals (  \" preemptedResourceMB   doesn ' t   match \"  ,    app . getRMAppMetrics (  )  . getResourcePreempted (  )  . getMemory (  )  ,    preemptedResourceMB )  ;", "assertEquals (  \" preemptedResourceVCores   doesn ' t   match \"  ,    app . getRMAppMetrics (  )  . getResourcePreempted (  )  . getVirtualCores (  )  ,    preemptedResourceVCores )  ;", "assertEquals (  \" numNonAMContainerPreempted   doesn ' t   match \"  ,    app . getRMAppMetrics (  )  . getNumNonAMContainersPreempted (  )  ,    numNonAMContainerPreempted )  ;", "assertEquals (  \" numAMContainerPreempted   doesn ' t   match \"  ,    app . getRMAppMetrics (  )  . getNumAMContainersPreempted (  )  ,    numAMContainerPreempted )  ;", "}", "METHOD_END"], "methodName": ["verifyAppInfoGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyAppInfoGeneric ( app ,    TestUtils . getXmlString ( element ,     \" id \"  )  ,    TestUtils . getXmlString ( element ,     \" user \"  )  ,    TestUtils . getXmlString ( element ,     \" name \"  )  ,    TestUtils . getXmlString ( element ,     \" applicationType \"  )  ,    TestUtils . getXmlString ( element ,     \" queue \"  )  ,    TestUtils . getXmlString ( element ,     \" state \"  )  ,    TestUtils . getXmlString ( element ,     \" finalStatus \"  )  ,    TestUtils . getXmlFloat ( element ,     \" progress \"  )  ,    TestUtils . getXmlString ( element ,     \" trackingUI \"  )  ,    TestUtils . getXmlString ( element ,     \" diagnostics \"  )  ,    TestUtils . getXmlLong ( element ,     \" clusterId \"  )  ,    TestUtils . getXmlLong ( element ,     \" startedTime \"  )  ,    TestUtils . getXmlLong ( element ,     \" finishedTime \"  )  ,    TestUtils . getXmlLong ( element ,     \" elapsedTime \"  )  ,    TestUtils . getXmlString ( element ,     \" amHostHttpAddress \"  )  ,    TestUtils . getXmlString ( element ,     \" amContainerLogs \"  )  ,    TestUtils . getXmlInt ( element ,     \" allocatedMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" allocatedVCores \"  )  ,    TestUtils . getXmlInt ( element ,     \" runningContainers \"  )  ,    TestUtils . getXmlInt ( element ,     \" preemptedResourceMB \"  )  ,    TestUtils . getXmlInt ( element ,     \" preemptedResourceVCores \"  )  ,    TestUtils . getXmlInt ( element ,     \" numNonAMContainerPreempted \"  )  ,    TestUtils . getXmlInt ( element ,     \" numAMContainerPreempted \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyAppsXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesApps"}, {"methodBody": ["METHOD_START", "{", "StringWriter   sw    =    new   StringWriter (  )  ;", "JSONJAXBContext   ctx    =    new   JSONJAXBContext ( State . class )  ;", "JSONMarshaller   jm    =    ctx . createJSONMarshaller (  )  ;", "jm . marshallToJSON ( state ,    sw )  ;", "return   sw . toString (  )  ;", "}", "METHOD_END"], "methodName": ["appStateToJSON"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "WebResource   rt    =    r ;", "for    ( String   path    :    paths )     {", "rt    =    rt . path ( path )  ;", "}", "if    ( isAuthenticationEnabled (  )  )     {", "rt    =    rt . queryParam (  \" user . name \"  ,    webserviceUserName )  ;", "}", "return   rt ;", "}", "METHOD_END"], "methodName": ["constructWebResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "WebResource   ws    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  ;", "return   this . constructWebResource ( ws ,    paths )  ;", "}", "METHOD_END"], "methodName": ["constructWebResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "return   Guice . createInjector ( new   TestRMWebServicesAppsModification . TestServletModule (  )     {", "@ Override", "protected   void   configureServlets (  )     {", "setAuthFilter    =    false ;", "super . configureServlets (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getNoAuthInjector"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "return   Guice . createInjector ( new   TestRMWebServicesAppsModification . TestServletModule (  )     {", "@ Override", "protected   void   configureServlets (  )     {", "setAuthFilter    =    true ;", "conf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "conf . setStrings ( YARN _ ADMIN _ ACL ,     \" testuser 1  \"  )  ;", "super . configureServlets (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getSimpleAuthInjector"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {     0     }  ,    new   Object [  ]  {     1     }     }  )  ;", "}", "METHOD_END"], "methodName": ["guiceConfigs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "return   setAuthFilter ;", "}", "METHOD_END"], "methodName": ["isAuthenticationEnabled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "String   lrKey    =     \" example \"  ;", "String   queueName    =     \" testqueue \"  ;", "String   appName    =     \" test \"  ;", "String   appType    =     \" test - type \"  ;", "String   urlPath    =     \" apps \"  ;", "String   appId    =    testGetNewApplication ( acceptMedia )  ;", "List < String >    commands    =    new   ArrayList < String >  (  )  ;", "commands . add (  \"  / bin / sleep    5  \"  )  ;", "HashMap < String ,    String >    environment    =    new   HashMap < String ,    String >  (  )  ;", "environment . put (  \" APP _ VAR \"  ,     \" ENV _ SETTING \"  )  ;", "HashMap < ApplicationAccessType ,    String >    acls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "acls . put ( MODIFY _ APP ,     \" testuser 1  ,    testuser 2  \"  )  ;", "acls . put ( VIEW _ APP ,     \" testuser 3  ,    testuser 4  \"  )  ;", "Set < String >    tags    =    new   HashSet < String >  (  )  ;", "tags . add (  \" tag 1  \"  )  ;", "tags . add (  \" tag    2  \"  )  ;", "CredentialsInfo   credentials    =    new   CredentialsInfo (  )  ;", "HashMap < String ,    String >    tokens    =    new   HashMap < String ,    String >  (  )  ;", "HashMap < String ,    String >    secrets    =    new   HashMap < String ,    String >  (  )  ;", "secrets . put (  \" secret 1  \"  ,    Base 6  4  . encodeBase 6  4 URLSafeString (  \" secret 1  \"  . getBytes (  \" UTF 8  \"  )  )  )  ;", "credentials . setSecrets ( secrets )  ;", "credentials . setTokens ( tokens )  ;", "ApplicationSubmissionContextInfo   appInfo    =    new   ApplicationSubmissionContextInfo (  )  ;", "appInfo . setApplicationId ( appId )  ;", "appInfo . setApplicationName ( appName )  ;", "appInfo . setPriority (  3  )  ;", "appInfo . setMaxAppAttempts (  2  )  ;", "appInfo . setQueue ( queueName )  ;", "appInfo . setApplicationType ( appType )  ;", "HashMap < String ,    LocalResourceInfo >    lr    =    new   HashMap < String ,    LocalResourceInfo >  (  )  ;", "LocalResourceInfo   y    =    new   LocalResourceInfo (  )  ;", "y . setUrl ( new   URI (  \" http :  /  / www . test . com / file . txt \"  )  )  ;", "y . setSize (  1  0  0  )  ;", "y . setTimestamp ( System . currentTimeMillis (  )  )  ;", "y . setType ( FILE )  ;", "y . setVisibility ( APPLICATION )  ;", "lr . put ( lrKey ,    y )  ;", "appInfo . getContainerLaunchContextInfo (  )  . setResources ( lr )  ;", "appInfo . getContainerLaunchContextInfo (  )  . setCommands ( commands )  ;", "appInfo . getContainerLaunchContextInfo (  )  . setEnvironment ( environment )  ;", "appInfo . getContainerLaunchContextInfo (  )  . setAcls ( acls )  ;", "appInfo . getContainerLaunchContextInfo (  )  . getAuxillaryServiceData (  )  . put (  \" test \"  ,    Base 6  4  . encodeBase 6  4 URLSafeString (  \" value 1  2  \"  . getBytes (  \" UTF 8  \"  )  )  )  ;", "appInfo . getContainerLaunchContextInfo (  )  . setCredentials ( credentials )  ;", "appInfo . getResource (  )  . setMemory (  1  0  2  4  )  ;", "appInfo . getResource (  )  . setvCores (  1  )  ;", "appInfo . setApplicationTags ( tags )  ;", "ClientResponse   response    =    this . constructWebResource ( urlPath )  . accept ( acceptMedia )  . entity ( appInfo ,    contentMedia )  . post ( ClientResponse . class )  ;", "if    (  ( this . isAuthenticationEnabled (  )  )     =  =    false )     {", "assertEquals ( UNAUTHORIZED ,    response . getClientResponseStatus (  )  )  ;", "return ;", "}", "assertEquals ( ACCEPTED ,    response . getClientResponseStatus (  )  )  ;", "assertTrue (  (  ( response . getHeaders (  )  . getFirst ( LOCATION )  . isEmpty (  )  )     =  =    false )  )  ;", "String   locURL    =    response . getHeaders (  )  . getFirst ( LOCATION )  ;", "assertTrue (  (  ( locURL . indexOf (  \"  / apps / application \"  )  )     !  =     (  -  1  )  )  )  ;", "appId    =    locURL . substring (  (  ( locURL . indexOf (  \"  / apps /  \"  )  )     +     (  \"  / apps /  \"  . length (  )  )  )  )  ;", "WebResource   res    =    resource (  )  . uri ( new   URI ( locURL )  )  ;", "res    =    res . queryParam (  \" user . name \"  ,    webserviceUserName )  ;", "response    =    res . get ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "RMApp   app    =     . rm . getRMContext (  )  . getRMApps (  )  . get ( ConverterUtils . toApplicationId ( appId )  )  ;", "assertEquals ( appName ,    app . getName (  )  )  ;", "assertEquals ( webserviceUserName ,    app . getUser (  )  )  ;", "assertEquals (  2  ,    app . getMaxAppAttempts (  )  )  ;", "assertEquals ( queueName ,    app . getQueue (  )  )  ;", "assertEquals ( appType ,    app . getApplicationType (  )  )  ;", "assertEquals ( tags ,    app . getApplicationTags (  )  )  ;", "ContainerLaunchContext   ctx    =    app . getApplicationSubmissionContext (  )  . getAMContainerSpec (  )  ;", "assertEquals ( commands ,    ctx . getCommands (  )  )  ;", "assertEquals ( environment ,    ctx . getEnvironment (  )  )  ;", "assertEquals ( acls ,    ctx . getApplicationACLs (  )  )  ;", "Map < String ,    LocalResource >    appLRs    =    ctx . getLocalResources (  )  ;", "assertTrue ( appLRs . containsKey ( lrKey )  )  ;", "LocalResource   exampleLR    =    appLRs . get ( lrKey )  ;", "assertEquals ( ConverterUtils . getYarnUrlFromURI ( y . getUrl (  )  )  ,    exampleLR . getResource (  )  )  ;", "assertEquals ( y . getSize (  )  ,    exampleLR . getSize (  )  )  ;", "assertEquals ( y . getTimestamp (  )  ,    exampleLR . getTimestamp (  )  )  ;", "assertEquals ( y . getType (  )  ,    exampleLR . getType (  )  )  ;", "assertEquals ( y . getPattern (  )  ,    exampleLR . getPattern (  )  )  ;", "assertEquals ( y . getVisibility (  )  ,    exampleLR . getVisibility (  )  )  ;", "response    =    this . constructWebResource (  \" apps \"  ,    appId )  . accept ( acceptMedia )  . get ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testAppSubmit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "String   urlPath    =     \" apps \"  ;", ". rm . start (  )  ;", "MockNM   amNodeManager    =     . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "ApplicationSubmissionContextInfo   appInfo    =    new   ApplicationSubmissionContextInfo (  )  ;", "appInfo . setApplicationName (  \" test \"  )  ;", "appInfo . setPriority (  3  )  ;", "appInfo . setMaxAppAttempts (  2  )  ;", "appInfo . setQueue (  \" testqueue \"  )  ;", "appInfo . setApplicationType (  \" test - type \"  )  ;", "HashMap < String ,    LocalResourceInfo >    lr    =    new   HashMap < String ,    LocalResourceInfo >  (  )  ;", "LocalResourceInfo   y    =    new   LocalResourceInfo (  )  ;", "y . setUrl ( new   URI (  \" http :  /  / www . test . com / file . txt \"  )  )  ;", "y . setSize (  1  0  0  )  ;", "y . setTimestamp ( System . currentTimeMillis (  )  )  ;", "y . setType ( FILE )  ;", "y . setVisibility ( APPLICATION )  ;", "lr . put (  \" example \"  ,    y )  ;", "appInfo . getContainerLaunchContextInfo (  )  . setResources ( lr )  ;", "appInfo . getResource (  )  . setMemory (  1  0  2  4  )  ;", "appInfo . getResource (  )  . setvCores (  1  )  ;", "String   body    =     \"  <  ? xml   version =  \\  \"  1  .  0  \\  \"    encoding =  \\  \" UTF -  8  \\  \"     \"     +     \" standalone =  \\  \" yes \\  \"  ?  >  < blah /  >  \"  ;", "ClientResponse   response    =    this . constructWebResource ( urlPath )  . accept ( APPLICATION _ XML )  . entity ( body ,    APPLICATION _ XML )  . post ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "body    =     \"  {  \\  \" a \\  \"     :     \\  \" b \\  \"  }  \"  ;", "response    =    this . constructWebResource ( urlPath )  . accept ( APPLICATION _ XML )  . entity ( body ,    APPLICATION _ JSON )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    BAD _ REQUEST )  ;", ". rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testAppSubmitBadJsonAndXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "String   urlPath    =     \" apps \"  ;", "String   appId    =     \"  \"  ;", "ApplicationSubmissionContextInfo   appInfo    =    new   ApplicationSubmissionContextInfo (  )  ;", "ClientResponse   response    =    this . constructWebResource ( urlPath )  . accept ( acceptMedia )  . entity ( appInfo ,    contentMedia )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    BAD _ REQUEST )  ;", "appId    =     \" random \"  ;", "appInfo . setApplicationId ( appId )  ;", "response    =    this . constructWebResource ( urlPath )  . accept ( acceptMedia )  . entity ( appInfo ,    contentMedia )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    BAD _ REQUEST )  ;", "appId    =     \" random _ junk \"  ;", "appInfo . setApplicationId ( appId )  ;", "response    =    this . constructWebResource ( urlPath )  . accept ( acceptMedia )  . entity ( appInfo ,    contentMedia )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    BAD _ REQUEST )  ;", "appInfo . getResource (  )  . setMemory (  (  (  . rm . getConfig (  )  . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ MB )  )     +     1  )  )  ;", "appInfo . getResource (  )  . setvCores (  1  )  ;", "response    =    this . constructWebResource ( urlPath )  . accept ( acceptMedia )  . entity ( appInfo ,    contentMedia )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    BAD _ REQUEST )  ;", "appInfo . getResource (  )  . setvCores (  (  (  . rm . getConfig (  )  . getInt ( RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES ,    DEFAULT _ RM _ SCHEDULER _ MAXIMUM _ ALLOCATION _ VCORES )  )     +     1  )  )  ;", "appInfo . getResource (  )  . setMemory (  . CONTAINER _ MB )  ;", "response    =    this . constructWebResource ( urlPath )  . accept ( acceptMedia )  . entity ( appInfo ,    contentMedia )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    BAD _ REQUEST )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testAppSubmitErrors"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesAppsModification . rm . start (  )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "for    ( String   acceptMedia    :    mediaTypes )     {", "testGetNewApplication ( acceptMedia )  ;", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testGetNewApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "ClientResponse   response    =    this . constructWebResource (  \" apps \"  ,     \" new - application \"  )  . accept ( mediaType )  . post ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    OK )  ;", "if    (  !  ( isAuthenticationEnabled (  )  )  )     {", "return    \"  \"  ;", "}", "return   validateGetNewApplicationResponse ( response )  ;", "}", "METHOD_END"], "methodName": ["testGetNewApplication"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesAppsModification . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesAppsModification . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "for    ( String   acceptMedia    :    mediaTypes )     {", "for    ( String   contentMedia    :    mediaTypes )     {", "testAppSubmit ( acceptMedia ,    contentMedia )  ;", "testAppSubmitErrors ( acceptMedia ,    contentMedia )  ;", "}", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testGetNewApplicationAndSubmit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesAppsModification . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesAppsModification . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "MediaType [  ]    contentTypes    =    new   MediaType [  ]  {    MediaType . APPLICATION _ JSON _ TYPE ,    MediaType . APPLICATION _ XML _ TYPE    }  ;", "for    ( String   mediaType    :    mediaTypes )     {", "for    ( MediaType   contentType    :    contentTypes )     {", "RMApp   app    =    TestRMWebServicesAppsModification . rm . submitApp ( TestRMWebServicesAppsModification . CONTAINER _ MB ,     \"  \"  ,    webserviceUserName )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "ClientResponse   response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . accept ( mediaType )  . get ( ClientResponse . class )  ;", "AppState   targetState    =    new   AppState ( KILLED . toString (  )  )  ;", "Object   entity ;", "if    ( contentType    =  =     ( MediaType . APPLICATION _ JSON _ TYPE )  )     {", "entity    =    TestRMWebServicesAppsModification . appStateToJSON ( targetState )  ;", "} else    {", "entity    =    targetState ;", "}", "response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . entity ( entity ,    contentType )  . accept ( mediaType )  . put ( ClientResponse . class )  ;", "if    (  !  ( isAuthenticationEnabled (  )  )  )     {", "assertEquals ( UNAUTHORIZED ,    response . getClientResponseStatus (  )  )  ;", "continue ;", "}", "assertEquals ( ACCEPTED ,    response . getClientResponseStatus (  )  )  ;", "if    ( mediaType    =  =     ( MediaType . APPLICATION _ JSON )  )     {", "TestRMWebServicesAppsModification . verifyAppStateJson ( response ,    RMAppState . KILLING ,    RMAppState . ACCEPTED )  ;", "} else    {", "TestRMWebServicesAppsModification . verifyAppStateXML ( response ,    RMAppState . KILLING ,    RMAppState . ACCEPTED )  ;", "}", "String   locationHeaderValue    =    response . getHeaders (  )  . getFirst ( LOCATION )  ;", "Client   c    =    Client . create (  )  ;", "WebResource   tmp    =    c . resource ( locationHeaderValue )  ;", "if    ( isAuthenticationEnabled (  )  )     {", "tmp    =    tmp . queryParam (  \" user . name \"  ,    webserviceUserName )  ;", "}", "response    =    tmp . get ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "assertTrue ( locationHeaderValue . endsWith (  (  (  \"  / ws / v 1  / cluster / apps /  \"     +     ( app . getApplicationId (  )  . toString (  )  )  )     +     \"  / state \"  )  )  )  ;", "while    ( true )     {", "Thread . sleep (  1  0  0  )  ;", "response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . accept ( mediaType )  . entity ( entity ,    contentType )  . put ( ClientResponse . class )  ;", "assertTrue (  (  (  ( response . getClientResponseStatus (  )  )     =  =     ( Status . ACCEPTED )  )     |  |     (  ( response . getClientResponseStatus (  )  )     =  =     ( Status . OK )  )  )  )  ;", "if    (  ( response . getClientResponseStatus (  )  )     =  =     ( Status . OK )  )     {", "assertEquals ( RMAppState . KILLED ,    app . getState (  )  )  ;", "if    ( mediaType    =  =     ( MediaType . APPLICATION _ JSON )  )     {", "TestRMWebServicesAppsModification . verifyAppStateJson ( response ,    RMAppState . KILLED )  ;", "} else    {", "TestRMWebServicesAppsModification . verifyAppStateXML ( response ,    RMAppState . KILLED )  ;", "}", "break ;", "}", "}", "}", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testSingleAppKill"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesAppsModification . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesAppsModification . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "String [  ]    testAppIds    =    new   String [  ]  {     \" application _  1  3  9  1  7  0  5  0  4  2  1  9  6  _  0  0  0  1  \"  ,     \" random _ string \"     }  ;", "for    ( String   testAppId    :    testAppIds )     {", "AppState   info    =    new   AppState (  \" KILLED \"  )  ;", "ClientResponse   response    =    this . constructWebResource (  \" apps \"  ,    testAppId ,     \" state \"  )  . accept ( APPLICATION _ XML )  . entity ( info ,    APPLICATION _ XML )  . put ( ClientResponse . class )  ;", "if    (  !  ( isAuthenticationEnabled (  )  )  )     {", "assertEquals ( UNAUTHORIZED ,    response . getClientResponseStatus (  )  )  ;", "continue ;", "}", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testSingleAppKillInvalidId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesAppsModification . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesAppsModification . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "MediaType [  ]    contentTypes    =    new   MediaType [  ]  {    MediaType . APPLICATION _ JSON _ TYPE ,    MediaType . APPLICATION _ XML _ TYPE    }  ;", "String [  ]    targetStates    =    new   String [  ]  {    FINISHED . toString (  )  ,     \" blah \"     }  ;", "for    ( String   mediaType    :    mediaTypes )     {", "for    ( MediaType   contentType    :    contentTypes )     {", "for    ( String   targetStateString    :    targetStates )     {", "RMApp   app    =    TestRMWebServicesAppsModification . rm . submitApp ( TestRMWebServicesAppsModification . CONTAINER _ MB ,     \"  \"  ,    webserviceUserName )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "ClientResponse   response ;", "AppState   targetState    =    new   AppState ( targetStateString )  ;", "Object   entity ;", "if    ( contentType    =  =     ( MediaType . APPLICATION _ JSON _ TYPE )  )     {", "entity    =    TestRMWebServicesAppsModification . appStateToJSON ( targetState )  ;", "} else    {", "entity    =    targetState ;", "}", "response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . entity ( entity ,    contentType )  . accept ( mediaType )  . put ( ClientResponse . class )  ;", "if    (  !  ( isAuthenticationEnabled (  )  )  )     {", "assertEquals ( UNAUTHORIZED ,    response . getClientResponseStatus (  )  )  ;", "continue ;", "}", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "}", "}", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testSingleAppKillInvalidState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "boolean   isCapacityScheduler    =     ( TestRMWebServicesAppsModification . rm . getResourceScheduler (  )  )    instanceof   CapacityScheduler ;", "assumeTrue (  \" Currently   this   test   is   only   supported   on   CapacityScheduler \"  ,    isCapacityScheduler )  ;", "CapacitySchedulerConfiguration   csconf    =    new   CapacitySchedulerConfiguration (  )  ;", "csconf . setAcl (  \" root \"  ,    ADMINISTER _ QUEUE ,     \" someuser \"  )  ;", "csconf . setAcl (  \" root . default \"  ,    ADMINISTER _ QUEUE ,     \" someuser \"  )  ;", "TestRMWebServicesAppsModification . rm . getResourceScheduler (  )  . reinitialize ( csconf ,    TestRMWebServicesAppsModification . rm . getRMContext (  )  )  ;", "TestRMWebServicesAppsModification . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesAppsModification . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "for    ( String   mediaType    :    mediaTypes )     {", "RMApp   app    =    TestRMWebServicesAppsModification . rm . submitApp ( TestRMWebServicesAppsModification . CONTAINER _ MB ,     \" test \"  ,     \" someuser \"  )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "ClientResponse   response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . accept ( mediaType )  . get ( ClientResponse . class )  ;", "AppState   info    =    response . getEntity ( AppState . class )  ;", "info . setState ( KILLED . toString (  )  )  ;", "response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . accept ( mediaType )  . entity ( info ,    APPLICATION _ XML )  . put ( ClientResponse . class )  ;", "validateResponseStatus ( response ,    FORBIDDEN )  ;", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testSingleAppKillUnauthorized"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesAppsModification . rm . start (  )  ;", "MockNM   amNodeManager    =    TestRMWebServicesAppsModification . rm . registerNode (  \"  1  2  7  .  0  .  0  .  1  :  1  2  3  4  \"  ,     2  0  4  8  )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "for    ( String   mediaType    :    mediaTypes )     {", "RMApp   app    =    TestRMWebServicesAppsModification . rm . submitApp ( TestRMWebServicesAppsModification . CONTAINER _ MB ,     \"  \"  ,    webserviceUserName )  ;", "amNodeManager . nodeHeartbeat ( true )  ;", "ClientResponse   response    =    this . constructWebResource (  \" apps \"  ,    app . getApplicationId (  )  . toString (  )  ,     \" state \"  )  . accept ( mediaType )  . get ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "if    ( mediaType    =  =     ( MediaType . APPLICATION _ JSON )  )     {", "TestRMWebServicesAppsModification . verifyAppStateJson ( response ,    RMAppState . ACCEPTED )  ;", "} else", "if    ( mediaType    =  =     ( MediaType . APPLICATION _ XML )  )     {", "TestRMWebServicesAppsModification . verifyAppStateXML ( response ,    RMAppState . ACCEPTED )  ;", "}", "}", "TestRMWebServicesAppsModification . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleAppState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "String   appId    =    json . getString (  \" application - id \"  )  ;", "assertTrue (  (  ( appId . isEmpty (  )  )     =  =    false )  )  ;", "JSONObject   maxResources    =    json . getJSONObject (  \" maximum -  - capability \"  )  ;", "long   memory    =    maxResources . getLong (  \" memory \"  )  ;", "long   vCores    =    maxResources . getLong (  \" vCores \"  )  ;", "assertTrue (  ( memory    !  =     0  )  )  ;", "assertTrue (  ( vCores    !  =     0  )  )  ;", "return   appId ;", "}", "METHOD_END"], "methodName": ["validateGetNewApplicationJsonResponse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "String   ret    =     \"  \"  ;", "if    ( resp . getType (  )  . equals ( APPLICATION _ JSON _ TYPE )  )     {", "JSONObject   json    =    resp . getEntity ( JSONObject . class )  ;", "ret    =    validateGetNewApplJsonResponse ( json )  ;", "} else", "if    ( resp . getType (  )  . equals ( MediaType . APPLICATION _ XML _ TYPE )  )     {", "String   xml    =    resp . getEntity ( String . class )  ;", "ret    =    validateGetNewApplXMLResponse ( xml )  ;", "} else    {", "assertTrue ( false )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["validateGetNewApplicationResponse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( response )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" NewApplication \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "Element   element    =     (  ( Element )     ( nodes . item (  0  )  )  )  ;", "String   appId    =    TestUtils . getXmlString ( element ,     \" application - id \"  )  ;", "assertTrue (  (  ( appId . isEmpty (  )  )     =  =    false )  )  ;", "NodeList   maxResourceNodes    =    element . getElementsByTagName (  \" maximum - resource - capability \"  )  ;", "assertEquals (  1  ,    maxResourceNodes . getLength (  )  )  ;", "Element   maxResourceCapability    =     (  ( Element )     ( maxResourceNodes . item (  0  )  )  )  ;", "long   memory    =    TestUtils . getXmlLong ( maxResourceCapability ,     \" memory \"  )  ;", "long   vCores    =    TestUtils . getXmlLong ( maxResourceCapability ,     \" vCores \"  )  ;", "assertTrue (  ( memory    !  =     0  )  )  ;", "assertTrue (  ( vCores    !  =     0  )  )  ;", "return   appId ;", "}", "METHOD_END"], "methodName": ["validateGetNewApplicationXMLResponse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "validateResponseStatus ( response ,    UNAUTHORIZED ,    expectedAuthorizedMode )  ;", "}", "METHOD_END"], "methodName": ["validateResponseStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( isAuthenticationEnabled (  )  )  )     {", "assertEquals ( expectedUnauthorizedMode ,    response . getClientResponseStatus (  )  )  ;", "} else    {", "assertEquals ( expectedAuthorizedMode ,    response . getClientResponseStatus (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateResponseStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "String   responseState    =    json . getString (  \" state \"  )  ;", "boolean   valid    =    false ;", "for    ( RMAppState   state    :    states )     {", "if    ( state . toString (  )  . equals ( responseState )  )     {", "valid    =    true ;", "}", "}", "String   msg    =     \" app   state   incorrect ,    got    \"     +    responseState ;", "assertTrue ( msg ,    valid )  ;", "return ;", "}", "METHOD_END"], "methodName": ["verifyAppStateJson"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" appstate \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "Element   element    =     (  ( Element )     ( nodes . item (  0  )  )  )  ;", "String   state    =    TestUtils . getXmlString ( element ,     \" state \"  )  ;", "boolean   valid    =    false ;", "for    ( RMAppState   appState    :    appStates )     {", "if    ( appState . toString (  )  . equals ( state )  )     {", "valid    =    true ;", "}", "}", "String   msg    =     \" app   state   incorrect ,    got    \"     +    state ;", "assertTrue ( msg ,    valid )  ;", "return ;", "}", "METHOD_END"], "methodName": ["verifyAppStateXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesAppsModification"}, {"methodBody": ["METHOD_START", "{", "queue . getJSONObject (  \" resourcesUsed \"  )  . getInt (  \" memory \"  )  ;", "queue . getJSONObject (  \" resourcesUsed \"  )  . getInt (  \" vCores \"  )  ;", "}", "METHOD_END"], "methodName": ["checkResourcesUsed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "NodeList   nodeList    =    node . getChildNodes (  )  ;", "for    ( int   i    =     0  ;    i    <     ( nodeList . getLength (  )  )  ;     +  + i )     {", "if    ( nodeList . item ( i )  . getNodeName (  )  . equals ( tagname )  )     {", "return   nodeList . item ( i )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getChildNodeByName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "JSONArray   queues    =    queue . getJSONObject (  \" queues \"  )  . getJSONArray (  \" queue \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( queues . length (  )  )  ;     +  + i )     {", "checkRsUsed ( queues . getJSONObject ( i )  )  ;", "if    ( queues . getJSONObject ( i )  . getString (  \" queueName \"  )  . equals ( subQueue )  )     {", "return   queues . getJSONObject ( i )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getSubQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "conf . setQueues ( CapacitySchedulerConfiguration . ROOT ,    new   String [  ]  {     \" a \"  ,     \" b \"     }  )  ;", "final   String   A    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . a \"  ;", "conf . setCapacity ( A ,     1  0  .  5 F )  ;", "conf . setMaximumCapacity ( A ,     5  0  )  ;", "final   String   B    =     ( CapacitySchedulerConfiguration . ROOT )     +     \"  . b \"  ;", "conf . setCapacity ( B ,     8  9  .  5 F )  ;", "final   String   A 1     =    A    +     \"  . a 1  \"  ;", "final   String   A 2     =    A    +     \"  . a 2  \"  ;", "conf . setQueues ( A ,    new   String [  ]  {     \" a 1  \"  ,     \" a 2  \"     }  )  ;", "conf . setCapacity ( A 1  ,     3  0  )  ;", "conf . setMaximumCapacity ( A 1  ,     5  0  )  ;", "conf . setUserLimitFactor ( A 1  ,     1  0  0  .  0 F )  ;", "conf . setCapacity ( A 2  ,     7  0  )  ;", "conf . setUserLimitFactor ( A 2  ,     1  0  0  .  0 F )  ;", "final   String   B 1     =    B    +     \"  . b 1  \"  ;", "final   String   B 2     =    B    +     \"  . b 2  \"  ;", "final   String   B 3     =    B    +     \"  . b 3  \"  ;", "conf . setQueues ( B ,    new   String [  ]  {     \" b 1  \"  ,     \" b 2  \"  ,     \" b 3  \"     }  )  ;", "conf . setCapacity ( B 1  ,     6  0  )  ;", "conf . setUserLimitFactor ( B 1  ,     1  0  0  .  0 F )  ;", "conf . setCapacity ( B 2  ,     3  9  .  5 F )  ;", "conf . setUserLimitFactor ( B 2  ,     1  0  0  .  0 F )  ;", "conf . setCapacity ( B 3  ,     0  .  5 F )  ;", "conf . setUserLimitFactor ( B 3  ,     1  0  0  .  0 F )  ;", "conf . setQueues ( A 1  ,    new   String [  ]  {     \" a 1 a \"  ,     \" a 1 b \"     }  )  ;", "final   String   A 1 A    =    A 1     +     \"  . a 1 a \"  ;", "conf . setCapacity ( A 1 A ,     8  5  )  ;", "final   String   A 1 B    =    A 1     +     \"  . a 1 b \"  ;", "conf . setCapacity ( A 1 B ,     1  5  )  ;", "}", "METHOD_END"], "methodName": ["setupQueueConfiguration"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterScheduler ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler \"  )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterScheduler ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyClusterScheduler ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler /  \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   scheduler    =    dom . getElementsByTagName (  \" scheduler \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    scheduler . getLength (  )  )  ;", "NodeList   schedulerInfo    =    dom . getElementsByTagName (  \" schedulerInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    schedulerInfo . getLength (  )  )  ;", "verifyClusterSchedulerXML ( schedulerInfo )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesCapacitySched . rm . start (  )  ;", "try    {", "TestRMWebServicesCapacitySched . rm . submitApp (  1  0  ,     \" app 1  \"  ,     \" user 1  \"  ,    null ,     \" b 1  \"  )  ;", "TestRMWebServicesCapacitySched . rm . submitApp (  2  0  ,     \" app 2  \"  ,     \" user 2  \"  ,    null ,     \" b 1  \"  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   schedulerInfo    =    json . getJSONObject (  \" scheduler \"  )  . getJSONObject (  \" schedulerInfo \"  )  ;", "JSONObject   b 1     =    getSubQueue ( getSubQueue ( schedulerInfo ,     \" b \"  )  ,     \" b 1  \"  )  ;", "JSONArray   users    =    b 1  . getJSONObject (  \" users \"  )  . getJSONArray (  \" user \"  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;     +  + i )     {", "JSONObject   user    =    users . getJSONObject ( i )  ;", "assertTrue (  \" User   isn ' t   user 1    or   user 2  \"  ,     (  ( user . getString (  \" username \"  )  . equals (  \" user 1  \"  )  )     |  |     ( user . getString (  \" username \"  )  . equals (  \" user 2  \"  )  )  )  )  ;", "user . getInt (  \" numActiveApplications \"  )  ;", "user . getInt (  \" numPendingApplications \"  )  ;", "checkResourcesUsed ( user )  ;", "}", "}    finally    {", "TestRMWebServicesCapacitySched . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPerUserResourcesJSON"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesCapacitySched . rm . start (  )  ;", "try    {", "TestRMWebServicesCapacitySched . rm . submitApp (  1  0  ,     \" app 1  \"  ,     \" user 1  \"  ,    null ,     \" b 1  \"  )  ;", "TestRMWebServicesCapacitySched . rm . submitApp (  2  0  ,     \" app 2  \"  ,     \" user 2  \"  ,    null ,     \" b 1  \"  )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws / v 1  / cluster / scheduler \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilder   db    =    DocumentBuilderFactory . newInstance (  )  . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   allUsers    =    dom . getElementsByTagName (  \" users \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( allUsers . getLength (  )  )  ;     +  + i )     {", "Node   perUserResources    =    allUsers . item ( i )  ;", "String   queueName    =    getChildNodeByName ( perUserResources . getParentNode (  )  ,     \" queueName \"  )  . getTextContent (  )  ;", "if    ( queueName . equals (  \" b 1  \"  )  )     {", "assertEquals (  2  ,    perUserResources . getChildNodes (  )  . getLength (  )  )  ;", "NodeList   users    =    perUserResources . getChildNodes (  )  ;", "for    ( int   j    =     0  ;    j    <     ( users . getLength (  )  )  ;     +  + j )     {", "Node   user    =    users . item ( j )  ;", "String   username    =    getChildNodeByName ( user ,     \" username \"  )  . getTextContent (  )  ;", "assertTrue (  (  ( username . equals (  \" user 1  \"  )  )     |  |     ( username . equals (  \" user 2  \"  )  )  )  )  ;", "Integer . parseInt ( getChildNodeByName ( getChildNodeByName ( user ,     \" resourcesUsed \"  )  ,     \" memory \"  )  . getTextContent (  )  )  ;", "Integer . parseInt ( getChildNodeByName ( user ,     \" numActiveApplications \"  )  . getTextContent (  )  )  ;", "Integer . parseInt ( getChildNodeByName ( user ,     \" numPendingApplications \"  )  . getTextContent (  )  )  ;", "}", "} else    {", "assertEquals (  0  ,    perUserResources . getChildNodes (  )  . getLength (  )  )  ;", "}", "}", "NodeList   allResourcesUsed    =    dom . getElementsByTagName (  \" resourcesUsed \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( allResourcesUsed . getLength (  )  )  ;     +  + i )     {", "Node   resourcesUsed    =    allResourcesUsed . item ( i )  ;", "Integer . parseInt ( getChildNodeByName ( resourcesUsed ,     \" memory \"  )  . getTextContent (  )  )  ;", "Integer . parseInt ( getChildNodeByName ( resourcesUsed ,     \" vCores \"  )  . getTextContent (  )  )  ;", "}", "}    finally    {", "TestRMWebServicesCapacitySched . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPerUserResourcesXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "Resource   res    =    Resources . createResource (  1  0  ,     1  )  ;", "assertEquals (  \"  < memory :  1  0  ,    vCores :  1  >  \"  ,    res . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   info    =    json . getJSONObject (  \" scheduler \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "info    =    info . getJSONObject (  \" schedulerInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     6  ,    info . length (  )  )  ;", "verifyClusterSchedulerGeneric ( info . getString (  \" type \"  )  ,     (  ( float )     ( info . getDouble (  \" usedCapacity \"  )  )  )  ,     (  ( float )     ( info . getDouble (  \" capacity \"  )  )  )  ,     (  ( float )     ( info . getDouble (  \" maxCapacity \"  )  )  )  ,    info . getString (  \" queueName \"  )  )  ;", "JSONArray   arr    =    info . getJSONObject (  \" queues \"  )  . getJSONArray (  \" queue \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    arr . length (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( arr . length (  )  )  ;    i +  +  )     {", "JSONObject   obj    =    arr . getJSONObject ( i )  ;", "String   q    =     (  ( ulerConfiguration . ROOT )     +     \"  .  \"  )     +     ( obj . getString (  \" queueName \"  )  )  ;", "verifySubQueue ( obj ,    q ,     1  0  0  ,     1  0  0  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyClusterScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" type   doesn ' t   match \"  ,     \" capacityScheduler \"  . matches ( type )  )  ;", "assertEquals (  \" usedCapacity   doesn ' t   match \"  ,     0  ,    usedCapacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" capacity   doesn ' t   match \"  ,     1  0  0  ,    capacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" maxCapacity   doesn ' t   match \"  ,     1  0  0  ,    maxCapacity ,     0  .  0  0  1 F )  ;", "assertTrue (  \" queueName   doesn ' t   match \"  ,     \" root \"  . matches ( queueName )  )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterSchedulerGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyClusterSchedulerGeneric ( WebServicesTestUtils . getXmlAttrString ( element ,     \" xsi : type \"  )  ,    WebServicesTestUtils . getXmlFloat ( element ,     \" usedCapacity \"  )  ,    WebServicesTestUtils . getXmlFloat ( element ,     \" capacity \"  )  ,    WebServicesTestUtils . getXmlFloat ( element ,     \" maxCapacity \"  )  ,    WebServicesTestUtils . getXmlString ( element ,     \" queueName \"  )  )  ;", "NodeList   children    =    element . getChildNodes (  )  ;", "for    ( int   j    =     0  ;    j    <     ( children . getLength (  )  )  ;    j +  +  )     {", "Element   qElem    =     (  ( Element )     ( children . item ( j )  )  )  ;", "if    ( qElem . getTagName (  )  . equals (  \" queues \"  )  )     {", "NodeList   qListInfos    =    qElem . getChildNodes (  )  ;", "for    ( int   k    =     0  ;    k    <     ( qListInfos . getLength (  )  )  ;    k +  +  )     {", "Element   qElem 2     =     (  ( Element )     ( qListInfos . item ( k )  )  )  ;", "String   qName 2     =    WebServicesTestUtils . getXmlString ( qElem 2  ,     \" queueName \"  )  ;", "String   q 2     =     (  ( ulerConfiguration . ROOT )     +     \"  .  \"  )     +    qName 2  ;", "verifySubQueueXML ( qElem 2  ,    q 2  ,     1  0  0  ,     1  0  0  )  ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["verifyClusterSchedulerXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" numActiveApplications   doesn ' t   match \"  ,     0  ,    info . numActiveApplications )  ;", "assertEquals (  \" numPendingApplications   doesn ' t   match \"  ,     0  ,    info . numPendingApplications )  ;", "assertEquals (  \" numContainers   doesn ' t   match \"  ,     0  ,    info . numContainers )  ;", "int   maxSystemApps    =    csConf . getMaximumSystemApplications (  )  ;", "int   expectedMaxApps    =     (  ( int )     ( maxSystemApps    *     (  ( info . absolute )     /     1  0  0  )  )  )  ;", "int   expectedMaxAppsPerUser    =     (  ( int )     (  ( expectedMaxApps    *     (  ( info . userLimit )     /     1  0  0  .  0 F )  )     *     ( info . userLimitFactor )  )  )  ;", "assertEquals (  \" maxApplications   doesn ' t   match \"  ,     (  ( float )     ( expectedMaxApps )  )  ,     (  ( float )     ( info . maxApplications )  )  ,     1  .  0 F )  ;", "assertEquals (  \" maxApplicationsPerUser   doesn ' t   match \"  ,     (  ( float )     ( expectedMaxAppsPerUser )  )  ,     (  ( float )     ( info . maxApplicationsPerUser )  )  ,    info . userLimitFactor )  ;", "assertTrue (  \" maxActiveApplications   doesn ' t   match \"  ,     (  ( info . maxActiveApplications )     >     0  )  )  ;", "assertTrue (  \" maxActiveApplicationsPerUser   doesn ' t   match \"  ,     (  ( info . maxActiveApplicationsPerUser )     >     0  )  )  ;", "assertEquals (  \" userLimit   doesn ' t   match \"  ,    csConf . getUserLimit ( q )  ,    info . userLimit )  ;", "assertEquals (  \" userLimitFactor   doesn ' t   match \"  ,    csConf . getUserLimitFactor ( q )  ,    info . userLimitFactor ,     0  .  0  0  1 F )  ;", "}", "METHOD_END"], "methodName": ["verifyLeafQueueGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "int   numExpectedElements    =     1  1  ;", "boolean   isParentQueue    =    true ;", "if    (  !  ( info . has (  \" queues \"  )  )  )     {", "numExpectedElements    =     2  1  ;", "isParentQueue    =    false ;", "}", "assertEquals (  \" incorrect   number   of   elements \"  ,    numExpectedElements ,    info . length (  )  )  ;", ". QueueInfo   qi    =     ( isParentQueue )     ?    new    . QueueInfo (  )     :    new    . LeafQueueInfo (  )  ;", "qi . capacity    =     (  ( float )     ( info . getDouble (  \" capacity \"  )  )  )  ;", "qi . usedCapacity    =     (  ( float )     ( info . getDouble (  \" usedCapacity \"  )  )  )  ;", "qi . maxCapacity    =     (  ( float )     ( info . getDouble (  \" maxCapacity \"  )  )  )  ;", "qi . absoluteCapacity    =     (  ( float )     ( info . getDouble (  \" absoluteCapacity \"  )  )  )  ;", "qi . absoluteMaxCapacity    =     (  ( float )     ( info . getDouble (  \" absoluteMaxCapacity \"  )  )  )  ;", "qi . absoluteUsedCapacity    =     (  ( float )     ( info . getDouble (  \" absoluteUsedCapacity \"  )  )  )  ;", "qi . numApplications    =    info . getInt (  \" numApplications \"  )  ;", "qi . queueName    =    info . getString (  \" queueName \"  )  ;", "qi . state    =    info . getString (  \" state \"  )  ;", "verifySubQueueGeneric ( q ,    qi ,    parentAbsCapacity ,    parentAbsMaxCapacity )  ;", "if    ( isParentQueue )     {", "JSONArray   arr    =    info . getJSONObject (  \" queues \"  )  . getJSONArray (  \" queue \"  )  ;", "for    ( int   i    =     0  ;    i    <     ( arr . length (  )  )  ;    i +  +  )     {", "JSONObject   obj    =    arr . getJSONObject ( i )  ;", "String   q 2     =     ( q    +     \"  .  \"  )     +     ( obj . getString (  \" queueName \"  )  )  ;", "verifySubQueue ( obj ,    q 2  ,    qi . absoluteCapacity ,    qi . absoluteMaxCapacity )  ;", "}", "} else    {", ". LeafQueueInfo   lqi    =     (  (  . LeafQueueInfo )     ( qi )  )  ;", "lqi . numActiveApplications    =    info . getInt (  \" numActiveApplications \"  )  ;", "lqi . numPendingApplications    =    info . getInt (  \" numPendingApplications \"  )  ;", "lqi . numContainers    =    info . getInt (  \" numContainers \"  )  ;", "lqi . maxApplications    =    info . getInt (  \" maxApplications \"  )  ;", "lqi . maxApplicationsPerUser    =    info . getInt (  \" maxApplicationsPerUser \"  )  ;", "lqi . maxActiveApplications    =    info . getInt (  \" maxActiveApplications \"  )  ;", "lqi . maxActiveApplicationsPerUser    =    info . getInt (  \" maxActiveApplicationsPerUser \"  )  ;", "lqi . userLimit    =    info . getInt (  \" userLimit \"  )  ;", "lqi . userLimitFactor    =     (  ( float )     ( info . getDouble (  \" userLimitFactor \"  )  )  )  ;", "verifyLeafQueueGeneric ( q ,    lqi )  ;", "}", "}", "METHOD_END"], "methodName": ["verifySubQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "String [  ]    qArr    =    q . split (  \"  \\  \\  .  \"  )  ;", "assertTrue (  (  \" q   name   invalid :     \"     +    q )  ,     (  ( qArr . length )     >     1  )  )  ;", "String   qshortName    =    qArr [  (  ( qArr . length )     -     1  )  ]  ;", "assertEquals (  \" usedCapacity   doesn ' t   match \"  ,     0  ,    info . usedCapacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" capacity   doesn ' t   match \"  ,    csConf . getCapacity ( q )  ,    info . capacity ,     0  .  0  0  1 F )  ;", "float   expectCapacity    =    csConf . getMaximumCapacity ( q )  ;", "float   expectAbsMaxCapacity    =    parentAbsMaxCapacity    *     (  ( info . maxCapacity )     /     1  0  0  )  ;", "if    (  ( ulerConfiguration . UNDEFINED )     =  =    expectCapacity )     {", "expectCapacity    =     1  0  0  ;", "expectAbsMaxCapacity    =     1  0  0  ;", "}", "assertEquals (  \" maxCapacity   doesn ' t   match \"  ,    expectCapacity ,    info . maxCapacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" absoluteCapacity   doesn ' t   match \"  ,     ( parentAbsCapacity    *     (  ( info . capacity )     /     1  0  0  )  )  ,    info . absoluteCapacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" absoluteMaxCapacity   doesn ' t   match \"  ,    expectAbsMaxCapacity ,    info . absoluteMaxCapacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" absoluteUsedCapacity   doesn ' t   match \"  ,     0  ,    info . absoluteUsedCapacity ,     0  .  0  0  1 F )  ;", "assertEquals (  \" numApplications   doesn ' t   match \"  ,     0  ,    info . numApplications )  ;", "assertTrue (  (  (  (  \" queueName   doesn ' t   match ,    got :     \"     +     ( info . queueName )  )     +     \"    expected :     \"  )     +    q )  ,    qshortName . matches ( info . queueName )  )  ;", "assertTrue (  \" state   doesn ' t   match \"  ,    csConf . getState ( q )  . toString (  )  . matches ( info . state )  )  ;", "}", "METHOD_END"], "methodName": ["verifySubQueueGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "NodeList   children    =    qElem . getChildNodes (  )  ;", "boolean   hasSubQueues    =    false ;", "for    ( int   j    =     0  ;    j    <     ( children . getLength (  )  )  ;    j +  +  )     {", "Element   qElem 2     =     (  ( Element )     ( children . item ( j )  )  )  ;", "if    ( qElem 2  . getTagName (  )  . equals (  \" queues \"  )  )     {", "NodeList   qListInfos    =    qElem 2  . getChildNodes (  )  ;", "if    (  ( qListInfos . getLength (  )  )     >     0  )     {", "hasSubQueues    =    true ;", "}", "}", "}", ". QueueInfo   qi    =     ( hasSubQueues )     ?    new    . QueueInfo (  )     :    new    . LeafQueueInfo (  )  ;", "qi . capacity    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" capacity \"  )  ;", "qi . usedCapacity    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" usedCapacity \"  )  ;", "qi . maxCapacity    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" maxCapacity \"  )  ;", "qi . absoluteCapacity    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" absoluteCapacity \"  )  ;", "qi . absoluteMaxCapacity    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" absoluteMaxCapacity \"  )  ;", "qi . absoluteUsedCapacity    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" absoluteUsedCapacity \"  )  ;", "qi . numApplications    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" numApplications \"  )  ;", "qi . queueName    =    WebServicesTestUtils . getXmlString ( qElem ,     \" queueName \"  )  ;", "qi . state    =    WebServicesTestUtils . getXmlString ( qElem ,     \" state \"  )  ;", "verifySubQueueGeneric ( q ,    qi ,    parentAbsCapacity ,    parentAbsMaxCapacity )  ;", "if    ( hasSubQueues )     {", "for    ( int   j    =     0  ;    j    <     ( children . getLength (  )  )  ;    j +  +  )     {", "Element   qElem 2     =     (  ( Element )     ( children . item ( j )  )  )  ;", "if    ( qElem 2  . getTagName (  )  . equals (  \" queues \"  )  )     {", "NodeList   qListInfos    =    qElem 2  . getChildNodes (  )  ;", "for    ( int   k    =     0  ;    k    <     ( qListInfos . getLength (  )  )  ;    k +  +  )     {", "Element   qElem 3     =     (  ( Element )     ( qListInfos . item ( k )  )  )  ;", "String   qName 3     =    WebServicesTestUtils . getXmlString ( qElem 3  ,     \" queueName \"  )  ;", "String   q 3     =     ( q    +     \"  .  \"  )     +    qName 3  ;", "verifySubQueueXML ( qElem 3  ,    q 3  ,    qi . absoluteCapacity ,    qi . absoluteMaxCapacity )  ;", "}", "}", "}", "} else    {", ". LeafQueueInfo   lqi    =     (  (  . LeafQueueInfo )     ( qi )  )  ;", "lqi . numActiveApplications    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" numActiveApplications \"  )  ;", "lqi . numPendingApplications    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" numPendingApplications \"  )  ;", "lqi . numContainers    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" numContainers \"  )  ;", "lqi . maxApplications    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" maxApplications \"  )  ;", "lqi . maxApplicationsPerUser    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" maxApplicationsPerUser \"  )  ;", "lqi . maxActiveApplications    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" maxActiveApplications \"  )  ;", "lqi . maxActiveApplicationsPerUser    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" maxActiveApplicationsPerUser \"  )  ;", "lqi . userLimit    =    WebServicesTestUtils . getXmlInt ( qElem ,     \" userLimit \"  )  ;", "lqi . userLimitFactor    =    WebServicesTestUtils . getXmlFloat ( qElem ,     \" userLimitFactor \"  )  ;", "verifyLeafQueueGeneric ( q ,    lqi )  ;", "}", "}", "METHOD_END"], "methodName": ["verifySubQueueXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesCapacitySched"}, {"methodBody": ["METHOD_START", "{", "KerberosTestUtils . doAsClient ( new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / delegation - token \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "conn . setRequestProperty ( RMWebServices . DELEGATION _ TOKEN _ HEADER ,    tokenString )  ;", ". setupConn ( conn ,     \" DELETE \"  ,    null ,    null )  ;", "InputStream   response    =    conn . getInputStream (  )  ;", "assertEquals ( OK . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "response . close (  )  ;", "return   null ;", "}", "}  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["cancelDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "String   token    =    KerberosTestUtils . doAsClient ( new   Callable < String >  (  )     {", "@ Override", "public   String   call (  )    throws   Exception    {", "String   ret    =    null ;", "String   body    =     (  \"  {  \\  \" renewer \\  \"  :  \\  \"  \"     +    renewer )     +     \"  \\  \"  }  \"  ;", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / delegation - token \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", ". setupConn ( conn ,     \" POST \"  ,    APPLICATION _ JSON ,    body )  ;", "InputStream   response    =    conn . getInputStream (  )  ;", "assertEquals ( OK . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "BufferedReader   reader    =    null ;", "try    {", "reader    =    new   BufferedReader ( new   InputStreamReader ( response ,     \" UTF 8  \"  )  )  ;", "for    ( String   line ;     ( line    =    reader . readLine (  )  )     !  =    null ;  )     {", "JSONObject   obj    =    new   JSONObject ( line )  ;", "if    ( obj . has (  \" token \"  )  )     {", "reader . close (  )  ;", "response . close (  )  ;", "ret    =    obj . getString (  \" token \"  )  ;", "break ;", "}", "}", "}    finally    {", "IOUtils . closeQuietly ( reader )  ;", "IOUtils . closeQuietly ( response )  ;", "}", "return   ret ;", "}", "}  )  ;", "return   token ;", "}", "METHOD_END"], "methodName": ["getDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "return   TestRMWebServicesDelegationTokenAuthentication . testMiniKDC ;", "}", "METHOD_END"], "methodName": ["getKdc"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "StringWriter   writer    =    new   StringWriter (  )  ;", "JAXBContext   context    =    JAXBContext . newInstance ( ApplSubmissionContextInfo . class )  ;", "Marshaller   m    =    context . createMarshaller (  )  ;", "m . marshal ( appInfo ,    writer )  ;", "return   writer . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getMarshalledAppInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "try    {", ". testMiniKDC    =    new   MiniKdc ( MiniKdc . createConf (  )  ,     . testRootDir )  ;", ". setupKDC (  )  ;", ". setupAndStartRM (  )  ;", "}    catch    ( Exception   e )     {", "assertTrue (  \" Couldn ' t   create   MiniKDC \"  ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "Configuration   rmconf    =    new   Configuration (  )  ;", "rmconf . setInt ( RM _ AM _ MAX _ ATTEMPTS ,    DEFAULT _ RM _ AM _ MAX _ ATTEMPTS )  ;", "rmconf . setClass ( RM _ SCHEDULER ,    FifoScheduler . class ,    ResourceScheduler . class )  ;", "rmconf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "String   httpPrefix    =     \" hadoop . http . authentication .  \"  ;", "rmconf . setStrings (  ( httpPrefix    +     \" type \"  )  ,     \" kerberos \"  )  ;", "rmconf . set (  ( httpPrefix    +     ( KerberosAuthenticationHandler . PRINCIPAL )  )  ,     . httpSpnegoPrincipal )  ;", "rmconf . set (  ( httpPrefix    +     ( KerberosAuthenticationHandler . KEYTAB )  )  ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "rmconf . set (  (  ( httpPrefix    +     ( AuthenticationFilter . SIGNATURE _ SECRET )  )     +     \"  . file \"  )  ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "rmconf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "rmconf . setBoolean ( RM _ WEBAPP _ DELEGATION _ TOKEN _ AUTH _ FILTER ,    true )  ;", "rmconf . set (  \" hadoop . http . filter . initializers \"  ,    AuthenticationFilterInitializer . class . getName (  )  )  ;", "rmconf . set ( RM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY ,     . httpSpnegoPrincipal )  ;", "rmconf . set ( RM _ KEYTAB ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "rmconf . set ( RM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "rmconf . set ( NM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY ,     . httpSpnegoPrincipal )  ;", "rmconf . set ( NM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY ,     . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "rmconf . setBoolean (  \" mockrm . webapp . enabled \"  ,    true )  ;", "UserGroupInformation . setConfiguration ( rmconf )  ;", ". rm    =    new   MockRM ( rmconf )  ;", ". rm . start (  )  ;", "}", "METHOD_END"], "methodName": ["setupAndStartRM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "conn . setRequestMethod ( method )  ;", "conn . setDoOutput ( true )  ;", "conn . setRequestProperty (  \" Accept - Charset \"  ,     \" UTF 8  \"  )  ;", "if    (  ( contentType    !  =    null )     &  &     (  !  ( contentType . isEmpty (  )  )  )  )     {", "conn . setRequestProperty (  \" Content - Type \"  ,     ( contentType    +     \"  ; charset = UTF 8  \"  )  )  ;", "if    (  ( body    !  =    null )     &  &     (  !  ( body . isEmpty (  )  )  )  )     {", "OutputStream   stream    =    conn . getOutputStream (  )  ;", "stream . write ( body . getBytes (  \" UTF 8  \"  )  )  ;", "stream . close (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setupConn"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestRMWebServicesDelegationTokenAuthentication . miniKDCStarted )     =  =    false )     {", "TestRMWebServicesDelegationTokenAuthentication . testMiniKDC . start (  )  ;", "TestRMWebServicesDelegationTokenAuthentication . getKdc (  )  . createPrincipal ( TestRMWebServicesDelegationTokenAuthentication . httpSpnegoKeytabFile ,     \" HTTP / localhost \"  ,     \" client \"  ,    UserGroupInformation . getLoginUser (  )  . getShortUserName (  )  )  ;", "TestRMWebServicesDelegationTokenAuthentication . miniKDCStarted    =    true ;", "}", "}", "METHOD_END"], "methodName": ["setupKDC"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestRMWebServicesDelegationTokenAuthentication . testMiniKDC )     !  =    null )     {", "TestRMWebServicesDelegationTokenAuthentication . testMiniKDC . stop (  )  ;", "}", "if    (  ( TestRMWebServicesDelegationTokenAuthentication . rm )     !  =    null )     {", "TestRMWebServicesDelegationTokenAuthentication . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "String   token    =    getDelegationToken (  \" client \"  )  ;", "cancelDelegationToken ( token )  ;", "ApplicationSubmissionContextInfo   app    =    new   ApplicationSubmissionContextInfo (  )  ;", "String   appid    =     \" application _  1  2  3  _  0  \"  ;", "app . setApplicationId ( appid )  ;", "String   requestBody    =     . getMarshalledAppInfo ( app )  ;", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "conn . setRequestProperty (  . DelegationTokenHeader ,    token )  ;", ". setupConn ( conn ,     \" POST \"  ,    APPLICATION _ XML ,    requestBody )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  \" Authentication   should   fail   with   expired   delegation   tokens \"  )  ;", "}    catch    ( IOException   e )     {", "assertEquals ( FORBIDDEN . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "return ;", "}", "METHOD_END"], "methodName": ["testCancelledDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "final   String   token    =    getDelegationToken (  \" test \"  )  ;", "ApplicationSubmissionContextInfo   app    =    new   ApplicationSubmissionContextInfo (  )  ;", "String   appid    =     \" application _  1  2  3  _  0  \"  ;", "app . setApplicationId ( appid )  ;", "String   requestBody    =     . getMarshalledAppInfo ( app )  ;", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", ". setupConn ( conn ,     \" POST \"  ,     \" application / xml \"  ,    requestBody )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  \" we   should   not   be   here \"  )  ;", "}    catch    ( IOException   e )     {", "assertEquals ( UNAUTHORIZED . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "conn . setRequestProperty (  . DelegationTokenHeader ,    token )  ;", ". setupConn ( conn ,     \" POST \"  ,    APPLICATION _ XML ,    requestBody )  ;", "conn . getInputStream (  )  ;", "boolean   appExists    =     . rm . getRMContext (  )  . getRMApps (  )  . containsKey ( ConverterUtils . toApplicationId ( appid )  )  ;", "assertTrue ( appExists )  ;", "RMApp   actualApp    =     . rm . getRMContext (  )  . getRMApps (  )  . get ( ConverterUtils . toApplicationId ( appid )  )  ;", "String   owner    =    actualApp . getUser (  )  ;", "assertEquals (  \" client \"  ,    owner )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testDelegationTokenAuth"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "String   token    =    getDelegationToken (  \" client \"  )  ;", "String   createRequest    =     \"  {  \\  \" renewer \\  \"  :  \\  \" test \\  \"  }  \"  ;", "String   renewRequest    =     (  \"  {  \\  \" token \\  \"  :     \\  \"  \"     +    token )     +     \"  \\  \"  }  \"  ;", "String [  ]    requests    =    new   String [  ]  {    createRequest ,    renewRequest    }  ;", "for    ( String   requestBody    :    requests )     {", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / delegation - token \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "conn . setRequestProperty (  . DelegationTokenHeader ,    token )  ;", ". setupConn ( conn ,     \" POST \"  ,    APPLICATION _ JSON ,    requestBody )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  (  \" Creation / Renewing   delegation   tokens   should   not   be    \"     +     \" allowed   with   token   auth \"  )  )  ;", "}    catch    ( IOException   e )     {", "assertEquals ( FORBIDDEN . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "}", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / delegation - token \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "conn . setRequestProperty (  . DelegationTokenHeader ,    token )  ;", "conn . setRequestProperty ( RMWebServices . DELEGATION _ TOKEN _ HEADER ,    token )  ;", ". setupConn ( conn ,     \" DELETE \"  ,    null ,    null )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  \" Cancelling   delegation   tokens   should   not   be   allowed   with   token   auth \"  )  ;", "}    catch    ( IOException   e )     {", "assertEquals ( FORBIDDEN . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "return ;", "}", "METHOD_END"], "methodName": ["testDelegationTokenOps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokenAuthentication"}, {"methodBody": ["METHOD_START", "{", "Token < RMDelegationTokenIdentifier >    realToken    =    new   Token < RMDelegationTokenIdentifier >  (  )  ;", "realToken . decodeFromUrlString ( encodedToken )  ;", "RMDelegationTokenIdentifier   ident    =    realToken . decodeIdentifier (  )  ;", "boolean   exceptionCaught    =    false ;", "try    {", ". rm . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . verifyToken ( ident ,    realToken . getPassword (  )  )  ;", "}    catch    ( InvalidToken   it )     {", "exceptionCaught    =    true ;", "}", "assertTrue (  \" InvalidToken   exception   not   thrown \"  ,    exceptionCaught )  ;", "assertFalse (  . rm . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllTokens (  )  . containsKey ( ident )  )  ;", "}", "METHOD_END"], "methodName": ["assertTokenCancelled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "Token < RMDelegationTokenIdentifier >    realToken    =    new   Token < RMDelegationTokenIdentifier >  (  )  ;", "realToken . decodeFromUrlString ( encodedToken )  ;", "RMDelegationTokenIdentifier   ident    =    realToken . decodeIdentifier (  )  ;", ". rm . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . verifyToken ( ident ,    realToken . getPassword (  )  )  ;", "assertTrue (  . rm . getRMContext (  )  . getRMDelegationTokenSecretManager (  )  . getAllTokens (  )  . containsKey ( ident )  )  ;", "}", "METHOD_END"], "methodName": ["assertValidRMToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "String   body    =     \"  \"  ;", "if    ( mediaType . equals ( APPLICATION _ JSON )  )     {", "body    =     (  \"  {  \\  \" token \\  \"  :     \\  \"  \"     +    token )     +     \"  \\  \"     }  \"  ;", "} else    {", "body    =     (  \"  < d - token >  < token >  \"     +    token )     +     \"  <  / token >  <  / d - token >  \"  ;", "}", "return   body ;", "}", "METHOD_END"], "methodName": ["generateRenewTokenBody"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "DelegationToken   ret    =    new   DelegationToken (  )  ;", "if    ( json . has (  \" token \"  )  )     {", "ret . setToken ( json . getString (  \" token \"  )  )  ;", "} else", "if    ( json . has (  \" expiration - time \"  )  )     {", "ret . setNextExpirationTime ( json . getLong (  \" expiration - time \"  )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getDelegationTokenFromJson"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "if    ( response . getType (  )  . toString (  )  . equals ( APPLICATION _ JSON )  )     {", "return    . getDelegationTokenFromJson ( response . getEntity ( JSONObject . class )  )  ;", "}", "return    . getDelegationTokenFromXML ( response . getEntity ( String . class )  )  ;", "}", "METHOD_END"], "methodName": ["getDelegationTokenFromResponse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( tokenXML )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" delegation - token \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "Element   element    =     (  ( Element )     ( nodes . item (  0  )  )  )  ;", "ret    =    new    (  )  ;", "String   token    =    WebServicesTestUtils . getXmlString ( element ,     \" token \"  )  ;", "if    ( token    !  =    null )     {", "ret . setToken ( token )  ;", "} else    {", "long   expiration    =    WebServicesTestUtils . getXmlLong ( element ,     \" expiration - time \"  )  ;", "ret . setNextExpirationTime ( expiration )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getDelegationTokenFromXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "return   Guice . createInjector ( new   TestRMWebServicesDelegationTokens . TestServletModule (  )     {", "@ Override", "protected   void   configureServlets (  )     {", "isKerberosAuth    =    true ;", "rmconf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "rmconf . set ( RM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY ,    TestRMWebServicesDelegationTokens . httpSpnegoPrincipal )  ;", "rmconf . set ( RM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY ,    TestRMWebServicesDelegationTokens . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "rmconf . set ( NM _ WEBAPP _ SPNEGO _ USER _ NAME _ KEY ,    TestRMWebServicesDelegationTokens . httpSpnegoPrincipal )  ;", "rmconf . set ( NM _ WEBAPP _ SPNEGO _ KEYTAB _ FILE _ KEY ,    TestRMWebServicesDelegationTokens . httpSpnegoKeytabFile . getAbsolutePath (  )  )  ;", "super . configureServlets (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getKerberosAuthInjector"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "return   Guice . createInjector ( new   TestRMWebServicesDelegationTokens . TestServletModule (  )     {", "@ Override", "protected   void   configureServlets (  )     {", "isKerberosAuth    =    false ;", "rmconf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" simple \"  )  ;", "super . configureServlets (  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getSimpleAuthInjector"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {     0     }  ,    new   Object [  ]  {     1     }     }  )  ;", "}", "METHOD_END"], "methodName": ["guiceConfigs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesDelegationTokens . testRootDir    =    new   File (  \" target \"  ,     (  ( TestRMWebServicesDelegationTokens . class . getName (  )  )     +     \"  - root \"  )  )  ;", "TestRMWebServicesDelegationTokens . testMiniKDC    =    new   MiniKdc ( MiniKdc . createConf (  )  ,    TestRMWebServicesDelegationTokens . testRootDir )  ;", "TestRMWebServicesDelegationTokens . testMiniKDC . start (  )  ;", "TestRMWebServicesDelegationTokens . testMiniKDC . createPrincipal ( TestRMWebServicesDelegationTokens . httpSpnegoKeytabFile ,     \" HTTP / localhost \"  ,     \" client \"  ,     \" client 2  \"  ,     \" client 3  \"  )  ;", "}", "METHOD_END"], "methodName": ["setupKDC"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestRMWebServicesDelegationTokens . testMiniKDC )     !  =    null )     {", "TestRMWebServicesDelegationTokens . testMiniKDC . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["shutdownKdc"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesDelegationTokens . rm . start (  )  ;", "this . client (  )  . addFilter ( new   LoggingFilter ( System . out )  )  ;", "if    (  ( isKerberosAuth )     =  =    false )     {", "verifySimpleAuthCancel (  )  ;", "return ;", "}", "final   DelegationToken   dtoken    =    new   DelegationToken (  )  ;", "String   renewer    =     \" client 2  \"  ;", "dtoken . setRenewer ( renewer )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "for    ( final   String   mediaType    :    mediaTypes )     {", "for    ( final   String   contentType    :    mediaTypes )     {", "KerberosTestUtils . doAsClient ( new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( dtoken ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "DelegationToken   tok    =    getDelegationTokenFromResponse ( response )  ;", "response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . header ( yarnTokenHeader ,    tok . getToken (  )  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "assertTokenCancelled ( tok . getToken (  )  )  ;", "return   null ;", "}", "}  )  ;", "final   DelegationToken   tmpToken    =    KerberosTestUtils . doAsClient ( new   Callable < DelegationToken >  (  )     {", "@ Override", "public   DelegationToken   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( dtoken ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "DelegationToken   tok    =    getDelegationTokenFromResponse ( response )  ;", "return   tok ;", "}", "}  )  ;", "KerberosTestUtils . doAs ( renewer ,    new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . header ( yarnTokenHeader ,    tmpToken . getToken (  )  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "assertTokenCancelled ( tmpToken . getToken (  )  )  ;", "return   null ;", "}", "}  )  ;", "final   DelegationToken   tmpToken 2     =    KerberosTestUtils . doAsClient ( new   Callable < DelegationToken >  (  )     {", "@ Override", "public   DelegationToken   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( dtoken ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "DelegationToken   tok    =    getDelegationTokenFromResponse ( response )  ;", "return   tok ;", "}", "}  )  ;", "KerberosTestUtils . doAs (  \" client 3  \"  ,    new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . header ( yarnTokenHeader ,    tmpToken 2  . getToken (  )  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "assertValidRMToken ( tmpToken 2  . getToken (  )  )  ;", "return   null ;", "}", "}  )  ;", "testCancelTokenBadRequests ( mediaType ,    contentType )  ;", "}", "}", "TestRMWebServicesDelegationTokens . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testCancelDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "final   String   mediaType    =    mType ;", "final   String   contentType    =    cType ;", "final      dtoken    =    new    (  )  ;", "String   renewer    =     \" client 2  \"  ;", "dtoken . setRenewer ( renewer )  ;", "KerberosTestUtils . doAsClient ( new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . header ( yarnTokenHeader ,     \" random - string \"  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "return   null ;", "}", "}  )  ;", "KerberosTestUtils . doAsClient ( new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "return   null ;", "}", "}  )  ;", "final      tmpToken    =    KerberosTestUtils . doAsClient ( new   Callable <  >  (  )     {", "@ Override", "public      call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( dtoken ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "tok    =    getFromResponse ( response )  ;", "return   tok ;", "}", "}  )  ;", "KerberosTestUtils . doAs ( renewer ,    new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . header ( yarnTokenHeader ,    tmpToken . getToken (  )  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . header ( yarnTokenHeader ,    tmpToken . getToken (  )  )  . accept ( contentType )  . delete ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["testCancelTokenBadRequests"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesDelegationTokens . rm . start (  )  ;", "this . client (  )  . addFilter ( new   LoggingFilter ( System . out )  )  ;", "final   String   renewer    =     \" test - renewer \"  ;", "String   jsonBody    =     (  \"  {     \\  \" renewer \\  \"     :     \\  \"  \"     +    renewer )     +     \"  \\  \"     }  \"  ;", "String   xmlBody    =     (  \"  < delegation - token >  < renewer >  \"     +    renewer )     +     \"  <  / renewer >  <  / delegation - token >  \"  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "Map < String ,    String >    bodyMap    =    new   HashMap < String ,    String >  (  )  ;", "bodyMap . put ( APPLICATION _ JSON ,    jsonBody )  ;", "bodyMap . put ( APPLICATION _ XML ,    xmlBody )  ;", "for    ( final   String   mediaType    :    mediaTypes )     {", "final   String   body    =    bodyMap . get ( mediaType )  ;", "for    ( final   String   contentType    :    mediaTypes )     {", "if    (  ( isKerberosAuth )     =  =    true )     {", "verifyKerberosAuthCreate ( mediaType ,    contentType ,    body ,    renewer )  ;", "} else    {", "verifySimpleAuthCreate ( mediaType ,    contentType ,    body )  ;", "}", "}", "}", "TestRMWebServicesDelegationTokens . rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testCreateDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "client (  )  . addFilter ( new   LoggingFilter ( System . out )  )  ;", ". rm . start (  )  ;", "final   String   renewer    =     \" client 2  \"  ;", "this . client (  )  . addFilter ( new   LoggingFilter ( System . out )  )  ;", "final   DelegationToken   dummyToken    =    new   DelegationToken (  )  ;", "dummyToken . setRenewer ( renewer )  ;", "String [  ]    mediaTypes    =    new   String [  ]  {    MediaType . APPLICATION _ JSON ,    MediaType . APPLICATION _ XML    }  ;", "for    ( final   String   mediaType    :    mediaTypes )     {", "for    ( final   String   contentType    :    mediaTypes )     {", "if    (  ( isKerberosAuth )     =  =    false )     {", "verifySimpleAuthRenew ( mediaType ,    contentType )  ;", "continue ;", "}", "final   DelegationToken   responseToken    =    KerberosTestUtils . doAsClient ( new   Callable < DelegationToken >  (  )     {", "@ Override", "public   DelegationToken   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( dummyToken ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "DelegationToken   tok    =    getDelegationTokenFromResponse ( response )  ;", "assertFalse ( tok . getToken (  )  . isEmpty (  )  )  ;", "String   body    =     . generateRenewTokenBody ( mediaType ,    tok . getToken (  )  )  ;", "response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . path (  \" expiration \"  )  . header ( yarnTokenHeader ,    tok . getToken (  )  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "return   tok ;", "}", "}  )  ;", "KerberosTestUtils . doAs ( renewer ,    new   Callable < DelegationToken >  (  )     {", "@ Override", "public   DelegationToken   call (  )    throws   Exception    {", "long   oldExpirationTime    =    Time . now (  )  ;", "assertValidRMToken ( responseToken . getToken (  )  )  ;", "String   body    =     . generateRenewTokenBody ( mediaType ,    responseToken . getToken (  )  )  ;", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . path (  \" expiration \"  )  . header ( yarnTokenHeader ,    responseToken . getToken (  )  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "DelegationToken   tok    =    getDelegationTokenFromResponse ( response )  ;", "String   message    =     (  (  \" Expiration   time   not   as   expected :    old    =     \"     +    oldExpirationTime )     +     \"  ;    new    =     \"  )     +     ( tok . getNextExpirationTime (  )  )  ;", "assertTrue ( message ,     (  ( tok . getNextExpirationTime (  )  )     >    oldExpirationTime )  )  ;", "oldExpirationTime    =    tok . getNextExpirationTime (  )  ;", "Thread . sleep (  1  0  0  0  )  ;", "response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . path (  \" expiration \"  )  . header ( yarnTokenHeader ,    responseToken . getToken (  )  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "tok    =    getDelegationTokenFromResponse ( response )  ;", "message    =     (  (  \" Expiration   time   not   as   expected :    old    =     \"     +    oldExpirationTime )     +     \"  ;    new    =     \"  )     +     ( tok . getNextExpirationTime (  )  )  ;", "assertTrue ( message ,     (  ( tok . getNextExpirationTime (  )  )     >    oldExpirationTime )  )  ;", "return   tok ;", "}", "}  )  ;", "KerberosTestUtils . doAs (  \" client 3  \"  ,    new   Callable < DelegationToken >  (  )     {", "@ Override", "public   DelegationToken   call (  )    throws   Exception    {", "String   body    =     . generateRenewTokenBody ( mediaType ,    responseToken . getToken (  )  )  ;", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . path (  \" expiration \"  )  . header ( yarnTokenHeader ,    responseToken . getToken (  )  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "return   null ;", "}", "}  )  ;", "KerberosTestUtils . doAsClient ( new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "String   token    =     \" TEST _ TOKEN _ STRING \"  ;", "String   body    =     \"  \"  ;", "if    ( mediaType . equals ( APPLICATION _ JSON )  )     {", "body    =     (  \"  {  \\  \" token \\  \"  :     \\  \"  \"     +    token )     +     \"  \\  \"     }  \"  ;", "} else    {", "body    =     (  \"  < delegation - token >  < token >  \"     +    token )     +     \"  <  / token >  <  / delegation - token >  \"  ;", "}", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . path (  \" expiration \"  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "return   null ;", "}", "}  )  ;", "}", "}", ". rm . stop (  )  ;", "return ;", "}", "METHOD_END"], "methodName": ["testRenewDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "final   String   mediaType    =    mType ;", "final   String   contentType    =    cType ;", "final   String   body    =    reqBody ;", "final   String   renewer    =    renUser ;", "KerberosTestUtils . doAsClient ( new   Callable < Void >  (  )     {", "@ Override", "public   Void   call (  )    throws   Exception    {", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "tok    =    getFromResponse ( response )  ;", "assertFalse ( tok . getToken (  )  . isEmpty (  )  )  ;", "Token < RMIdentifier >    token    =    new   Token < RMIdentifier >  (  )  ;", "token . decodeFromUrlString ( tok . getToken (  )  )  ;", "assertEquals ( renewer ,    token . decodeIdentifier (  )  . getRenewer (  )  . toString (  )  )  ;", "assertValidRMToken ( tok . getToken (  )  )  ;", "dtoken    =    new    (  )  ;", "response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . accept ( contentType )  . entity ( dtoken ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( OK ,    response . getClientResponseStatus (  )  )  ;", "tok    =    getFromResponse ( response )  ;", "assertFalse ( tok . getToken (  )  . isEmpty (  )  )  ;", "token    =    new   Token < RMIdentifier >  (  )  ;", "token . decodeFromUrlString ( tok . getToken (  )  )  ;", "assertEquals (  \"  \"  ,    token . decodeIdentifier (  )  . getRenewer (  )  . toString (  )  )  ;", "assertValidRMToken ( tok . getToken (  )  )  ;", "return   null ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["verifyKerberosAuthCreate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . queryParam (  \" user . name \"  ,     \" testuser \"  )  . header ( RMWebServices . DELEGATION _ TOKEN _ HEADER ,     \" random \"  )  . delete ( ClientResponse . class )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifySimpleAuthCancel"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" delegation - token \"  )  . queryParam (  \" user . name \"  ,     \" testuser \"  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifySimpleAuthCreate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "String   token    =     \" TEST _ TOKEN _ STRING \"  ;", "String   body    =     \"  \"  ;", "if    ( mediaType . equals ( APPLICATION _ JSON )  )     {", "body    =     (  \"  {  \\  \" token \\  \"  :     \\  \"  \"     +    token )     +     \"  \\  \"     }  \"  ;", "body    =     \"  {  \\  \" abcd \\  \"  :     \\  \" test -  1  2  3  \\  \"     }  \"  ;", "} else    {", "body    =     (  \"  < d - token >  < token >  \"     +    token )     +     \"  <  / token >  <  / d - token >  \"  ;", "body    =     \"  < d - token >  < xml > abcd <  / xml >  <  / d - token >  \"  ;", "}", "ClientResponse   response    =    resource (  )  . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" d - token \"  )  . queryParam (  \" user . name \"  ,     \" testuser \"  )  . accept ( contentType )  . entity ( body ,    mediaType )  . post ( ClientResponse . class )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifySimpleAuthRenew"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesDelegationTokens"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyCluste ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesFairScheduler"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" scheduler /  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "verifyCluste ( json )  ;", "}", "METHOD_END"], "methodName": ["testClusterSchedulerSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesFairScheduler"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   info    =    json . getJSONObject (  \" s \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    info . length (  )  )  ;", "info    =    info . getJSONObject (  \" sInfo \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    info . length (  )  )  ;", "JSONObject   rootQueue    =    info . getJSONObject (  \" rootQueue \"  )  ;", "assertEquals (  \" root \"  ,    rootQueue . getString (  \" queueName \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyClusterScheduler"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesFairScheduler"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path (  \" node _ invalid _ foo \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   non - existent   nodeid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   message \"  ,     \" Invalid   NodeId    \\  \\  [ node _ invalid _ foo \\  \\  ]  .    Expected   host : port \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" IllegalArgumentException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" IllegalArgumentException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testInvalidNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "testNodesHelper (  \" nodes \"  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . start (  )  ;", "WebResource   r    =    resource (  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodesApps    =    dom . getElementsByTagName (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodesApps . getLength (  )  )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    nodes . getLength (  )  )  ;", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNodes2XML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "testNodesHelper (  \" nodes /  \"  ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testNodesDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =    TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "TestRMWebServicesNodes . rm . sendNodeStarted ( nm 1  )  ;", "TestRMWebServicesNodes . rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", "TestRMWebServicesNodes . rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    NEW )  ;", "MockNM   nm 3     =    TestRMWebServicesNodes . rm . registerNode (  \" h 3  :  1  2  3  6  \"  ,     5  1  2  2  )  ;", "TestRMWebServicesNodes . rm . NMwaitForState ( nm 3  . getNodeId (  )  ,    NEW )  ;", "TestRMWebServicesNodes . rm . sendNodeStarted ( nm 3  )  ;", "TestRMWebServicesNodes . rm . NMwaitForState ( nm 3  . getNodeId (  )  ,    RUNNING )  ;", "RMNodeImpl   node    =     (  ( RMNodeImpl )     ( TestRMWebServicesNodes . rm . getRMContext (  )  . getRMNodes (  )  . get ( nm 3  . getNodeId (  )  )  )  )  ;", "NodeHealthStatus   nodeHealth    =    NodeHealthStatus . newInstance ( false ,     \" test   health   report \"  ,    System . currentTimeMillis (  )  )  ;", "node . handle ( new   RMNodeStatusEvent ( nm 3  . getNodeId (  )  ,    nodeHealth ,    new   ArrayList < api . records . ContainerStatus >  (  )  ,    null ,    null )  )  ;", "TestRMWebServicesNodes . rm . NMwaitForState ( nm 3  . getNodeId (  )  ,    UNHEALTHY )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   nodes    =    json . getJSONObject (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . length (  )  )  ;", "JSONArray   nodeArray    =    nodes . getJSONArray (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    nodeArray . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesDefaultWithUnHealthyNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . sendNodeStarted ( nm 2  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    RUNNING )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path ( path )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   nodes    =    json . getJSONObject (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . length (  )  )  ;", "JSONArray   nodeArray    =    nodes . getJSONArray (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    nodeArray . length (  )  )  ;", "JSONObject   info    =    nodeArray . getJSONObject (  0  )  ;", "String   id    =    info . get (  \" id \"  )  . toString (  )  ;", "if    ( id . matches (  \" h 1  :  1  2  3  4  \"  )  )     {", "verifyNodeInfo ( info ,    nm 1  )  ;", "verifyNodeInfo ( nodeArray . getJSONObject (  1  )  ,    nm 2  )  ;", "} else    {", "verifyNodeInfo ( info ,    nm 2  )  ;", "verifyNodeInfo ( nodeArray . getJSONObject (  1  )  ,    nm 1  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodesHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    NEW )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,     \" UNHEALTHY \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "assertEquals (  \" nodes   is   not   null \"  ,    NULL ,    json . get (  \" nodes \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesQueryHealthyFalse"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    NEW )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,    NEW . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   nodes    =    json . getJSONObject (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . length (  )  )  ;", "JSONArray   nodeArray    =    nodes . getJSONArray (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodeArray . length (  )  )  ;", "JSONObject   info    =    nodeArray . getJSONObject (  0  )  ;", "verifyNodeInfo ( info ,    nm 2  )  ;", "}", "METHOD_END"], "methodName": ["testNodesQueryNew"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    NEW )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,     \" running \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   nodes    =    json . getJSONObject (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . length (  )  )  ;", "JSONArray   nodeArray    =    nodes . getJSONArray (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodeArray . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesQueryRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,     \" BOGUSSTATE \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   querying   invalid   state \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( BAD _ REQUEST ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "WebServicesTestUtils . checkStringContains (  \" exception   message \"  ,     \" api . records . NodeState . BOGUSSTATE \"  ,    message )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   type \"  ,     \" IllegalArgumentException \"  ,    type )  ;", "WebServicesTestUtils . checkStringMatch (  \" exception   classname \"  ,     \" IllegalArgumentException \"  ,    classname )  ;", "}    finally    {", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodesQueryStateInvalid"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . sendNodeStarted ( nm 2  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    RUNNING )  ;", ". rm . sendNodeLost ( nm 1  )  ;", ". rm . sendNodeLost ( nm 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,    LOST . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   nodes    =    json . getJSONObject (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . length (  )  )  ;", "JSONArray   nodeArray    =    nodes . getJSONArray (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     2  ,    nodeArray . length (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( nodeArray . length (  )  )  ;     +  + i )     {", "JSONObject   info    =    nodeArray . getJSONObject ( i )  ;", "String   host    =    info . get (  \" id \"  )  . toString (  )  . split (  \"  :  \"  )  [  0  ]  ;", "RMNode   rmNode    =     . rm . getRMContext (  )  . getInactiveRMNodes (  )  . get ( host )  ;", "WebServicesTestUtils . checkStringMatch (  \" nodeHTTPAddress \"  ,     \"  \"  ,    info . getString (  \" nodeHTTPAddress \"  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" state \"  ,    rmNode . getState (  )  . toString (  )  ,    info . getString (  \" state \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNodesQueryStateLost"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", ". rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", ". rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,    DECOMMISSIONED . toString (  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "assertEquals (  \" nodes   is   not   null \"  ,    NULL ,    json . get (  \" nodes \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodesQueryStateNone"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "testNodesHelper (  \" nodes /  \"  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testNodesSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . start (  )  ;", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =    TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodesApps    =    dom . getElementsByTagName (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodesApps . getLength (  )  )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "verifyNodesXML ( nodes ,    nm 1  )  ;", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testNodesXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path (  \" node _ invalid :  9  9  \"  )  . accept ( APPLICATION _ JSON )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   non - existent   nodeid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "verifyNonexistNodeException ( message ,    type ,    classname )  ;", "}    finally    {", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonexistNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path (  \" node _ invalid :  9  9  \"  )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   non - existent   nodeid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   msg    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   exception    =    msg . getJSONObject (  \" RemoteException \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    exception . length (  )  )  ;", "String   message    =    exception . getString (  \" message \"  )  ;", "String   type    =    exception . getString (  \" exception \"  )  ;", "String   classname    =    exception . getString (  \" javaClassName \"  )  ;", "verifyNonexistNodeException ( message ,    type ,    classname )  ;", "}    finally    {", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonexistNodeDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "WebResource   r    =    resource (  )  ;", "try    {", "r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path (  \" node _ invalid :  9  9  \"  )  . accept ( APPLICATION _ XML )  . get ( JSONObject . class )  ;", "fail (  \" should   have   thrown   exception   on   non - existent   nodeid \"  )  ;", "}    catch    ( UniformInterfaceException   ue )     {", "ClientResponse   response    =    ue . getResponse (  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   msg    =    response . getEntity ( String . class )  ;", "System . out . println ( msg )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( msg )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" RemoteException \"  )  ;", "Element   element    =     (  ( Element )     ( nodes . item (  0  )  )  )  ;", "String   message    =    WebServicesTestUtils . getXmlString ( element ,     \" message \"  )  ;", "String   type    =    WebServicesTestUtils . getXmlString ( element ,     \" exception \"  )  ;", "String   classname    =    WebServicesTestUtils . getXmlString ( element ,     \" javaClassName \"  )  ;", "verifyNonexistNodeException ( message ,    type ,    classname )  ;", "}    finally    {", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testNonexistNodeXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "MockNM   nm 3     =     . rm . registerNode (  \" h 3  :  1  2  3  6  \"  ,     5  1  2  2  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . sendNodeStarted ( nm 3  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    NEW )  ;", ". rm . sendNodeLost ( nm 3  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . queryParam (  \" states \"  ,    Joiner . on (  '  ,  '  )  . join ( EnumSet . allOf ( NodeState . class )  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   nodes    =    json . getJSONObject (  \" nodes \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . length (  )  )  ;", "JSONArray   nodeArray    =    nodes . getJSONArray (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     3  ,    nodeArray . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testQueryAll"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =    TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "testSingleNodeHelper (  \" h 2  :  1  2  3  5  \"  ,    nm 2  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testSingleNode"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm 1     =    TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "testSingleNodeHelper (  \" h 1  :  1  2  3  4  /  \"  ,    nm 1  ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testSingleNodeDefault"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path ( nodeid )  . accept ( media )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    json . length (  )  )  ;", "JSONObject   info    =    json . getJSONObject (  \" node \"  )  ;", "verifyNodeInfo ( info ,    nm )  ;", "}", "METHOD_END"], "methodName": ["testSingleNodeHelper"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =     . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "MockNM   nm 2     =     . rm . registerNode (  \" h 2  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", ". rm . sendNodeStarted ( nm 1  )  ;", ". rm . sendNodeStarted ( nm 2  )  ;", ". rm . NMwaitForState ( nm 1  . getNodeId (  )  ,    RUNNING )  ;", ". rm . NMwaitForState ( nm 2  . getNodeId (  )  ,    RUNNING )  ;", ". rm . sendNodeLost ( nm 1  )  ;", ". rm . sendNodeLost ( nm 2  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path (  \" h 2  :  1  2  3  4  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "JSONObject   json    =    response . getEntity ( JSONObject . class )  ;", "JSONObject   info    =    json . getJSONObject (  \" node \"  )  ;", "String   id    =    info . get (  \" id \"  )  . toString (  )  ;", "assertEquals (  \" Incorrect   Node   Information .  \"  ,     \" h 2  :  1  2  3  4  \"  ,    id )  ;", "RMNode   rmNode    =     . rm . getRMContext (  )  . getInactiveRMNodes (  )  . get (  \" h 2  \"  )  ;", "WebServicesTestUtils . checkStringMatch (  \" nodeHTTPAddress \"  ,     \"  \"  ,    info . getString (  \" nodeHTTPAddress \"  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" state \"  ,    rmNode . getState (  )  . toString (  )  ,    info . getString (  \" state \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSingleNodeQueryStateLost"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "MockNM   nm 1     =    TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "TestRMWebServicesNodes . rm . registerNode (  \" h 2  :  1  2  3  5  \"  ,     5  1  2  1  )  ;", "testSingleNodeHelper (  \" h 1  :  1  2  3  4  /  \"  ,    nm 1  ,    APPLICATION _ JSON )  ;", "}", "METHOD_END"], "methodName": ["testSingleNodeSlash"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "TestRMWebServicesNodes . rm . start (  )  ;", "WebResource   r    =    resource (  )  ;", "MockNM   nm 1     =    TestRMWebServicesNodes . rm . registerNode (  \" h 1  :  1  2  3  4  \"  ,     5  1  2  0  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" cluster \"  )  . path (  \" nodes \"  )  . path (  \" h 1  :  1  2  3  4  \"  )  . accept ( APPLICATION _ XML )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ XML _ TYPE ,    response . getType (  )  )  ;", "String   xml    =    response . getEntity ( String . class )  ;", "DocumentBuilderFactory   dbf    =    DocumentBuilderFactory . newInstance (  )  ;", "DocumentBuilder   db    =    dbf . newDocumentBuilder (  )  ;", "InputSource   is    =    new   InputSource (  )  ;", "is . setCharacterStream ( new   StringReader ( xml )  )  ;", "Document   dom    =    db . parse ( is )  ;", "NodeList   nodes    =    dom . getElementsByTagName (  \" node \"  )  ;", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  ,    nodes . getLength (  )  )  ;", "verifyNodesXML ( nodes ,    nm 1  )  ;", "TestRMWebServicesNodes . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSingleNodesXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" incorrect   number   of   elements \"  ,     1  3  ,    nodeInfo . length (  )  )  ;", "verifyInfoGeneric ( nm ,    nodeInfo . getString (  \" state \"  )  ,    nodeInfo . getString (  \" rack \"  )  ,    nodeInfo . getString (  \" id \"  )  ,    nodeInfo . getString (  \" nodeHostName \"  )  ,    nodeInfo . getString (  \" nodeHTTPAddress \"  )  ,    nodeInfo . getLong (  \" lastHealthUpdate \"  )  ,    nodeInfo . getString (  \" healthReport \"  )  ,    nodeInfo . getInt (  \" numContainers \"  )  ,    nodeInfo . getLong (  \" usedMemoryMB \"  )  ,    nodeInfo . getLong (  \" availMemoryMB \"  )  ,    nodeInfo . getLong (  \" usedVirtualCores \"  )  ,    nodeInfo . getLong (  \" availableVirtualCores \"  )  ,    nodeInfo . getString (  \" version \"  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNodeInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "RMNode   node    =    TestRMWebServicesNodes . rm . getRMContext (  )  . getRMNodes (  )  . get ( nm . getNodeId (  )  )  ;", "ResourceScheduler   sched    =    TestRMWebServicesNodes . rm . getResourceScheduler (  )  ;", "SchedulerNodeReport   report    =    sched . getNodeReport ( nm . getNodeId (  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" state \"  ,    node . getState (  )  . toString (  )  ,    state )  ;", "WebServicesTestUtils . checkStringMatch (  \" rack \"  ,    node . getRackName (  )  ,    rack )  ;", "WebServicesTestUtils . checkStringMatch (  \" id \"  ,    nm . getNodeId (  )  . toString (  )  ,    id )  ;", "WebServicesTestUtils . checkStringMatch (  \" nodeHostName \"  ,    nm . getNodeId (  )  . getHost (  )  ,    nodeHostName )  ;", "WebServicesTestUtils . checkStringMatch (  \" healthReport \"  ,    String . valueOf ( node . getHealthReport (  )  )  ,    healthReport )  ;", "String   expectedHttpAddress    =     (  ( nm . getNodeId (  )  . getHost (  )  )     +     \"  :  \"  )     +     ( nm . getHttpPort (  )  )  ;", "WebServicesTestUtils . checkStringMatch (  \" nodeHTTPAddress \"  ,    expectedHttpAddress ,    nodeHTTPAddress )  ;", "WebServicesTestUtils . checkStringMatch (  \" version \"  ,    node . getNodeManagerVersion (  )  ,    version )  ;", "long   expectedHealthUpdate    =    node . getLastHealthReportTime (  )  ;", "assertEquals (  (  (  (  \" lastHealthUpdate   doesn ' t   match ,    got :     \"     +    lastHealthUpdate )     +     \"    expected :     \"  )     +    expectedHealthUpdate )  ,    expectedHealthUpdate ,    lastHealthUpdate )  ;", "if    ( report    !  =    null )     {", "assertEquals (  (  \" numContainers   doesn ' t   match :     \"     +    numContainers )  ,    report . getNumContainers (  )  ,    numContainers )  ;", "assertEquals (  (  \" usedMemoryMB   doesn ' t   match :     \"     +    usedMemoryMB )  ,    report . getUsedResource (  )  . getMemory (  )  ,    usedMemoryMB )  ;", "assertEquals (  (  \" availMemoryMB   doesn ' t   match :     \"     +    availMemoryMB )  ,    report . getAvailableResource (  )  . getMemory (  )  ,    availMemoryMB )  ;", "assertEquals (  (  \" usedVirtualCores   doesn ' t   match :     \"     +    usedVirtualCores )  ,    report . getUsedResource (  )  . getVirtualCores (  )  ,    usedVirtualCores )  ;", "assertEquals (  (  \" availVirtualCores   doesn ' t   match :     \"     +    availVirtualCores )  ,    report . getAvailableResource (  )  . getVirtualCores (  )  ,    availVirtualCores )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyNodeInfoGeneric"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( nodes . getLength (  )  )  ;    i +  +  )     {", "Element   element    =     (  ( Element )     ( nodes . item ( i )  )  )  ;", "verifyNodeInfoGeneric ( nm ,    TestUtils . getXmlString ( element ,     \" state \"  )  ,    TestUtils . getXmlString ( element ,     \" rack \"  )  ,    TestUtils . getXmlString ( element ,     \" id \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeHostName \"  )  ,    TestUtils . getXmlString ( element ,     \" nodeHTTPAddress \"  )  ,    TestUtils . getXmlLong ( element ,     \" lastHealthUpdate \"  )  ,    TestUtils . getXmlString ( element ,     \" healthReport \"  )  ,    TestUtils . getXmlInt ( element ,     \" numContainers \"  )  ,    TestUtils . getXmlLong ( element ,     \" usedMemoryMB \"  )  ,    TestUtils . getXmlLong ( element ,     \" availMemoryMB \"  )  ,    TestUtils . getXmlLong ( element ,     \" usedVirtualCores \"  )  ,    TestUtils . getXmlLong ( element ,     \" availableVirtualCores \"  )  ,    TestUtils . getXmlString ( element ,     \" version \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyNodesXML"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" exception   message   incorrect \"  ,     \" Exception :    nodeId ,    node _ invalid :  9  9  ,    is   not   found \"  . matches ( message )  )  ;", "assertTrue (  \" exception   type   incorrect \"  ,     \" NotFoundException \"  . matches ( type )  )  ;", "assertTrue (  \" exception   className   incorrect \"  ,     \" webapp . NotFoundException \"  . matches ( classname )  )  ;", "}", "METHOD_END"], "methodName": ["verifyNonexistNodeException"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebServicesNodes"}, {"methodBody": ["METHOD_START", "{", "return   TestRMWebappAuthentication . testMiniKDC ;", "}", "METHOD_END"], "methodName": ["getKdc"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "return   Arrays . asList ( new   Object [  ]  [  ]  {    new   Object [  ]  {     1  ,    TestRMWebappAuthentication . simpleConf    }  ,    new   Object [  ]  {     2  ,    TestRMWebappAuthentication . kerberosConf    }     }  )  ;", "}", "METHOD_END"], "methodName": ["params"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "try    {", ". testMiniKDC    =    new   MiniKdc ( MiniKdc . createConf (  )  ,     . testRootDir )  ;", ". setupKDC (  )  ;", "}    catch    ( Exception   e )     {", "assertTrue (  \" Couldn ' t   create   MiniKDC \"  ,    false )  ;", "}", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "UserGroupInformation . setConfiguration ( conf )  ;", ". rm    =    new   MockRM ( conf )  ;", "}", "METHOD_END"], "methodName": ["setupAndStartRM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( TestRMWebappAuthentication . miniKDCStarted )  )     {", "TestRMWebappAuthentication . testMiniKDC . start (  )  ;", "TestRMWebappAuthentication . getKdc (  )  . createPrincipal ( TestRMWebappAuthentication . httpSpnegoKeytabFile ,     \" HTTP / localhost \"  ,     \" client \"  ,    UserGroupInformation . getLoginUser (  )  . getShortUserName (  )  )  ;", "TestRMWebappAuthentication . miniKDCStarted    =    true ;", "}", "}", "METHOD_END"], "methodName": ["setupKDC"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestRMWebappAuthentication . testMiniKDC )     !  =    null )     {", "TestRMWebappAuthentication . testMiniKDC . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContextInfo   app    =    new   ApplicationSubmissionContextInfo (  )  ;", "String   appid    =     \" application _  1  2  3  _  0  \"  ;", "app . setApplicationId ( appid )  ;", "String   requestBody    =    TestRMWebServicesDelegationToken . getMarshalledAppInfo ( app )  ;", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps / new - application \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "TestRMWebServicesDelegationToken . setupConn ( conn ,     \" POST \"  ,     \" application / xml \"  ,    requestBody )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  \" Anonymous   users   should   not   be   allowed   to   get   new   application   ids   in   secure   mode .  \"  )  ;", "}    catch    ( IOException   ie )     {", "assertEquals ( FORBIDDEN . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps \"  )  ;", "conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "TestRMWebServicesDelegationToken . setupConn ( conn ,     \" POST \"  ,     \" application / xml \"  ,    requestBody )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  \" Anonymous   users   should   not   be   allowed   to   submit   apps   in   secure   mode .  \"  )  ;", "}    catch    ( IOException   ie )     {", "assertEquals ( FORBIDDEN . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "requestBody    =     \"  {     \\  \" state \\  \"  :     \\  \" KILLED \\  \"  }  \"  ;", "url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps / application _  1  2  3  _  0  / state \"  )  ;", "conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "TestRMWebServicesDelegationToken . setupConn ( conn ,     \" PUT \"  ,     \" application / json \"  ,    requestBody )  ;", "try    {", "conn . getInputStream (  )  ;", "fail (  \" Anonymous   users   should   not   be   allowed   to   kill   apps   in   secure   mode .  \"  )  ;", "}    catch    ( IOException   ie )     {", "assertEquals ( FORBIDDEN . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAnonymousKerberosUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContextInfo   app    =    new   ApplicationSubmissionContextInfo (  )  ;", "String   appid    =     \" application _  1  2  3  _  0  \"  ;", "app . setApplicationId ( appid )  ;", "String   requestBody    =    TestRMWebServicesDelegationTokenAuthentication . getMarshalledAppInfo ( app )  ;", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "TestRMWebServicesDelegationTokenAuthentication . setupConn ( conn ,     \" POST \"  ,     \" application / xml \"  ,    requestBody )  ;", "conn . getInputStream (  )  ;", "assertEquals ( ACCEPTED . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "boolean   appExists    =     . rm . getRMContext (  )  . getRMApps (  )  . containsKey ( ConverterUtils . toApplicationId ( appid )  )  ;", "assertTrue ( appExists )  ;", "RMApp   actualApp    =     . rm . getRMContext (  )  . getRMApps (  )  . get ( ConverterUtils . toApplicationId ( appid )  )  ;", "String   owner    =    actualApp . getUser (  )  ;", "assertEquals (  . rm . getConfig (  )  . get ( HADOOP _ HTTP _ STATIC _ USER ,    DEFAULT _ HADOOP _ HTTP _ STATIC _ USER )  ,    owner )  ;", "appid    =     \" application _  1  2  3  _  1  \"  ;", "app . setApplicationId ( appid )  ;", "requestBody    =    TestRMWebServicesDelegationTokenAuthentication . getMarshalledAppInfo ( app )  ;", "url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / ws / v 1  / cluster / apps ? user . name = client \"  )  ;", "conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "TestRMWebServicesDelegationTokenAuthentication . setupConn ( conn ,     \" POST \"  ,    APPLICATION _ XML ,    requestBody )  ;", "conn . getInputStream (  )  ;", "appExists    =     . rm . getRMContext (  )  . getRMApps (  )  . containsKey ( ConverterUtils . toApplicationId ( appid )  )  ;", "assertTrue ( appExists )  ;", "actualApp    =     . rm . getRMContext (  )  . getRMApps (  )  . get ( ConverterUtils . toApplicationId ( appid )  )  ;", "owner    =    actualApp . getUser (  )  ;", "assertEquals (  \" client \"  ,    owner )  ;", "}", "METHOD_END"], "methodName": ["testAnonymousSimpleUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "TestRMWebappAuthentication . rm . start (  )  ;", "URL   url    =    new   URL (  \" http :  /  / localhost :  8  0  8  8  / cluster \"  )  ;", "HttpURLConnection   conn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "try    {", "conn . getInputStream (  )  ;", "assertEquals ( OK . getStatusCode (  )  ,    conn . getResponseCode (  )  )  ;", "}    catch    ( Exception   e )     {", "fail (  \" Fetching   url   failed \"  )  ;", "}", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "testAnonymousKerberosUser (  )  ;", "} else    {", "testAnonymousSimpleUser (  )  ;", "}", "TestRMWebappAuthentication . rm . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testSimpleAuth"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.TestRMWebappAuthentication"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getAttemptId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . logsLink ;", "}", "METHOD_END"], "methodName": ["getLogsLink"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeHttpAddress ;", "}", "METHOD_END"], "methodName": ["getNodeHttpAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . startTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "this . attempt . add ( info )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . attempt ;", "}", "METHOD_END"], "methodName": ["getAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppAttemptsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . amContainerLogsExist ;", "}", "METHOD_END"], "methodName": ["amContainerLogsExist"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . amContainerLogs ;", "}", "METHOD_END"], "methodName": ["getAMContainerLogs"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . amHostHttpAddress ;", "}", "METHOD_END"], "methodName": ["getAMHostHttpAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . allocatedMB ;", "}", "METHOD_END"], "methodName": ["getAllocatedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . allocatedVCores ;", "}", "METHOD_END"], "methodName": ["getAllocatedVCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getAppId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . appIdNum ;", "}", "METHOD_END"], "methodName": ["getAppIdNum"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationTags ;", "}", "METHOD_END"], "methodName": ["getApplicationTags"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationType ;", "}", "METHOD_END"], "methodName": ["getApplicationType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . clusterId ;", "}", "METHOD_END"], "methodName": ["getClusterId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . elapsedTime ;", "}", "METHOD_END"], "methodName": ["getElapsedTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . finalStatus . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getFinalStatus"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . finishedTime ;", "}", "METHOD_END"], "methodName": ["getFinishTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . diagnostics ;", "}", "METHOD_END"], "methodName": ["getNote"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   numAMContainerPreempted ;", "}", "METHOD_END"], "methodName": ["getNumAMContainersPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   numNonAMContainerPreempted ;", "}", "METHOD_END"], "methodName": ["getNumNonAMContainersPreempted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   preemptedResourceMB ;", "}", "METHOD_END"], "methodName": ["getPreemptedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   preemptedResourceVCores ;", "}", "METHOD_END"], "methodName": ["getPreemptedVCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . progress ;", "}", "METHOD_END"], "methodName": ["getProgress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . runningContainers ;", "}", "METHOD_END"], "methodName": ["getRunningContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . startedTime ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . state . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . trackingUI ;", "}", "METHOD_END"], "methodName": ["getTrackingUI"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . trackingUrl ;", "}", "METHOD_END"], "methodName": ["getTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . trackingUrlPretty ;", "}", "METHOD_END"], "methodName": ["getTrackingUrlPretty"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return    !  ( this . trackingUrlIsNotReady )  ;", "}", "METHOD_END"], "methodName": ["isTrackingUrlReady"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . state ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppState"}, {"methodBody": ["METHOD_START", "{", "this . state    =    state ;", "}", "METHOD_END"], "methodName": ["setState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppState"}, {"methodBody": ["METHOD_START", "{", "this . statItem . add ( statItem )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationStatisticsInfo"}, {"methodBody": ["METHOD_START", "{", "return   statItem ;", "}", "METHOD_END"], "methodName": ["getStatItems"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationStatisticsInfo"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   applicationName ;", "}", "METHOD_END"], "methodName": ["getApplicationName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   tags ;", "}", "METHOD_END"], "methodName": ["getApplicationTags"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   applicationType ;", "}", "METHOD_END"], "methodName": ["getApplicationType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   cancelTokensWhenComplete ;", "}", "METHOD_END"], "methodName": ["getCancelTokensWhenComplete"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   containerInfo ;", "}", "METHOD_END"], "methodName": ["getContainerLaunchContextInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   keepContainers ;", "}", "METHOD_END"], "methodName": ["getKeepContainersAcrossApplicationAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxAppAttempts ;", "}", "METHOD_END"], "methodName": ["getMaxAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   resource ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   isUnmanagedAM ;", "}", "METHOD_END"], "methodName": ["getUnmanagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . applicationId    =    applicationId ;", "}", "METHOD_END"], "methodName": ["setApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . applicationName    =    applicationName ;", "}", "METHOD_END"], "methodName": ["setApplicationName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . tags    =    tags ;", "}", "METHOD_END"], "methodName": ["setApplicationTags"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . applicationType    =    applicationType ;", "}", "METHOD_END"], "methodName": ["setApplicationType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . cancelTokensWhenComplete    =    cancelTokensWhenComplete ;", "}", "METHOD_END"], "methodName": ["setCancelTokensWhenComplete"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . containerInfo    =    containerLaunchContext ;", "}", "METHOD_END"], "methodName": ["setContainerLaunchContextInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . keepContainers    =    keepContainers ;", "}", "METHOD_END"], "methodName": ["setKeepContainersAcrossApplicationAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . maxAppAttempts    =    maxAppAttempts ;", "}", "METHOD_END"], "methodName": ["setMaxAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . priority    =    priority ;", "}", "METHOD_END"], "methodName": ["setPriority"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . queue    =    queue ;", "}", "METHOD_END"], "methodName": ["setQueue"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . resource    =    resource ;", "}", "METHOD_END"], "methodName": ["setResource"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . isUnmanagedAM    =    isUnmanagedAM ;", "}", "METHOD_END"], "methodName": ["setUnmanagedAM"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ApplicationSubmissionContextInfo"}, {"methodBody": ["METHOD_START", "{", "app . add ( appinfo )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppsInfo"}, {"methodBody": ["METHOD_START", "{", "return   app ;", "}", "METHOD_END"], "methodName": ["getApps"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . capacity ;", "}", "METHOD_END"], "methodName": ["getCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . maxCapacity ;", "}", "METHOD_END"], "methodName": ["getMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queueName ;", "}", "METHOD_END"], "methodName": ["getQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queues ;", "}", "METHOD_END"], "methodName": ["getQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "CSQueue   parentQueue    =    parent ;", "QueueInfoList   queuesInfo    =    new   QueueInfoList (  )  ;", "for    ( CSQueue   queue    :    parentQueue . getChildQueues (  )  )     {", "QueueInfo   info ;", "if    ( queue   instanceof   LeafQueue )     {", "info    =    new   LeafQueueInfo (  (  ( LeafQueue )     ( queue )  )  )  ;", "} else    {", "info    =    new   QueueInfo ( queue )  ;", "info . queues    =    getQueues ( queue )  ;", "}", "queuesInfo . addToQueueInfoList ( info )  ;", "}", "return   queuesInfo ;", "}", "METHOD_END"], "methodName": ["getQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . usedCapacity ;", "}", "METHOD_END"], "methodName": ["getUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxActiveApplications ;", "}", "METHOD_END"], "methodName": ["getMaxActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxActiveApplicationsPerUser ;", "}", "METHOD_END"], "methodName": ["getMaxActiveApplicationsPerUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxApplications ;", "}", "METHOD_END"], "methodName": ["getMaxApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxApplicationsPerUser ;", "}", "METHOD_END"], "methodName": ["getMaxApplicationsPerUser"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   numActiveApplications ;", "}", "METHOD_END"], "methodName": ["getNumActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   numPendingApplications ;", "}", "METHOD_END"], "methodName": ["getNumPendingApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   userLimit ;", "}", "METHOD_END"], "methodName": ["getUserLimit"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   userLimitFactor ;", "}", "METHOD_END"], "methodName": ["getUserLimitFactor"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   users ;", "}", "METHOD_END"], "methodName": ["getUsers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   Math . min ( Math . max ( val ,    low )  ,    hi )  ;", "}", "METHOD_END"], "methodName": ["cap"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   absoluteCapacity ;", "}", "METHOD_END"], "methodName": ["getAbsoluteCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   absoluteMaxCapacity ;", "}", "METHOD_END"], "methodName": ["getAbsoluteMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   absoluteUsedCapacity ;", "}", "METHOD_END"], "methodName": ["getAbsoluteUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . capacity ;", "}", "METHOD_END"], "methodName": ["getCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . maxCapacity ;", "}", "METHOD_END"], "methodName": ["getMaxCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   numApplications ;", "}", "METHOD_END"], "methodName": ["getNumApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queueName ;", "}", "METHOD_END"], "methodName": ["getQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queuePath ;", "}", "METHOD_END"], "methodName": ["getQueuePath"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . state . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getQueueState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queues ;", "}", "METHOD_END"], "methodName": ["getQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   resourcesUsed ;", "}", "METHOD_END"], "methodName": ["getResourcesUsed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . usedCapacity ;", "}", "METHOD_END"], "methodName": ["getUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . queue . add ( e )  ;", "}", "METHOD_END"], "methodName": ["addToQueueInfoList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfoList"}, {"methodBody": ["METHOD_START", "{", "return   this . queue . get ( i )  ;", "}", "METHOD_END"], "methodName": ["getQueueInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfoList"}, {"methodBody": ["METHOD_START", "{", "return   this . queue ;", "}", "METHOD_END"], "methodName": ["getQueueInfoList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CapacitySchedulerQueueInfoList"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getClusterId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . haState . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getHAState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . hadoopBuildVersion ;", "}", "METHOD_END"], "methodName": ["getHadoopBuildVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . hadoopVersion ;", "}", "METHOD_END"], "methodName": ["getHadoopVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . hadoopVersionBuiltOn ;", "}", "METHOD_END"], "methodName": ["getHadoopVersionBuiltOn"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceManagerBuildVersion ;", "}", "METHOD_END"], "methodName": ["getRMBuildVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceManagerVersion ;", "}", "METHOD_END"], "methodName": ["getRMVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . resourceManagerVersionBuiltOn ;", "}", "METHOD_END"], "methodName": ["getRMVersionBuiltOn"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . startedOn ;", "}", "METHOD_END"], "methodName": ["getStartedOn"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . state . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . activeNodes ;", "}", "METHOD_END"], "methodName": ["getActiveNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . allocatedMB ;", "}", "METHOD_END"], "methodName": ["getAllocatedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . allocatedVirtualCores ;", "}", "METHOD_END"], "methodName": ["getAllocatedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsCompleted ;", "}", "METHOD_END"], "methodName": ["getAppsCompleted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsFailed ;", "}", "METHOD_END"], "methodName": ["getAppsFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsKilled ;", "}", "METHOD_END"], "methodName": ["getAppsKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsPending ;", "}", "METHOD_END"], "methodName": ["getAppsPending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsRunning ;", "}", "METHOD_END"], "methodName": ["getAppsRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . appsSubmitted ;", "}", "METHOD_END"], "methodName": ["getAppsSubmitted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . availableMB ;", "}", "METHOD_END"], "methodName": ["getAvailableMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . availableVirtualCores ;", "}", "METHOD_END"], "methodName": ["getAvailableVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . containersAllocated ;", "}", "METHOD_END"], "methodName": ["getContainersAllocated"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . decommissionedNodes ;", "}", "METHOD_END"], "methodName": ["getDecommissionedNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . lostNodes ;", "}", "METHOD_END"], "methodName": ["getLostNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . containersPending ;", "}", "METHOD_END"], "methodName": ["getPendingContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . rebootedNodes ;", "}", "METHOD_END"], "methodName": ["getRebootedNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . containersReserved ;", "}", "METHOD_END"], "methodName": ["getReservedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . reservedMB ;", "}", "METHOD_END"], "methodName": ["getReservedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . reservedVirtualCores ;", "}", "METHOD_END"], "methodName": ["getReservedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalMB ;", "}", "METHOD_END"], "methodName": ["getTotalMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalNodes ;", "}", "METHOD_END"], "methodName": ["getTotalNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalVirtualCores ;", "}", "METHOD_END"], "methodName": ["getTotalVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . unhealthyNodes ;", "}", "METHOD_END"], "methodName": ["getUnhealthyNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ClusterMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   acls ;", "}", "METHOD_END"], "methodName": ["getAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   servicedata ;", "}", "METHOD_END"], "methodName": ["getAuxillaryServiceData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   commands ;", "}", "METHOD_END"], "methodName": ["getCommands"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   credentials ;", "}", "METHOD_END"], "methodName": ["getCredentials"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   environment ;", "}", "METHOD_END"], "methodName": ["getEnvironment"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   local _ resources ;", "}", "METHOD_END"], "methodName": ["getResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . acls    =    acls ;", "}", "METHOD_END"], "methodName": ["setAcls"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . servicedata    =    serviceData ;", "}", "METHOD_END"], "methodName": ["setAuxillaryServiceData"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . commands    =    commands ;", "}", "METHOD_END"], "methodName": ["setCommands"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . credentials    =    credentials ;", "}", "METHOD_END"], "methodName": ["setCredentials"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . environment    =    environment ;", "}", "METHOD_END"], "methodName": ["setEnvironment"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "this . local _ resources    =    resources ;", "}", "METHOD_END"], "methodName": ["setResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ContainerLaunchContextInfo"}, {"methodBody": ["METHOD_START", "{", "return   secrets ;", "}", "METHOD_END"], "methodName": ["getSecrets"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CredentialsInfo"}, {"methodBody": ["METHOD_START", "{", "return   tokens ;", "}", "METHOD_END"], "methodName": ["getTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CredentialsInfo"}, {"methodBody": ["METHOD_START", "{", "this . secrets    =    secrets ;", "}", "METHOD_END"], "methodName": ["setSecrets"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CredentialsInfo"}, {"methodBody": ["METHOD_START", "{", "this . tokens    =    tokens ;", "}", "METHOD_END"], "methodName": ["setTokens"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.CredentialsInfo"}, {"methodBody": ["METHOD_START", "{", "return   kind ;", "}", "METHOD_END"], "methodName": ["getKind"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "return   maxValidity ;", "}", "METHOD_END"], "methodName": ["getMaxValidity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "return   nextExpirationTime ;", "}", "METHOD_END"], "methodName": ["getNextExpirationTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "return   owner ;", "}", "METHOD_END"], "methodName": ["getOwner"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "return   renewer ;", "}", "METHOD_END"], "methodName": ["getRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "return   token ;", "}", "METHOD_END"], "methodName": ["getToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "this . kind    =    kind ;", "}", "METHOD_END"], "methodName": ["setKind"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "this . maxValidity    =    maxValidity ;", "}", "METHOD_END"], "methodName": ["setMaxValidity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "this . nextExpirationTime    =    Long . valueOf ( nextExpirationTime )  ;", "}", "METHOD_END"], "methodName": ["setNextExpirationTime"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "this . owner    =    owner ;", "}", "METHOD_END"], "methodName": ["setOwner"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "this . renewer    =    renewer ;", "}", "METHOD_END"], "methodName": ["setRenewer"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "this . token    =    token ;", "}", "METHOD_END"], "methodName": ["setToken"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.DelegationToken"}, {"methodBody": ["METHOD_START", "{", "return   scheduler . getSchedulerApp ( appAttemptId )  . getFairShare (  )  . getMemory (  )  ;", "}", "METHOD_END"], "methodName": ["getAppFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   rootQueue ;", "}", "METHOD_END"], "methodName": ["getRootQueueInfo"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   numActiveApps ;", "}", "METHOD_END"], "methodName": ["getNumActiveApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   numPendingApps ;", "}", "METHOD_END"], "methodName": ["getNumPendingApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerLeafQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   childQueues ;", "}", "METHOD_END"], "methodName": ["getChildQueues"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   fairResources ;", "}", "METHOD_END"], "methodName": ["getFairShare"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   fractionMemFairShare ;", "}", "METHOD_END"], "methodName": ["getFairShareMemoryFraction"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxApps ;", "}", "METHOD_END"], "methodName": ["getMaxApplications"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   maxResources ;", "}", "METHOD_END"], "methodName": ["getMaxResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   fractionMemMaxShare ;", "}", "METHOD_END"], "methodName": ["getMaxResourcesFraction"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   minResources ;", "}", "METHOD_END"], "methodName": ["getMinResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   fractionMemMinShare ;", "}", "METHOD_END"], "methodName": ["getMinShareMemoryFraction"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   queueName ;", "}", "METHOD_END"], "methodName": ["getQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   schedulingPolicy ;", "}", "METHOD_END"], "methodName": ["getSchedulingPolicy"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   fractionMemUsed ;", "}", "METHOD_END"], "methodName": ["getUsedMemoryFraction"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   usedResources ;", "}", "METHOD_END"], "methodName": ["getUsedResources"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FairSchedulerQueueInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . availNodeCapacity ;", "}", "METHOD_END"], "methodName": ["getAvailNodeCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . capacity ;", "}", "METHOD_END"], "methodName": ["getCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . maxQueueMemoryCapacity ;", "}", "METHOD_END"], "methodName": ["getMaxQueueMemoryCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . minQueueMemoryCapacity ;", "}", "METHOD_END"], "methodName": ["getMinQueueMemoryCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . numNodes ;", "}", "METHOD_END"], "methodName": ["getNumNodes"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . qName ;", "}", "METHOD_END"], "methodName": ["getQueueName"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . qstate . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . totalNodeCapacity ;", "}", "METHOD_END"], "methodName": ["getTotalNodeCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . usedCapacity ;", "}", "METHOD_END"], "methodName": ["getUsedCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . usedNodeCapacity ;", "}", "METHOD_END"], "methodName": ["getUsedNodeCapacity"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.FifoSchedulerInfo"}, {"methodBody": ["METHOD_START", "{", "return   pattern ;", "}", "METHOD_END"], "methodName": ["getPattern"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   size ;", "}", "METHOD_END"], "methodName": ["getSize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   timestamp ;", "}", "METHOD_END"], "methodName": ["getTimestamp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   url ;", "}", "METHOD_END"], "methodName": ["getUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   visibility ;", "}", "METHOD_END"], "methodName": ["getVisibility"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "this . pattern    =    pattern ;", "}", "METHOD_END"], "methodName": ["setPattern"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "if    ( size    <  =     0  )     {", "throw   new   IllegalAumentException (  \" size   must   be   greater   than    0  \"  )  ;", "}", "this . size    =    size ;", "}", "METHOD_END"], "methodName": ["setSize"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "if    ( timestamp    <  =     0  )     {", "throw   new   IllegalArgumentException (  \" timestamp   must   be   greater   than    0  \"  )  ;", "}", "this . timestamp    =    timestamp ;", "}", "METHOD_END"], "methodName": ["setTimestamp"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "this . type    =    type ;", "}", "METHOD_END"], "methodName": ["setType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "this . url    =    url ;", "}", "METHOD_END"], "methodName": ["setUrl"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "this . visibility    =    visibility ;", "}", "METHOD_END"], "methodName": ["setVisibility"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.LocalResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   applicationId ;", "}", "METHOD_END"], "methodName": ["getApplicationId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NewApplication"}, {"methodBody": ["METHOD_START", "{", "return   maximumResourceCapability ;", "}", "METHOD_END"], "methodName": ["getMaximumResourceCapability"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NewApplication"}, {"methodBody": ["METHOD_START", "{", "return   this . availMemoryMB ;", "}", "METHOD_END"], "methodName": ["getAvailableMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . availableVirtualCores ;", "}", "METHOD_END"], "methodName": ["getAvailableVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . healthReport ;", "}", "METHOD_END"], "methodName": ["getHealthReport"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . lastHealthUpdate ;", "}", "METHOD_END"], "methodName": ["getLastHealthUpdate"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . nodeHTTPAddress ;", "}", "METHOD_END"], "methodName": ["getNodeHTTPAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . id ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . numContainers ;", "}", "METHOD_END"], "methodName": ["getNumContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . rack ;", "}", "METHOD_END"], "methodName": ["getRack"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   String . valueOf ( this . state )  ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . usedMemoryMB ;", "}", "METHOD_END"], "methodName": ["getUsedMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . usedVirtualCores ;", "}", "METHOD_END"], "methodName": ["getUsedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . version ;", "}", "METHOD_END"], "methodName": ["getVersion"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "this . nodeHTTPAddress    =    nodeHTTPAddress ;", "}", "METHOD_END"], "methodName": ["setNodeHTTPAddress"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodeInfo"}, {"methodBody": ["METHOD_START", "{", "node . add ( nodeinfo )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.NodesInfo"}, {"methodBody": ["METHOD_START", "{", "return   memory ;", "}", "METHOD_END"], "methodName": ["getMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   vCores ;", "}", "METHOD_END"], "methodName": ["getvCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo"}, {"methodBody": ["METHOD_START", "{", "this . memory    =    memory ;", "}", "METHOD_END"], "methodName": ["setMemory"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo"}, {"methodBody": ["METHOD_START", "{", "this . vCores    =    vCores ;", "}", "METHOD_END"], "methodName": ["setvCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ResourceInfo"}, {"methodBody": ["METHOD_START", "{", "return   count ;", "}", "METHOD_END"], "methodName": ["getCount"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.StatisticsItemInfo"}, {"methodBody": ["METHOD_START", "{", "return   state ;", "}", "METHOD_END"], "methodName": ["getState"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.StatisticsItemInfo"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.StatisticsItemInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . allocatedMB ;", "}", "METHOD_END"], "methodName": ["getAllocatedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . allocatedVirtualCores ;", "}", "METHOD_END"], "methodName": ["getAllocatedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsCompleted ;", "}", "METHOD_END"], "methodName": ["getAppsCompleted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsFailed ;", "}", "METHOD_END"], "methodName": ["getAppsFailed"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsKilled ;", "}", "METHOD_END"], "methodName": ["getAppsKilled"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsPending ;", "}", "METHOD_END"], "methodName": ["getAppsPending"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   appsRunning ;", "}", "METHOD_END"], "methodName": ["getAppsRunning"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . appsSubmitted ;", "}", "METHOD_END"], "methodName": ["getAppsSubmitted"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . pendingContainers ;", "}", "METHOD_END"], "methodName": ["getPendingContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . pendingMB ;", "}", "METHOD_END"], "methodName": ["getPendingMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . pendingVirtualCores ;", "}", "METHOD_END"], "methodName": ["getPendingVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . reservedContainers ;", "}", "METHOD_END"], "methodName": ["getReservedContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . reservedMB ;", "}", "METHOD_END"], "methodName": ["getReservedMB"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . reservedVirtualCores ;", "}", "METHOD_END"], "methodName": ["getReservedVirtualCores"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . runningContainers ;", "}", "METHOD_END"], "methodName": ["getRunningContainers"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   userMetricsAvailable ;", "}", "METHOD_END"], "methodName": ["metricsAvailable"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UserMetricsInfo"}, {"methodBody": ["METHOD_START", "{", "return   usersList ;", "}", "METHOD_END"], "methodName": ["getUsersList"], "fileName": "org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.UsersInfo"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", "return   new   MasterKeyData (  (  ( serialNo )  +  +  )  ,    generate (  )  )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createNewMasterKey"], "fileName": "org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return   this . currentMasterKey . gsterKey (  )  ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getCurrentKey"], "fileName": "org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "if    ( BaseContainerTokenSecretManager . LOG . isDebugEnabled (  )  )     {", "BaseContainerTokenSecretManager . LOG . debug (  (  (  (  (  (  \" Retrieving   password   for    \"     +     ( identifier . getContainerID (  )  )  )     +     \"    for   user    \"  )     +     ( identifier . getUser (  )  )  )     +     \"    to   be   run   on   NM    \"  )     +     ( identifier . getNmHostAddress (  )  )  )  )  ;", "}", "return   createPassword ( identifier . getBytes (  )  ,    masterKey . getSecretKey (  )  )  ;", "}", "METHOD_END"], "methodName": ["retrievePasswordInternal"], "fileName": "org.apache.hadoop.yarn.server.security.BaseContainerTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    password ;", "NMTokenIdentifier   identifier ;", "this . readLock . lock (  )  ;", "try    {", "identifier    =    new   NMTokenIdentifier ( applicationAttemptId ,    nodeId ,    applicationSubmitter ,    this . currentMasterKey . getMasterKey (  )  . getKeyId (  )  )  ;", "password    =    this . createPassword ( identifier )  ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "return    . newInstance ( password ,    identifier )  ;", "}", "METHOD_END"], "methodName": ["createNMToken"], "fileName": "org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . writeLock . lock (  )  ;", "try    {", "return   new   MasterKeyData (  (  ( serialNo )  +  +  )  ,    generate (  )  )  ;", "}    finally    {", "this . writeLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createNewMasterKey"], "fileName": "org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "this . readLock . lock (  )  ;", "try    {", "return   this . currentMasterKey . gsterKey (  )  ;", "}    finally    {", "this . readLock . unlock (  )  ;", "}", "}", "METHOD_END"], "methodName": ["getCurrentKey"], "fileName": "org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId    =    identifier . getNodeId (  )  ;", "InetSocketAddress   addr    =    NetUtils . createSocketAddrForHost ( nodeId . getHost (  )  ,    nodeId . getPort (  )  )  ;", "Token   nmToken    =    Token . newInstance ( identifier . getBytes (  )  ,    KIND . toString (  )  ,    password ,    SUtil . buildTokenService ( addr )  . toString (  )  )  ;", "return   nmToken ;", "}", "METHOD_END"], "methodName": ["newInstance"], "fileName": "org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "if    ( BaseNMTokenSecretManager . LOG . isDebugEnabled (  )  )     {", "BaseNMTokenSecretManager . LOG . debug (  (  (  (  (  (  \" creating   password   for    \"     +     ( identifier . getApplicationAttemptId (  )  )  )     +     \"    for   user    \"  )     +     ( identifier . getApplicationSubmitter (  )  )  )     +     \"    to   run   on   NM    \"  )     +     ( identifier . getNodeId (  )  )  )  )  ;", "}", "return   createPassword ( identifier . getBytes (  )  ,    masterKey . getSecretKey (  )  )  ;", "}", "METHOD_END"], "methodName": ["retrivePasswordInternal"], "fileName": "org.apache.hadoop.yarn.server.security.BaseNMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "return   this . masterKeyRecord ;", "}", "METHOD_END"], "methodName": ["getMasterKey"], "fileName": "org.apache.hadoop.yarn.server.security.MasterKeyData"}, {"methodBody": ["METHOD_START", "{", "return   this . generatedSecretKey ;", "}", "METHOD_END"], "methodName": ["getSecretKey"], "fileName": "org.apache.hadoop.yarn.server.security.MasterKeyData"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    filterConfig    =    new   HashMap < String ,    String >  (  )  ;", "filterConfig . put ( COOKIE _ PATH ,    cookiePath )  ;", "for    ( Map . Entry < String ,    String >    entry    :    conf )     {", "String   name    =    entry . getKey (  )  ;", "if    ( name . startsWith ( configPrefix )  )     {", "String   value    =    conf . get ( name )  ;", "name    =    name . substring ( configPrefix . length (  )  )  ;", "filterConfig . put ( name ,    value )  ;", "}", "}", "String   signatureSecretFile    =    filterConfig . get ( signatureSecretFileProperty )  ;", "if    ( signatureSecretFile    !  =    null )     {", "Reader   reader    =    null ;", "try    {", "StringBuilder   secret    =    new   StringBuilder (  )  ;", "reader    =    new   InputStreamReader ( new   FileInputStream ( signatureSecretFile )  ,     \" UTF -  8  \"  )  ;", "int   c    =    reader . read (  )  ;", "while    ( c    >     (  -  1  )  )     {", "secret . append (  (  ( char )     ( c )  )  )  ;", "c    =    reader . read (  )  ;", "}", "filterConfig . put ( SIGNATURE _ SECRET ,    secret . toString (  )  )  ;", "}    catch    ( IOException   ex )     {", "if    ( UserGroupInformation . isSEnabled (  )  )     {", "throw   new   RuntimeException (  (  \" Could   not   read   HTTP   signature   secret   file :     \"     +    signatureSecretFile )  )  ;", "}", "}    finally    {", "IOUtils . closeQuietly ( reader )  ;", "}", "}", "String   bindAddress    =    conf . get ( BIND _ ADDRESS )  ;", "String   principal    =    filterConfig . get ( kerberosPrincipalProperty )  ;", "if    ( principal    !  =    null )     {", "try    {", "principal    =    SUtil . getServerPrincipal ( principal ,    bindAddress )  ;", "}    catch    ( IOException   ex )     {", "throw   new   RuntimeException (  (  \" Could   not   resolve   Kerberos   principal   name :     \"     +     ( ex . toString (  )  )  )  ,    ex )  ;", "}", "filterConfig . put ( PRINCIPAL ,    principal )  ;", "}", "return   filterConfig ;", "}", "METHOD_END"], "methodName": ["createFilterConfig"], "fileName": "org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilterInitializer"}, {"methodBody": ["METHOD_START", "{", "return   id ;", "}", "METHOD_END"], "methodName": ["getId"], "fileName": "org.apache.hadoop.yarn.server.timeline.EntityIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.yarn.server.timeline.EntityIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   GenericObjectMapper . read ( b ,     0  )  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.yarn.server.timeline.GenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "if    (  ( b    =  =    null )     |  |     (  ( b . length )     =  =     0  )  )     {", "return   null ;", "}", "return    . OBJECT _ READER . readValue ( b ,    offset ,     (  ( b . length )     -    offset )  )  ;", "}", "METHOD_END"], "methodName": ["read"], "fileName": "org.apache.hadoop.yarn.server.timeline.GenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "long   l    =     ( b [ offset ]  )     &     2  5  5  ;", "for    ( int   i    =     1  ;    i    <     8  ;    i +  +  )     {", "l    =    l    <  <     8  ;", "l    =    l    |     (  ( b [  ( offset    +    i )  ]  )     &     2  5  5  )  ;", "}", "return   l    ^     9  2  2  3  3  7  2  0  3  6  8  5  4  7  7  5  8  0  7 L ;", "}", "METHOD_END"], "methodName": ["readReverseOrderedLong"], "fileName": "org.apache.hadoop.yarn.server.timeline.GenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "if    ( o    =  =    null )     {", "return    . EMPTY _ BYTES ;", "}", "return    . OBJECT _ WRITER . writeValueAsBytes ( o )  ;", "}", "METHOD_END"], "methodName": ["write"], "fileName": "org.apache.hadoop.yarn.server.timeline.GenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    b    =    new   byte [  8  ]  ;", "return    . writeReverseOrderedLong ( l ,    b ,     0  )  ;", "}", "METHOD_END"], "methodName": ["writeReverseOrderedLong"], "fileName": "org.apache.hadoop.yarn.server.timeline.GenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "b [ offset ]     =     (  ( byte )     (  1  2  7     ^     (  ( l    >  >     5  6  )     &     2  5  5  )  )  )  ;", "for    ( int   i    =    offset    +     1  ;    i    <     ( offset    +     7  )  ;    i +  +  )     {", "b [ i ]     =     (  ( byte )     (  2  5  5     ^     (  ( l    >  >     (  8     *     (  7     -    i )  )  )     &     2  5  5  )  )  )  ;", "}", "b [  ( offset    +     7  )  ]     =     (  ( byte )     (  2  5  5     ^     ( l    &     2  5  5  )  )  )  ;", "return   b ;", "}", "METHOD_END"], "methodName": ["writeReverseOrderedLong"], "fileName": "org.apache.hadoop.yarn.server.timeline.GenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "LeveldbTimelineStore . KeyParser   kp    =    new   LeveldbTimelineStore . KeyParser ( key ,    offset )  ;", "String   name    =    kp . getNextString (  )  ;", "Object   value    =    GenericObjectMapper . read ( key ,    kp . getOffset (  )  )  ;", "entity . addPrimaryFilter ( name ,    value )  ;", "}", "METHOD_END"], "methodName": ["addPrimaryFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . INDEXED _ ENTRY _ PREFIX )  . add ( primaryFilterName )  . add ( GenericObjectMapper . write ( primaryFilterValue )  ,    true )  . add ( key )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["addPrimaryFilterToKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "LeveldbTimelineStore . KeyParser   kp    =    new   LeveldbTimelineStore . KeyParser ( key ,    offset )  ;", "String   type    =    kp . getNextString (  )  ;", "String   id    =    kp . getNextString (  )  ;", "entity . addRelatedEntity ( type ,    id )  ;", "}", "METHOD_END"], "methodName": ["addRelatedEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "LeveldbTimelineStore . StartAndInsertTime   startAndInsertTime    =    null ;", "byte [  ]    b    =    LeveldbTimelineStore . createStartTimeLookupKey ( entity . getId (  )  ,    entity . getType (  )  )  ;", "byte [  ]    v    =    db . get ( b )  ;", "if    ( v    =  =    null )     {", "if    ( suggestedStartTime    =  =    null )     {", "return   null ;", "}", "startAndInsertTime    =    new   LeveldbTimelineStore . StartAndInsertTime ( suggestedStartTime ,    System . currentTimeMillis (  )  )  ;", "v    =    new   byte [  1  6  ]  ;", "GenericObjectMapper . writeReverseOrderedLong ( suggestedStartTime ,    v ,     0  )  ;", "GenericObjectMapper . writeReverseOrderedLong ( startAndInsertTime . insertTime ,    v ,     8  )  ;", "WriteOptions   writeOptions    =    new   WriteOptions (  )  ;", "writeOptions . sync ( true )  ;", "db . put ( b ,    v ,    writeOptions )  ;", "} else    {", "startAndInsertTime    =    new   LeveldbTimelineStore . StartAndInsertTime ( GenericObjectMapper . readReverseOrderedLong ( v ,     0  )  ,    GenericObjectMapper . readReverseOrderedLong ( v ,     8  )  )  ;", "}", "startTimeWriteCache . put ( entity ,    startAndInsertTime )  ;", "startTimeReadCache . put ( entity ,    startAndInsertTime . startTime )  ;", "return   startAndInsertTime ;", "}", "METHOD_END"], "methodName": ["checkStartTimeInDb"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "Version   loadedVersion    =    loadVersion (  )  ;", ". LOG . info (  (  \" Loaded   timeline   store   version   info    \"     +    loadedVersion )  )  ;", "if    ( loadedVersion . equals ( getCurrentVersion (  )  )  )     {", "return ;", "}", "if    ( loadedVersion . isCompatibleTo ( getCurrentVersion (  )  )  )     {", ". LOG . info (  (  \" Storing   timeline   store   version   info    \"     +     ( getCurrentVersion (  )  )  )  )  ;", "dbStoreVersion (  . CURRENT _ VERSION _ INFO )  ;", "} else    {", "String   incompatibleMessage    =     (  (  \" Incompatible   version   for   timeline   store :    expecting   version    \"     +     ( getCurrentVersion (  )  )  )     +     \"  ,    but   loading   version    \"  )     +    loadedVersion ;", ". LOG . fatal ( incompatibleMessage )  ;", "throw   new   IOException ( incompatibleMessage )  ;", "}", "}", "METHOD_END"], "methodName": ["checkVersion"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "startTimeWriteCache . clear (  )  ;", "startTimeReadCache . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clearStartTimeCache"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . add ( revStartTime )  . add ( entityId )  . add ( LeveldbTimelineStore . EVENTS _ COLUMN )  . add ( revEventTimestamp )  . add ( eventType )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["createEntityEventKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . add ( revStartTime )  . add ( entityId )  . getBytesForLookup (  )  ;", "}", "METHOD_END"], "methodName": ["createEntityMarkerKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . add ( revStartTime )  . add ( entityId )  . add ( LeveldbTimelineStore . OTHER _ INFO _ COLUMN )  . add ( name )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["createOtherInfoKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . add ( revStartTime )  . add ( entityId )  . add ( LeveldbTimelineStore . PRIMARY _ FILTERS _ COLUMN )  . add ( name )  . add ( GenericObjectMapper . write ( value )  )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["createPrimaryFilterKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . add ( revStartTime )  . add ( entityId )  . add ( LeveldbTimelineStore . RELATED _ ENTITIES _ COLUMN )  . add ( relatedEntityType )  . add ( relatedEntityId )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["createRelatedEntityKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . add ( revStartTime )  . add ( entityId )  . add ( LeveldbTimelineStore . INVISIBLE _ REVERSE _ RELATED _ ENTITIES _ COLUMN )  . add ( relatedEntityType )  . add ( relatedEntityId )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["createReverseRelatedEntityKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . KeyBuilder . newInstance (  )  . add ( LeveldbTimelineStore . START _ TIME _ LOOKUP _ PREFIX )  . add ( entityType )  . add ( entityId )  . getBytes (  )  ;", "}", "METHOD_END"], "methodName": ["createStartTimeLookupKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "String   key    =    LeveldbTimelineStore . TIMELINE _ STORE _ VERSION _ KEY ;", "byte [  ]    data    =     (  ( VersionPBImpl )     ( state )  )  . getProto (  )  . toByteArray (  )  ;", "try    {", "db . put ( bytes ( key )  ,    data )  ;", "}    catch    ( DBException   e )     {", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["dbStoreVersion"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "for    ( iterator . seek ( prefix )  ;    iterator . hasNext (  )  ;    iterator . next (  )  )     {", "byte [  ]    key    =    iterator . peekNext (  )  . getKey (  )  ;", "if    (  !  (  . prefixMatches ( prefix ,    prefix . length ,    key )  )  )     {", "break ;", "}", "writeBatch . delete ( key )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteKeysWithPrefix"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "WriteBatch   writeBatch    =    null ;", "try    {", ". KeyBuilder   kb    =     . KeyBuilder . newInstance (  )  . add (  . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  ;", "byte [  ]    typePrefix    =    kb . getBytesForLookup (  )  ;", "kb . add ( reverseTimestamp )  ;", "if    (  ! seeked )     {", "iterator . seek ( kb . getBytesForLookup (  )  )  ;", "}", "if    (  !  ( iterator . hasNext (  )  )  )     {", "return   false ;", "}", "byte [  ]    entityKey    =    iterator . peekNext (  )  . getKey (  )  ;", "if    (  !  (  . prefixMatches ( typePrefix ,    typePrefix . length ,    entityKey )  )  )     {", "return   false ;", "}", ". KeyParser   kp    =    new    . KeyParser ( entityKey ,     (  ( typePrefix . length )     +     8  )  )  ;", "String   entityId    =    kp . getNextString (  )  ;", "int   prefixlen    =    kp . getOffset (  )  ;", "byte [  ]    deletePrefix    =    new   byte [ prefixlen ]  ;", "System . arraycopy ( entityKey ,     0  ,    deletePrefix ,     0  ,    prefixlen )  ;", "writeBatch    =    db . createWriteBatch (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Deleting   entity   type :  \"     +    entityType )     +     \"    id :  \"  )     +    entityId )  )  ;", "}", "writeBatch . delete (  . createStartTimeLookupKey ( entityId ,    entityType )  )  ;", "EntityIdentifier   entityIdentifier    =    new   EntityIdentifier ( entityId ,    entityType )  ;", "startTimeReadCache . remove ( entityIdentifier )  ;", "startTimeWriteCache . remove ( entityIdentifier )  ;", "for    (  ;    iterator . hasNext (  )  ;    iterator . next (  )  )     {", "byte [  ]    key    =    iterator . peekNext (  )  . getKey (  )  ;", "if    (  !  (  . prefixMatches ( entityKey ,    prefixlen ,    key )  )  )     {", "break ;", "}", "writeBatch . delete ( key )  ;", "if    (  ( key . length )     =  =    prefixlen )     {", "continue ;", "}", "if    (  ( key [ prefixlen ]  )     =  =     (  . PRIMARY _ FILTERS _ COLUMN [  0  ]  )  )     {", "kp    =    new    . KeyParser ( key ,     ( prefixlen    +     (  . PRIMARY _ FILTERS _ COLUMN . length )  )  )  ;", "String   name    =    kp . getNextString (  )  ;", "Object   value    =    GenericObjectMapper . read ( key ,    kp . getOffset (  )  )  ;", "deleteKeysWithPrefix ( writeBatch ,     . addPrimaryFilterToKey ( name ,    value ,    deletePrefix )  ,    pfIterator )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  \" Deleting   entity   type :  \"     +    entityType )     +     \"    id :  \"  )     +    entityId )     +     \"    primary   filter   entry    \"  )     +    name )     +     \"     \"  )     +    value )  )  ;", "}", "} else", "if    (  ( key [ prefixlen ]  )     =  =     (  . RELATED _ ENTITIES _ COLUMN [  0  ]  )  )     {", "kp    =    new    . KeyParser ( key ,     ( prefixlen    +     (  . RELATED _ ENTITIES _ COLUMN . length )  )  )  ;", "String   type    =    kp . getNextString (  )  ;", "String   id    =    kp . getNextString (  )  ;", "byte [  ]    relatedEntityStartTime    =    getStartTime ( id ,    type )  ;", "if    ( relatedEntityStartTime    =  =    null )     {", ". LOG . warn (  (  (  (  (  (  (  (  (  (  \" Found   no   start   time   for    \"     +     \" related   entity    \"  )     +    id )     +     \"    of   type    \"  )     +    type )     +     \"    while    \"  )     +     \" deleting    \"  )     +    entityId )     +     \"    of   type    \"  )     +    entityType )  )  ;", "continue ;", "}", "writeBatch . delete (  . createReverseRelatedEntityKey ( id ,    type ,    relatedEntityStartTime ,    entityId ,    entityType )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  \" Deleting   entity   type :  \"     +    entityType )     +     \"    id :  \"  )     +    entityId )     +     \"    from   invisible   reverse   related   entity    \"  )     +     \" entry   of   type :  \"  )     +    type )     +     \"    id :  \"  )     +    id )  )  ;", "}", "} else", "if    (  ( key [ prefixlen ]  )     =  =     (  . INVISIBLE _ REVERSE _ RELATED _ ENTITIES _ COLUMN [  0  ]  )  )     {", "kp    =    new    . KeyParser ( key ,     ( prefixlen    +     (  . INVISIBLE _ REVERSE _ RELATED _ ENTITIES _ COLUMN . length )  )  )  ;", "String   type    =    kp . getNextString (  )  ;", "String   id    =    kp . getNextString (  )  ;", "byte [  ]    relatedEntityStartTime    =    getStartTime ( id ,    type )  ;", "if    ( relatedEntityStartTime    =  =    null )     {", ". LOG . warn (  (  (  (  (  (  (  (  (  (  \" Found   no   start   time   for   reverse    \"     +     \" related   entity    \"  )     +    id )     +     \"    of   type    \"  )     +    type )     +     \"    while    \"  )     +     \" deleting    \"  )     +    entityId )     +     \"    of   type    \"  )     +    entityType )  )  ;", "continue ;", "}", "writeBatch . delete (  . createRelatedEntityKey ( id ,    type ,    relatedEntityStartTime ,    entityId ,    entityType )  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  \" Deleting   entity   type :  \"     +    entityType )     +     \"    id :  \"  )     +    entityId )     +     \"    from   related   entity   entry   of   type :  \"  )     +    type )     +     \"    id :  \"  )     +    id )  )  ;", "}", "}", "}", "WriteOptions   writeOptions    =    new   WriteOptions (  )  ;", "writeOptions . sync ( true )  ;", "db . write ( writeBatch ,    writeOptions )  ;", "return   true ;", "}    finally    {", "IOUtils . cleanup (  . LOG ,    writeBatch )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteNextEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    reverseTimestamp    =    GenericObjectMapper . writeReverseOrderedLong ( timestamp )  ;", "long   totalCount    =     0  ;", "long   t 1     =    System . currentTimeMillis (  )  ;", "try    {", "List < String >    entityTypes    =    getEntityTypes (  )  ;", "for    ( String   entityType    :    entityTypes )     {", "DBIterator   iterator    =    null ;", "DBIterator   pfIterator    =    null ;", "long   typeCount    =     0  ;", "try    {", "deleteLock . writeLock (  )  . lock (  )  ;", "iterator    =    getDbIterator ( false )  ;", "pfIterator    =    getDbIterator ( false )  ;", "if    (  (  ( deletionThread )     !  =    null )     &  &     ( deletionThread . isInterrupted (  )  )  )     {", "throw   new   InterruptedException (  )  ;", "}", "boolean   seeked    =    false ;", "while    ( deleteNextEntity ( entityType ,    reverseTimestamp ,    iterator ,    pfIterator ,    seeked )  )     {", "typeCount +  +  ;", "totalCount +  +  ;", "seeked    =    true ;", "if    (  (  ( deletionThread )     !  =    null )     &  &     ( deletionThread . isInterrupted (  )  )  )     {", "throw   new   InterruptedException (  )  ;", "}", "}", "}    catch    ( IOException   e )     {", ". LOG . error (  (  (  \" Got   IOException   while   deleting   entities   for   type    \"     +    entityType )     +     \"  ,    continuing   to   next   type \"  )  ,    e )  ;", "}    finally    {", "IOUtils . cleanup (  . LOG ,    iterator ,    pfIterator )  ;", "deleteLock . writeLock (  )  . unlock (  )  ;", "if    ( typeCount    >     0  )     {", ". LOG . info (  (  (  (  \" Deleted    \"     +    typeCount )     +     \"    entities   of   type    \"  )     +    entityType )  )  ;", "}", "}", "}", "}    finally    {", "long   t 2     =    System . currentTimeMillis (  )  ;", ". LOG . info (  (  (  (  (  (  (  \" Discarded    \"     +    totalCount )     +     \"    entities   for   timestamp    \"  )     +    timestamp )     +     \"    and   earlier   in    \"  )     +     (  ( t 2     -    t 1  )     /     1  0  0  0  .  0  )  )     +     \"    seconds \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["discardOldEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "EntityIdentifier   entity    =    new   EntityIdentifier ( entityId ,    entityType )  ;", "if    ( startTime    =  =    null )     {", "if    ( startTimeWriteCache . containsKey ( entity )  )     {", "return   startTimeWriteCache . get ( entity )  ;", "} else    {", "if    ( events    !  =    null )     {", "Long   min    =    Long . MAX _ VALUE ;", "for    ( Event   e    :    events )     {", "if    ( min    >     ( e . getTimestamp (  )  )  )     {", "min    =    e . getTimestamp (  )  ;", "}", "}", "startTime    =    min ;", "}", "return   checkStartTimeInDb ( entity ,    startTime )  ;", "}", "} else    {", "if    ( startTimeWriteCache . containsKey ( entity )  )     {", "return   startTimeWriteCache . get ( entity )  ;", "} else    {", "return   checkStartTimeInDb ( entity ,    startTime )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["getAndSetStartTime"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   LeveldbTimelineStore . CURRENT _ VERSION _ INFO ;", "}", "METHOD_END"], "methodName": ["getCurrentVersion"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "ReadOptions   readOptions    =    new   ReadOptions (  )  ;", "readOptions . fillC ( fillC )  ;", "return   db . iterator ( readOptions )  ;", "}", "METHOD_END"], "methodName": ["getDbIterator"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "if    ( fields    =  =    null )     {", "fields    =    EnumSet . allOf ( TimelineReader . Field . class )  ;", "}", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "boolean   events    =    false ;", "boolean   lastEvent    =    false ;", "if    ( fields . contains ( TimelineReader . Field . EVENTS )  )     {", "events    =    true ;", "} else", "if    ( fields . contains ( TimelineReader . Field . LAST _ EVENT _ ONLY )  )     {", "lastEvent    =    true ;", "} else    {", "entity . setEvents ( null )  ;", "}", "boolean   relatedEntities    =    false ;", "if    ( fields . contains ( TimelineReader . Field . RELATED _ ENTITIES )  )     {", "relatedEntities    =    true ;", "} else    {", "entity . setRelatedEntities ( null )  ;", "}", "boolean   primaryFilters    =    false ;", "if    ( fields . contains ( TimelineReader . Field . PRIMARY _ FILTERS )  )     {", "primaryFilters    =    true ;", "} else    {", "entity . setPrimaryFilters ( null )  ;", "}", "boolean   otherInfo    =    false ;", "if    ( fields . contains ( TimelineReader . Field . OTHER _ INFO )  )     {", "otherInfo    =    true ;", "} else    {", "entity . setOtherInfo ( null )  ;", "}", "for    (  ;    iterator . hasNext (  )  ;    iterator . next (  )  )     {", "byte [  ]    key    =    iterator . peekNext (  )  . getKey (  )  ;", "if    (  !  ( LeveldbTimelineStore . prefixMatches ( prefix ,    prefixlen ,    key )  )  )     {", "break ;", "}", "if    (  ( key . length )     =  =    prefixlen )     {", "continue ;", "}", "if    (  ( key [ prefixlen ]  )     =  =     ( LeveldbTimelineStore . PRIMARY _ FILTERS _ COLUMN [  0  ]  )  )     {", "if    ( primaryFilters )     {", "LeveldbTimelineStore . addPrimaryFilter ( entity ,    key ,     ( prefixlen    +     ( LeveldbTimelineStore . PRIMARY _ FILTERS _ COLUMN . length )  )  )  ;", "}", "} else", "if    (  ( key [ prefixlen ]  )     =  =     ( LeveldbTimelineStore . OTHER _ INFO _ COLUMN [  0  ]  )  )     {", "if    ( otherInfo )     {", "entity . addOtherInfo ( LeveldbTimelineStore . parseRemainingKey ( key ,     ( prefixlen    +     ( LeveldbTimelineStore . OTHER _ INFO _ COLUMN . length )  )  )  ,    GenericObjectMapper . read ( iterator . peekNext (  )  . getValue (  )  )  )  ;", "}", "} else", "if    (  ( key [ prefixlen ]  )     =  =     ( LeveldbTimelineStore . RELATED _ ENTITIES _ COLUMN [  0  ]  )  )     {", "if    ( relatedEntities )     {", "LeveldbTimelineStore . addRelatedEntity ( entity ,    key ,     ( prefixlen    +     ( LeveldbTimelineStore . RELATED _ ENTITIES _ COLUMN . length )  )  )  ;", "}", "} else", "if    (  ( key [ prefixlen ]  )     =  =     ( LeveldbTimelineStore . EVENTS _ COLUMN [  0  ]  )  )     {", "if    ( events    |  |     ( lastEvent    &  &     (  ( entity . getEvents (  )  . size (  )  )     =  =     0  )  )  )     {", "api . records . timeline . TimelineEvent   event    =    LeveldbTimelineStore . getEntityEvent ( null ,    key ,     ( prefixlen    +     ( LeveldbTimelineStore . EVENTS _ COLUMN . length )  )  ,    iterator . peekNext (  )  . getValue (  )  )  ;", "if    ( event    !  =    null )     {", "entity . addEvent ( event )  ;", "}", "}", "} else    {", "if    (  ( key [ prefixlen ]  )     !  =     ( LeveldbTimelineStore . INVISIBLE _ REVERSE _ RELATED _ ENTITIES _ COLUMN [  0  ]  )  )     {", "LeveldbTimelineStore . LOG . warn ( String . format (  (  \" Found   unexpected   column   for   entity    % s   of    \"     +     \" type    % s    (  0 x %  0  2 x )  \"  )  ,    entityId ,    entityType ,    key [ prefixlen ]  )  )  ;", "}", "}", "}", "entity . setEntityId ( entityId )  ;", "entity . setEntityType ( entityType )  ;", "entity . setStartTime ( startTime )  ;", "return   entity ;", "}", "METHOD_END"], "methodName": ["getEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "DBIterator   iterator    =    null ;", "try    {", ". KeyBuilder   kb    =     . KeyBuilder . newInstance (  )  . add ( base )  . add ( entityType )  ;", "byte [  ]    prefix    =    kb . getBytesForLookup (  )  ;", "if    ( endtime    =  =    null )     {", "endtime    =    Long . MAX _ VALUE ;", "}", "byte [  ]    first    =    null ;", "if    ( fromId    !  =    null )     {", "Long   fromIdStartTime    =    getStartTimeLong ( fromId ,    entityType )  ;", "if    ( fromIdStartTime    =  =    null )     {", "return   new   TimelineEntities (  )  ;", "}", "if    ( fromIdStartTime    <  =    endtime )     {", "first    =    kb . add ( GenericObjectMapper . writeReverseOrderedLong ( fromIdStartTime )  )  . add ( fromId )  . getBytesForLookup (  )  ;", "}", "}", "if    ( first    =  =    null )     {", "first    =    kb . add ( GenericObjectMapper . writeReverseOrderedLong ( endtime )  )  . getBytesForLookup (  )  ;", "}", "byte [  ]    last    =    null ;", "if    ( starttime    !  =    null )     {", "last    =     . KeyBuilder . newInstance (  )  . add ( base )  . add ( entityType )  . add ( GenericObjectMapper . writeReverseOrderedLong ( starttime )  )  . getBytesForLookup (  )  ;", "}", "if    ( limit    =  =    null )     {", "limit    =    TimelineReader . DEFAULT _ LIMIT ;", "}", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "iterator    =    db . iterator (  )  ;", "iterator . seek ( first )  ;", "while    (  (  ( entities . getEntities (  )  . size (  )  )     <    limit )     &  &     ( iterator . hasNext (  )  )  )     {", "byte [  ]    key    =    iterator . peekNext (  )  . getKey (  )  ;", "if    (  (  !  (  . prefixMatches ( prefix ,    prefix . length ,    key )  )  )     |  |     (  ( last    !  =    null )     &  &     (  ( WritableComparator . compareBytes ( key ,     0  ,    key . length ,    last ,     0  ,    last . length )  )     >     0  )  )  )     {", "break ;", "}", ". KeyParser   kp    =    new    . KeyParser ( key ,    prefix . length )  ;", "Long   startTime    =    kp . getNextLong (  )  ;", "String   entityId    =    kp . getNextString (  )  ;", "if    ( fromTs    !  =    null )     {", "long   insertTime    =    GenericObjectMapper . readReverseOrderedLong ( iterator . peekNext (  )  . getValue (  )  ,     0  )  ;", "if    ( insertTime    >    fromTs )     {", "byte [  ]    firstKey    =    key ;", "while    (  ( iterator . hasNext (  )  )     &  &     (  . prefixMatches ( firstKey ,    kp . getOffset (  )  ,    key )  )  )     {", "iterator . next (  )  ;", "key    =    iterator . peekNext (  )  . getKey (  )  ;", "}", "continue ;", "}", "}", "TimelineEntity   entity    =     . getEntity ( entityId ,    entityType ,    startTime ,    fields ,    iterator ,    key ,    kp . getOffset (  )  )  ;", "boolean   filterPassed    =    true ;", "if    ( secondaryFilters    !  =    null )     {", "for    ( NameValuePair   filter    :    secondaryFilters )     {", "Object   v    =    entity . getOtherInfo (  )  . get ( filter . getName (  )  )  ;", "if    ( v    =  =    null )     {", "Set < Object >    vs    =    entity . getPrimaryFilters (  )  . get ( filter . getName (  )  )  ;", "if    (  ( vs    !  =    null )     &  &     (  !  ( vs . contains ( filter . getValue (  )  )  )  )  )     {", "filterPassed    =    false ;", "break ;", "}", "} else", "if    (  !  ( v . equals ( filter . getValue (  )  )  )  )     {", "filterPassed    =    false ;", "break ;", "}", "}", "}", "if    ( filterPassed )     {", "entities . addEntity ( entity )  ;", "}", "}", "return   entities ;", "}    finally    {", "IOUtils . cleanup (  . LOG ,    iterator )  ;", "}", "}", "METHOD_END"], "methodName": ["getEntityByTime"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "LeveldbTimelineStore . KeyParser   kp    =    new   LeveldbTimelineStore . KeyParser ( key ,    offset )  ;", "long   ts    =    kp . getNextLong (  )  ;", "String   tstype    =    kp . getNextString (  )  ;", "if    (  ( eventTypes    =  =    null )     |  |     ( eventTypes . contains ( tstype )  )  )     {", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setTimestamp ( ts )  ;", "event . setEventType ( tstype )  ;", "Object   o    =    GenericObjectMapper . read ( value )  ;", "if    ( o    =  =    null )     {", "event . setEventInfo ( null )  ;", "} else", "if    ( o   instanceof   Map )     {", "@ SuppressWarnings (  \" unchecked \"  )", "Map < String ,    Object >    m    =     (  ( Map < String ,    Object >  )     ( o )  )  ;", "event . setEventInfo ( m )  ;", "} else    {", "throw   new   IOException (  \" Couldn ' t   deserialize   event   info   map \"  )  ;", "}", "return   event ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getEntityEvent"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "DBIterator   iterator    =    null ;", "try    {", "iterator    =    getDbIterator ( false )  ;", "List < String >    entityTypes    =    new   ArrayList < String >  (  )  ;", "iterator . seek (  . ENTITY _ ENTRY _ PREFIX )  ;", "while    ( iterator . hasNext (  )  )     {", "byte [  ]    key    =    iterator . peekNext (  )  . getKey (  )  ;", "if    (  ( key [  0  ]  )     !  =     (  . ENTITY _ ENTRY _ PREFIX [  0  ]  )  )     {", "break ;", "}", ". KeyParser   kp    =    new    . KeyParser ( key ,     . ENTITY _ ENTRY _ PREFIX . length )  ;", "String   entityType    =    kp . getNextString (  )  ;", "entityTypes . add ( entityType )  ;", "byte [  ]    lookupKey    =     . KeyBuilder . newInstance (  )  . add (  . ENTITY _ ENTRY _ PREFIX )  . add ( entityType )  . getBytesForLookup (  )  ;", "if    (  ( lookupKey [  (  ( lookupKey . length )     -     1  )  ]  )     !  =     0  )     {", "throw   new   IOException (  \" Found   unexpected   end   byte   in   lookup   key \"  )  ;", "}", "lookupKey [  (  ( lookupKey . length )     -     1  )  ]     =     1  ;", "iterator . seek ( lookupKey )  ;", "}", "return   entityTypes ;", "}    finally    {", "IOUtils . cleanup (  . LOG ,    iterator )  ;", "}", "}", "METHOD_END"], "methodName": ["getEntityTypes"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "Long   l    =    getStartTimeLong ( entityId ,    entityType )  ;", "return   l    =  =    null    ?    null    :    GenericObjectMapper . writeReverseOrderedLong ( l )  ;", "}", "METHOD_END"], "methodName": ["getStartTime"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "EntityIdentifier   entity    =    new   EntityIdentifier ( entityId ,    entityType )  ;", "if    ( startTimeReadCache . containsKey ( entity )  )     {", "return   startTimeReadCache . get ( entity )  ;", "} else    {", "byte [  ]    b    =     . createStartTimeLookupKey ( entity . getId (  )  ,    entity . getType (  )  )  ;", "byte [  ]    v    =    db . get ( b )  ;", "if    ( v    =  =    null )     {", "return   null ;", "} else    {", "Long   l    =    GenericObjectMapper . readReverseOrderedLong ( v ,     0  )  ;", "startTimeReadCache . put ( entity ,    l )  ;", "return   l ;", "}", "}", "}", "METHOD_END"], "methodName": ["getStartTimeLong"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   conf . getInt ( TIMELINE _ SERVICE _ LEVELDB _ START _ TIME _ READ _ CACHE _ SIZE ,    DEFAULT _ TIMELINE _ SERVICE _ LEVELDB _ START _ TIME _ READ _ CACHE _ SIZE )  ;", "}", "METHOD_END"], "methodName": ["getStartTimeReadCacheSize"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   conf . getInt ( TIMELINE _ SERVICE _ LEVELDB _ START _ TIME _ WRITE _ CACHE _ SIZE ,    DEFAULT _ TIMELINE _ SERVICE _ LEVELDB _ START _ TIME _ WRITE _ CACHE _ SIZE )  ;", "}", "METHOD_END"], "methodName": ["getStartTimeWriteCacheSize"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    data    =    db . get ( bytes ( LeveldbTimelineStore . TIMELINE _ STORE _ VERSION _ KEY )  )  ;", "if    (  ( data    =  =    null )     |  |     (  ( data . length )     =  =     0  )  )     {", "return   Version . newInstance (  1  ,     0  )  ;", "}", "Version   version    =    new   VersionPBImpl ( VersionProto . parseFrom ( data )  )  ;", "return   version ;", "}", "METHOD_END"], "methodName": ["loadVersion"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   new   String ( b ,    offset ,     (  ( b . length )     -    offset )  )  ;", "}", "METHOD_END"], "methodName": ["parseRemainingKey"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( b . length )     <    prefixlen )     {", "return   false ;", "}", "return    ( WritableCompara . compareBytes ( prefix ,     0  ,    prefixlen ,    b ,     0  ,    prefixlen )  )     =  =     0  ;", "}", "METHOD_END"], "methodName": ["prefixMatches"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "LeveldbTimelineStore . LockMap . CountingReentrantLock < EntityIdentifier >    lock    =    writeLocks . getLock ( new   EntityIdentifier ( entity . getEntityId (  )  ,    entity . getEntityType (  )  )  )  ;", "lock . lock (  )  ;", "WriteBatch   writeBatch    =    null ;", "List < EntityIdentifier >    relatedEntitiesWithoutStartTimes    =    new   ArrayList < EntityIdentifier >  (  )  ;", "byte [  ]    revStartTime    =    null ;", "try    {", "writeBatch    =    db . createWriteBatch (  )  ;", "List < TimelineEvent >    events    =    entity . getEvents (  )  ;", "LeveldbTimelineStore . StartAndInsertTime   startAndInsertTime    =    getAndSetStartTime ( entity . getEntityId (  )  ,    entity . getEntityType (  )  ,    entity . getStartTime (  )  ,    events )  ;", "if    ( startAndInsertTime    =  =    null )     {", "TimelinePutError   error    =    new   TimelinePutError (  )  ;", "error . setEntityId ( entity . getEntityId (  )  )  ;", "error . setEntityType ( entity . getEntityType (  )  )  ;", "error . setErrorCode ( NO _ START _ TIME )  ;", "response . addError ( error )  ;", "return ;", "}", "revStartTime    =    GenericObjectMapper . writeReverseOrderedLong ( startAndInsertTime . startTime )  ;", "Map < String ,    Set < Object >  >    primaryFilters    =    entity . getPrimaryFilters (  )  ;", "byte [  ]    markerKey    =    LeveldbTimelineStore . createEntityMarkerKey ( entity . getEntityId (  )  ,    entity . getEntityType (  )  ,    revStartTime )  ;", "byte [  ]    markerValue    =    GenericObjectMapper . writeReverseOrderedLong ( startAndInsertTime . insertTime )  ;", "writeBatch . put ( markerKey ,    markerValue )  ;", "LeveldbTimelineStore . writePrimaryFilterEntries ( writeBatch ,    primaryFilters ,    markerKey ,    markerValue )  ;", "if    (  ( events    !  =    null )     &  &     (  !  ( events . isEmpty (  )  )  )  )     {", "for    ( TimelineEvent   event    :    events )     {", "byte [  ]    revts    =    GenericObjectMapper . writeReverseOrderedLong ( event . getTimestamp (  )  )  ;", "byte [  ]    key    =    LeveldbTimelineStore . createEntityEventKey ( entity . getEntityId (  )  ,    entity . getEntityType (  )  ,    revStartTime ,    revts ,    event . getEventType (  )  )  ;", "byte [  ]    value    =    GenericObjectMapper . write ( event . getEventInfo (  )  )  ;", "writeBatch . put ( key ,    value )  ;", "LeveldbTimelineStore . writePrimaryFilterEntries ( writeBatch ,    primaryFilters ,    key ,    value )  ;", "}", "}", "Map < String ,    Set < String >  >    relatedEntities    =    entity . getRelatedEntities (  )  ;", "if    (  ( relatedEntities    !  =    null )     &  &     (  !  ( relatedEntities . isEmpty (  )  )  )  )     {", "for    ( Map . Entry < String ,    Set < String >  >    relatedEntityList    :    relatedEntities . entrySet (  )  )     {", "String   relatedEntityType    =    relatedEntityList . getKey (  )  ;", "for    ( String   relatedEntityId    :    relatedEntityList . getValue (  )  )     {", "byte [  ]    key    =    LeveldbTimelineStore . createReverseRelatedEntityKey ( entity . getEntityId (  )  ,    entity . getEntityType (  )  ,    revStartTime ,    relatedEntityId ,    relatedEntityType )  ;", "writeBatch . put ( key ,    LeveldbTimelineStore . EMPTY _ BYTES )  ;", "byte [  ]    relatedEntityStartTime    =    getStartTime ( relatedEntityId ,    relatedEntityType )  ;", "if    ( relatedEntityStartTime    =  =    null )     {", "relatedEntitiesWithoutStartTimes . add ( new   EntityIdentifier ( relatedEntityId ,    relatedEntityType )  )  ;", "continue ;", "}", "key    =    LeveldbTimelineStore . createRelatedEntityKey ( relatedEntityId ,    relatedEntityType ,    relatedEntityStartTime ,    entity . getEntityId (  )  ,    entity . getEntityType (  )  )  ;", "writeBatch . put ( key ,    LeveldbTimelineStore . EMPTY _ BYTES )  ;", "}", "}", "}", "if    (  ( primaryFilters    !  =    null )     &  &     (  !  ( primaryFilters . isEmpty (  )  )  )  )     {", "for    ( Map . Entry < String ,    Set < Object >  >    primaryFilter    :    primaryFilters . entrySet (  )  )     {", "for    ( Object   primaryFilterValue    :    primaryFilter . getValue (  )  )     {", "byte [  ]    key    =    LeveldbTimelineStore . createPrimaryFilterKey ( entity . getEntityId (  )  ,    entity . getEntityType (  )  ,    revStartTime ,    primaryFilter . getKey (  )  ,    primaryFilterValue )  ;", "writeBatch . put ( key ,    LeveldbTimelineStore . EMPTY _ BYTES )  ;", "LeveldbTimelineStore . writePrimaryFilterEntries ( writeBatch ,    primaryFilters ,    key ,    LeveldbTimelineStore . EMPTY _ BYTES )  ;", "}", "}", "}", "Map < String ,    Object >    otherInfo    =    entity . getOtherInfo (  )  ;", "if    (  ( otherInfo    !  =    null )     &  &     (  !  ( otherInfo . isEmpty (  )  )  )  )     {", "for    ( Map . Entry < String ,    Object >    i    :    otherInfo . entrySet (  )  )     {", "byte [  ]    key    =    LeveldbTimelineStore . createOtherInfoKey ( entity . getEntityId (  )  ,    entity . getEntityType (  )  ,    revStartTime ,    i . getKey (  )  )  ;", "byte [  ]    value    =    GenericObjectMapper . write ( i . getValue (  )  )  ;", "writeBatch . put ( key ,    value )  ;", "LeveldbTimelineStore . writePrimaryFilterEntries ( writeBatch ,    primaryFilters ,    key ,    value )  ;", "}", "}", "db . write ( writeBatch )  ;", "}    catch    ( IOException   e )     {", "LeveldbTimelineStore . LOG . error (  (  (  (  \" Error   putting   entity    \"     +     ( entity . getEntityId (  )  )  )     +     \"    of   type    \"  )     +     ( entity . getEntityType (  )  )  )  ,    e )  ;", "TimelinePutError   error    =    new   TimelinePutError (  )  ;", "error . setEntityId ( entity . getEntityId (  )  )  ;", "error . setEntityType ( entity . getEntityType (  )  )  ;", "error . setErrorCode ( IO _ EXCEPTION )  ;", "response . addError ( error )  ;", "}    finally    {", "lock . unlock (  )  ;", "writeLocks . returnLock ( lock )  ;", "IOUtils . cleanup ( LeveldbTimelineStore . LOG ,    writeBatch )  ;", "}", "for    ( EntityIdentifier   relatedEntity    :    relatedEntitiesWithoutStartTimes )     {", "lock    =    writeLocks . getLock ( relatedEntity )  ;", "lock . lock (  )  ;", "try    {", "LeveldbTimelineStore . StartAndInsertTime   relatedEntityStartAndInsertTime    =    getAndSetStartTime ( relatedEntity . getId (  )  ,    relatedEntity . getType (  )  ,    GenericObjectMapper . readReverseOrderedLong ( revStartTime ,     0  )  ,    null )  ;", "if    ( relatedEntityStartAndInsertTime    =  =    null )     {", "throw   new   IOException (  \" Error   setting   start   time   for   related   entity \"  )  ;", "}", "byte [  ]    relatedEntityStartTime    =    GenericObjectMapper . writeReverseOrderedLong ( relatedEntityStartAndInsertTime . startTime )  ;", "db . put ( LeveldbTimelineStore . createRelatedEntityKey ( relatedEntity . getId (  )  ,    relatedEntity . getType (  )  ,    relatedEntityStartTime ,    entity . getEntityId (  )  ,    entity . getEntityType (  )  )  ,    LeveldbTimelineStore . EMPTY _ BYTES )  ;", "db . put ( LeveldbTimelineStore . createEntityMarkerKey ( relatedEntity . getId (  )  ,    relatedEntity . getType (  )  ,    relatedEntityStartTime )  ,    GenericObjectMapper . writeReverseOrderedLong ( relatedEntityStartAndInsertTime . insertTime )  )  ;", "}    catch    ( IOException   e )     {", "LeveldbTimelineStore . LOG . error (  (  (  (  (  (  (  (  \" Error   putting   related   entity    \"     +     ( relatedEntity . getId (  )  )  )     +     \"    of   type    \"  )     +     ( relatedEntity . getType (  )  )  )     +     \"    for   entity    \"  )     +     ( entity . getEntityId (  )  )  )     +     \"    of   type    \"  )     +     ( entity . getEntityType (  )  )  )  ,    e )  ;", "TimelinePutError   error    =    new   TimelinePutError (  )  ;", "error . setEntityId ( entity . getEntityId (  )  )  ;", "error . setEntityType ( entity . getEntityType (  )  )  ;", "error . setErrorCode ( IO _ EXCEPTION )  ;", "response . addError ( error )  ;", "}    finally    {", "lock . unlock (  )  ;", "writeLocks . returnLock ( lock )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["put"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "dbStoreVersion ( state )  ;", "}", "METHOD_END"], "methodName": ["storeVersion"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( primaryFilters    !  =    null )     &  &     (  !  ( primaryFilters . isEmpty (  )  )  )  )     {", "for    ( Map . Entry < String ,    Set < Object >  >    pf    :    primaryFilters . entrySet (  )  )     {", "for    ( Object   pfval    :    pf . getValue (  )  )     {", "writeBatch . put (  . addPrimaryFilterToKey ( pf . getKey (  )  ,    pfval ,    key )  ,    value )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["writePrimaryFilterEntries"], "fileName": "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "TimelineEntity   entityToReturn    =    new   TimelineEntity (  )  ;", "entityToReturn . setEntityId ( entity . getEntityId (  )  )  ;", "entityToReturn . setEntityType ( entity . getEntityType (  )  )  ;", "entityToReturn . setStartTime ( entity . getStartTime (  )  )  ;", "if    ( fields . contains ( TimelineReader . Field . EVENTS )  )     {", "entityToReturn . addEvents ( entity . getEvents (  )  )  ;", "} else", "if    ( fields . contains ( TimelineReader . Field . LAST _ EVENT _ ONLY )  )     {", "entityToReturn . addEvent ( entity . getEvents (  )  . get (  0  )  )  ;", "} else    {", "entityToReturn . setEvents ( null )  ;", "}", "if    ( fields . contains ( TimelineReader . Field . RELATED _ ENTITIES )  )     {", "entityToReturn . addRelatedEntities ( entity . getRelatedEntities (  )  )  ;", "} else    {", "entityToReturn . setRelatedEntities ( null )  ;", "}", "if    ( fields . contains ( TimelineReader . Field . PRIMARY _ FILTERS )  )     {", "entityToReturn . addPrimaryFilters ( entity . getPrimaryFilters (  )  )  ;", "} else    {", "entityToReturn . setPrimaryFilters ( null )  ;", "}", "if    ( fields . contains ( TimelineReader . Field . OTHER _ INFO )  )     {", "entityToReturn . addOtherInfo ( entity . getOtherInfo (  )  )  ;", "} else    {", "entityToReturn . setOtherInfo ( null )  ;", "}", "return   entityToReturn ;", "}", "METHOD_END"], "methodName": ["maskFields"], "fileName": "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "Object   value    =    tags . get ( filter . getName (  )  )  ;", "if    ( value    =  =    null )     {", "return   false ;", "} else", "if    (  !  ( value . equals ( filter . getValue (  )  )  )  )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["matchFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "Set < Object >    value    =    tags . get ( filter . getName (  )  )  ;", "if    ( value    =  =    null )     {", "return   false ;", "} else    {", "return   value . contains ( filter . getValue (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["matchPrimaryFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "if    ( o   instanceof   Long )     {", "Long   l    =     (  ( Long )     ( o )  )  ;", "if    (  ( l    >  =     ( IntegMIN _ VALUE )  )     &  &     ( l    <  =     ( IntegMAX _ VALUE )  )  )     {", "return   l . intValue (  )  ;", "}", "}", "return   o ;", "}", "METHOD_END"], "methodName": ["maybeConvert"], "fileName": "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.yarn.server.timeline.NameValuePair"}, {"methodBody": ["METHOD_START", "{", "return   value ;", "}", "METHOD_END"], "methodName": ["getValue"], "fileName": "org.apache.hadoop.yarn.server.timeline.NameValuePair"}, {"methodBody": ["METHOD_START", "{", "TestGenericObjectMapper . testEncoding ( Long . MAX _ VALUE )  ;", "TestGenericObjectMapper . testEncoding ( Long . MIN _ VALUE )  ;", "TestGenericObjectMapper . testEncoding (  0 L )  ;", "TestGenericObjectMapper . testEncoding (  1  2  8 L )  ;", "TestGenericObjectMapper . testEncoding (  2  5  6 L )  ;", "TestGenericObjectMapper . testEncoding (  5  1  2 L )  ;", "TestGenericObjectMapper . testEncoding (  (  -  2  5  6 L )  )  ;", "}", "METHOD_END"], "methodName": ["testEncoding"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestGenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    b    =    GenericObjectMapper . writeReverseOrderedLong ( l )  ;", "assertEquals (  \" error   decoding \"  ,    l ,    GenericObjectMapper . readReverseOrderedLong ( b ,     0  )  )  ;", "byte [  ]    buf    =    new   byte [  1  6  ]  ;", "System . arraycopy ( b ,     0  ,    buf ,     5  ,     8  )  ;", "assertEquals (  \" error   decoding   at   offset \"  ,    l ,    GenericObjectMapper . readReverseOrderedLong ( buf ,     5  )  )  ;", "if    ( l    >     ( Long . MIN _ VALUE )  )     {", "byte [  ]    a    =    GenericObjectMapper . writeReverseOrderedLong (  ( l    -     1  )  )  ;", "assertEquals (  \" error   preserving   ordering \"  ,     1  ,    WritableComparator . compareBytes ( a ,     0  ,    a . length ,    b ,     0  ,    b . length )  )  ;", "}", "if    ( l    <     ( Long . MAX _ VALUE )  )     {", "byte [  ]    c    =    GenericObjectMapper . writeReverseOrderedLong (  ( l    +     1  )  )  ;", "assertEquals (  \" error   preserving   ordering \"  ,     1  ,    WritableComparator . compareBytes ( b ,     0  ,    b . length ,    c ,     0  ,    c . length )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testEncoding"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestGenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "TestGenericObjectMapper . verify ( Integer . MAX _ VALUE )  ;", "TestGenericObjectMapper . verify ( Integer . MIN _ VALUE )  ;", "assertEquals ( Integer . MAX _ VALUE ,    GenericObjectMapper . read ( GenericObjectMapper . write (  (  ( long )     ( Integer . MAX _ VALUE )  )  )  )  )  ;", "assertEquals ( Integer . MIN _ VALUE ,    GenericObjectMapper . read ( GenericObjectMapper . write (  (  ( long )     ( Integer . MIN _ VALUE )  )  )  )  )  ;", "TestGenericObjectMapper . verify (  (  (  ( long )     ( Integer . MAX _ VALUE )  )     +     1 L )  )  ;", "TestGenericObjectMapper . verify (  (  (  ( long )     ( Integer . MIN _ VALUE )  )     -     1 L )  )  ;", "TestGenericObjectMapper . verify ( Long . MAX _ VALUE )  ;", "TestGenericObjectMapper . verify ( Long . MIN _ VALUE )  ;", "assertEquals (  4  2  ,    GenericObjectMapper . read ( GenericObjectMapper . write (  4  2 L )  )  )  ;", "TestGenericObjectMapper . verify (  4  2  )  ;", "TestGenericObjectMapper . verify (  1  .  2  3  )  ;", "TestGenericObjectMapper . verify (  \" abc \"  )  ;", "TestGenericObjectMapper . verify ( true )  ;", "List < String >    list    =    new   ArrayList < String >  (  )  ;", "list . add (  \"  1  2  3  \"  )  ;", "list . add (  \" abc \"  )  ;", "TestGenericObjectMapper . verify ( list )  ;", "Map < String ,    String >    map    =    new   HashMap < String ,    String >  (  )  ;", "map . put (  \" k 1  \"  ,     \" v 1  \"  )  ;", "map . put (  \" k 2  \"  ,     \" v 2  \"  )  ;", "TestGenericObjectMapper . verify ( map )  ;", "}", "METHOD_END"], "methodName": ["testValueTypes"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestGenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( o ,    GenericObjectMapper . read ( GenericObjectMapper . write ( o )  )  )  ;", "}", "METHOD_END"], "methodName": ["verify"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestGenericObjectMapper"}, {"methodBody": ["METHOD_START", "{", "DBIterator   iterator    =    null ;", "DBIterator   pfIterator    =    null ;", "try    {", "iterator    =     (  (  )     ( store )  )  . getDbIterator ( false )  ;", "pfIterator    =     (  (  )     ( store )  )  . getDbIterator ( false )  ;", "return    (  (  )     ( store )  )  . deleteNextEntity ( entityType ,    ts ,    iterator ,    pfIterator ,    false )  ;", "}    finally    {", "IOUtils . cleanup ( null ,    iterator ,    pfIterator )  ;", "}", "}", "METHOD_END"], "methodName": ["deleteNextEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( store )     !  =    null )     {", "store . close (  )  ;", "}", "store    =    new    (  )  ;", "store . init ( config )  ;", "store . start (  )  ;", "}", "METHOD_END"], "methodName": ["restartTimelineStore"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "fsContext    =    FileContext . getLocalFSFileContext (  )  ;", "fsPath    =    new   File (  \" target \"  ,     (  ( this . getClass (  )  . getSimpleName (  )  )     +     \"  - tmpDir \"  )  )  . getAbsoluteFile (  )  ;", "fsContext . delete ( new   Path ( fsPath . getAbsolutePath (  )  )  ,    true )  ;", "config . set ( TIMELINE _ SERVICE _ LEVELDB _ PATH ,    fsPath . getAbsolutePath (  )  )  ;", "config . setBoolean ( TIMELINE _ SERVICE _ TTL _ ENABLE ,    false )  ;", "store    =    new    (  )  ;", "store . init ( config )  ;", "store . start (  )  ;", "loadTestData (  )  ;", "loadVerificationData (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "store . stop (  )  ;", "fsContext . delete ( new   Path ( fsPath . getAbsolutePath (  )  )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "assertEquals (  1  0  0  0  0  ,     . getStartTimeReadCacheSize ( conf )  )  ;", "assertEquals (  1  0  0  0  0  ,     . getStartTimeWriteCacheSize ( conf )  )  ;", "conf . setInt ( TIMELINE _ SERVICE _ LEVELDB _ START _ TIME _ READ _ CACHE _ SIZE ,     1  0  0  0  1  )  ;", "assertEquals (  1  0  0  0  1  ,     . getStartTimeReadCacheSize ( conf )  )  ;", "conf    =    new   Configuration (  )  ;", "conf . setInt ( TIMELINE _ SERVICE _ LEVELDB _ START _ TIME _ WRITE _ CACHE _ SIZE ,     1  0  0  0  2  )  ;", "assertEquals (  1  0  0  0  2  ,     . getStartTimeWriteCacheSize ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["testCacheSizes"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "LeveldbTimelineStore   dbStore    =     (  ( LeveldbTimelineStore )     ( store )  )  ;", "Version   defaultVersion    =    dbStore . getCurrentVersion (  )  ;", "Assert . assertEquals ( defaultVersion ,    dbStore . loadVersion (  )  )  ;", "Version   compatibleVersion    =    Version . newInstance ( defaultVersion . getMajorVersion (  )  ,     (  ( defaultVersion . getMinorVersion (  )  )     +     2  )  )  ;", "dbStore . storeVersion ( compatibleVersion )  ;", "Assert . assertEquals ( compatibleVersion ,    dbStore . loadVersion (  )  )  ;", "restartTimelineStore (  )  ;", "dbStore    =     (  ( LeveldbTimelineStore )     ( store )  )  ;", "Assert . assertEquals ( defaultVersion ,    dbStore . loadVersion (  )  )  ;", "Version   incompatibleVersion    =    Version . newInstance (  (  ( defaultVersion . getMajorVersion (  )  )     +     1  )  ,    defaultVersion . getMinorVersion (  )  )  ;", "dbStore . storeVersion ( incompatibleVersion )  ;", "try    {", "restartTimelineStore (  )  ;", "Assert . fail (  \" Incompatible   version ,    should   expect   fail   here .  \"  )  ;", "}    catch    ( ServiceStateException   e )     {", "Assert . assertTrue (  \" Exception   message   mismatch \"  ,    e . getMessage (  )  . contains (  \" Incompatible   version   for   timeline   store \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCheckVersion"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  2  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  1  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals ( false ,    deleteNextEntity ( entityType 1  ,    GenericObjectMapper . writeReverseOrderedLong (  1  2  2 L )  )  )  ;", "assertEquals (  2  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  1  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals ( true ,    deleteNextEntity ( entityType 1  ,    GenericObjectMapper . writeReverseOrderedLong (  1  2  3 L )  )  )  ;", "List < TimelineEntity >    entities    =    getEntities (  \" type _  2  \"  )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 2  ,    entityType 2  ,    events 2  ,    Collections . singletonMap ( entityType 1  ,    Collections . singleton ( entityId 1 b )  )  ,    TimelineStoreTestUtils . EMPTY _ PRIMARY _ FILTERS ,    TimelineStoreTestUtils . EMPTY _ MAP ,    entities . get (  0  )  )  ;", "entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "(  (  )     ( store )  )  . discardOldEntities (  (  -  1  2  3 L )  )  ;", "assertEquals (  1  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals (  3  ,     (  (  )     ( store )  )  . getEntityTypes (  )  . size (  )  )  ;", "(  (  )     ( store )  )  . discardOldEntities (  1  2  3 L )  ;", "assertEquals (  0  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals (  0  ,     (  (  )     ( store )  )  . getEntityTypes (  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    Set < Object >  >    primaryFilter    =    Collections . singletonMap (  \" user \"  ,    Collections . singleton (  (  ( Object )     (  \" otheruser \"  )  )  )  )  ;", "TimelineEntities   atsEntities    =    new   TimelineEntities (  )  ;", "atsEntities . setEntities ( Collections . singletonList ( TimelineStoreTestUtils . createEntity ( entityId 1 b ,    entityType 1  ,     7  8  9 L ,    Collections . singletonList ( ev 2  )  ,    null ,    primaryFilter ,    null )  )  )  ;", "TimelinePutResponse   response    =    store . put ( atsEntities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "NameValuePair   pfPair    =    new   NameValuePair (  \" user \"  ,     \" otheruser \"  )  ;", "List < TimelineEntity >    entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    pfPair )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    Collections . singletonList ( ev 2  )  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilter ,    TimelineStoreTestUtils . EMPTY _ MAP ,    entities . get (  0  )  )  ;", "entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "(  (  )     ( store )  )  . discardOldEntities (  (  -  1  2  3 L )  )  ;", "assertEquals (  1  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    pfPair )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  . size (  )  )  ;", "(  (  )     ( store )  )  . discardOldEntities (  1  2  3 L )  ;", "assertEquals (  0  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals (  0  ,     (  (  )     ( store )  )  . getEntityTypes (  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    pfPair )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDeleteEntitiesPrimaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "long   l    =    System . currentTimeMillis (  )  ;", "assertEquals (  2  ,    getEntitiesFromTs (  \" type _  1  \"  ,    l )  . size (  )  )  ;", "assertEquals (  1  ,    getEntitiesFromTs (  \" type _  2  \"  ,    l )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    l )  . size (  )  )  ;", "(  (  )     ( store )  )  . discardOldEntities (  1  2  3 L )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  1  \"  ,    l )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  2  \"  ,    l )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    l )  . size (  )  )  ;", "assertEquals (  0  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    l )  . size (  )  )  ;", "loadTestData (  )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  1  \"  ,    l )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  2  \"  ,    l )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    l )  . size (  )  )  ;", "assertEquals (  2  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  1  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFromTsWithDeletion"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntities (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithFromId (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithFromId"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithFromTs (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithFromTs"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithPrimaryFilters (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithPrimaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithSecondaryFilters (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithSecondaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "List < String >    entityTypes    =     (  ( LeveldbTimelineStore )     ( store )  )  . getEntityTypes (  )  ;", "assertEquals (  4  ,    entityTypes . size (  )  )  ;", "assertEquals ( entityType 1  ,    entityTypes . get (  0  )  )  ;", "assertEquals ( entityType 2  ,    entityTypes . get (  1  )  )  ;", "assertEquals ( entityType 4  ,    entityTypes . get (  2  )  )  ;", "assertEquals ( entityType 5  ,    entityTypes . get (  3  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntityTypes"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEvents (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEvents"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetSingleEntity (  )  ;", "(  (  )     ( store )  )  . clearStartTimeCache (  )  ;", "super . testGetSingleEntity (  )  ;", "loadTestData (  )  ;", "}", "METHOD_END"], "methodName": ["testGetSingleEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "FileSystem   fs    =    FileSystem . getLocal ( new   YarnConfiguration (  )  )  ;", "FileStatus   file    =    fs . getFileStatus ( new   Path ( fsPath . getAbsolutePath (  )  ,     . FILENAME )  )  ;", "assertNotNull ( file )  ;", "assertEquals (  . LEVELDB _ DIR _ UMASK ,    file . getPermission (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRootDirPermission"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestLeveldbTimelineStore"}, {"methodBody": ["METHOD_START", "{", "return   store ;", "}", "METHOD_END"], "methodName": ["getTimelineStore"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "store    =    new   MemoryTimelineStore (  )  ;", "store . init ( new   YarnConfiguration (  )  )  ;", "store . start (  )  ;", "loadTestData (  )  ;", "loadVerificationData (  )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "store . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntities (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithFromId (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithFromId"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithFromTs (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithFromTs"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithPrimaryFilters (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithPrimaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEntitiesWithSecondaryFilters (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithSecondaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetEvents (  )  ;", "}", "METHOD_END"], "methodName": ["testGetEvents"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "super . testGetSingleEntity (  )  ;", "}", "METHOD_END"], "methodName": ["testGetSingleEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.TestMemoryTimelineStore"}, {"methodBody": ["METHOD_START", "{", "if    (  ( timelineEntity . getPrimaryFilters (  )  )     !  =    null )     {", "timelineEntity . getPrimaryFilters (  )  . remove ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["cleanupOwnerInfo"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "boolean   modified    =    false ;", "if    (  ( fieldEnums    !  =    null )     &  &     (  !  ( fieldEnums . contains ( Reader . Field . PRIMARY _ FILTERS )  )  )  )     {", "fieldEnums . add ( Reader . Field . PRIMARY _ FILTERS )  ;", "modified    =    true ;", "}", "return   modified ;", "}", "METHOD_END"], "methodName": ["extendFields"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "TimelineEntities   entities    =    null ;", "boolean   modified    =     . extendFields ( fields )  ;", "entities    =    store . getEntities ( entityType ,    limit ,    windowStart ,    windowEnd ,    fromId ,    fromTs ,    primaryFilter ,    secondaryFilter ,    fields )  ;", "if    ( entities    !  =    null )     {", "Iterator < TimelineEntity >    entitiesItr    =    entities . getEntities (  )  . iterator (  )  ;", "while    ( entitiesItr . hasNext (  )  )     {", "TimelineEntity   entity    =    entitiesItr . next (  )  ;", "try    {", "if    (  !  ( timelineACLsManager . checkAccess ( callerUGI ,    entity )  )  )     {", "entitiesItr . remove (  )  ;", "} else    {", "if    ( modified )     {", "entity . setPrimaryFilters ( null )  ;", "} else    {", ". cleanupOwnerInfo ( entity )  ;", "}", "}", "}    catch    ( YarnException   e )     {", ". LOG . error (  (  (  (  \" Error   when   verifying   access   for   user    \"     +    callerUGI )     +     \"    on   the   events   of   the   timeline   entity    \"  )     +     ( new   EntityIdentifier ( entity . getEntityId (  )  ,    entity . getEntityType (  )  )  )  )  ,    e )  ;", "entitiesItr . remove (  )  ;", "}", "}", "}", "if    ( entities    =  =    null )     {", "return   new   TimelineEntities (  )  ;", "}", "return   entities ;", "}", "METHOD_END"], "methodName": ["getEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "TimelineEntity   entity    =    null ;", "boolean   modified    =     . extendFields ( fields )  ;", "entity    =    store . getEntity ( entityId ,    entityType ,    fields )  ;", "if    ( entity    !  =    null )     {", "if    (  !  ( timelineACLsManager . checkAccess ( callerUGI ,    entity )  )  )     {", "entity    =    null ;", "} else    {", "if    ( modified )     {", "entity . setPrimaryFilters ( null )  ;", "} else    {", ". cleanupOwnerInfo ( entity )  ;", "}", "}", "}", "return   entity ;", "}", "METHOD_END"], "methodName": ["getEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "TimelineEvents   events    =    null ;", "events    =    store . getEntityTimelines ( entityType ,    entityIds ,    limit ,    windowStart ,    windowEnd ,    eventTypes )  ;", "if    ( events    !  =    null )     {", "Iterator < TimelineEvents . EventsOfOneEntity >    eventsItr    =    events . getAllEvents (  )  . iterator (  )  ;", "while    ( eventsItr . hasNext (  )  )     {", "TimelineEvents . EventsOfOneEntity   eventsOfOneEntity    =    eventsItr . next (  )  ;", "try    {", "TimelineEntity   entity    =    store . getEntity ( eventsOfOneEntity . getEntityId (  )  ,    eventsOfOneEntity . getEntityType (  )  ,    EnumSet . of ( TimelineReader . Field . PRIMARY _ FILTERS )  )  ;", "if    (  !  ( timelineACLsManager . checkAccess ( callerUGI ,    entity )  )  )     {", "eventsItr . remove (  )  ;", "}", "}    catch    ( Exception   e )     {", ". LOG . error (  (  (  (  \" Error   when   verifying   access   for   user    \"     +    callerUGI )     +     \"    on   the   events   of   the   timeline   entity    \"  )     +     ( new   EntityIdentifier ( eventsOfOneEntity . getEntityId (  )  ,    eventsOfOneEntity . getEntityType (  )  )  )  )  ,    e )  ;", "eventsItr . remove (  )  ;", "}", "}", "}", "if    ( events    =  =    null )     {", "return   new   TimelineEvents (  )  ;", "}", "return   events ;", "}", "METHOD_END"], "methodName": ["getEvents"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( timelineEntity . getPrimaryFilters (  )  )     !  =    null )     &  &     ( timelineEntity . getPrimaryFilters (  )  . containsKey ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  )  )  )     {", "throw   new   YarnException (  (  \" User   should   not   use   the   timeline   system   filter   key :     \"     +     ( TimelineStore . SystemFilter . ENTITY _ OWNER )  )  )  ;", "}", "timelineEntity . addPrimaryFilter ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  ,    owner )  ;", "}", "METHOD_END"], "methodName": ["injectOwnerInfo"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "if    ( entities    =  =    null )     {", "return   new   TimelinePutResponse (  )  ;", "}", "List < EntityIdentifier >    entityIDs    =    new   ArrayList < EntityIdentifier >  (  )  ;", "TimelineEntities   entitiesToPut    =    new   TimelineEntities (  )  ;", "List < TimelinePutResponse . TimelinePutError >    errors    =    new   ArrayList < TimelinePutResponse . TimelinePutError >  (  )  ;", "for    ( TimelineEntity   entity    :    entities . getEntities (  )  )     {", "EntityIdentifier   entityID    =    new   EntityIdentifier ( entity . getEntityId (  )  ,    entity . getEntityType (  )  )  ;", "TimelineEntity   existingEntity    =    null ;", "try    {", "existingEntity    =    store . getEntity ( entityID . getId (  )  ,    entityID . getType (  )  ,    EnumSet . of ( TimelineReader . Field . PRIMARY _ FILTERS )  )  ;", "if    (  ( existingEntity    !  =    null )     &  &     (  !  ( timelineACLsManager . checkAccess ( callerUGI ,    existingEntity )  )  )  )     {", "throw   new   YarnException (  (  (  (  (  \" The   timeline   entity    \"     +    entityID )     +     \"    was   not   put   by    \"  )     +    callerUGI )     +     \"    before \"  )  )  ;", "}", "}    catch    ( Exception   e )     {", ". LOG . error (  (  (  (  \" Skip   the   timeline   entity :     \"     +    entityID )     +     \"  ,    because    \"  )     +     ( e . getMessage (  )  )  )  )  ;", "TimelinePutResponse . TimelinePutError   error    =    new   TimelinePutResponse . TimelinePutError (  )  ;", "error . setEntityId ( entityID . getId (  )  )  ;", "error . setEntityType ( entityID . getType (  )  )  ;", "error . setErrorCode ( ACCESS _ DENIED )  ;", "errors . add ( error )  ;", "continue ;", "}", "try    {", "if    ( existingEntity    =  =    null )     {", ". injectOwnerInfo ( entity ,    callerUGI . getShortUserName (  )  )  ;", "}", "}    catch    ( YarnException   e )     {", ". LOG . error (  (  (  (  \" Skip   the   timeline   entity :     \"     +    entityID )     +     \"  ,    because    \"  )     +     ( e . getMessage (  )  )  )  )  ;", "TimelinePutResponse . TimelinePutError   error    =    new   TimelinePutResponse . TimelinePutError (  )  ;", "error . setEntityId ( entityID . getId (  )  )  ;", "error . setEntityType ( entityID . getType (  )  )  ;", "error . setErrorCode ( SYSTEM _ FILTER _ CONFLICT )  ;", "errors . add ( error )  ;", "continue ;", "}", "entityIDs . add ( entityID )  ;", "entitiesToPut . addEntity ( entity )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Storing   the   entity    \"     +    entityID )     +     \"  ,    JSON - style   content :     \"  )     +     ( TimelineUtils . dumpTimelineRecordtoJSON ( entity )  )  )  )  ;", "}", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Storing   entities :     \"     +     ( CSV _ JOINER . join ( entityIDs )  )  )  )  ;", "}", "TimelinePutResponse   response    =    store . put ( entitiesToPut )  ;", "response . addErrors ( errors )  ;", "return   response ;", "}", "METHOD_END"], "methodName": ["postEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineDataManager"}, {"methodBody": ["METHOD_START", "{", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId ( entityId )  ;", "entity . setEntityType ( entityType )  ;", "entity . setStartTime ( startTime )  ;", "entity . setEvents ( events )  ;", "if    ( relatedEntities    !  =    null )     {", "for    ( Map . Entry < String ,    Set < String >  >    e    :    relatedEntities . entrySet (  )  )     {", "for    ( String   v    :    e . getValue (  )  )     {", "entity . addRelatedEntity ( e . getKey (  )  ,    v )  ;", "}", "}", "} else    {", "entity . setRelatedEntities ( null )  ;", "}", "entity . setPrimaryFilters ( primaryFilters )  ;", "entity . setOtherInfo ( otherInfo )  ;", "return   entity ;", "}", "METHOD_END"], "methodName": ["createEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setTimestamp ( timestamp )  ;", "event . setEventType ( type )  ;", "event . setEventInfo ( info )  ;", "return   event ;", "}", "METHOD_END"], "methodName": ["createEvent"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    null ,    null ,    null ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    limit ,    windowStart ,    windowEnd ,    null ,    null ,    primaryFilter ,    null ,    fields )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    fromId ,    null ,    null ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesFromId"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    fromId ,    null ,    primaryFilter ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesFromIdWithPrimaryFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    windowEnd ,    fromId ,    null ,    primaryFilter ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesFromIdWithPrimaryFilterAndWindow"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    windowEnd ,    fromId ,    null ,    null ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesFromIdWithWindow"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    null ,    fromTs ,    null ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesFromTs"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    null ,    fromTs ,    primaryFilter ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesFromTsWithPrimaryFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    null ,    null ,    primaryFilter ,    secondaryFilters ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesWithFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "return   store . getEntities ( entityType ,    null ,    null ,    null ,    null ,    null ,    primaryFilter ,    null ,    null )  . getEntities (  )  ;", "}", "METHOD_END"], "methodName": ["getEntitiesWithPrimaryFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "beforeTs    =     ( System . currentTimeMillis (  )  )     -     1  ;", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "Map < String ,    Set < Object >  >    primaryFilters    =    new   HashMap < String ,    Set < Object >  >  (  )  ;", "Set < Object >    l 1     =    new   HashSet < Object >  (  )  ;", "l 1  . add (  \" username \"  )  ;", "Set < Object >    l 2     =    new   HashSet < Object >  (  )  ;", "l 2  . add (  (  ( long )     ( Integer . MAX _ VALUE )  )  )  ;", "Set < Object >    l 3     =    new   HashSet < Object >  (  )  ;", "l 3  . add (  \"  1  2  3 abc \"  )  ;", "Set < Object >    l 4     =    new   HashSet < Object >  (  )  ;", "l 4  . add (  (  (  ( long )     ( Integer . MAX _ VALUE )  )     +     1 L )  )  ;", "primaryFilters . put (  \" user \"  ,    l 1  )  ;", "primaryFilters . put (  \" appname \"  ,    l 2  )  ;", "primaryFilters . put (  \" other \"  ,    l 3  )  ;", "primaryFilters . put (  \" long \"  ,    l 4  )  ;", "Map < String ,    Object >    secondaryFilters    =    new   HashMap < String ,    Object >  (  )  ;", "secondaryFilters . put (  \" startTime \"  ,     1  2  3  4  5  6 L )  ;", "secondaryFilters . put (  \" status \"  ,     \" RUNNING \"  )  ;", "Map < String ,    Object >    otherInfo 1     =    new   HashMap < String ,    Object >  (  )  ;", "otherInfo 1  . put (  \" info 1  \"  ,     \" val 1  \"  )  ;", "otherInfo 1  . putAll ( secondaryFilters )  ;", "String   entityId 1     =     \" id _  1  \"  ;", "String   entityType 1     =     \" type _  1  \"  ;", "String   entityId 1 b    =     \" id _  2  \"  ;", "String   entityId 2     =     \" id _  2  \"  ;", "String   entityType 2     =     \" type _  2  \"  ;", "String   entityId 4     =     \" id _  4  \"  ;", "String   entityType 4     =     \" type _  4  \"  ;", "String   entityId 5     =     \" id _  5  \"  ;", "String   entityType 5     =     \" type _  5  \"  ;", "Map < String ,    Set < String >  >    relatedEntities    =    new   HashMap < String ,    Set < String >  >  (  )  ;", "relatedEntities . put ( entityType 2  ,    Collections . singleton ( entityId 2  )  )  ;", "TimelineEvent   ev 3     =     . createEvent (  7  8  9 L ,     \" launch _ event \"  ,    null )  ;", "TimelineEvent   ev 4     =     . createEvent (  (  -  1  2  3 L )  ,     \" init _ event \"  ,    null )  ;", "List < TimelineEvent >    events    =    new   ArrayList < TimelineEvent >  (  )  ;", "events . add ( ev 3  )  ;", "events . add ( ev 4  )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity ( entityId 2  ,    entityType 2  ,    null ,    events ,    null ,    null ,    null )  )  )  ;", "TimelinePutResponse   response    =    store . put ( entities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "TimelineEvent   ev 1     =     . createEvent (  1  2  3 L ,     \" start _ event \"  ,    null )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity ( entityId 1  ,    entityType 1  ,     1  2  3 L ,    Collections . singletonList ( ev 1  )  ,    relatedEntities ,    primaryFilters ,    otherInfo 1  )  )  )  ;", "response    =    store . put ( entities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity ( entityId 1 b ,    entityType 1  ,    null ,    Collections . singletonList ( ev 1  )  ,    relatedEntities ,    primaryFilters ,    otherInfo 1  )  )  )  ;", "response    =    store . put ( entities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "Map < String ,    Object >    eventInfo    =    new   HashMap < String ,    Object >  (  )  ;", "eventInfo . put (  \" event   info    1  \"  ,     \" val 1  \"  )  ;", "TimelineEvent   ev 2     =     . createEvent (  4  5  6 L ,     \" end _ event \"  ,    eventInfo )  ;", "Map < String ,    Object >    otherInfo 2     =    new   HashMap < String ,    Object >  (  )  ;", "otherInfo 2  . put (  \" info 2  \"  ,     \" val 2  \"  )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity ( entityId 1  ,    entityType 1  ,    null ,    Collections . singletonList ( ev 2  )  ,    null ,    primaryFilters ,    otherInfo 2  )  )  )  ;", "response    =    store . put ( entities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity ( entityId 1 b ,    entityType 1  ,     7  8  9 L ,    Collections . singletonList ( ev 2  )  ,    null ,    primaryFilters ,    otherInfo 2  )  )  )  ;", "response    =    store . put ( entities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity (  \" badentityid \"  ,     \" badentity \"  ,    null ,    null ,    null ,    null ,    otherInfo 1  )  )  )  ;", "response    =    store . put ( entities )  ;", "assertEquals (  1  ,    response . getErrors (  )  . size (  )  )  ;", "TimelinePutError   error    =    response . getErrors (  )  . get (  0  )  ;", "assertEquals (  \" badentityid \"  ,    error . getEntityId (  )  )  ;", "assertEquals (  \" badentity \"  ,    error . getEntityType (  )  )  ;", "assertEquals ( NO _ START _ TIME ,    error . getErrorCode (  )  )  ;", "relatedEntities . clear (  )  ;", "relatedEntities . put ( entityType 5  ,    Collections . singleton ( entityId 5  )  )  ;", "entities . setEntities ( Collections . singletonList (  . createEntity ( entityId 4  ,    entityType 4  ,     4  2 L ,    null ,    relatedEntities ,    null ,    null )  )  )  ;", "response    =    store . put ( entities )  ;", "assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["loadTestData"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "userFilter    =    new   NameValuePair (  \" user \"  ,     \" username \"  )  ;", "numericFilter 1     =    new   NameValuePair (  \" appname \"  ,    Integer . MAX _ VALUE )  ;", "numericFilter 2     =    new   NameValuePair (  \" long \"  ,     (  (  ( long )     ( Integer . MAX _ VALUE )  )     +     1 L )  )  ;", "numericFilter 3     =    new   NameValuePair (  \" other \"  ,     \"  1  2  3 abc \"  )  ;", "goodTestingFilters    =    new   ArrayList < NameValuePair >  (  )  ;", "goodTestingFilters . add ( new   NameValuePair (  \" appname \"  ,    Integer . MAX _ VALUE )  )  ;", "goodTestingFilters . add ( new   NameValuePair (  \" status \"  ,     \" RUNNING \"  )  )  ;", "badTestingFilters    =    new   ArrayList < NameValuePair >  (  )  ;", "badTestingFilters . add ( new   NameValuePair (  \" appname \"  ,    Integer . MAX _ VALUE )  )  ;", "badTestingFilters . add ( new   NameValuePair (  \" status \"  ,     \" FINISHED \"  )  )  ;", "primaryFilters    =    new   HashMap < String ,    Set < Object >  >  (  )  ;", "Set < Object >    l 1     =    new   HashSet < Object >  (  )  ;", "l 1  . add (  \" username \"  )  ;", "Set < Object >    l 2     =    new   HashSet < Object >  (  )  ;", "l 2  . add ( Integer . MAX _ VALUE )  ;", "Set < Object >    l 3     =    new   HashSet < Object >  (  )  ;", "l 3  . add (  \"  1  2  3 abc \"  )  ;", "Set < Object >    l 4     =    new   HashSet < Object >  (  )  ;", "l 4  . add (  (  (  ( long )     ( Integer . MAX _ VALUE )  )     +     1 L )  )  ;", "primaryFilters . put (  \" user \"  ,    l 1  )  ;", "primaryFilters . put (  \" appname \"  ,    l 2  )  ;", "primaryFilters . put (  \" other \"  ,    l 3  )  ;", "primaryFilters . put (  \" long \"  ,    l 4  )  ;", "secondaryFilters    =    new   HashMap < String ,    Object >  (  )  ;", "secondaryFilters . put (  \" startTime \"  ,     1  2  3  4  5  6  )  ;", "secondaryFilters . put (  \" status \"  ,     \" RUNNING \"  )  ;", "allFilters    =    new   HashMap < String ,    Object >  (  )  ;", "allFilters . putAll ( secondaryFilters )  ;", "for    ( Map . Entry < String ,    Set < Object >  >    pf    :    primaryFilters . entrySet (  )  )     {", "for    ( Object   o    :    pf . getValue (  )  )     {", "allFilters . put ( pf . getKey (  )  ,    o )  ;", "}", "}", "otherInfo    =    new   HashMap < String ,    Object >  (  )  ;", "otherInfo . put (  \" info 1  \"  ,     \" val 1  \"  )  ;", "otherInfo . put (  \" info 2  \"  ,     \" val 2  \"  )  ;", "otherInfo . putAll ( secondaryFilters )  ;", "entityId 1     =     \" id _  1  \"  ;", "entityType 1     =     \" type _  1  \"  ;", "entityId 1 b    =     \" id _  2  \"  ;", "entityId 2     =     \" id _  2  \"  ;", "entityType 2     =     \" type _  2  \"  ;", "entityId 4     =     \" id _  4  \"  ;", "entityType 4     =     \" type _  4  \"  ;", "entityId 5     =     \" id _  5  \"  ;", "entityType 5     =     \" type _  5  \"  ;", "ev 1     =    TimelineStoreTestUtils . createEvent (  1  2  3 L ,     \" start _ event \"  ,    null )  ;", "eventInfo    =    new   HashMap < String ,    Object >  (  )  ;", "eventInfo . put (  \" event   info    1  \"  ,     \" val 1  \"  )  ;", "ev 2     =    TimelineStoreTestUtils . createEvent (  4  5  6 L ,     \" end _ event \"  ,    eventInfo )  ;", "events 1     =    new   ArrayList < api . records . timeline . TimelineEvent >  (  )  ;", "events 1  . add ( ev 2  )  ;", "events 1  . add ( ev 1  )  ;", "relEntityMap    =    new   HashMap < String ,    Set < String >  >  (  )  ;", "Set < String >    ids    =    new   HashSet < String >  (  )  ;", "ids . add ( entityId 1  )  ;", "ids . add ( entityId 1 b )  ;", "relEntityMap . put ( entityType 1  ,    ids )  ;", "relEntityMap 2     =    new   HashMap < String ,    Set < String >  >  (  )  ;", "relEntityMap 2  . put ( entityType 4  ,    Collections . singleton ( entityId 4  )  )  ;", "ev 3     =    TimelineStoreTestUtils . createEvent (  7  8  9 L ,     \" launch _ event \"  ,    null )  ;", "ev 4     =    TimelineStoreTestUtils . createEvent (  (  -  1  2  3 L )  ,     \" init _ event \"  ,    null )  ;", "events 2     =    new   ArrayList < api . records . timeline . TimelineEvent >  (  )  ;", "events 2  . add ( ev 3  )  ;", "events 2  . add ( ev 4  )  ;", "}", "METHOD_END"], "methodName": ["loadVerificationData"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" nonzero   entities   size   for   nonexistent   type \"  ,     0  ,    getEntities (  \" type _  0  \"  )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   nonexistent   type \"  ,     0  ,    getEntities (  \" type _  3  \"  )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   nonexistent   type \"  ,     0  ,    getEntities (  \" type _  6  \"  )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   nonexistent   type \"  ,     0  ,    getEntitiesWithPrimaryFilter (  \" type _  0  \"  ,    userFilter )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   nonexistent   type \"  ,     0  ,    getEntitiesWithPrimaryFilter (  \" type _  3  \"  ,    userFilter )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   nonexistent   type \"  ,     0  ,    getEntitiesWithPrimaryFilter (  \" type _  6  \"  ,    userFilter )  . size (  )  )  ;", "List < TimelineEntity >    entities    =    getEntities (  \" type _  1  \"  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntities (  \" type _  2  \"  )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 2  ,    entityType 2  ,    events 2  ,    relEntityMap ,     . EMPTY _ PRIMARY _ FILTERS ,     . EMPTY _ MAP ,    entities . get (  0  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,     1 L ,    null ,    null ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,     1 L ,     0 L ,    null ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,     2  3  4 L ,    null ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,     1  2  3 L ,    null ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,     2  3  4 L ,     3  4  5 L ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,    null ,     3  4  5 L ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,    null ,     1  2  3 L ,    null ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "List < TimelineEntity >    entities    =    getEntitiesFromId (  \" type _  1  \"  ,    entityId 1  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesFromId (  \" type _  1  \"  ,    entityId 1 b )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "entities    =    getEntitiesFromIdWithWindow (  \" type _  1  \"  ,     0 L ,    entityId 1  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromId (  \" type _  2  \"  ,     \" a \"  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromId (  \" type _  2  \"  ,    entityId 2  )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 2  ,    entityType 2  ,    events 2  ,    relEntityMap ,     . EMPTY _ PRIMARY _ FILTERS ,     . EMPTY _ MAP ,    entities . get (  0  )  )  ;", "entities    =    getEntitiesFromIdWithWindow (  \" type _  2  \"  ,     (  -  4  5  6 L )  ,    null )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromIdWithWindow (  \" type _  2  \"  ,     (  -  4  5  6 L )  ,     \" a \"  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromIdWithWindow (  \" type _  2  \"  ,     0 L ,    null )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromIdWithWindow (  \" type _  2  \"  ,     0 L ,    entityId 2  )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromIdWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    entityId 1  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesFromIdWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    entityId 1 b )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "entities    =    getEntitiesFromIdWithPrimaryFilterAndWindow (  \" type _  1  \"  ,     0 L ,    entityId 1  ,    userFilter )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesFromIdWithPrimaryFilter (  \" type _  2  \"  ,    userFilter ,     \" a \"  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithFromId"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  1  \"  ,    beforeTs )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  2  \"  ,    beforeTs )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    beforeTs )  . size (  )  )  ;", "long   afterTs    =    System . currentTimeMillis (  )  ;", "assertEquals (  2  ,    getEntitiesFromTs (  \" type _  1  \"  ,    afterTs )  . size (  )  )  ;", "assertEquals (  1  ,    getEntitiesFromTs (  \" type _  2  \"  ,    afterTs )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    afterTs )  . size (  )  )  ;", "assertEquals (  2  ,    getEntities (  \" type _  1  \"  )  . size (  )  )  ;", "assertEquals (  1  ,    getEntities (  \" type _  2  \"  )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  . size (  )  )  ;", "long   beforeTs    =    this . beforeTs ;", "loadTestData (  )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  1  \"  ,    beforeTs )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTs (  \" type _  2  \"  ,    beforeTs )  . size (  )  )  ;", "assertEquals (  0  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    beforeTs )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesFromTs (  \" type _  1  \"  ,    afterTs )  . size (  )  )  ;", "assertEquals (  1  ,    getEntitiesFromTs (  \" type _  2  \"  ,    afterTs )  . size (  )  )  ;", "assertEquals (  2  ,    getEntitiesFromTsWithPrimaryFilter (  \" type _  1  \"  ,    userFilter ,    afterTs )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithFromTs"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \" nonzero   entities   size   for   primary   filter \"  ,     0  ,    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    new   NameValuePair (  \" none \"  ,     \" none \"  )  )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   primary   filter \"  ,     0  ,    getEntitiesWithPrimaryFilter (  \" type _  2  \"  ,    new   NameValuePair (  \" none \"  ,     \" none \"  )  )  . size (  )  )  ;", "assertEquals (  \" nonzero   entities   size   for   primary   filter \"  ,     0  ,    getEntitiesWithPrimaryFilter (  \" type _  3  \"  ,    new   NameValuePair (  \" none \"  ,     \" none \"  )  )  . size (  )  )  ;", "List < TimelineEntity >    entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    userFilter )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    numericFilter 1  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    numericFilter 2  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesWithPrimaryFilter (  \" type _  1  \"  ,    numericFilter 3  )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesWithPrimaryFilter (  \" type _  2  \"  ,    userFilter )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,     1 L ,    null ,    null ,    userFilter ,    null )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,     1 L ,     0 L ,    null ,    userFilter ,    null )  ;", "assertEquals (  1  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,     2  3  4 L ,    null ,    userFilter ,    null )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,     2  3  4 L ,     3  4  5 L ,    userFilter ,    null )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntities (  \" type _  1  \"  ,    null ,    null ,     3  4  5 L ,    userFilter ,    null )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithPrimaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "List < TimelineEntity >    entities    =    getEntitiesWithFilters (  \" type _  1  \"  ,    null ,    goodTestingFilters )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesWithFilters (  \" type _  1  \"  ,    userFilter ,    goodTestingFilters )  ;", "assertEquals (  2  ,    entities . size (  )  )  ;", ". verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  0  )  )  ;", ". verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,     . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    entities . get (  1  )  )  ;", "entities    =    getEntitiesWithFilters (  \" type _  1  \"  ,    null ,    Collections . singleton ( new   NameValuePair (  \" user \"  ,     \" none \"  )  )  )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesWithFilters (  \" type _  1  \"  ,    null ,    badTestingFilters )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "entities    =    getEntitiesWithFilters (  \" type _  1  \"  ,    userFilter ,    badTestingFilters )  ;", "assertEquals (  0  ,    entities . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithSecondaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "SortedSet < String >    sortedSet    =    new   TreeSet < String >  (  )  ;", "sortedSet . add ( entityId 1  )  ;", "List < EventsOfOneEntity >    timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,    null ,    null ,    null )  . getAllEvents (  )  ;", "assertEquals (  1  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 2  ,    ev 1  )  ;", "sortedSet . add ( entityId 1 b )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,    null ,    null ,    null )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 2  ,    ev 1  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 2  ,    ev 1  )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,     1 L ,    null ,    null ,    null )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 2  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 2  )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,     3  4  5 L ,    null ,    null )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 2  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 2  )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,     1  2  3 L ,    null ,    null )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 2  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 2  )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,    null ,     3  4  5 L ,    null )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 1  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 1  )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,    null ,     1  2  3 L ,    null )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 1  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 1  )  ;", "timelines    =    store . getEntityTimelines ( entityType 1  ,    sortedSet ,    null ,    null ,    null ,    Collections . singleton (  \" end _ event \"  )  )  . getAllEvents (  )  ;", "assertEquals (  2  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 1  ,    entityType 1  ,    ev 2  )  ;", ". verifyEntityTimeline ( timelines . get (  1  )  ,    entityId 1 b ,    entityType 1  ,    ev 2  )  ;", "sortedSet . add ( entityId 2  )  ;", "timelines    =    store . getEntityTimelines ( entityType 2  ,    sortedSet ,    null ,    null ,    null ,    null )  . getAllEvents (  )  ;", "assertEquals (  1  ,    timelines . size (  )  )  ;", ". verifyEntityTimeline ( timelines . get (  0  )  ,    entityId 2  ,    entityType 2  ,    ev 3  ,    ev 4  )  ;", "}", "METHOD_END"], "methodName": ["testGetEvents"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "TimelineStoreTestUtils . verifyEntityInfo ( null ,    null ,    null ,    null ,    null ,    null ,    store . getEntity (  \" id _  1  \"  ,     \" type _  2  \"  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,     1  2  3 L ,    store . getEntity ( entityId 1  ,    entityType 1  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,     1  2  3 L ,    store . getEntity ( entityId 1 b ,    entityType 1  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 2  ,    entityType 2  ,    events 2  ,    relEntityMap ,    TimelineStoreTestUtils . EMPTY _ PRIMARY _ FILTERS ,    TimelineStoreTestUtils . EMPTY _ MAP ,     (  -  1  2  3 L )  ,    store . getEntity ( entityId 2  ,    entityType 2  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 4  ,    entityType 4  ,    TimelineStoreTestUtils . EMPTY _ EVENTS ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    TimelineStoreTestUtils . EMPTY _ PRIMARY _ FILTERS ,    TimelineStoreTestUtils . EMPTY _ MAP ,     4  2 L ,    store . getEntity ( entityId 4  ,    entityType 4  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 5  ,    entityType 5  ,    TimelineStoreTestUtils . EMPTY _ EVENTS ,    relEntityMap 2  ,    TimelineStoreTestUtils . EMPTY _ PRIMARY _ FILTERS ,    TimelineStoreTestUtils . EMPTY _ MAP ,     4  2 L ,    store . getEntity ( entityId 5  ,    entityType 5  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1  ,    entityType 1  ,    events 1  ,    null ,    null ,    null ,    store . getEntity ( entityId 1  ,    entityType 1  ,    EnumSet . of ( TimelineReader . Field . EVENTS )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1  ,    entityType 1  ,    Collections . singletonList ( ev 2  )  ,    null ,    null ,    null ,    store . getEntity ( entityId 1  ,    entityType 1  ,    EnumSet . of ( TimelineReader . Field . LAST _ EVENT _ ONLY )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1 b ,    entityType 1  ,    events 1  ,    TimelineStoreTestUtils . EMPTY _ REL _ ENTITIES ,    primaryFilters ,    otherInfo ,    store . getEntity ( entityId 1 b ,    entityType 1  ,    null )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1  ,    entityType 1  ,    null ,    null ,    primaryFilters ,    null ,    store . getEntity ( entityId 1  ,    entityType 1  ,    EnumSet . of ( TimelineReader . Field . PRIMARY _ FILTERS )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 1  ,    entityType 1  ,    null ,    null ,    null ,    otherInfo ,    store . getEntity ( entityId 1  ,    entityType 1  ,    EnumSet . of ( TimelineReader . Field . OTHER _ INFO )  )  )  ;", "TimelineStoreTestUtils . verifyEntityInfo ( entityId 2  ,    entityType 2  ,    null ,    relEntityMap ,    null ,    null ,    store . getEntity ( entityId 2  ,    entityType 2  ,    EnumSet . of ( TimelineReader . Field . RELATED _ ENTITIES )  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSingleEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "TimelineStoreTestUtils . verifyEntityInfo ( entityId ,    entityType ,    events ,    relatedEntities ,    primaryFilters ,    otherInfo ,    retrievedEntityInfo )  ;", "assertEquals ( startTime ,    retrievedEntityInfo . getStartTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyEntityInfo"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( entityId    =  =    null )     {", "astNull ( retrievedEntityInfo )  ;", "return ;", "}", "astEquals ( entityId ,    retrievedEntityInfo . getEntityId (  )  )  ;", "astEquals ( entityType ,    retrievedEntityInfo . getEntityType (  )  )  ;", "if    ( events    =  =    null )     {", "astNull ( retrievedEntityInfo . getEvents (  )  )  ;", "} else    {", "astEquals ( events ,    retrievedEntityInfo . getEvents (  )  )  ;", "}", "if    ( relatedEntities    =  =    null )     {", "astNull ( retrievedEntityInfo . getRelatedEntities (  )  )  ;", "} else    {", "astEquals ( relatedEntities ,    retrievedEntityInfo . getRelatedEntities (  )  )  ;", "}", "if    ( primaryFilters    =  =    null )     {", "astNull ( retrievedEntityInfo . getPrimaryFilters (  )  )  ;", "} else    {", "astTrue ( primaryFilters . equals ( retrievedEntityInfo . getPrimaryFilters (  )  )  )  ;", "}", "if    ( otherInfo    =  =    null )     {", "astNull ( retrievedEntityInfo . getOtherInfo (  )  )  ;", "} else    {", "astTrue ( otherInfo . equals ( retrievedEntityInfo . getOtherInfo (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyEntityInfo"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( entityId ,    retrievedEvents . getEntityId (  )  )  ;", "assertEquals ( entityType ,    retrievedEvents . getEntityType (  )  )  ;", "assertEquals ( actualEvents . length ,    retrievedEvents . getEvents (  )  . size (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( actualEvents . length )  ;    i +  +  )     {", "assertEquals ( actualEvents [ i ]  ,    retrievedEvents . getEvents (  )  . get ( i )  )  ;", "}", "}", "METHOD_END"], "methodName": ["verifyEntityTimeline"], "fileName": "org.apache.hadoop.yarn.server.timeline.TimelineStoreTestUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "conf . set ( YARN _ ADMIN _ ACL ,     \" owner \"  )  ;", "timelineACLsManager    =    new    ( conf )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "try    {", "timelineACLsManager . checkAccess ( UserGroupInformation . createRemoteUser (  \" owner \"  )  ,    entity )  ;", "Assert . fail (  \" Exception   is   expected \"  )  ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue (  \" It ' s   not   the   exact   expected   exception \"  ,    e . getMessage (  )  . contains (  \" is   corrupted .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCorruptedOwnerInfo"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TestTimelineACLsManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "conf . set ( YARN _ ADMIN _ ACL ,     \" admin \"  )  ;", "timelineACLsManager    =    new    ( conf )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . addPrimaryFilter ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  ,     \" owner \"  )  ;", "Assert . assertTrue (  \" Owner   should   be   allowed   to   access \"  ,    timelineACLsManager . checkAccess ( UserGroupInformation . createRemoteUser (  \" owner \"  )  ,    entity )  )  ;", "Assert . assertFalse (  \" Other   shouldn ' t   be   allowed   to   access \"  ,    timelineACLsManager . checkAccess ( UserGroupInformation . createRemoteUser (  \" other \"  )  ,    entity )  )  ;", "Assert . assertTrue (  \" Admin   should   be   allowed   to   access \"  ,    timelineACLsManager . checkAccess ( UserGroupInformation . createRemoteUser (  \" admin \"  )  ,    entity )  )  ;", "}", "METHOD_END"], "methodName": ["testYarnACLsEnabled"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TestTimelineACLsManager"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( YARN _ ACL _ ENABLE ,    false )  ;", "timelineACLsManager    =    new    ( conf )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . addPrimaryFilter ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  ,     \" owner \"  )  ;", "Assert . assertTrue (  \" Always   true   when   ACLs   are   not   enabled \"  ,    timelineACLsManager . checkAccess ( UserGroupInformation . createRemoteUser (  \" user \"  )  ,    entity )  )  ;", "}", "METHOD_END"], "methodName": ["testYarnACLsNotEnabled"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TestTimelineACLsManager"}, {"methodBody": ["METHOD_START", "{", "if    ( TimelineACLsManager . LOG . isDebugEnabled (  )  )     {", "TimelineACLsManager . LOG . debug (  (  (  (  \" Verifying   the   access   of    \"     +     ( callerUGI    =  =    null    ?    null    :    callerUGI . getShortUserName (  )  )  )     +     \"    on   the   timeline   entity    \"  )     +     ( new   EntityIdentifier ( entity . getEntityId (  )  ,    entity . getEntityType (  )  )  )  )  )  ;", "}", "if    (  !  ( adminAclsManager . areACLsEnabled (  )  )  )     {", "return   true ;", "}", "Set < Object >    values    =    entity . getPrimaryFilters (  )  . get ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  )  ;", "if    (  ( values    =  =    null )     |  |     (  ( values . size (  )  )     !  =     1  )  )     {", "throw   new   YarnException (  (  (  \" Owner   information   of   the   timeline   entity    \"     +     ( new   EntityIdentifier ( entity . getEntityId (  )  ,    entity . getEntityType (  )  )  )  )     +     \"    is   corrupted .  \"  )  )  ;", "}", "String   owner    =    values . iterator (  )  . next (  )  . toString (  )  ;", "if    (  ( callerUGI    !  =    null )     &  &     (  ( adminAclsManager . isAdmin ( callerUGI )  )     |  |     ( callerUGI . getShortUserName (  )  . equals ( owner )  )  )  )     {", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager"}, {"methodBody": ["METHOD_START", "{", "AdminACLsManager   oldAdminACLsManager    =    this . adminAclsManager ;", "this . adminAclsManager    =    adminAclsManager ;", "return   oldAdminACLsManager ;", "}", "METHOD_END"], "methodName": ["setAdminACLsManager"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager"}, {"methodBody": ["METHOD_START", "{", "secretManager . cancelToken ( token ,    canceler )  ;", "}", "METHOD_END"], "methodName": ["cancelToken"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TimelineDelegationTokenSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "renewer    =     ( renewer    =  =    null )     ?    ugi . getShortUserName (  )     :    renewer ;", "String   user    =    ugi . getUserName (  )  ;", "Text   owner    =    new   Text ( user )  ;", "Text   realUser    =    null ;", "if    (  ( ugi . getRealUser (  )  )     !  =    null )     {", "realUser    =    new   Text ( ugi . getRealUser (  )  . getUserName (  )  )  ;", "}", "Identifier   tokenIdentifier    =    new   Identifier ( owner ,    new   Text ( renewer )  ,    realUser )  ;", "Token < Identifier >    token    =    new   Token < Identifier >  ( tokenIdentifier ,    secretManager )  ;", "SecurityUtil . setTokenService ( token ,    serviceAddr )  ;", "return   token ;", "}", "METHOD_END"], "methodName": ["createToken"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TimelineDelegationTokenSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "return   secretManager . renewToken ( token ,    renewer )  ;", "}", "METHOD_END"], "methodName": ["renewToken"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TimelineDelegationTokenSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "ByteArrayInputStream   buf    =    new   ByteArrayInputStream ( token . getIdentifier (  )  )  ;", "DataInputStream   dis    =    new   DataInputStream ( buf )  ;", "Identifier   id    =    new   Identifier (  )  ;", "try    {", "id . readFields ( dis )  ;", "secretManager . verifyToken ( id ,    token . getPassword (  )  )  ;", "}    finally    {", "dis . close (  )  ;", "}", "return   id . getUser (  )  ;", "}", "METHOD_END"], "methodName": ["verifyToken"], "fileName": "org.apache.hadoop.yarn.server.timeline.security.TimelineDelegationTokenSecretManagerService"}, {"methodBody": ["METHOD_START", "{", "if    ( accessControlRequestHeaders    =  =    null )     {", "return   true ;", "}", "String [  ]    headers    =    accessControlRequestHeaders . trim (  )  . split (  \"  \\  \\ s *  ,  \\  \\ s *  \"  )  ;", "return   allowedHeaders . containsAll ( Arrays . asList ( headers )  )  ;", "}", "METHOD_END"], "methodName": ["areHeadersAllowed"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "String   origin    =    CrossOriginFilter . encodeHeader ( req . getHeader ( CrossOriginFilter . ORIGIN )  )  ;", "if    (  !  ( CrossOriginFilter . isCrossOrigin ( origin )  )  )     {", "return ;", "}", "if    (  !  ( isOriginAllowed ( origin )  )  )     {", "return ;", "}", "String   accessControlRequestMethod    =    req . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ METHOD )  ;", "if    (  !  ( isMethodAllowed ( accessControlRequestMethod )  )  )     {", "return ;", "}", "String   accessControlRequestHeaders    =    req . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ HEADERS )  ;", "if    (  !  ( areHeadersAllowed ( accessControlRequestHeaders )  )  )     {", "return ;", "}", "res . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ ORIGIN ,    origin )  ;", "res . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ CREDENTIALS ,    Boolean . TRUE . toString (  )  )  ;", "res . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ METHODS ,    getAllowedMethodsHeader (  )  )  ;", "res . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ HEADERS ,    getAllowedHeadersHeader (  )  )  ;", "res . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ MAX _ AGE ,    maxAge )  ;", "}", "METHOD_END"], "methodName": ["doCrossFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "if    ( head =  =    null )     {", "return   null ;", "}", "try    {", "return   URLEncodencode ( head    \" ASCII \"  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["encodeHeader"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "return   StringUtils . join ( allowedHeaders ,     '  ,  '  )  ;", "}", "METHOD_END"], "methodName": ["getAllowedHeadersHeader"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "return   StringUtils . join ( allowedMethods ,     '  ,  '  )  ;", "}", "METHOD_END"], "methodName": ["getAllowedMethodsHeader"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "String   allowedHeadersConfig    =    filterConfig . getInitParameter ( CrossOriginFilter . ALLOWED _ HEADERS )  ;", "if    ( allowedHeadersConfig    =  =    null )     {", "allowedHeadersConfig    =    CrossOriginFilter . ALLOWED _ HEADERS _ DEFAULT ;", "}", "allowedHeaders    =    Arrays . asList ( allowedHeadersConfig . trim (  )  . split (  \"  \\  \\ s *  ,  \\  \\ s *  \"  )  )  ;", "CrossOriginFilter . LOG . info (  (  \" Allowed   Headers :     \"     +     ( getAllowedHeadersHeader (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["initializeAllowedHeaders"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "String   allowedMethodsConfig    =    filterConfig . getInitParameter ( CrossOriginFilter . ALLOWED _ METHODS )  ;", "if    ( allowedMethodsConfig    =  =    null )     {", "allowedMethodsConfig    =    CrossOriginFilter . ALLOWED _ METHODS _ DEFAULT ;", "}", "allowedMethods    =    Arrays . asList ( allowedMethodsConfig . trim (  )  . split (  \"  \\  \\ s *  ,  \\  \\ s *  \"  )  )  ;", "CrossOriginFilter . LOG . info (  (  \" Allowed   Methods :     \"     +     ( getAllowedMethodsHeader (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["initializeAllowedMethods"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "String   allowedOriginsConfig    =    filterConfig . getInitParameter ( CrossOriginFilter . ALLOWED _ ORIGINS )  ;", "if    ( allowedOriginsConfig    =  =    null )     {", "allowedOriginsConfig    =    CrossOriginFilter . ALLOWED _ ORIGINS _ DEFAULT ;", "}", "allowedOrigins    =    Arrays . asList ( allowedOriginsConfig . trim (  )  . split (  \"  \\  \\ s *  ,  \\  \\ s *  \"  )  )  ;", "CrossOriginFilter . LOG . info (  (  \" Allowed   Origins :     \"     +     ( StringUtils . join ( allowedOrigins ,     '  ,  '  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["initializeAllowedOrigins"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "maxAge    =    filterConfig . getInitParameter ( CrossOriginFilter . MAX _ AGE )  ;", "if    (  ( maxAge )     =  =    null )     {", "maxAge    =    CrossOriginFilter . MAX _ AGE _ DEFAULT ;", "}", "CrossOriginFilter . LOG . info (  (  \" Max   Age :     \"     +     ( maxAge )  )  )  ;", "}", "METHOD_END"], "methodName": ["initializeMaxAge"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "return   origin    !  =    null ;", "}", "METHOD_END"], "methodName": ["isCrossOrigin"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "if    ( accessControlRequestMethod    =  =    null )     {", "return   false ;", "}", "return   allowedMethods . contains ( accessControlRequestMethod )  ;", "}", "METHOD_END"], "methodName": ["isMethodAllowed"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "return   allowedOrigins . contains ( origin )  ;", "}", "METHOD_END"], "methodName": ["isOriginAllowed"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "return   conf . getValByRegex ( CrossOriginFilterInitializer . PREFIX )  ;", "}", "METHOD_END"], "methodName": ["getFilterParameters"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.CrossOriginFilterInitializer"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    conf    =    new   HashMap < String ,    String >  (  )  ;", "conf . put ( CrossOriginFilter . ALLOWED _ ORIGINS ,     \" example . com \"  )  ;", "FilterConfig   filterConfig    =    new    . FilterConfigTest ( conf )  ;", "HttpServletRequest   mockReq    =    mock ( HttpServletRequest . class )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ORIGIN )  )  . thenReturn (  \" example . com \"  )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ METHOD )  )  . thenReturn (  \" GET \"  )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ HEADERS )  )  . thenReturn (  \" X - Requested - With \"  )  ;", "HttpServletResponse   mockRes    =    mock ( HttpServletResponse . class )  ;", "FilterChain   mockChain    =    mock ( FilterChain . class )  ;", "CrossOriginFilter   filter    =    new   CrossOriginFilter (  )  ;", "filter . init ( filterConfig )  ;", "filter . doFilter ( mockReq ,    mockRes ,    mockChain )  ;", "verify ( mockRes )  . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ ORIGIN ,     \" example . com \"  )  ;", "verify ( mockRes )  . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ CREDENTIALS ,    Boolean . TRUE . toString (  )  )  ;", "verify ( mockRes )  . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ METHODS ,    filter . getAllowedMethodsHeader (  )  )  ;", "verify ( mockRes )  . setHeader ( CrossOriginFilter . ACCESS _ CONTROL _ ALLOW _ HEADERS ,    filter . getAllowedHeadersHeader (  )  )  ;", "verify ( mockChain )  . doFilter ( mockReq ,    mockRes )  ;", "}", "METHOD_END"], "methodName": ["testCrossOriginFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestCrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    conf    =    new   HashMap < String ,    String >  (  )  ;", "conf . put ( CrossOriginFilter . ALLOWED _ ORIGINS ,     \" example . com \"  )  ;", "FilterConfig   filterConfig    =    new    . FilterConfigTest ( conf )  ;", "HttpServletRequest   mockReq    =    mock ( HttpServletRequest . class )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ORIGIN )  )  . thenReturn (  \" example . com \"  )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ METHOD )  )  . thenReturn (  \" GET \"  )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ HEADERS )  )  . thenReturn (  \" Disallowed - Header \"  )  ;", "HttpServletResponse   mockRes    =    mock ( HttpServletResponse . class )  ;", "FilterChain   mockChain    =    mock ( FilterChain . class )  ;", "CrossOriginFilter   filter    =    new   CrossOriginFilter (  )  ;", "filter . init ( filterConfig )  ;", "filter . doFilter ( mockReq ,    mockRes ,    mockChain )  ;", "verifyZeroInteractions ( mockRes )  ;", "verify ( mockChain )  . doFilter ( mockReq ,    mockRes )  ;", "}", "METHOD_END"], "methodName": ["testDisallowedHeader"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestCrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    conf    =    new   HashMap < String ,    String >  (  )  ;", "conf . put ( CrossOriginFilter . ALLOWED _ ORIGINS ,     \" example . com \"  )  ;", "FilterConfig   filterConfig    =    new    . FilterConfigTest ( conf )  ;", "HttpServletRequest   mockReq    =    mock ( HttpServletRequest . class )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ORIGIN )  )  . thenReturn (  \" example . com \"  )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ACCESS _ CONTROL _ REQUEST _ METHOD )  )  . thenReturn (  \" DISALLOWED _ METHOD \"  )  ;", "HttpServletResponse   mockRes    =    mock ( HttpServletResponse . class )  ;", "FilterChain   mockChain    =    mock ( FilterChain . class )  ;", "CrossOriginFilter   filter    =    new   CrossOriginFilter (  )  ;", "filter . init ( filterConfig )  ;", "filter . doFilter ( mockReq ,    mockRes ,    mockChain )  ;", "verifyZeroInteractions ( mockRes )  ;", "verify ( mockChain )  . doFilter ( mockReq ,    mockRes )  ;", "}", "METHOD_END"], "methodName": ["testDisallowedMethod"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestCrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    conf    =    new   HashMap < String ,    String >  (  )  ;", "conf . put ( CrossOriginFilter . ALLOWED _ ORIGINS ,     \" example . com \"  )  ;", "FilterConfig   filterConfig    =    new    . FilterConfigTest ( conf )  ;", "HttpServletRequest   mockReq    =    mock ( HttpServletRequest . class )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ORIGIN )  )  . thenReturn (  \" example . org \"  )  ;", "HttpServletResponse   mockRes    =    mock ( HttpServletResponse . class )  ;", "FilterChain   mockChain    =    mock ( FilterChain . class )  ;", "CrossOriginFilter   filter    =    new   CrossOriginFilter (  )  ;", "filter . init ( filterConfig )  ;", "filter . doFilter ( mockReq ,    mockRes ,    mockChain )  ;", "verifyZeroInteractions ( mockRes )  ;", "verify ( mockChain )  . doFilter ( mockReq ,    mockRes )  ;", "}", "METHOD_END"], "methodName": ["testDisallowedOrigin"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestCrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    conf    =    new   HashMap < String ,    String >  (  )  ;", "conf . put ( CrossOriginFilter . ALLOWED _ ORIGINS ,     \"  \"  )  ;", "FilterConfig   filterConfig    =    new    . FilterConfigTest ( conf )  ;", "HttpServletRequest   mockReq    =    mock ( HttpServletRequest . class )  ;", "when ( mockReq . getHeader ( CrossOriginFilter . ORIGIN )  )  . thenReturn ( null )  ;", "HttpServletResponse   mockRes    =    mock ( HttpServletResponse . class )  ;", "FilterChain   mockChain    =    mock ( FilterChain . class )  ;", "CrossOriginFilter   filter    =    new   CrossOriginFilter (  )  ;", "filter . init ( filterConfig )  ;", "filter . doFilter ( mockReq ,    mockRes ,    mockChain )  ;", "verifyZeroInteractions ( mockRes )  ;", "verify ( mockChain )  . doFilter ( mockReq ,    mockRes )  ;", "}", "METHOD_END"], "methodName": ["testSameOrigin"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestCrossOriginFilter"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set (  (  (  . PREFIX )     +     \" rootparam \"  )  ,     \" rootvalue \"  )  ;", "conf . set (  (  (  . PREFIX )     +     \" nested . param \"  )  ,     \" nestedvalue \"  )  ;", "conf . set (  \" outofscopeparam \"  ,     \" outofscopevalue \"  )  ;", "Map < String ,    String >    filterParameters    =     . getFilterParameters ( conf )  ;", "String   rootvalue    =    filterParameters . get (  (  (  . PREFIX )     +     \" rootparam \"  )  )  ;", "String   nestedvalue    =    filterParameters . get (  (  (  . PREFIX )     +     \" nested . param \"  )  )  ;", "String   outofscopeparam    =    filterParameters . get (  \" outofscopeparam \"  )  ;", "Assert . assertEquals (  \" Could   not   find   filter   parameter \"  ,     \" rootvalue \"  ,    rootvalue )  ;", "Assert . assertEquals (  \" Could   not   find   filter   parameter \"  ,     \" nestedvalue \"  ,    nestedvalue )  ;", "Assert . assertNull (  \" Found   unexpected   value   in   filter   parameters \"  ,    outofscopeparam )  ;", "}", "METHOD_END"], "methodName": ["testGetFilterParameters"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestCrossOriginFilterInitializer"}, {"methodBody": ["METHOD_START", "{", "beforeTime    =     ( System . currentTimeMillis (  )  )     -     1  ;", "TestMemoryTimelineStore   store    =    new   TestMemoryTimelineStore (  )  ;", "store . setup (  )  ;", "return   store . geStore (  )  ;", "}", "METHOD_END"], "methodName": ["mockTimelineStore"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". AboutInfo   about    =    response . getEntity (  . AboutInfo . class )  ;", "Assert . assertNotNull ( about )  ;", "Assert . assertEquals (  \" Timeline   API \"  ,    about . getAbout (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAbout"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" fromId \"  ,     \" id _  2  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals (  1  ,    response . getEntity ( TimelineEntities . class )  . getEntities (  )  . size (  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" fromId \"  ,     \" id _  1  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals (  2  ,    response . getEntity ( TimelineEntities . class )  . getEntities (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFromId"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" fromTs \"  ,    Long . toString ( beforeTime )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals (  0  ,    response . getEntity ( TimelineEntities . class )  . getEntities (  )  . size (  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" fromTs \"  ,    Long . toString ( System . currentTimeMillis (  )  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals (  2  ,    response . getEntity ( TimelineEntities . class )  . getEntities (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testFromTs"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" type _  1  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". verifyEntities ( response . getEntity ( TimelineEntities . class )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "AdminACLsManager   oldAdminACLsManager    =    TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( TestTimelineWebServices . adminACLsManager )  ;", "try    {", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    4  \"  )  ;", "entity . setEntityType (  \" test   type    4  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "entities . addEntity ( entity )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "entities    =    new   TimelineEntities (  )  ;", "entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    5  \"  )  ;", "entity . setEntityType (  \" test   type    4  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "entities . addEntity ( entity )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" other \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" other \"  )  . path (  \" test   type    4  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "entities    =    response . getEntity ( TimelineEntities . class )  ;", "assertEquals (  1  ,    entities . getEntities (  )  . size (  )  )  ;", "assertEquals (  \" test   type    4  \"  ,    entities . getEntities (  )  . get (  0  )  . getEntityType (  )  )  ;", "assertEquals (  \" test   id    5  \"  ,    entities . getEntities (  )  . get (  0  )  . getEntityId (  )  )  ;", "}    finally    {", "TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( oldAdminACLsManager )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetEntitiesWithYarnACLsEnabled"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . path (  \" id _  1  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelineEntity   entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNotNull ( entity )  ;", "Assert . assertEquals (  \" id _  1  \"  ,    entity . getEntityId (  )  )  ;", "Assert . assertEquals (  \" type _  1  \"  ,    entity . getEntityType (  )  )  ;", "Assert . assertEquals (  1  2  3 L ,    entity . getStartTime (  )  . longValue (  )  )  ;", "Assert . assertEquals (  2  ,    entity . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity . getOtherInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . path (  \" id _  1  \"  )  . queryParam (  \" fields \"  ,     \" events , otherinfo \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelineEntity   entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNotNull ( entity )  ;", "Assert . assertEquals (  \" id _  1  \"  ,    entity . getEntityId (  )  )  ;", "Assert . assertEquals (  \" type _  1  \"  ,    entity . getEntityType (  )  )  ;", "Assert . assertEquals (  1  2  3 L ,    entity . getStartTime (  )  . longValue (  )  )  ;", "Assert . assertEquals (  2  ,    entity . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    entity . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity . getOtherInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntityFields1"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . path (  \" id _  1  \"  )  . queryParam (  \" fields \"  ,     (  \" lasteventonly ,  \"     +     \" primaryfilters , relatedentities \"  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelineEntity   entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNotNull ( entity )  ;", "Assert . assertEquals (  \" id _  1  \"  ,    entity . getEntityId (  )  )  ;", "Assert . assertEquals (  \" type _  1  \"  ,    entity . getEntityType (  )  )  ;", "Assert . assertEquals (  1  2  3 L ,    entity . getStartTime (  )  . longValue (  )  )  ;", "Assert . assertEquals (  1  ,    entity . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    entity . getOtherInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEntityFields2"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "AdminACLsManager   oldAdminACLsManager    =    TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( TestTimelineWebServices . adminACLsManager )  ;", "try    {", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    3  \"  )  ;", "entity . setEntityType (  \" test   type    3  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "entities . addEntity ( entity )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" test   type    3  \"  )  . path (  \" test   id    3  \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNull ( entity . getPrimaryFilters (  )  . get ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" test   type    3  \"  )  . path (  \" test   id    3  \"  )  . queryParam (  \" fields \"  ,     \" relatedentities \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNull ( entity . getPrimaryFilters (  )  . get ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" test   type    3  \"  )  . path (  \" test   id    3  \"  )  . queryParam (  \" fields \"  ,     \" primaryfilters \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNull ( entity . getPrimaryFilters (  )  . get ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" test   type    3  \"  )  . path (  \" test   id    3  \"  )  . queryParam (  \" user . name \"  ,     \" other \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals ( NOT _ FOUND ,    response . getClientResponseStatus (  )  )  ;", "}    finally    {", "TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( oldAdminACLsManager )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetEntityWithYarnACLsEnabled"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . path (  \" events \"  )  . queryParam (  \" entityId \"  ,     \" id _  1  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelineEvents   events    =    response . getEntity ( TimelineEvents . class )  ;", "Assert . assertNotNull ( events )  ;", "Assert . assertEquals (  1  ,    events . getAllEvents (  )  . size (  )  )  ;", "TimelineEvents . EventsOfOneEntity   partEvents    =    events . getAllEvents (  )  . get (  0  )  ;", "Assert . assertEquals (  2  ,    partEvents . getEvents (  )  . size (  )  )  ;", "TimelineEvent   event 1     =    partEvents . getEvents (  )  . get (  0  )  ;", "Assert . assertEquals (  4  5  6 L ,    event 1  . getTimestamp (  )  )  ;", "Assert . assertEquals (  \" end _ event \"  ,    event 1  . getEventType (  )  )  ;", "Assert . assertEquals (  1  ,    event 1  . getEventInfo (  )  . size (  )  )  ;", "TimelineEvent   event 2     =    partEvents . getEvents (  )  . get (  1  )  ;", "Assert . assertEquals (  1  2  3 L ,    event 2  . getTimestamp (  )  )  ;", "Assert . assertEquals (  \" start _ event \"  ,    event 2  . getEventType (  )  )  ;", "Assert . assertEquals (  0  ,    event 2  . getEventInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetEvents"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "AdminACLsManager   oldAdminACLsManager    =    TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( TestTimelineWebServices . adminACLsManager )  ;", "try    {", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    5  \"  )  ;", "entity . setEntityType (  \" test   type    5  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setEventType (  \" event   type    1  \"  )  ;", "event . setTimestamp ( System . currentTimeMillis (  )  )  ;", "entity . addEvent ( event )  ;", "entities . addEntity ( entity )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "entities    =    new   TimelineEntities (  )  ;", "entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    6  \"  )  ;", "entity . setEntityType (  \" test   type    5  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "event    =    new   TimelineEvent (  )  ;", "event . setEventType (  \" event   type    2  \"  )  ;", "event . setTimestamp ( System . currentTimeMillis (  )  )  ;", "entity . addEvent ( event )  ;", "entities . addEntity ( entity )  ;", "r    =    resource (  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" other \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" test   type    5  \"  )  . path (  \" events \"  )  . queryParam (  \" user . name \"  ,     \" other \"  )  . queryParam (  \" entityId \"  ,     \" test   id    5  , test   id    6  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelineEvents   events    =    response . getEntity ( TimelineEvents . class )  ;", "assertEquals (  1  ,    events . getAllEvents (  )  . size (  )  )  ;", "assertEquals (  \" test   id    6  \"  ,    events . getAllEvents (  )  . get (  0  )  . getEntityId (  )  )  ;", "}    finally    {", "TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( oldAdminACLsManager )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetEventsWithYarnACLsEnabled"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    1  \"  )  ;", "entity . setEntityType (  \" test   type    1  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "entities . addEntity ( entity )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals ( FORBIDDEN ,    response . getClientResponseStatus (  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelinePutResponse   putResposne    =    response . getEntity ( TimelinePutResponse . class )  ;", "Assert . assertNotNull ( putResposne )  ;", "Assert . assertEquals (  0  ,    putResposne . getErrors (  )  . size (  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" test   type    1  \"  )  . path (  \" test   id    1  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "entity    =    response . getEntity ( TimelineEntity . class )  ;", "Assert . assertNotNull ( entity )  ;", "Assert . assertEquals (  \" test   id    1  \"  ,    entity . getEntityId (  )  )  ;", "Assert . assertEquals (  \" test   type    1  \"  ,    entity . getEntityType (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPostEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "Map < String ,    Set < Object >  >    filters    =    new   HashMap < String ,    Set < Object >  >  (  )  ;", "filters . put ( TimelineStore . SystemFilter . ENTITY _ OWNER . toString (  )  ,    new   HashSet < Object >  (  )  )  ;", "entity . setPrimaryFilters ( filters )  ;", "entity . setEntityId (  \" test   id    6  \"  )  ;", "entity . setEntityType (  \" test   type    6  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "entities . addEntity ( entity )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "TimelinePutResponse   putResposne    =    response . getEntity ( TimelinePutResponse . class )  ;", "Assert . assertEquals (  1  ,    putResposne . getErrors (  )  . size (  )  )  ;", "List < TimelinePutError >    errors    =    putResposne . getErrors (  )  ;", "Assert . assertEquals ( SYSTEM _ FILTER _ CONFLICT ,    errors . get (  0  )  . getErrorCode (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPostEntitiesWithPrimaryFilter"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "AdminACLsManager   oldAdminACLsManager    =    TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( TestTimelineWebServices . adminACLsManager )  ;", "try    {", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" test   id    2  \"  )  ;", "entity . setEntityType (  \" test   type    2  \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "entities . addEntity ( entity )  ;", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" tester \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "TimelinePutResponse   putResponse    =    response . getEntity ( TimelinePutResponse . class )  ;", "Assert . assertNotNull ( putResponse )  ;", "Assert . assertEquals (  0  ,    putResponse . getErrors (  )  . size (  )  )  ;", "response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . queryParam (  \" user . name \"  ,     \" other \"  )  . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "putResponse    =    response . getEntity ( TimelinePutResponse . class )  ;", "Assert . assertNotNull ( putResponse )  ;", "Assert . assertEquals (  1  ,    putResponse . getErrors (  )  . size (  )  )  ;", "Assert . assertEquals ( ACCESS _ DENIED ,    putResponse . getErrors (  )  . get (  0  )  . getErrorCode (  )  )  ;", "}    finally    {", "TestTimelineWebServices . timelineACLsManager . setAdminACLsManager ( oldAdminACLsManager )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntitiesWithYarnACLsEnabled"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" primaryFilter \"  ,     (  \" appname :  \"     +     ( Integer . toString ( Integer . MAX _ VALUE )  )  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". verifyEntities ( response . getEntity ( TimelineEntities . class )  )  ;", "}", "METHOD_END"], "methodName": ["testPrimaryFilterInteger"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" primaryFilter \"  ,     (  \" long :  \"     +     ( Long . toString (  (  (  ( long )     ( Integer . MAX _ VALUE )  )     +     1 L )  )  )  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". verifyEntities ( response . getEntity ( TimelineEntities . class )  )  ;", "}", "METHOD_END"], "methodName": ["testPrimaryFilterLong"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \"  \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" primaryFilter \"  ,     \" other :  1  2  3 abc \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", "assertEquals (  0  ,    response . getEntity ( TimelineEntities . class )  . getEntities (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testPrimaryFilterNumericString"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" primaryFilter \"  ,     \" other :  \\  \"  1  2  3 abc \\  \"  \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". verifyEntities ( response . getEntity ( TimelineEntities . class )  )  ;", "}", "METHOD_END"], "methodName": ["testPrimaryFilterNumericStringWithQuotes"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" primaryFilter \"  ,     \" user : username \"  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". verifyEntities ( response . getEntity ( TimelineEntities . class )  )  ;", "}", "METHOD_END"], "methodName": ["testPrimaryFilterString"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "WebResource   r    =    resource (  )  ;", "ClientResponse   response    =    r . path (  \" ws \"  )  . path (  \" v 1  \"  )  . path (  \" timeline \"  )  . path (  \" type _  1  \"  )  . queryParam (  \" secondaryFilter \"  ,     (  \" user : username , appname :  \"     +     ( Integer . toString ( Integer . MAX _ VALUE )  )  )  )  . accept ( APPLICATION _ JSON )  . get ( ClientResponse . class )  ;", "assertEquals ( APPLICATION _ JSON _ TYPE ,    response . getType (  )  )  ;", ". verifyEntities ( response . getEntity ( TimelineEntities . class )  )  ;", "}", "METHOD_END"], "methodName": ["testSecondaryFilters"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "Assert . assertNotNull ( entities )  ;", "Assert . assertEquals (  2  ,    entities . getEntities (  )  . size (  )  )  ;", "Entity   entity 1     =    entities . getEntities (  )  . get (  0  )  ;", "Assert . assertNotNull ( entity 1  )  ;", "Assert . assertEquals (  \" id _  1  \"  ,    entity 1  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" type _  1  \"  ,    entity 1  . getEntityType (  )  )  ;", "Assert . assertEquals (  1  2  3 L ,    entity 1  . getStartTime (  )  . longValue (  )  )  ;", "Assert . assertEquals (  2  ,    entity 1  . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity 1  . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity 1  . getOtherInfo (  )  . size (  )  )  ;", "Entity   entity 2     =    entities . getEntities (  )  . get (  1  )  ;", "Assert . assertNotNull ( entity 2  )  ;", "Assert . assertEquals (  \" id _  2  \"  ,    entity 2  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" type _  1  \"  ,    entity 2  . getEntityType (  )  )  ;", "Assert . assertEquals (  1  2  3 L ,    entity 2  . getStartTime (  )  . longValue (  )  )  ;", "Assert . assertEquals (  2  ,    entity 2  . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity 2  . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  4  ,    entity 2  . getOtherInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["verifyEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "TestTimelineWebServicesWithSSL . conf    =    new   YarnConfiguration (  )  ;", "TestTimelineWebServicesWithSSL . conf . setBoolean ( TIMELINE _ SERVICE _ ENABLED ,    true )  ;", "TestTimelineWebServicesWithSSL . conf . setClass ( TIMELINE _ SERVICE _ STORE ,    MemoryTimelineStore . class ,    TimelineStore . class )  ;", "TestTimelineWebServicesWithSSL . conf . set ( YARN _ HTTP _ POLICY _ KEY ,     \" HTTPS _ ONLY \"  )  ;", "File   base    =    new   File ( TestTimelineWebServicesWithSSL . BASEDIR )  ;", "FileUtil . fullyDelete ( base )  ;", "base . mkdirs (  )  ;", "TestTimelineWebServicesWithSSL . keystoresDir    =    new   File ( TestTimelineWebServicesWithSSL . BASEDIR )  . getAbsolutePath (  )  ;", "TestTimelineWebServicesWithSSL . sslConfDir    =    KeyStoreTestUtil . getClasspathDir ( TestTimelineWebServicesWithSSL . class )  ;", "KeyStoreTestUtil . setupSSLConfig ( TestTimelineWebServicesWithSSL . keystoresDir ,    TestTimelineWebServicesWithSSL . sslConfDir ,    TestTimelineWebServicesWithSSL . conf ,    false )  ;", "TestTimelineWebServicesWithSSL . conf . addResource (  \" ssl - server . xml \"  )  ;", "TestTimelineWebServicesWithSSL . conf . addResource (  \" ssl - client . xml \"  )  ;", "TestTimelineWebServicesWithSSL . timelineServer    =    new   ApplicationHistoryServer (  )  ;", "TestTimelineWebServicesWithSSL . timelineServer . init ( TestTimelineWebServicesWithSSL . conf )  ;", "TestTimelineWebServicesWithSSL . timelineServer . start (  )  ;", "TestTimelineWebServicesWithSSL . store    =    TestTimelineWebServicesWithSSL . timelineServer . getTimelineStore (  )  ;", "}", "METHOD_END"], "methodName": ["setupServer"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL"}, {"methodBody": ["METHOD_START", "{", "if    (  ( TestTimelineWebServicesWithSSL . timelineServer )     !  =    null )     {", "TestTimelineWebServicesWithSSL . timelineServer . stop (  )  ;", "}", "AHSWebApp . resetInstance (  )  ;", "}", "METHOD_END"], "methodName": ["tearDownServer"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL"}, {"methodBody": ["METHOD_START", "{", "TestTimelineWebServicesWithSSL . TestTimelineClient   client    =    new   TestTimelineWebServicesWithSSL . TestTimelineClient (  )  ;", "try    {", "client . init ( TestTimelineWebServicesWithSSL . conf )  ;", "client . start (  )  ;", "TimelineEntity   expectedEntity    =    new   TimelineEntity (  )  ;", "expectedEntity . setEntityType (  \" test   entity   type \"  )  ;", "expectedEntity . setEntityId (  \" test   entity   id \"  )  ;", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setEventType (  \" test   event   type \"  )  ;", "event . setTimestamp (  0 L )  ;", "expectedEntity . addEvent ( event )  ;", "TimelinePutResponse   response    =    client . putEntities ( expectedEntity )  ;", "Assert . assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "Assert . assertTrue ( client . resp . toString (  )  . contains (  \" https \"  )  )  ;", "TimelineEntity   actualEntity    =    TestTimelineWebServicesWithSSL . store . getEntity ( expectedEntity . getEntityId (  )  ,    expectedEntity . getEntityType (  )  ,    EnumSet . allOf ( TimelineReader . Field . class )  )  ;", "Assert . assertNotNull ( actualEntity )  ;", "Assert . assertEquals ( expectedEntity . getEntityId (  )  ,    actualEntity . getEntityId (  )  )  ;", "Assert . assertEquals ( expectedEntity . getEntityType (  )  ,    actualEntity . getEntityType (  )  )  ;", "}    finally    {", "client . stop (  )  ;", "client . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPutEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TestTimelineWebServicesWithSSL"}, {"methodBody": ["METHOD_START", "{", "init ( res )  ;", "return   new    . AboutInfo (  \" Timeline   API \"  )  ;", "}", "METHOD_END"], "methodName": ["about"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "init ( res )  ;", "try    {", "return   timelineDataManager . getEntities (  . parseStr ( entityType )  ,     . parsePairStr ( primaryFilter ,     \"  :  \"  )  ,     . parsePairsStr ( secondaryFilter ,     \"  ,  \"  ,     \"  :  \"  )  ,     . parseLongStr ( windowStart )  ,     . parseLongStr ( windowEnd )  ,     . parseStr ( fromId )  ,     . parseLongStr ( fromTs )  ,     . parseLongStr ( limit )  ,     . parseFieldsStr ( fields ,     \"  ,  \"  )  ,     . getUser ( req )  )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   BadRequestException (  \" windowStart ,    windowEnd   or   limit   is   not   a   numeric   value .  \"  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "throw   new   BadRequestException (  \" requested   invalid   field .  \"  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Error   getting   entities \"  ,    e )  ;", "throw   new   WebApplicationException ( e ,    Status . INTERNAL _ SERVER _ ERROR )  ;", "}", "}", "METHOD_END"], "methodName": ["getEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "init ( res )  ;", "TimelineEntity   entity    =    null ;", "try    {", "entity    =    timelineDataManager . getEntity (  . parseStr ( entityType )  ,     . parseStr ( entityId )  ,     . parseFieldsStr ( fields ,     \"  ,  \"  )  ,     . getUser ( req )  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "throw   new   BadRequestException (  \" requested   invalid   field .  \"  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Error   getting   entity \"  ,    e )  ;", "throw   new   WebApplicationException ( e ,    Status . INTERNAL _ SERVER _ ERROR )  ;", "}", "if    ( entity    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" Timeline   entity    \"     +     ( new   EntityIdentifier (  . parseStr ( entityId )  ,     . parseStr ( entityType )  )  )  )     +     \"    is   not   found \"  )  )  ;", "}", "return   entity ;", "}", "METHOD_END"], "methodName": ["getEntity"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "init ( res )  ;", "try    {", "return   timelineDataManager . getEvents (  . parseStr ( entityType )  ,     . parseArrayStr ( entityId ,     \"  ,  \"  )  ,     . parseArrayStr ( eventType ,     \"  ,  \"  )  ,     . parseLongStr ( windowStart )  ,     . parseLongStr ( windowEnd )  ,     . parseLongStr ( limit )  ,     . getUser ( req )  )  ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   BadRequestException (  \" windowStart ,    windowEnd   or   limit   is   not   a   numeric   value .  \"  )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Error   getting   entity   timelines \"  ,    e )  ;", "throw   new   WebApplicationException ( e ,    Status . INTERNAL _ SERVER _ ERROR )  ;", "}", "}", "METHOD_END"], "methodName": ["getEvents"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "String   remoteUser    =    req . getRemoteUser (  )  ;", "UserGroupInformation   callerUGI    =    null ;", "if    ( remoteUser    !  =    null )     {", "callerUGI    =    UserGroupInformation . createRemoteUser ( remoteUser )  ;", "}", "return   callerUGI ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "response . setContentType ( null )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "if    ( str    =  =    null )     {", "return   null ;", "}", "SortedSet < String >    strSet    =    new   TreeSet < String >  (  )  ;", "String [  ]    strs    =    str . split ( dmiter )  ;", "for    ( String   aStr    :    strs )     {", "strSet . add ( aStr . trim (  )  )  ;", "}", "return   strSet ;", "}", "METHOD_END"], "methodName": ["parseArrayStr"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "if    ( str    =  =    null )     {", "return   null ;", "}", "String [  ]    strs    =    str . split ( delimiter )  ;", "List < Reader . Field >    fieldList    =    new   ArrayList < Reader . Field >  (  )  ;", "for    ( String   s    :    strs )     {", "s    =    s . trim (  )  . toUpperCase (  )  ;", "if    ( s . equals (  \" EVENTS \"  )  )     {", "fieldList . add ( Reader . Field . EVENTS )  ;", "} else", "if    ( s . equals (  \" LASTEVENTONLY \"  )  )     {", "fieldList . add ( Reader . Field . LAST _ EVENT _ ONLY )  ;", "} else", "if    ( s . equals (  \" RELATEDENTITIES \"  )  )     {", "fieldList . add ( Reader . Field . RELATED _ ENTITIES )  ;", "} else", "if    ( s . equals (  \" PRIMARYFILTERS \"  )  )     {", "fieldList . add ( Reader . Field . PRIMARY _ FILTERS )  ;", "} else", "if    ( s . equals (  \" OTHERINFO \"  )  )     {", "fieldList . add ( Reader . Field . OTHER _ INFO )  ;", "} else    {", "throw   new   IllegalArgumentException (  (  \" Requested   nonexistent   field    \"     +    s )  )  ;", "}", "}", "if    (  ( fieldList . size (  )  )     =  =     0  )     {", "return   null ;", "}", "Reader . Field   f 1     =    fieldList . remove (  (  ( fieldList . size (  )  )     -     1  )  )  ;", "if    (  ( fieldList . size (  )  )     =  =     0  )     {", "return   EnumSet . of ( f 1  )  ;", "} else    {", "return   EnumSet . of ( f 1  ,    fieldList . toArray ( new   Reader . Field [ fieldList . size (  )  ]  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["parseFieldsStr"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "return   str    =  =    null    ?    null    :    Long . parseLong ( str . trim (  )  )  ;", "}", "METHOD_END"], "methodName": ["parseLongStr"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "if    ( str    =  =    null )     {", "return   null ;", "}", "String [  ]    strs    =    str . split ( dmiter ,     2  )  ;", "try    {", "return   new   NameValuePair ( strs [  0  ]  . trim (  )  ,    GenericObjectMapper . OBJECT _ READER . readValue ( strs [  1  ]  . trim (  )  )  )  ;", "}    catch    ( Exception   e )     {", "return   new   NameValuePair ( strs [  0  ]  . trim (  )  ,    strs [  1  ]  . trim (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["parsePairStr"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "if    ( str    =  =    null )     {", "return   null ;", "}", "String [  ]    strs    =    str . split ( aDelimiter )  ;", "Set < NameValuePair >    pairs    =    new   HashSet < NameValuePair >  (  )  ;", "for    ( String   aStr    :    strs )     {", "pairs . add (  . parsePairStr ( aStr ,    pDelimiter )  )  ;", "}", "return   pairs ;", "}", "METHOD_END"], "methodName": ["parsePairsStr"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "return   str    =  =    null    ?    null    :    str . trim (  )  ;", "}", "METHOD_END"], "methodName": ["parseStr"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "init ( res )  ;", "UserGroupInformation   callerUGI    =     . getUser ( req )  ;", "if    ( callerUGI    =  =    null )     {", "String   msg    =     \" The   owner   of   the   posted   timeline   entities   is   not   set \"  ;", ". LOG . error ( msg )  ;", "throw   new   ForbiddenException ( msg )  ;", "}", "try    {", "return   timelineDataManager . postEntities ( entities ,    callerUGI )  ;", "}    catch    ( Exception   e )     {", ". LOG . error (  \" Error   putting   entities \"  ,    e )  ;", "throw   new   WebApplicationException ( e ,    Status . INTERNAL _ SERVER _ ERROR )  ;", "}", "}", "METHOD_END"], "methodName": ["postEntities"], "fileName": "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationId . newInstance ( clustertimestamp ,    Integer . valueOf ( id . toString (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convert"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newToken ( Token . class ,    identifier ,    kind ,    password ,    service )  ;", "}", "METHOD_END"], "methodName": ["newAMRMToken"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "AllocateResponse   response    =    BuilderUtils . recordFactory . newRecordInstance ( AllocateResponse . class )  ;", "response . setNumClusterNodes ( numClusterNodes )  ;", "response . setResponseId ( responseId )  ;", "response . setCompletedContainersStatuses ( completedContainers )  ;", "response . setAllocatedContainers ( allocatedContainers )  ;", "response . setUpdatedNodes ( updatedNodes )  ;", "response . setAvailableResources ( availResources )  ;", "response . setAMCommand ( command )  ;", "response . setPreemptionMessage ( preempt )  ;", "return   response ;", "}", "METHOD_END"], "methodName": ["newAllocateResponse"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationAttemptId . newInstance ( appId ,    attemptId )  ;", "}", "METHOD_END"], "methodName": ["newApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationId . newInstance ( clusterTimeStamp ,    id )  ;", "}", "METHOD_END"], "methodName": ["newApplicationId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationId . newInstance ( clusterTimeStamp ,    id )  ;", "}", "METHOD_END"], "methodName": ["newApplicationId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationId . newInstance ( clustertimestamp ,    Integer . valueOf ( id . toString (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["newApplicationId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationReport   report    =    BuilderUtils . recordFactory . newRecordInstance ( ApplicationReport . class )  ;", "report . setApplicationId ( applicationId )  ;", "report . setCurrentApplicationAttemptId ( applicationAttemptId )  ;", "report . setUser ( user )  ;", "report . setQueue ( queue )  ;", "report . setName ( name )  ;", "report . setHost ( host )  ;", "report . setRpcPort ( rpcPort )  ;", "report . setClientToAMToken ( clientToAMToken )  ;", "report . setYarnApplicationState ( state )  ;", "report . setDiagnostics ( diagnostics )  ;", "report . setTrackingUrl ( url )  ;", "report . setStartTime ( startTime )  ;", "report . setFinishTime ( finishTime )  ;", "report . setFinalApplicationStatus ( finalStatus )  ;", "report . setApplicationResourceUsageReport ( appResources )  ;", "report . setOriginalTrackingUrl ( origTrackingUrl )  ;", "report . setProgress ( progress )  ;", "report . setApplicationType ( appType )  ;", "report . setAMRMToken ( amRmToken )  ;", "report . setApplicationTags ( tags )  ;", "return   report ;", "}", "METHOD_END"], "methodName": ["newApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationResourceUsageReport   report    =    BuilderUtils . recordFactory . newRecordInstance ( ApplicationResourceUsageReport . class )  ;", "report . setNumUsedContainers ( numUsedContainers )  ;", "report . setNumReservedContainers ( numReservedContainers )  ;", "report . setUsedResources ( usedResources )  ;", "report . setReservedResources ( reservedResources )  ;", "report . setNeededResources ( neededResources )  ;", "return   report ;", "}", "METHOD_END"], "methodName": ["newApplicationResourceUsageReport"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newApplicationSubmissionContext ( applicationId ,    applicationName ,    queue ,    priority ,    amContainer ,    isUnmanagedAM ,    cancelTokensWhenComplete ,    maxAppAttempts ,    resource ,    null )  ;", "}", "METHOD_END"], "methodName": ["newApplicationSubmissionContext"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationSubmissionContext   context    =    BuilderUtils . recordFactory . newRecordInstance ( ApplicationSubmissionContext . class )  ;", "context . setApplicationId ( applicationId )  ;", "context . setApplicationName ( applicationName )  ;", "context . setQueue ( queue )  ;", "context . setPriority ( priority )  ;", "context . setAMContainerSpec ( amContainer )  ;", "context . setUnmanagedAM ( isUnmanagedAM )  ;", "context . setCancelTokensWhenComplete ( cancelTokensWhenComplete )  ;", "context . setMaxAppAttempts ( maxAppAttempts )  ;", "context . setResource ( resource )  ;", "context . setApplicationType ( applicationType )  ;", "return   context ;", "}", "METHOD_END"], "methodName": ["newApplicationSubmissionContext"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newToken ( Token . class ,    identifier ,    kind ,    password ,    service )  ;", "}", "METHOD_END"], "methodName": ["newClientToAMToken"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "Container   container    =    BuilderUtils . recordFactory . newRecordInstance ( Container . class )  ;", "container . setId ( containerId )  ;", "container . setNodeId ( nodeId )  ;", "container . setNodeHttpAddress ( nodeHttpAddress )  ;", "container . setResource ( resource )  ;", "container . setPriority ( priority )  ;", "container . setContainerToken ( containerToken )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["newContainer"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    BuilderUtils . newApplicationId ( timestamp ,    appId )  ;", "ApplicationAttemptId   applicationAttemptId    =    BuilderUtils . newApplicationAttemptId ( applicationId ,    appAttemptId )  ;", "ContainerId   cId    =    BuilderUtils . newContainerId ( applicationAttemptId ,    id )  ;", "return   cId ;", "}", "METHOD_END"], "methodName": ["newContainerId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   ContainerId . newInstance ( appAttemptId ,    containerId )  ;", "}", "METHOD_END"], "methodName": ["newContainerId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   ContainerId . newInstance ( appAttemptId ,    containerId )  ;", "}", "METHOD_END"], "methodName": ["newContainerId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerLaunchContext   container    =    BuilderUtils . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "container . setLocalResources ( localResources )  ;", "container . setEnvironment ( environment )  ;", "container . setCommands ( commands )  ;", "container . setServiceData ( serviceData )  ;", "container . setTokens ( tokens )  ;", "container . setApplicationACLs ( acls )  ;", "return   container ;", "}", "METHOD_END"], "methodName": ["newContainerLaunchContext"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerStatus   containerStatus    =    BuilderUtils . recordFactory . newRecordInstance ( ContainerStatus . class )  ;", "containerStatus . setState ( containerState )  ;", "containerStatus . setContainerId ( containerId )  ;", "containerStatus . setDiagnostics ( diagnostics )  ;", "containerStatus . setExitStatus ( exitStatus )  ;", "return   containerStatus ;", "}", "METHOD_END"], "methodName": ["newContainerStatus"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ContainerTokenIdentifier   identifier    =    new   ContainerTokenIdentifier ( cId ,     (  ( host    +     \"  :  \"  )     +    port )  ,    user ,    r ,    expiryTime ,    masterKeyId ,    rmIdentifier ,    Priority . newInstance (  0  )  ,     0  )  ;", "return    . newContainerToken (  . newNodeId ( host ,    port )  ,    password ,    identifier )  ;", "}", "METHOD_END"], "methodName": ["newContainerToken"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    NetUtils . createSocketAddrForHost ( nodeId . getHost (  )  ,    nodeId . getPort (  )  )  ;", "Token   containerToken    =     . newToken ( Token . class ,    tokenIdentifier . getBytes (  )  ,    KIND . toString (  )  ,    password ,    SecurityUtil . buildTokenService ( addr )  . toString (  )  )  ;", "return   containerToken ;", "}", "METHOD_END"], "methodName": ["newContainerToken"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "Token < ContainerTokenIdentifier >    token    =    new   Token < ContainerTokenIdentifier >  ( containerToken . getIdentifier (  )  . array (  )  ,    containerToken . getPassword (  )  . array (  )  ,    new   Text ( containerToken . getKind (  )  )  ,    new   Text ( containerToken . getService (  )  )  )  ;", "return   token . decodeIdentifier (  )  ;", "}", "METHOD_END"], "methodName": ["newContainerTokenIdentifier"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newToken ( Token . class ,    identifier ,    kind ,    password ,    service )  ;", "}", "METHOD_END"], "methodName": ["newDelegationToken"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   BuilderUtils . newLocalResource ( ConverterUtils . getYarnUrlFromURI ( uri )  ,    type ,    visibility ,    size ,    timestamp )  ;", "}", "METHOD_END"], "methodName": ["newLocalResource"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "LocalResource   resource    =    BuilderUtils . recordFactory . newRecordInstance ( LocalResource . class )  ;", "resource . setResource ( url )  ;", "resource . setType ( type )  ;", "resource . setVisibility ( visibility )  ;", "resource . setSize ( size )  ;", "resource . setTimestamp ( timestamp )  ;", "return   resource ;", "}", "METHOD_END"], "methodName": ["newLocalResource"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "return   NodeId . newInstance ( host ,    port )  ;", "}", "METHOD_END"], "methodName": ["newNodeId"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "NodeReport   nodeReport    =    BuilderUtils . recordFactory . newRecordInstance ( NodeReport . class )  ;", "nodeReport . setNodeId ( nodeId )  ;", "nodeReport . setNodeState ( nodeState )  ;", "nodeReport . setHttpAddress ( httpAddress )  ;", "nodeReport . setRackName ( rackName )  ;", "nodeReport . setUsed ( used )  ;", "nodeReport . setCapability ( capability )  ;", "nodeReport . setNumContainers ( numContainers )  ;", "nodeReport . setHealthReport ( healthReport )  ;", "nodeReport . setLastHealthReportTime ( lastHealthReportTime )  ;", "return   nodeReport ;", "}", "METHOD_END"], "methodName": ["newNodeReport"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "Priority   priority    =    BuilderUtils . recordFactory . newRecordInstance ( Priority . class )  ;", "priority . setPriority ( p )  ;", "return   priority ;", "}", "METHOD_END"], "methodName": ["newPriority"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "Resource   resource    =    BuilderUtils . recordFactory . newRecordInstance ( Resource . class )  ;", "resource . setMemory ( memory )  ;", "resource . setVirtualCores ( vCores )  ;", "return   resource ;", "}", "METHOD_END"], "methodName": ["newResource"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    BuilderUtils . recordFactory . newRecordInstance ( ResourceRequest . class )  ;", "request . setPriority ( priority )  ;", "request . setResourceName ( hostName )  ;", "request . setCapability ( capability )  ;", "request . setNumContainers ( numContainers )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["newResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ResourceRequest   request    =    BuilderUtils . recordFactory . newRecordInstance ( ResourceRequest . class )  ;", "request . setPriority ( r . getPriority (  )  )  ;", "request . setResourceName ( r . getResourceName (  )  )  ;", "request . setCapability ( r . getCapability (  )  )  ;", "request . setNumContainers ( r . getNumContainers (  )  )  ;", "return   request ;", "}", "METHOD_END"], "methodName": ["newResourceRequest"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "T   token    =    BuilderUtils . recordFactory . newRecordInstance ( tokenClass )  ;", "token . setIdentifier ( ByteBuffer . wrap ( identifier )  )  ;", "token . setKind ( kind )  ;", "token . setPassword ( ByteBuffer . wrap ( password )  )  ;", "token . setService ( service )  ;", "return   token ;", "}", "METHOD_END"], "methodName": ["newToken"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "URL   url    =    BuilderUtils . recordFactory . newRecordInstance ( URL . class )  ;", "url . setScheme ( scheme )  ;", "url . setHost ( host )  ;", "url . setPort ( port )  ;", "url . setFile ( file )  ;", "return   url ;", "}", "METHOD_END"], "methodName": ["newURL"], "fileName": "org.apache.hadoop.yarn.server.utils.BuilderUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   i . hasNext (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["hasNext"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   i . hasPrev (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["hasPrev"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   i . peekNext (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["peekNext"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   i . peekPrev (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["peekPrev"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   i . prev (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["prev"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "i . seek ( key )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["seek"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "i . seekToFirst (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["seekToFirst"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "try    {", "i . seekToLast (  )  ;", "}    catch    ( DBException   e )     {", "throw   e ;", "}    catch    ( RuntimeException   e )     {", "throw   new   DBException ( e . getMessage (  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["seekToLast"], "fileName": "org.apache.hadoop.yarn.server.utils.LeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "InvocationHandler   rtExcHandler    =    new   InvocationHandler (  )     {", "@ Override", "public   Object   invoke ( Object   proxy ,    Method   method ,    Object [  ]    args )    throws   Throwable    {", "throw   new   RuntimeException (  \" forced   runtime   error \"  )  ;", "}", "}  ;", "DBIterator   dbiter    =     (  ( DBIterator )     ( Proxy . newProxyInstance ( DBIterator . class . getClassLoader (  )  ,    new   Class [  ]  {    DBIterator . class    }  ,    rtExcHandler )  )  )  ;", "LeveldbIterator   iter    =    new   LeveldbIterator ( dbiter )  ;", "for    (  . CallInfo   ci    :     . RTEXC _ METHODS )     {", "Method   method    =    iter . getClass (  )  . getMethod ( ci . methodName ,    ci . argTypes )  ;", "assertNotNull (  (  \" unable   to   locate   method    \"     +     ( ci . methodName )  )  ,    method )  ;", "try    {", "method . invoke ( iter ,    ci . args )  ;", "fail (  \" operation   should   have   thrown \"  )  ;", "}    catch    ( InvocationTargetException   ite )     {", "Throwable   exc    =    ite . getTargetException (  )  ;", "assertTrue (  (  (  (  \" Method    \"     +     ( ci . methodName )  )     +     \"    threw   non - DBException :     \"  )     +    exc )  ,     ( exc   instanceof   DBException )  )  ;", "assertFalse (  (  (  \" Method    \"     +     ( ci . methodName )  )     +     \"    double - wrapped   DBException \"  )  ,     (  ( exc . getCause (  )  )    instanceof   DBException )  )  ;", "}", "}", "try    {", "iter . close (  )  ;", "fail (  \" operation   shoul   have   thrown \"  )  ;", "}    catch    ( IOException   e )     {", "}", "}", "METHOD_END"], "methodName": ["testExceptionHandling"], "fileName": "org.apache.hadoop.yarn.server.utils.TestLeveldbIterator"}, {"methodBody": ["METHOD_START", "{", "NodeHeartbeatResponse   response    =    YarnServerBuilderUtils . recordFactory . newRecordInstance ( NodeHeartbeatResponse . class )  ;", "response . setResponseId ( responseId )  ;", "response . setNodeAction ( action )  ;", "response . setContainerTokenMasterKey ( containerTokenMasterKey )  ;", "response . setNMTokenMasterKey ( nmTokenMasterKey )  ;", "response . setNextHeartBeatInterval ( nextHeartbeatInterval )  ;", "if    ( containersToCleanUp    !  =    null )     {", "response . addAllContainersToCleanup ( containersToCleanUp )  ;", "}", "if    ( applicationsToCleanUp    !  =    null )     {", "response . addAllApplicationsToCleanup ( applicationsToCleanUp )  ;", "}", "return   response ;", "}", "METHOD_END"], "methodName": ["newNodeHeartbeatResponse"], "fileName": "org.apache.hadoop.yarn.server.utils.YarnServerBuilderUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   id    =    WebServices . parseApplicationId ( appId )  ;", "ApplicationReport   app    =    null ;", "try    {", "app    =    appContext . getApplication ( id )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebApplicationException ( e )  ;", "}", "if    ( app    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   with   id :     \"     +    appId )     +     \"    not   found \"  )  )  ;", "}", "return   new   AppInfo ( app )  ;", "}", "METHOD_END"], "methodName": ["getApp"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   aid    =    WebServices . parseApplicationId ( appId )  ;", "ApplicationAttemptId   aaid    =    WebServices . parseApplicationAttemptId ( appAttemptId )  ;", "validateIds ( aid ,    aaid ,    null )  ;", "ApplicationAttemptReport   appAttempt    =    null ;", "try    {", "appAttempt    =    appContext . getApplicationAttempt ( aaid )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebApplicationException ( e )  ;", "}", "if    ( appAttempt    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" app   attempt   with   id :     \"     +    appAttemptId )     +     \"    not   found \"  )  )  ;", "}", "return   new   AppAttemptInfo ( appAttempt )  ;", "}", "METHOD_END"], "methodName": ["getAppAttempt"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   id    =    WebServices . parseApplicationId ( appId )  ;", "Collection < ApplicationAttemptReport >    appAttemptReports    =    null ;", "try    {", "appAttemptReports    =    appContext . getApplicationAttempts ( id )  . values (  )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebApplicationException ( e )  ;", "}", "AppAttemptsInfo   appAttemptsInfo    =    new   AppAttemptsInfo (  )  ;", "for    ( ApplicationAttemptReport   appAttemptReport    :    appAttemptReports )     {", "AppAttemptInfo   appAttemptInfo    =    new   AppAttemptInfo ( appAttemptReport )  ;", "appAttemptsInfo . add ( appAttemptInfo )  ;", "}", "return   appAttemptsInfo ;", "}", "METHOD_END"], "methodName": ["getAppAttempts"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "long   num    =     0  ;", "boolean   checkCount    =    false ;", "boolean   checkStart    =    false ;", "boolean   checkEnd    =    false ;", "boolean   checkAppTypes    =    false ;", "boolean   checkAppStates    =    false ;", "long   countNum    =     0  ;", "long   sBegin    =     0  ;", "long   sEnd    =    Long . MAX _ VALUE ;", "long   fBegin    =     0  ;", "long   fEnd    =    Long . MAX _ VALUE ;", "if    (  ( count    !  =    null )     &  &     (  !  ( count . isEmpty (  )  )  )  )     {", "checkCount    =    true ;", "countNum    =    Long . parseLong ( count )  ;", "if    ( countNum    <  =     0  )     {", "throw   new   BadRequestException (  \" limit   value   must   be   greater   then    0  \"  )  ;", "}", "}", "if    (  ( startedBegin    !  =    null )     &  &     (  !  ( startedBegin . isEmpty (  )  )  )  )     {", "checkStart    =    true ;", "sBegin    =    Long . parseLong ( startedBegin )  ;", "if    ( sBegin    <     0  )     {", "throw   new   BadRequestException (  \" startedTimeBegin   must   be   greater   than    0  \"  )  ;", "}", "}", "if    (  ( startedEnd    !  =    null )     &  &     (  !  ( startedEnd . isEmpty (  )  )  )  )     {", "checkStart    =    true ;", "sEnd    =    Long . parseLong ( startedEnd )  ;", "if    ( sEnd    <     0  )     {", "throw   new   BadRequestException (  \" startedTimeEnd   must   be   greater   than    0  \"  )  ;", "}", "}", "if    ( sBegin    >    sEnd )     {", "throw   new   BadRequestException (  \" startedTimeEnd   must   be   greater   than   startTimeBegin \"  )  ;", "}", "if    (  ( finishBegin    !  =    null )     &  &     (  !  ( finishBegin . isEmpty (  )  )  )  )     {", "checkEnd    =    true ;", "fBegin    =    Long . parseLong ( finishBegin )  ;", "if    ( fBegin    <     0  )     {", "throw   new   BadRequestException (  \" finishTimeBegin   must   be   greater   than    0  \"  )  ;", "}", "}", "if    (  ( finishEnd    !  =    null )     &  &     (  !  ( finishEnd . isEmpty (  )  )  )  )     {", "checkEnd    =    true ;", "fEnd    =    Long . parseLong ( finishEnd )  ;", "if    ( fEnd    <     0  )     {", "throw   new   BadRequestException (  \" finishTimeEnd   must   be   greater   than    0  \"  )  ;", "}", "}", "if    ( fBegin    >    fEnd )     {", "throw   new   BadRequestException (  \" finishTimeEnd   must   be   greater   than   finishTimeBegin \"  )  ;", "}", "Set < String >    appTypes    =     . parseQueries ( applicationTypes ,    false )  ;", "if    (  !  ( appTypes . isEmpty (  )  )  )     {", "checkAppTypes    =    true ;", "}", "if    (  ( stateQuery    !  =    null )     &  &     (  !  ( stateQuery . isEmpty (  )  )  )  )     {", "statesQuery . add ( stateQuery )  ;", "}", "Set < String >    appStates    =     . parseQueries ( statesQuery ,    true )  ;", "if    (  !  ( appStates . isEmpty (  )  )  )     {", "checkAppStates    =    true ;", "}", "AppsInfo   allApps    =    new   AppsInfo (  )  ;", "Collection < ApplicationReport >    appReports    =    null ;", "try    {", "appReports    =    appContext . getAllApplications (  )  . values (  )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebApplicationException ( e )  ;", "}", "for    ( ApplicationReport   appReport    :    appReports )     {", "if    ( checkCount    &  &     ( num    =  =    countNum )  )     {", "break ;", "}", "if    ( checkAppStates    &  &     (  !  ( appStates . contains ( appReport . getYarnApplicationState (  )  . toString (  )  . toLowerCase (  )  )  )  )  )     {", "continue ;", "}", "if    (  ( finalStatusQuery    !  =    null )     &  &     (  !  ( finalStatusQuery . isEmpty (  )  )  )  )     {", "FinalApplicationStatus . valueOf ( finalStatusQuery )  ;", "if    (  !  ( appReport . getFinalApplicationStatus (  )  . toString (  )  . equalsIgnoreCase ( finalStatusQuery )  )  )     {", "continue ;", "}", "}", "if    (  ( userQuery    !  =    null )     &  &     (  !  ( userQuery . isEmpty (  )  )  )  )     {", "if    (  !  ( appReport . getUser (  )  . equals ( userQuery )  )  )     {", "continue ;", "}", "}", "if    (  ( queueQuery    !  =    null )     &  &     (  !  ( queueQuery . isEmpty (  )  )  )  )     {", "if    (  !  ( appReport . getQueue (  )  . equals ( queueQuery )  )  )     {", "continue ;", "}", "}", "if    ( checkAppTypes    &  &     (  !  ( appTypes . contains ( appReport . getApplicationType (  )  . trim (  )  . toLowerCase (  )  )  )  )  )     {", "continue ;", "}", "if    ( checkStart    &  &     (  (  ( appReport . getStartTime (  )  )     <    sBegin )     |  |     (  ( appReport . getStartTime (  )  )     >    sEnd )  )  )     {", "continue ;", "}", "if    ( checkEnd    &  &     (  (  ( appReport . getFinishTime (  )  )     <    fBegin )     |  |     (  ( appReport . getFinishTime (  )  )     >    fEnd )  )  )     {", "continue ;", "}", "AppInfo   app    =    new   AppInfo ( appReport )  ;", "allApps . add ( app )  ;", "num +  +  ;", "}", "return   allApps ;", "}", "METHOD_END"], "methodName": ["getApps"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   aid    =    WebServices . parseApplicationId ( appId )  ;", "ApplicationAttemptId   aaid    =    WebServices . parseApplicationAttemptId ( appAttemptId )  ;", "ContainerId   cid    =    WebServices . parseContainerId ( containerId )  ;", "validateIds ( aid ,    aaid ,    cid )  ;", "ContainerReport   container    =    null ;", "try    {", "container    =    appContext . getContainer ( cid )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebApplicationException ( e )  ;", "}", "if    ( container    =  =    null )     {", "throw   new   NotFoundException (  (  (  \" container   with   id :     \"     +    containerId )     +     \"    not   found \"  )  )  ;", "}", "return   new   ContainerInfo ( container )  ;", "}", "METHOD_END"], "methodName": ["getContainer"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   aid    =    WebServices . parseApplicationId ( appId )  ;", "ApplicationAttemptId   aaid    =    WebServices . parseApplicationAttemptId ( appAttemptId )  ;", "validateIds ( aid ,    aaid ,    null )  ;", "Collection < ContainerReport >    containerReports    =    null ;", "try    {", "containerReports    =    appContext . getContainers ( aaid )  . values (  )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebApplicationException ( e )  ;", "}", "ContainersInfo   containersInfo    =    new   ContainersInfo (  )  ;", "for    ( ContainerReport   containerReport    :    containerReports )     {", "ContainerInfo   containerInfo    =    new   ContainerInfo ( containerReport )  ;", "containersInfo . add ( containerInfo )  ;", "}", "return   containersInfo ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "response . setContentType ( null )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( appAttemptId    =  =    null )     |  |     ( appAttemptId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" appAttemptId ,     \"     +    appAttemptId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ApplicationAttemptId   aaid    =    ConverterUtils . toApplicationAttemptId ( appAttemptId )  ;", "if    ( aaid    =  =    null )     {", "throw   new   NotFoundException (  \" appAttemptId   is   null \"  )  ;", "}", "return   aaid ;", "}", "METHOD_END"], "methodName": ["parseApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( appId    =  =    null )     |  |     ( appId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" appId ,     \"     +    appId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ApplicationId   aid    =    ConverterUtils . toApplicationId ( appId )  ;", "if    ( aid    =  =    null )     {", "throw   new   NotFoundException (  \" appId   is   null \"  )  ;", "}", "return   aid ;", "}", "METHOD_END"], "methodName": ["parseApplicationId"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  ( containerId    =  =    null )     |  |     ( containerId . isEmpty (  )  )  )     {", "throw   new   NotFoundException (  (  (  \" containerId ,     \"     +    containerId )     +     \"  ,    is   empty   or   null \"  )  )  ;", "}", "ContainerId   cid    =    ConterUtils . toContainerId ( containerId )  ;", "if    ( cid    =  =    null )     {", "throw   new   NotFoundException (  \" containerId   is   null \"  )  ;", "}", "return   cid ;", "}", "METHOD_END"], "methodName": ["parseContainerId"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "Set < String >    params    =    new   HashSet < String >  (  )  ;", "if    (  !  ( queries . isEmpty (  )  )  )     {", "for    ( String   query    :    queries )     {", "if    (  ( query    !  =    null )     &  &     (  !  ( query . trim (  )  . isEmpty (  )  )  )  )     {", "String [  ]    paramStrs    =    query . split (  \"  ,  \"  )  ;", "for    ( String   paramStr    :    paramStrs )     {", "if    (  ( paramStr    !  =    null )     &  &     (  !  ( paramStr . trim (  )  . isEmpty (  )  )  )  )     {", "if    ( isState )     {", "try    {", "YApplicationState . valueOf ( paramStr . trim (  )  . toUpperCase (  )  )  ;", "}    catch    ( RuntimeException   e )     {", "YApplicationState [  ]    stateArray    =    YApplicationState . values (  )  ;", "String   allAppStates    =    Arrays . toString ( stateArray )  ;", "throw   new   BadRequestException (  (  (  (  \" Invalid   application - state    \"     +     ( paramStr . trim (  )  )  )     +     \"    specified .    It   should   be   one   of    \"  )     +    allAppStates )  )  ;", "}", "}", "params . add ( paramStr . trim (  )  . toLowerCase (  )  )  ;", "}", "}", "}", "}", "}", "return   params ;", "}", "METHOD_END"], "methodName": ["parseQueries"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( appAttemptId . getApplicationId (  )  . equals ( appId )  )  )     {", "throw   new   NotFoundException (  \" appId   and   appAttemptId   don ' t   match \"  )  ;", "}", "if    (  ( containerId    !  =    null )     &  &     (  !  ( containerId . getApplicationAttemptId (  )  . equals ( appAttemptId )  )  )  )     {", "throw   new   NotFoundException (  \" appAttemptId   and   containerId   don ' t   match \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["validateIds"], "fileName": "org.apache.hadoop.yarn.server.webapp.WebServices"}, {"methodBody": ["METHOD_START", "{", "return   amContainerId ;", "}", "METHOD_END"], "methodName": ["getAmContainerId"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptId ;", "}", "METHOD_END"], "methodName": ["getAppAttemptId"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptState ;", "}", "METHOD_END"], "methodName": ["getAppAttemptState"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   rpcPort ;", "}", "METHOD_END"], "methodName": ["getRpcPort"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "return   trackingUrl ;", "}", "METHOD_END"], "methodName": ["getTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptInfo"}, {"methodBody": ["METHOD_START", "{", "this . attempt . add ( info )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptsInfo"}, {"methodBody": ["METHOD_START", "{", "return   this . attempt ;", "}", "METHOD_END"], "methodName": ["getAttempts"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppAttemptsInfo"}, {"methodBody": ["METHOD_START", "{", "return   allocatedMB ;", "}", "METHOD_END"], "methodName": ["getAllocatedMB"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   allocatedVCores ;", "}", "METHOD_END"], "methodName": ["getAllocatedVCores"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   appId ;", "}", "METHOD_END"], "methodName": ["getAppId"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   appState ;", "}", "METHOD_END"], "methodName": ["getAppState"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   currentAppAttemptId ;", "}", "METHOD_END"], "methodName": ["getCurrentAppAttemptId"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   elapsedTime ;", "}", "METHOD_END"], "methodName": ["getElapsedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   finalAppStatus ;", "}", "METHOD_END"], "methodName": ["getFinalAppStatus"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   finishedTime ;", "}", "METHOD_END"], "methodName": ["getFinishedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   host ;", "}", "METHOD_END"], "methodName": ["getHost"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   originalTrackingUrl ;", "}", "METHOD_END"], "methodName": ["getOriginalTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   progress ;", "}", "METHOD_END"], "methodName": ["getProgress"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   queue ;", "}", "METHOD_END"], "methodName": ["getQueue"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   rpcPort ;", "}", "METHOD_END"], "methodName": ["getRpcPort"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   startedTime ;", "}", "METHOD_END"], "methodName": ["getStartedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   submittedTime ;", "}", "METHOD_END"], "methodName": ["getSubmittedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   trackingUrl ;", "}", "METHOD_END"], "methodName": ["getTrackingUrl"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   type ;", "}", "METHOD_END"], "methodName": ["getType"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "return   user ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppInfo"}, {"methodBody": ["METHOD_START", "{", "app . add ( appinfo )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppsInfo"}, {"methodBody": ["METHOD_START", "{", "return   app ;", "}", "METHOD_END"], "methodName": ["getApps"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.AppsInfo"}, {"methodBody": ["METHOD_START", "{", "return   allocatedMB ;", "}", "METHOD_END"], "methodName": ["getAllocatedMB"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   allocatedVCores ;", "}", "METHOD_END"], "methodName": ["getAllocatedVCores"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   assignedNodeId ;", "}", "METHOD_END"], "methodName": ["getAssignedNodeId"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   containerExitStatus ;", "}", "METHOD_END"], "methodName": ["getContainerExitStatus"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   containerId ;", "}", "METHOD_END"], "methodName": ["getContainerId"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   containerState ;", "}", "METHOD_END"], "methodName": ["getContainerState"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   diagnosticsInfo ;", "}", "METHOD_END"], "methodName": ["getDiagnosticsInfo"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   elapsedTime ;", "}", "METHOD_END"], "methodName": ["getElapsedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   finishedTime ;", "}", "METHOD_END"], "methodName": ["getFinishedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   logUrl ;", "}", "METHOD_END"], "methodName": ["getLogUrl"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "return   startedTime ;", "}", "METHOD_END"], "methodName": ["getStartedTime"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainerInfo"}, {"methodBody": ["METHOD_START", "{", "container . add ( containerInfo )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainersInfo"}, {"methodBody": ["METHOD_START", "{", "return   container ;", "}", "METHOD_END"], "methodName": ["getContainers"], "fileName": "org.apache.hadoop.yarn.server.webapp.dao.ContainersInfo"}, {"methodBody": ["METHOD_START", "{", "GetApplicationReportRequest   request    =    recordFactory . newRecordInstance ( GetApplicationReportRequest . class )  ;", "request . setApplicationId ( appId )  ;", "GetApplicationReportResponse   response    =    applicationsManager . getApplicationReport ( request )  ;", "return   response . getApplicationReport (  )  ;", "}", "METHOD_END"], "methodName": ["getApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.webproxy.AppReportFetcher"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationsManager )     !  =    null )     {", "RPC . stopP ( this . applicationsManager )  ;", "}", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.webproxy.AppReportFetcher"}, {"methodBody": ["METHOD_START", "{", "if    (  ( qu    !  =    null )     &  &     (  !  ( qu . isEmpty (  )  )  )  )     {", "if    ( first    &  &     (  !  ( qu . startsWith (  \"  ?  \"  )  )  )  )     {", "buildappend (  '  ?  '  )  ;", "}", "if    (  (  ! first )     &  &     (  !  ( qu . startsWith (  \"  &  \"  )  )  )  )     {", "buildappend (  '  &  '  )  ;", "}", "buildappend ( qu )  ;", "return   false ;", "}", "return   first ;", "}", "METHOD_END"], "methodName": ["appendQuery"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( id    =  =    null )     {", "throw   new   IllegalArgumentException (  \" Application   id   cannot   be   null    \"  )  ;", "}", "return   ujoin (  . PROXY _ BASE ,     . uriEncode ( id )  )  ;", "}", "METHOD_END"], "methodName": ["getPath"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( path    =  =    null )     {", "return    . getPath ( id )  ;", "} else    {", "return   ujoin (  . getPath ( id )  ,    path )  ;", "}", "}", "METHOD_END"], "methodName": ["getPath"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   newp    =    new   StringBuilder (  )  ;", "newp . append (  . getPath ( id ,    path )  )  ;", "boolean   first    =     . appendQuery ( newp ,    query ,    true )  ;", "if    ( approved )     {", ". appendQuery ( newp ,     (  (  . PROXY _ APPROVAL _ PARAM )     +     \"  = true \"  )  ,    first )  ;", "}", "return   newp . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getPathAndQuery"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "String   path    =     . getPath ( id ,     ( originalUri    =  =    null    ?     \"  /  \"     :    originalUri . getPath (  )  )  )  ;", "return   new   URI ( proxyUri . getScheme (  )  ,    proxyUri . getAuthority (  )  ,    path ,     ( originalUri    =  =    null    ?    null    :    originalUri . getQuery (  )  )  ,     ( originalUri    =  =    null    ?    null    :    originalUri . getFragment (  )  )  )  ;", "}    catch    ( URISyntaxException   e )     {", "throw   new   RuntimeException (  (  \" Could   not   proxify    \"     +    originalUri )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["getProxyUri"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "int   index    =     0  ;", "if    ( url    !  =    null )     {", "index    =    url . indexOf (  \"  :  /  /  \"  )  ;", "}", "if    ( index    >     0  )     {", "retu   url . substring (  0  ,    index )  ;", "} else    {", "retu    \"  \"  ;", "}", "}", "METHOD_END"], "methodName": ["getSchemeFromUrl"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( ProxyUriUtils . getSchemeFromUrl ( noSchemeUrl )  . isEmpty (  )  )     {", "return   new   URI (  ( scheme    +    noSchemeUrl )  )  ;", "} else    {", "return   new   URI ( noSchemeUrl )  ;", "}", "}", "METHOD_END"], "methodName": ["getUriFromAMUrl"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "URI   toRet    =    null ;", "for    ( TrackingPlugin   plugin    :    trackingPlugins )     {", "toRet    =    plugin . getTracking ( id )  ;", "if    ( toRet    !  =    null )     {", "return   toRet ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getUriFromTrackingPlugins"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "try    {", "ast   o    !  =    null    :     \" o   canot   be   null \"  ;", "return   URLEncoder . encode ( o . toString (  )  ,     \" UTF -  8  \"  )  ;", "}    catch    ( UnsupportedEncodingException   e )     {", "throw   new   RuntimeException (  \" UTF -  8    is   not   supported   by   this   system ?  \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["uriEncode"], "fileName": "org.apache.hadoop.yarn.server.webproxy.ProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  / static / app ? foo = bar \"  ,    ProxyUriUtils . getPathAndQuery ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ,     \"  / static / app \"  ,     \"  ? foo = bar \"  ,    false )  )  ;", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  / static / app ? foo = bar & bad = good & proxyapproved = true \"  ,    ProxyUriUtils . getPathAndQuery ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ,     \"  / static / app \"  ,     \" foo = bar & bad = good \"  ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["testGetPathAndQuery"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \"  / proxy / application _  1  0  0  _  0  0  0  1  \"  ,    ProxyUriUtils . getPath ( BuilderUtils . newApplicationId (  1  0  0 L ,     1  )  )  )  ;", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  \"  ,    ProxyUriUtils . getPath ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetPathApplicationId"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "ProxyUriUtils . getPath ( null )  ;", "}", "METHOD_END"], "methodName": ["testGetPathApplicationIdBad"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  \"  ,    ProxyUriUtils . getPath ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ,    null )  )  ;", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  / static / app \"  ,    ProxyUriUtils . getPath ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ,     \"  / static / app \"  )  )  ;", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  /  \"  ,    ProxyUriUtils . getPath ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ,     \"  /  \"  )  )  ;", "assertEquals (  \"  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  / some / path \"  ,    ProxyUriUtils . getPath ( BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ,     \" some / path \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetPathApplicationIdString"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "URI   originalUri    =    new   URI (  \" http :  /  / host . com / static / foo ? bar = bar \"  )  ;", "URI   proxyUri    =    new   URI (  \" http :  /  / proxy . net :  8  0  8  0  /  \"  )  ;", "ApplicationId   id    =    BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ;", "URI   expected    =    new   URI (  \" http :  /  / proxy . net :  8  0  8  0  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  / static / foo ? bar = bar \"  )  ;", "URI   result    =     . getProxyUri ( originalUri ,    proxyUri ,    id )  ;", "assertEquals ( expected ,    result )  ;", "}", "METHOD_END"], "methodName": ["testGetProxyUri"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   id    =    BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ;", "List < TrackingUriPlugin >    list    =    Lists . newArrayListWithExpectedSize (  0  )  ;", "assertNull (  . getUriFromTrackingPlugins ( id ,    list )  )  ;", "}", "METHOD_END"], "methodName": ["testGetProxyUriFromPluginsReturnsNullIfNoPlugins"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   id    =    BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ;", "List < TrackingUriPlugin >    list    =    Lists . newArrayListWithExpectedSize (  2  )  ;", "list . add ( new   TrackingUriPlugin (  )     {", "public   URI   getTrackingUri ( ApplicationId   id )    throws   URISyntaxException    {", "return   null ;", "}", "}  )  ;", "list . add ( new   TrackingUriPlugin (  )     {", "public   URI   getTrackingUri ( ApplicationId   id )    throws   URISyntaxException    {", "return   new   URI (  \" http :  /  / history . server . net /  \"  )  ;", "}", "}  )  ;", "URI   result    =     . getUriFromTrackingPlugins ( id ,    list )  ;", "assertNotNull ( result )  ;", "}", "METHOD_END"], "methodName": ["testGetProxyUriFromPluginsReturnsValidUriWhenAble"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "URI   originalUri    =    null ;", "URI   proxyUri    =    new   URI (  \" http :  /  / proxy . net :  8  0  8  0  /  \"  )  ;", "ApplicationId   id    =    BuilderUtils . newApplicationId (  6  3  8  4  6  2  3 L ,     5  )  ;", "URI   expected    =    new   URI (  \" http :  /  / proxy . net :  8  0  8  0  / proxy / application _  6  3  8  4  6  2  3  _  0  0  0  5  /  \"  )  ;", "URI   result    =     . getProxyUri ( originalUri ,    proxyUri ,    id )  ;", "assertEquals ( expected ,    result )  ;", "}", "METHOD_END"], "methodName": ["testGetProxyUriNull"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestProxyUriUtils"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( PROXY _ ADDRESS ,    proxyAddress )  ;", "webAppProxy    =    new    (  )  ;", "webAppProxy . init ( conf )  ;", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "webAppProxy . stop (  )  ;", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "InetSocketAddress   defaultBindAddress    =     . getBindAddress ( conf )  ;", "Assert . assertEquals (  \" Web   Proxy   default   bind   address   port   is   incorrect \"  ,    DEFAULT _ PROXY _ PORT ,    defaultBindAddress . getPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBindAddress"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( INITED ,    webAppProxy . getServiceState (  )  )  ;", "webAppProxy . start (  )  ;", "for    ( Service   service    :    webAppProxy . getServices (  )  )     {", "if    ( service   instanceof    )     {", "assertEquals (  (  (  )     ( service )  )  . getBindAddress (  )  ,    proxyAddress )  ;", "}", "}", "assertEquals ( STARTED ,    webAppProxy . getServiceState (  )  )  ;", "}", "METHOD_END"], "methodName": ["testStart"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    List < String >  >    headerFields    =    proxyConn . getHeaderFields (  )  ;", "List < String >    cookiesHeader    =    headerFields . get (  \" Set - Cookie \"  )  ;", "if    ( cookiesHeader    !  =    null )     {", "for    ( String   cookie    :    cookiesHeader )     {", "HttpCookie   c    =    HttpCookie . parse ( cookie )  . get (  0  )  ;", "if    (  ( c . getName (  )  . equals ( expectedName )  )     &  &     ( c . getValue (  )  . equals ( expectedValue )  )  )     {", "return   true ;", "}", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isResponseCookiePresent"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "byte [  ]    buff    =    new   byte [  5  1  2  ]  ;", "int   read ;", "while    (  ( read    =    input . read ( buff )  )     >  =     0  )     {", "data . write ( buff ,     0  ,    read )  ;", "}", "return   new   String ( data . toByteArray (  )  ,     \" UTF -  8  \"  )  ;", "}", "METHOD_END"], "methodName": ["readInputStream"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "TestWebAppProxyServlet . server    =    new   Server (  0  )  ;", "Context   context    =    new   Context (  )  ;", "context . setContextPath (  \"  / foo \"  )  ;", "TestWebAppProxyServlet . server . setHandler ( context )  ;", "context . addServlet ( new   ServletHolder ( TestWebAppProxyServlet . TestServlet . class )  ,     \"  / bar /  \"  )  ;", "TestWebAppProxyServlet . server . getConnectors (  )  [  0  ]  . setHost (  \" localhost \"  )  ;", "TestWebAppProxyServlet . server . start (  )  ;", "TestWebAppProxyServlet . originalPort    =    TestWebAppProxyServlet . server . getConnectors (  )  [  0  ]  . getLocalPort (  )  ;", "TestWebAppProxyServlet . LOG . info (  (  \" Running   embedded   servlet   container   at :    http :  /  / localhost :  \"     +     ( TestWebAppProxyServlet . originalPort )  )  )  ;", "}", "METHOD_END"], "methodName": ["start"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "try    {", ". server . stop (  )  ;", "}    catch    ( Exception   e )     {", "}", "try    {", ". server . destroy (  )  ;", "}    catch    ( Exception   e )     {", "}", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "WebAppProxyServer   mainServer    =    null ;", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( PROXY _ ADDRESS ,     \" localhost :  9  0  9  9  \"  )  ;", "try    {", "mainServer    =    WebAppProxyServer . startServer ( conf )  ;", "int   counter    =     2  0  ;", "URL   wrongUrl    =    new   URL (  \" http :  /  / localhost :  9  0  9  9  / proxy / app \"  )  ;", "HttpURLConnection   proxyConn    =    null ;", "while    ( counter    >     0  )     {", "counter -  -  ;", "try    {", "proxyConn    =     (  ( HttpURLConnection )     ( wrongUrl . openConnection (  )  )  )  ;", "proxyConn . connect (  )  ;", "proxyConn . getResponseCode (  )  ;", "counter    =     0  ;", "}    catch    ( Exception   e )     {", "Thread . sleep (  1  0  0  )  ;", "}", "}", "assertNotNull ( proxyConn )  ;", "assertEquals ( HttpURLConnection . HTTP _ INTERNAL _ ERROR ,    proxyConn . getResponseCode (  )  )  ;", "}    finally    {", "if    ( mainServer    !  =    null )     {", "mainServer . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testWebAppProxyServerMainMethod"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "Configuration   configuration    =    new   Configuration (  )  ;", "configuration . set ( PROXY _ ADDRESS ,     \" localhost :  9  0  9  0  \"  )  ;", "configuration . setInt (  \" hadoop . http . max . threads \"  ,     5  )  ;", ". WebAppProxyServerForTest   proxy    =    new    . WebAppProxyServerForTest (  )  ;", "proxy . init ( configuration )  ;", "proxy . start (  )  ;", "int   proxyPort    =    proxy . proxy . proxyServer . getConnectorAddress (  0  )  . getPort (  )  ;", ". AppReportFetcherForTest   appReportFetcher    =    proxy . proxy . appReportFetcher ;", "try    {", "URL   wrongUrl    =    new   URL (  (  (  \" http :  /  / localhost :  \"     +    proxyPort )     +     \"  / proxy / app \"  )  )  ;", "HttpURLConnection   proxyConn    =     (  ( HttpURLConnection )     ( wrongUrl . openConnection (  )  )  )  ;", "proxyConn . connect (  )  ;", "assertEquals ( HttpURLConnection . HTTP _ INTERNAL _ ERROR ,    proxyConn . getResponseCode (  )  )  ;", "URL   url    =    new   URL (  (  (  \" http :  /  / localhost :  \"     +    proxyPort )     +     \"  / proxy / application _  0  0  _  0  \"  )  )  ;", "proxyConn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "proxyConn . setRequestProperty (  \" Cookie \"  ,     \" checked _ application _  0  _  0  0  0  0  = true \"  )  ;", "proxyConn . connect (  )  ;", "assertEquals ( HttpURLConnection . HTTP _ OK ,    proxyConn . getResponseCode (  )  )  ;", "assertTrue ( isResponseCookiePresent ( proxyConn ,     \" checked _ application _  0  _  0  0  0  0  \"  ,     \" true \"  )  )  ;", "appReportFetcher . answer    =     1  ;", "proxyConn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "proxyConn . setRequestProperty (  \" Cookie \"  ,     \" checked _ application _  0  _  0  0  0  0  = true \"  )  ;", "proxyConn . connect (  )  ;", "assertEquals ( HttpURLConnection . HTTP _ NOT _ FOUND ,    proxyConn . getResponseCode (  )  )  ;", "assertFalse ( isResponseCookiePresent ( proxyConn ,     \" checked _ application _  0  _  0  0  0  0  \"  ,     \" true \"  )  )  ;", "appReportFetcher . answer    =     4  ;", "proxyConn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "proxyConn . setRequestProperty (  \" Cookie \"  ,     \" checked _ application _  0  _  0  0  0  0  = true \"  )  ;", "proxyConn . connect (  )  ;", "assertEquals ( HttpURLConnection . HTTP _ NOT _ FOUND ,    proxyConn . getResponseCode (  )  )  ;", "assertFalse ( isResponseCookiePresent ( proxyConn ,     \" checked _ application _  0  _  0  0  0  0  \"  ,     \" true \"  )  )  ;", "appReportFetcher . answer    =     2  ;", "proxyConn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "proxyConn . connect (  )  ;", "assertEquals ( HttpURLConnection . HTTP _ OK ,    proxyConn . getResponseCode (  )  )  ;", "String   s    =    readInputStream ( proxyConn . getInputStream (  )  )  ;", "assertTrue ( s . contains (  \" to   continue   to   an   Application   Master   web   interface   owned   by \"  )  )  ;", "assertTrue ( s . contains (  \" WARNING :    The   following   page   may   not   be   safe !  \"  )  )  ;", "appReportFetcher . answer    =     3  ;", "proxyConn    =     (  ( HttpURLConnection )     ( url . openConnection (  )  )  )  ;", "proxyConn . setRequestProperty (  \" Cookie \"  ,     \" checked _ application _  0  _  0  0  0  0  = true \"  )  ;", "proxyConn . connect (  )  ;", "assertEquals ( HttpURLConnection . HTTP _ OK ,    proxyConn . getResponseCode (  )  )  ;", "}    finally    {", "proxy . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testWebAppProxyServlet"], "fileName": "org.apache.hadoop.yarn.server.webproxy.TestWebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "return    (  ( bindAddress )     +     \"  :  \"  )     +     ( port )  ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxy"}, {"methodBody": ["METHOD_START", "{", "if    (  ( proxyS    !  =    null )     {", "try    {", "proxySjoin (  )  ;", "}    catch    ( InterruptedException   e )     {", "}", "}", "}", "METHOD_END"], "methodName": ["join"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxy"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   socAddr    =    WebAppProxyServer . getBindAddress ( conf )  ;", "SecurityUtil . login ( conf ,    PROXY _ KEYTAB ,    PROXY _ PRINCIPAL ,    socAddr . getHostName (  )  )  ;", "}", "METHOD_END"], "methodName": ["doSecureLogin"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "return   conf . getSocketAddr ( PROXY _ ADDRESS ,    DEFAULT _ PROXY _ ADDRESS ,    DEFAULT _ PROXY _ PORT )  ;", "}", "METHOD_END"], "methodName": ["getBindAddress"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "Thread . setDefaultUncaughtExceptionHandler ( new   YarnUncaughtExceptionHandler (  )  )  ;", "StringUtils . startupShutdownMessage (  . class ,    args ,     . LOG )  ;", "try    {", "YarnConfiguration   configuration    =    new   YarnConfiguration (  )  ;", "proxyServer    =     . startServer ( configuration )  ;", "proxyServer . proxy . join (  )  ;", "}    catch    ( Throwable   t )     {", ". LOG . fatal (  \" Error   starting   Proxy   server \"  ,    t )  ;", "System . exit (  (  -  1  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "WebAppProxyServer   proxy    =    new   WebAppProxyServer (  )  ;", "ShutdownHookManager . get (  )  . addShutdownHook ( new   CompositeServiceShutdownHook ( proxy )  ,    WebAppProxyServer . SHUTDOWN _ HOOK _ PRIORITY )  ;", "proxy . init ( configuration )  ;", "proxy . start (  )  ;", "return   proxy ;", "}", "METHOD_END"], "methodName": ["startServer"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer"}, {"methodBody": ["METHOD_START", "{", "return    (  ( AppReportFetcher )     ( getServletContext (  )  . getAttribute ( WebAppProxy . FETCHER _ ATTRIBUTE )  )  )  . getApplicationReport ( id )  ;", "}", "METHOD_END"], "methodName": ["getApplicationReport"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "return    \" checked _  \"     +    id ;", "}", "METHOD_END"], "methodName": ["getCheckCookieName"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "return    (  ( String )     ( getServletContext (  )  . getAttribute ( WebAppProxy . PROXY _ HOST _ ATTRIBUTE )  )  )  ;", "}", "METHOD_END"], "methodName": ["getProxyHost"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "Boolean   b    =     (  ( Boolean )     ( getServletContext (  )  . getAttribute ( WebAppProxy . IS _ SECURITY _ ENABLED _ ATTRIBUTE )  )  )  ;", "if    ( b    !  =    null )", "return   b ;", "return   false ;", "}", "METHOD_END"], "methodName": ["isSecurityEnabled"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "Cookie   c    =    new   Cookie ( WebAppProxyServlet . getCheckCookieName ( id )  ,    String . valueOf ( isSet )  )  ;", "c . setPath ( ProxyUriUtils . getPath ( id )  )  ;", "c . setMaxAge (  (  (  6  0     *     6  0  )     *     2  )  )  ;", "return   c ;", "}", "METHOD_END"], "methodName": ["makeCheckCookie"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "resp . setStatus ( SC _ NOT _ FOUND )  ;", "resp . setContentType ( HTML )  ;", ". Page   p    =    new    . Page ( resp . getWriter (  )  )  ;", "p . html (  )  . h 1  ( message )  .  _  (  )  ;", "}", "METHOD_END"], "methodName": ["notFound"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "URI   uri    =    new   URI ( link . toString (  )  ,    false )  ;", "HttpClientParams   params    =    new   HttpClientParams (  )  ;", "params . setCookiePolicy ( BROWSER _ COMPATIBILITY )  ;", "params . setBooleanParameter ( ALLOW _ CIRCULAR _ REDIRECTS ,    true )  ;", "HttpClient   client    =    new   HttpClient ( params )  ;", "HostConfiguration   config    =    new   HostConfiguration (  )  ;", "InetAddress   localAddress    =    InetAddress . getByName ( proxyHost )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" local   InetAddress   for   proxy   host :     \"     +     ( localAddress . toString (  )  )  )  )  ;", "}", "config . setLocalAddress ( localAddress )  ;", "HttpMethod   method    =    new   GetMethod ( uri . getEscapedURI (  )  )  ;", "@ SuppressWarnings (  \" unchecked \"  )", "Enumeration < String >    names    =    req . getHeaderNames (  )  ;", "while    ( names . hasMoreElements (  )  )     {", "String   name    =    names . nextElement (  )  ;", "if    (  . passThroughHeaders . contains ( name )  )     {", "String   value    =    req . getHeader ( name )  ;", ". LOG . debug (  (  (  (  \" REQ   HEADER :     \"     +    name )     +     \"     :     \"  )     +    value )  )  ;", "method . setRequestHeader ( name ,    value )  ;", "}", "}", "String   user    =    req . getRemoteUser (  )  ;", "if    (  ( user    !  =    null )     &  &     (  !  ( user . isEmpty (  )  )  )  )     {", "method . setRequestHeader (  \" Cookie \"  ,     (  (  (  . PROXY _ USER _ COOKIE _ NAME )     +     \"  =  \"  )     +     ( URLEncoder . encode ( user ,     \" ASCII \"  )  )  )  )  ;", "}", "OutputStream   out    =    resp . getOutputStream (  )  ;", "try    {", "resp . setStatus ( client . executeMethod ( config ,    method )  )  ;", "for    ( Header   header    :    method . getResponseHeaders (  )  )     {", "resp . setHeader ( header . getName (  )  ,    header . getValue (  )  )  ;", "}", "if    ( c    !  =    null )     {", "resp . addCookie ( c )  ;", "}", "InputStream   in    =    method . getResponseBodyAsStream (  )  ;", "if    ( in    !  =    null )     {", "IOUtils . copyBytes ( in ,    out ,     4  0  9  6  ,    true )  ;", "}", "}    finally    {", "method . releaseConnection (  )  ;", "}", "}", "METHOD_END"], "methodName": ["proxyLink"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "resp . addCookie ( WebAppProxyServlet . makeCheckCookie ( id ,    false )  )  ;", "resp . setContentType ( HTML )  ;", "WebAppProxyServlet . Page   p    =    new   WebAppProxyServlet . Page ( resp . getWriter (  )  )  ;", "p . html (  )  . h 1  (  \" WARNING :    The   following   page   may   not   be   safe !  \"  )  . h 3  (  )  .  _  (  \" click    \"  )  . a ( link ,     \" here \"  )  .  _  (  \"    to   continue   to   an   Application   Master   web   interface   owned   by    \"  ,    user )  .  _  (  )  .  _  (  )  ;", "}", "METHOD_END"], "methodName": ["warnUserPage"], "fileName": "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"}, {"methodBody": ["METHOD_START", "{", "return   System . getenv ( APPLICATION _ WEB _ PROXY _ BASE _ ENV )  ;", "}", "METHOD_END"], "methodName": ["getApplicationWebProxyBase"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer"}, {"methodBody": ["METHOD_START", "{", "String   addr ;", "if    (  ( proxyUriBases . size (  )  )     =  =     1  )     {", "addr    =    proxyUriBases . values (  )  . iterator (  )  . next (  )  ;", "} else    {", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "String   activeRMId    =    RMHAUtils . findActiveRMHAId ( conf )  ;", "String   addressPropertyPrefix    =     ( YarnConfiguration . useHttps ( conf )  )     ?    YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS    :    YarnConfiguration . RM _ WEBAPP _ ADDRESS ;", "String   host    =    conf . get ( HAUtil . addSuffix ( addressPropertyPrefix ,    activeRMId )  )  ;", "addr    =    proxyUriBases . get ( host )  ;", "}", "if    ( addr    =  =    null )     {", "throw   new   ServletException (  \" Could   not   determine   the   proxy      for   redirection \"  )  ;", "}", "return   addr ;", "}", "METHOD_END"], "methodName": ["findRedirectUrl"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "synchronized ( this )     {", "if    (  (  ( proxyAddresses )     =  =    null )     |  |     (  (  ( lastUpdate )     +     (  . updateInterval )  )     >  =    now )  )     {", "proxyAddresses    =    new   HashSet < String >  (  )  ;", "for    ( String   proxyHost    :    proxyHosts )     {", "try    {", "for    ( InetAddress   add    :    InetAddress . getAllByName ( proxyHost )  )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" proxy   address   is :     \"     +     ( add . getHostAddress (  )  )  )  )  ;", "}", "proxyAddresses . add ( add . getHostAddress (  )  )  ;", "}", "lastUpdate    =    now ;", "}    catch    ( UnknownHostException   e )     {", ". LOG . warn (  (  (  \" Could   not   locate    \"     +    proxyHost )     +     \"     -    skipping \"  )  ,    e )  ;", "}", "}", "if    ( proxyAddresses . isEmpty (  )  )     {", "throw   new   ServletException (  \" Could   not   locate   any   of   the   proxy   hosts \"  )  ;", "}", "}", "return   proxyAddresses ;", "}", "}", "METHOD_END"], "methodName": ["getProxyAddresses"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter"}, {"methodBody": ["METHOD_START", "{", "HttpServletRequest   request    =    Mockito . mock ( HttpServletRequest . class )  ;", "Mockito . when ( request . getCookies (  )  )  . thenReturn ( null )  ;", "Mockito . when ( request . getRemoteAddr (  )  )  . thenReturn ( proxyHost )  ;", "HttpServletResponse   response    =    Mockito . mock ( HttpServletResponse . class )  ;", "final   AtomicBoolean   invoked    =    new   AtomicBoolean (  )  ;", "FilterChain   chain    =    new   FilterChain (  )     {", "@ Override", "public   void   doFilter ( ServletRequest   servletRequest ,    ServletResponse   servletResponse )    throws   IOException ,    ServletException    {", "invoked . set ( true )  ;", "}", "}  ;", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( AmIpFilter . PROXY _ HOST ,    proxyHost )  ;", "params . put ( AmIpFilter . PROXY _ URI _ BASE ,    proxyUri )  ;", "FilterConfig   conf    =    new    . DummyFilterConfig ( params )  ;", "Filter   filter    =    new    . TestAmIpFilter (  )  ;", "filter . init ( conf )  ;", "filter . doFilter ( request ,    response ,    chain )  ;", "assertTrue ( invoked . get (  )  )  ;", "filter . destroy (  )  ;", "}", "METHOD_END"], "methodName": ["filterNullCookies"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilter"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( AmIpFilter . PROXY _ HOST ,    proxyHost )  ;", "params . put ( AmIpFilter . PROXY _ URI _ BASE ,    proxyUri )  ;", "FilterConfig   config    =    new   TestAmFilter . DummyFilterConfig ( params )  ;", "FilterChain   chain    =    new   FilterChain (  )     {", "@ Override", "public   void   doFilter ( ServletRequest   servletRequest ,    ServletResponse   servletResponse )    throws   IOException ,    ServletException    {", "doFilterRequest    =    servletRequest . getClass (  )  . getName (  )  ;", "if    ( servletRequest   instanceof   AmIpServletRequestWrapper )     {", "servletWrapper    =     (  ( AmIpServletRequestWrapper )     ( servletRequest )  )  ;", "}", "}", "}  ;", "AmIpFilter   testFilter    =    new   AmIpFilter (  )  ;", "testFilter . init ( config )  ;", "TestAmFilter . HttpServletResponseForTest   response    =    new   TestAmFilter . HttpServletResponseForTest (  )  ;", "ServletRequest   failRequest    =    Mockito . mock ( ServletRequest . class )  ;", "try    {", "testFilter . doFilter ( failRequest ,    response ,    chain )  ;", "fail (  )  ;", "}    catch    ( ServletException   e )     {", "assertEquals (  \" This   filter   only   works   for   HTTP / HTTPS \"  ,    e . getMessage (  )  )  ;", "}", "HttpServletRequest   request    =    Mockito . mock ( HttpServletRequest . class )  ;", "Mockito . when ( request . getRemoteAddr (  )  )  . thenReturn (  \" redirect \"  )  ;", "Mockito . when ( request . getRequestURI (  )  )  . thenReturn (  \"  / redirect \"  )  ;", "testFilter . doFilter ( request ,    response ,    chain )  ;", "assertEquals (  \" http :  /  / bogus / redirect \"  ,    response . getRedirect (  )  )  ;", "Mockito . when ( request . getRemoteAddr (  )  )  . thenReturn (  \"  1  2  7  .  0  .  0  .  1  \"  )  ;", "testFilter . doFilter ( request ,    response ,    chain )  ;", "assertTrue ( doFilterRequest . contains (  \" HttpServletRequest \"  )  )  ;", "Cookie [  ]    cookies    =    new   Cookie [  1  ]  ;", "cookies [  0  ]     =    new   Cookie ( WebAppProxyServlet . PROXY _ USER _ COOKIE _ NAME ,     \" user \"  )  ;", "Mockito . when ( request . getCookies (  )  )  . thenReturn ( cookies )  ;", "testFilter . doFilter ( request ,    response ,    chain )  ;", "assertEquals (  \" AmIpServletRequestWrapper \"  ,    doFilterRequest )  ;", "assertEquals (  \" user \"  ,    servletWrapper . getUserPrincipal (  )  . getName (  )  )  ;", "assertEquals (  \" user \"  ,    servletWrapper . getRemoteUser (  )  )  ;", "assertFalse ( servletWrapper . isUserInRole (  \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testFilter"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilter"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration ( false )  ;", "List < String >    proxyHosts    =    WebAppUtils . getProxyHostsAndPortsFor ( conf )  ;", "assertEquals (  1  ,    proxyHosts . size (  )  )  ;", "assertEquals ( WebAppUtils . getResolvedRMWebAppURLWithoutScheme ( conf )  ,    proxyHosts . get (  0  )  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . set ( PROXY _ ADDRESS ,     \" host 1  :  1  0  0  0  \"  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  , rm 3  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 2  :  2  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 3  :  3  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 3  \"  )  ,     \" host 4  :  4  0  0  0  \"  )  ;", "proxyHosts    =    WebAppUtils . getProxyHostsAndPortsFor ( conf )  ;", "assertEquals (  1  ,    proxyHosts . size (  )  )  ;", "assertEquals (  \" host 1  :  1  0  0  0  \"  ,    proxyHosts . get (  0  )  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . set ( RM _ WEBAPP _ ADDRESS ,     \" host 2  :  2  0  0  0  \"  )  ;", "proxyHosts    =    WebAppUtils . getProxyHostsAndPortsFor ( conf )  ;", "assertEquals (  1  ,    proxyHosts . size (  )  )  ;", "Collections . sort ( proxyHosts )  ;", "assertEquals (  \" host 2  :  2  0  0  0  \"  ,    proxyHosts . get (  0  )  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  , rm 3  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 2  :  2  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 3  :  3  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 3  \"  )  ,     \" host 4  :  4  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 4  \"  )  ,     \" dummy \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 5  :  5  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 6  :  6  0  0  0  \"  )  ;", "proxyHosts    =    WebAppUtils . getProxyHostsAndPortsFor ( conf )  ;", "assertEquals (  3  ,    proxyHosts . size (  )  )  ;", "Collections . sort ( proxyHosts )  ;", "assertEquals (  \" host 2  :  2  0  0  0  \"  ,    proxyHosts . get (  0  )  )  ;", "assertEquals (  \" host 3  :  3  0  0  0  \"  ,    proxyHosts . get (  1  )  )  ;", "assertEquals (  \" host 4  :  4  0  0  0  \"  ,    proxyHosts . get (  2  )  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . set ( YARN _ HTTP _ POLICY _ KEY ,    HTTPS _ ONLY . toString (  )  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  , rm 3  , dummy \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 2  :  2  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 3  :  3  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 3  \"  )  ,     \" host 4  :  4  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 5  :  5  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 6  :  6  0  0  0  \"  )  ;", "proxyHosts    =    WebAppUtils . getProxyHostsAndPortsFor ( conf )  ;", "assertEquals (  2  ,    proxyHosts . size (  )  )  ;", "Collections . sort ( proxyHosts )  ;", "assertEquals (  \" host 5  :  5  0  0  0  \"  ,    proxyHosts . get (  0  )  )  ;", "assertEquals (  \" host 6  :  6  0  0  0  \"  ,    proxyHosts . get (  1  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetProxyHostsAndPortsForAmFilter"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilterInitializer"}, {"methodBody": ["METHOD_START", "{", "TestAmFilterInitializer . MockFilterContainer   con    =    new   TestAmFilterInitializer . MockFilterContainer (  )  ;", "Configuration   conf    =    new   Configuration ( false )  ;", "conf . set ( PROXY _ ADDRESS ,     \" host 1  :  1  0  0  0  \"  )  ;", "AmFilterInitializer   afi    =    new   TestAmFilterInitializer . MockAmFilterInitializer (  )  ;", "assertNull ( con . givenParameters )  ;", "afi . initFilter ( con ,    conf )  ;", "assertEquals (  2  ,    con . givenParameters . size (  )  )  ;", "assertEquals (  \" host 1  \"  ,    con . givenParameters . get ( AmIpFilter . PROXY _ HOSTS )  )  ;", "assertEquals (  \" http :  /  / host 1  :  1  0  0  0  / foo \"  ,    con . givenParameters . get ( AmIpFilter . PROXY _ URI _ BASES )  )  ;", "con    =    new   TestAmFilterInitializer . MockFilterContainer (  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . set ( RM _ WEBAPP _ ADDRESS ,     \" host 2  :  2  0  0  0  \"  )  ;", "afi    =    new   TestAmFilterInitializer . MockAmFilterInitializer (  )  ;", "assertNull ( con . givenParameters )  ;", "afi . initFilter ( con ,    conf )  ;", "assertEquals (  2  ,    con . givenParameters . size (  )  )  ;", "assertEquals (  \" host 2  \"  ,    con . givenParameters . get ( AmIpFilter . PROXY _ HOSTS )  )  ;", "assertEquals (  \" http :  /  / host 2  :  2  0  0  0  / foo \"  ,    con . givenParameters . get ( AmIpFilter . PROXY _ URI _ BASES )  )  ;", "con    =    new   TestAmFilterInitializer . MockFilterContainer (  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  , rm 3  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 2  :  2  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 3  :  3  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  . rm 3  \"  )  ,     \" host 4  :  4  0  0  0  \"  )  ;", "afi    =    new   TestAmFilterInitializer . MockAmFilterInitializer (  )  ;", "assertNull ( con . givenParameters )  ;", "afi . initFilter ( con ,    conf )  ;", "assertEquals (  2  ,    con . givenParameters . size (  )  )  ;", "String [  ]    proxyHosts    =    con . givenParameters . get ( AmIpFilter . PROXY _ HOSTS )  . split ( AmIpFilter . PROXY _ HOSTS _ DELIMITER )  ;", "assertEquals (  3  ,    proxyHosts . length )  ;", "Arrays . sort ( proxyHosts )  ;", "assertEquals (  \" host 2  \"  ,    proxyHosts [  0  ]  )  ;", "assertEquals (  \" host 3  \"  ,    proxyHosts [  1  ]  )  ;", "assertEquals (  \" host 4  \"  ,    proxyHosts [  2  ]  )  ;", "String [  ]    proxyBases    =    con . givenParameters . get ( AmIpFilter . PROXY _ URI _ BASES )  . split ( AmIpFilter . PROXY _ URI _ BASES _ DELIMITER )  ;", "assertEquals (  3  ,    proxyBases . length )  ;", "Arrays . sort ( proxyBases )  ;", "assertEquals (  \" http :  /  / host 2  :  2  0  0  0  / foo \"  ,    proxyBases [  0  ]  )  ;", "assertEquals (  \" http :  /  / host 3  :  3  0  0  0  / foo \"  ,    proxyBases [  1  ]  )  ;", "assertEquals (  \" http :  /  / host 4  :  4  0  0  0  / foo \"  ,    proxyBases [  2  ]  )  ;", "con    =    new   TestAmFilterInitializer . MockFilterContainer (  )  ;", "conf    =    new   Configuration ( false )  ;", "conf . set ( YARN _ HTTP _ POLICY _ KEY ,    HTTPS _ ONLY . toString (  )  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  . rm 1  \"  )  ,     \" host 5  :  5  0  0  0  \"  )  ;", "conf . set (  (  ( YarnConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  . rm 2  \"  )  ,     \" host 6  :  6  0  0  0  \"  )  ;", "afi    =    new   TestAmFilterInitializer . MockAmFilterInitializer (  )  ;", "assertNull ( con . givenParameters )  ;", "afi . initFilter ( con ,    conf )  ;", "assertEquals (  2  ,    con . givenParameters . size (  )  )  ;", "proxyHosts    =    con . givenParameters . get ( AmIpFilter . PROXY _ HOSTS )  . split ( AmIpFilter . PROXY _ HOSTS _ DELIMITER )  ;", "assertEquals (  2  ,    proxyHosts . length )  ;", "Arrays . sort ( proxyHosts )  ;", "assertEquals (  \" host 5  \"  ,    proxyHosts [  0  ]  )  ;", "assertEquals (  \" host 6  \"  ,    proxyHosts [  1  ]  )  ;", "proxyBases    =    con . givenParameters . get ( AmIpFilter . PROXY _ URI _ BASES )  . split ( AmIpFilter . PROXY _ URI _ BASES _ DELIMITER )  ;", "assertEquals (  2  ,    proxyBases . length )  ;", "Arrays . sort ( proxyBases )  ;", "assertEquals (  \" https :  /  / host 5  :  5  0  0  0  / foo \"  ,    proxyBases [  0  ]  )  ;", "assertEquals (  \" https :  /  / host 6  :  6  0  0  0  / foo \"  ,    proxyBases [  1  ]  )  ;", "}", "METHOD_END"], "methodName": ["testInitFilter"], "fileName": "org.apache.hadoop.yarn.server.webproxy.amfilter.TestAmFilterInitializer"}]