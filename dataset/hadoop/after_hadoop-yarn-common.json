[{"methodBody": ["METHOD_START", "{", "return   this . containerLogDir ;", "}", "METHOD_END"], "methodName": ["getContainerLogDir"], "fileName": "org.apache.hadoop.yarn.ContainerLogAppender"}, {"methodBody": ["METHOD_START", "{", "return    ( maxEvents )     *     ( ContainerLogAppender . EVENT _ SIZE )  ;", "}", "METHOD_END"], "methodName": ["getTotalLogFileSize"], "fileName": "org.apache.hadoop.yarn.ContainerLogAppender"}, {"methodBody": ["METHOD_START", "{", "this . containerLogDir    =    containerLogDir ;", "}", "METHOD_END"], "methodName": ["setContainerLogDir"], "fileName": "org.apache.hadoop.yarn.ContainerLogAppender"}, {"methodBody": ["METHOD_START", "{", "maxEvents    =     (  ( int )     ( logSize )  )     /     ( ContainerLogAppender . EVENT _ SIZE )  ;", "}", "METHOD_END"], "methodName": ["setTotalLogFileSize"], "fileName": "org.apache.hadoop.yarn.ContainerLogAppender"}, {"methodBody": ["METHOD_START", "{", "return   this . containerLogDir ;", "}", "METHOD_END"], "methodName": ["getContainerLogDir"], "fileName": "org.apache.hadoop.yarn.ContainerRollingLogAppender"}, {"methodBody": ["METHOD_START", "{", "this . containerLogDir    =    containerLogDir ;", "}", "METHOD_END"], "methodName": ["setContainerLogDir"], "fileName": "org.apache.hadoop.yarn.ContainerRollingLogAppender"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationId . newInstance ( MockApps . TS ,    i )  ;", "}", "METHOD_END"], "methodName": ["newAppID"], "fileName": "org.apache.hadoop.yarn.MockApps"}, {"methodBody": ["METHOD_START", "{", "synchronized ( MockApps . NAMES )     {", "return   MockApps . NAMES . next (  )  ;", "}", "}", "METHOD_END"], "methodName": ["newAppName"], "fileName": "org.apache.hadoop.yarn.MockApps"}, {"methodBody": ["METHOD_START", "{", "synchronized ( MockApps . STATES )     {", "return   MockApps . STATES . next (  )  ;", "}", "}", "METHOD_END"], "methodName": ["newAppState"], "fileName": "org.apache.hadoop.yarn.MockApps"}, {"methodBody": ["METHOD_START", "{", "synchronized ( MockApps . QUEUES )     {", "return   MockApps . QUEUES . next (  )  ;", "}", "}", "METHOD_END"], "methodName": ["newQueue"], "fileName": "org.apache.hadoop.yarn.MockApps"}, {"methodBody": ["METHOD_START", "{", "synchronized ( MockApps . USERS )     {", "return   MockApps . USERS . next (  )  ;", "}", "}", "METHOD_END"], "methodName": ["newUserName"], "fileName": "org.apache.hadoop.yarn.MockApps"}, {"methodBody": ["METHOD_START", "{", "testRPCTimeout ( HadoopYarnProtoRPC . class . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHadoopProtoRPCTimeout"], "fileName": "org.apache.hadoop.yarn.TestContainerLaunchRPC"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setInt (  \" yarn . rpc . nm - command - timeout \"  ,     3  0  0  0  )  ;", "conf . set ( IPC _ RPC _ IMPL ,    rpcClass )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "String   bindAddr    =     \" localhost :  0  \"  ;", "InetSocketAddress   addr    =    NetUtils . createSocketAddr ( bindAddr )  ;", "Server   server    =    rpc . getServer ( ContainerManagementProtocol . class ,    new    . DummyContainerManager (  )  ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "try    {", "ContainerManagementProtocol   proxy    =     (  ( ContainerManagementProtocol )     ( rpc . getProxy ( ContainerManagementProtocol . class ,    server . getListenerAddress (  )  ,    conf )  )  )  ;", "ContainerLaunchContext   containerLaunchContext    =     . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "ApplicationId   applicationId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     0  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( applicationAttemptId ,     1  0  0  )  ;", "NodeId   nodeId    =    NodeId . newInstance (  \" localhost \"  ,     1  2  3  4  )  ;", "Resource   resource    =    Resource . newInstance (  1  2  3  4  ,     2  )  ;", "ContainerTokenIdentifier   containerTokenIdentifier    =    new   ContainerTokenIdentifier ( containerId ,     \" localhost \"  ,     \" user \"  ,    resource ,     (  ( System . currentTimeMillis (  )  )     +     1  0  0  0  0  )  ,     4  2  ,     4  2  ,    Priority . newInstance (  0  )  ,     0  )  ;", "Token   containerToken    =    TestRPC . newContainerToken ( nodeId ,     \" password \"  . getBytes (  )  ,    containerTokenIdentifier )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "try    {", "proxy . startContainers ( allRequests )  ;", "}    catch    ( Exception   e )     {", ". LOG . info ( StringUtils . stringifyException ( e )  )  ;", "Assert . assertEquals (  (  \" Error ,    exception   is   not :     \"     +     ( SocketTimeoutException . class . getName (  )  )  )  ,    SocketTimeoutException . class . getName (  )  ,    e . getClass (  )  . getName (  )  )  ;", "return ;", "}", "}    finally    {", "server . stop (  )  ;", "}", "Assert . fail (  \" timeout   exception   should   have   occurred !  \"  )  ;", "}", "METHOD_END"], "methodName": ["testRPCTimeout"], "fileName": "org.apache.hadoop.yarn.TestContainerLaunchRPC"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    NetUtils . createSocketAddrForHost ( nodeId . getHost (  )  ,    nodeId . getPort (  )  )  ;", "Token   containerToken    =    Token . newInstance ( tokenIdentifier . getBytes (  )  ,    ContainerTokenIdentifier . KIND . toString (  )  ,    password ,    SecurityUtil . buildTokenService ( addr )  . toString (  )  )  ;", "return   containerToken ;", "}", "METHOD_END"], "methodName": ["newContainerToken"], "fileName": "org.apache.hadoop.yarn.TestRPC"}, {"methodBody": ["METHOD_START", "{", "Token < ContainerTokenIdentifier >    token    =    new   Token < ContainerTokenIdentifier >  ( containerToken . getIdentifier (  )  . array (  )  ,    containerToken . getPassword (  )  . array (  )  ,    new   Text ( containerToken . getKind (  )  )  ,    new   Text ( containerToken . getService (  )  )  )  ;", "return   token . decodeIdentifier (  )  ;", "}", "METHOD_END"], "methodName": ["newContainerTokenIdentifier"], "fileName": "org.apache.hadoop.yarn.TestRPC"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( IPC _ RPC _ IMPL ,    rpcClass )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "String   bindAddr    =     \" localhost :  0  \"  ;", "InetSocketAddress   addr    =    NetUtils . createSocketAddr ( bindAddr )  ;", "Server   server    =    rpc . getServer ( ContainerManagementProtocol . class ,    new    . DummyContainerManager (  )  ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "RPC . setProtocolEngine ( conf ,    ContainerManagementProtocolPB . class ,    ProtobufRpcEngine . class )  ;", "ContainerManagementProtocol   proxy    =     (  ( ContainerManagementProtocol )     ( rpc . getProxy ( ContainerManagementProtocol . class ,    NetUtils . getConnectAddress ( server )  ,    conf )  )  )  ;", "ContainerLaunchContext   containerLaunchContext    =     . recordFactory . newRecordInstance ( ContainerLaunchContext . class )  ;", "ApplicationId   applicationId    =    ApplicationId . newInstance (  0  ,     0  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     0  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( applicationAttemptId ,     1  0  0  )  ;", "NodeId   nodeId    =    NodeId . newInstance (  \" localhost \"  ,     1  2  3  4  )  ;", "Resource   resource    =    Resource . newInstance (  1  2  3  4  ,     2  )  ;", "ContainerTokenIdentifier   containerTokenIdentifier    =    new   ContainerTokenIdentifier ( containerId ,     \" localhost \"  ,     \" user \"  ,    resource ,     (  ( System . currentTimeMillis (  )  )     +     1  0  0  0  0  )  ,     4  2  ,     4  2  ,    Priority . newInstance (  0  )  ,     0  )  ;", "Token   containerToken    =     . newContainerToken ( nodeId ,     \" password \"  . getBytes (  )  ,    containerTokenIdentifier )  ;", "StartContainerRequest   scRequest    =    StartContainerRequest . newInstance ( containerLaunchContext ,    containerToken )  ;", "List < StartContainerRequest >    list    =    new   ArrayList < StartContainerRequest >  (  )  ;", "list . add ( scRequest )  ;", "StartContainersRequest   allRequests    =    StartContainersRequest . newInstance ( list )  ;", "proxy . startContainers ( allRequests )  ;", "List < ContainerId >    containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "containerIds . add ( containerId )  ;", "GetContainerStatusesRequest   gcsRequest    =    GetContainerStatusesRequest . newInstance ( containerIds )  ;", "GetContainerStatusesResponse   response    =    proxy . getContainerStatuses ( gcsRequest )  ;", "List < ContainerStatus >    statuses    =    response . getContainerStatuses (  )  ;", "boolean   exception    =    false ;", "try    {", "StopContainersRequest   stopRequest    =     . recordFactory . newRecordInstance ( StopContainersRequest . class )  ;", "stopRequest . setContainerIds ( containerIds )  ;", "proxy . stopContainers ( stopRequest )  ;", "}    catch    ( YarnException   e )     {", "exception    =    true ;", "Assert . assertTrue ( e . getMessage (  )  . contains (  . EXCEPTION _ MSG )  )  ;", "Assert . assertTrue ( e . getMessage (  )  . contains (  . EXCEPTION _ CAUSE )  )  ;", "System . out . println (  (  \" Test   Exception   is    \"     +     ( e . getMessage (  )  )  )  )  ;", "}    catch    ( Exception   ex )     {", "ex . printStackTrace (  )  ;", "}", "Assert . assertTrue ( exception )  ;", "server . stop (  )  ;", "Assert . assertNotNull ( statuses . get (  0  )  )  ;", "Assert . assertEquals ( RUNNING ,    statuses . get (  0  )  . getState (  )  )  ;", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.TestRPC"}, {"methodBody": ["METHOD_START", "{", "test ( HadoopYarnProtoRPC . class . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testHadoopProtoRPC"], "fileName": "org.apache.hadoop.yarn.TestRPC"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( IPC _ RPC _ IMPL ,    HadoopYarnProtoRPC . class . getName (  )  )  ;", "YarnRPC   rpc    =    YarnRPC . create ( conf )  ;", "String   bindAddr    =     \" localhost :  0  \"  ;", "InetSocketAddress   addr    =    NetUtils . createSocketAddr ( bindAddr )  ;", "Server   server    =    rpc . getServer ( ContainerManagementProtocol . class ,    new   TestRPC . DummyContainerManager (  )  ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "ApplicationClientProtocol   proxy    =     (  ( ApplicationClientProtocol )     ( rpc . getProxy ( ApplicationClientProtocol . class ,    NetUtils . getConnectAddress ( server )  ,    conf )  )  )  ;", "try    {", "proxy . getNewApplication ( Records . newRecord ( GetNewApplicationRequest . class )  )  ;", "Assert . fail (  \" Excepted   RPC   call   to   fail   with   unknown   method .  \"  )  ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . matches (  (  \" Unknown   method   getNewApplication   called   on .  *  \"     +     (  \" proto . ApplicationClientProtocol \"     +     \"  \\  \\  $ ApplicationClientProtocolService \\  \\  $ BlockingInterface   protocol .  \"  )  )  )  )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testUnknownCall"], "fileName": "org.apache.hadoop.yarn.TestRPC"}, {"methodBody": ["METHOD_START", "{", "testPbServerFactory (  )  ;", "testPbClientFactory (  )  ;", "}", "METHOD_END"], "methodName": ["test"], "fileName": "org.apache.hadoop.yarn.TestRPCFactories"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    new   InetSocketAddress (  0  )  ;", "System . err . println (  (  ( addr . getHostName (  )  )     +     ( addr . getPort (  )  )  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "ApplicationMasterProtocol   instance    =    new    . AMRMProtocolTestImpl (  )  ;", "Server   server    =    null ;", "try    {", "server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( ApplicationMasterProtocol . class ,    instance ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "System . err . println ( server . getListenerAddress (  )  )  ;", "System . err . println ( NetUtils . getConnectAddress ( server )  )  ;", "ApplicationMasterProtocol   amrmClient    =    null ;", "try    {", "amrmClient    =     (  ( ApplicationMasterProtocol )     ( RpcClientFactoryPBImpl . get (  )  . getClient ( ApplicationMasterProtocol . class ,     1  ,    NetUtils . getConnectAddress ( server )  ,    conf )  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   client \"  )  ;", "}", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   server \"  )  ;", "}    finally    {", "if    ( server    !  =    null )     {", "server . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testPbClientFactory"], "fileName": "org.apache.hadoop.yarn.TestRPCFactories"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    new   InetSocketAddress (  0  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "ApplicationMasterProtocol   instance    =    new    . AMRMProtocolTestImpl (  )  ;", "Server   server    =    null ;", "try    {", "server    =    RpcServerFactoryPBImpl . get (  )  . getServer ( ApplicationMasterProtocol . class ,    instance ,    addr ,    conf ,    null ,     1  )  ;", "server . start (  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   create   server \"  )  ;", "}    finally    {", "if    ( server    !  =    null )     {", "server . stop (  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testPbServerFactory"], "fileName": "org.apache.hadoop.yarn.TestRPCFactories"}, {"methodBody": ["METHOD_START", "{", "RecordFactory   pbRecordFactory    =    RecordFactoryPBImpl . get (  )  ;", "try    {", "AllocateResponse   response    =    pbRecordFactory . newRecordInstance ( AllocateResponse . class )  ;", "Assert . assertEquals ( AllocateResponsePBImpl . class ,    response . getClass (  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   crete   record \"  )  ;", "}", "try    {", "AllocateRequest   response    =    pbRecordFactory . newRecordInstance ( AllocateRequest . class )  ;", "Assert . assertEquals ( AllocateRequestPBImpl . class ,    response . getClass (  )  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "e . printStackTrace (  )  ;", "Assert . fail (  \" Failed   to   crete   record \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPbRecordFactory"], "fileName": "org.apache.hadoop.yarn.TestRecordFactory"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "RpcClientFactory   clientFactory    =    null ;", "RpcServerFactory   serverFactory    =    null ;", "clientFactory    =     . getClientFactory ( conf )  ;", "serverFactory    =     . getServerFactory ( conf )  ;", "Assert . assertEquals ( RpcClientFactoryPBImpl . class ,    clientFactory . getClass (  )  )  ;", "Assert . assertEquals ( RpcServerFactoryPBImpl . class ,    serverFactory . getClass (  )  )  ;", "conf . set ( IPC _ CLIENT _ FACTORY _ CLASS ,     \" unknown \"  )  ;", "conf . set ( IPC _ SERVER _ FACTORY _ CLASS ,     \" unknown \"  )  ;", "conf . set ( IPC _ RECORD _ FACTORY _ CLASS ,     \" unknown \"  )  ;", "try    {", "clientFactory    =     . getClientFactory ( conf )  ;", "Assert . fail (  \" Expected   an   exception    -    unknown   serializer \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "}", "try    {", "serverFactory    =     . getServerFactory ( conf )  ;", "Assert . fail (  \" Expected   an   exception    -    unknown   serializer \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "}", "conf    =    new   Configuration (  )  ;", "conf . set ( IPC _ CLIENT _ FACTORY _ CLASS ,     \" NonExistantClass \"  )  ;", "conf . set ( IPC _ SERVER _ FACTORY _ CLASS ,    RpcServerFactoryPBImpl . class . getName (  )  )  ;", "try    {", "clientFactory    =     . getClientFactory ( conf )  ;", "Assert . fail (  \" Expected   an   exception    -    unknown   class \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "}", "try    {", "serverFactory    =     . getServerFactory ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "Assert . fail (  (  (  \" Error   while   loading   factory   using   reflection :     [  \"     +     ( RpcServerFactoryPBImpl . class . getName (  )  )  )     +     \"  ]  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testFactoryProvider"], "fileName": "org.apache.hadoop.yarn.TestRpcFactoryProvider"}, {"methodBody": ["METHOD_START", "{", "ExitUtil . disableSystemExit (  )  ;", "final   YarnUncaughtExceptionHandler   spyErrorHandler    =    spy (  . exHandler )  ;", "final   Error   error    =    new   Error (  \" test - error \"  )  ;", "final   Thread   errorThread    =    new   Thread ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "throw   error ;", "}", "}  )  ;", "errorThread . setUncaughtExceptionHandler ( spyErrorHandler )  ;", "assertSame ( spyErrorHandler ,    errorThread . getUncaughtExceptionHandler (  )  )  ;", "errorThread . start (  )  ;", "errorThread . join (  )  ;", "verify ( spyErrorHandler )  . uncaughtException ( errorThread ,    error )  ;", "}", "METHOD_END"], "methodName": ["testUncaughtExceptionHandlerWithError"], "fileName": "org.apache.hadoop.yarn.TestYarnUncaughtExceptionHandler"}, {"methodBody": ["METHOD_START", "{", "ExitUtil . disableSystemHalt (  )  ;", "final   YarnUncaughtExceptionHandler   spyOomHandler    =    spy (  . exHandler )  ;", "final   OutOfMemoryError   oomError    =    new   OutOfMemoryError (  \" out - of - memory - error \"  )  ;", "final   Thread   oomThread    =    new   Thread ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "throw   oomError ;", "}", "}  )  ;", "oomThread . setUncaughtExceptionHandler ( spyOomHandler )  ;", "assertSame ( spyOomHandler ,    oomThread . getUncaughtExceptionHandler (  )  )  ;", "oomThread . start (  )  ;", "oomThread . join (  )  ;", "verify ( spyOomHandler )  . uncaughtException ( oomThread ,    oomError )  ;", "}", "METHOD_END"], "methodName": ["testUncaughtExceptionHandlerWithOutOfMemoryError"], "fileName": "org.apache.hadoop.yarn.TestYarnUncaughtExceptionHandler"}, {"methodBody": ["METHOD_START", "{", "final   YarnUncaughtExceptionHandler   spyYarnHandler    =    spy ( TestYarnUncaughtExceptionHandler . exHandler )  ;", "final   YarnRuntimeException   yarnException    =    new   YarnRuntimeException (  \" test - yarn - runtime - exception \"  )  ;", "final   Thread   yarnThread    =    new   Thread ( new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "throw   yarnException ;", "}", "}  )  ;", "yarnThread . setUncaughtExceptionHandler ( spyYarnHandler )  ;", "assertSame ( spyYarnHandler ,    yarnThread . getUncaughtExceptionHandler (  )  )  ;", "yarnThread . start (  )  ;", "yarnThread . join (  )  ;", "verify ( spyYarnHandler )  . uncaughtException ( yarnThread ,    yarnException )  ;", "}", "METHOD_END"], "methodName": ["testUncaughtExceptionHandlerWithRuntimeException"], "fileName": "org.apache.hadoop.yarn.TestYarnUncaughtExceptionHandler"}, {"methodBody": ["METHOD_START", "{", "List < ContainerResourceIncreaseRequest >    incRequests    =    new   ArrayList < ContainerResourceIncreaseRequest >  (  )  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "incRequests . add ( ContainerResourceIncreaseRequest . newInstance ( null ,    Resource . newInstance (  0  ,    i )  )  )  ;", "}", "r    =     . newInstance (  1  2  3  ,     0  .  0 F ,    null ,    null ,    null ,    incRequests )  ;", "Proto   p    =     (  ( PBImpl )     ( r )  )  . getProto (  )  ;", "r    =    new   PBImpl ( p )  ;", "Assert . assertEquals (  1  2  3  ,    r . getResponseId (  )  )  ;", "Assert . assertEquals ( incRequests . size (  )  ,    r . getIncreaseRequests (  )  . size (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( incRequests . size (  )  )  ;    i +  +  )     {", "Assert . assertEquals ( r . getIncreaseRequests (  )  . get ( i )  . getCapability (  )  . getVirtualCores (  )  ,    incRequests . get ( i )  . getCapability (  )  . getVirtualCores (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAllcoateRequestWithIncrease"], "fileName": "org.apache.hadoop.yarn.api.TestAllocateRequest"}, {"methodBody": ["METHOD_START", "{", "AllocateRequest   r    =    AllocateRequest . newInstance (  1  2  3  ,     0  .  0 F ,    null ,    null ,    null ,    null )  ;", "AllocateRequestProto   p    =     (  ( AllocateRequestPBImpl )     ( r )  )  . getProto (  )  ;", "r    =    new   AllocateRequestPBImpl ( p )  ;", "Assert . assertEquals (  1  2  3  ,    r . getResponseId (  )  )  ;", "Assert . assertEquals (  0  ,    r . getIncreaseRequests (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAllcoateRequestWithoutIncrease"], "fileName": "org.apache.hadoop.yarn.api.TestAllocateRequest"}, {"methodBody": ["METHOD_START", "{", "List < ContainerResourceIncrease >    incContainers    =    new   ArrayList < ContainerResourceIncrease >  (  )  ;", "List < ContainerResourceDecrease >    decContainers    =    new   ArrayList < ContainerResourceDecrease >  (  )  ;", "for    ( int   i    =     0  ;    i    <     3  ;    i +  +  )     {", "incContainers . add ( ContainerResourceIncrease . newInstance ( null ,    Resource . newInstance (  1  0  2  4  ,    i )  ,    null )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     5  ;    i +  +  )     {", "decContainers . add ( ContainerResourceDecrease . newInstance ( null ,    Resource . newInstance (  1  0  2  4  ,    i )  )  )  ;", "}", "AllocateResponse   r    =    AllocateResponse . newInstance (  3  ,    new   ArrayList < records . ContainerStatus >  (  )  ,    new   ArrayList < records . Container >  (  )  ,    new   ArrayList < records . NodeReport >  (  )  ,    null ,    AM _ RESYNC ,     3  ,    null ,    new   ArrayList < records . NMToken >  (  )  ,    incContainers ,    decContainers )  ;", "AllocateResponseProto   p    =     (  ( AllocateResponsePBImpl )     ( r )  )  . getProto (  )  ;", "r    =    new   AllocateResponsePBImpl ( p )  ;", "Assert . assertEquals ( incContainers . size (  )  ,    r . getIncreasedContainers (  )  . size (  )  )  ;", "Assert . assertEquals ( decContainers . size (  )  ,    r . getDecreasedContainers (  )  . size (  )  )  ;", "for    ( int   i    =     0  ;    i    <     ( incContainers . size (  )  )  ;    i +  +  )     {", "Assert . assertEquals ( i ,    r . getIncreasedContainers (  )  . get ( i )  . getCapability (  )  . getVirtualCores (  )  )  ;", "}", "for    ( int   i    =     0  ;    i    <     ( decContainers . size (  )  )  ;    i +  +  )     {", "Assert . assertEquals ( i ,    r . getDecreasedContainers (  )  . get ( i )  . getCapability (  )  . getVirtualCores (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testAllocateResponseWithIncDecContainers"], "fileName": "org.apache.hadoop.yarn.api.TestAllocateResponse"}, {"methodBody": ["METHOD_START", "{", "AllocateResponse   r    =    AllocateResponse . newInstance (  3  ,    new   ArrayList < ContainerStatus >  (  )  ,    new   ArrayList < Container >  (  )  ,    new   ArrayList < NodeReport >  (  )  ,    null ,    AM _ RESYNC ,     3  ,    null ,    new   ArrayList < NMToken >  (  )  ,    null ,    null )  ;", "AllocateResponseProto   p    =     (  ( AllocateResponsePBImpl )     ( r )  )  . getProto (  )  ;", "r    =    new   AllocateResponsePBImpl ( p )  ;", "Assert . assertEquals (  0  ,    r . getIncreasedContainers (  )  . size (  )  )  ;", "Assert . assertEquals (  0  ,    r . getDecreasedContainers (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testAllocateResponseWithoutIncDecContainers"], "fileName": "org.apache.hadoop.yarn.api.TestAllocateResponse"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance ( clusterTimeStamp ,    id )  ;", "return    . newInstance ( appId ,    attemptId )  ;", "}", "METHOD_END"], "methodName": ["createAppAttemptId"], "fileName": "org.apache.hadoop.yarn.api.TestApplicationAttemptId"}, {"methodBody": ["METHOD_START", "{", "TestApplicationAttemptId   t    =    new   TestApplicationAttemptId (  )  ;", "t . testApplicationAttemptId (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.api.TestApplicationAttemptId"}, {"methodBody": ["METHOD_START", "{", "ApplicationAttemptId   a 1     =    createAppAttemptId (  1  0 L ,     1  ,     1  )  ;", "ApplicationAttemptId   a 2     =    createAppAttemptId (  1  0 L ,     1  ,     2  )  ;", "ApplicationAttemptId   a 3     =    createAppAttemptId (  1  0 L ,     2  ,     1  )  ;", "ApplicationAttemptId   a 4     =    createAppAttemptId (  8 L ,     1  ,     4  )  ;", "ApplicationAttemptId   a 5     =    createAppAttemptId (  1  0 L ,     1  ,     1  )  ;", "Assert . assertTrue ( a 1  . equals ( a 5  )  )  ;", "Assert . assertFalse ( a 1  . equals ( a 2  )  )  ;", "Assert . assertFalse ( a 1  . equals ( a 3  )  )  ;", "Assert . assertFalse ( a 1  . equals ( a 4  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 5  )  )     =  =     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 2  )  )     <     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 3  )  )     <     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 4  )  )     >     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . hashCode (  )  )     =  =     ( a 5  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( a 1  . hashCode (  )  )     =  =     ( a 2  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( a 1  . hashCode (  )  )     =  =     ( a 3  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( a 1  . hashCode (  )  )     =  =     ( a 4  . hashCode (  )  )  )  )  ;", "long   ts    =    System . currentTimeMillis (  )  ;", "ApplicationAttemptId   a 6     =    createAppAttemptId ( ts ,     5  4  3  6  2  7  ,     3  3  4  9  2  6  1  1  )  ;", "Assert . assertEquals (  \" appattempt _  1  0  _  0  0  0  1  _  0  0  0  0  0  1  \"  ,    a 1  . toString (  )  )  ;", "Assert . assertEquals (  (  (  \" appattempt _  \"     +    ts )     +     \"  _  5  4  3  6  2  7  _  3  3  4  9  2  6  1  1  \"  )  ,    a 6  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.api.TestApplicationAttemptId"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   a 1     =    ApplicationId . newInstance (  1  0 L ,     1  )  ;", "ApplicationId   a 2     =    ApplicationId . newInstance (  1  0 L ,     2  )  ;", "ApplicationId   a 3     =    ApplicationId . newInstance (  1  0 L ,     1  )  ;", "ApplicationId   a 4     =    ApplicationId . newInstance (  8 L ,     3  )  ;", "Assert . assertFalse ( a 1  . equals ( a 2  )  )  ;", "Assert . assertFalse ( a 1  . equals ( a 4  )  )  ;", "Assert . assertTrue ( a 1  . equals ( a 3  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 2  )  )     <     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 3  )  )     =  =     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . compareTo ( a 4  )  )     >     0  )  )  ;", "Assert . assertTrue (  (  ( a 1  . hashCode (  )  )     =  =     ( a 3  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( a 1  . hashCode (  )  )     =  =     ( a 2  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( a 2  . hashCode (  )  )     =  =     ( a 4  . hashCode (  )  )  )  )  ;", "long   ts    =    System . currentTimeMillis (  )  ;", "ApplicationId   a 5     =    ApplicationId . newInstance ( ts ,     4  5  4  3  6  3  4  3  )  ;", "Assert . assertEquals (  \" application _  1  0  _  0  0  0  1  \"  ,    a 1  . toString (  )  )  ;", "Assert . assertEquals (  (  (  \" application _  \"     +    ts )     +     \"  _  4  5  4  3  6  3  4  3  \"  )  ,    a 5  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationId"], "fileName": "org.apache.hadoop.yarn.api.TestApplicationId"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance ( timestamp ,    appIdInt )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    appAttemptIdInt )  ;", "ApplicationReport   appReport    =    ApplicationReport . newInstance ( appId ,    appAttemptId ,     \" user \"  ,     \" queue \"  ,     \" appname \"  ,     \" host \"  ,     1  2  4  ,    null ,    FINISHED ,     \" diagnostics \"  ,     \" url \"  ,     0  ,     0  ,    SUCCEEDED ,    null ,     \" N / A \"  ,     0  .  5  3  7  8  9 F ,    DEFAULT _ APPLICATION _ TYPE ,    null )  ;", "return   appReport ;", "}", "METHOD_END"], "methodName": ["createApplicationReport"], "fileName": "org.apache.hadoop.yarn.api.TestApplicatonReport"}, {"methodBody": ["METHOD_START", "{", "long   timestamp    =    System . currentTimeMillis (  )  ;", "ApplicationReport   appReport 1     =     . createApplicationReport (  1  ,     1  ,    timestamp )  ;", "ApplicationReport   appReport 2     =     . createApplicationReport (  1  ,     1  ,    timestamp )  ;", "ApplicationReport   appReport 3     =     . createApplicationReport (  1  ,     1  ,    timestamp )  ;", "Assert . assertEquals ( appReport 1  ,    appReport 2  )  ;", "Assert . assertEquals ( appReport 2  ,    appReport 3  )  ;", "appReport 1  . setApplicationId ( null )  ;", "Assert . assertNull ( appReport 1  . getApplicationId (  )  )  ;", "Assert . assertNotSame ( appReport 1  ,    appReport 2  )  ;", "appReport 2  . setCurrentApplicationAttemptId ( null )  ;", "Assert . assertNull ( appReport 2  . getCurrentApplicationAttemptId (  )  )  ;", "Assert . assertNotSame ( appReport 2  ,    appReport 3  )  ;", "Assert . assertNull ( appReport 1  . getAMRMToken (  )  )  ;", "}", "METHOD_END"], "methodName": ["testApplicationReport"], "fileName": "org.apache.hadoop.yarn.api.TestApplicatonReport"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   applicationId    =    ApplicationId . newInstance ( timestamp ,    appId )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,    appAttemptId )  ;", "return    . newInstance ( applicationAttemptId ,    containerId )  ;", "}", "METHOD_END"], "methodName": ["newContainerId"], "fileName": "org.apache.hadoop.yarn.api.TestContainerId"}, {"methodBody": ["METHOD_START", "{", "ContainerId   c 1     =    TestContainerId . newContainerId (  1  ,     1  ,     1  0 L ,     1  )  ;", "ContainerId   c 2     =    TestContainerId . newContainerId (  1  ,     1  ,     1  0 L ,     2  )  ;", "ContainerId   c 3     =    TestContainerId . newContainerId (  1  ,     1  ,     1  0 L ,     1  )  ;", "ContainerId   c 4     =    TestContainerId . newContainerId (  1  ,     3  ,     1  0 L ,     1  )  ;", "ContainerId   c 5     =    TestContainerId . newContainerId (  1  ,     3  ,     8 L ,     1  )  ;", "Assert . assertTrue ( c 1  . equals ( c 3  )  )  ;", "Assert . assertFalse ( c 1  . equals ( c 2  )  )  ;", "Assert . assertFalse ( c 1  . equals ( c 4  )  )  ;", "Assert . assertFalse ( c 1  . equals ( c 5  )  )  ;", "Assert . assertTrue (  (  ( c 1  . compareTo ( c 3  )  )     =  =     0  )  )  ;", "Assert . assertTrue (  (  ( c 1  . compareTo ( c 2  )  )     <     0  )  )  ;", "Assert . assertTrue (  (  ( c 1  . compareTo ( c 4  )  )     <     0  )  )  ;", "Assert . assertTrue (  (  ( c 1  . compareTo ( c 5  )  )     >     0  )  )  ;", "Assert . assertTrue (  (  ( c 1  . hashCode (  )  )     =  =     ( c 3  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( c 1  . hashCode (  )  )     =  =     ( c 2  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( c 1  . hashCode (  )  )     =  =     ( c 4  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( c 1  . hashCode (  )  )     =  =     ( c 5  . hashCode (  )  )  )  )  ;", "long   ts    =    System . currentTimeMillis (  )  ;", "ContainerId   c 6     =    TestContainerId . newContainerId (  3  6  4  7  3  ,     4  3  6  5  4  7  2  ,    ts ,     2  5  6  4  5  8  1  1  )  ;", "Assert . assertEquals (  \" container _  1  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  ,    c 1  . toString (  )  )  ;", "Assert . assertEquals (  (  (  \" container _  \"     +    ts )     +     \"  _  3  6  4  7  3  _  4  3  6  5  4  7  2  _  2  5  6  4  5  8  1  1  \"  )  ,    c 6  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerId"], "fileName": "org.apache.hadoop.yarn.api.TestContainerId"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  ,     3  )  ,     3  )  ,     7  )  ;", "Resource   resource    =    Resource . newInstance (  1  0  2  3  ,     3  )  ;", "ctx    =     . newInstance ( containerId ,    resource )  ;", "Proto   proto    =     (  ( PBImpl )     ( ctx )  )  . getProto (  )  ;", "ctx    =    new   PBImpl ( proto )  ;", "Assert . assertEquals ( ctx . getCapability (  )  ,    resource )  ;", "Assert . assertEquals ( ctx . getContainerId (  )  ,    containerId )  ;", "}", "METHOD_END"], "methodName": ["testResourceDecreaseContext"], "fileName": "org.apache.hadoop.yarn.api.TestContainerResourceDecrease"}, {"methodBody": ["METHOD_START", "{", "ContainerResourceDecrease   ctx    =    ContainerResourceDecrease . newInstance ( null ,    null )  ;", "ContainerResourceDecreaseProto   proto    =     (  ( ContainerResourceDecreasePBImpl )     ( ctx )  )  . getProto (  )  ;", "ctx    =    new   ContainerResourceDecreasePBImpl ( proto )  ;", "Assert . assertNull ( ctx . getCapability (  )  )  ;", "Assert . assertNull ( ctx . getContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceDecreaseContextWithNull"], "fileName": "org.apache.hadoop.yarn.api.TestContainerResourceDecrease"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    identifier    =    new   byte [  ]  {     1  ,     2  ,     3  ,     4     }  ;", "Token   token    =    Token . newInstance ( identifier ,     \"  \"  ,     \"  \"  . getBytes (  )  ,     \"  \"  )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  ,     3  )  ,     3  )  ,     7  )  ;", "Resource   resource    =    Resource . newInstance (  1  0  2  3  ,     3  )  ;", "ctx    =     . newInstance ( containerId ,    resource ,    token )  ;", "Proto   proto    =     (  ( PBImpl )     ( ctx )  )  . getProto (  )  ;", "ctx    =    new   PBImpl ( proto )  ;", "Assert . assertEquals ( ctx . getCapability (  )  ,    resource )  ;", "Assert . assertEquals ( ctx . getContainerId (  )  ,    containerId )  ;", "Assert . assertTrue ( Arrays . equals ( ctx . getContainerToken (  )  . getIdentifier (  )  . array (  )  ,    identifier )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceIncreaseContext"], "fileName": "org.apache.hadoop.yarn.api.TestContainerResourceIncrease"}, {"methodBody": ["METHOD_START", "{", "ContainerResourceIncrease   ctx    =    ContainerResourceIncrease . newInstance ( null ,    null ,    null )  ;", "ContainerResourceIncreaseProto   proto    =     (  ( ContainerResourceIncreasePBImpl )     ( ctx )  )  . getProto (  )  ;", "ctx    =    new   ContainerResourceIncreasePBImpl ( proto )  ;", "Assert . assertNull ( ctx . getContainerToken (  )  )  ;", "Assert . assertNull ( ctx . getCapability (  )  )  ;", "Assert . assertNull ( ctx . getContainerId (  )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceIncreaseContextWithNull"], "fileName": "org.apache.hadoop.yarn.api.TestContainerResourceIncrease"}, {"methodBody": ["METHOD_START", "{", "ContainerId   containerId    =    ContainerId . newInstance ( ApplicationAttemptId . newInstance ( ApplicationId . newInstance (  1  2  3  4  ,     3  )  ,     3  )  ,     7  )  ;", "Resource   resource    =    Resource . newInstance (  1  0  2  3  ,     3  )  ;", "context    =     . newInstance ( containerId ,    resource )  ;", "Proto   proto    =     (  ( PBImpl )     ( context )  )  . getProto (  )  ;", "contextRecover    =    new   PBImpl ( proto )  ;", "Assert . assertEquals ( contextRecover . getContainerId (  )  ,    containerId )  ;", "Assert . assertEquals ( contextRecover . getCapability (  )  ,    resource )  ;", "}", "METHOD_END"], "methodName": ["ContainerResourceIncreaseRequest"], "fileName": "org.apache.hadoop.yarn.api.TestContainerResourceIncreaseRequest"}, {"methodBody": ["METHOD_START", "{", "ContainerResourceIncreaseRequest   context    =    ContainerResourceIncreaseRequest . newInstance ( null ,    null )  ;", "ContainerResourceIncreaseRequestProto   proto    =     (  ( ContainerResourceIncreaseRequestPBImpl )     ( context )  )  . getProto (  )  ;", "ContainerResourceIncreaseRequest   contextRecover    =    new   ContainerResourceIncreaseRequestPBImpl ( proto )  ;", "Assert . assertNull ( contextRecover . getContainerId (  )  )  ;", "Assert . assertNull ( contextRecover . getCapability (  )  )  ;", "}", "METHOD_END"], "methodName": ["testResourceChangeContextWithNullField"], "fileName": "org.apache.hadoop.yarn.api.TestContainerResourceIncreaseRequest"}, {"methodBody": ["METHOD_START", "{", "GetApplicationsRequest   request    =    GetApplicationsRequest . newInstance (  )  ;", "EnumSet < YarnApplicationState >    appStates    =    EnumSet . of ( ACCEPTED )  ;", "request . setApplicationStates ( appStates )  ;", "Set < String >    tags    =    new   HashSet < String >  (  )  ;", "tags . add (  \" tag 1  \"  )  ;", "request . setApplicationTags ( tags )  ;", "Set < String >    types    =    new   HashSet < String >  (  )  ;", "types . add (  \" type 1  \"  )  ;", "request . setApplicationTypes ( types )  ;", "long   startBegin    =    System . currentTimeMillis (  )  ;", "long   startEnd    =     ( System . currentTimeMillis (  )  )     +     1  ;", "request . setStartRange ( startBegin ,    startEnd )  ;", "long   finishBegin    =     ( System . currentTimeMillis (  )  )     +     2  ;", "long   finishEnd    =     ( System . currentTimeMillis (  )  )     +     3  ;", "request . setFinishRange ( finishBegin ,    finishEnd )  ;", "long   limit    =     1  0  0 L ;", "request . setLimit ( limit )  ;", "Set < String >    queues    =    new   HashSet < String >  (  )  ;", "queues . add (  \" queue 1  \"  )  ;", "request . setQueues ( queues )  ;", "Set < String >    users    =    new   HashSet < String >  (  )  ;", "users . add (  \" user 1  \"  )  ;", "request . setUsers ( users )  ;", "ApplicationsRequestScope   scope    =    ApplicationsRequestScope . ALL ;", "request . setScope ( scope )  ;", "GetApplicationsRequest   requestFromProto    =    new   GetApplicationsRequestPBImpl (  (  ( GetApplicationsRequestPBImpl )     ( request )  )  . getProto (  )  )  ;", "Assert . assertEquals ( requestFromProto ,    request )  ;", "Assert . assertEquals (  \" ApplicationStates   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getApplicationStates (  )  ,    appStates )  ;", "Assert . assertEquals (  \" ApplicationTags   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getApplicationTags (  )  ,    tags )  ;", "Assert . assertEquals (  \" ApplicationTypes   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getApplicationTypes (  )  ,    types )  ;", "Assert . assertEquals (  \" StartRange   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getStartRange (  )  ,    new   LongRange ( startBegin ,    startEnd )  )  ;", "Assert . assertEquals (  \" FinishRange   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getFinishRange (  )  ,    new   LongRange ( finishBegin ,    finishEnd )  )  ;", "Assert . assertEquals (  \" Limit   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getLimit (  )  ,    limit )  ;", "Assert . assertEquals (  \" Queues   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getQueues (  )  ,    queues )  ;", "Assert . assertEquals (  \" Users   from   proto   is   not   the   same   with   original   request \"  ,    requestFromProto . getUsers (  )  ,    users )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationsRequest"], "fileName": "org.apache.hadoop.yarn.api.TestGetApplicationsRequest"}, {"methodBody": ["METHOD_START", "{", "NodeId   nodeId 1     =    NodeId . newInstance (  \"  1  0  .  1  8  .  5  2  .  1  2  4  \"  ,     8  0  4  1  )  ;", "NodeId   nodeId 2     =    NodeId . newInstance (  \"  1  0  .  1  8  .  5  2  .  1  2  5  \"  ,     8  0  3  8  )  ;", "NodeId   nodeId 3     =    NodeId . newInstance (  \"  1  0  .  1  8  .  5  2  .  1  2  4  \"  ,     8  0  4  1  )  ;", "NodeId   nodeId 4     =    NodeId . newInstance (  \"  1  0  .  1  8  .  5  2  .  1  2  4  \"  ,     8  0  3  9  )  ;", "Assert . assertTrue ( nodeId 1  . equals ( nodeId 3  )  )  ;", "Assert . assertFalse ( nodeId 1  . equals ( nodeId 2  )  )  ;", "Assert . assertFalse ( nodeId 3  . equals ( nodeId 4  )  )  ;", "Assert . assertTrue (  (  ( nodeId 1  . compareTo ( nodeId 3  )  )     =  =     0  )  )  ;", "Assert . assertTrue (  (  ( nodeId 1  . compareTo ( nodeId 2  )  )     <     0  )  )  ;", "Assert . assertTrue (  (  ( nodeId 3  . compareTo ( nodeId 4  )  )     >     0  )  )  ;", "Assert . assertTrue (  (  ( nodeId 1  . hashCode (  )  )     =  =     ( nodeId 3  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( nodeId 1  . hashCode (  )  )     =  =     ( nodeId 2  . hashCode (  )  )  )  )  ;", "Assert . assertFalse (  (  ( nodeId 3  . hashCode (  )  )     =  =     ( nodeId 4  . hashCode (  )  )  )  )  ;", "Assert . assertEquals (  \"  1  0  .  1  8  .  5  2  .  1  2  4  :  8  0  4  1  \"  ,    nodeId 1  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testNodeId"], "fileName": "org.apache.hadoop.yarn.api.TestNodeId"}, {"methodBody": ["METHOD_START", "{", "Object   ret    =    TestPBImplRecords . typeValueCache . get ( type )  ;", "if    ( ret    !  =    null )     {", "return   ret ;", "}", "if    ( type . equals ( boolean . class )  )     {", "return   TestPBImplRecords . rand . nextBoolean (  )  ;", "} else", "if    ( type . equals ( byte . class )  )     {", "return   TestPBImplRecords . bytes [ TestPBImplRecords . rand . nextInt (  4  )  ]  ;", "} else", "if    ( type . equals ( int . class )  )     {", "return   TestPBImplRecords . rand . nextInt (  1  0  0  0  0  0  0  )  ;", "} else", "if    ( type . equals ( long . class )  )     {", "return   Long . valueOf ( TestPBImplRecords . rand . nextInt (  1  0  0  0  0  0  0  )  )  ;", "} else", "if    ( type . equals ( float . class )  )     {", "return   TestPBImplRecords . rand . nextFloat (  )  ;", "} else", "if    ( type . equals ( double . class )  )     {", "return   TestPBImplRecords . rand . nextDouble (  )  ;", "} else", "if    ( type . equals ( String . class )  )     {", "return   String . format (  \"  % c % c % c \"  ,     (  ' a '     +     ( TestPBImplRecords . rand . nextInt (  2  6  )  )  )  ,     (  ' a '     +     ( TestPBImplRecords . rand . nextInt (  2  6  )  )  )  ,     (  ' a '     +     ( TestPBImplRecords . rand . nextInt (  2  6  )  )  )  )  ;", "} else", "if    ( type   instanceof   Class )     {", "Class   clazz    =     (  ( Class )     ( type )  )  ;", "if    ( clazz . isArray (  )  )     {", "Class   compClass    =    clazz . getComponentType (  )  ;", "if    ( compClass    !  =    null )     {", "ret    =    newInstance ( compClass ,     2  )  ;", "set ( ret ,     0  ,    TestPBImplRecords . genTypeValue ( compClass )  )  ;", "set ( ret ,     1  ,    TestPBImplRecords . genTypeValue ( compClass )  )  ;", "}", "} else", "if    ( clazz . isEnum (  )  )     {", "Object [  ]    values    =    clazz . getEnumConstants (  )  ;", "ret    =    values [ TestPBImplRecords . rand . nextInt ( values . length )  ]  ;", "} else", "if    ( clazz . equals ( class )  )     {", "ByteBuffer   buff    =    allocate (  4  )  ;", "TestPBImplRecords . rand . nextBytes ( buff . array (  )  )  ;", "return   buff ;", "}", "} else", "if    ( type   instanceof   ParameterizedType )     {", "ParameterizedType   pt    =     (  ( ParameterizedType )     ( type )  )  ;", "Type   rawType    =    pt . getRawType (  )  ;", "Type [  ]    params    =    pt . getActualTypeArguments (  )  ;", "if    ( rawType . equals ( class )  )     {", "if    (  ( params [  0  ]  )    instanceof   Class )     {", "Class   c    =     (  ( Class )     ( params [  0  ]  )  )  ;", "return   allOf ( c )  ;", "}", "}", "if    ( rawType . equals ( class )  )     {", "ret    =    com . google . common . collect . Lists . newArrayList ( TestPBImplRecords . genTypeValue ( params [  0  ]  )  )  ;", "} else", "if    ( rawType . equals ( class )  )     {", "ret    =    com . google . common . collect . Sets . newHashSet ( TestPBImplRecords . genTypeValue ( params [  0  ]  )  )  ;", "} else", "if    ( rawType . equals ( class )  )     {", "Map < Object ,    Object >    map    =    com . google . common . collect . Maps . newHashMap (  )  ;", "map . put ( TestPBImplRecords . genTypeValue ( params [  0  ]  )  ,    TestPBImplRecords . genTypeValue ( params [  1  ]  )  )  ;", "ret    =    map ;", "}", "}", "if    ( ret    =  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" type    \"     +    type )     +     \"    is   not   supported \"  )  )  ;", "}", "TestPBImplRecords . typeValueCache . put ( type ,    ret )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["genTypeValue"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "Object   ret    =    TestPBImplRecords . typeValueCache . get ( clazz )  ;", "if    ( ret    !  =    null )     {", "return   ret ;", "}", "Method   newInstance    =    null ;", "Type [  ]    paramTypes    =    new   Type [  0  ]  ;", "for    ( Method   m    :    clazz . getMethods (  )  )     {", "int   mod    =    m . getModifiers (  )  ;", "if    (  (  (  ( m . getDeclaringClass (  )  . equals ( clazz )  )     &  &     ( Modifier . isPublic ( mod )  )  )     &  &     ( Modifier . isStatic ( mod )  )  )     &  &     ( m . getName (  )  . equals (  \" newInstance \"  )  )  )     {", "Type [  ]    pts    =    m . getGenericParameterTypes (  )  ;", "if    (  ( newInstance    =  =    null )     |  |     (  ( pts . length )     >     ( paramTypes . length )  )  )     {", "newInstance    =    m ;", "paramTypes    =    pts ;", "}", "}", "}", "if    ( newInstance    =  =    null )     {", "throw   new   IllegalArgumentException (  (  (  \" type    \"     +     ( clazz . getName (  )  )  )     +     \"    does   not   have   newInstance   method \"  )  )  ;", "}", "Object [  ]    args    =    new   Object [ paramTypes . length ]  ;", "for    ( int   i    =     0  ;    i    <     ( args . length )  ;    i +  +  )     {", "args [ i ]     =    TestPBImplRecords . genTypeValue ( paramTypes [ i ]  )  ;", "}", "ret    =    newInstance . invoke ( null ,    args )  ;", "TestPBImplRecords . typeValueCache . put ( clazz ,    ret )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["generateByNewInstance"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    TestPBImplRecords . GetSetPair >    ret    =    new   HashMap < String ,    TestPBImplRecords . GetSetPair >  (  )  ;", "Method [  ]    methods    =    recordClass . getDeclaredMethods (  )  ;", "for    ( int   i    =     0  ;    i    <     ( methods . length )  ;    i +  +  )     {", "Method   m    =    methods [ i ]  ;", "int   mod    =    m . getModifiers (  )  ;", "if    (  (  ( m . getDeclaringClass (  )  . equals ( recordClass )  )     &  &     ( Modifier . isPublic ( mod )  )  )     &  &     (  !  ( Modifier . isStatic ( mod )  )  )  )     {", "String   name    =    m . getName (  )  ;", "if    ( name . equals (  \" getProto \"  )  )     {", "continue ;", "}", "if    (  (  (  ( name . length (  )  )     >     3  )     &  &     ( name . startsWith (  \" get \"  )  )  )     &  &     (  ( m . getParameterTypes (  )  . length )     =  =     0  )  )     {", "String   propertyName    =    name . substring (  3  )  ;", "Type   valueType    =    m . getGenericReturnType (  )  ;", "TestPBImplRecords . GetSetPair   p    =    ret . get ( propertyName )  ;", "if    ( p    =  =    null )     {", "p    =    new   TestPBImplRecords . GetSetPair (  )  ;", "p . propertyName    =    propertyName ;", "p . type    =    valueType ;", "p . getMethod    =    m ;", "ret . put ( propertyName ,    p )  ;", "} else    {", "Assert . fail (  (  (  \" Multiple   get   method   with   same   name :     \"     +    recordClass )     +     ( p . propertyName )  )  )  ;", "}", "}", "}", "}", "for    ( int   i    =     0  ;    i    <     ( methods . length )  ;    i +  +  )     {", "Method   m    =    methods [ i ]  ;", "int   mod    =    m . getModifiers (  )  ;", "if    (  (  ( m . getDeclaringClass (  )  . equals ( recordClass )  )     &  &     ( Modifier . isPublic ( mod )  )  )     &  &     (  !  ( Modifier . isStatic ( mod )  )  )  )     {", "String   name    =    m . getName (  )  ;", "if    (  ( name . startsWith (  \" set \"  )  )     &  &     (  ( m . getParameterTypes (  )  . length )     =  =     1  )  )     {", "String   propertyName    =    name . substring (  3  )  ;", "Type   valueType    =    m . getGenericParameterTypes (  )  [  0  ]  ;", "TestPBImplRecords . GetSetPair   p    =    ret . get ( propertyName )  ;", "if    (  ( p    !  =    null )     &  &     ( p . type . equals ( valueType )  )  )     {", "p . setMethod    =    m ;", "}", "}", "}", "}", "Iterator < Map . Entry < String ,    TestPBImplRecords . GetSetPair >  >    itr    =    ret . entrySet (  )  . iterator (  )  ;", "while    ( itr . hasNext (  )  )     {", "Map . Entry < String ,    TestPBImplRecords . GetSetPair >    cur    =    itr . next (  )  ;", "TestPBImplRecords . GetSetPair   gsp    =    cur . getValue (  )  ;", "if    (  (  ( gsp . getMethod )     =  =    null )     |  |     (  ( gsp . setMethod )     =  =    null )  )     {", "TestPBImplRecords . LOG . info ( String . format (  \" Exclude   protential   property :     % s \\ n \"  ,    gsp . propertyName )  )  ;", "itr . remove (  )  ;", "} else    {", "TestPBImplRecords . LOG . info ( String . format (  \" New   property :     % s   type :     % s \"  ,    gsp . toString (  )  ,    gsp . type )  )  ;", "gsp . testValue    =    TestPBImplRecords . genTypeValue ( gsp . type )  ;", "TestPBImplRecords . LOG . info ( String . format (  \"    testValue :     % s \\ n \"  ,    gsp . testValue )  )  ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getGetSetPairs"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "TestPBImplRecords . typeValueCache . put ( LongRange . class ,    new   LongRange (  1  0  0  0  ,     2  0  0  0  )  )  ;", "TestPBImplRecords . typeValueCache . put ( URL . class ,    URL . newInstance (  \" http \"  ,     \" localhost \"  ,     8  0  8  0  ,     \" file 0  \"  )  )  ;", "TestPBImplRecords . typeValueCache . put ( SerializedException . class ,    SerializedException . newInstance ( new   IOException (  \" exception   for   test \"  )  )  )  ;", "TestPBImplRecords . generateByNewInstance ( ApplicationId . class )  ;", "TestPBImplRecords . generateByNewInstance ( ApplicationAttemptId . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerId . class )  ;", "TestPBImplRecords . generateByNewInstance ( Resource . class )  ;", "TestPBImplRecords . generateByNewInstance ( ResourceBlacklistRequest . class )  ;", "TestPBImplRecords . generateByNewInstance ( ResourceOption . class )  ;", "TestPBImplRecords . generateByNewInstance ( LocalResource . class )  ;", "TestPBImplRecords . generateByNewInstance ( Priority . class )  ;", "TestPBImplRecords . generateByNewInstance ( NodeId . class )  ;", "TestPBImplRecords . generateByNewInstance ( NodeReport . class )  ;", "TestPBImplRecords . generateByNewInstance ( Token . class )  ;", "TestPBImplRecords . generateByNewInstance ( NMToken . class )  ;", "TestPBImplRecords . generateByNewInstance ( ResourceRequest . class )  ;", "TestPBImplRecords . generateByNewInstance ( ApplicationAttemptReport . class )  ;", "TestPBImplRecords . generateByNewInstance ( ApplicationResourceUsageReport . class )  ;", "TestPBImplRecords . generateByNewInstance ( ApplicationReport . class )  ;", "TestPBImplRecords . generateByNewInstance ( Container . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerLaunchContext . class )  ;", "TestPBImplRecords . generateByNewInstance ( ApplicationSubmissionContext . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerReport . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerResourceDecrease . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerResourceIncrease . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerResourceIncreaseRequest . class )  ;", "TestPBImplRecords . generateByNewInstance ( ContainerStatus . class )  ;", "TestPBImplRecords . generateByNewInstance ( PreemptionContainer . class )  ;", "TestPBImplRecords . generateByNewInstance ( PreemptionResourceRequest . class )  ;", "TestPBImplRecords . generateByNewInstance ( PreemptionContainer . class )  ;", "TestPBImplRecords . generateByNewInstance ( PreemptionContract . class )  ;", "TestPBImplRecords . generateByNewInstance ( StrictPreemptionContract . class )  ;", "TestPBImplRecords . generateByNewInstance ( PreemptionMessage . class )  ;", "TestPBImplRecords . generateByNewInstance ( StartContainerRequest . class )  ;", "TestPBImplRecords . typeValueCache . put ( QueueInfo . class ,    QueueInfo . newInstance (  \" root \"  ,     1  .  0 F ,     1  .  0 F ,     0  .  1 F ,    null ,    null ,    RUNNING )  )  ;", "TestPBImplRecords . generateByNewInstance ( QueueUserACLInfo . class )  ;", "TestPBImplRecords . generateByNewInstance ( YarnClusterMetrics . class )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( AllocateRequestPBImpl . class ,    AllocateRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testAllocateRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( AllocateResponsePBImpl . class ,    AllocateResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testAllocateResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ApplicationAttemptIdPBImpl . class ,    ApplicationAttemptIdProto . class )  ;", "}", "METHOD_END"], "methodName": ["testApplicationAttemptIdPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ApplicationAttemptReportPBImpl . class ,    ApplicationAttemptReportProto . class )  ;", "}", "METHOD_END"], "methodName": ["testApplicationAttemptReportPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ApplicationIdPBImpl . class ,    ApplicationIdProto . class )  ;", "}", "METHOD_END"], "methodName": ["testApplicationIdPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ApplicationReportPBImpl . class ,    ApplicationReportProto . class )  ;", "}", "METHOD_END"], "methodName": ["testApplicationReportPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ApplicationResourceUsageReportPBImpl . class ,    ApplicationResourceUsageReportProto . class )  ;", "}", "METHOD_END"], "methodName": ["testApplicationResourceUsageReportPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ApplicationSubmissionContextPBImpl . class ,    ApplicationSubmissionContextProto . class )  ;", "}", "METHOD_END"], "methodName": ["testApplicationSubmissionContextPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( CancelDelegationTokenRequestPBImpl . class ,    CancelDelegationTokenRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testCancelDelegationTokenRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( CancelDelegationTokenResponsePBImpl . class ,    CancelDelegationTokenResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testCancelDelegationTokenResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerIdPBImpl . class ,    ContainerIdProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerIdPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerLaunchContextPBImpl . class ,    ContainerLaunchContextProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerLaunchContextPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerPBImpl . class ,    ContainerProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerReportPBImpl . class ,    ContainerReportProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerReportPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerResourceDecreasePBImpl . class ,    ContainerResourceDecreaseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerResourceDecreasePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerResourceIncreasePBImpl . class ,    ContainerResourceIncreaseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerResourceIncreasePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerResourceIncreaseRequestPBImpl . class ,    ContainerResourceIncreaseRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerResourceIncreaseRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ContainerStatusPBImpl . class ,    ContainerStatusProto . class )  ;", "}", "METHOD_END"], "methodName": ["testContainerStatusPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( FinishApplicationMasterRequestPBImpl . class ,    FinishApplicationMasterRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testFinishApplicationMasterRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( FinishApplicationMasterResponsePBImpl . class ,    FinishApplicationMasterResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testFinishApplicationMasterResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationAttemptReportRequestPBImpl . class ,    GetApplicationAttemptReportRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationAttemptReportRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationAttemptReportResponsePBImpl . class ,    GetApplicationAttemptReportResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationAttemptReportResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationAttemptsRequestPBImpl . class ,    GetApplicationAttemptsRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationAttemptsRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationAttemptsResponsePBImpl . class ,    GetApplicationAttemptsResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationAttemptsResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationReportRequestPBImpl . class ,    GetApplicationReportRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationReportRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationReportResponsePBImpl . class ,    GetApplicationReportResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationReportResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationsRequestPBImpl . class ,    GetApplicationsRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationsRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetApplicationsResponsePBImpl . class ,    GetApplicationsResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetApplicationsResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetClusterMetricsRequestPBImpl . class ,    GetClusterMetricsRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetClusterMetricsRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetClusterMetricsResponsePBImpl . class ,    GetClusterMetricsResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetClusterMetricsResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetClusterNodesRequestPBImpl . class ,    GetClusterNodesRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetClusterNodesRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetClusterNodesResponsePBImpl . class ,    GetClusterNodesResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetClusterNodesResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetContainerReportRequestPBImpl . class ,    GetContainerReportRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetContainerReportRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetContainerReportResponsePBImpl . class ,    GetContainerReportResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetContainerReportResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetContainerStatusesRequestPBImpl . class ,    GetContainerStatusesRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetContainerStatusesRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetContainerStatusesResponsePBImpl . class ,    GetContainerStatusesResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetContainerStatusesResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetContainersRequestPBImpl . class ,    GetContainersRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetContainersRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetContainersResponsePBImpl . class ,    GetContainersResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetContainersResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetDelegationTokenRequestPBImpl . class ,    GetDelegationTokenRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetDelegationTokenRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetDelegationTokenResponsePBImpl . class ,    GetDelegationTokenResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetDelegationTokenResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetNewApplicationRequestPBImpl . class ,    GetNewApplicationRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetNewApplicationRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetNewApplicationResponsePBImpl . class ,    GetNewApplicationResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetNewApplicationResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetQueueInfoRequestPBImpl . class ,    GetQueueInfoRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetQueueInfoRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetQueueInfoResponsePBImpl . class ,    GetQueueInfoResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetQueueInfoResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetQueueUserAclsInfoRequestPBImpl . class ,    GetQueueUserAclsInfoRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetQueueUserAclsInfoRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( GetQueueUserAclsInfoResponsePBImpl . class ,    GetQueueUserAclsInfoResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testGetQueueUserAclsInfoResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( KillApplicationRequestPBImpl . class ,    KillApplicationRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testKillApplicationRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( KillApplicationResponsePBImpl . class ,    KillApplicationResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testKillApplicationResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( LocalResourcePBImpl . class ,    LocalResourceProto . class )  ;", "}", "METHOD_END"], "methodName": ["testLocalResourcePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( MoveApplicationAcrossQueuesRequestPBImpl . class ,    MoveApplicationAcrossQueuesRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testMoveApplicationAcrossQueuesRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( MoveApplicationAcrossQueuesResponsePBImpl . class ,    MoveApplicationAcrossQueuesResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testMoveApplicationAcrossQueuesResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( NMTokenPBImpl . class ,    NMTokenProto . class )  ;", "}", "METHOD_END"], "methodName": ["testNMTokenPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( NodeIdPBImpl . class ,    NodeIdProto . class )  ;", "}", "METHOD_END"], "methodName": ["testNodeIdPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( NodeReportPBImpl . class ,    NodeReportProto . class )  ;", "}", "METHOD_END"], "methodName": ["testNodeReportPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( PreemptionContainerPBImpl . class ,    PreemptionContainerProto . class )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionContainerPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( PreemptionContractPBImpl . class ,    PreemptionContractProto . class )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionContractPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( PreemptionMessagePBImpl . class ,    PreemptionMessageProto . class )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionMessagePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( PreemptionResourceRequestPBImpl . class ,    PreemptionResourceRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testPreemptionResourceRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( PriorityPBImpl . class ,    PriorityProto . class )  ;", "}", "METHOD_END"], "methodName": ["testPriorityPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( QueueInfoPBImpl . class ,    QueueInfoProto . class )  ;", "}", "METHOD_END"], "methodName": ["testQueueInfoPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( QueueUserACLInfoPBImpl . class ,    QueueUserACLInfoProto . class )  ;", "}", "METHOD_END"], "methodName": ["testQueueUserACLInfoPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshAdminAclsRequestPBImpl . class ,    RefreshAdminAclsRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshAdminAclsRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshAdminAclsResponsePBImpl . class ,    RefreshAdminAclsResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshAdminAclsResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshNodesRequestPBImpl . class ,    RefreshNodesRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshNodesRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshNodesResponsePBImpl . class ,    RefreshNodesResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshNodesResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshQueuesRequestPBImpl . class ,    RefreshQueuesRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshQueuesRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshQueuesResponsePBImpl . class ,    RefreshQueuesResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshQueuesResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshServiceAclsRequestPBImpl . class ,    RefreshServiceAclsRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshServiceAclsRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshServiceAclsResponsePBImpl . class ,    RefreshServiceAclsResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshServiceAclsResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshSuperUserGroupsConfigurationRequestPBImpl . class ,    RefreshSuperUserGroupsConfigurationRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshSuperUserGroupsConfigurationRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshSuperUserGroupsConfigurationResponsePBImpl . class ,    RefreshSuperUserGroupsConfigurationResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshSuperUserGroupsConfigurationResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshUserToGroupsMappingsRequestPBImpl . class ,    RefreshUserToGroupsMappingsRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshUserToGroupsMappingsRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RefreshUserToGroupsMappingsResponsePBImpl . class ,    RefreshUserToGroupsMappingsResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRefreshUserToGroupsMappingsResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RegisterApplicationMasterRequestPBImpl . class ,    RegisterApplicationMasterRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRegisterApplicationMasterRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RegisterApplicationMasterResponsePBImpl . class ,    RegisterApplicationMasterResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRegisterApplicationMasterResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RenewDelegationTokenRequestPBImpl . class ,    RenewDelegationTokenRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRenewDelegationTokenRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( RenewDelegationTokenResponsePBImpl . class ,    RenewDelegationTokenResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testRenewDelegationTokenResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ResourceBlacklistRequestPBImpl . class ,    ResourceBlacklistRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testResourceBlacklistRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ResourceOptionPBImpl . class ,    ResourceOptionProto . class )  ;", "}", "METHOD_END"], "methodName": ["testResourceOptionPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ResourcePBImpl . class ,    ResourceProto . class )  ;", "}", "METHOD_END"], "methodName": ["testResourcePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( ResourceRequestPBImpl . class ,    ResourceRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testResourceRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( SerializedExceptionPBImpl . class ,    SerializedExceptionProto . class )  ;", "}", "METHOD_END"], "methodName": ["testSerializedExceptionPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( StartContainerRequestPBImpl . class ,    StartContainerRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testStartContainerRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( StartContainersRequestPBImpl . class ,    StartContainersRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testStartContainersRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( StartContainersResponsePBImpl . class ,    StartContainersResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testStartContainersResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( StopContainersRequestPBImpl . class ,    StopContainersRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testStopContainersRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( StopContainersResponsePBImpl . class ,    StopContainersResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testStopContainersResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( StrictPreemptionContractPBImpl . class ,    StrictPreemptionContractProto . class )  ;", "}", "METHOD_END"], "methodName": ["testStrictPreemptionContractPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( SubmitApplicationRequestPBImpl . class ,    SubmitApplicationRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testSubmitApplicationRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( SubmitApplicationResponsePBImpl . class ,    SubmitApplicationResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testSubmitApplicationResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( TokenPBImpl . class ,    TokenProto . class )  ;", "}", "METHOD_END"], "methodName": ["testTokenPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( URLPBImpl . class ,    URLProto . class )  ;", "}", "METHOD_END"], "methodName": ["testURLPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( UpdateNodeResourceRequestPBImpl . class ,    UpdateNodeResourceRequestProto . class )  ;", "}", "METHOD_END"], "methodName": ["testUpdateNodeResourceRequestPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( UpdateNodeResourceResponsePBImpl . class ,    UpdateNodeResourceResponseProto . class )  ;", "}", "METHOD_END"], "methodName": ["testUpdateNodeResourceResponsePBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "validatePBImplRecord ( YarnClusterMetricsPBImpl . class ,    YarnClusterMetricsProto . class )  ;", "}", "METHOD_END"], "methodName": ["testYarnClusterMetricsPBImpl"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "TestPBImplRecords . LOG . info ( String . format (  \" Validate    % s    % s \\ n \"  ,    recordClass . getName (  )  ,    protoClass . getName (  )  )  )  ;", "Constructor < R >    emptyConstructor    =    recordClass . getConstructor (  )  ;", "Constructor < R >    pbConstructor    =    recordClass . getConstructor ( protoClass )  ;", "Method   getProto    =    recordClass . getDeclaredMethod (  \" getProto \"  )  ;", "Map < String ,    TestPBImplRecords . GetSetPair >    getSetPairs    =    getGetSetPairs ( recordClass )  ;", "R   origRecord    =    emptyConstructor . newInstance (  )  ;", "for    ( TestPBImplRecords . GetSetPair   gsp    :    getSetPairs . values (  )  )     {", "gsp . setMethod . invoke ( origRecord ,    gsp . testValue )  ;", "}", "Object   ret    =    getProto . invoke ( origRecord )  ;", "Assert . assertNotNull (  (  ( recordClass . getName (  )  )     +     \"  # getProto   returns   null \"  )  ,    ret )  ;", "if    (  !  ( protoClass . isAssignableFrom ( ret . getClass (  )  )  )  )     {", "Assert . fail (  (  \" Illegal   getProto   method   return   type :     \"     +     ( ret . getClass (  )  )  )  )  ;", "}", "R   deserRecord    =    pbConstructor . newInstance ( ret )  ;", "Assert . assertEquals (  (  (  \" whole    \"     +    recordClass )     +     \"    records   should   be   equal \"  )  ,    origRecord ,    deserRecord )  ;", "for    ( TestPBImplRecords . GetSetPair   gsp    :    getSetPairs . values (  )  )     {", "Object   origValue    =    gsp . getMethod . invoke ( origRecord )  ;", "Object   deserValue    =    gsp . getMethod . invoke ( deserRecord )  ;", "Assert . assertEquals (  (  (  (  (  \" property    \"     +     ( recordClass . getName (  )  )  )     +     \"  #  \"  )     +     ( gsp . propertyName )  )     +     \"    should   be   equal \"  )  ,    origValue ,    deserValue )  ;", "}", "}", "METHOD_END"], "methodName": ["validatePBImplRecord"], "fileName": "org.apache.hadoop.yarn.api.TestPBImplRecords"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearAsk (  )  ;", "if    (  ( ask )     =  =    null )", "return ;", "Iterable < Resourcroto >    iterable    =    new   Iterable < Resourcroto >  (  )     {", "@ Override", "public   Iterator < Resourcroto >    iterator (  )     {", "return   new   Iterator < Resourcroto >  (  )     {", "Iterator < ResourceRequest >    iter    =    ask . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   Resourcroto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllAsk ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addAsksToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearIncreaseRequest (  )  ;", "if    (  ( increaseRequests )     =  =    null )     {", "return ;", "}", "Iterable < ContainerResourceIncreasroto >    iterable    =    new   Iterable < ContainerResourceIncreasroto >  (  )     {", "@ Override", "public   Iterator < ContainerResourceIncreasroto >    iterator (  )     {", "return   new   Iterator < ContainerResourceIncreasroto >  (  )     {", "Iterator < ContainerResourceIncreaseRequest >    iter    =    increaseRequests . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ContainerResourceIncreasroto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllIncreaseRequest ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addIncreaseRequestsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearRelease (  )  ;", "if    (  ( release )     =  =    null )", "return ;", "Iterable < ContainerIdP >    iterable    =    new   Iterable < ContainerIdP >  (  )     {", "@ Override", "public   Iterator < ContainerIdP >    iterator (  )     {", "return   new   Iterator < ContainerIdP >  (  )     {", "Iterator < ContainerId >    iter    =    release . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ContainerIdP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllRelease ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addReleasesToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerResourceIncreaseRequestPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceBlacklistRequestPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceRequestPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerResourceIncreaseRequestPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourceBlacklistRequestPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourceRequestPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . ask )     !  =    null )     {", "return ;", "}", "AllocateRequestProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ResourceRequestProto >    list    =    p . getAskList (  )  ;", "this . ask    =    new   ArrayList < records . ResourceRequest >  (  )  ;", "for    ( ResourceRequestProto   c    :    list )     {", "this . ask . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initAsks"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . increaseRequests )     !  =    null )     {", "return ;", "}", "AllocateRequestProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerResourceIncreaseRequestProto >    list    =    p . getIncreaseRequestList (  )  ;", "this . increaseRequests    =    new   ArrayList < records . ContainerResourceIncreaseRequest >  (  )  ;", "for    ( ContainerResourceIncreaseRequestProto   c    :    list )     {", "this . increaseRequests . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initIncreaseRequests"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . release )     !  =    null )     {", "return ;", "}", "AllocateRequestProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerIdProto >    list    =    p . getReleaseList (  )  ;", "this . release    =    new   ArrayList < records . ContainerId >  (  )  ;", "for    ( ContainerIdProto   c    :    list )     {", "this . release . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initReleases"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . ask )     !  =    null )     {", "addAsksToProto (  )  ;", "}", "if    (  ( this . release )     !  =    null )     {", "addReleasesToProto (  )  ;", "}", "if    (  ( this . increass )     !  =    null )     {", "addIncreassToProto (  )  ;", "}", "if    (  ( this . blacklistRequest )     !  =    null )     {", "builder . setBlacklistRequest ( convertToProtoFormat ( this . blacklistRequest )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerResourceDecreasePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerResourceIncreasePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerStatusPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PreemptionMessagePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NMTokenPBImpl ( proto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerResourceDecreasePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerResourceIncreasePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerStatusPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NMTokenPBImpl )     ( token )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PreemptionMessagePBImpl )     ( r )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( r )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < ContainerResourceDecreaseP >  (  )     {", "@ Override", "public   synchronized   Iterator < ContainerResourceDecreaseP >    iterator (  )     {", "return   new   Iterator < ContainerResourceDecreaseP >  (  )     {", "Iterator < ContainerResourceDecrease >    iter    =    newContainersList . iterator (  )  ;", "@ Override", "public   synchronized   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   synchronized   ContainerResourceDecreaseP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   synchronized   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getChangeProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < ContainerP >  (  )     {", "@ Override", "public   synchronized   Iterator < ContainerP >    iterator (  )     {", "return   new   Iterator < ContainerP >  (  )     {", "Iterator < Container >    iter    =    newContainersList . iterator (  )  ;", "@ Override", "public   synchronized   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   synchronized   ContainerP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   synchronized   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getContainerProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < ContainerStatusP >  (  )     {", "@ Override", "public   synchronized   Iterator < ContainerStatusP >    iterator (  )     {", "return   new   Iterator < ContainerStatusP >  (  )     {", "Iterator < ContainerStatus >    iter    =    newContainersList . iterator (  )  ;", "@ Override", "public   synchronized   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   synchronized   ContainerStatusP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   synchronized   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getContainerStatusProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < ContainerResourceIncreaseP >  (  )     {", "@ Override", "public   synchronized   Iterator < ContainerResourceIncreaseP >    iterator (  )     {", "return   new   Iterator < ContainerResourceIncreaseP >  (  )     {", "Iterator < ContainerResourceIncrease >    iter    =    newContainersList . iterator (  )  ;", "@ Override", "public   synchronized   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   synchronized   ContainerResourceIncreaseP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   synchronized   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getIncreaseProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < NodeReportP >  (  )     {", "@ Override", "public   synchronized   Iterator < NodeReportP >    iterator (  )     {", "return   new   Iterator < NodeReportP >  (  )     {", "Iterator < NodeReport >    iter    =    newNodeReportsList . iterator (  )  ;", "@ Override", "public   synchronized   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   synchronized   NodeReportP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   synchronized   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getNodeReportProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < NMTokenP >  (  )     {", "@ Override", "public   synchronized   Iterator < NMTokenP >    iterator (  )     {", "return   new   Iterator < NMTokenP >  (  )     {", "Iterator < NMToken >    iter    =    nmTokenList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   NMTokenP   next (  )     {", "return   convertToPFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getTokenProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . decreasedContainers )     !  =    null )     {", "return ;", "}", "AllocateResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerResourceDecreaseProto >    list    =    p . getDecreasedContainersList (  )  ;", "decreasedContainers    =    new   ArrayList < records . ContainerResourceDecrease >  (  )  ;", "for    ( ContainerResourceDecreaseProto   c    :    list )     {", "decreasedContainers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalDecreasedContainerList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . completedContainersStatuses )     !  =    null )     {", "return ;", "}", "AllocateResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerStatusProto >    list    =    p . getCompletedContainerStatusesList (  )  ;", "completedContainersStatuses    =    new   ArrayList < records . ContainerStatus >  (  )  ;", "for    ( ContainerStatusProto   c    :    list )     {", "completedContainersStatuses . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalFinishedContainerList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . increasedContainers )     !  =    null )     {", "return ;", "}", "AllocateResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerResourceIncreaseProto >    list    =    p . getIncreasedContainersList (  )  ;", "increasedContainers    =    new   ArrayList < records . ContainerResourceIncrease >  (  )  ;", "for    ( ContainerResourceIncreaseProto   c    :    list )     {", "increasedContainers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalIncreasedContainerList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . allocatedContainers )     !  =    null )     {", "return ;", "}", "AllocateResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerProto >    list    =    p . getAllocatedContainersList (  )  ;", "allocatedContainers    =    new   ArrayList < records . Container >  (  )  ;", "for    ( ContainerProto   c    :    list )     {", "allocatedContainers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalNewContainerList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( nmTokens )     !  =    null )     {", "return ;", "}", "AllocateResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NMTokenProto >    list    =    p . getNmTokensList (  )  ;", "nmTokens    =    new   ArrayList < records . NMToken >  (  )  ;", "for    ( NMTokenProto   t    :    list )     {", "nmTokens . add ( convertFromProtoFormat ( t )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalNewNMTokenList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . updatedNodes )     !  =    null )     {", "return ;", "}", "AllocateResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NodeReportProto >    list    =    p . getUpdatedNodesList (  )  ;", "updatedNodes    =    new   ArrayList < records . NodeReport >  ( list . size (  )  )  ;", "for    ( NodeReportProto   n    :    list )     {", "updatedNodes . add ( convertFromProtoFormat ( n )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalNewNodeReportList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . allocatedContainers )     !  =    null )     {", "builder . cleardContainers (  )  ;", "Iterable < ContainerProto >    iterable    =    getContainerProtoIterable ( this . allocatedContainers )  ;", "builder . addAlldContainers ( iterable )  ;", "}", "if    (  ( nmTokens )     !  =    null )     {", "builder . clearNmTokens (  )  ;", "Iterable < NMTokenProto >    iterable    =    getTokenProtoIterable ( nmTokens )  ;", "builder . addAllNmTokens ( iterable )  ;", "}", "if    (  ( this . completedContainersStatuses )     !  =    null )     {", "builder . clearCompletedContainerStatuses (  )  ;", "Iterable < ContainerStatusProto >    iterable    =    getContainerStatusProtoIterable ( this . completedContainersStatuses )  ;", "builder . addAllCompletedContainerStatuses ( iterable )  ;", "}", "if    (  ( this . updatedNodes )     !  =    null )     {", "builder . clearUpdatedNodes (  )  ;", "Iterable < NodeReportProto >    iterable    =    getNodeReportProtoIterable ( this . updatedNodes )  ;", "builder . addAllUpdatedNodes ( iterable )  ;", "}", "if    (  ( this . limit )     !  =    null )     {", "builder . setLimit ( convertToProtoFormat ( this . limit )  )  ;", "}", "if    (  ( this . preempt )     !  =    null )     {", "builder . setPreempt ( convertToProtoFormat ( this . preempt )  )  ;", "}", "if    (  ( this . increasedContainers )     !  =    null )     {", "builder . clearIncreasedContainers (  )  ;", "Iterable < ContainerResourceIncreaseProto >    iterable    =    getIncreaseProtoIterable ( this . increasedContainers )  ;", "builder . addAllIncreasedContainers ( iterable )  ;", "}", "if    (  ( this . decreasedContainers )     !  =    null )     {", "builder . clearDecreasedContainers (  )  ;", "Iterable < ContainerResourceDecreaseProto >    iterable    =    getChangeProtoIterable ( this . decreasedContainers )  ;", "builder . addAllDecreasedContainers ( iterable )  ;", "}", "if    (  ( this . amrmToken )     !  =    null )     {", "builder . setAmRmToken ( convertToProtoFormat ( this . amrmToken )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( token )     !  =    null )     {", "builder . set ( convertToProtoFormat ( this . token )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.CancelDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.FinishApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( applicationAttemptId )     !  =    null )     {", "builder . sId ( convertToProtoFormat ( this . applicationAttemptId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationAttemptReport )     !  =    null )     {", "builder . s ( convertToProtoFormat ( this . applicationAttemptReport )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( applicationId )     !  =    null )     {", "builder . sId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clear (  )  ;", "if    (  ( applicationAttemptList )     =  =    null )     {", "return ;", "}", "Iterable < ApplicationAttemptReportProto >    iterable    =    new   Iterable < ApplicationAttemptReportProto >  (  )     {", "@ Override", "public   Iterator < ApplicationAttemptReportProto >    iterator (  )     {", "return   new   Iterator < ApplicationAttemptReportProto >  (  )     {", "Iterator < ApplicationAttemptReport >    iter    =    applicationAttemptList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ApplicationAttemptReportProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAll ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addLocalApplicationAttemptsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationAttemptList )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationAttemptReportProto >    list    =    p . getApplicationAttemptsList (  )  ;", "applicationAttemptList    =    new   ArrayList < ApplicationAttemptReport >  (  )  ;", "for    ( ApplicationAttemptReportProto   a    :    list )     {", "applicationAttemptList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalApplicationAttemptsList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationAttemptList )     !  =    null )     {", "addLocalToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationAttemptsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( applicationId )     !  =    null )     {", "builder . sId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationReport )     !  =    null )     {", "builder . s ( convertToProtoFormat ( this . applicationReport )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationStates )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < YarnApplicationStateProto >    appStatesList    =    p . getApplicationStatesList (  )  ;", "this . applicationStates    =    EnumSet . noneOf ( YarnApplicationState . class )  ;", "for    ( YarnApplicationStateProto   c    :    appStatesList )     {", "this . applicationStates . add ( ProtoUtils . convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initApplicationStates"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationTags )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "this . applicationTags    =    new   HashSet < String >  (  )  ;", "this . applicationTags . addAll ( p . getApplicationTagsList (  )  )  ;", "}", "METHOD_END"], "methodName": ["initApplicationTags"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationTypes )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < String >    appTypeList    =    p . getApplicationTypesList (  )  ;", "this . applicationTypes    =    new   HashSet < String >  (  )  ;", "this . applicationTypes . addAll ( appTypeList )  ;", "}", "METHOD_END"], "methodName": ["initApplicationTypes"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . queues )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < String >    queuesList    =    p . getQueuesList (  )  ;", "this . queues    =    new   HashSet < String >  (  )  ;", "this . queues . addAll ( queuesList )  ;", "}", "METHOD_END"], "methodName": ["initQueues"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . scope )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "this . scope    =    ProtoUtils . convertFromProtoFormat ( p . getScope (  )  )  ;", "}", "METHOD_END"], "methodName": ["initScope"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . users )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < String >    usersList    =    p . getUsersList (  )  ;", "this . users    =    new   HashSet < String >  (  )  ;", "this . users . addAll ( usersList )  ;", "}", "METHOD_END"], "methodName": ["initUsers"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( applicationTypes )     !  =    null )     &  &     (  !  ( applicationTypes . isEmpty (  )  )  )  )     {", "builder . clearTypes (  )  ;", "builder . addAllTypes ( applicationTypes )  ;", "}", "if    (  (  ( applicationStates )     !  =    null )     &  &     (  !  ( applicationStates . isEmpty (  )  )  )  )     {", "builder . clearStates (  )  ;", "builder . addAllStates ( Iterables . transform ( applicationStates ,    new   com . google . common . base . Function < YarnState ,    YarnStateProto >  (  )     {", "@ Override", "public   YarnStateProto   apply ( YarnState   input )     {", "return   ProtoUtils . convertToProtoFormat ( input )  ;", "}", "}  )  )  ;", "}", "if    (  (  ( applicationTags )     !  =    null )     &  &     (  !  ( applicationTags . isEmpty (  )  )  )  )     {", "builder . clearTags (  )  ;", "builder . addAllTags ( this . applicationTags )  ;", "}", "if    (  ( scope )     !  =    null )     {", "builder . setScope ( ProtoUtils . convertToProtoFormat ( scope )  )  ;", "}", "if    (  ( start )     !  =    null )     {", "builder . setStartBegin ( start . getMinimumLong (  )  )  ;", "builder . setStartEnd ( start . getMaximumLong (  )  )  ;", "}", "if    (  ( finish )     !  =    null )     {", "builder . setFinishBegin ( finish . getMinimumLong (  )  )  ;", "builder . setFinishEnd ( finish . getMaximumLong (  )  )  ;", "}", "if    (  ( limit )     !  =     ( Long . MAX _ VALUE )  )     {", "builder . setLimit ( limit )  ;", "}", "if    (  (  ( users )     !  =    null )     &  &     (  !  ( users . isEmpty (  )  )  )  )     {", "builder . clearUsers (  )  ;", "builder . addAllUsers ( users )  ;", "}", "if    (  (  ( queues )     !  =    null )     &  &     (  !  ( queues . isEmpty (  )  )  )  )     {", "builder . clearQueues (  )  ;", "builder . addAllQueues ( queues )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "if    ( spe    =  =    null )     {", "builder . clearSpe (  )  ;", "}", "this . spe    =    spe ;", "}", "METHOD_END"], "methodName": ["setScope"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "if    ( users    =  =    null )     {", "builder . cleUsers (  )  ;", "}", "this . users    =    users ;", "}", "METHOD_END"], "methodName": ["setUsers"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clear (  )  ;", "if    (  ( applicationList )     =  =    null )", "return ;", "Iterable < ApplicationReportProto >    iterable    =    new   Iterable < ApplicationReportProto >  (  )     {", "@ Override", "public   Iterator < ApplicationReportProto >    iterator (  )     {", "return   new   Iterator < ApplicationReportProto >  (  )     {", "Iterator < ApplicationReport >    iter    =    applicationList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ApplicationReportProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAll ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addLocalApplicationsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationList )     !  =    null )     {", "return ;", "}", "GetApplicationsResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationReportProto >    list    =    p . getApplicationsList (  )  ;", "applicationList    =    new   ArrayList < records . ApplicationReport >  (  )  ;", "for    ( ApplicationReportProto   a    :    list )     {", "applicationList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalApplicationsList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationList )     !  =    null )     {", "addLocalToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetApplicationsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   YarnClusterMetricsPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( YarnClusterMetricsPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . yarnClusterMetrics )     !  =    null )     {", "builder . s ( convertToProtoFormat ( this . yarnClusterMetrics )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . states )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NodeStateProto >    list    =    p . getNodeStatesList (  )  ;", "this . states    =    EnumSet . noneOf ( NodeState . class )  ;", "for    ( NodeStateProto   c    :    list )     {", "this . states . add ( ProtoUtils . convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initNodeStates"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . states )     !  =    null )     {", "maybeInitBuilder (  )  ;", "builder . cleaStates (  )  ;", "Iterable < NodeStateProto >    iterable    =    new   Iterable < NodeStateProto >  (  )     {", "@ Override", "public   Iterator < NodeStateProto >    iterator (  )     {", "return   new   Iterator < NodeStateProto >  (  )     {", "Iterator < NodeState >    iter    =    states . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   NodeStateProto   next (  )     {", "return   ProtoUtils . convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllNodeStates ( iterable )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . cleaReports (  )  ;", "if    (  ( nodeManagerInfoList )     =  =    null )", "return ;", "Iterable < NodeReportProto >    iterable    =    new   Iterable < NodeReportProto >  (  )     {", "@ Override", "public   Iterator < NodeReportProto >    iterator (  )     {", "return   new   Iterator < NodeReportProto >  (  )     {", "Iterator < NodeReport >    iter    =    nodeManagerInfoList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   NodeReportProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllNodeReports ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addLocalNodeManagerInfosToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeManagerInfoList )     !  =    null )     {", "return ;", "}", "GetClusterNodesResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NodeReportProto >    list    =    p . getNodeReportsList (  )  ;", "nodeManagerInfoList    =    new   ArrayList < records . NodeReport >  (  )  ;", "for    ( NodeReportProto   a    :    list )     {", "nodeManagerInfoList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalNodeManagerInfosList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeManagerInfoList )     !  =    null )     {", "addLocalManagerInfosToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( containerId )     !  =    null )     {", "builder . sId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerReport )     !  =    null )     {", "builder . s ( convertToProtoFormat ( this . containerReport )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerReportResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearId (  )  ;", "if    (  ( this . containerIds )     =  =    null )", "return ;", "List < IdProto >    protoList    =    new   ArrayList < IdProto >  (  )  ;", "for    ( Id   id    :    containerIds )     {", "protoList . add ( convertToProtoFormat ( id )  )  ;", "}", "builder . addAllId ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addLocalContainerIdsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerIds )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerIdProto >    containerIds    =    p . getContainerIdList (  )  ;", "this . containerIds    =    new   ArrayList < ContainerId >  (  )  ;", "for    ( ContainerIdProto   id    :    containerIds )     {", "this . containerIds . add ( convertFromProtoFormat ( id )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalContainerIds"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerIds )     !  =    null )     {", "addLocalIdsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearFailedRequests (  )  ;", "if    (  ( this . failedRequests )     =  =    null )", "return ;", "List < ContainerExceptionMapProto >    protoList    =    new   ArrayList < ContainerExceptionMapProto >  (  )  ;", "for    ( Map . Entry < ContainerId ,    SerializedException >    entry    :    this . failedRequests . entrySet (  )  )     {", "protoList . add ( ContainerExceptionMapProto . newBuilder (  )  . sId ( convertToProtoFormat ( entry . getKey (  )  )  )  . setException ( convertToProtoFormat ( entry . getValue (  )  )  )  . build (  )  )  ;", "}", "builder . addAllFailedRequests ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addFailedRequestsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearStatus (  )  ;", "if    (  ( this . c )     =  =    null )", "return ;", "List < ContainerStatusProto >    protoList    =    new   ArrayList < ContainerStatusProto >  (  )  ;", "for    ( ContainerStatus   status    :    c )     {", "protoList . add ( convertToProtoFormat ( status )  )  ;", "}", "builder . addAllStatus ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addLocalContainerStatusesToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerStatusPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   SerializedExceptionPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerStatusPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( SerializedExceptionPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . failedRequests )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerExceptionMapProto >    protoList    =    p . getFailedRequestsList (  )  ;", "this . failedRequests    =    new   HashMap < ContainerId ,    SerializedException >  (  )  ;", "for    ( ContainerExceptionMapProto   ce    :    protoList )     {", "this . failedRequests . put ( convertFromProtoFormat ( ce . getContainerId (  )  )  ,    convertFromProtoFormat ( ce . getException (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initFailedRequests"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerStatuses )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerStatusProto >    statuses    =    p . getStatusList (  )  ;", "this . containerStatuses    =    new   ArrayList < ContainerStatus >  (  )  ;", "for    ( ContainerStatusProto   status    :    statuses )     {", "this . containerStatuses . add ( convertFromProtoFormat ( status )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalContainerStatuses"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerStatuses )     !  =    null )     {", "addLocalToProto (  )  ;", "}", "if    (  ( this . failedRequests )     !  =    null )     {", "addFailedRequestsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainerStatusesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( applicationAttemptId )     !  =    null )     {", "builder . setApplicationAttemptId ( convertToPFormat ( this . applicationAttemptId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clear (  )  ;", "if    (  ( containerList )     =  =    null )     {", "return ;", "}", "Iterable < ContainerReportProto >    iterable    =    new   Iterable < ContainerReportProto >  (  )     {", "@ Override", "public   Iterator < ContainerReportProto >    iterator (  )     {", "return   new   Iterator < ContainerReportProto >  (  )     {", "Iterator < ContainerReport >    iter    =    containerList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ContainerReportProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAll ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addLocalContainersToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerReportPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerList )     !  =    null )     {", "return ;", "}", "GetContainersResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerReportProto >    list    =    p . getContainersList (  )  ;", "containerList    =    new   ArrayList < records . ContainerReport >  (  )  ;", "for    ( ContainerReportProto   c    :    list )     {", "containerList . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalContainerList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerList )     !  =    null )     {", "addLocalToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( renewer )     !  =    null )     {", "builder . setRenewer ( this . renewer )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( appToken )     !  =    null )     {", "builder . setToken ( convertToProtoFormat ( this . appToken )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( resource )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( resource )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( applicationId )     !  =    null )     {", "builder . setId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "if    (  ( maximumResourceCapability )     !  =    null )     {", "builder . setMaximumCapability ( convertToProtoFormat ( this . maximumResourceCapability )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetNewApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   QueueInfoPBImpl ( queueInfo )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( QueueInfoPBImpl )     ( queueInfo )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . queueInfo )     !  =    null )     {", "builder . s ( convertToProtoFormat ( this . queueInfo )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearQueueUserAcls (  )  ;", "if    (  ( qList )     =  =    null )", "return ;", "Iterable < QueueUserACLInfoProto >    iterable    =    new   Iterable < QueueUserACLInfoProto >  (  )     {", "@ Override", "public   Iterator < QueueUserACLInfoProto >    iterator (  )     {", "return   new   Iterator < QueueUserACLInfoProto >  (  )     {", "Iterator < QueueUserACLInfo >    iter    =    qList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   QueueUserACLInfoProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllQueueUserAcls ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addLocalQueueUserACLInfosToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   QueueUserACLInfoPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( QueueUserACLInfoPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . queueUserAclsInfoList )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < QueueUserACLInfoProto >    list    =    p . getQueueUserAclsList (  )  ;", "queueUserAclsInfoList    =    new   ArrayList < QueueUserACLInfo >  (  )  ;", "for    ( QueueUserACLInfoProto   a    :    list )     {", "queueUserAclsInfoList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalQueueUserAclsList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . queueUserAclsInfoList )     !  =    null )     {", "addLocalQueueUserACLInfosToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetQueueUserAclsInfoResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationId )     !  =    null )     {", "builder . setId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.KillApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( applicationId )     !  =    null )     {", "builder . setId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "if    (  ( targetQueue )     !  =    null )     {", "builder . setTargetQueue ( this . targetQueue )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.MoveApplicationAcrossQueuesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . cleaACLs (  )  ;", "if    (  ( applicationACLS )     =  =    null )     {", "return ;", "}", "Iterable <  ?    extends   ApplicationACLMapProto >    values    =    new   Iterable < ApplicationACLMapProto >  (  )     {", "@ Override", "public   Iterator < ApplicationACLMapProto >    iterator (  )     {", "return   new   Iterator < ApplicationACLMapProto >  (  )     {", "Iterator < ApplicationAccessType >    aclsIterator    =    applicationACLS . keySet (  )  . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   aclsIterator . hasNext (  )  ;", "}", "@ Override", "public   ApplicationACLMapProto   next (  )     {", "ApplicationAccessType   key    =    aclsIterator . next (  )  ;", "return   ApplicationACLMapProto . newBuilder (  )  . setAcl ( applicationACLS . get ( key )  )  . setAccessType ( ProtoUtils . convertToProtoFormat ( key )  )  . build (  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "this . builder . addAllApplicationACLs ( values )  ;", "}", "METHOD_END"], "methodName": ["addApplicationACLs"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearContainersFromPreviousAttempts (  )  ;", "List < ContainerP >    list    =    new   ArrayList < ContainerP >  (  )  ;", "for    ( Container   c    :    containersFromPreviousAttempts )     {", "list . add ( convertToPFormat ( c )  )  ;", "}", "builder . addAllContainersFromPreviousAttempts ( list )  ;", "}", "METHOD_END"], "methodName": ["addContainersFromPreviousAttemptToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( resource )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NMTokenPBImpl ( proto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NMTokenPBImpl )     ( token )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( resource )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "return   new   Iterable < NMTokenProto >  (  )     {", "@ Override", "public   synchronized   Iterator < NMTokenProto >    iterator (  )     {", "return   new   Iterator < NMTokenProto >  (  )     {", "Iterator < NMToken >    iter    =    nmTokenList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   NMTokenProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperException (  )  ;", "}", "}  ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["getTokenProtoIterable"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationACLS )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationACLMapProto >    list    =    p . getApplicationACLsList (  )  ;", "this . applicationACLS    =    new   HashMap < ApplicationAccessType ,    String >  ( list . size (  )  )  ;", "for    ( ApplicationACLMapProto   aclProto    :    list )     {", "this . applicationACLS . put ( ProtoUtils . convertFromProtoFormat ( aclProto . getAccessType (  )  )  ,    aclProto . getAcl (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initApplicationACLs"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "RegisterApplicationMasterResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerProto >    list    =    p . getContainersFromPreviousAttemptsList (  )  ;", "containersFromPreviousAttempts    =    new   ArrayList < Container >  (  )  ;", "for    ( ContainerProto   c    :    list )     {", "containersFromPreviousAttempts . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initContainersPreviousAttemptList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "RegisterApplicationMasterResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NMTokenProto >    list    =    p . getNmTokensFromPreviousAttemptsList (  )  ;", "nmTokens    =    new   ArrayList < NMToken >  (  )  ;", "for    ( NMTokenProto   t    :    list )     {", "nmTokens . add ( convertFromProtoFormat ( t )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalNewNMTokenList"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . maximumResourceCapability )     !  =    null )     {", "builder . setMaximumCapability ( convertToProtoFormat ( this . maximumResourceCapability )  )  ;", "}", "if    (  ( this . applicationACLS )     !  =    null )     {", "addACLs (  )  ;", "}", "if    (  ( this . containersFromPreviousAttempts )     !  =    null )     {", "addContainersFromPreviousAttemptToProto (  )  ;", "}", "if    (  ( nmTokens )     !  =    null )     {", "builder . clearNmTokensFromPreviousAttempts (  )  ;", "Iterable < NMTokenProto >    iterable    =    getTokenProtoIterable ( nmTokens )  ;", "builder . addAllNmTokensFromPreviousAttempts ( iterable )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( token )     !  =    null )     {", "builder . set ( convertToProtoFormat ( this . token )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RenewDelegationTokenResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( containerProto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerLaunchContextPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerLaunchContextPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( container )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerLaunchContext )     !  =    null )     {", "builder . seLaunchContext ( convertToProtoFormat ( this . containerLaunchContext )  )  ;", "}", "if    (  ( this . containerToken )     !  =    null )     {", "builder . seToken ( convertToProtoFormat ( this . containerToken )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearRequest (  )  ;", "List < RequestProto >    protoList    =    new   ArrayList < RequestProto >  (  )  ;", "for    ( Request   r    :    this . requests )     {", "protoList . add ( convertToProtoFormat ( r )  )  ;", "}", "builder . addAllRequest ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addLocalRequestsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   StartContainerRequestPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( StartContainerRequestPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "StartContainersRequestProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < StartContainerRequestProto >    requestList    =    p . getStartContainerRequestList (  )  ;", "this . requests    =    new   ArrayList < StartContainerRequest >  (  )  ;", "for    ( StartContainerRequestProto   r    :    requestList )     {", "this . requests . add ( convertFromProtoFormat ( r )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalRequests"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( requests )     !  =    null )     {", "addLocalsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearFailedRequests (  )  ;", "if    (  ( this . failed )     =  =    null )", "return ;", "List < ContainerExceptionMapProto >    protoList    =    new   ArrayList < ContainerExceptionMapProto >  (  )  ;", "for    ( Map . Entry < ContainerId ,    SerializedException >    entry    :    this . failed . entrySet (  )  )     {", "protoList . add ( ContainerExceptionMapProto . newBuilder (  )  . setContainerId ( convertToProtoFormat ( entry . getKey (  )  )  )  . setException ( convertToProtoFormat ( entry . getValue (  )  )  )  . build (  )  )  ;", "}", "builder . addAllFailedRequests ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addFailedContainersToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearServicesMetaData (  )  ;", "if    (  ( servicesMetaData )     =  =    null )", "return ;", "Iterable < StringBytesMapP >    iterable    =    new   Iterable < StringBytesMapP >  (  )     {", "@ Override", "public   Iterator < StringBytesMapP >    iterator (  )     {", "return   new   Iterator < StringBytesMapP >  (  )     {", "Iterator < String >    keyIter    =    servicesMetaData . keySet (  )  . iterator (  )  ;", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "@ Override", "public   StringBytesMapP   next (  )     {", "String   key    =    keyIter . next (  )  ;", "return   StringBytesMapP . newBuilder (  )  . setKey ( key )  . setValue ( convertToPFormat ( servicesMetaData . get ( key )  )  )  . build (  )  ;", "}", "@ Override", "public   boolean   hasNext (  )     {", "return   keyIter . hasNext (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllServicesMetaData ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addServicesMetaDataToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearSucceededRequests (  )  ;", "if    (  ( this . succeeded )     =  =    null )     {", "return ;", "}", "Iterable < ContainerIdProto >    iterable    =    new   Iterable < ContainerIdProto >  (  )     {", "@ Override", "public   Iterator < ContainerIdProto >    iterator (  )     {", "return   new   Iterator < ContainerIdProto >  (  )     {", "Iterator < ContainerId >    iter    =    succeeded . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ContainerIdProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllSucceededRequests ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addSucceededContainersToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( byteString )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   SerializedExceptionPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( byteBuffer )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( SerializedExceptionPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . failedContainers )     !  =    null )     {", "return ;", "}", "StartContainersResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerExceptionMapProto >    protoList    =    p . getFailedRequestsList (  )  ;", "this . failedContainers    =    new   HashMap < records . ContainerId ,    records . SerializedException >  (  )  ;", "for    ( ContainerExceptionMapProto   ce    :    protoList )     {", "this . failedContainers . put ( convertFromProtoFormat ( ce . getContainerId (  )  )  ,    convertFromProtoFormat ( ce . getException (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initFailedContainers"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . servicesMetaData )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < StringBytesMapProto >    list    =    p . getServicesMetaDataList (  )  ;", "this . servicesMetaData    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "for    ( StringBytesMapProto   c    :    list )     {", "this . servicesMetaData . put ( c . getKey (  )  ,    convertFromProtoFormat ( c . getValue (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initServicesMetaData"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . succeededContainers )     !  =    null )", "return ;", "StartContainersResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerIdProto >    list    =    p . getSucceededRequestsList (  )  ;", "this . succeededContainers    =    new   ArrayList < records . ContainerId >  (  )  ;", "for    ( ContainerIdProto   c    :    list )     {", "this . succeededContainers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initSucceededContainers"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . servicesMetaData )     !  =    null )     {", "addServicesMetaDataToProto (  )  ;", "}", "if    (  ( this . succeeded )     !  =    null )     {", "addSucceededToProto (  )  ;", "}", "if    (  ( this . failed )     !  =    null )     {", "addFailedToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearId (  )  ;", "if    (  ( this . containerIds )     =  =    null )", "return ;", "List < IdProto >    protoList    =    new   ArrayList < IdProto >  (  )  ;", "for    ( Id   id    :    containerIds )     {", "protoList . add ( convertToProtoFormat ( id )  )  ;", "}", "builder . addAllId ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addLocalContainerIdsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerIds )     !  =    null )     {", "return ;", "}", "StopContainersRequestProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerIdProto >    containerIds    =    p . getContainerIdList (  )  ;", "this . containerIds    =    new   ArrayList < records . ContainerId >  (  )  ;", "for    ( ContainerIdProto   id    :    containerIds )     {", "this . containerIds . add ( convertFromProtoFormat ( id )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalContainerIds"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containerIds )     !  =    null )     {", "addLocalIdsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearFailedRequests (  )  ;", "if    (  ( this . failedRequests )     =  =    null )", "return ;", "List < ExceptionMapProto >    protoList    =    new   ArrayList < ExceptionMapProto >  (  )  ;", "for    ( Map . Entry < Id ,    SerializedException >    entry    :    this . failedRequests . entrySet (  )  )     {", "protoList . add ( ExceptionMapProto . newBuilder (  )  . setId ( convertToProtoFormat ( entry . getKey (  )  )  )  . setException ( convertToProtoFormat ( entry . getValue (  )  )  )  . build (  )  )  ;", "}", "builder . addAllFailedRequests ( protoList )  ;", "}", "METHOD_END"], "methodName": ["addFailedRequestsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearSucceededRequests (  )  ;", "if    (  ( this . succeededRequests )     =  =    null )     {", "return ;", "}", "Iterable < IdProto >    iterable    =    new   Iterable < IdProto >  (  )     {", "@ Override", "public   Iterator < IdProto >    iterator (  )     {", "return   new   Iterator < IdProto >  (  )     {", "Iterator < Id >    iter    =    succeededRequests . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   IdProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllSucceededRequests ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addSucceededRequestsToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   SerializedExceptionPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( SerializedExceptionPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . failedRequests )     !  =    null )     {", "return ;", "}", "StopContainersResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerExceptionMapProto >    protoList    =    p . getFailedRequestsList (  )  ;", "this . failedRequests    =    new   HashMap < records . ContainerId ,    records . SerializedException >  (  )  ;", "for    ( ContainerExceptionMapProto   ce    :    protoList )     {", "this . failedRequests . put ( convertFromProtoFormat ( ce . getContainerId (  )  )  ,    convertFromProtoFormat ( ce . getException (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initFailedRequests"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . succeededRequests )     !  =    null )", "return ;", "StopContainersResponseProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ContainerIdProto >    list    =    p . getSucceededRequestsList (  )  ;", "this . succeededRequests    =    new   ArrayList < records . ContainerId >  (  )  ;", "for    ( ContainerIdProto   c    :    list )     {", "this . succeededRequests . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initSucceededRequests"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . succeededRequests )     !  =    null )     {", "addSucceededRequestsToP (  )  ;", "}", "if    (  ( this . failedRequests )     !  =    null )     {", "addFailedRequestsToP (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StopContainersResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationSubmissionContextPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationSubmissionContextPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationSubmissionContext )     !  =    null )     {", "builder . seSubmissionContext ( convertToProtoFormat ( this . applicationSubmissionContext )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.protocolrecords.impl.pb.SubmitApplicationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( applicationAttemptId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( amContainerId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( yarnApplicationAttemptState )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( amContainerId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( state )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . ApplicationAttemptId )     !  =    null )     &  &     (  !  (  (  ( ApplicationAttemptIdPBImpl )     ( this . ApplicationAttemptId )  )  . getProto (  )  . equals ( builder . getApplicationAttemptId (  )  )  )  )  )     {", "builder . setApplicationAttemptId ( convertToProtoFormat ( this . ApplicationAttemptId )  )  ;", "}", "if    (  (  ( this . amContainerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( this . amContainerId )  )  . getProto (  )  . equals ( builder . getAmContainerId (  )  )  )  )  )     {", "builder . setAmContainerId ( convertToProtoFormat ( this . amContainerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( applicationAttemptId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( applicationId )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( s )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationTags )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "this . applicationTags    =    new   HashSet < String >  (  )  ;", "this . applicationTags . addAll ( p . getApplicationTagsList (  )  )  ;", "}", "METHOD_END"], "methodName": ["initApplicationTags"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . applicationId )     !  =    null )     &  &     (  !  (  (  ( ApplicationIdPBImpl )     ( this . applicationId )  )  . getProto (  )  . equals ( builder . getApplicationId (  )  )  )  )  )     {", "builder . setApplicationId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "if    (  (  ( this . currentApplicationAttemptId )     !  =    null )     &  &     (  !  (  (  ( ApplicationAttemptIdPBImpl )     ( this . currentApplicationAttemptId )  )  . getProto (  )  . equals ( builder . getCurrentApplicationAttemptId (  )  )  )  )  )     {", "builder . setCurrentApplicationAttemptId ( convertToProtoFormat ( this . currentApplicationAttemptId )  )  ;", "}", "if    (  (  ( this . clientToAMToken )     !  =    null )     &  &     (  !  (  (  ( TokenPBImpl )     ( this . clientToAMToken )  )  . getProto (  )  . equals ( builder . getClientToAmToken (  )  )  )  )  )     {", "builder . setClientToAmToken ( convertToProtoFormat ( this . clientToAMToken )  )  ;", "}", "if    (  (  ( this . amRmToken )     !  =    null )     &  &     (  !  (  (  ( TokenPBImpl )     ( this . amRmToken )  )  . getProto (  )  . equals ( builder . getAmRmToken (  )  )  )  )  )     {", "builder . setAmRmToken ( convertToProtoFormat ( this . amRmToken )  )  ;", "}", "if    (  (  ( this . applicationTags )     !  =    null )     &  &     (  !  ( this . applicationTags . isEmpty (  )  )  )  )     {", "builder . clearApplicationTags (  )  ;", "builder . addAllApplicationTags ( this . applicationTags )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "if    ( appInfo    =  =    null )     {", "builder . clearResourceUsage (  )  ;", "return ;", "}", "builder . setResourceUsage ( convertToProtoFormat ( appInfo )  )  ;", "}", "METHOD_END"], "methodName": ["setApplicationResourceUsageReport"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . usedResources )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . usedResources )  )  . getProto (  )  . equals ( builder . getUsedResources (  )  )  )  )  )     {", "builder . setUsedResources ( convertToProtoFormat ( this . usedResources )  )  ;", "}", "if    (  (  ( this . reservedResources )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . reservedResources )  )  . getProto (  )  . equals ( builder . getReservedResources (  )  )  )  )  )     {", "builder . setReservedResources ( convertToProtoFormat ( this . reservedResources )  )  ;", "}", "if    (  (  ( this . neededResources )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . neededResources )  )  . getProto (  )  . equals ( builder . getNeededResources (  )  )  )  )  )     {", "builder . setNeededResources ( convertToProtoFormat ( this . neededResources )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationResourceUsageReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( tags . size (  )  )     >     ( YarnConfiguration . APPLICATION _ MAX _ TAGS )  )     {", "throw   new   IllegalArgumentException (  (  (  \" Too   many   aTags ,    a   maximum   of   only    \"     +     ( YarnConfiguration . APPLICATION _ MAX _ TAGS )  )     +     \"    are   allowed !  \"  )  )  ;", "}", "for    ( String   tag    :    tags )     {", "if    (  ( tag . length (  )  )     >     ( YarnConfiguration . APPLICATION _ MAX _ TAG _ LENGTH )  )     {", "throw   new   IllegalArgumentException (  (  (  (  (  \" Tag    \"     +    tag )     +     \"    is   too   long ,     \"  )     +     \" maximum   allowed   length   of   a   tag   is    \"  )     +     ( YarnConfiguration . APPLICATION _ MAX _ TAG _ LENGTH )  )  )  ;", "}", "if    (  !  ( ASCII . matchesAllOf ( tag )  )  )     {", "throw   new   IllegalArgumentException (  (  (  \" A   tag   can   only   have   ASCII    \"     +     \" characters !    Invalid   tag    -     \"  )     +    tag )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["checkTags"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerLaunchContextPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PriorityPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerLaunchContextPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PriorityPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationTags )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "this . applicationTags    =    new   HashSet < String >  (  )  ;", "this . applicationTags . addAll ( p . getApplicationTagsList (  )  )  ;", "}", "METHOD_END"], "methodName": ["initApplicationTags"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationId )     !  =    null )     {", "builder . setId ( convertToProtoFormat ( this . applicationId )  )  ;", "}", "if    (  ( this . priority )     !  =    null )     {", "builder . setPriority ( convertToProtoFormat ( this . priority )  )  ;", "}", "if    (  ( this . amContainer )     !  =    null )     {", "builder . setAmContainerSpec ( convertToProtoFormat ( this . amContainer )  )  ;", "}", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . resource )  )  . getProto (  )  . equals ( builder . getResource (  )  )  )  )  )     {", "builder . setResource ( convertToProtoFormat ( this . resource )  )  ;", "}", "if    (  (  ( this . applicationTags )     !  =    null )     &  &     (  !  ( this . applicationTags . isEmpty (  )  )  )  )     {", "builder . clearTags (  )  ;", "builder . addAllTags ( this . applicationTags )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationAttemptIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationAttemptIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearApplicationACLs (  )  ;", "if    (  ( applicationACLS )     =  =    null )     {", "return ;", "}", "Iterable <  ?    ends   ApplicationACLMapProto >    values    =    new   Iterable < ApplicationACLMapProto >  (  )     {", "@ Override", "public   Iterator < ApplicationACLMapProto >    iterator (  )     {", "return   new   Iterator < ApplicationACLMapProto >  (  )     {", "Iterator < ApplicationAccessType >    aclsIterator    =    applicationACLS . keySet (  )  . iterator (  )  ;", "@ Override", "public   boolean   hasN (  )     {", "return   aclsIterator . hasN (  )  ;", "}", "@ Override", "public   ApplicationACLMapProto   n (  )     {", "ApplicationAccessType   key    =    aclsIterator . n (  )  ;", "return   ApplicationACLMapProto . newBuilder (  )  . setAcl ( applicationACLS . get ( key )  )  . setAccessType ( ProtoUtils . convertToProtoFormat ( key )  )  . build (  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "this . builder . addAllApplicationACLs ( values )  ;", "}", "METHOD_END"], "methodName": ["addApplicationACLs"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearCommand (  )  ;", "if    (  ( this . commands )     =  =    null )", "return ;", "builder . addAllCommand ( this . commands )  ;", "}", "METHOD_END"], "methodName": ["addCommandsToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearEnvironment (  )  ;", "if    (  ( environment )     =  =    null )", "return ;", "Iterable < StringStringMapProto >    iterable    =    new   Iterable < StringStringMapProto >  (  )     {", "@ Override", "public   Iterator < StringStringMapProto >    iterator (  )     {", "return   new   Iterator < StringStringMapProto >  (  )     {", "Iterator < String >    keyIter    =    environment . keySet (  )  . iterator (  )  ;", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "@ Override", "public   StringStringMapProto   n (  )     {", "String   key    =    keyIter . n (  )  ;", "return   StringStringMapProto . newBuilder (  )  . setKey ( key )  . setValue ( environment . get ( key )  )  . build (  )  ;", "}", "@ Override", "public   boolean   hasN (  )     {", "return   keyIter . hasN (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllEnvironment ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addEnvToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearLocalResources (  )  ;", "if    (  ( localResources )     =  =    null )", "return ;", "Iterable < StringLocalResourceMapProto >    iterable    =    new   Iterable < StringLocalResourceMapProto >  (  )     {", "@ Override", "public   Iterator < StringLocalResourceMapProto >    iterator (  )     {", "return   new   Iterator < StringLocalResourceMapProto >  (  )     {", "Iterator < String >    keyIter    =    localResources . keySet (  )  . iterator (  )  ;", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "@ Override", "public   StringLocalResourceMapProto   n (  )     {", "String   key    =    keyIter . n (  )  ;", "return   StringLocalResourceMapProto . newBuilder (  )  . setKey ( key )  . setValue ( convertToProtoFormat ( localResources . get ( key )  )  )  . build (  )  ;", "}", "@ Override", "public   boolean   hasN (  )     {", "return   keyIter . hasN (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllLocalResources ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addLocalResourcesToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearServiceData (  )  ;", "if    (  ( serviceData )     =  =    null )", "return ;", "Iterable < StringBytesMapProto >    iterable    =    new   Iterable < StringBytesMapProto >  (  )     {", "@ Override", "public   Iterator < StringBytesMapProto >    iterator (  )     {", "return   new   Iterator < StringBytesMapProto >  (  )     {", "Iterator < String >    keyIter    =    serviceData . keySet (  )  . iterator (  )  ;", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "@ Override", "public   StringBytesMapProto   n (  )     {", "String   key    =    keyIter . n (  )  ;", "return   StringBytesMapProto . newBuilder (  )  . setKey ( key )  . setValue ( convertToProtoFormat ( serviceData . get ( key )  )  )  . build (  )  ;", "}", "@ Override", "public   boolean   hasN (  )     {", "return   keyIter . hasN (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllServiceData ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addServiceDataToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( byteString )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   LocalResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( byteBuffer )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( LocalResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationACLS )     !  =    null )     {", "return ;", "}", "ContainerLaunchContextProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationACLMapProto >    list    =    p . getApplicationACLsList (  )  ;", "this . applicationACLS    =    new   HashMap < ApplicationAccessType ,    String >  ( list . size (  )  )  ;", "for    ( ApplicationACLMapProto   aclProto    :    list )     {", "this . applicationACLS . put ( ProtoUtils . convertFromProtoFormat ( aclProto . getAccessType (  )  )  ,    aclProto . getAcl (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initApplicationACLs"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . commands )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < String >    list    =    p . getCommandList (  )  ;", "this . commands    =    new   ArrayList < String >  (  )  ;", "for    ( String   c    :    list )     {", "this . commands . add ( c )  ;", "}", "}", "METHOD_END"], "methodName": ["initCommands"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . environment )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < StringStringMapProto >    list    =    p . getEnvironmentList (  )  ;", "this . environment    =    new   HashMap < String ,    String >  (  )  ;", "for    ( StringStringMapProto   c    :    list )     {", "this . environment . put ( c . getKey (  )  ,    c . getValue (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initEnv"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . localResources )     !  =    null )     {", "return ;", "}", "ContainerLaunchContextProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < StringLocalResourceMapProto >    list    =    p . getLocalResourcesList (  )  ;", "this . localResources    =    new   HashMap < String ,    LocalResource >  (  )  ;", "for    ( StringLocalResourceMapProto   c    :    list )     {", "this . localResources . put ( c . getKey (  )  ,    convertFromProtoFormat ( c . getValue (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalResources"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . serviceData )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < StringBytesMapProto >    list    =    p . getServiceDataList (  )  ;", "this . serviceData    =    new   HashMap < String ,    ByteBuffer >  (  )  ;", "for    ( StringBytesMapProto   c    :    list )     {", "this . serviceData . put ( c . getKey (  )  ,    convertFromProtoFormat ( c . getValue (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initServiceData"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . localResources )     !  =    null )     {", "addLocalResourcesToProto (  )  ;", "}", "if    (  ( this . tokens )     !  =    null )     {", "builder . setTokens ( convertToProtoFormat ( this . tokens )  )  ;", "}", "if    (  ( this . serviceData )     !  =    null )     {", "addServiceDataToProto (  )  ;", "}", "if    (  ( this . environment )     !  =    null )     {", "addEnvToProto (  )  ;", "}", "if    (  ( this . commands )     !  =    null )     {", "addCommandsToProto (  )  ;", "}", "if    (  ( thisplicationACLS )     !  =    null )     {", "addApplicationACLs (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild (  )  ;", "mgeLocalToBuild (  )  ;", "proto    =    build . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PriorityPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PriorityPBImpl )     ( p )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . containerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( containerId )  )  . getProto (  )  . equals ( builder . getId (  )  )  )  )  )     {", "builder . setId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "if    (  (  ( this . nodeId )     !  =    null )     &  &     (  !  (  (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  . equals ( builder . getNodeId (  )  )  )  )  )     {", "builder . setNodeId ( convertToProtoFormat ( this . nodeId )  )  ;", "}", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . resource )  )  . getProto (  )  . equals ( builder . getResource (  )  )  )  )  )     {", "builder . setResource ( convertToProtoFormat ( this . resource )  )  ;", "}", "if    (  (  ( this . priority )     !  =    null )     &  &     (  !  (  (  ( PriorityPBImpl )     ( this . priority )  )  . getProto (  )  . equals ( builder . getPriority (  )  )  )  )  )     {", "builder . setPriority ( convertToProtoFormat ( this . priority )  )  ;", "}", "if    (  (  ( this . containerToken )     !  =    null )     &  &     (  !  (  (  ( TokenPBImpl )     ( this . containerToken )  )  . getProto (  )  . equals ( builder . getContainerToken (  )  )  )  )  )     {", "builder . setContainerToken ( convertToProtoFormat ( this . containerToken )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild (  )  ;", "mgeLocalToBuild (  )  ;", "proto    =    build . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append (  \"  :     [  \"  )  ;", "sb . append (  \" Id :     \"  )  . append ( getId (  )  )  . append (  \"  ,     \"  )  ;", "sb . append (  \" NodeId :     \"  )  . append ( getNodeId (  )  )  . append (  \"  ,     \"  )  ;", "sb . append (  \" NodeHttpAddress :     \"  )  . append ( getNodeHttpAddress (  )  )  . append (  \"  ,     \"  )  ;", "sb . append (  \" Resource :     \"  )  . append ( getResource (  )  )  . append (  \"  ,     \"  )  ;", "sb . append (  \" Priority :     \"  )  . append ( getPriority (  )  )  . append (  \"  ,     \"  )  ;", "sb . append (  \" Token :     \"  )  . append ( getToken (  )  )  . append (  \"  ,     \"  )  ;", "sb . append (  \"  ]  \"  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( containerState )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PriorityPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( containerState )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PriorityPBImpl )     ( p )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . containerId )     !  =    null )     &  &     (  !  (  (  ( ContainerIdPBImpl )     ( containerId )  )  . getProto (  )  . equals ( builder . getContainerId (  )  )  )  )  )     {", "builder . setContainerId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "if    (  (  ( this . nodeId )     !  =    null )     &  &     (  !  (  (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  . equals ( builder . getNodeId (  )  )  )  )  )     {", "builder . setNodeId ( convertToProtoFormat ( this . nodeId )  )  ;", "}", "if    (  (  ( this . resource )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . resource )  )  . getProto (  )  . equals ( builder . getResource (  )  )  )  )  )     {", "builder . setResource ( convertToProtoFormat ( this . resource )  )  ;", "}", "if    (  (  ( this . priority )     !  =    null )     &  &     (  !  (  (  ( PriorityPBImpl )     ( this . priority )  )  . getProto (  )  . equals ( builder . getPriority (  )  )  )  )  )     {", "builder . setPriority ( convertToProtoFormat ( this . priority )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild (  )  ;", "mgeLocalToBuild (  )  ;", "proto    =    build . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . existingContainerId )     !  =    null )     {", "builder . setContainerId ( convertToProtoFormat ( this . existingContainerId )  )  ;", "}", "if    (  ( this . targetCapability )     !  =    null )     {", "builder . setCapability ( convertToProtoFormat ( this . targetCapability )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuild (  )  ;", "}", "mgeLocalToBuild (  )  ;", "proto    =    build . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceDecreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . existingContainerId )     !  =    null )     {", "builder . setContainerId ( convertToProtoFormat ( this . existingContainerId )  )  ;", "}", "if    (  ( this . targetCapability )     !  =    null )     {", "builder . setCapability ( convertToProtoFormat ( this . targetCapability )  )  ;", "}", "if    (  ( this . token )     !  =    null )     {", "builder . setContainerToken ( convertToProtoFormat ( this . token )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreasePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . existingContainerId )     !  =    null )     {", "builder . setContainerId ( convertToProtoFormat ( this . existingContainerId )  )  ;", "}", "if    (  ( this . targetCapability )     !  =    null )     {", "builder . setCapability ( convertToProtoFormat ( this . targetCapability )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybitBuilder (  )  ;", "}", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerResourceIncreaseRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( e )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( e )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( containerId )     !  =    null )     {", "builder . setId ( convertToProtoFormat ( this . containerId )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuild (  )  ;", "mgeLocalToBuild (  )  ;", "proto    =    build . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ContainerStatusPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( e )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( e )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   URLPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( e )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( e )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( URLPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToBuilder (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "LocalResourceProtoOrBuilder   l    =     ( viaProto )     ?    proto    :    builder ;", "if    (  (  ( this . url )     !  =    null )     &  &     (  !  ( l . getResource (  )  . equals (  (  ( URLPBImpl )     ( url )  )  . getProto (  )  )  )  )  )     {", "maybeInitBuilder (  )  ;", "l    =    builder ;", "builder . setResource ( convertToProtoFormat ( this . url )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.LocalResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   TokenPBImpl ( proto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( TokenPBImpl )     ( token )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeId )     !  =    null )     {", "builder . setNodeId ( convertToProtoFormat ( nodeId )  )  ;", "}", "if    (  ( this . token )     !  =    null )     {", "builder . set ( convertToProtoFormat ( token )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NMTokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeIdPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( r )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  (  ( this . nodeId )     !  =    null )     &  &     (  !  (  (  ( NodeIdPBImpl )     ( this . nodeId )  )  . getProto (  )  . equals ( builder . getNodeId (  )  )  )  )  )     {", "builder . setNodeId ( convertToProtoFormat ( this . nodeId )  )  ;", "}", "if    (  (  ( this . used )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . used )  )  . getProto (  )  . equals ( builder . getUsed (  )  )  )  )  )     {", "builder . setUsed ( convertToProtoFormat ( this . used )  )  ;", "}", "if    (  (  ( this . capability )     !  =    null )     &  &     (  !  (  (  ( ResourcePBImpl )     ( this . capability )  )  . getProto (  )  . equals ( builder . getCapability (  )  )  )  )  )     {", "builder . setCapability ( convertToProtoFormat ( this . capability )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilr (  )  ;", "mergeLocalToBuilr (  )  ;", "proto    =    builr . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.NodeReportPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ContainerIdPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ContainerIdPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( id )     !  =    null )     {", "build . setId ( convtToProtoFormat ( id )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContainerPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearContainer (  )  ;", "if    ( null    =  =     ( containers )  )     {", "return ;", "}", "Iterable < ainerProto >    iterable    =    new   Iterable < ainerProto >  (  )     {", "@ Override", "public   Iterator < ainerProto >    iterator (  )     {", "return   new   Iterator < ainerProto >  (  )     {", "Iterator < ainer >    iter    =    containers . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ainerProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllContainer ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addContainersToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearResource (  )  ;", "if    ( null    =  =     ( resources )  )     {", "return ;", "}", "Iterable < ResourceRequestProto >    iterable    =    new   Iterable < ResourceRequestProto >  (  )     {", "@ Override", "public   Iterator < ResourceRequestProto >    iterator (  )     {", "return   new   Iterator < ResourceRequestProto >  (  )     {", "Iterator < ResourceRequest >    iter    =    resources . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ResourceRequestProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllResource ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addResourcesToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PreemptionContainerPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PreemptionResourceRequestPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PreemptionContainerPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PreemptionResourceRequestPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( containers )     !  =    null )     {", "return ;", "}", "PreemptionContractProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < PreemptionContainerProto >    list    =    p . getContainerList (  )  ;", "containers    =    new   HashSet < PreemptionContainer >  (  )  ;", "for    ( PreemptionContainerProto   c    :    list )     {", "containers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initPreemptionContainers"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( resources )     !  =    null )     {", "return ;", "}", "PreemptionContractProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < PreemptionResourceRequestProto >    list    =    p . getResourceList (  )  ;", "resources    =    new   ArrayList < PreemptionResourceRequest >  (  )  ;", "for    ( PreemptionResourceRequestProto   rr    :    list )     {", "resources . add ( convertFromProtoFormat ( rr )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initPreemptionResourceRequests"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . resources )     !  =    null )     {", "addResourcesToProto (  )  ;", "}", "if    (  ( this . containers )     !  =    null )     {", "addainersToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PreemptionContractPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   StrictPreemptionContractPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PreemptionContractPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( StrictPreemptionContractPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( strict )     !  =    null )     {", "builder . setStrictCtract ( cvertToProtoFormat ( strict )  )  ;", "}", "if    (  ( ctract )     !  =    null )     {", "builder . setCtract ( cvertToProtoFormat ( ctract )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionMessagePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceRequestPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourceRequestPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( rr )     !  =    null )     {", "builder . set ( convertToProtoFormat ( rr )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PreemptionResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PriorityPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.PriorityPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( byteString )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoBase"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( byteBuffer )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoBase"}, {"methodBody": ["METHOD_START", "{", "int   capacity    =    byteString . asReadOnlyByteBuffer (  )  . rewind (  )  . remaining (  )  ;", "byte [  ]    b    =    new   byte [ capacity ]  ;", "byteString . asReadOnlyByteBuffer (  )  . get ( b ,     0  ,    capacity )  ;", "return   ByteBuffer . wrap ( b )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   AMCommand . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationAccessType . valueOf ( e . name (  )  . replace ( ProtoUtils . APP _ ACCESS _ TYPE _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationResourceUsageReportPBImpl ( e )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   ContainerState . valueOf ( e . name (  )  . replace ( ProtoUtils . CONTAINER _ STATE _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   FinalApplicationStatus . valueOf ( e . name (  )  . replace ( ProtoUtils . FINAL _ APPLICATION _ STATUS _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   LocalResourceType . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   LocalResourceVisibility . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( e )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   NodeState . valueOf ( e . name (  )  . replace ( ProtoUtils . NODE _ STATE _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   QueueACL . valueOf ( e . name (  )  . replace ( ProtoUtils . QUEUE _ ACL _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   QueueState . valueOf ( e . name (  )  . replace ( ProtoUtils . QUEUE _ STATE _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   YarnApplicationAttemptState . valueOf ( e . name (  )  . replace ( ProtoUtils . YARN _ APPLICATION _ ATTEMPT _ STATE _ PREFIX ,     \"  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   YarnApplicationState . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationsRequestScope . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "int   oldPos    =    byteBuffer . position (  )  ;", "byteBufferwind (  )  ;", "ByteString   bs    =    ByteString . copyFrom ( byteBuffer )  ;", "byteBuffer . position ( oldPos )  ;", "turn   bs ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationsRequestScopeProto . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   AMCommandProto . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   ApplicationAccessTypeProto . valueOf (  (  ( ProtoUtils . APP _ ACCESS _ TYPE _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationResourceUsageReportPBImpl )     ( e )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   ContainerStateProto . valueOf (  (  ( ProtoUtils . CONTAINER _ STATE _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   FinalApplicationStatusProto . valueOf (  (  ( ProtoUtils . FINAL _ APPLICATION _ STATUS _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   LocalResourceTypeProto . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   LocalResourceVisibilityProto . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( e )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   NodeStateProto . valueOf (  (  ( ProtoUtils . NODE _ STATE _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   QueueACLProto . valueOf (  (  ( ProtoUtils . QUEUE _ ACL _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   QueueStateProto . valueOf (  (  ( ProtoUtils . QUEUE _ STATE _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   YarnApplicationAttemptStateProto . valueOf (  (  ( ProtoUtils . YARN _ APPLICATION _ ATTEMPT _ STATE _ PREFIX )     +     ( e . name (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "return   YarnApplicationStateProto . valueOf ( e . name (  )  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearApplications (  )  ;", "if    (  ( applicationsList )     =  =    null )", "return ;", "Iterable < ApplicationReportProto >    iterable    =    new   Iterable < ApplicationReportProto >  (  )     {", "@ Override", "public   Iterator < ApplicationReportProto >    iterator (  )     {", "return   new   Iterator < ApplicationReportProto >  (  )     {", "Iterator < ApplicationReport >    iter    =    applicationsList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ApplicationReportProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllApplications ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addApplicationsToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearChildQueues (  )  ;", "if    (  ( childQueuesList )     =  =    null )", "return ;", "Iterable < roto >    iterable    =    new   Iterable < roto >  (  )     {", "@ Override", "public   Iterator < roto >    iterator (  )     {", "return   new   Iterator < roto >  (  )     {", "Iterator < QueueInfo >    iter    =    childQueuesList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   roto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllChildQueues ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addChildQueuesInfoToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ApplicationReportPBImpl ( a )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   QueueInfoPBImpl ( a )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( q )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationReportPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( QueueInfoPBImpl )     ( q )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( queueState )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . applicationsList )     !  =    null )     {", "return ;", "}", "QueueInfoProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < ApplicationReportProto >    list    =    p . getApplicationsList (  )  ;", "applicationsList    =    new   ArrayList < ApplicationReport >  (  )  ;", "for    ( ApplicationReportProto   a    :    list )     {", "applicationsList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalApplicationsList"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . childQueuesList )     !  =    null )     {", "return ;", "}", "QueueInfoProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < QueueInfoProto >    list    =    p . getChildQueuesList (  )  ;", "childQueuesList    =    new   ArrayList < QueueInfo >  (  )  ;", "for    ( QueueInfoProto   a    :    list )     {", "childQueuesList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalChildQueuesList"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . childQueuesList )     !  =    null )     {", "addChildQueuesInfoToProto (  )  ;", "}", "if    (  ( this . applicationsList )     !  =    null )     {", "addApplicationsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearcls (  )  ;", "if    (  ( userAclsList )     =  =    null )", "return ;", "Iterable < QueueACLProto >    iterable    =    new   Iterable < QueueACLProto >  (  )     {", "@ Override", "public   Iterator < QueueACLProto >    iterator (  )     {", "return   new   Iterator < QueueACLProto >  (  )     {", "Iterator < QueueACL >    iter    =    userAclsList . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   QueueACLProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllcls ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addQueueACLsToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( q )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( queueAcl )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . userAclsList )     !  =    null )     {", "return ;", "}", "QueueUserACLInfoProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < QueueACLProto >    list    =    p . getUserAclsList (  )  ;", "userAclsList    =    new   ArrayList < QueueACL >  (  )  ;", "for    ( QueueACLProto   a    :    list )     {", "userAclsList . add ( convertFromProtoFormat ( a )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initLocalQueueUserAclsList"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . userAclsList )     !  =    null )     {", "addACLsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.QueueUserACLInfoPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearAdditions (  )  ;", "if    (  ( this . blacklistAdditions )     =  =    null )     {", "return ;", "}", "builder . addAllAdditions ( this . blacklistAdditions )  ;", "}", "METHOD_END"], "methodName": ["addBlacklistAdditionsToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearmovals (  )  ;", "if    (  ( this . blacklistRemovals )     =  =    null )     {", "return ;", "}", "builder . addAllmovals ( this . blacklistRemovals )  ;", "}", "METHOD_END"], "methodName": ["addBlacklistRemovalsToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . blacklistAdditions )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < String >    list    =    p . getBlacklistAdditionsList (  )  ;", "this . blacklistAdditions    =    new   ArrayList < String >  (  )  ;", "this . blacklistAdditions . addAll ( list )  ;", "}", "METHOD_END"], "methodName": ["initBlacklistAdditions"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . blacklistRemovals )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < String >    list    =    p . getBlacklistRemovalsList (  )  ;", "this . blacklistRemovals    =    new   ArrayList < String >  (  )  ;", "this . blacklistRemovals . addAll ( list )  ;", "}", "METHOD_END"], "methodName": ["initBlacklistRemovals"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . blacklistAdditions )     !  =    null )     {", "addBlacklistAdditionsToProto (  )  ;", "}", "if    (  ( this . blacklistRemovals )     !  =    null )     {", "addmovalsToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )     {", "maybeInitBuilder (  )  ;", "}", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceBlacklistRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceOptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( resource )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceOptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceOptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "turn   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourcePBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PriorityPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourcePBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PriorityPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourcePBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . priority )     !  =    null )     {", "builder . setPriority ( convertToProtoFormat ( this . priority )  )  ;", "}", "if    (  ( this . cbility )     !  =    null )     {", "builder . setCbility ( convertToProtoFormat ( this . cbility )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.ResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . setMessage ( message )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "init ( t )  ;", "if    ( message    !  =    null )", "build . setMessage ( message )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "if    ( t    =  =    null )     {", "return ;", "}", "if    (  ( t . getCause (  )  )     =  =    null )     {", "} else    {", "builder . setCause ( new    ( t . getCause (  )  )  . getProto (  )  )  ;", "}", "StringWriter   sw    =    new   StringWriter (  )  ;", "PrintWriter   pw    =    new   PrintWriter ( sw )  ;", "t . printStackTrace ( pw )  ;", "pw . close (  )  ;", "if    (  ( sw . toString (  )  )     !  =    null )", "builder . setTrace ( sw . toString (  )  )  ;", "if    (  ( t . getMessage (  )  )     !  =    null )", "builder . setMessage ( t . getMessage (  )  )  ;", "builder . setClassName ( t . getClass (  )  . getCanonicalName (  )  )  ;", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "Constructor <  ?    extends   T >    cn ;", "T   ex    =    null ;", "try    {", "cn    =    cls . getConstructor ( String . class )  ;", "cn . setAccessible ( true )  ;", "ex    =    cn . newInstance ( message )  ;", "ex . initCause ( cause )  ;", "}    catch    ( SecurityException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( NoSuchMetho   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( IllegalArgumentException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( InstantiationException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( IllegalAccessException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( InvocationTargetException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}", "return   ex ;", "}", "METHOD_END"], "methodName": ["instantiateException"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearContainer (  )  ;", "if    (  ( containers )     =  =    null )     {", "return ;", "}", "Iterable < ainerProto >    iterable    =    new   Iterable < ainerProto >  (  )     {", "@ Override", "public   Iterator < ainerProto >    iterator (  )     {", "return   new   Iterator < ainerProto >  (  )     {", "Iterator < ainer >    iter    =    containers . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   iter . hasNext (  )  ;", "}", "@ Override", "public   ainerProto   next (  )     {", "return   convertToProtoFormat ( iter . next (  )  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "builder . addAllContainer ( iterable )  ;", "}", "METHOD_END"], "methodName": ["addContainersToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   PreemptionContainerPBImpl ( p )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( PreemptionContainerPBImpl )     ( t )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( containers )     !  =    null )     {", "return ;", "}", "StrictPreemptionContractProtoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < PreemptionContainerProto >    list    =    p . getContainerList (  )  ;", "containers    =    new   HashSet < PreemptionContainer >  (  )  ;", "for    ( PreemptionContainerProto   c    :    list )     {", "containers . add ( convertFromProtoFormat ( c )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initIds"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . containers )     !  =    null )     {", "addainersToProto (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.StrictPreemptionContractPBImpl"}, {"methodBody": ["METHOD_START", "{", "SerializedExceptionProto   defaultProto    =    SerializedExceptionProto . newBuilder (  )  . build (  )  ;", "pb 1     =    new    (  )  ;", "Assert . assertNull ( pb 1  . getCause (  )  )  ;", "pb 2     =    new    (  )  ;", "Assert . assertEquals ( defaultProto ,    pb 2  . getProto (  )  )  ;", "pb 3     =    new    (  )  ;", "Assert . assertEquals ( defaultProto . getTrace (  )  ,    pb 3  . getRemoteTrace (  )  )  ;", "}", "METHOD_END"], "methodName": ["testBeforeInit"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TestSerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "Exception   ex    =    new   Exception (  \" test   exception \"  )  ;", "pb    =    new    (  )  ;", "try    {", "pb . deSerialize (  )  ;", "Assert . fail (  \" deSerialze   should   throw   YarnRuntimeException \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "Assert . assertEquals ( ClassNotFoundException . class ,    e . getCause (  )  . getClass (  )  )  ;", "}", "pb . init ( ex )  ;", "Assert . assertEquals ( ex . toString (  )  ,    pb . deSerialize (  )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["testDeserialize"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TestSerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "SerializedExceptionPBImpl   orig    =    new   SerializedExceptionPBImpl (  )  ;", "orig . init ( new   Exception (  \" test   exception \"  )  )  ;", "SerializedExceptionProto   proto    =    orig . getProto (  )  ;", "SerializedExceptionPBImpl   deser    =    new   SerializedExceptionPBImpl ( proto )  ;", "Assert . assertEquals ( orig ,    deser )  ;", "Assert . assertEquals ( orig . getMessage (  )  ,    deser . getMessage (  )  )  ;", "Assert . assertEquals ( orig . getRemoteTrace (  )  ,    deser . getRemoteTrace (  )  )  ;", "Assert . assertEquals ( orig . getCause (  )  ,    deser . getCause (  )  )  ;", "}", "METHOD_END"], "methodName": ["testSerializedException"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TestSerializedExceptionPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertFromProtoFormat ( byteString )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   ProtoUtils . convertToProtoFormat ( byteBuffer )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . identifier )     !  =    null )     {", "builder . setIdentifier ( convertToProtoFormat ( this . identifier )  )  ;", "}", "if    (  ( this . password )     !  =    null )     {", "builder . setPassword ( convertToProtoFormat ( this . password )  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "meeLocalToBuilder (  )  ;", "proto    =    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.TokenPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "turn   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.URLPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.api.records.impl.pb.YarnClusterMetricsPBImpl"}, {"methodBody": ["METHOD_START", "{", "TimelineEntities   entities    =    new   TimelineEntities (  )  ;", "for    ( int   j    =     0  ;    j    <     2  ;     +  + j )     {", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  (  \" entity   id    \"     +    j )  )  ;", "entity . setEntityType (  (  \" entity   type    \"     +    j )  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;     +  + i )     {", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setTimestamp ( System . currentTimeMillis (  )  )  ;", "event . setEventType (  (  \" event   type    \"     +    i )  )  ;", "event . addEventInfo (  \" key 1  \"  ,     \" val 1  \"  )  ;", "event . addEventInfo (  \" key 2  \"  ,     \" val 2  \"  )  ;", "entity . addEvent ( event )  ;", "}", "entity . addRelatedEntity (  \" test   ref   type    1  \"  ,     \" test   ref   id    1  \"  )  ;", "entity . addRelatedEntity (  \" test   ref   type    2  \"  ,     \" test   ref   id    2  \"  )  ;", "entity . addPrimaryFilter (  \" pkey 1  \"  ,     \" pval 1  \"  )  ;", "entity . addPrimaryFilter (  \" pkey 2  \"  ,     \" pval 2  \"  )  ;", "entity . addOtherInfo (  \" okey 1  \"  ,     \" oval 1  \"  )  ;", "entity . addOtherInfo (  \" okey 2  \"  ,     \" oval 2  \"  )  ;", "entities . addEntity ( entity )  ;", "}", ". LOG . info (  \" Entities   in   JSON :  \"  )  ;", ". LOG . info ( TimelineUtils . dumpTimelineRecordtoJSON ( entities ,    true )  )  ;", "Assert . assertEquals (  2  ,    entities . getEntities (  )  . size (  )  )  ;", "TimelineEntity   entity 1     =    entities . getEntities (  )  . get (  0  )  ;", "Assert . assertEquals (  \" entity   id    0  \"  ,    entity 1  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" entity   type    0  \"  ,    entity 1  . getEntityType (  )  )  ;", "Assert . assertEquals (  2  ,    entity 1  . getRelatedEntities (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    entity 1  . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    entity 1  . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    entity 1  . getOtherInfo (  )  . size (  )  )  ;", "TimelineEntity   entity 2     =    entities . getEntities (  )  . get (  1  )  ;", "Assert . assertEquals (  \" entity   id    1  \"  ,    entity 2  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" entity   type    1  \"  ,    entity 2  . getEntityType (  )  )  ;", "Assert . assertEquals (  2  ,    entity 2  . getRelatedEntities (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    entity 2  . getEvents (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    entity 2  . getPrimaryFilters (  )  . size (  )  )  ;", "Assert . assertEquals (  2  ,    entity 2  . getOtherInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testEntities"], "fileName": "org.apache.hadoop.yarn.api.records.timeline.TestTimelineRecords"}, {"methodBody": ["METHOD_START", "{", "TimelineEvents   events    =    new   TimelineEvents (  )  ;", "for    ( int   j    =     0  ;    j    <     2  ;     +  + j )     {", "TimelineEvents . EventsOfOneEntity   partEvents    =    new   TimelineEvents . EventsOfOneEntity (  )  ;", "partEvents . setEntityId (  (  \" entity   id    \"     +    j )  )  ;", "partEvents . setEntityType (  (  \" entity   type    \"     +    j )  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;     +  + i )     {", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setTimestamp ( System . currentTimeMillis (  )  )  ;", "event . setEventType (  (  \" event   type    \"     +    i )  )  ;", "event . addEventInfo (  \" key 1  \"  ,     \" val 1  \"  )  ;", "event . addEventInfo (  \" key 2  \"  ,     \" val 2  \"  )  ;", "partEvents . addEvent ( event )  ;", "}", "events . addEvent ( partEvents )  ;", "}", ". LOG . info (  \" Events   in   JSON :  \"  )  ;", ". LOG . info ( TimelineUtils . dumpTimelineRecordtoJSON ( events ,    true )  )  ;", "Assert . assertEquals (  2  ,    events . getAllEvents (  )  . size (  )  )  ;", "TimelineEvents . EventsOfOneEntity   partEvents 1     =    events . getAllEvents (  )  . get (  0  )  ;", "Assert . assertEquals (  \" entity   id    0  \"  ,    partEvents 1  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" entity   type    0  \"  ,    partEvents 1  . getEntityType (  )  )  ;", "Assert . assertEquals (  2  ,    partEvents 1  . getEvents (  )  . size (  )  )  ;", "TimelineEvent   event 1  1     =    partEvents 1  . getEvents (  )  . get (  0  )  ;", "Assert . assertEquals (  \" event   type    0  \"  ,    event 1  1  . getEventType (  )  )  ;", "Assert . assertEquals (  2  ,    event 1  1  . getEventInfo (  )  . size (  )  )  ;", "TimelineEvent   event 1  2     =    partEvents 1  . getEvents (  )  . get (  1  )  ;", "Assert . assertEquals (  \" event   type    1  \"  ,    event 1  2  . getEventType (  )  )  ;", "Assert . assertEquals (  2  ,    event 1  2  . getEventInfo (  )  . size (  )  )  ;", "TimelineEvents . EventsOfOneEntity   partEvents 2     =    events . getAllEvents (  )  . get (  1  )  ;", "Assert . assertEquals (  \" entity   id    1  \"  ,    partEvents 2  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" entity   type    1  \"  ,    partEvents 2  . getEntityType (  )  )  ;", "Assert . assertEquals (  2  ,    partEvents 2  . getEvents (  )  . size (  )  )  ;", "TimelineEvent   event 2  1     =    partEvents 2  . getEvents (  )  . get (  0  )  ;", "Assert . assertEquals (  \" event   type    0  \"  ,    event 2  1  . getEventType (  )  )  ;", "Assert . assertEquals (  2  ,    event 2  1  . getEventInfo (  )  . size (  )  )  ;", "TimelineEvent   event 2  2     =    partEvents 2  . getEvents (  )  . get (  1  )  ;", "Assert . assertEquals (  \" event   type    1  \"  ,    event 2  2  . getEventType (  )  )  ;", "Assert . assertEquals (  2  ,    event 2  2  . getEventInfo (  )  . size (  )  )  ;", "}", "METHOD_END"], "methodName": ["testEvents"], "fileName": "org.apache.hadoop.yarn.api.records.timeline.TestTimelineRecords"}, {"methodBody": ["METHOD_START", "{", "TimelinePutResponse   TimelinePutErrors    =    new   TimelinePutResponse (  )  ;", "TimelinePutError   error 1     =    new   TimelinePutError (  )  ;", "error 1  . setEntityId (  \" entity   id    1  \"  )  ;", "error 1  . setEntityId (  \" entity   type    1  \"  )  ;", "error 1  . setErrorCode ( NO _ START _ TIME )  ;", "TimelinePutErrors . addError ( error 1  )  ;", "List < TimelinePutError >    response    =    new   ArrayList < TimelinePutError >  (  )  ;", "response . add ( error 1  )  ;", "TimelinePutError   error 2     =    new   TimelinePutError (  )  ;", "error 2  . setEntityId (  \" entity   id    2  \"  )  ;", "error 2  . setEntityId (  \" entity   type    2  \"  )  ;", "error 2  . setErrorCode ( IO _ EXCEPTION )  ;", "response . add ( error 2  )  ;", "TimelinePutErrors . addErrors ( response )  ;", ". LOG . info (  \" Errors   in   JSON :  \"  )  ;", ". LOG . info ( TimelineUtils . dumpTimelineRecordtoJSON ( TimelinePutErrors ,    true )  )  ;", "Assert . assertEquals (  3  ,    TimelinePutErrors . getErrors (  )  . size (  )  )  ;", "TimelinePutError   e    =    TimelinePutErrors . getErrors (  )  . get (  0  )  ;", "Assert . assertEquals ( error 1  . getEntityId (  )  ,    e . getEntityId (  )  )  ;", "Assert . assertEquals ( error 1  . getEntityType (  )  ,    e . getEntityType (  )  )  ;", "Assert . assertEquals ( error 1  . getErrorCode (  )  ,    e . getErrorCode (  )  )  ;", "e    =    TimelinePutErrors . getErrors (  )  . get (  1  )  ;", "Assert . assertEquals ( error 1  . getEntityId (  )  ,    e . getEntityId (  )  )  ;", "Assert . assertEquals ( error 1  . getEntityType (  )  ,    e . getEntityType (  )  )  ;", "Assert . assertEquals ( error 1  . getErrorCode (  )  ,    e . getErrorCode (  )  )  ;", "e    =    TimelinePutErrors . getErrors (  )  . get (  2  )  ;", "Assert . assertEquals ( error 2  . getEntityId (  )  ,    e . getEntityId (  )  )  ;", "Assert . assertEquals ( error 2  . getEntityType (  )  ,    e . getEntityType (  )  )  ;", "Assert . assertEquals ( error 2  . getErrorCode (  )  ,    e . getErrorCode (  )  )  ;", "}", "METHOD_END"], "methodName": ["testTimelinePutErrors"], "fileName": "org.apache.hadoop.yarn.api.records.timeline.TestTimelineRecords"}, {"methodBody": ["METHOD_START", "{", "AHSProxy . LOG . info (  (  \" Connecting   to   Application   History   server   at    \"     +    ahsAddress )  )  ;", "return    (  ( T )     ( AHSProxy . getProxy ( conf ,    protocol ,    ahsAddress )  )  )  ;", "}", "METHOD_END"], "methodName": ["createAHSProxy"], "fileName": "org.apache.hadoop.yarn.client.AHSProxy"}, {"methodBody": ["METHOD_START", "{", "return   UserGroupInformation . getCurrentUser (  )  . doAs ( new   PrivilegedAction < T >  (  )     {", "@ Override", "public   T   run (  )     {", "return    (  ( T )     ( YarnRPC . create ( conf )  . get ( protocol ,    rmAddress ,    conf )  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getProxy"], "fileName": "org.apache.hadoop.yarn.client.AHSProxy"}, {"methodBody": ["METHOD_START", "{", "return   RMProxy . createRMProxy ( configuration ,    protocol ,    ClientRMProxy . INSTANCE )  ;", "}", "METHOD_END"], "methodName": ["createRMProxy"], "fileName": "org.apache.hadoop.yarn.client.ClientRMProxy"}, {"methodBody": ["METHOD_START", "{", "if    ( HAUtil . isHAEnabled ( conf )  )     {", "ArrayList < String >    services    =    new   ArrayList < String >  (  )  ;", "YarnConfiguration   yarnConf    =    new   YarnConfiguration ( conf )  ;", "for    ( String   rmId    :    HAUtil . getRMHAIds ( conf )  )     {", "yarnConf . set ( RM _ HA _ ID ,    rmId )  ;", "services . add ( SecurityUtil . buildTokenService ( yarnConf . getSocketAddr ( RM _ ADDRESS ,    DEFAULT _ RM _ ADDRESS ,    DEFAULT _ RM _ PORT )  )  . toString (  )  )  ;", "}", "return   new   io . Text ( Joiner . on (  '  ,  '  )  . join ( services )  )  ;", "}", "return   SecurityUtil . buildTokenService ( conf . getSocketAddr ( RM _ ADDRESS ,    DEFAULT _ RM _ ADDRESS ,    DEFAULT _ RM _ PORT )  )  ;", "}", "METHOD_END"], "methodName": ["getRMDelegationTokenService"], "fileName": "org.apache.hadoop.yarn.client.ClientRMProxy"}, {"methodBody": ["METHOD_START", "{", "for    ( Token <  ?    extends   TokenIdentifier >    token    :    UserGroupInformation . getCurrentUser (  )  . getTokens (  )  )     {", "if    ( token . getKind (  )  . equals ( AMRMTokenIdentifier . KIND _ NAME )  )     {", "SecurityUtil . setTokenService ( token ,    resourceManagerAddress )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setupTokens"], "fileName": "org.apache.hadoop.yarn.client.ClientRMProxy"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   InetSocketAddress   rmAddress    =    rmProxy . getRMAddress ( conf ,    protocol )  ;", "return   RMProxy . getProxy ( conf ,    protocol ,    rmAddress )  ;", "}    catch    ( IOException   ioe )     {", ". LOG . error (  (  \" Unable   to   create   proxy   to   the   ResourceManager    \"     +     ( rmServiceIds [ currentProxyIndex ]  )  )  ,    ioe )  ;", "return   null ;", "}", "}", "METHOD_END"], "methodName": ["getProxyInternal"], "fileName": "org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   RMFailoverProxyProvider < T >  >    defaultProviderClass ;", "try    {", "defaultProviderClass    =     (  ( Class <  ?    extends   RMFailoverProxyProvider < T >  >  )     ( Class . forName ( DEFAULT _ CLIENT _ FAILOVER _ PROXY _ PROVIDER )  )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   exceptions . YarnRuntimeException (  (  \" Invalid   default   failover   provider   class \"     +     ( YarnConfiguration . DEFAULT _ CLIENT _ FAILOVER _ PROXY _ PROVIDER )  )  ,    e )  ;", "}", "RMFailoverProxyProvider < T >    provider    =    ReflectionUtils . newInstance ( conf . getClass ( CLIENT _ FAILOVER _ PROXY _ PROVIDER ,    defaultProviderClass ,    RMFailoverProxyProvider . class )  ,    conf )  ;", "provider . init ( conf ,     (  ( RMProxy < T >  )     ( this )  )  ,    protocol )  ;", "return   provider ;", "}", "METHOD_END"], "methodName": ["createRMFailoverProxyProvider"], "fileName": "org.apache.hadoop.yarn.client.RMProxy"}, {"methodBody": ["METHOD_START", "{", "RetryPolicy   retryPolicy    =    RMProxy . createRetryPolicy ( conf )  ;", "T   proxy    =    RMProxy .  < T > getProxy ( conf ,    protocol ,    rmAddress )  ;", "RMProxy . LOG . info (  (  \" Connecting   to   ResourceManager   at    \"     +    rmAddress )  )  ;", "return    (  ( T )     ( RetryProxy . create ( protocol ,    proxy ,    retryPolicy )  )  )  ;", "}", "METHOD_END"], "methodName": ["createRMProxy"], "fileName": "org.apache.hadoop.yarn.client.RMProxy"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =     ( configuration   instanceof   YarnConfiguration )     ?     (  ( YarnConfiguration )     ( configuration )  )     :    new   YarnConfiguration ( configuration )  ;", "RetryPolicy   retryPolicy    =     . createRetryPolicy ( conf )  ;", "if    ( HAUtil . isHAEnabled ( conf )  )     {", "RMFailoverProxyProvider < T >    provider    =    instance . createRMFailoverProxyProvider ( conf ,    protocol )  ;", "return    (  ( T )     ( RetryProxy . create ( protocol ,    provider ,    retryPolicy )  )  )  ;", "} else    {", "InetSocketAddress   rmAddress    =    instance . getRMAddress ( conf ,    protocol )  ;", ". LOG . info (  (  \" Connecting   to   ResourceManager   at    \"     +    rmAddress )  )  ;", "T   proxy    =     .  < T > getProxy ( conf ,    protocol ,    rmAddress )  ;", "return    (  ( T )     ( RetryProxy . create ( protocol ,    proxy ,    retryPolicy )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createRMProxy"], "fileName": "org.apache.hadoop.yarn.client.RMProxy"}, {"methodBody": ["METHOD_START", "{", "long   rmConnectWaitMS    =    conf . getInt ( RESOURCEMANAGER _ CONNECT _ MAX _ WAIT _ MS ,    DEFAULT _ RESOURCEMANAGER _ CONNECT _ MAX _ WAIT _ MS )  ;", "long   rmConnectionRetryIntervalMS    =    conf . getLong ( RESOURCEMANAGER _ CONNECT _ RETRY _ INTERVAL _ MS ,    DEFAULT _ RESOURCEMANAGER _ CONNECT _ RETRY _ INTERVAL _ MS )  ;", "boolean   waitForEver    =    rmConnectWaitMS    =  =     (  -  1  )  ;", "if    (  ! waitForEver )     {", "if    ( rmConnectWaitMS    <     0  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  \" Invalid   Configuration .     \"     +     ( YarnConfiguration . RESOURCEMANAGER _ CONNECT _ MAX _ WAIT _ MS )  )     +     \"    can   be    -  1  ,    but   can   not   be   other   negative   numbers \"  )  )  ;", "}", "if    ( rmConnectWaitMS    <    rmConnectionRetryIntervalMS )     {", "RMProxy . LOG . warn (  (  (  (  ( YarnConfiguration . RESOURCEMANAGER _ CONNECT _ MAX _ WAIT _ MS )     +     \"    is   smaller   than    \"  )     +     ( YarnConfiguration . RESOURCEMANAGER _ CONNECT _ RETRY _ INTERVAL _ MS )  )     +     \"  .    Only   try   connect   once .  \"  )  )  ;", "rmConnectWaitMS    =     0  ;", "}", "}", "if    ( HAUtil . isHAEnabled ( conf )  )     {", "final   long   failoverSleepBaseMs    =    conf . getLong ( CLIENT _ FAILOVER _ SLEEPTIME _ BASE _ MS ,    rmConnectionRetryIntervalMS )  ;", "final   long   failoverSleepMaxMs    =    conf . getLong ( CLIENT _ FAILOVER _ SLEEPTIME _ MAX _ MS ,    rmConnectionRetryIntervalMS )  ;", "int   maxFailoverAttempts    =    conf . getInt ( CLIENT _ FAILOVER _ MAX _ ATTEMPTS ,     (  -  1  )  )  ;", "if    ( maxFailoverAttempts    =  =     (  -  1  )  )     {", "if    ( waitForEver )     {", "maxFailoverAttempts    =    Integer . MAX _ VALUE ;", "} else    {", "maxFailoverAttempts    =     (  ( int )     ( rmConnectWaitMS    /    failoverSleepBaseMs )  )  ;", "}", "}", "return   RetryPolicies . failoverOnNetworkException ( TRY _ ONCE _ THEN _ FAIL ,    maxFailoverAttempts ,    failoverSleepBaseMs ,    failoverSleepMaxMs )  ;", "}", "if    ( waitForEver )     {", "return   RetryPolicies . RETRY _ FOREVER ;", "}", "if    ( rmConnectionRetryIntervalMS    <     0  )     {", "throw   new   exceptions . YarnRuntimeException (  (  (  \" Invalid   Configuration .     \"     +     ( YarnConfiguration . RESOURCEMANAGER _ CONNECT _ RETRY _ INTERVAL _ MS )  )     +     \"    should   not   be   negative .  \"  )  )  ;", "}", "RetryPolicy   retryPolicy    =    RetryPolicies . retryUpToMaximumTimeWithFixedSleep ( rmConnectWaitMS ,    rmConnectionRetryIntervalMS ,    TimeUnit . MILLISECONDS )  ;", "Map < Class <  ?    extends   Exception >  ,    RetryPolicy >    exceptionToPolicyMap    =    new   HashMap < Class <  ?    extends   Exception >  ,    RetryPolicy >  (  )  ;", "exceptionToPolicyMap . put ( ConnectException . class ,    retryPolicy )  ;", "exceptionToPolicyMap . put ( IOException . class ,    retryPolicy )  ;", "return   RetryPolicies . retryByException ( TRY _ ONCE _ THEN _ FAIL ,    exceptionToPolicyMap )  ;", "}", "METHOD_END"], "methodName": ["createRetryPolicy"], "fileName": "org.apache.hadoop.yarn.client.RMProxy"}, {"methodBody": ["METHOD_START", "{", "return   UserGroupInformation . getCurrentUser (  )  . doAs ( new   PrivilegedAction < T >  (  )     {", "@ Override", "public   T   run (  )     {", "return    (  ( T )     ( YarnRPC . create ( conf )  . get ( protocol ,    rmAddress ,    conf )  )  )  ;", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["getProxy"], "fileName": "org.apache.hadoop.yarn.client.RMProxy"}, {"methodBody": ["METHOD_START", "{", "throw   new   UnsupportedOperationException (  (  \" This   method   should   be   invoked    \"     +     \" from   an   instance   of   ClientRMProxy   or   ServerRMProxy \"  )  )  ;", "}", "METHOD_END"], "methodName": ["getRMAddress"], "fileName": "org.apache.hadoop.yarn.client.RMProxy"}, {"methodBody": ["METHOD_START", "{", "String   defaultRMAddress    =    YarnConfiguration . DEFAULT _ RM _ ADDRESS ;", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "Text   tokenService    =     . getRMDelegationTokenService ( conf )  ;", "String [  ]    services    =    tokenService . toString (  )  . split (  \"  ,  \"  )  ;", "assertEquals (  1  ,    services . length )  ;", "for    ( String   service    :    services )     {", "assertTrue (  \" Incorrect   token   service   name \"  ,    service . contains ( defaultRMAddress )  )  ;", "}", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ IDS ,     \" rm 1  , rm 2  \"  )  ;", "conf . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     \" rm 1  \"  )  ,     \"  0  .  0  .  0  .  0  \"  )  ;", "conf . set ( HAUtil . addSuffix ( RM _ HOSTNAME ,     \" rm 2  \"  )  ,     \"  0  .  0  .  0  .  0  \"  )  ;", "tokenService    =     . getRMDelegationTokenService ( conf )  ;", "services    =    tokenService . toString (  )  . split (  \"  ,  \"  )  ;", "assertEquals (  2  ,    services . length )  ;", "for    ( String   service    :    services )     {", "assertTrue (  \" Incorrect   token   service   name \"  ,    service . contains ( defaultRMAddress )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testGetRMDelegationTokenService"], "fileName": "org.apache.hadoop.yarn.client.TestClientRMProxy"}, {"methodBody": ["METHOD_START", "{", "TimelineClient   client    =    new   TimelineClientImpl (  )  ;", "return   client ;", "}", "METHOD_END"], "methodName": ["createTimelineClient"], "fileName": "org.apache.hadoop.yarn.client.api.TimelineClient"}, {"methodBody": ["METHOD_START", "{", "TimelineAuthenticator   authenticator    =    new   TimelineAuthenticator (  )  ;", "Assert . assertFalse ( authenticator . hasDelegationToken ( new   URL (  \" http :  /  / localhost :  8  / resource \"  )  )  )  ;", "Assert . assertFalse ( authenticator . hasDelegationToken ( new   URL (  \" http :  /  / localhost :  8  / resource ? other = xxxx \"  )  )  )  ;", "Assert . assertTrue ( authenticator . hasDelegationToken ( new   URL (  \" http :  /  / localhost :  8  / resource ? delegation = yyyy \"  )  )  )  ;", "Assert . assertTrue ( authenticator . hasDelegationToken ( new   URL (  \" http :  /  / localhost :  8  / resource ? other = xxxx & delegation = yyyy \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testHasDelegationTokens"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "TimelineClientImpl   client    =    spy (  (  ( TimelineClientImpl )     ( TimelineClient . createTimelineClient (  )  )  )  )  ;", "client . init ( conf )  ;", "client . start (  )  ;", "return   client ;", "}", "METHOD_END"], "methodName": ["createTimelineClient"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "TimelineEntity   entity    =    new   TimelineEntity (  )  ;", "entity . setEntityId (  \" entity   id \"  )  ;", "entity . setEntityType (  \" entity   type \"  )  ;", "entity . setStartTime ( System . currentTimeMillis (  )  )  ;", "for    ( int   i    =     0  ;    i    <     2  ;     +  + i )     {", "TimelineEvent   event    =    new   TimelineEvent (  )  ;", "event . setTimestamp ( System . currentTimeMillis (  )  )  ;", "event . setEventType (  (  \" test   event   type    \"     +    i )  )  ;", "event . addEventInfo (  \" key 1  \"  ,     \" val 1  \"  )  ;", "event . addEventInfo (  \" key 2  \"  ,     \" val 2  \"  )  ;", "entity . addEvent ( event )  ;", "}", "entity . addRelatedEntity (  \" test   ref   type    1  \"  ,     \" test   ref   id    1  \"  )  ;", "entity . addRelatedEntity (  \" test   ref   type    2  \"  ,     \" test   ref   id    2  \"  )  ;", "entity . addPrimaryFilter (  \" pkey 1  \"  ,     \" pval 1  \"  )  ;", "entity . addPrimaryFilter (  \" pkey 2  \"  ,     \" pval 2  \"  )  ;", "entity . addOtherInfo (  \" okey 1  \"  ,     \" oval 1  \"  )  ;", "entity . addOtherInfo (  \" okey 2  \"  ,     \" oval 2  \"  )  ;", "return   entity ;", "}", "METHOD_END"], "methodName": ["generateEntity"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "ClientResponse   response    =    mock ( ClientResponse . class )  ;", "if    ( hasRuntimeError )     {", "doThrow ( new   ClientHandlerException ( new   ConnectException (  )  )  )  . when ( client )  . doPostingEntities ( any ( Entities . class )  )  ;", "return   response ;", "}", "doReturn ( response )  . when ( client )  . doPostingEntities ( any ( Entities . class )  )  ;", "when ( response . getClientResponseStatus (  )  )  . thenReturn ( status )  ;", "PutResponse . PutError   error    =    new   PutResponse . PutError (  )  ;", "error . setEntityId (  \" test   entity   id \"  )  ;", "error . setEntityType (  \" test   entity   type \"  )  ;", "error . setErrorCode ( IO _ EXCEPTION )  ;", "PutResponse   putResponse    =    new   PutResponse (  )  ;", "if    ( hasError )     {", "putResponse . addError ( error )  ;", "}", "when ( response . getEntity ( PutResponse . class )  )  . thenReturn ( putResponse )  ;", "return   response ;", "}", "METHOD_END"], "methodName": ["mockClientResponse"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( TIMELINE _ SERVICE _ ENABLED ,    true )  ;", "client    =     . createTimelineClient ( conf )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "if    (  (     !  =    null )     {", "stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["tearDown"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "TestTimelineClient . mockClientResponse ( client ,    OK ,    false ,    false )  ;", "try    {", "TimelinePutResponse   response    =    client . putEntities ( TestTimelineClient . generateEntity (  )  )  ;", "Assert . assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" Exception   is   not   expected \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntities"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "TestTimelineClient . mockClientResponse ( client ,    null ,    false ,    true )  ;", "try    {", "client . putEntities ( TestTimelineClient . generateEntity (  )  )  ;", "Assert . fail (  \" RuntimeException   is   expected \"  )  ;", "}    catch    ( RuntimeException   re )     {", "Assert . assertTrue (  ( re   instanceof   ClientHandlerException )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntitiesConnectionRefused"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "TestTimelineClient . mockClientResponse ( client ,    INTERNAL _ SERVER _ ERROR ,    false ,    false )  ;", "try    {", "client . putEntities ( TestTimelineClient . generateEntity (  )  )  ;", "Assert . fail (  \" Exception   is   expected \"  )  ;", "}    catch    ( YarnException   e )     {", "Assert . assertTrue ( e . getMessage (  )  . contains (  \" Failed   to   get   the   response   from   the   timeline   server .  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntitiesNoResponse"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . unset ( TIMELINE _ SERVICE _ ENABLED )  ;", "TimelineClientImpl   client    =     . createTimelineClient ( conf )  ;", ". mockClientResponse ( client ,    INTERNAL _ SERVER _ ERROR ,    false ,    false )  ;", "try    {", "TimelinePutResponse   response    =    client . putEntities (  . generateEntity (  )  )  ;", "Assert . assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" putEntities   should   already   return   before   throwing   the   exception \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntitiesTimelineServiceDefaultNotEnabled"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . setBoolean ( TIMELINE _ SERVICE _ ENABLED ,    false )  ;", "TimelineClientImpl   client    =     . createTimelineClient ( conf )  ;", ". mockClientResponse ( client ,    INTERNAL _ SERVER _ ERROR ,    false ,    false )  ;", "try    {", "TimelinePutResponse   response    =    client . putEntities (  . generateEntity (  )  )  ;", "Assert . assertEquals (  0  ,    response . getErrors (  )  . size (  )  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" putEntities   should   already   return   before   throwing   the   exception \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntitiesTimelineServiceNotEnabled"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "TestTimelineClient . mockClientResponse ( client ,    OK ,    true ,    false )  ;", "try    {", "TimelinePutResponse   response    =    client . putEntities ( TestTimelineClient . generateEntity (  )  )  ;", "Assert . assertEquals (  1  ,    response . getErrors (  )  . size (  )  )  ;", "Assert . assertEquals (  \" test   entity   id \"  ,    response . getErrors (  )  . get (  0  )  . getEntityId (  )  )  ;", "Assert . assertEquals (  \" test   entity   type \"  ,    response . getErrors (  )  . get (  0  )  . getEntityType (  )  )  ;", "Assert . assertEquals ( IO _ EXCEPTION ,    response . getErrors (  )  . get (  0  )  . getErrorCode (  )  )  ;", "}    catch    ( YarnException   e )     {", "Assert . fail (  \" Exception   is   not   expected \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["testPostEntitiesWithError"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TestTimelineClient"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "sb . append ( url )  ;", "String   separ    =     ( url . toString (  )  . contains (  \"  ?  \"  )  )     ?     \"  &  \"     :     \"  ?  \"  ;", "for    ( Map . Entry < String ,    String >    entry    :    params . entrySet (  )  )     {", "sb . append ( separ )  . append ( entry . getKey (  )  )  . append (  \"  =  \"  )  . append ( URLEncoder . encode ( entry . getValue (  )  ,     \" UTF 8  \"  )  )  ;", "separ    =     \"  &  \"  ;", "}", "return   new   URL ( sb . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["appendParams"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( TimelineAuthenticationConsts . OP _ PARAM ,    TimelineDelegationTokenOperation . CANCELDELEGATIONTOKEN . toString (  )  )  ;", "params . put ( TimelineAuthenticationConsts . TOKEN _ PARAM ,    dToken . encodeToUrlString (  )  )  ;", "url    =     . appendParams ( url ,    params )  ;", "AuthenticatedURL   aUrl    =    new   AuthenticatedURL (  . authenticator ,     . connConfigurator )  ;", "try    {", "HttpURLConnection   conn    =    aUrl . openConnection ( url ,    token )  ;", "conn . setRequestMethod ( TimelineDelegationTokenOperation . CANCELDELEGATIONTOKEN . getHttpMethod (  )  )  ;", ". validateAndParseResponse ( conn )  ;", "}    catch    ( AuthenticationException   ex )     {", "throw   new   IOException ( ex . toString (  )  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["cancelDelegationToken"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "TimelineDelegationTokenOperation   op    =    TimelineDelegationTokenOperation . GETDELEGATIONTOKEN ;", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( TimelineAuthenticationConsts . OP _ PARAM ,    op . toString (  )  )  ;", "params . put ( TimelineAuthenticationConsts . RENEWER _ PARAM ,    renewer )  ;", "url    =     . appendParams ( url ,    params )  ;", "AuthenticatedURL   aUrl    =    new   AuthenticatedURL (  . authenticator ,     . connConfigurator )  ;", "try    {", "HttpURLConnection   conn    =    aUrl . openConnection ( url ,    token )  ;", "conn . setRequestMethod ( op . getHttpMethod (  )  )  ;", "TimelineDelegationTokenResponse   dtRes    =     . validateAndParseResponse ( conn )  ;", "if    (  !  ( dtRes . getType (  )  . equals ( TimelineAuthenticationConsts . DELEGATION _ TOKEN _ URL )  )  )     {", "throw   new   IOException (  (  \" The   response   content   is   not   expected :     \"     +     ( dtRes . getContent (  )  )  )  )  ;", "}", "String   tokenStr    =    dtRes . getContent (  )  . toString (  )  ;", "Token < TimelineDelegationTokenIdentifier >    dToken    =    new   Token < TimelineDelegationTokenIdentifier >  (  )  ;", "dToken . decodeFromUrlString ( tokenStr )  ;", "return   dToken ;", "}    catch    ( AuthenticationException   ex )     {", "throw   new   IOException ( ex . toString (  )  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["getDelegationToken"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "if    (  ( url . getQuery (  )  )     =  =    null )     {", "return   false ;", "} else    {", "return   url . getQuery (  )  . contains (  (  ( ionConsts . DELEGATION _ PARAM )     +     \"  =  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["hasDelegationToken"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "if    ( dtToken    !  =    null )     {", "params . put ( ionConsts . DELEGATION _ PARAM ,    dtToken . encodeToUrlString (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["injectDelegationToken"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( TimelineAuthenticationConsts . OP _ PARAM ,    TimelineDelegationTokenOperation . RENEWDELEGATIONTOKEN . toString (  )  )  ;", "params . put ( TimelineAuthenticationConsts . TOKEN _ PARAM ,    dToken . encodeToUrlString (  )  )  ;", "url    =     . appendParams ( url ,    params )  ;", "AuthenticatedURL   aUrl    =    new   AuthenticatedURL (  . authenticator ,     . connConfigurator )  ;", "try    {", "HttpURLConnection   conn    =    aUrl . openConnection ( url ,    token )  ;", "conn . setRequestMethod ( TimelineDelegationTokenOperation . RENEWDELEGATIONTOKEN . getHttpMethod (  )  )  ;", "TimelineDelegationTokenResponse   dtRes    =     . validateAndParseResponse ( conn )  ;", "if    (  !  ( dtRes . getType (  )  . equals ( TimelineAuthenticationConsts . DELEGATION _ TOKEN _ EXPIRATION _ TIME )  )  )     {", "throw   new   IOException (  (  \" The   response   content   is   not   expected :     \"     +     ( dtRes . getContent (  )  )  )  )  ;", "}", "return   Long . valueOf ( dtRes . getContent (  )  . toString (  )  )  ;", "}    catch    ( AuthenticationException   ex )     {", "throw   new   IOException ( ex . toString (  )  ,    ex )  ;", "}", "}", "METHOD_END"], "methodName": ["renewDelegationToken"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "TimelineAuthenticator . connConfigurator    =    connConfigurator ;", "}", "METHOD_END"], "methodName": ["setStaticConnectionConfigurator"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "int   status    =    conn . getResponseCode (  )  ;", "JsonNode   json    =     . mapper . readTree ( conn . getInputStream (  )  )  ;", "if    ( status    =  =     ( HttpURLConnection . HTTP _ OK )  )     {", "return    . mapper . readValue ( json ,    TimelineDelegationTokenResponse . class )  ;", "} else    {", "try    {", "String   message    =    json . get ( TimelineAuthenticationConsts . ERROR _ MESSAGE _ JSON )  . getTextValue (  )  ;", "String   exception    =    json . get ( TimelineAuthenticationConsts . ERROR _ EXCEPTION _ JSON )  . getTextValue (  )  ;", "String   className    =    json . get ( TimelineAuthenticationConsts . ERROR _ CLASSNAME _ JSON )  . getTextValue (  )  ;", "try    {", "ClassLoader   cl    =     . class . getClassLoader (  )  ;", "Class <  ?  >    klass    =    cl . loadClass ( className )  ;", "Constructor <  ?  >    constr    =    klass . getConstructor ( String . class )  ;", "throw    (  ( IOException )     ( constr . newInstance ( message )  )  )  ;", "}    catch    ( IOException   ex )     {", "throw   ex ;", "}    catch    ( Exception   ex )     {", "throw   new   IOException ( MessageFormat . format (  \"  {  0  }     -     {  1  }  \"  ,    exception ,    message )  )  ;", "}", "}    catch    ( IOException   ex )     {", "if    (  ( ex . getCause (  )  )    instanceof   IOException )     {", "throw    (  ( IOException )     ( ex . getCause (  )  )  )  ;", "}", "throw   new   IOException ( MessageFormat . format (  \" HTTP   status    [  {  0  }  ]  ,     {  1  }  \"  ,    status ,    conn . getResponseMessage (  )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["validateAndParseResponse"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineAuthenticator"}, {"methodBody": ["METHOD_START", "{", "WebResource   webResource    =    client . resource ( resURI )  ;", "return   webResource . accept ( APPLICATION _ JSON )  . type ( APPLICATION _ JSON )  . post ( ClientResponse . class ,    entities )  ;", "}", "METHOD_END"], "methodName": ["doPostingEntities"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "CommandLine   cliParser    =    new   GnuParser (  )  . parse ( TimelineClientImpl . opts ,    argv )  ;", "if    ( cliParser . hasOption (  \" put \"  )  )     {", "String   path    =    cliParser . getOptionValue (  \" put \"  )  ;", "if    (  ( path    !  =    null )     &  &     (  ( path . length (  )  )     >     0  )  )     {", "TimelineClientImpl . putTimelineEntitiesInJSONFile ( path )  ;", "return ;", "}", "}", "TimelineClientImpl . printUsage (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "try    {", "return    . newSslConnConfigurator (  . DEFAULT _ SOCKET _ TIMEOUT ,    conf )  ;", "}    catch    ( Exception   e )     {", ". LOG . debug (  (  \" Cannot   load   customized   ssl   related   configuration .     \"     +     \" Fallback   to   system - generic   settings .  \"  )  ,    e )  ;", "return    . DEFAULT _ TIMEOUT _ CONN _ CONFIGURATOR ;", "}", "}", "METHOD_END"], "methodName": ["newConnConfigurator"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "final   SSLFactory   factory ;", "final   SSLSocketFactory   sf ;", "final   HostnameVerifier   hv ;", "factory    =    new   SSLFactory ( Mode . CLIENT ,    conf )  ;", "factory . init (  )  ;", "sf    =    factory . createSSLSocketFactory (  )  ;", "hv    =    factory . getHostnameVerifier (  )  ;", "return   new   ConnectionConfigurator (  )     {", "@ Override", "public   HttpURLConnection   configure ( HttpURLConnection   conn )    throws   IOException    {", "if    ( conn   instanceof   HttpsURLConnection )     {", "HttpsURLConnection   c    =     (  ( HttpsURLConnection )     ( conn )  )  ;", "c . setSSLSocketFactory ( sf )  ;", "c . setHostnameVerifier ( hv )  ;", "}", ". setTimeouts ( conn ,    timeout )  ;", "return   conn ;", "}", "}  ;", "}", "METHOD_END"], "methodName": ["newSslConnConfigurator"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "new   HelpFormatter (  )  . printHelp (  \" TimelineClient \"  ,    TimelineClientImpl . opts )  ;", "}", "METHOD_END"], "methodName": ["printUsage"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "File   jsonFile    =    new   File ( path )  ;", "if    (  !  ( jsonFile . exists (  )  )  )     {", "System . out . println (  (  (  \" Error :    File    [  \"     +     ( jsonFile . getAbsolutePath (  )  )  )     +     \"  ]    doesn ' t   exist \"  )  )  ;", "return ;", "}", "ObjectMapper   mapper    =    new   ObjectMapper (  )  ;", "YarnJacksonJaxbJsonProvider . configObjectMapper ( mapper )  ;", "TimelineEntities   entities    =    null ;", "try    {", "entities    =    mapper . readValue ( jsonFile ,    TimelineEntities . class )  ;", "}    catch    ( Exception   e )     {", "System . err . println (  (  \" Error :     \"     +     ( e . getMessage (  )  )  )  )  ;", "e . printStackTrace ( System . err )  ;", "return ;", "}", "Configuration   conf    =    new   YarnConfiguration (  )  ;", "client    =     . create (  )  ;", "client . init ( conf )  ;", "client . start (  )  ;", "try    {", "if    (  ( UserGroupInformation . isSecurityEnabled (  )  )     &  &     ( conf . getBoolean ( TIMELINE _ SERVICE _ ENABLED ,    false )  )  )     {", "Token < TimelineDelegationTokenIdentifier >    token    =    client . getDelegationToken ( UserGroupInformation . getCurrentUser (  )  . getUserName (  )  )  ;", "UserGroupInformation . getCurrentUser (  )  . addToken ( token )  ;", "}", "TimelinePutResponse   response    =    client . putEntities ( entities . getEntities (  )  . toArray ( new   TimelineEntity [ entities . getEntities (  )  . size (  )  ]  )  )  ;", "if    (  ( response . getErrors (  )  . size (  )  )     =  =     0  )     {", "System . out . println (  \" Timeline   data   is   successfully   put \"  )  ;", "} else    {", "for    ( TimelinePutResponse . TimelinePutError   error    :    response . getErrors (  )  )     {", "System . out . println (  (  (  (  (  (  \" TimelineEntity    [  \"     +     ( error . getEntityType (  )  )  )     +     \"  :  \"  )     +     ( error . getEntityId (  )  )  )     +     \"  ]    is   not   successfully   put .    Error   code :     \"  )     +     ( error . getErrorCode (  )  )  )  )  ;", "}", "}", "}    catch    ( Exception   e )     {", "System . err . println (  (  \" Error :     \"     +     ( e . getMessage (  )  )  )  )  ;", "e . printStackTrace ( System . err )  ;", "}    finally    {", "client . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["putTimelineEntitiesInJSONFile"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "isEnabled    =    conf . getBoolean ( TIMELINE _ SERVICE _ ENABLED ,    DEFAULT _ TIMELINE _ SERVICE _ ENABLED )  ;", "if    (  !  ( isEnabled )  )     {", ". LOG . info (  \" Timeline   service   is   not   enabled \"  )  ;", "} else    {", "ClientConfig   cc    =    new   DefaultClientConfig (  )  ;", "cc . getClasses (  )  . add ( YarnJacksonJaxbJsonProvider . class )  ;", "ConnectionConfigurator   connConfigurator    =     . newConnConfigurator ( conf )  ;", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "TimelineAuthenticator . setStaticConnectionConfigurator ( connConfigurator )  ;", "urlFactory    =    new    . KerberosAuthenticatedURLConnectionFactory ( connConfigurator )  ;", "client    =    new   com . sun . jersey . api . client . Client ( new   URLConnectionClientHandler ( urlFactory )  ,    cc )  ;", "} else    {", "client    =    new   com . sun . jersey . api . client . Client ( new   URLConnectionClientHandler ( new    . PseudoAuthenticatedURLConnectionFactory ( connConfigurator )  )  ,    cc )  ;", "}", "if    ( YarnConfiguration . useHttps ( conf )  )     {", "resURI    =    URI . create (  . JOINER . join (  \" https :  /  /  \"  ,    conf . get ( TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ ADDRESS )  ,     . RESOURCE _ URI _ STR )  )  ;", "} else    {", "resURI    =    URI . create (  . JOINER . join (  \" http :  /  /  \"  ,    conf . get ( TIMELINE _ SERVICE _ WEBAPP _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ ADDRESS )  ,     . RESOURCE _ URI _ STR )  )  ;", "}", "if    ( UserGroupInformation . isSecurityEnabled (  )  )     {", "urlFactory . setService ( TimelineUtils . buildTimelineTokenService ( conf )  )  ;", "}", ". LOG . info (  (  \" Timeline   service   address :     \"     +     ( resURI )  )  )  ;", "}", "super . serviceInit ( conf )  ;", "}", "METHOD_END"], "methodName": ["serviceInit"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "connection . setConnectTimeout ( socketTimeout )  ;", "connection . setReadTimeout ( socketTimeout )  ;", "}", "METHOD_END"], "methodName": ["setTimeouts"], "fileName": "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"}, {"methodBody": ["METHOD_START", "{", "conf    =    new   Configuration (  )  ;", "conf . set ( RM _ HA _ IDS ,     . RM _ NODE _ IDS _ UNTRIMMED )  ;", "conf . set ( RM _ HA _ ID ,     . RM 1  _ NODE _ ID _ UNTRIMMED )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 1  _ NODE _ ID )  ,     . RM 1  _ ADDRESS _ UNTRIMMED )  ;", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 2  _ NODE _ ID )  ,     . RM 2  _ ADDRESS )  ;", "}", "}", "METHOD_END"], "methodName": ["setUp"], "fileName": "org.apache.hadoop.yarn.conf.TestHAUtil"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" RM   instance   id   is   not   suffixed \"  ,    HAUtil . getConfKeyForRMInstance ( RM _ ADDRESS ,    conf )  . contains ( HAUtil . getRMHAId ( conf )  )  )  ;", "assertFalse (  \" RM   instance   id   is   suffixed \"  ,    HAUtil . getConfKeyForRMInstance ( NM _ ADDRESS ,    conf )  . contains ( HAUtil . getRMHAId ( conf )  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetConfKeyForRMInstance"], "fileName": "org.apache.hadoop.yarn.conf.TestHAUtil"}, {"methodBody": ["METHOD_START", "{", "conf . set ( RM _ HA _ ID ,    TestHAUtil . RM 1  _ NODE _ ID )  ;", "assertEquals (  (  \" Does   not   honor    \"     +     ( YarnConfiguration . RM _ HA _ ID )  )  ,    TestHAUtil . RM 1  _ NODE _ ID ,    HAUtil . getRMHAId ( conf )  )  ;", "conf . clear (  )  ;", "assertNull (  (  (  \" Return   null   when    \"     +     ( YarnConfiguration . RM _ HA _ ID )  )     +     \"    is   not   set \"  )  ,    HAUtil . getRMHAId ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["testGetRMId"], "fileName": "org.apache.hadoop.yarn.conf.TestHAUtil"}, {"methodBody": ["METHOD_START", "{", "conf . set ( RM _ HA _ IDS ,     (  (  ( TestHAUtil . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     ( TestHAUtil . RM 2  _ NODE _ ID )  )  )  ;", "Collection < String >    rmhaIds    =    HAUtil . getRMHAIds ( conf )  ;", "assertEquals (  2  ,    rmhaIds . size (  )  )  ;", "String [  ]    ids    =    rmhaIds . toArray ( new   String [  0  ]  )  ;", "assertEquals ( TestHAUtil . RM 1  _ NODE _ ID ,    ids [  0  ]  )  ;", "assertEquals ( TestHAUtil . RM 2  _ NODE _ ID ,    ids [  1  ]  )  ;", "}", "METHOD_END"], "methodName": ["testGetRMServiceId"], "fileName": "org.apache.hadoop.yarn.conf.TestHAUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "HAUtil . verifyAndSetConfiguration ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "fail (  \" Should   not   throw   any   exceptions .  \"  )  ;", "}", "assertEquals (  \" Should   be   saved   as   Trimmed   collection \"  ,    StringUtils . getStringCollection (  . RM _ NODE _ IDS )  ,    HAUtil . getRMHAIds ( conf )  )  ;", "assertEquals (  \" Should   be   saved   as   Trimmed   string \"  ,     . RM 1  _ NODE _ ID ,    HAUtil . getRMHAId ( conf )  )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "assertEquals (  (  \" RPC   address   not   set   for    \"     +    confKey )  ,     . RM 1  _ ADDRESS ,    conf . get ( confKey )  )  ;", "}", "conf . clear (  )  ;", "conf . set ( RM _ HA _ IDS ,     . RM 1  _ NODE _ ID )  ;", "try    {", "HAUtil . verifyAndSetConfiguration ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertEquals (  \" YarnRuntimeException   by   verifyAndSetRMHAIds (  )  \"  ,     (  ( HAUtil . BAD _ CONFIG _ MESSAGE _ PREFIX )     +     ( HAUtil . getInvalidValueMessage ( RM _ HA _ IDS ,     (  ( conf . get ( RM _ HA _ IDS )  )     +     \"  \\ nHA   mode   requires   atleast   two   RMs \"  )  )  )  )  ,    e . getMessage (  )  )  ;", "}", "conf . clear (  )  ;", "conf . set ( RM _ HA _ IDS ,     (  (  (  . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 2  _ NODE _ ID )  )  )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 1  _ NODE _ ID )  ,     . RM 1  _ ADDRESS )  ;", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 2  _ NODE _ ID )  ,     . RM 2  _ ADDRESS )  ;", "}", "try    {", "HAUtil . verifyAndSetConfiguration ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertEquals (  \" YarnRuntimeException   by   getRMId (  )  \"  ,     (  ( HAUtil . BAD _ CONFIG _ MESSAGE _ PREFIX )     +     ( HAUtil . getNeedToSetValueMessage ( RM _ HA _ ID )  )  )  ,    e . getMessage (  )  )  ;", "}", "conf . clear (  )  ;", "conf . set ( RM _ HA _ ID ,     . RM _ INVALID _ NODE _ ID )  ;", "conf . set ( RM _ HA _ IDS ,     (  (  (  . RM _ INVALID _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 1  _ NODE _ ID )  )  )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "conf . set (  ( confKey    +     (  . RM _ INVALID _ NODE _ ID )  )  ,     . RM _ INVALID _ NODE _ ID )  ;", "}", "try    {", "HAUtil . verifyAndSetConfiguration ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertEquals (  \" YarnRuntimeException   by   addSuffix (  )  \"  ,     (  ( HAUtil . BAD _ CONFIG _ MESSAGE _ PREFIX )     +     ( HAUtil . getInvalidValueMessage ( RM _ HA _ ID ,     . RM _ INVALID _ NODE _ ID )  )  )  ,    e . getMessage (  )  )  ;", "}", "conf . clear (  )  ;", "conf . set ( RM _ HA _ ID ,     . RM 1  _ NODE _ ID )  ;", "conf . set ( RM _ HA _ IDS ,     (  (  (  . RM 1  _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 2  _ NODE _ ID )  )  )  ;", "try    {", "HAUtil . verifyAndSetConfiguration ( conf )  ;", "fail (  \" Should   throw   YarnRuntimeException .    by   Configuration # set (  )  \"  )  ;", "}    catch    ( YarnRuntimeException   e )     {", "String   confKey    =    HAUtil . addSuffix ( RM _ ADDRESS ,     . RM 1  _ NODE _ ID )  ;", "assertEquals (  \" YarnRuntimeException   by   Configuration # set (  )  \"  ,     (  ( HAUtil . BAD _ CONFIG _ MESSAGE _ PREFIX )     +     ( HAUtil . getNeedToSetValueMessage (  (  (  ( HAUtil . addSuffix ( RM _ HOSTNAME ,     . RM 1  _ NODE _ ID )  )     +     \"    or    \"  )     +    confKey )  )  )  )  ,    e . getMessage (  )  )  ;", "}", "conf . clear (  )  ;", "conf . set ( RM _ HA _ IDS ,     (  (  (  . RM 2  _ NODE _ ID )     +     \"  ,  \"  )     +     (  . RM 3  _ NODE _ ID )  )  )  ;", "conf . set ( RM _ HA _ ID ,     . RM 1  _ NODE _ ID _ UNTRIMMED )  ;", "for    ( String   confKey    :    YarnConfiguration . getServiceAddressConfKeys ( conf )  )     {", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 1  _ NODE _ ID )  ,     . RM 1  _ ADDRESS _ UNTRIMMED )  ;", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 2  _ NODE _ ID )  ,     . RM 2  _ ADDRESS )  ;", "conf . set ( HAUtil . addSuffix ( confKey ,     . RM 3  _ NODE _ ID )  ,     . RM 3  _ ADDRESS )  ;", "}", "try    {", "HAUtil . verifyAndSetConfiguration ( conf )  ;", "}    catch    ( YarnRuntimeException   e )     {", "assertEquals (  \" YarnRuntimeException   by   getRMId (  )  ' s   validation \"  ,     (  ( HAUtil . BAD _ CONFIG _ MESSAGE _ PREFIX )     +     ( HAUtil . getRMHAIdNeedToBeIncludedMessage (  \"  [ rm 2  ,    rm 3  ]  \"  ,     . RM 1  _ NODE _ ID )  )  )  ,    e . getMessage (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testVerifyAndSetConfiguration"], "fileName": "org.apache.hadoop.yarn.conf.TestHAUtil"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "String   rmWebUrl    =    WebAppUtils . getRMWebAppURLWithScheme ( conf )  ;", "Assert . assertNotSame (  \" RM   Web   Url   is   not   correct \"  ,     \" http :  /  /  0  .  0  .  0  .  0  :  8  0  8  8  \"  ,    rmWebUrl )  ;", "}", "METHOD_END"], "methodName": ["testDefaultRMWebUrl"], "fileName": "org.apache.hadoop.yarn.conf.TestYarnConfiguration"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf ;", "InetSocketAddress   resourceTrackerAddress ;", "conf    =    new   YarnConfiguration (  )  ;", "resourceTrackerAddress    =    conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ;", "assertEquals ( new   InetSocketAddress ( DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS . split (  \"  :  \"  )  [  0  ]  ,    YarnConfiguration . DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ,    resourceTrackerAddress )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     \"  1  0  .  0  .  0  .  1  \"  )  ;", "resourceTrackerAddress    =    conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ;", "assertEquals ( new   InetSocketAddress (  \"  1  0  .  0  .  0  .  1  \"  ,    YarnConfiguration . DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ,    resourceTrackerAddress )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     \"  1  0  .  0  .  0  .  2  :  5  0  0  1  \"  )  ;", "resourceTrackerAddress    =    conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ;", "assertEquals ( new   InetSocketAddress (  \"  1  0  .  0  .  0  .  2  \"  ,     5  0  0  1  )  ,    resourceTrackerAddress )  ;", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ BIND _ HOST ,     \"  1  0  .  0  .  0  .  3  \"  )  ;", "resourceTrackerAddress    =    conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ;", "assertEquals ( new   InetSocketAddress (  \"  1  0  .  0  .  0  .  3  \"  ,    YarnConfiguration . DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ,    resourceTrackerAddress )  ;", "conf . set ( RM _ BIND _ HOST ,     \"  0  .  0  .  0  .  0  \"  )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     \"  1  0  .  0  .  0  .  2  \"  )  ;", "resourceTrackerAddress    =    conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ;", "assertEquals ( new   InetSocketAddress (  \"  0  .  0  .  0  .  0  \"  ,    YarnConfiguration . DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ,    resourceTrackerAddress )  ;", "conf . set ( RM _ BIND _ HOST ,     \"  0  .  0  .  0  .  0  \"  )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     \"  1  0  .  0  .  0  .  2  :  5  0  0  3  \"  )  ;", "resourceTrackerAddress    =    conf . getSocketAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ PORT )  ;", "assertEquals ( new   InetSocketAddress (  \"  0  .  0  .  0  .  0  \"  ,     5  0  0  3  )  ,    resourceTrackerAddress )  ;", "}", "METHOD_END"], "methodName": ["testGetSocketAddr"], "fileName": "org.apache.hadoop.yarn.conf.TestYarnConfiguration"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( NM _ ADDRESS ,     \"  0  .  0  .  0  .  0  :  1  2  3  4  \"  )  ;", "conf . setBoolean ( RM _ HA _ ENABLED ,    true )  ;", "conf . set ( RM _ HA _ ID ,     \" rm 1  \"  )  ;", "assertTrue ( HAUtil . isHAEnabled ( conf )  )  ;", "InetSocketAddress   addr    =    conf . getSocketAddr ( NM _ ADDRESS ,    DEFAULT _ NM _ ADDRESS ,    DEFAULT _ NM _ PORT )  ;", "assertEquals (  1  2  3  4  ,    addr . getPort (  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetSocketAddressForNMWithHA"], "fileName": "org.apache.hadoop.yarn.conf.TestYarnConfiguration"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ WEBAPP _ ADDRESS ,     \" fortesting :  2  4  5  4  3  \"  )  ;", "conf . set ( RM _ ADDRESS ,     \" rmtesting :  9  9  9  9  \"  )  ;", "String   rmWebUrl    =    WebAppUtils . getRMWebAppURLWithScheme ( conf )  ;", "String [  ]    parts    =    rmWebUrl . split (  \"  :  \"  )  ;", "Assert . assertEquals (  \" RM   Web   URL   Port   is   incrrect \"  ,     2  4  5  4  3  ,    Integer . valueOf ( parts [  (  ( parts . length )     -     1  )  ]  )  . intValue (  )  )  ;", "Assert . assertNotSame (  \" RM   Web   Url   not   resolved   correctly .    Should   not   be   rmtesting \"  ,     \" http :  /  / rmtesting :  2  4  5  4  3  \"  ,    rmWebUrl )  ;", "}", "METHOD_END"], "methodName": ["testRMWebUrlSpecified"], "fileName": "org.apache.hadoop.yarn.conf.TestYarnConfiguration"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   conf ;", "InetSocketAddress   resourceTrackerConnectAddress ;", "InetSocketAddress   serverAddress ;", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     \" yo . yo . yo \"  )  ;", "serverAddress    =    new   InetSocketAddress ( DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS . split (  \"  :  \"  )  [  0  ]  ,    Integer . valueOf ( DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS . split (  \"  :  \"  )  [  1  ]  )  )  ;", "resourceTrackerConnectAddress    =    conf . updateConnectAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    serverAddress )  ;", "assertFalse ( resourceTrackerConnectAddress . toString (  )  . startsWith (  \" yo . yo . yo \"  )  )  ;", "conf    =    new   YarnConfiguration (  )  ;", "conf . set ( RM _ RESOURCE _ TRACKER _ ADDRESS ,     \" yo . yo . yo \"  )  ;", "conf . set ( RM _ BIND _ HOST ,     \"  0  .  0  .  0  .  0  \"  )  ;", "serverAddress    =    new   InetSocketAddress ( DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS . split (  \"  :  \"  )  [  0  ]  ,    Integer . valueOf ( DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS . split (  \"  :  \"  )  [  1  ]  )  )  ;", "resourceTrackerConnectAddress    =    conf . updateConnectAddr ( RM _ BIND _ HOST ,    RM _ RESOURCE _ TRACKER _ ADDRESS ,    DEFAULT _ RM _ RESOURCE _ TRACKER _ ADDRESS ,    serverAddress )  ;", "assertTrue ( resourceTrackerConnectAddress . toString (  )  . startsWith (  \" yo . yo . yo \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testUpdateConnectAddr"], "fileName": "org.apache.hadoop.yarn.conf.TestYarnConfiguration"}, {"methodBody": ["METHOD_START", "{", "return   new   Runnable (  )     {", "@ Override", "public   void   run (  )     {", "while    (  (  !  ( stopped )  )     &  &     (  !  ( Thread . currentThread (  )  . isInterrupted (  )  )  )  )     {", "drained    =    eventQueue . isEmpty (  )  ;", "if    ( blockNewEvents )     {", "synchronized ( waitForDrained )     {", "if    ( drained )     {", "waitForDrained . notify (  )  ;", "}", "}", "}", "Event   event ;", "try    {", "event    =    eventQueue . take (  )  ;", "}    catch    ( InterruptedException   ie )     {", "if    (  !  ( stopped )  )     {", ". LOG . warn (  \"    thread   interrupted \"  ,    ie )  ;", "}", "return ;", "}", "if    ( event    !  =    null )     {", "dispatch ( event )  ;", "}", "}", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createThread"], "fileName": "org.apache.hadoop.yarn.event.AsyncDispatcher"}, {"methodBody": ["METHOD_START", "{", "if    ( AsyncDispatcher . LOG . isDebugEnabled (  )  )     {", "AsyncDispatcher . LOG . debug (  (  (  (  \" Dispatching   the   event    \"     +     ( event . getClass (  )  . getName (  )  )  )     +     \"  .  \"  )     +     ( event . toString (  )  )  )  )  ;", "}", "Class <  ?    extends   Enum >    type    =    event . getType (  )  . getDeclaringClass (  )  ;", "try    {", "EventHandler   handler    =    eventDispatchers . get ( type )  ;", "if    ( handler    !  =    null )     {", "handler . handle ( event )  ;", "} else    {", "throw   new   Exception (  (  \" No   handler   for   registered   for    \"     +    type )  )  ;", "}", "}    catch    ( Throwable   t )     {", "AsyncDispatcher . LOG . fatal (  \" Error   in   dispatcher   thread \"  ,    t )  ;", "if    (  (  ( exitOnDispatchException )     &  &     (  ( ShutdownHookManager . get (  )  . isShutdownInProgress (  )  )     =  =    false )  )     &  &     (  ( stopped )     =  =    false )  )     {", "AsyncDispatcher . LOG . info (  \" Exiting ,    bbye .  .  \"  )  ;", "System . exit (  (  -  1  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["dispatch"], "fileName": "org.apache.hadoop.yarn.event.AsyncDispatcher"}, {"methodBody": ["METHOD_START", "{", "drainEventsOnStop    =    true ;", "}", "METHOD_END"], "methodName": ["setDrainEventsOnStop"], "fileName": "org.apache.hadoop.yarn.event.AsyncDispatcher"}, {"methodBody": ["METHOD_START", "{", "while    (  !  ( drained )  )     {", "Thread . yield (  )  ;", "}", "}", "METHOD_END"], "methodName": ["await"], "fileName": "org.apache.hadoop.yarn.event.DrainDispatcher"}, {"methodBody": ["METHOD_START", "{", "return   RecordFactoryPBImpl . self ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RecordFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   fqName    =    clazz . getName (  )  ;", "return   fqName . substring (  (  ( fqName . lastIndexOf (  \"  .  \"  )  )     +     1  )  ,    fqName . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["getClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RecordFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   srcPackagePart    =    getPackageName ( clazz )  ;", "String   srcClassName    =    getClassName ( clazz )  ;", "String   destPackagePart    =     ( srcPackagePart    +     \"  .  \"  )     +     (  . PB _ IMPL _ PACKAGE _ SUFFIX )  ;", "String   destClassPart    =    srcClassName    +     (  . PB _ IMPL _ CLASS _ SUFFIX )  ;", "return    ( destPackagePart    +     \"  .  \"  )     +    destClassPart ;", "}", "METHOD_END"], "methodName": ["getPBImplClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RecordFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   clazz . getPackage (  )  . getName (  )  ;", "}", "METHOD_END"], "methodName": ["getPackageName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RecordFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RpcClientFactoryPBImpl . self ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   fqName    =    clazz . getName (  )  ;", "return   fqName . substring (  (  ( fqName . lastIndexOf (  \"  .  \"  )  )     +     1  )  ,    fqName . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["getClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "Constructor <  ?  >    constructor    =    cache . get ( protocol )  ;", "if    ( constructor    =  =    null )     {", "Class <  ?  >    pbClazz    =    null ;", "try    {", "pbClazz    =    localConf . getClassByName ( getClassName ( protocol )  )  ;", "}    catch    ( ClassNotFoundException   e )     {", "throw   new   YarnRuntimeException (  (  (  \" Failed   to   load   class :     [  \"     +     ( getClassName ( protocol )  )  )     +     \"  ]  \"  )  ,    e )  ;", "}", "try    {", "constructor    =    pbClazz . getConstructor ( Long . TYPE ,    InetSocketAddress . class ,    Configuration . class )  ;", "constructor . setAccessible ( true )  ;", "cache . putIfAbsent ( protocol ,    constructor )  ;", "}    catch    ( NoSuchMethodException   e )     {", "throw   new   YarnRuntimeException (  (  (  (  (  (  \" Could   not   find   constructor   with   params :     \"     +     ( Long . TYPE )  )     +     \"  ,     \"  )     +     ( InetSocketAddress . class )  )     +     \"  ,     \"  )     +     ( Configuration . class )  )  ,    e )  ;", "}", "}", "try    {", "Object   retObject    =    constructor . newInstance ( clientVersion ,    addr ,    conf )  ;", "return   retObject ;", "}    catch    ( InvocationTargetException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( IllegalAccessException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( InstantiationException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getClient"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   srcPackagePart    =    getPackageName ( clazz )  ;", "String   srcClassName    =    getClassName ( clazz )  ;", "String   destPackagePart    =     ( srcPackagePart    +     \"  .  \"  )     +     (  . PB _ IMPL _ PACKAGE _ SUFFIX )  ;", "String   destClassPart    =    srcClassName    +     (  . PB _ IMPL _ CLASS _ SUFFIX )  ;", "return    ( destPackagePart    +     \"  .  \"  )     +    destClassPart ;", "}", "METHOD_END"], "methodName": ["getPBImplClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   clazz . getPackage (  )  . getName (  )  ;", "}", "METHOD_END"], "methodName": ["getPackageName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "RPC . setProtocolEngine ( conf ,    pbProtocol ,    ProtobufRpcEngine . class )  ;", "RPC . Server   server    =    new   RPC . Builder ( conf )  . setProtocol ( pbProtocol )  . setInstance ( blockingService )  . setBindAddress ( addr . getHostName (  )  )  . setPort ( addr . getPort (  )  )  . setNumHandlers ( numHandlers )  . setVerbose ( false )  . setSecretManager ( secretManager )  . setPortRangeConfig ( portRangeConfig )  . build (  )  ;", ". LOG . info (  (  (  \" Adding   protocol    \"     +     ( pbProtocol . getCanonicalName (  )  )  )     +     \"    to   the   server \"  )  )  ;", "server . addProtocol ( RPC _ PROTOCOL _ BUFFER ,    pbProtocol ,    blockingService )  ;", "return   server ;", "}", "METHOD_END"], "methodName": ["createServer"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   RpcServerFactoryPBImpl . self ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   fqName    =    clazz . getName (  )  ;", "return   fqName . substring (  (  ( fqName . lastIndexOf (  \"  .  \"  )  )     +     1  )  ,    fqName . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["getClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   clazz . getPackage (  )  . getName (  )  ;", "}", "METHOD_END"], "methodName": ["getPackageName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   srcPackagePart    =    getPackageName ( clazz )  ;", "String   srcClassName    =    getClassName ( clazz )  ;", "String   destPackagePart    =     ( srcPackagePart    +     \"  .  \"  )     +     (  . PB _ IMPL _ PACKAGE _ SUFFIX )  ;", "String   destClassPart    =    srcClassName    +     (  . PB _ IMPL _ CLASS _ SUFFIX )  ;", "return    ( destPackagePart    +     \"  .  \"  )     +    destClassPart ;", "}", "METHOD_END"], "methodName": ["getPbServiceImplClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   srcClassName    =    getClassName ( clazz )  ;", "return    (  (  (  (  (  . PROTO _ GEN _ PACKAGE _ NAME )     +     \"  .  \"  )     +    srcClassName )     +     \"  $  \"  )     +    srcClassName )     +     (  . PROTO _ GEN _ CLASS _ SUFFIX )  ;", "}", "METHOD_END"], "methodName": ["getProtoClassName"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   getServer ( protocol ,    instance ,    addr ,    conf ,    secretManager ,    numHandlers ,    null )  ;", "}", "METHOD_END"], "methodName": ["getServer"], "fileName": "org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl"}, {"methodBody": ["METHOD_START", "{", "String   clientFactoryClassName    =    conf . get ( IPC _ CLIENT _ FACTORY _ CLASS ,    DEFAULT _ IPC _ CLIENT _ FACTORY _ CLASS )  ;", "return    (  ( RpcClientFactory )     (  . getFactoryClassInstance ( clientFactoryClassName )  )  )  ;", "}", "METHOD_END"], "methodName": ["getClientFactory"], "fileName": "org.apache.hadoop.yarn.factory.providers.RpcFactoryProvider"}, {"methodBody": ["METHOD_START", "{", "try    {", "Class <  ?  >    clazz    =    Class . forName ( ClassName )  ;", "Method   method    =    clazz . getMethod (  \" get \"  ,    null )  ;", "method . setAccessible ( true )  ;", "return   method . invoke ( null ,    null )  ;", "}    catch    ( ClassNotFoundException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( NoSuchMethodException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( InvocationTargetException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}    catch    ( IllegalAccessException   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getFactoryClassInstance"], "fileName": "org.apache.hadoop.yarn.factory.providers.RpcFactoryProvider"}, {"methodBody": ["METHOD_START", "{", "if    ( conf    =  =    null )     {", "conf    =    new   Configuration (  )  ;", "}", "String   serverFactoryClassName    =    conf . get ( IPC _ SERVER _ FACTORY _ CLASS ,    DEFAULT _ IPC _ SERVER _ FACTORY _ CLASS )  ;", "return    (  ( RpcServerFactory )     (  . getFactoryClassInstance ( serverFactoryClassName )  )  )  ;", "}", "METHOD_END"], "methodName": ["getServerFactory"], "fileName": "org.apache.hadoop.yarn.factory.providers.RpcFactoryProvider"}, {"methodBody": ["METHOD_START", "{", "return   new   YarnException ( message )  ;", "}", "METHOD_END"], "methodName": ["getRemoteException"], "fileName": "org.apache.hadoop.yarn.ipc.RPCUtil"}, {"methodBody": ["METHOD_START", "{", "return   new   YarnException ( t )  ;", "}", "METHOD_END"], "methodName": ["getRemoteException"], "fileName": "org.apache.hadoop.yarn.ipc.RPCUtil"}, {"methodBody": ["METHOD_START", "{", "try    {", "Construct <  ?    extends   T >    cn    =    cls . getConstruct ( String . class )  ;", "cn . setAccessible ( true )  ;", "T   ex    =    cn . newInstance ( re . getMessage (  )  )  ;", "ex . initCause ( re )  ;", "return   ex ;", "}    catch    ( NoSuchMethodException   e )     {", "throw   re ;", "}    catch    ( IllegalArgumentException   e )     {", "throw   re ;", "}    catch    ( SecurityException   e )     {", "throw   re ;", "}    catch    ( InstantiationException   e )     {", "throw   re ;", "}    catch    ( IllegalAccessException   e )     {", "throw   re ;", "}    catch    ( InvocationTargetException   e )     {", "throw   re ;", "}", "}", "METHOD_END"], "methodName": ["instantiateException"], "fileName": "org.apache.hadoop.yarn.ipc.RPCUtil"}, {"methodBody": ["METHOD_START", "{", "Throwable   cause    =    se . getCause (  )  ;", "if    ( cause    =  =    null )     {", "throw   new   IOException ( se )  ;", "} else    {", "if    ( cause   instanceof   RemoteException )     {", "RemoteException   re    =     (  ( RemoteException )     ( cause )  )  ;", "Class <  ?  >    realClass    =    null ;", "try    {", "realClass    =    Class . forName ( re . getClassName (  )  )  ;", "}    catch    ( ClassNotFoundException   cnf )     {", "throw    . instantiateException ( YarnException . class ,    re )  ;", "}", "if    ( YarnException . class . isAssignableFrom ( realClass )  )     {", "throw    . instantiateException ( realClass . asSubclass ( YarnException . class )  ,    re )  ;", "} else", "if    ( IOException . class . isAssignableFrom ( realClass )  )     {", "throw    . instantiateException ( realClass . asSubclass ( IOException . class )  ,    re )  ;", "} else", "if    ( RuntimeException . class . isAssignableFrom ( realClass )  )     {", "throw    . instantiateException ( realClass . asSubclass ( RuntimeException . class )  ,    re )  ;", "} else    {", "throw   re ;", "}", "} else", "if    ( cause   instanceof   IOException )     {", "throw    (  ( IOException )     ( cause )  )  ;", "} else", "if    ( cause   instanceof   RuntimeException )     {", "throw    (  ( RuntimeException )     ( cause )  )  ;", "} else    {", "throw   new   IOException ( se )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["unwrapAndThrowException"], "fileName": "org.apache.hadoop.yarn.ipc.RPCUtil"}, {"methodBody": ["METHOD_START", "{", "String   message    =     \" DirectIOExceptionMessage \"  ;", "IOException   ioException    =    new   FileNotFoundException ( message )  ;", "ServiceException   se    =    new   ServiceException ( ioException )  ;", "Throwable   t    =    null ;", "try    {", ". unwrapAndThrowException ( se )  ;", "}    catch    ( Throwable   thrown )     {", "t    =    thrown ;", "}", "Assert . assertTrue ( FileNotFoundException . class . isInstance ( t )  )  ;", "Assert . assertTrue ( t . getMessage (  )  . contains ( message )  )  ;", "}", "METHOD_END"], "methodName": ["testRPCIOExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "String   message    =     \" RPCRuntimeExceptionUnwrapping \"  ;", "RuntimeException   re    =    new   NullPointerException ( message )  ;", "ServiceException   se    =    new   ServiceException ( re )  ;", "Throwable   t    =    null ;", "try    {", ". unwrapAndThrowException ( se )  ;", "}    catch    ( Throwable   thrown )     {", "t    =    thrown ;", "}", "Assert . assertTrue ( NullPointerException . class . isInstance ( t )  )  ;", "Assert . assertTrue ( t . getMessage (  )  . contains ( message )  )  ;", "}", "METHOD_END"], "methodName": ["testRPCRuntimeExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "String   message    =     \" ServiceExceptionMessage \"  ;", "ServiceException   se    =    new   ServiceException ( message )  ;", "Throwable   t    =    null ;", "try    {", ". unwrapAndThrowException ( se )  ;", "}    catch    ( Throwable   thrown )     {", "t    =    thrown ;", "}", "Assert . assertTrue ( IOException . class . isInstance ( t )  )  ;", "Assert . assertTrue ( t . getMessage (  )  . contains ( message )  )  ;", "}", "METHOD_END"], "methodName": ["testRPCServiceExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    FileNotFoundException . class ;", "verifyRemoteExceptionUnwrapping ( exception ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoteIOExceptionDerivativeUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    IOException . class ;", "verifyRemoteExceptionUnwrapping ( exception ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoteIOExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    NullPointerException . class ;", "verifyRemoteExceptionUnwrapping ( exception ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoteRuntimeExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    TestRPCUtil . YarnTestException . class ;", "verifyRemoteExceptionUnwrapping ( exception ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoteYarnExceptionDerivativeUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    YarnException . class ;", "verifyRemoteExceptionUnwrapping ( exception ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoteYarnExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    TestRPCUtil . YarnTestExceptionNoConstructor . class ;", "verifyRemoteExceptionUnwrapping ( RemoteException . class ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testRemoteYarnExceptionWithoutStringConstructor"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    Exception . class ;", "verifyRemoteExceptionUnwrapping ( RemoteException . class ,    exception . getName (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUnexpectedRemoteExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "Class <  ?    extends   Throwable >    exception    =    YarnException . class ;", "String   className    =     \" UnknownException . class \"  ;", "verifyRemoteExceptionUnwrapping ( exception ,    className )  ;", "}", "METHOD_END"], "methodName": ["testUnknownExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "String   message    =    realExceptionClassName    +     \" Message \"  ;", "RemoteException   re    =    new   RemoteException ( realExceptionClassName ,    message )  ;", "ServiceException   se    =    new   ServiceException ( re )  ;", "Throwable   t    =    null ;", "try    {", ". unwrapAndThrowException ( se )  ;", "}    catch    ( Throwable   thrown )     {", "t    =    thrown ;", "}", "Assert . assertTrue (  (  (  (  \" Expected   exception    [  \"     +    expectedLocalException )     +     \"  ]    but   found    \"  )     +    t )  ,    expectedLocalException . isInstance ( t )  )  ;", "Assert . assertTrue (  (  (  (  \" Expected   message    [  \"     +    message )     +     \"  ]    but   found    \"  )     +     ( t . getMessage (  )  )  )  ,    t . getMessage (  )  . contains ( message )  )  ;", "}", "METHOD_END"], "methodName": ["verifyRemoteExceptionUnwrapping"], "fileName": "org.apache.hadoop.yarn.ipc.TestRPCUtil"}, {"methodBody": ["METHOD_START", "{", "YarnRPC . LOG . debug (  (  \" Creating   YarnRPC   for    \"     +     ( conf . get ( IPC _ RPC _ IMPL )  )  )  )  ;", "String   clazzName    =    conf . get ( IPC _ RPC _ IMPL )  ;", "if    ( clazzName    =  =    null )     {", "clazzName    =    YarnConfiguration . DEFAULT _ IPC _ RPC _ IMPL ;", "}", "try    {", "return    (  ( YarnRPC )     ( Class . forName ( clazzName )  . newInstance (  )  )  )  ;", "}    catch    ( Exception   e )     {", "throw   new   YarnRuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["create"], "fileName": "org.apache.hadoop.yarn.ipc.YarnRPC"}, {"methodBody": ["METHOD_START", "{", "return   getServer ( protocol ,    instance ,    addr ,    conf ,    secretManager ,    numHandlers ,    null )  ;", "}", "METHOD_END"], "methodName": ["getServer"], "fileName": "org.apache.hadoop.yarn.ipc.YarnRPC"}, {"methodBody": ["METHOD_START", "{", "return   new   Configuration (  )  ;", "}", "METHOD_END"], "methodName": ["createConf"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "return   checkIntervalMsecs ;", "}", "METHOD_END"], "methodName": ["getCheckIntervalMsecs"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "if    ( e   instanceof   AccessControlException )     {", "String   message    =    e . getMessage (  )  ;", "message    =    message . split (  \"  \\ n \"  )  [  0  ]  ;", ". LOG . warn (  (  ( comment    +     \"     \"  )     +    message )  )  ;", "} else    {", ". LOG . error ( comment ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["logIOException"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( getServiceState (  )  )     =  =     ( STATE . STARTED )  )     {", "Configuration   conf    =    createConf (  )  ;", "setConfig ( conf )  ;", "stopTimer (  )  ;", "scheduleLogDeletionTask (  )  ;", "} else    {", ". LOG . warn (  \" Failed   to   execute   refreshLogRetentionSettings    :    Aggregated   Log   Deletion   Service   is   not   started \"  )  ;", "}", "}", "METHOD_END"], "methodName": ["refreshLogRetentionSettings"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "if    (  !  ( conf . getBoolean ( LOG _ AGGREGATION _ ENABLED ,    DEFAULT _ LOG _ AGGREGATION _ ENABLED )  )  )     {", "return ;", "}", "long   retentionSecs    =    conf . getLong ( LOG _ AGGREGATION _ RETAIN _ SECONDS ,    DEFAULT _ LOG _ AGGREGATION _ RETAIN _ SECONDS )  ;", "if    ( retentionSecs    <     0  )     {", ". LOG . info (  (  (  (  \" Log   Aggregation   deletion   is   disabled   because   retention   is \"     +     \"    too   small    (  \"  )     +    retentionSecs )     +     \"  )  \"  )  )  ;", "return ;", "}", "setLogAggCheckIntervalMsecs ( retentionSecs )  ;", "TimerTask   task    =    new    . LogDeletionTask ( conf ,    retentionSecs )  ;", "timer    =    new   Timer (  )  ;", "timer . scheduleAtFixedRate ( task ,     0  ,    checkIntervalMsecs )  ;", "}", "METHOD_END"], "methodName": ["scheduleLogDeletionTask"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    getConfig (  )  ;", "checkIntervalMsecs    =     1  0  0  0     *     ( conf . getLong ( LOG _ AGGREGATION _ RETAIN _ CHECK _ INTERVAL _ SECONDS ,    DEFAULT _ LOG _ AGGREGATION _ RETAIN _ CHECK _ INTERVAL _ SECONDS )  )  ;", "if    (  ( checkIntervalMsecs )     <  =     0  )     {", "checkIntervalMsecs    =     ( retencs    *     1  0  0  0  )     /     1  0  ;", "}", "}", "METHOD_END"], "methodName": ["setLogAggCheckIntervalMsecs"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "if    (  ( timer )     !  =    null )     {", "timer . cancel (  )  ;", "}", "}", "METHOD_END"], "methodName": ["stopTimer"], "fileName": "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "return   nodeId . toString (  )  . replace (  \"  :  \"  ,     \"  _  \"  )  ;", "}", "METHOD_END"], "methodName": ["getNodeString"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( LogAggregationUtils . getRemoteLogSuffixedDir ( remoteRootLogDir ,    user ,    suffix )  ,    appId . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["getRemoteAppLogDir"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( suffix    =  =    null )     |  |     ( suffix . isEmpty (  )  )  )     {", "return    . getRemoteLogUserDir ( remoteRootLogDir ,    user )  ;", "}", "return   new   Path (  . getRemoteLogUserDir ( remoteRootLogDir ,    user )  ,    suffix )  ;", "}", "METHOD_END"], "methodName": ["getRemoteLogSuffixedDir"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( remoteRootLogDir ,    user )  ;", "}", "METHOD_END"], "methodName": ["getRemoteLogUserDir"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"}, {"methodBody": ["METHOD_START", "{", "return   conf . get ( NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX ,    DEFAULT _ NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX )  ;", "}", "METHOD_END"], "methodName": ["getRemoteNodeLogDirSuffix"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"}, {"methodBody": ["METHOD_START", "{", "return   new   Path ( LogAggregationUtils . getRemoteAppLogDir ( remoteRootLogDir ,    appId ,    user ,    suffix )  ,    LogAggregationUtils . getNodeString ( nodeId )  )  ;", "}", "METHOD_END"], "methodName": ["getRemoteNodeLogFileForApp"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"}, {"methodBody": ["METHOD_START", "{", "DataInputStream   valueStream ;", "AedLogFormat . LogKey   key    =    new   AedLogFormat . LogKey (  )  ;", "valueStream    =    reader . next ( key )  ;", "while    (  ( valueStream    !  =    null )     &  &     (  !  ( key . toString (  )  . equals ( containerIdStr )  )  )  )     {", "key    =    new   AedLogFormat . LogKey (  )  ;", "valueStream    =    reader . next ( key )  ;", "}", "if    ( valueStream    =  =    null )     {", "System . out . println (  (  (  \" Logs   for   container    \"     +    containerIdStr )     +     \"    are   not   present   in   this   log - file .  \"  )  )  ;", "return    -  1  ;", "}", "while    ( true )     {", "try    {", "AedLogFormat . LogReader . readAContainerLogsForALogType ( valueStream ,    out )  ;", "}    catch    ( EOFException   eof )     {", "break ;", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["dumpAContainerLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogCLIHelpers"}, {"methodBody": ["METHOD_START", "{", "Path   remoteRootLogDir    =    new   Path ( getConf (  )  . get ( NM _ REMOTE _ APP _ LOG _ DIR ,    DEFAULT _ NM _ REMOTE _ APP _ LOG _ DIR )  )  ;", "String   suffix    =    LogAggregationUtils . getRemoteNodeLogDirSuffix ( getConf (  )  )  ;", "Path   logPath    =    LogAggregationUtils . getRemoteNodeLogFileForApp ( remoteRootLogDir ,    ConverterUtils . toApplicationId ( appId )  ,    jobOwner ,    ConverterUtils . toNodeId ( nodeId )  ,    suffix )  ;", "AggregatedLogFormat . LogReader   reader ;", "try    {", "reader    =    new   AggregatedLogFormat . LogReader ( getConf (  )  ,    logPath )  ;", "}    catch    ( FileNotFoundException   fnfe )     {", "System . out . println (  (  \" Logs   not   available   at    \"     +     ( logPath . toString (  )  )  )  )  ;", "System . out . println (  \" Log      has   not   completed   or   is   not   enabled .  \"  )  ;", "return    -  1  ;", "}", "return   dumpAContainerLogs ( containerId ,    reader ,    System . out )  ;", "}", "METHOD_END"], "methodName": ["dumpAContainersLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogCLIHelpers"}, {"methodBody": ["METHOD_START", "{", "Path   remoteRootLogDir    =    new   Path ( getConf (  )  . get ( NM _ REMOTE _ APP _ LOG _ DIR ,    DEFAULT _ NM _ REMOTE _ APP _ LOG _ DIR )  )  ;", "String   user    =    appOwner ;", "String   logDirSuffix    =    LogAggregationUtils . getRemoteNodeLogDirSuffix ( getConf (  )  )  ;", "Path   remoteAppLogDir    =    LogAggregationUtils . getRemoteAppLogDir ( remoteRootLogDir ,    appId ,    user ,    logDirSuffix )  ;", "RemoteIterator < FileStatus >    nodeFiles ;", "try    {", "Path   qualifiedLogDir    =    FileContext . getFileContext ( getConf (  )  )  . makeQualified ( remoteAppLogDir )  ;", "nodeFiles    =    FileContext . getFileContext ( qualifiedLogDir . toUri (  )  ,    getConf (  )  )  . listStatus ( remoteAppLogDir )  ;", "}    catch    ( FileNotFoundException   fnf )     {", "System . out . println (  (  \" Logs   not   available   at    \"     +     ( remoteAppLogDir . toString (  )  )  )  )  ;", "System . out . println (  \" Log      has   not   completed   or   is   not   enabled .  \"  )  ;", "return    -  1  ;", "}", "while    ( nodeFiles . hasNext (  )  )     {", "FileStatus   thisNodeFile    =    nodeFiles . next (  )  ;", "AggregatedLogFormat . LogReader   reader    =    new   AggregatedLogFormat . LogReader ( getConf (  )  ,    new   Path ( remoteAppLogDir ,    thisNodeFile . getPath (  )  . getName (  )  )  )  ;", "try    {", "DataInputStream   valueStream ;", "AggregatedLogFormat . LogKey   key    =    new   AggregatedLogFormat . LogKey (  )  ;", "valueStream    =    reader . next ( key )  ;", "while    ( valueStream    !  =    null )     {", "String   containerString    =     (  (  \"  \\ n \\ nContainer :     \"     +    key )     +     \"    on    \"  )     +     ( thisNodeFile . getPath (  )  . getName (  )  )  ;", "out . println ( containerString )  ;", "out . println ( StringUtils . repeat (  \"  =  \"  ,    containerString . length (  )  )  )  ;", "while    ( true )     {", "try    {", "AggregatedLogFormat . LogReader . readAContainerLogsForALogType ( valueStream ,    out )  ;", "}    catch    ( EOFException   eof )     {", "break ;", "}", "}", "key    =    new   AggregatedLogFormat . LogKey (  )  ;", "valueStream    =    reader . next ( key )  ;", "}", "}    finally    {", "reader . close (  )  ;", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["dumpAllContainersLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.LogCLIHelpers"}, {"methodBody": ["METHOD_START", "{", "FileSystem . closeAll (  )  ;", "}", "METHOD_END"], "methodName": ["closeFilesystems"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "long   RETENTION _ SECS    =     (  1  0     *     2  4  )     *     3  6  0  0  ;", "long   now    =    System . currentTimeMillis (  )  ;", "long   toDeleteTime    =    now    -     ( RETENTION _ SECS    *     1  0  0  0  )  ;", "String   root    =     \" mockfs :  /  / foo /  \"  ;", "String   remoteRootLogDir    =    root    +     \" tmp / logs \"  ;", "String   suffix    =     \" logs \"  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass (  \" fs . mockfs . impl \"  ,     . MockFileSystem . class ,    FileSystem . class )  ;", "conf . set ( LOG _ AGGREGATION _ ENABLED ,     \" true \"  )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ SECONDS ,     \"  8  6  4  0  0  0  \"  )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ CHECK _ INTERVAL _ SECONDS ,     \"  1  \"  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    remoteRootLogDir )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX ,    suffix )  ;", "FileSystem . closeAll (  )  ;", "Path   rootPath    =    new   Path ( root )  ;", "FileSystem   rootFs    =    rootPath . getFileSystem ( conf )  ;", "FileSystem   mockFs    =     (  ( FilterFileSystem )     ( rootFs )  )  . getRawFileSystem (  )  ;", "Path   remoteRootLogPath    =    new   Path ( remoteRootLogDir )  ;", "Path   userDir    =    new   Path ( remoteRootLogPath ,     \" me \"  )  ;", "FileStatus   userDirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    now ,    userDir )  ;", "when ( mockFs . listStatus ( remoteRootLogPath )  )  . thenReturn ( new   FileStatus [  ]  {    userDirStatus    }  )  ;", "Path   userLogDir    =    new   Path ( userDir ,    suffix )  ;", "Path   app 1 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  1  \"  )  ;", "FileStatus   app 1 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    now ,    app 1 Dir )  ;", "when ( mockFs . listStatus ( userLogDir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 DirStatus    }  )  ;", "Path   app 1 Log 1     =    new   Path ( app 1 Dir ,     \" host 1  \"  )  ;", "FileStatus   app 1 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    now ,    app 1 Log 1  )  ;", "when ( mockFs . listStatus ( app 1 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 Log 1 Status    }  )  ;", "AggregatedLogDeletionService   deletionSvc    =    new   AggregatedLogDeletionService (  )  ;", "deletionSvc . init ( conf )  ;", "deletionSvc . start (  )  ;", "verify ( mockFs ,    timeout (  1  0  0  0  0  )  . atLeast (  4  )  )  . listStatus ( any ( Path . class )  )  ;", "verify ( mockFs ,    never (  )  )  . delete ( app 1 Dir ,    true )  ;", "app 1 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    toDeleteTime ,    app 1 Dir )  ;", "app 1 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toDeleteTime ,    app 1 Log 1  )  ;", "when ( mockFs . listStatus ( userLogDir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 DirStatus    }  )  ;", "when ( mockFs . listStatus ( app 1 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 Log 1 Status    }  )  ;", "verify ( mockFs ,    timeout (  1  0  0  0  0  )  )  . delete ( app 1 Dir ,    true )  ;", "deletionSvc . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testCheckInterval"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "long   toDeleteTime    =    now    -     (  2  0  0  0     *     1  0  0  0  )  ;", "long   toKeepTime    =    now    -     (  1  5  0  0     *     1  0  0  0  )  ;", "String   root    =     \" mockfs :  /  / foo /  \"  ;", "String   remoteRootLogDir    =    root    +     \" tmp / logs \"  ;", "String   suffix    =     \" logs \"  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass (  \" fs . mockfs . impl \"  ,     . MockFileSystem . class ,    FileSystem . class )  ;", "conf . set ( LOG _ AGGREGATION _ ENABLED ,     \" true \"  )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ SECONDS ,     \"  1  8  0  0  \"  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    remoteRootLogDir )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX ,    suffix )  ;", "Path   rootPath    =    new   Path ( root )  ;", "FileSystem   rootFs    =    rootPath . getFileSystem ( conf )  ;", "FileSystem   mockFs    =     (  ( FilterFileSystem )     ( rootFs )  )  . getRawFileSystem (  )  ;", "Path   remoteRootLogPath    =    new   Path ( remoteRootLogDir )  ;", "Path   userDir    =    new   Path ( remoteRootLogPath ,     \" me \"  )  ;", "FileStatus   userDirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    toKeepTime ,    userDir )  ;", "when ( mockFs . listStatus ( remoteRootLogPath )  )  . thenReturn ( new   FileStatus [  ]  {    userDirStatus    }  )  ;", "Path   userLogDir    =    new   Path ( userDir ,    suffix )  ;", "Path   app 1 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  1  \"  )  ;", "FileStatus   app 1 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    toDeleteTime ,    app 1 Dir )  ;", "Path   app 2 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  2  \"  )  ;", "FileStatus   app 2 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    toDeleteTime ,    app 2 Dir )  ;", "Path   app 3 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  3  \"  )  ;", "FileStatus   app 3 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    toDeleteTime ,    app 3 Dir )  ;", "Path   app 4 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  4  \"  )  ;", "FileStatus   app 4 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    toDeleteTime ,    app 4 Dir )  ;", "when ( mockFs . listStatus ( userLogDir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 DirStatus ,    app 2 DirStatus ,    app 3 DirStatus ,    app 4 DirStatus    }  )  ;", "when ( mockFs . listStatus ( app 1 Dir )  )  . thenReturn ( new   FileStatus [  ]  {        }  )  ;", "Path   app 2 Log 1     =    new   Path ( app 2 Dir ,     \" host 1  \"  )  ;", "FileStatus   app 2 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toDeleteTime ,    app 2 Log 1  )  ;", "Path   app 2 Log 2     =    new   Path ( app 2 Dir ,     \" host 2  \"  )  ;", "FileStatus   app 2 Log 2 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toKeepTime ,    app 2 Log 2  )  ;", "when ( mockFs . listStatus ( app 2 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 2 Log 1 Status ,    app 2 Log 2 Status    }  )  ;", "Path   app 3 Log 1     =    new   Path ( app 3 Dir ,     \" host 1  \"  )  ;", "FileStatus   app 3 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toDeleteTime ,    app 3 Log 1  )  ;", "Path   app 3 Log 2     =    new   Path ( app 3 Dir ,     \" host 2  \"  )  ;", "FileStatus   app 3 Log 2 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toDeleteTime ,    app 3 Log 2  )  ;", "when ( mockFs . delete ( app 3 Dir ,    true )  )  . thenThrow ( new   AccessControlException (  \" Injected   Error \\ nStack   Trace    :  (  \"  )  )  ;", "when ( mockFs . listStatus ( app 3 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 3 Log 1 Status ,    app 3 Log 2 Status    }  )  ;", "Path   app 4 Log 1     =    new   Path ( app 4 Dir ,     \" host 1  \"  )  ;", "FileStatus   app 4 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toDeleteTime ,    app 4 Log 1  )  ;", "Path   app 4 Log 2     =    new   Path ( app 4 Dir ,     \" host 2  \"  )  ;", "FileStatus   app 4 Log 2 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    toDeleteTime ,    app 4 Log 2  )  ;", "when ( mockFs . listStatus ( app 4 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 4 Log 1 Status ,    app 4 Log 2 Status    }  )  ;", "AggregatedLogDeletionService . LogDeletionTask   task    =    new   AggregatedLogDeletionService . LogDeletionTask ( conf ,     1  8  0  0  )  ;", "task . run (  )  ;", "verify ( mockFs )  . delete ( app 1 Dir ,    true )  ;", "verify ( mockFs ,    times (  0  )  )  . delete ( app 2 Dir ,    true )  ;", "verify ( mockFs )  . delete ( app 3 Dir ,    true )  ;", "verify ( mockFs )  . delete ( app 4 Dir ,    true )  ;", "}", "METHOD_END"], "methodName": ["testDeletion"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "long   before 2  0  0  0 Secs    =    now    -     (  2  0  0  0     *     1  0  0  0  )  ;", "long   before 5  0 Secs    =    now    -     (  5  0     *     1  0  0  0  )  ;", "String   root    =     \" mockfs :  /  / foo /  \"  ;", "String   remoteRootLogDir    =    root    +     \" tmp / logs \"  ;", "String   suffix    =     \" logs \"  ;", "final   Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass (  \" fs . mockfs . impl \"  ,     . MockFileSystem . class ,    FileSystem . class )  ;", "conf . set ( LOG _ AGGREGATION _ ENABLED ,     \" true \"  )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ SECONDS ,     \"  1  8  0  0  \"  )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ CHECK _ INTERVAL _ SECONDS ,     \"  1  \"  )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR ,    remoteRootLogDir )  ;", "conf . set ( NM _ REMOTE _ APP _ LOG _ DIR _ SUFFIX ,    suffix )  ;", "Path   rootPath    =    new   Path ( root )  ;", "FileSystem   rootFs    =    rootPath . getFileSystem ( conf )  ;", "FileSystem   mockFs    =     (  ( FilterFileSystem )     ( rootFs )  )  . getRawFileSystem (  )  ;", "Path   remoteRootLogPath    =    new   Path ( remoteRootLogDir )  ;", "Path   userDir    =    new   Path ( remoteRootLogPath ,     \" me \"  )  ;", "FileStatus   userDirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    before 5  0 Secs ,    userDir )  ;", "when ( mockFs . listStatus ( remoteRootLogPath )  )  . thenReturn ( new   FileStatus [  ]  {    userDirStatus    }  )  ;", "Path   userLogDir    =    new   Path ( userDir ,    suffix )  ;", "Path   app 1 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  1  \"  )  ;", "FileStatus   app 1 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    before 2  0  0  0 Secs ,    app 1 Dir )  ;", "Path   app 2 Dir    =    new   Path ( userLogDir ,     \" application _  1  _  2  \"  )  ;", "FileStatus   app 2 DirStatus    =    new   FileStatus (  0  ,    true ,     0  ,     0  ,    before 5  0 Secs ,    app 2 Dir )  ;", "when ( mockFs . listStatus ( userLogDir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 DirStatus ,    app 2 DirStatus    }  )  ;", "Path   app 1 Log 1     =    new   Path ( app 1 Dir ,     \" host 1  \"  )  ;", "FileStatus   app 1 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    before 2  0  0  0 Secs ,    app 1 Log 1  )  ;", "when ( mockFs . listStatus ( app 1 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 1 Log 1 Status    }  )  ;", "Path   app 2 Log 1     =    new   Path ( app 2 Dir ,     \" host 1  \"  )  ;", "FileStatus   app 2 Log 1 Status    =    new   FileStatus (  1  0  ,    false ,     1  ,     1  ,    before 5  0 Secs ,    app 2 Log 1  )  ;", "when ( mockFs . listStatus ( app 2 Dir )  )  . thenReturn ( new   FileStatus [  ]  {    app 2 Log 1 Status    }  )  ;", "AggregatedLogDeletionService   deletionSvc    =    new   AggregatedLogDeletionService (  )     {", "@ Override", "protected   Configuration   createConf (  )     {", "return   conf ;", "}", "}  ;", "deletionSvc . init ( conf )  ;", "deletionSvc . start (  )  ;", "verify ( mockFs ,    timeout (  1  0  0  0  0  )  )  . delete ( app 1 Dir ,    true )  ;", "verify ( mockFs ,    timeout (  3  0  0  0  )  . times (  0  )  )  . delete ( app 2 Dir ,    true )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ SECONDS ,     \"  5  0  \"  )  ;", "conf . set ( LOG _ AGGREGATION _ RETAIN _ CHECK _ INTERVAL _ SECONDS ,     \"  2  \"  )  ;", "Assert . assertTrue (  (  2  0  0  0 L    !  =     ( deletionSvc . getCheckIntervalMsecs (  )  )  )  )  ;", "deletionSvc . refreshLogRetentionSettings (  )  ;", "Assert . assertTrue (  (  2  0  0  0 L    =  =     ( deletionSvc . getCheckIntervalMsecs (  )  )  )  )  ;", "verify ( mockFs ,    timeout (  1  0  0  0  0  )  )  . delete ( app 2 Dir ,    true )  ;", "deletionSvc . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testRefreshLogRetentionSettings"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogDeletionService"}, {"methodBody": ["METHOD_START", "{", "Path   workDirPath    =    new   Path ( TestAggregatedLogFormat . testWorkDir . getAbsolutePath (  )  )  ;", "TestAggregatedLogFormat . LOG . info (  (  (  \" Cleaning   test   directory    [  \"     +    workDirPath )     +     \"  ]  \"  )  )  ;", "TestAggregatedLogFormat . fs . delete ( workDirPath ,    true )  ;", "}", "METHOD_END"], "methodName": ["cleanupTestDir"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "File   dir    =    new   File ( srcFilePath . toString (  )  )  ;", "if    (  !  ( dir . exists (  )  )  )     {", "if    (  !  ( dir . mkdirs (  )  )  )     {", "throw   new   IOExcep (  (  \" Unable   to   create   directory    :     \"     +    dir )  )  ;", "}", "}", "File   outputFile    =    new   File ( new   File ( srcFilePath . toString (  )  )  ,    fileName )  ;", "FileOutputStream   os    =    new   FileOutputStream ( outputFile )  ;", "OutputStreamWriter   osw    =    new   OutputStreamWriter ( os ,     \" UTF 8  \"  )  ;", "return   osw ;", "}", "METHOD_END"], "methodName": ["getOutputStreamWriter"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "Assume . assumeTrue ( NativeIO . isAvailable (  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( HADOOP _ SECURITY _ AUTHENTICATION ,     \" kerberos \"  )  ;", "UserGroupInformation . setConfiguration ( conf )  ;", "File   workDir    =    new   File (  . testWorkDir ,     \" testContainerLogsFileAccess 1  \"  )  ;", "Path   remoteAppLogFile    =    new   Path ( workDir . getAbsolutePath (  )  ,     \" aggregatedLogFile \"  )  ;", "Path   srcFileRoot    =    new   Path ( workDir . getAbsolutePath (  )  ,     \" srcFiles \"  )  ;", "String   data    =     \" Log   File   content   for   container    :     \"  ;", "ApplicationId   applicationId    =    ApplicationId . newInstance (  1  ,     1  )  ;", "ApplicationAttemptId   applicationAttemptId    =    ApplicationAttemptId . newInstance ( applicationId ,     1  )  ;", "ContainerId   testContainerId 1     =    ContainerId . newInstance ( applicationAttemptId ,     1  )  ;", "Path   appDir    =    new   Path ( srcFileRoot ,    testContainerId 1  . getApplicationAttemptId (  )  . getApplicationId (  )  . toString (  )  )  ;", "Path   srcFilePath 1     =    new   Path ( appDir ,    testContainerId 1  . toString (  )  )  ;", "String   stdout    =     \" stdout \"  ;", "String   stderr    =     \" stderr \"  ;", "writeSrcFile ( srcFilePath 1  ,    stdout ,     (  ( data    +     ( testContainerId 1  . toString (  )  )  )     +    stdout )  )  ;", "writeSrcFile ( srcFilePath 1  ,    stderr ,     (  ( data    +     ( testContainerId 1  . toString (  )  )  )     +    stderr )  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getCurrentUser (  )  ;", "AggregatedLogFormat . LogWriter   logWriter    =    new   AggregatedLogFormat . LogWriter ( conf ,    remoteAppLogFile ,    ugi )  ;", "AggregatedLogFormat . LogKey   logKey    =    new   AggregatedLogFormat . LogKey ( testContainerId 1  )  ;", "String   randomUser    =     \" randomUser \"  ;", "AggregatedLogFormat . LogValue   logValue    =    spy ( new   AggregatedLogFormat . LogValue ( Collections . singletonList ( srcFileRoot . toString (  )  )  ,    testContainerId 1  ,    randomUser )  )  ;", "when ( logValue . getUser (  )  )  . thenReturn ( randomUser )  . thenReturn ( ugi . getShortUserName (  )  )  ;", "logWriter . append ( logKey ,    logValue )  ;", "logWriter . close (  )  ;", "BufferedReader   in    =    new   BufferedReader ( new   FileReader ( new   File ( remoteAppLogFile . toUri (  )  . getRawPath (  )  )  )  )  ;", "String   line ;", "StringBuffer   sb    =    new   StringBuffer (  \"  \"  )  ;", "while    (  ( line    =    in . readLine (  )  )     !  =    null )     {", ". LOG . info ( line )  ;", "sb . append ( line )  ;", "}", "line    =    sb . toString (  )  ;", "String   expectedOwner    =    ugi . getShortUserName (  )  ;", "if    ( Path . WINDOWS )     {", "final   String   adminsGroupString    =     \" Administrators \"  ;", "if    ( Arrays . asList ( ugi . getGroupNames (  )  )  . contains ( adminsGroupString )  )     {", "expectedOwner    =    adminsGroupString ;", "}", "}", "String   stdoutFile 1     =    StringUtils . join ( File . separator ,    Arrays . asList ( new   String [  ]  {    workDir . getAbsolutePath (  )  ,     \" srcFiles \"  ,    testContainerId 1  . getApplicationAttemptId (  )  . getApplicationId (  )  . toString (  )  ,    testContainerId 1  . toString (  )  ,    stderr    }  )  )  ;", "String   message 1     =     (  (  (  (  (  \" Owner    '  \"     +    expectedOwner )     +     \"  '    for   path    \"  )     +    stdoutFile 1  )     +     \"    did   not   match   expected   owner    '  \"  )     +    randomUser )     +     \"  '  \"  ;", "String   stdoutFile 2     =    StringUtils . join ( File . separator ,    Arrays . asList ( new   String [  ]  {    workDir . getAbsolutePath (  )  ,     \" srcFiles \"  ,    testContainerId 1  . getApplicationAttemptId (  )  . getApplicationId (  )  . toString (  )  ,    testContainerId 1  . toString (  )  ,    stdout    }  )  )  ;", "String   message 2     =     (  (  (  (  (  \" Owner    '  \"     +    expectedOwner )     +     \"  '    for   path    \"  )     +    stdoutFile 2  )     +     \"    did   not   match   expected   owner    '  \"  )     +     ( ugi . getShortUserName (  )  )  )     +     \"  '  \"  ;", "Assert . assertTrue ( line . contains ( message 1  )  )  ;", "Assert . assertFalse ( line . contains ( message 2  )  )  ;", "Assert . assertFalse ( line . contains (  (  ( data    +     ( testContainerId 1  . toString (  )  )  )     +    stderr )  )  )  ;", "Assert . assertTrue ( line . contains (  (  ( data    +     ( testContainerId 1  . toString (  )  )  )     +    stdout )  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerLogsFileAccess"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "File   workDir    =    new   File (  . testWorkDir ,     \" testReadAcontainerLogs 1  \"  )  ;", "Path   remoteAppLogFile    =    new   Path ( workDir . getAbsolutePath (  )  ,     \" aggregatedLogFile \"  )  ;", "Path   srcFileRoot    =    new   Path ( workDir . getAbsolutePath (  )  ,     \" srcFiles \"  )  ;", "ContainerId   testContainerId    =    TestContainerId . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "Path   t    =    new   Path ( srcFileRoot ,    testContainerId . getApplicationAttemptId (  )  . getApplicationId (  )  . toString (  )  )  ;", "Path   srcFilePath    =    new   Path ( t ,    testContainerId . toString (  )  )  ;", "long   numChars    =     9  5  0  0  0  0  ;", "writeSrcFileAndALog ( srcFilePath ,     \" stdout \"  ,    numChars ,    remoteAppLogFile ,    srcFileRoot ,    testContainerId )  ;", "AggregatedLogFormat . LogReader   logReader    =    new   AggregatedLogFormat . LogReader ( conf ,    remoteAppLogFile )  ;", "AggregatedLogFormat . LogKey   rLogKey    =    new   AggregatedLogFormat . LogKey (  )  ;", "DataInputStream   dis    =    logReader . next ( rLogKey )  ;", "Writer   writer    =    new   StringWriter (  )  ;", "try    {", "AggregatedLogFormat . LogReader . readAcontainerLogs ( dis ,    writer )  ;", "}    catch    ( Exception   e )     {", "if    ( e . toString (  )  . contains (  \" NumberFormatException \"  )  )     {", "Assert . fail (  \" Aggregated   logs   are   corrupted .  \"  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["testForCorruptedAggregatedLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "File   workDir    =    new   File (  . testWorkDir ,     \" testReadAcontainerLogs 1  \"  )  ;", "Path   remoteAppLogFile    =    new   Path ( workDir . getAbsolutePath (  )  ,     \" aggregatedLogFile \"  )  ;", "Path   srcFileRoot    =    new   Path ( workDir . getAbsolutePath (  )  ,     \" srcFiles \"  )  ;", "ContainerId   testContainerId    =    TestContainerId . newContainerId (  1  ,     1  ,     1  ,     1  )  ;", "Path   t    =    new   Path ( srcFileRoot ,    testContainerId . getApplicationAttemptId (  )  . getApplicationId (  )  . toString (  )  )  ;", "Path   srcFilePath    =    new   Path ( t ,    testContainerId . toString (  )  )  ;", "int   numChars    =     8  0  0  0  0  ;", "writeSrcFile ( srcFilePath ,     \" stdout \"  ,    numChars )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getCurrentUser (  )  ;", "AggregatedLogFormat . LogWriter   logWriter    =    new   AggregatedLogFormat . LogWriter ( conf ,    remoteAppLogFile ,    ugi )  ;", "AggregatedLogFormat . LogKey   logKey    =    new   AggregatedLogFormat . LogKey ( testContainerId )  ;", "AggregatedLogFormat . LogValue   logValue    =    new   AggregatedLogFormat . LogValue ( Collections . singletonList ( srcFileRoot . toString (  )  )  ,    testContainerId ,    ugi . getShortUserName (  )  )  ;", "logWriter . append ( logKey ,    logValue )  ;", "logWriter . close (  )  ;", "FileStatus   fsStatus    =     . fs . getFileStatus ( remoteAppLogFile )  ;", "Assert . assertEquals (  \" permissions   on   log   aggregation   file   are   wrong \"  ,    FsPermission . createImmutable (  (  ( short )     (  4  1  6  )  )  )  ,    fsStatus . getPermission (  )  )  ;", "AggregatedLogFormat . LogReader   logReader    =    new   AggregatedLogFormat . LogReader ( conf ,    remoteAppLogFile )  ;", "AggregatedLogFormat . LogKey   rLogKey    =    new   AggregatedLogFormat . LogKey (  )  ;", "DataInputStream   dis    =    logReader . next ( rLogKey )  ;", "Writer   writer    =    new   StringWriter (  )  ;", "AggregatedLogFormat . LogReader . readAcontainerLogs ( dis ,    writer )  ;", "String   s    =    writer . toString (  )  ;", "int   expectedLength    =     (  (  (  \"  \\ n \\ nLogType : stdout \"  . length (  )  )     +     (  (  \"  \\ nLogLength :  \"     +    numChars )  . length (  )  )  )     +     (  \"  \\ nLog   Contents :  \\ n \"  . length (  )  )  )     +    numChars ;", "Assert . assertTrue (  \" LogType   not   matched \"  ,    s . contains (  \" LogType : stdout \"  )  )  ;", "Assert . assertTrue (  \" LogLength   not   matched \"  ,    s . contains (  (  \" LogLength :  \"     +    numChars )  )  )  ;", "Assert . assertTrue (  \" Log   Contents   not   matched \"  ,    s . contains (  \" Log   Contents \"  )  )  ;", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "for    ( int   i    =     0  ;    i    <    numChars ;    i +  +  )     {", "sb . append (  . filler )  ;", "}", "String   expectedContent    =    sb . toString (  )  ;", "Assert . assertTrue (  \" Log   content   incorrect \"  ,    s . contains ( expectedContent )  )  ;", "Assert . assertEquals ( expectedLength ,    s . length (  )  )  ;", "}", "METHOD_END"], "methodName": ["testReadAcontainerLogs1"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "OutputStreamWriter   osw    =    getOutputStreamWriter ( srcFilePath ,    fileName )  ;", "osw . write ( data )  ;", "osw . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeSrcFile"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "OutputStreamWriter   osw    =    getOutputStreamWriter ( srcFilePath ,    fileName )  ;", "int   ch    =     . filler ;", "for    ( int   i    =     0  ;    i    <    length ;    i +  +  )     {", "osw . write ( ch )  ;", "}", "osw . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeSrcFile"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "File   dir    =    new   File ( srcFilePath . toString (  )  )  ;", "if    (  !  ( dir . exists (  )  )  )     {", "if    (  !  ( dir . mkdirs (  )  )  )     {", "throw   new   IOException (  (  \" Unable   to   create   directory    :     \"     +    dir )  )  ;", "}", "}", "File   outputFile    =    new   File ( new   File ( srcFilePath . toString (  )  )  ,    fileName )  ;", "FileOutputStream   os    =    new   FileOutputStream ( outputFile )  ;", "final   OutputStreamWriter   osw    =    new   OutputStreamWriter ( os ,     \" UTF 8  \"  )  ;", "final   int   ch    =     . filler ;", "UserGroupInformation   ugi    =    UserGroupInformation . getCurrentUser (  )  ;", "AggregatedLogFormat . LogWriter   logWriter    =    new   AggregatedLogFormat . LogWriter (  . conf ,    remoteAppLogFile ,    ugi )  ;", "AggregatedLogFormat . LogKey   logKey    =    new   AggregatedLogFormat . LogKey ( testContainerId )  ;", "AggregatedLogFormat . LogValue   logValue    =    spy ( new   AggregatedLogFormat . LogValue ( Collections . singletonList ( srcFileRoot . toString (  )  )  ,    testContainerId ,    ugi . getShortUserName (  )  )  )  ;", "final   CountDownLatch   latch    =    new   CountDownLatch (  1  )  ;", "Thread   t    =    new   Thread (  )     {", "public   void   run (  )     {", "try    {", "for    ( int   i    =     0  ;    i    <     ( length    /     3  )  ;    i +  +  )     {", "osw . write ( ch )  ;", "}", "latch . countDown (  )  ;", "for    ( int   i    =     0  ;    i    <     (  (  2     *    length )     /     3  )  ;    i +  +  )     {", "osw . write ( ch )  ;", "}", "osw . close (  )  ;", "}    catch    ( IOException   e )     {", "e . printStackTrace (  )  ;", "}", "}", "}  ;", "t . start (  )  ;", "latch . await (  )  ;", "logWriter . append ( logKey ,    logValue )  ;", "logWriter . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeSrcFileAndALog"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogFormat"}, {"methodBody": ["METHOD_START", "{", "HttpServletRequest   request    =    mock ( HttpServletRequest . class )  ;", "when ( request . getRemoteUser (  )  )  . thenReturn ( user )  ;", "ForTest   aggregatedBlock    =    new   ForTest ( configuration )  ;", "aggregatedBlock . setRequest ( request )  ;", "aggregatedBlock . moreParams (  )  . put ( YarnWebParams . CONTAINER _ ID ,    containerId )  ;", "aggregatedBlock . moreParams (  )  . put ( YarnWebParams . NM _ NODENAME ,     \" localhost :  1  2  3  4  \"  )  ;", "aggregatedBlock . moreParams (  )  . put ( YarnWebParams . APP _ OWNER ,    user )  ;", "aggregatedBlock . moreParams (  )  . put (  \" start \"  ,     \"  \"  )  ;", "aggregatedBlock . moreParams (  )  . put (  \" end \"  ,     \"  \"  )  ;", "aggregatedBlock . moreParams (  )  . put ( YarnWebParams . ENTITY _ STRING ,     \" entity \"  )  ;", "return   aggregatedBlock ;", "}", "METHOD_END"], "methodName": ["getAggregatedLogsBlockForTest"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "Configurconfigur =    new   Configur )  ;", "configursetBoolean ( LOG _ AGGREGATION _ ENABLED ,    true )  ;", "configurset ( NM _ REMOTE _ APP _ LOG _ DIR ,     \" target / logs \"  )  ;", "configursetBoolean ( YARN _ ACL _ ENABLE ,    true )  ;", "configurset ( YARN _ ADMIN _ ACL ,     \" admin \"  )  ;", "return   configur", "}", "METHOD_END"], "methodName": ["getConfiguration"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( new   File (  \" target / logs \"  )  )  ;", "Configuration   configuration    =    getConfiguration (  )  ;", "writeLogs (  \" target / logs / logs / application _  0  _  0  0  0  1  / container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "writeLog ( configuration ,     \" owner \"  )  ;", "AggregatedLogsBlockForTest   aggregatedBlock    =    geForTest ( configuration ,     \" owner \"  ,     \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "PrintWriter   printWriter    =    new   PrintWriter ( data )  ;", "HtmlBlock   html    =    new   HtmlBlockForTest (  )  ;", "HtmlBlock . Block   block    =    new   BlockForTest ( html ,    printWriter ,     1  0  ,    false )  ;", "aggregatedBlock . render ( block )  ;", "block . getWriter (  )  . flush (  )  ;", "String   out    =    data . toString (  )  ;", "assertTrue ( out . contains (  \" User    [ owner ]    is   not   authorized   to   view   the   logs   for   entity \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testAccessDenied"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( new   File (  \" target / logs \"  )  )  ;", "Configuration   configuration    =    getConfiguration (  )  ;", "writeLogs (  \" target / logs / logs / application _  0  _  0  0  0  1  / container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "writeLog ( configuration ,     \" admin \"  )  ;", "AggregatedLogsBlockForTest   aggregatedBlock    =    geForTest ( configuration ,     \" admin \"  ,     \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "PrintWriter   printWriter    =    new   PrintWriter ( data )  ;", "HtmlBlock   html    =    new   HtmlBlockForTest (  )  ;", "HtmlBlock . Block   block    =    new   BlockForTest ( html ,    printWriter ,     1  0  ,    false )  ;", "aggregatedBlock . render ( block )  ;", "block . getWriter (  )  . flush (  )  ;", "String   out    =    data . toString (  )  ;", "assertTrue ( out . contains (  \" test   log 1  \"  )  )  ;", "assertTrue ( out . contains (  \" test   log 2  \"  )  )  ;", "assertTrue ( out . contains (  \" test   log 3  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testAggregatedLogsBlock"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( new   File (  \" target / logs \"  )  )  ;", "Configuration   configuration    =    getConfiguration (  )  ;", "writeLogs (  \" target / logs / logs / application _  0  _  0  0  0  1  / container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "writeLog ( configuration ,     \" owner \"  )  ;", "AggregatedLogsBlockForTest   aggregatedBlock    =    geForTest ( configuration ,     \" admin \"  ,     \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "PrintWriter   printWriter    =    new   PrintWriter ( data )  ;", "HtmlBlock   html    =    new   HtmlBlockForTest (  )  ;", "HtmlBlock . Block   block    =    new   BlockForTest ( html ,    printWriter ,     1  0  ,    false )  ;", "aggregatedBlock . render ( block )  ;", "block . getWriter (  )  . flush (  )  ;", "String   out    =    data . toString (  )  ;", "assertTrue ( out . contains (  \" Logs   not   available   for   entity .    Aggregation   may   not   be   complete ,    Check   back   later   or   try   the   nodemanager   at   localhost :  1  2  3  4  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testBadLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "FileUtil . fullyDelete ( new   File (  \" target / logs \"  )  )  ;", "Configuration   configuration    =    getConfiguration (  )  ;", "File   f    =    new   File (  \" target / logs / logs / application _  0  _  0  0  0  1  / container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "if    (  !  ( f . exists (  )  )  )     {", "assertTrue ( f . mkdirs (  )  )  ;", "}", "writeLog ( configuration ,     \" admin \"  )  ;", "AggregatedLogsBlockForTest   aggregatedBlock    =    geForTest ( configuration ,     \" admin \"  ,     \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ;", "ByteArrayOutputStream   data    =    new   ByteArrayOutputStream (  )  ;", "PrintWriter   printWriter    =    new   PrintWriter ( data )  ;", "HtmlBlock   html    =    new   HtmlBlockForTest (  )  ;", "HtmlBlock . Block   block    =    new   BlockForTest ( html ,    printWriter ,     1  0  ,    false )  ;", "aggregatedBlock . render ( block )  ;", "block . getWriter (  )  . flush (  )  ;", "String   out    =    data . toString (  )  ;", "assertTrue ( out . contains (  \" No   logs   available   for   container   container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNoLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "File   f    =    new   File ( fileName )  ;", "Wrir   wrir    =    new   FileWrir ( f )  ;", "wrir . wri ( xt )  ;", "wrir . flush (  )  ;", "wrir . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeLog"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationIdPBImpl . newInstance (  0  ,     1  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptIdPBImpl . newInstance ( appId ,     1  )  ;", "ContainerId   containerId    =    ContainerIdPBImpl . newInstance ( appAttemptId ,     1  )  ;", "String   path    =     (  \" target / logs /  \"     +    user )     +     \"  / logs / application _  0  _  0  0  0  1  / localhost _  1  2  3  4  \"  ;", "File   f    =    new   File ( path )  ;", "if    (  !  ( f . getParentFile (  )  . exists (  )  )  )     {", "assertTrue ( f . getParentFile (  )  . mkdirs (  )  )  ;", "}", "List < String >    rootLogDirs    =    Arrays . asList (  \" target / logs / logs \"  )  ;", "UserGroupInformation   ugi    =    UserGroupInformation . getCurrentUser (  )  ;", "Format . LogWriter   writer    =    new   Format . LogWriter ( configuration ,    new   Path ( path )  ,    ugi )  ;", "writer . writeApplicationOwner ( ugi . getUserName (  )  )  ;", "Map < ApplicationAccessType ,    String >    appAcls    =    new   HashMap < ApplicationAccessType ,    String >  (  )  ;", "appAcls . put ( VIEW _ APP ,    ugi . getUserName (  )  )  ;", "writer . writeApplicationACLs ( appAcls )  ;", "writer . append ( new   Format . LogKey (  \" container _  0  _  0  0  0  1  _  0  1  _  0  0  0  0  0  1  \"  )  ,    new   Format . LogValue ( rootLogDirs ,    containerId ,    UserGroupInformation . getCurrentUser (  )  . getShortUserName (  )  )  )  ;", "writer . close (  )  ;", "}", "METHOD_END"], "methodName": ["writeLog"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "File   f    =    new   File (  (  ( dirName    +     ( File . separator )  )     +     \" log 1  \"  )  )  ;", "if    (  !  ( f . getParentFile (  )  . exists (  )  )  )     {", "assertTrue ( f . getParentFile (  )  . mkdirs (  )  )  ;", "}", "writeLog (  (  ( dirName    +     ( File . separator )  )     +     \" log 1  \"  )  ,     \" test   log 1  \"  )  ;", "writeLog (  (  ( dirName    +     ( File . separator )  )     +     \" log 2  \"  )  ,     \" test   log 2  \"  )  ;", "writeLog (  (  ( dirName    +     ( File . separator )  )     +     \" log 3  \"  )  ,     \" test   log 3  \"  )  ;", "}", "METHOD_END"], "methodName": ["writeLogs"], "fileName": "org.apache.hadoop.yarn.logaggregation.TestAggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.security.AMRMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . keyId ;", "}", "METHOD_END"], "methodName": ["getKeyId"], "fileName": "org.apache.hadoop.yarn.security.AMRMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "if    ( service    =  =    null )     {", "return   null ;", "}", ". LOG . debug (  (  \" Looking   for   a   token   with   service    \"     +     ( service . toString (  )  )  )  )  ;", "for    ( Token <  ?    extends   TokenIdentifier >    token    :    tokens )     {", ". LOG . debug (  (  (  (  \" Token   kind   is    \"     +     ( token . getKind (  )  . toString (  )  )  )     +     \"    and   the   token ' s   service   name   is    \"  )     +     ( token . getService (  )  )  )  )  ;", "if    (  ( AMRMTokenIdentifier . KIND _ NAME . equals ( token . getKind (  )  )  )     &  &     ( service . equals ( token . getService (  )  )  )  )     {", "return    (  ( Token < AMRMTokenIdentifier >  )     ( token )  )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["selectToken"], "fileName": "org.apache.hadoop.yarn.security.AMRMTokenSelector"}, {"methodBody": ["METHOD_START", "{", "return   aclsEnabled ;", "}", "METHOD_END"], "methodName": ["areACLsEnabled"], "fileName": "org.apache.hadoop.yarn.security.AdminACLsManager"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( areACLsEnabled (  )  )  )     {", "return   true ;", "}", "return   is ( callerUGI )  ;", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.security.AdminACLsManager"}, {"methodBody": ["METHOD_START", "{", "return   adminAcl ;", "}", "METHOD_END"], "methodName": ["getAdminAcl"], "fileName": "org.apache.hadoop.yarn.security.AdminACLsManager"}, {"methodBody": ["METHOD_START", "{", "return   owner ;", "}", "METHOD_END"], "methodName": ["getOwner"], "fileName": "org.apache.hadoop.yarn.security.AdminACLsManager"}, {"methodBody": ["METHOD_START", "{", "return   adminAcl . isUserAllowed ( callerUGI )  ;", "}", "METHOD_END"], "methodName": ["isAdmin"], "fileName": "org.apache.hadoop.yarn.security.AdminACLsManager"}, {"methodBody": ["METHOD_START", "{", "return   this . appSubmitter ;", "}", "METHOD_END"], "methodName": ["getApplicationSubmitter"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . containerId ;", "}", "METHOD_END"], "methodName": ["getContainerID"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . creationTime ;", "}", "METHOD_END"], "methodName": ["getCreationTime"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . expiryTimeStamp ;", "}", "METHOD_END"], "methodName": ["getExpiryTimeStamp"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . masterKeyId ;", "}", "METHOD_END"], "methodName": ["getMasterKeyId"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . nmHostAddr ;", "}", "METHOD_END"], "methodName": ["getNmHostAddress"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . priority ;", "}", "METHOD_END"], "methodName": ["getPriority"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . rmIdentifier ;", "}", "METHOD_END"], "methodName": ["getRMIdentifer"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . resource ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.security.ContainerTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   appAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.security.NMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   appSubmitter ;", "}", "METHOD_END"], "methodName": ["getApplicationSubmitter"], "fileName": "org.apache.hadoop.yarn.security.NMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   keyId ;", "}", "METHOD_END"], "methodName": ["getKeyId"], "fileName": "org.apache.hadoop.yarn.security.NMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   nodeId ;", "}", "METHOD_END"], "methodName": ["getNodeId"], "fileName": "org.apache.hadoop.yarn.security.NMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . applicationAttemptId ;", "}", "METHOD_END"], "methodName": ["getApplicationAttemptID"], "fileName": "org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "return   this . clientName . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getClientName"], "fileName": "org.apache.hadoop.yarn.security.client.ClientToAMTokenIdentifier"}, {"methodBody": ["METHOD_START", "{", "this . masterKey    =    SecretManager . createSecretKey ( key )  ;", "}", "METHOD_END"], "methodName": ["setMasterKey"], "fileName": "org.apache.hadoop.yarn.security.client.ClientToAMTokenSecretManager"}, {"methodBody": ["METHOD_START", "{", "if    ( service    =  =    null )     {", "return   null ;", "}", ". LOG . debug (  (  \" Looking   for   a   token   with   service    \"     +     ( service . toString (  )  )  )  )  ;", "for    ( Token <  ?    extends   TokenIdentifier >    token    :    tokens )     {", ". LOG . debug (  (  (  (  \" Token   kind   is    \"     +     ( token . getKind (  )  . toString (  )  )  )     +     \"    and   the   token ' s   service   name   is    \"  )     +     ( token . getService (  )  )  )  )  ;", "if    (  ( ClientToAMTokenIdentifier . KIND _ NAME . equals ( token . getKind (  )  )  )     &  &     ( service . equals ( token . getService (  )  )  )  )     {", "return    (  ( Token < ClientToAMTokenIdentifier >  )     ( token )  )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["selectToken"], "fileName": "org.apache.hadoop.yarn.security.client.ClientToAMTokenSelector"}, {"methodBody": ["METHOD_START", "{", "if    (  ( service    =  =    null )     |  |     (  ( token . getService (  )  )     =  =    null )  )     {", "return   false ;", "}", "return   token . getService (  )  . toString (  )  . contains ( service . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["checkService"], "fileName": "org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector"}, {"methodBody": ["METHOD_START", "{", "if    ( service    =  =    null )     {", "return   null ;", "}", ". LOG . debug (  (  \" Looking   for   a   token   with   service    \"     +     ( service . toString (  )  )  )  )  ;", "for    ( Token <  ?    extends   TokenIdentifier >    token    :    tokens )     {", ". LOG . debug (  (  (  (  \" Token   kind   is    \"     +     ( token . getKind (  )  . toString (  )  )  )     +     \"    and   the   token ' s   service   name   is    \"  )     +     ( token . getService (  )  )  )  )  ;", "if    (  ( RMDelegationTokenIdentifier . KIND _ NAME . equals ( token . getKind (  )  )  )     &  &     ( checkService ( service ,    token )  )  )     {", "return    (  ( Token < RMDelegationTokenIdentifier >  )     ( token )  )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["selectToken"], "fileName": "org.apache.hadoop.yarn.security.client.RMDelegationTokenSelector"}, {"methodBody": ["METHOD_START", "{", "return   httpMethod ;", "}", "METHOD_END"], "methodName": ["getHttpMethod"], "fileName": "org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation"}, {"methodBody": ["METHOD_START", "{", "return   requiresKerberosCredentials ;", "}", "METHOD_END"], "methodName": ["requiresKerberosCredentials"], "fileName": "org.apache.hadoop.yarn.security.client.TimelineDelegationTokenOperation"}, {"methodBody": ["METHOD_START", "{", "if    ( service    =  =    null )     {", "return   null ;", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  \" Looking   for   a   token   with   service    \"     +     ( service . toString (  )  )  )  )  ;", "}", "for    ( Token <  ?    extends   TokenIdentifier >    token    :    tokens )     {", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" Token   kind   is    \"     +     ( token . getKind (  )  . toString (  )  )  )     +     \"    and   the   token ' s   service   name   is    \"  )     +     ( token . getService (  )  )  )  )  ;", "}", "if    (  ( TimelineDelegationTokenIdentifier . KIND _ NAME . equals ( token . getKind (  )  )  )     &  &     ( service . equals ( token . getService (  )  )  )  )     {", "return    (  ( Token < TimelineDelegationTokenIdentifier >  )     ( token )  )  ;", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["selectToken"], "fileName": "org.apache.hadoop.yarn.security.client.TimelineDelegationTokenSelector"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshAdminAclsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshAdminAclsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshNodesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshNodesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshQueuesRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshQueuesResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshServiceAclsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshServiceAclsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshSuperUserGroupsConfigurationRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshSuperUserGroupsConfigurationResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshUserToGroupsMappingsRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.RefreshUserToGroupsMappingsResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "maybeInitBuilder (  )  ;", "builder . clearMap (  )  ;", "if    (  ( nodeResourceMap )     =  =    null )     {", "return ;", "}", "Iterable <  ?    extends   MapProto >    values    =    new   Iterable < MapProto >  (  )     {", "@ Override", "public   Iterator < MapProto >    iterator (  )     {", "return   new   Iterator < MapProto >  (  )     {", "Iterator < NodeId >    nodeIterator    =    nodeResourceMap . keySet (  )  . iterator (  )  ;", "@ Override", "public   boolean   hasNext (  )     {", "return   nodeIterator . hasNext (  )  ;", "}", "@ Override", "public   MapProto   next (  )     {", "NodeId   nodeId    =    nodeIterator . next (  )  ;", "return   MapProto . newBuilder (  )  . setNodeId ( convertToProtoFormat ( nodeId )  )  . setResourceOption ( convertToProtoFormat ( nodeResourceMap . get ( nodeId )  )  )  . build (  )  ;", "}", "@ Override", "public   void   remove (  )     {", "throw   new   UnsupportedOperationException (  )  ;", "}", "}  ;", "}", "}  ;", "this . builder . addAllMap ( values )  ;", "}", "METHOD_END"], "methodName": ["addNodeResourceMap"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   NodeIdPBImpl ( proto )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   ResourceOptionPBImpl ( c )  ;", "}", "METHOD_END"], "methodName": ["convertFromProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( NodeIdPBImpl )     ( nodeId )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ResourceOptionPBImpl )     ( c )  )  . getProto (  )  ;", "}", "METHOD_END"], "methodName": ["convertToProtoFormat"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "mergeLocalToProto (  )  ;", "=     ( viaProto )     ?        :    builder . build (  )  ;", "viaProto    =    true ;", "return    ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeResourceMap )     !  =    null )     {", "return ;", "}", "rotoOrBuilder   p    =     ( viaProto )     ?    proto    :    builder ;", "List < NodeResourceMapProto >    list    =    p . getNodeResourceMapList (  )  ;", "this . nodeResourceMap    =    new   HashMap < NodeId ,    ResourceOption >  ( list . size (  )  )  ;", "for    ( NodeResourceMapProto   nodeResourceProto    :    list )     {", "this . nodeResourceMap . put ( convertFromProtoFormat ( nodeResourceProto . getNodeId (  )  )  ,    convertFromProtoFormat ( nodeResourceProto . getResourceOption (  )  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initNodeResourceMap"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( viaProto )     |  |     (  ( builder )     =  =    null )  )     {", "builder    =    roto . newBuilder ( proto )  ;", "}", "viaProto    =    false ;", "}", "METHOD_END"], "methodName": ["maybeInitBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . nodeResourceMap )     !  =    null )     {", "addMap (  )  ;", "}", "}", "METHOD_END"], "methodName": ["mergeLocalToBuilder"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( viaProto )", "maybeInitBuilder (  )  ;", "mergeLocalToBuilder (  )  ;", "=    builder . build (  )  ;", "viaProto    =    true ;", "}", "METHOD_END"], "methodName": ["mergeLocalToProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceRequestPBImpl"}, {"methodBody": ["METHOD_START", "{", "proto    =     ( viaProto )     ?    proto    :    builder . build (  )  ;", "viaProto    =    true ;", "return   proto ;", "}", "METHOD_END"], "methodName": ["getProto"], "fileName": "org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb.UpdateNodeResourceResponsePBImpl"}, {"methodBody": ["METHOD_START", "{", "Map < ApplicationAccessType ,    AccessControlList >    finalMap    =    new   HashMap < ApplicationAccessType ,    AccessControlList >  ( acls . size (  )  )  ;", "for    ( Map . Entry < ApplicationAccessType ,    String >    acl    :    acls . entrySet (  )  )     {", "finalMap . put ( acl . getKey (  )  ,    new   AccessControlList ( acl . getValue (  )  )  )  ;", "}", "this . aS . put ( appId ,    finalMap )  ;", "}", "METHOD_END"], "methodName": ["addApplication"], "fileName": "org.apache.hadoop.yarn.server.security.ApplicationACLsManager"}, {"methodBody": ["METHOD_START", "{", "return   adminAclsManager . areACLsEnabled (  )  ;", "}", "METHOD_END"], "methodName": ["areACLsEnabled"], "fileName": "org.apache.hadoop.yarn.server.security.ApplicationACLsManager"}, {"methodBody": ["METHOD_START", "{", "if    ( ApplicationACLsManager . LOG . isDebugEnabled (  )  )     {", "ApplicationACLsManager . LOG . debug (  (  (  (  (  (  (  (  \" Verifying   access - type    \"     +    applicationAccessType )     +     \"    for    \"  )     +    callerUGI )     +     \"    on   application    \"  )     +    applicationId )     +     \"    owned   by    \"  )     +    applicationOwner )  )  ;", "}", "String   user    =    callerUGI . getShortUserName (  )  ;", "if    (  !  ( areACLsEnabled (  )  )  )     {", "return   true ;", "}", "AccessControlList   applicationACL    =    this . applicationACLS . get ( applicationId )  . get ( applicationAccessType )  ;", "if    ( applicationACL    =  =    null )     {", "if    ( ApplicationACLsManager . LOG . isDebugEnabled (  )  )     {", "ApplicationACLsManager . LOG . debug (  (  (  (  (  (  (  (  (  \" ACL   not   found   for   access - type    \"     +    applicationAccessType )     +     \"    for   application    \"  )     +    applicationId )     +     \"    owned   by    \"  )     +    applicationOwner )     +     \"  .    Using   default    [  \"  )     +     ( YarnConfiguration . DEFAULT _ YARN _ APP _ ACL )  )     +     \"  ]  \"  )  )  ;", "}", "applicationACL    =    new   AccessControlList ( YarnConfiguration . DEFAULT _ YARN _ APP _ ACL )  ;", "}", "if    (  (  ( this . adminAclsManager . isAdmin ( callerUGI )  )     |  |     ( user . equals ( applicationOwner )  )  )     |  |     ( applicationACL . isUserAllowed ( callerUGI )  )  )     {", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["checkAccess"], "fileName": "org.apache.hadoop.yarn.server.security.ApplicationACLsManager"}, {"methodBody": ["METHOD_START", "{", "this . applicationACLS . remove ( appId )  ;", "}", "METHOD_END"], "methodName": ["removeApplication"], "fileName": "org.apache.hadoop.yarn.server.security.ApplicationACLsManager"}, {"methodBody": ["METHOD_START", "{", "subgraphs . add ( graph )  ;", "graph . parent    =    this ;", "}", "METHOD_END"], "methodName": ["addSubGraph"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "List < Graph . Edge >    ret    =    new   ArrayList < Graph . Edge >  (  )  ;", "for    ( Graph . Edge   edge    :    edges )     {", "boolean   found    =    false ;", "for    ( int   i    =     0  ;    i    <     ( ret . size (  )  )  ;    i +  +  )     {", "Graph . Edge   current    =    ret . get ( i )  ;", "if    ( edge . sameAs ( current )  )     {", "ret . set ( i ,    current . combine ( edge )  )  ;", "found    =    true ;", "break ;", "}", "}", "if    (  ! found )     {", "ret . add ( edge )  ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["combineEdges"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "return   generateGraphViz (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["generateGraphViz"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "if    (  ( this . parent )     =  =    null )     {", "sb . append (  (  (  \" digraph    \"     +     ( name )  )     +     \"     {  \\ n \"  )  )  ;", "sb . append ( String . format (  \" graph    [    label =  % s ,    fontsize =  2  4  ,    fontname = Helvetica ]  ;  \\ n \"  ,     . wrapSafeString ( name )  )  )  ;", "sb . append (  \" node    [ fontsize =  1  2  ,    fontname = Helvetica ]  ;  \\ n \"  )  ;", "sb . append (  \" edge    [ fontsize =  9  ,    fontcolor = blue ,    fontname = Arial ]  ;  \\ n \"  )  ;", "} else    {", "sb . append (  (  (  (  (  \" subgraph   cluster _  \"     +     ( name )  )     +     \"     {  \\ nlabel =  \\  \"  \"  )     +     ( name )  )     +     \"  \\  \"  \\ n \"  )  )  ;", "}", "for    (    g    :    subgraphs )     {", "String   ginfo    =    g . generateViz (  ( indent    +     \"        \"  )  )  ;", "sb . append ( ginfo )  ;", "sb . append (  \"  \\ n \"  )  ;", "}", "for    (  . Node   n    :    nodes )     {", "sb . append ( String . format (  \"  % s % s    [    label    =     % s    ]  ;  \\ n \"  ,    indent ,     . wrapSafeString ( n . getUniqueId (  )  )  ,    n . id )  )  ;", "List <  . Edge >    combinedOuts    =     . combineEdges ( n . outs )  ;", "for    (  . Edge   e    :    combinedOuts )     {", "sb . append ( String . format (  \"  % s % s    -  >     % s    [    label    =     % s    ]  ;  \\ n \"  ,    indent ,     . wrapSafeString ( e . from . getUniqueId (  )  )  ,     . wrapSafeString ( e . to . getUniqueId (  )  )  ,     . wrapSafeString ( e . label )  )  )  ;", "}", "}", "sb . append (  \"  }  \\ n \"  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["generateGraphViz"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "return   name ;", "}", "METHOD_END"], "methodName": ["getName"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "for    ( Graph . Node   node    :    nodes )     {", "if    ( node . id . equals ( id )  )     {", "return   node ;", "}", "}", "return   newNode ( id )  ;", "}", "METHOD_END"], "methodName": ["getNode"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "return   parent ;", "}", "METHOD_END"], "methodName": ["getParent"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "Graph . Node   ret    =    new   Graph . Node ( id )  ;", "nodes . add ( ret )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["newNode"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "Graph   ret    =    new   Graph ( name ,    this )  ;", "subgraphs . add ( ret )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["newSubGraph"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "FileWriter   fout    =    new   FileWriter ( filepath )  ;", "fout . write ( generateViz (  )  )  ;", "fout . close (  )  ;", "}", "METHOD_END"], "methodName": ["save"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "if    (  ( label . indexOf (  '  ,  '  )  )     >  =     0  )     {", "if    (  ( label . length (  )  )     >     1  4  )     {", "label    =    label . repleAll (  \"  ,  \"  ,     \"  ,  \\ n \"  )  ;", "}", "}", "label    =     (  \"  \\  \"  \"     +     ( StringEscapeUtils . escapeJava ( label )  )  )     +     \"  \\  \"  \"  ;", "return   label ;", "}", "METHOD_END"], "methodName": ["wrapSafeString"], "fileName": "org.apache.hadoop.yarn.state.Graph"}, {"methodBody": ["METHOD_START", "{", "return   currentState ;", "}", "METHOD_END"], "methodName": ["getCurrentState"], "fileName": "org.apache.hadoop.yarn.state.InvalidStateTransitonException"}, {"methodBody": ["METHOD_START", "{", "return   event ;", "}", "METHOD_END"], "methodName": ["getEvent"], "fileName": "org.apache.hadoop.yarn.state.InvalidStateTransitonException"}, {"methodBody": ["METHOD_START", "{", "return   addTransition ( preState ,    postState ,    eventType ,    null )  ;", "}", "METHOD_END"], "methodName": ["addTransition"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "return   new   StateMachineFactory < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  ( this ,    new   StateMachineFactory . ApplicableSingleOrMultipleTransition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  ( preState ,    eventType ,    new   SingleInternalArc ( postState ,    hook )  )  )  ;", "}", "METHOD_END"], "methodName": ["addTransition"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "return   addTransition ( preState ,    postState ,    eventTypes ,    null )  ;", "}", "METHOD_END"], "methodName": ["addTransition"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "StateMachineFactory < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >    factory    =    null ;", "for    ( EVENTTYPE   event    :    eventTypes )     {", "if    ( factory    =  =    null )     {", "factory    =    addTransition ( preState ,    postState ,    event ,    hook )  ;", "} else    {", "factory    =    factory . addTransition ( preState ,    postState ,    event ,    hook )  ;", "}", "}", "return   factory ;", "}", "METHOD_END"], "methodName": ["addTransition"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "return   new   StateMachineFactory < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  ( this ,    new   StateMachineFactory . ApplicableSingleOrMultipleTransition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  ( preState ,    eventType ,    new   MultipleInternalArc ( postStates ,    hook )  )  )  ;", "}", "METHOD_END"], "methodName": ["addTransition"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "Map < EVENTTYPE ,    StateMachineFactory . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >    transitionMap    =    stateMachineTable . get ( oldState )  ;", "if    ( transitionMap    !  =    null )     {", "StateMachineFactory . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >    transition    =    transitionMap . get ( eventType )  ;", "if    ( transition    !  =    null )     {", "return   transition . doTransition ( operand ,    oldState ,    event ,    eventType )  ;", "}", "}", "throw   new   InvalidStateTransitonException ( oldState ,    eventType )  ;", "}", "METHOD_END"], "methodName": ["doTransition"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "maybeMakeStateMachineTable (  )  ;", "Graph   g    =    new   Graph ( name )  ;", "for    ( STATE   startState    :    stateMachineTable . keySet (  )  )     {", "Map < EVENTTYPE ,     . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >    transitions    =    stateMachineTable . get ( startState )  ;", "for    ( Map . Entry < EVENTTYPE ,     . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >    entry    :    transitions . entrySet (  )  )     {", ". Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >    transition    =    entry . getValue (  )  ;", "if    ( transition   instanceof    . SingleInternalArc )     {", ". SingleInternalArc   sa    =     (  (  . SingleInternalArc )     ( transition )  )  ;", "Graph . Node   fromNode    =    g . getNode ( startState . toString (  )  )  ;", "Graph . Node   toNode    =    g . getNode ( sa . postState . toString (  )  )  ;", "fromNode . addEdge ( toNode ,    entry . getKey (  )  . toString (  )  )  ;", "} else", "if    ( transition   instanceof    . MultipleInternalArc )     {", ". MultipleInternalArc   ma    =     (  (  . MultipleInternalArc )     ( transition )  )  ;", "Iterator   iter    =    ma . validPostStates . iterator (  )  ;", "while    ( iter . hasNext (  )  )     {", "Graph . Node   fromNode    =    g . getNode ( startState . toString (  )  )  ;", "Graph . Node   toNode    =    g . getNode ( iter . next (  )  . toString (  )  )  ;", "fromNode . addEdge ( toNode ,    entry . getKey (  )  . toString (  )  )  ;", "}", "}", "}", "}", "return   g ;", "}", "METHOD_END"], "methodName": ["generateStateGraph"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "return   new   StateMachineFactory < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  ( this ,    true )  ;", "}", "METHOD_END"], "methodName": ["installTopology"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "return   new   InternalStateMachine ( operand ,    defaultInitialState )  ;", "}", "METHOD_END"], "methodName": ["make"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "return   new   InternalStateMachine ( operand ,    initialState )  ;", "}", "METHOD_END"], "methodName": ["make"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "Stack < StateMachineFactory . ApplicableTransition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >    stack    =    new   Stack < StateMachineFactory . ApplicableTransition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >  (  )  ;", "Map < STATE ,    Map < EVENTTYPE ,    StateMachineFactory . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >  >    prototype    =    new   HashMap < STATE ,    Map < EVENTTYPE ,    StateMachineFactory . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >  >  (  )  ;", "prototype . put ( defaultInitialState ,    null )  ;", "stateMachineTable    =    new   EnumMap < STATE ,    Map < EVENTTYPE ,    StateMachineFactory . Transition < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  >  >  ( prototype )  ;", "for    ( StateMachineFactory < OPERAND ,    STATE ,    EVENTTYPE ,    EVENT >  . TransitionsListNode   cursor    =    transitionsListNode ;    cursor    !  =    null ;    cursor    =    cursor . next )     {", "stack . push ( cursor . transition )  ;", "}", "while    (  !  ( stack . isEmpty (  )  )  )     {", "stack . pop (  )  . apply ( this )  ;", "}", "}", "METHOD_END"], "methodName": ["makeStateMachineTable"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "if    (  ( stateMachineTable )     =  =    null )     {", "makeTable (  )  ;", "}", "}", "METHOD_END"], "methodName": ["maybeMakeStateMachineTable"], "fileName": "org.apache.hadoop.yarn.state.StateMachineFactory"}, {"methodBody": ["METHOD_START", "{", "Graph   ret    =    null ;", "if    (  ( classes . size (  )  )     !  =     1  )     {", "ret    =    new   Graph ( graphName )  ;", "}", "for    ( String   className    :    classes )     {", "Class   clz    =    Class . forName ( className )  ;", "Field   factoryField    =    clz . getDeclaredField (  \" stateMachineFactory \"  )  ;", "factoryField . setAccessible ( true )  ;", "Factory   factory    =     (  ( Factory )     ( factoryField . get ( null )  )  )  ;", "if    (  ( classes . size (  )  )     =  =     1  )     {", "return   factory . generateStateGraph ( graphName )  ;", "}", "String   gname    =    clz . getSimpleName (  )  ;", "if    ( gname . endsWith (  \" Impl \"  )  )     {", "gname    =    gname . substring (  0  ,     (  ( gname . length (  )  )     -     4  )  )  ;", "}", "ret . addSubGraph ( factory . generateStateGraph ( gname )  )  ;", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["getGraphFromClasses"], "fileName": "org.apache.hadoop.yarn.state.VisualizeStateMachine"}, {"methodBody": ["METHOD_START", "{", "if    (  ( args . length )     <     3  )     {", "System . err . printf (  \" Usage :     % s    < GraphName >     < class [  , class [  ,  .  .  .  ]  ]  >     < OutputFile >  \\ n \"  ,     . class . getName (  )  )  ;", "System . exit (  1  )  ;", "}", "String [  ]    classes    =    args [  1  ]  . split (  \"  ,  \"  )  ;", "ArrayList < String >    validClasses    =    new   ArrayList < String >  (  )  ;", "for    ( String   c    :    classes )     {", "String   vc    =    c . trim (  )  ;", "if    (  ( vc . length (  )  )     >     0  )     {", "validClasses . add ( vc )  ;", "}", "}", "Graph   g    =     . getGraphFromClasses ( args [  0  ]  ,    validClasses )  ;", "g . save ( args [  2  ]  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.state.VisualizeStateMachine"}, {"methodBody": ["METHOD_START", "{", "if    ( running . containsKey ( ob )  )     {", "running . put ( ob ,    clock . getTime (  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["receivedPing"], "fileName": "org.apache.hadoop.yarn.util.AbstractLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "running . put ( ob ,    clock . getTime (  )  )  ;", "}", "METHOD_END"], "methodName": ["register"], "fileName": "org.apache.hadoop.yarn.util.AbstractLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "this . expireInterval    =    expireInterval ;", "}", "METHOD_END"], "methodName": ["setExpireInterval"], "fileName": "org.apache.hadoop.yarn.util.AbstractLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "this . monitorInterval    =    monitorInterval ;", "}", "METHOD_END"], "methodName": ["setMonitorInterval"], "fileName": "org.apache.hadoop.yarn.util.AbstractLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "running . remove ( ob )  ;", "}", "METHOD_END"], "methodName": ["unregister"], "fileName": "org.apache.hadoop.yarn.util.AbstractLivelinessMonitor"}, {"methodBody": ["METHOD_START", "{", "Apps . addToEnvironment ( environment ,    variable ,    value ,    File . pathSeparator )  ;", "}", "METHOD_END"], "methodName": ["addToEnvironment"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "String   val    =    environment . get ( variable )  ;", "if    ( val    =  =    null )     {", "val    =    value ;", "} else    {", "val    =     ( val    +    classPathSeparator )     +    value ;", "}", "environment . put ( StringInterner . weakIntern ( variable )  ,    StringInterner . weakIntern ( val )  )  ;", "}", "METHOD_END"], "methodName": ["addToEnvironment"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "return    (  ( ApplicationConstants . PARAMETER _ EXPANSION _ LEFT )     +    var )     +     ( ApplicationConstants . PARAMETER _ EXPANSION _ RIGHT )  ;", "}", "METHOD_END"], "methodName": ["crossPlatformify"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "Apps . setEnvFromInputString ( env ,    envString ,    File . pathSeparator )  ;", "}", "METHOD_END"], "methodName": ["setEnvFromInputString"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "if    (  ( envString    !  =    null )     &  &     (  ( envString . length (  )  )     >     0  )  )     {", "String [  ]    childEnvs    =    envString . split (  \"  ,  \"  )  ;", "Pattern   p    =    Pattern . compile ( Shell . getEnvironmentVariableRegex (  )  )  ;", "for    ( String   cEnv    :    childEnvs )     {", "String [  ]    parts    =    cEnv . split (  \"  =  \"  )  ;", "Matcher   m    =    p . matcher ( parts [  1  ]  )  ;", "StringBuffer   sb    =    new   StringBuffer (  )  ;", "while    ( m . find (  )  )     {", "String   var    =    m . group (  1  )  ;", "String   replace    =    env . get ( var )  ;", "if    ( replace    =  =    null )", "replace    =    System . getenv ( var )  ;", "if    ( replace    =  =    null )", "replace    =     \"  \"  ;", "m . appendReplacement ( sb ,    Matcher . quoteReplacement ( replace )  )  ;", "}", "m . appendTail ( sb )  ;", ". addToEnvironment ( env ,    parts [  0  ]  ,    sb . toString (  )  ,    classPathSeparator )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setEnvFromInputString"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( it . hasNext (  )  )  )     {", ". throwParseException ( StringHelper . sjoin ( prefix ,     . ID )  ,    s )  ;", "}", "}", "METHOD_END"], "methodName": ["shouldHaveNext"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "throw   new   YarnRuntimeException ( StringHelper . join (  \" Error   parsing    \"  ,    name ,     \"  :     \"  ,    s )  )  ;", "}", "METHOD_END"], "methodName": ["throwParseException"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "Iterator < String >    it    =    StringHelper .  _ split ( aid )  . iterator (  )  ;", "return    . toAppID (  . APP ,    aid ,    it )  ;", "}", "METHOD_END"], "methodName": ["toAppID"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "if    (  (  !  ( it . hasNext (  )  )  )     |  |     (  !  ( it . next (  )  . equals ( prefix )  )  )  )     {", ". throwParseException ( StringHelper . sjoin ( prefix ,     . ID )  ,    s )  ;", "}", ". shouldHaveNext ( prefix ,    s ,    it )  ;", "ApplicationId   appId    =    ApplicationId . newInstance ( Long . parseLong ( it . next (  )  )  ,    Integer . parseInt ( it . next (  )  )  )  ;", "return   appId ;", "}", "METHOD_END"], "methodName": ["toAppID"], "fileName": "org.apache.hadoop.yarn.util.Apps"}, {"methodBody": ["METHOD_START", "{", "return    ( AuxiliaryServiceHelper . NM _ AUX _ SERVICE )     +    serviceName ;", "}", "METHOD_END"], "methodName": ["getPrefixServiceName"], "fileName": "org.apache.hadoop.yarn.util.AuxiliaryServiceHelper"}, {"methodBody": ["METHOD_START", "{", "String   meta    =    env . get ( AuxiliaryServiceHelper . getPrefixServiceName ( serviceName )  )  ;", "if    ( null    =  =    meta )     {", "return   null ;", "}", "byte [  ]    metaData    =    Base 6  4  . decodeBase 6  4  ( meta )  ;", "return   ByteBuffer . wrap ( metaData )  ;", "}", "METHOD_END"], "methodName": ["getServiceDataFromEnv"], "fileName": "org.apache.hadoop.yarn.util.AuxiliaryServiceHelper"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    byteData    =    metaData . array (  )  ;", "env . put (  . getPrefixServiceName ( serviceName )  ,    Base 6  4  . encodeBase 6  4 String ( byteData )  )  ;", "}", "METHOD_END"], "methodName": ["setServiceDataIntoEnv"], "fileName": "org.apache.hadoop.yarn.util.AuxiliaryServiceHelper"}, {"methodBody": ["METHOD_START", "{", "Token < T >    token    =    new   Token < T >  ( protoToken . getIdentifier (  )  . array (  )  ,    protoToken . getPassword (  )  . array (  )  ,    new   Text ( protoToken . getKind (  )  )  ,    new   Text ( protoToken . getService (  )  )  )  ;", "if    ( serviceAddr    !  =    null )     {", "SecurityUtil . setTokenService ( token ,    serviceAddr )  ;", "}", "return   token ;", "}", "METHOD_END"], "methodName": ["convertFromYarn"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Token < T >    token    =    new   Token < T >  ( protoToken . getIdentifier (  )  . array (  )  ,    protoToken . getPassword (  )  . array (  )  ,    new   Text ( protoToken . getKind (  )  )  ,    new   Text ( protoToken . getService (  )  )  )  ;", "if    ( service    !  =    null )     {", "token . setService ( service )  ;", "}", "return   token ;", "}", "METHOD_END"], "methodName": ["convertFromYarn"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Map < String ,    String >    stringMap    =    new   HashMap < String ,    String >  (  )  ;", "for    ( Map . Entry < CharSequence ,    CharSequence >    entry    :    env . entrySet (  )  )     {", "stringMap . put ( entry . getKey (  )  . toString (  )  ,    entry . getValue (  )  . toString (  )  )  ;", "}", "return   stringMap ;", "}", "METHOD_END"], "methodName": ["convertToString"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "String   scheme    =     (  ( url . getScheme (  )  )     =  =    null )     ?     \"  \"     :    url . getScheme (  )  ;", "String   authority    =     \"  \"  ;", "if    (  ( url . getHost (  )  )     !  =    null )     {", "authority    =    url . getHost (  )  ;", "if    (  ( url . getUserInfo (  )  )     !  =    null )     {", "authority    =     (  ( url . getUserInfo (  )  )     +     \"  @  \"  )     +    authority ;", "}", "if    (  ( url . getPort (  )  )     >     0  )     {", "authority    +  =     \"  :  \"     +     ( url . getPort (  )  )  ;", "}", "}", "return   new   Path ( new   URI ( scheme ,    authority ,    url . getFile (  )  ,    null ,    null )  . normalize (  )  )  ;", "}", "METHOD_END"], "methodName": ["getPathFromYarnURL"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "return   ConverterUtils . getYarnUrlFromURI ( path . toUri (  )  )  ;", "}", "METHOD_END"], "methodName": ["getYarnUrlFromPath"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "URL   url    =    RecordFactoryProvider . getRecordFactory ( null )  . newRecordInstance ( URL . class )  ;", "if    (  ( uri . getHost (  )  )     !  =    null )     {", "url . setHost ( uri . getHost (  )  )  ;", "}", "if    (  ( uri . getUserInfo (  )  )     !  =    null )     {", "url . setUserInfo ( uri . getUserInfo (  )  )  ;", "}", "url . setPort ( uri . getPort (  )  )  ;", "url . setSme ( uri . getSme (  )  )  ;", "url . setFile ( uri . getPath (  )  )  ;", "return   url ;", "}", "METHOD_END"], "methodName": ["getYarnUrlFromURI"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Iterator < String >    it    =    StringHelper .  _ split ( applicationAttmeptIdStr )  . iterator (  )  ;", "if    (  !  ( it . next (  )  . equals (  . APPLICATION _ ATTEMPT _ PREFIX )  )  )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   AppAttemptId   prefix :     \"     +    applicationAttmeptIdStr )  )  ;", "}", "try    {", "return    . toApplicationAttemptId ( it )  ;", "}    catch    ( NumberFormatException   n )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   AppAttemptId :     \"     +    applicationAttmeptIdStr )  ,    n )  ;", "}", "}", "METHOD_END"], "methodName": ["toApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance ( Long . parseLong ( it . next (  )  )  ,    Integer . parseInt ( it . next (  )  )  )  ;", "ApplicationAttemptId   appAttemptId    =    ApplicationAttemptId . newInstance ( appId ,    Integer . parseInt ( it . next (  )  )  )  ;", "return   appAttemptId ;", "}", "METHOD_END"], "methodName": ["toApplicationAttemptId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Iterator < String >    it    =    StringHelper .  _ split ( appIdStr )  . iterator (  )  ;", "if    (  !  ( it . next (  )  . equals (  . APPLICATION _ PREFIX )  )  )     {", "throw   new   IllegalArgumentException (  (  (  (  \" Invalid   ApplicationId   prefix :     \"     +    appIdStr )     +     \"  .    The   valid   ApplicationId   should   start   with   prefix    \"  )     +     (  . APPLICATION _ PREFIX )  )  )  ;", "}", "try    {", "return    . toApplicationId ( it )  ;", "}    catch    ( NumberFormatException   n )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   AppAttemptId :     \"     +    appIdStr )  ,    n )  ;", "}", "}", "METHOD_END"], "methodName": ["toApplicationId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance ( Long . parseLong ( it . next (  )  )  ,    Integer . parseInt ( it . next (  )  )  )  ;", "return   appId ;", "}", "METHOD_END"], "methodName": ["toApplicationId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Iterator < String >    it    =    StringHelper .  _ split ( appIdStr )  . iterator (  )  ;", "it . next (  )  ;", "return    . toApplicationId ( recordFactory ,    it )  ;", "}", "METHOD_END"], "methodName": ["toApplicationId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "ApplicationId   appId    =    ApplicationId . newInstance ( Long . parseLong ( it . next (  )  )  ,    Integer . parseInt ( it . next (  )  )  )  ;", "return   appId ;", "}", "METHOD_END"], "methodName": ["toApplicationId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Iterator < String >    it    =    StringHelper .  _ split ( containerIdStr )  . iterator (  )  ;", "if    (  !  ( it . next (  )  . equals (  . CONTAINER _ PREFIX )  )  )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   ContainerId   prefix :     \"     +    containerIdStr )  )  ;", "}", "try    {", "ApplicationAttemptId   appAttemptID    =     . toApplicationAttemptId ( it )  ;", "ContainerId   containerId    =    ContainerId . newInstance ( appAttemptID ,    Integer . parseInt ( it . next (  )  )  )  ;", "return   containerId ;", "}    catch    ( NumberFormatException   n )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   ContainerId :     \"     +    containerIdStr )  ,    n )  ;", "}", "}", "METHOD_END"], "methodName": ["toContainerId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "String [  ]    parts    =    nodeIdStr . split (  \"  :  \"  )  ;", "if    (  ( parts . length )     !  =     2  )     {", "throw   new   IllegalArgumentException (  (  (  \" Invalid   NodeId    [  \"     +    nodeIdStr )     +     \"  ]  .    Expected   host : port \"  )  )  ;", "}", "try    {", "NodeId   nodeId    =    NodeId . newInstance ( parts [  0  ]  ,    Integer . parseInt ( parts [  1  ]  )  )  ;", "return   nodeId ;", "}    catch    ( NumberFormatException   e )     {", "throw   new   IllegalArgumentException (  (  \" Invalid   port :     \"     +     ( parts [  1  ]  )  )  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["toNodeId"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "return   appId . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "return   cId    =  =    null    ?    null    :    cId . toString (  )  ;", "}", "METHOD_END"], "methodName": ["toString"], "fileName": "org.apache.hadoop.yarn.util.ConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Path   current    =    path ;", "while    ( current    !  =    null )     {", "if    (  !  (  . checkPermissionOfOther ( fs ,    current ,    EXECUTE ,    statCache )  )  )     {", "return   false ;", "}", "current    =    current . getParent (  )  ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["ancestorsHaveExecutePermissions"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "FileStatus   fStatus    =    fs . getFileStatus ( path )  ;", "FsPermission   perm    =     . cachePerms ;", "if    (  ( resource . getVisibility (  )  )     =  =     ( LocalResourceVisibility . PUBLIC )  )     {", "perm    =     ( fStatus . isDirectory (  )  )     ?     . PUBLIC _ DIR _ PERMS    :     . PUBLIC _ FILE _ PERMS ;", "} else    {", "perm    =     ( fStatus . isDirectory (  )  )     ?     . PRIVATE _ DIR _ PERMS    :     . PRIVATE _ FILE _ PERMS ;", "}", ". LOG . debug (  (  (  (  \" Changing   permissions   for   path    \"     +    path )     +     \"    to   perm    \"  )     +    perm )  )  ;", "final   FsPermission   fPerm    =    perm ;", "if    ( null    =  =     ( userUgi )  )     {", "files . setPermission ( path ,    perm )  ;", "} else    {", "userUgi . doAs ( new   PrivilegedExceptionAction < Void >  (  )     {", "public   Void   run (  )    throws   Exception    {", "files . setPermission ( path ,    fPerm )  ;", "return   null ;", "}", "}  )  ;", "}", "if    (  ( fStatus . isDirectory (  )  )     &  &     (  !  ( fStatus . isSymlink (  )  )  )  )     {", "FileStatus [  ]    statuses    =    fs . listStatus ( path )  ;", "for    ( FileStatus   status    :    statuses )     {", "changePermissions ( fs ,    status . getPath (  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["changePermissions"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "FileStatus   status    =    FSDownload . getFileStatus ( fs ,    path ,    statCache )  ;", "FsPermission   perms    =    status . getPermission (  )  ;", "FsAction   otherAction    =    perms . getOtherAction (  )  ;", "return   otherAction . implies ( action )  ;", "}", "METHOD_END"], "methodName": ["checkPermissionOfOther"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "FsPermission   perms    =    status . getPermission (  )  ;", "FsAction   otherAction    =    perms . getOtherAction (  )  ;", "if    ( status . isDirectory (  )  )     {", "if    (  !  ( otherAction . implies ( dir )  )  )     {", "return   false ;", "}", "for    ( FileStatus   child    :    fs . listStatus ( status . getPath (  )  )  )     {", "if    (  !  (  . checkPublicPermsForAll ( fs ,    child ,    dir ,    file )  )  )     {", "return   false ;", "}", "}", "return   true ;", "}", "return   otherAction . implies ( file )  ;", "}", "METHOD_END"], "methodName": ["checkPublicPermsForAll"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "FileSystem   sourceFs    =    sCopy . getFileSystem ( conf )  ;", "Path   dCopy    =    new   Path ( dstdir ,     (  \" tmp _  \"     +     ( sCopy . getName (  )  )  )  )  ;", "FileStatus   sStat    =    sourceFs . getFileStatus ( sCopy )  ;", "if    (  ( sStat . getModificationTime (  )  )     !  =     ( resource . getTimestamp (  )  )  )     {", "throw   new   IOException (  (  (  (  (  (  \" Resource    \"     +    sCopy )     +     \"    changed   on   src   filesystem    ( expected    \"  )     +     ( resource . getTimestamp (  )  )  )     +     \"  ,    was    \"  )     +     ( sStat . getModificationTime (  )  )  )  )  ;", "}", "if    (  ( resource . getVisibility (  )  )     =  =     ( LocalResourceVisibility . PUBLIC )  )     {", "if    (  !  (  . isPublic ( sourceFs ,    sCopy ,    sStat ,    statCache )  )  )     {", "throw   new   IOException (  (  (  (  \" Resource    \"     +    sCopy )     +     \"    is   not   publicly   accessable   and   as   such   cannot   be   part   of   the \"  )     +     \"    public   cache .  \"  )  )  ;", "}", "}", "FileUtil . copy ( sourceFs ,    sStat ,    FileSystem . getLocal ( conf )  ,    dCopy ,    false ,    true ,    conf )  ;", "return   dCopy ;", "}", "METHOD_END"], "methodName": ["copy"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "files . mkdir ( path ,    perm ,    false )  ;", "if    (  !  ( perm . equals ( files . getUMask (  ) plyUMask ( perm )  )  )  )     {", "files . setPermission ( path ,    perm )  ;", "}", "}", "METHOD_END"], "methodName": ["createDir"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "return   new   com . google . common . cache . CacheLoader < Path ,    Future < FileStatus >  >  (  )     {", "public   Future < FileStatus >    load ( Path   path )     {", "try    {", "FileSystem   fs    =    path . getFileSystem ( conf )  ;", "return   Futures . immediateFuture ( fs . getFileStatus ( path )  )  ;", "}    catch    ( Throwable   th )     {", "return   Futures . immediateFailedFuture ( th )  ;", "}", "}", "}  ;", "}", "METHOD_END"], "methodName": ["createStatusCacheLoader"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "if    ( statC =  =    null )     {", "return   fs . getFileStatus ( path )  ;", "}", "try    {", "return   statCget ( path )  . get (  )  ;", "}    catch    ( ExecutionException   e )     {", "Throwable   cause    =    e . getCause (  )  ;", "if    ( cause   instanceof   IOException )     {", "throw    (  ( IOException )     ( cause )  )  ;", "} else    {", "throw   new   IOException ( cause )  ;", "}", "}    catch    ( InterruptedException   e )     {", "Thread . currentThread (  )  . interrupt (  )  ;", "throw   new   IOException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getFileStatus"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "return   resource ;", "}", "METHOD_END"], "methodName": ["getResource"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "current    =    fs . makeQualified ( current )  ;", "if    (  !  (  . checkPublicPermsForAll ( fs ,    sStat ,    READ _ EXECUTE ,    READ )  )  )     {", "return   false ;", "}", "if    (  ( Shell . WINDOWS )     &  &     ( fs   instanceof   LocalFileSystem )  )     {", "return   true ;", "}", "return    . ancestorsHaveExecutePermissions ( fs ,    current . getParent (  )  ,    statCache )  ;", "}", "METHOD_END"], "methodName": ["isPublic"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "switch    ( resource . getType (  )  )     {", "case   ARCHIVE    :", "{", "String   lowerDst    =    dst . getName (  )  . toLowerCase (  )  ;", "if    ( lowerDst . endsWith (  \"  . jar \"  )  )     {", "RunJar . unJar ( localrsrc ,    dst )  ;", "} else", "if    ( lowerDst . endsWith (  \"  . zip \"  )  )     {", "fs . FileUtil . unZip ( localrsrc ,    dst )  ;", "} else", "if    (  (  ( lowerDst . endsWith (  \"  . tar . gz \"  )  )     |  |     ( lowerDst . endsWith (  \"  . tgz \"  )  )  )     |  |     ( lowerDst . endsWith (  \"  . tar \"  )  )  )     {", "fs . FileUtil . unTar ( localrsrc ,    dst )  ;", "} else    {", "FSDownload . LOG . warn (  (  \" Cannot   unpack    \"     +    localrsrc )  )  ;", "if    (  !  ( localrsrc . renameTo ( dst )  )  )     {", "throw   new   IOException (  (  (  (  (  \" Unable   to   rename   file :     [  \"     +    localrsrc )     +     \"  ]    to    [  \"  )     +    dst )     +     \"  ]  \"  )  )  ;", "}", "}", "}", "break ;", "case   PATTERN    :", "{", "String   lowerDst    =    dst . getName (  )  . toLowerCase (  )  ;", "if    ( lowerDst . endsWith (  \"  . jar \"  )  )     {", "RunJar . unJar ( localrsrc ,    dst ,    pattern )  ;", "File   newDst    =    new   File ( dst ,    dst . getName (  )  )  ;", "if    (  (  !  ( dst . exists (  )  )  )     &  &     (  !  ( dst . mkdir (  )  )  )  )     {", "throw   new   IOException (  (  (  \" Unable   to   create   directory :     [  \"     +    dst )     +     \"  ]  \"  )  )  ;", "}", "if    (  !  ( localrsrc . renameTo ( newDst )  )  )     {", "throw   new   IOException (  (  (  (  (  \" Unable   to   rename   file :     [  \"     +    localrsrc )     +     \"  ]    to    [  \"  )     +    newDst )     +     \"  ]  \"  )  )  ;", "}", "} else", "if    ( lowerDst . endsWith (  \"  . zip \"  )  )     {", "FSDownload . LOG . warn (  (  (  (  \" Treating    [  \"     +    localrsrc )     +     \"  ]    as   an   archive   even   though   it    \"  )     +     \" was   specified   as   PATTERN \"  )  )  ;", "fs . FileUtil . unZip ( localrsrc ,    dst )  ;", "} else", "if    (  (  ( lowerDst . endsWith (  \"  . tar . gz \"  )  )     |  |     ( lowerDst . endsWith (  \"  . tgz \"  )  )  )     |  |     ( lowerDst . endsWith (  \"  . tar \"  )  )  )     {", "FSDownload . LOG . warn (  (  (  (  \" Treating    [  \"     +    localrsrc )     +     \"  ]    as   an   archive   even   though   it    \"  )     +     \" was   specified   as   PATTERN \"  )  )  ;", "fs . FileUtil . unTar ( localrsrc ,    dst )  ;", "} else    {", "FSDownload . LOG . warn (  (  \" Cannot   unpack    \"     +    localrsrc )  )  ;", "if    (  !  ( localrsrc . renameTo ( dst )  )  )     {", "throw   new   IOException (  (  (  (  (  \" Unable   to   rename   file :     [  \"     +    localrsrc )     +     \"  ]    to    [  \"  )     +    dst )     +     \"  ]  \"  )  )  ;", "}", "}", "}", "break ;", "case   FILE    :", "default    :", "if    (  !  ( localrsrc . renameTo ( dst )  )  )     {", "throw   new   IOException (  (  (  (  (  \" Unable   to   rename   file :     [  \"     +    localrsrc )     +     \"  ]    to    [  \"  )     +    dst )     +     \"  ]  \"  )  )  ;", "}", "break ;", "}", "if    ( localrsrc . isFile (  )  )     {", "try    {", "files . delete ( new   Path ( localrsrc . toString (  )  )  ,    false )  ;", "}    catch    ( IOException   ignore )     {", "}", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["unpack"], "fileName": "org.apache.hadoop.yarn.util.FSDownload"}, {"methodBody": ["METHOD_START", "{", "return   System . currentTimeMillis (  )  ;", "}", "METHOD_END"], "methodName": ["getCurrentTime"], "fileName": "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "LinuxResourceCalculatorPlugin   plugin    =    new   LinuxResourceCalculatorPlugin (  )  ;", "System . out . println (  (  \" Physical   memory   Size    ( bytes )     :     \"     +     ( plugin . getPhysicalMemorySize (  )  )  )  )  ;", "System . out . println (  (  \" Total   Virtual   memory   Size    ( bytes )     :     \"     +     ( plugin . getVirtualMemorySize (  )  )  )  )  ;", "System . out . println (  (  \" Available   Physical   memory   Size    ( bytes )     :     \"     +     ( plugin . getAvailablePhysicalMemorySize (  )  )  )  )  ;", "System . out . println (  (  \" Total   Available   Virtual   memory   Size    ( bytes )     :     \"     +     ( plugin . getAvailableVirtualMemorySize (  )  )  )  )  ;", "System . out . println (  (  \" Number   of   Processors    :     \"     +     ( plugin . getNumProcessors (  )  )  )  )  ;", "System . out . println (  (  \" CPU   frequency    ( kHz )     :     \"     +     ( plugin . getCpuFrequency (  )  )  )  )  ;", "System . out . println (  (  \" Cumulative   CPU   time    ( ms )     :     \"     +     ( plugin . getCumulativeCpuTime (  )  )  )  )  ;", "try    {", "Thread . sleep (  5  0  0 L )  ;", "}    catch    ( InterruptedException   e )     {", "}", "System . out . println (  (  \" CPU   usage    %     :     \"     +     ( plugin . getCpuUsage (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "if    ( readCpuInfoFile )     {", "return ;", "}", "BufferedReader   in    =    null ;", "FileReader   fReader    =    null ;", "try    {", "fReader    =    new   FileReader ( procfsCpuFile )  ;", "in    =    new   BufferedReader ( fReader )  ;", "}    catch    ( FileNotFoundException   f )     {", "return ;", "}", "Matcher   mat    =    null ;", "try    {", "numProcessors    =     0  ;", "String   str    =    in . readLine (  )  ;", "while    ( str    !  =    null )     {", "mat    =     . PROCESSOR _ FORMAT . matcher ( str )  ;", "if    ( mat . find (  )  )     {", "( numProcessors )  +  +  ;", "}", "mat    =     . FREQUENCY _ FORMAT . matcher ( str )  ;", "if    ( mat . find (  )  )     {", "cpuFrequency    =     (  ( long )     (  ( Double . parseDouble ( mat . group (  1  )  )  )     *     1  0  0  0  )  )  ;", "}", "str    =    in . readLine (  )  ;", "}", "}    catch    ( IOException   io )     {", ". LOG . warn (  (  \" Error   reading   the   stream    \"     +    io )  )  ;", "}    finally    {", "try    {", "fReader . close (  )  ;", "try    {", "in . close (  )  ;", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    in )  )  ;", "}", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    fReader )  )  ;", "}", "}", "readCpuInfoFile    =    true ;", "}", "METHOD_END"], "methodName": ["readProcCpuInfoFile"], "fileName": "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "readProcMemInfoFile ( false )  ;", "}", "METHOD_END"], "methodName": ["readProcMemInfoFile"], "fileName": "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "if    (  ( readMemInfoFile )     &  &     (  ! readAgain )  )     {", "return ;", "}", "BufferedReader   in    =    null ;", "FileReader   fReader    =    null ;", "try    {", "fReader    =    new   FileReader ( procfsMemFile )  ;", "in    =    new   BufferedReader ( fReader )  ;", "}    catch    ( FileNotFoundException   f )     {", "return ;", "}", "Matcher   mat    =    null ;", "try    {", "String   str    =    in . readLine (  )  ;", "while    ( str    !  =    null )     {", "mat    =     . PROCFS _ MEMFILE _ FORMAT . matcher ( str )  ;", "if    ( mat . find (  )  )     {", "if    ( mat . group (  1  )  . equals (  . MEMTOTAL _ STRING )  )     {", "ramSize    =    Long . parseLong ( mat . group (  2  )  )  ;", "} else", "if    ( mat . group (  1  )  . equals (  . SWAPTOTAL _ STRING )  )     {", "swapSize    =    Long . parseLong ( mat . group (  2  )  )  ;", "} else", "if    ( mat . group (  1  )  . equals (  . MEMFREE _ STRING )  )     {", "ramSizeFree    =    Long . parseLong ( mat . group (  2  )  )  ;", "} else", "if    ( mat . group (  1  )  . equals (  . SWAPFREE _ STRING )  )     {", "swapSizeFree    =    Long . parseLong ( mat . group (  2  )  )  ;", "} else", "if    ( mat . group (  1  )  . equals (  . INACTIVE _ STRING )  )     {", "inactiveSize    =    Long . parseLong ( mat . group (  2  )  )  ;", "}", "}", "str    =    in . readLine (  )  ;", "}", "}    catch    ( IOException   io )     {", ". LOG . warn (  (  \" Error   reading   the   stream    \"     +    io )  )  ;", "}    finally    {", "try    {", "fReader . close (  )  ;", "try    {", "in . close (  )  ;", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    in )  )  ;", "}", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    fReader )  )  ;", "}", "}", "readMemInfoFile    =    true ;", "}", "METHOD_END"], "methodName": ["readProcMemInfoFile"], "fileName": "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    null ;", "FileReader   fReader    =    null ;", "try    {", "fReader    =    new   FileReader ( procfsStatFile )  ;", "in    =    new   BufferedReader ( fReader )  ;", "}    catch    ( FileNotFoundException   f )     {", "return ;", "}", "Matcher   mat    =    null ;", "try    {", "String   str    =    in . readLine (  )  ;", "while    ( str    !  =    null )     {", "mat    =     . CPU _ TIME _ FORMAT . matcher ( str )  ;", "if    ( mat . find (  )  )     {", "long   uTime    =    Long . parseLong ( mat . group (  1  )  )  ;", "long   nTime    =    Long . parseLong ( mat . group (  2  )  )  ;", "long   sTime    =    Long . parseLong ( mat . group (  3  )  )  ;", "cumulativeCpuTime    =     ( uTime    +    nTime )     +    sTime ;", "break ;", "}", "str    =    in . readLine (  )  ;", "}", "cumulativeCpuTime    *  =    jiffyLengthInMillis ;", "}    catch    ( IOException   io )     {", ". LOG . warn (  (  \" Error   reading   the   stream    \"     +    io )  )  ;", "}    finally    {", "try    {", "fReader . close (  )  ;", "try    {", "in . close (  )  ;", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    in )  )  ;", "}", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    fReader )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["readProcStatFile"], "fileName": "org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "ProcfsBasedProcessTree . ProcessInfo   pInfo    =    new   ProcfsBasedProcessTree . ProcessInfo (  _ pid )  ;", "pInfo    =    ProcfsBasedProcessTree . constructProcessInfo ( pInfo ,    procfs )  ;", "if    ( pInfo    =  =    null )", "return   true ;", "String   pgrpId    =    pInfo . getPgrpId (  )  . toString (  )  ;", "return   pgrpId . equals (  _ pid )  ;", "}", "METHOD_END"], "methodName": ["checkPidPgrpidForMatch"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "ProcfsBasedProcessTree . ProcessInfo   ret    =    null ;", "BufferedReader   in    =    null ;", "FileReader   fReader    =    null ;", "try    {", "File   pidDir    =    new   File ( procfsDir ,    pinfo . getPid (  )  )  ;", "fReader    =    new   FileReader ( new   File ( pidDir ,    ProcfsBasedProcessTree . PROCFS _ STAT _ FILE )  )  ;", "in    =    new   BufferedReader ( fReader )  ;", "}    catch    ( FileNotFoundException   f )     {", "return   ret ;", "}", "ret    =    pinfo ;", "try    {", "String   str    =    in . readLine (  )  ;", "Matcher   m    =    ProcfsBasedProcessTree . PROCFS _ STAT _ FILE _ FORMAT . matcher ( str )  ;", "boolean   mat    =    m . find (  )  ;", "if    ( mat )     {", "pinfo . updateProcessInfo ( m . group (  2  )  ,    m . group (  3  )  ,    Integer . parseInt ( m . group (  4  )  )  ,    Integer . parseInt ( m . group (  5  )  )  ,    Long . parseLong ( m . group (  7  )  )  ,    new   BigInteger ( m . group (  8  )  )  ,    Long . parseLong ( m . group (  1  0  )  )  ,    Long . parseLong ( m . group (  1  1  )  )  )  ;", "} else    {", "ProcfsBasedProcessTree . LOG . warn (  (  (  \" Unexpected :    procfs   stat   file   is   not   in   the   expected   format \"     +     \"    for   process   with   pid    \"  )     +     ( pinfo . getPid (  )  )  )  )  ;", "ret    =    null ;", "}", "}    catch    ( IOException   io )     {", "ProcfsBasedProcessTree . LOG . warn (  (  \" Error   reading   the   stream    \"     +    io )  )  ;", "ret    =    null ;", "}    finally    {", "try    {", "fReader . close (  )  ;", "try    {", "in . close (  )  ;", "}    catch    ( IOException   i )     {", "ProcfsBasedProcessTree . LOG . warn (  (  \" Error   closing   the   stream    \"     +    in )  )  ;", "}", "}    catch    ( IOException   i )     {", "ProcfsBasedProcessTree . LOG . warn (  (  \" Error   closing   the   stream    \"     +    fReader )  )  ;", "}", "}", "return   ret ;", "}", "METHOD_END"], "methodName": ["constructProcessInfo"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   in    =    null ;", "FileReader   fReader    =    null ;", "try    {", "File   pidDir    =    new   File ( procfsDir ,    pInfo . getPid (  )  )  ;", "File   file    =    new   File ( pidDir ,     . SMAPS )  ;", "if    (  !  ( file . exists (  )  )  )     {", "return ;", "}", "fReader    =    new   FileReader ( file )  ;", "in    =    new   BufferedReader ( fReader )  ;", ". ProcessSmapMemoryInfo   memoryMappingInfo    =    null ;", "List < String >    lines    =    IOUtils . readLines ( in )  ;", "for    ( String   line    :    lines )     {", "line    =    line . trim (  )  ;", "try    {", "Matcher   address    =     . ADDRESS _ PATTERN . matcher ( line )  ;", "if    ( address . find (  )  )     {", "memoryMappingInfo    =    new    . ProcessSmapMemoryInfo ( line )  ;", "memoryMappingInfo . setPermission ( address . group (  4  )  )  ;", "pInfo . getMemoryInfoList (  )  . add ( memoryMappingInfo )  ;", "continue ;", "}", "Matcher   memInfo    =     . MEM _ INFO _ PATTERN . matcher ( line )  ;", "if    ( memInfo . find (  )  )     {", "String   key    =    memInfo . group (  1  )  . trim (  )  ;", "String   value    =    memInfo . group (  2  )  . replace (  . KB ,     \"  \"  )  . trim (  )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  \" MemInfo    :     \"     +    key )     +     \"     :    Value       :     \"  )     +    value )  )  ;", "}", "memoryMappingInfo . setMemInfo ( key ,    value )  ;", "}", "}    catch    ( Throwable   t )     {", ". LOG . warn (  (  (  (  \" Error   parsing   smaps   line    :     \"     +    line )     +     \"  ;     \"  )     +     ( t . getMessage (  )  )  )  )  ;", "}", "}", "}    catch    ( FileNotFoundException   f )     {", ". LOG . error ( f . getMessage (  )  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error ( e . getMessage (  )  )  ;", "}    catch    ( Throwable   t )     {", ". LOG . error ( t . getMessage (  )  )  ;", "}    finally    {", "IOUtils . closeQuietly ( in )  ;", "}", "}", "METHOD_END"], "methodName": ["constructProcessSMAPInfo"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "List < String >    currentPIDs    =    new   ArrayList < String >  (  )  ;", "currentPIDs . addAll ( p . keySet (  )  )  ;", "return   currentPIDs ;", "}", "METHOD_END"], "methodName": ["getCurrentProcessIDs"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "String [  ]    processDirs    =    new   File ( procfsDir )  . list (  )  ;", "List < String >    processList    =    new   ArrayList < String >  (  )  ;", "for    ( String   dir    :    processDirs )     {", "Matcher   m    =     . numberPattern . matcher ( dir )  ;", "if    (  !  ( m . matches (  )  )  )", "continue ;", "try    {", "if    ( new   File ( procfsDir ,    dir )  . isDirectory (  )  )     {", "processList . add ( dir )  ;", "}", "}    catch    ( SecurityException   s )     {", "}", "}", "return   processList ;", "}", "METHOD_END"], "methodName": ["getProcessList"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "long   total    =     0  ;", "for    (  . ProcessInfo   p    :    processTree . values (  )  )     {", "if    (  ( p    !  =    null )     &  &     (  ( p . getAge (  )  )     >    olderThanAge )  )     {", ". ProcessTreeSmapMemInfo   procMemInfo    =    processSMAPTree . get ( p . getPid (  )  )  ;", "if    ( procMemInfo    !  =    null )     {", "for    (  . ProcessSmapMemoryInfo   info    :    procMemInfo . getMemoryInfoList (  )  )     {", "if    (  ( info . getPermission (  )  . trim (  )  . equalsIgnoreCase (  . READ _ ONLY _ WITH _ SHARED _ PERMISSION )  )     |  |     ( info . getPermission (  )  . trim (  )  . equalsIgnoreCase (  . READ _ EXECUTE _ WITH _ SHARED _ PERMISSION )  )  )     {", "continue ;", "}", "total    +  =     (  ( Math . min ( info . sharedDirty ,    info . pss )  )     +     ( info . privateDirty )  )     +     ( info . privateClean )  ;", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug (  (  (  (  (  (  (  (  (  (  (  (  (  (  \"    total (  \"     +    olderThanAge )     +     \"  )  :    PID    :     \"  )     +     ( p . getPid (  )  )  )     +     \"  ,    SharedDirty    :     \"  )     +     ( info . sharedDirty )  )     +     \"  ,    PSS    :     \"  )     +     ( info . pss )  )     +     \"  ,    Private _ Dirty    :     \"  )     +     ( info . privateDirty )  )     +     \"  ,    Private _ Clean    :     \"  )     +     ( info . privateClean )  )     +     \"  ,    total    :     \"  )     +     ( total    *     (  . KB _ TO _ BYTES )  )  )  )  ;", "}", "}", "}", "if    (  . LOG . isDebugEnabled (  )  )     {", ". LOG . debug ( procMemInfo . toString (  )  )  ;", "}", "}", "}", "total    =    total    *     (  . KB _ TO _ BYTES )  ;", ". LOG . info (  (  \" SmapBasedCumulativeRssmem    ( bytes )     :     \"     +    total )  )  ;", "return   total ;", "}", "METHOD_END"], "methodName": ["getSmapBasedCumulativeRssmem"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "if    ( pid    =  =    null )", "return    . deadPid ;", "Matcher   m    =     . numberPattern . matcher ( pid )  ;", "if    ( m . matches (  )  )", "return   pid ;", "return    . deadPid ;", "}", "METHOD_END"], "methodName": ["getValidPID"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "try    {", "if    (  !  ( Shell . LINUX )  )     {", ". LOG . info (  (  \"    currently   is   supported   only   on    \"     +     \" Linux .  \"  )  )  ;", "return   false ;", "}", "}    catch    ( SecurityException   se )     {", ". LOG . warn (  (  \" Failed   to   get   Operating   System   name .     \"     +    se )  )  ;", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["isAvailable"], "fileName": "org.apache.hadoop.yarn.util.ProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "YarnConfiguration   yarnConf    =    new   YarnConfiguration ( conf )  ;", "Collection < String >    rmIds    =    yarnConf . getStringCollection ( RM _ HA _ IDS )  ;", "for    ( String   currentId    :    rmIds )     {", "yarnConf . set ( RM _ HA _ ID ,    currentId )  ;", "try    {", "HAServiceState   haState    =     . getHAState ( yarnConf )  ;", "if    ( haState . equals ( ACTIVE )  )     {", "return   currentId ;", "}", "}    catch    ( Exception   e )     {", "}", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["findActiveRMHAId"], "fileName": "org.apache.hadoop.yarn.util.RMHAUtils"}, {"methodBody": ["METHOD_START", "{", "HAServiceTarget   haServiceTarget ;", "int   rpcTimeoutForChecks    =    Conf . getInt ( HA _ FC _ CLI _ CHECK _ TIMEOUT _ KEY ,    HA _ FC _ CLI _ CHECK _ TIMEOUT _ DEFAULT )  ;", "Conf . set ( HADOOP _ SECURITY _ SERVICE _ USER _ NAME _ KEY ,    Conf . get ( RM _ PRINCIPAL ,     \"  \"  )  )  ;", "haServiceTarget    =    new   RMHAServiceTarget ( Conf )  ;", "HAServiceProtocol   proto    =    haServiceTarget . getProxy ( Conf ,    rpcTimeoutForChecks )  ;", "HAServiceState   haState    =    proto . getServiceStatus (  )  . getState (  )  ;", "return   haState ;", "}", "METHOD_END"], "methodName": ["getHAState"], "fileName": "org.apache.hadoop.yarn.util.RMHAUtils"}, {"methodBody": ["METHOD_START", "{", "Collection < String >    rmIds    =    conf . getStringCollection ( RM _ HA _ IDS )  ;", "List < String >    addrs    =    new   ArrayList < String >  (  )  ;", "if    ( YConfiguration . useHttps ( conf )  )     {", "for    ( String   id    :    rmIds )     {", "String   addr    =    conf . get (  (  (  ( YConfiguration . RM _ WEBAPP _ HTTPS _ ADDRESS )     +     \"  .  \"  )     +    id )  )  ;", "if    ( addr    !  =    null )     {", "addrs . add ( addr )  ;", "}", "}", "} else    {", "for    ( String   id    :    rmIds )     {", "String   addr    =    conf . get (  (  (  ( YConfiguration . RM _ WEBAPP _ ADDRESS )     +     \"  .  \"  )     +    id )  )  ;", "if    ( addr    !  =    null )     {", "addrs . add ( addr )  ;", "}", "}", "}", "return   addrs ;", "}", "METHOD_END"], "methodName": ["getRMHAWebappAddresses"], "fileName": "org.apache.hadoop.yarn.util.RMHAUtils"}, {"methodBody": ["METHOD_START", "{", "List < String >    tmpList    =    new   ArrayList < String >  (  1  )  ;", "tmpList . add ( hostName )  ;", "List < String >    rNameList    =     . dnsToSwitchMapping . resolve ( tmpList )  ;", "String   rName    =    null ;", "if    (  ( rNameList    =  =    null )     |  |     (  ( rNameList . get (  0  )  )     =  =    null )  )     {", "rName    =    NetworkTopology . DEFAULT _ RACK ;", ". LOG . info (  (  (  (  \" Couldn ' t   resolve    \"     +    hostName )     +     \"  .    Falling   back   to    \"  )     +     ( NetworkTopology . DEFAULT _ RACK )  )  )  ;", "} else    {", "rName    =    rNameList . get (  0  )  ;", ". LOG . info (  (  (  (  \" Resolved    \"     +    hostName )     +     \"    to    \"  )     +    rName )  )  ;", "}", "return   new   NodeBase ( hostName ,    rName )  ;", "}", "METHOD_END"], "methodName": ["coreResolve"], "fileName": "org.apache.hadoop.yarn.util.RackResolver"}, {"methodBody": ["METHOD_START", "{", "return   RackResolver . dnsToSwitchMapping ;", "}", "METHOD_END"], "methodName": ["getDnsToSwitchMapping"], "fileName": "org.apache.hadoop.yarn.util.RackResolver"}, {"methodBody": ["METHOD_START", "{", "if    ( RackResolver . initCalled )     {", "return ;", "} else    {", "RackResolver . initCalled    =    true ;", "}", "Class <  ?    extends   DNSToSwitchMapping >    dnsToSwitchMappingClass    =    conf . getClass ( NET _ TOPOLOGY _ NODE _ SWITCH _ MAPPING _ IMPL _ KEY ,    ScriptBasedMapping . class ,    DNSToSwitchMapping . class )  ;", "try    {", "DNSToSwitchMapping   newInstance    =    ReflectionUtils . newInstance ( dnsToSwitchMappingClass ,    conf )  ;", "RackResolver . dnsToSwitchMapping    =     ( newInstance   instanceof   CachedDNSToSwitchMapping )     ?    newInstance    :    new   CachedDNSToSwitchMapping ( newInstance )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["init"], "fileName": "org.apache.hadoop.yarn.util.RackResolver"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( RackResolver . initCalled )  )     {", "throw   new   IllegalStateException (  \" RackResolver   class   not   yet   initialized \"  )  ;", "}", "return   RackResolver . coreResolve ( hostName )  ;", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.apache.hadoop.yarn.util.RackResolver"}, {"methodBody": ["METHOD_START", "{", "RackResolver . init ( conf )  ;", "return   RackResolver . coreResolve ( hostName )  ;", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.apache.hadoop.yarn.util.RackResolver"}, {"methodBody": ["METHOD_START", "{", "if    ( clazz    !  =    null )     {", "return   ReflectionUtils . newInstance ( clazz ,    conf )  ;", "}", "try    {", "if    ( Shell . LINUX )     {", "return   new   Linux (  )  ;", "}", "if    ( Shell . WINDOWS )     {", "return   new   Windows (  )  ;", "}", "}    catch    ( SecurityException   se )     {", "return   null ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getResourceCalculatorPlugin"], "fileName": "org.apache.hadoop.yarn.util.ResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "processPid    =    pid ;", "}", "METHOD_END"], "methodName": ["setProcessPid"], "fileName": "org.apache.hadoop.yarn.util.ResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "return   getCumulativeRssmem (  0  )  ;", "}", "METHOD_END"], "methodName": ["getCumulativeRssmem"], "fileName": "org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree"}, {"methodBody": ["METHOD_START", "{", "return   getCumulativeVmem (  0  )  ;", "}", "METHOD_END"], "methodName": ["getCumulativeVmem"], "fileName": "org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree"}, {"methodBody": ["METHOD_START", "{", "if    ( clazz    !  =    null )     {", "try    {", "Constructor <  ?    extends    >    c    =    clazz . getConstructor ( String . class )  ;", "rctree    =    c . newInstance ( pid )  ;", "rctree . setConf ( conf )  ;", "return   rctree ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "if    ( ProcfsBasedProcessTree . isAvailable (  )  )     {", "return   new   ProcfsBasedProcessTree ( pid )  ;", "}", "if    ( WindowsBasedProcessTree . isAvailable (  )  )     {", "return   new   WindowsBasedProcessTree ( pid )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getResourceCalculatorProcessTree"], "fileName": "org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper .  _ JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["_join"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper .  _ SPLITTER . split ( s )  ;", "}", "METHOD_END"], "methodName": ["_split"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . CSV _ JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["cjoin"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . DOT _ JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["djoin"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . ABS _ URL _ RE . matcher ( url )  . find (  )  ;", "}", "METHOD_END"], "methodName": ["isAbsUrl"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["join"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   Joiner . on ( sep )  . join ( args )  ;", "}", "METHOD_END"], "methodName": ["joins"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . PATH _ ARG _ JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["pajoin"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   String . format (  \"  %  .  2 f \"  ,     ( value    *     1  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["percent"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . PATH _ JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["pjoin"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . SSV _ JOINER . join ( args )  ;", "}", "METHOD_END"], "methodName": ["sjoin"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . SSV _ SPLITTER . split ( s )  ;", "}", "METHOD_END"], "methodName": ["split"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  ( sb . length (  )  )     <  =     0  )     |  |     (  ( sb . charAt (  (  ( sb . length (  )  )     -     1  )  )  )     !  =     '  /  '  )  )     &  &     (  !  ( part . startsWith (  \"  /  \"  )  )  )  )     {", "sbpend (  '  /  '  )  ;", "}", "sbpend ( part )  ;", "}", "METHOD_END"], "methodName": ["uappend"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "boolean   first    =    true ;", "for    ( String   part    :    args )     {", "if    ( first )     {", "first    =    false ;", "if    (  ( part . startsWith (  \"  #  \"  )  )     |  |     (  . isAbsUrl ( part )  )  )     {", "sb . append ( part )  ;", "} else    {", ". uappend ( sb ,    pathPrefix )  ;", ". uappend ( sb ,    part )  ;", "}", "} else    {", ". uappend ( sb ,    part )  ;", "}", "}", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["ujoin"], "fileName": "org.apache.hadoop.yarn.util.StringHelper"}, {"methodBody": ["METHOD_START", "{", "return   System . currentTimeMillis (  )  ;", "}", "METHOD_END"], "methodName": ["getTime"], "fileName": "org.apache.hadoop.yarn.util.SystemClock"}, {"methodBody": ["METHOD_START", "{", "ContainerId   id    =    TestContainerId . newContainerId (  0  ,     0  ,     0  ,     0  )  ;", "String   cid    =     . toString ( id )  ;", "assertEquals (  \" container _  0  _  0  0  0  0  _  0  0  _  0  0  0  0  0  0  \"  ,    cid )  ;", "ContainerId   gen    =     . toContainerId ( cid )  ;", "assertEquals ( gen ,    id )  ;", "}", "METHOD_END"], "methodName": ["testContainerId"], "fileName": "org.apache.hadoop.yarn.util.TestConverterUtils"}, {"methodBody": ["METHOD_START", "{", "assertNull ( ConverterUtils . toString (  (  ( ContainerId )     ( null )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testContainerIdNull"], "fileName": "org.apache.hadoop.yarn.util.TestConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Path   expectedPath    =    new   Path (  \" hdfs :  /  / foo . com \"  )  ;", "URL   url    =     . getYarnUrlFromPath ( expectedPath )  ;", "Path   actualPath    =     . getPathFromYarnURL ( url )  ;", "assertEquals ( expectedPath ,    actualPath )  ;", "}", "METHOD_END"], "methodName": ["testConvertUrlWithNoPort"], "fileName": "org.apache.hadoop.yarn.util.TestConverterUtils"}, {"methodBody": ["METHOD_START", "{", "Path   expectedPath    =    new   Path (  \" foo :  /  / username : password @ example . com :  8  0  4  2  \"  )  ;", "URL   url    =     . getYarnUrlFromPath ( expectedPath )  ;", "Path   actualPath    =     . getPathFromYarnURL ( url )  ;", "assertEquals ( expectedPath ,    actualPath )  ;", "}", "METHOD_END"], "methodName": ["testConvertUrlWithUserinfo"], "fileName": "org.apache.hadoop.yarn.util.TestConverterUtils"}, {"methodBody": ["METHOD_START", "{", "FSDataOutputStream   out    =    null ;", "try    {", "byte [  ]    bytes    =    new   byte [ len ]  ;", "out    =    files . create ( p ,    EnumSet . of ( CREATE ,    OVERWRITE )  )  ;", "r . nextBytes ( bytes )  ;", "out . write ( bytes )  ;", "}    finally    {", "if    ( out    !  =    null )", "out . close (  )  ;", "}", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "TestFSDownload . createFile ( files ,    p ,    len ,    r )  ;", "LocalResource   ret    =    TestFSDownload . recordFactory . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( p )  )  ;", "ret . setSize ( len )  ;", "ret . setType ( FILE )  ;", "ret . setVisibility ( vis )  ;", "ret . setTimestamp ( files . getFileStatus ( p )  . getModificationTime (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createFile"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "TestFSDownload . LOG . info (  (  \" Create   jar   file    \"     +    p )  )  ;", "File   jarFile    =    new   File ( files . makeQualified ( p )  . toUri (  )  )  ;", "FileOutputStream   stream    =    new   FileOutputStream ( jarFile )  ;", "TestFSDownload . LOG . info (  \" Create   jar   out   stream    \"  )  ;", "JarOutputStream   out    =    new   JarOutputStream ( stream ,    new   Manifest (  )  )  ;", "TestFSDownload . LOG . info (  \" Done   writing   jar   stream    \"  )  ;", "out . close (  )  ;", "LocalResource   ret    =    TestFSDownload . recordFactory . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( p )  )  ;", "FileStatus   status    =    files . getFileStatus ( p )  ;", "ret . setSize ( status . getLen (  )  )  ;", "ret . setTimestamp ( status . getModificationTime (  )  )  ;", "ret . setType ( PATTERN )  ;", "ret . setVisibility ( vis )  ;", "ret . setPattern (  \" classes /  .  *  \"  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createJar"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [ len ]  ;", "r . nextBytes ( bytes )  ;", "File   archiveFile    =    new   File (  (  ( p . toUri (  )  . getPath (  )  )     +     \"  . jar \"  )  )  ;", "archiveFile . createNewFile (  )  ;", "JarOutputStream   out    =    new   JarOutputStream ( new   FileOutputStream ( archiveFile )  )  ;", "out . putNextEntry ( new   JarEntry ( p . getName (  )  )  )  ;", "out . write ( bytes )  ;", "out . closeEntry (  )  ;", "out . close (  )  ;", "LocalResource   ret    =     . recordFactory . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( new   Path (  (  ( p . toString (  )  )     +     \"  . jar \"  )  )  )  )  ;", "ret . setSize ( len )  ;", "ret . setType ( ARCHIVE )  ;", "ret . setVisibility ( vis )  ;", "ret . setTimestamp ( files . getFileStatus ( new   Path (  (  ( p . toString (  )  )     +     \"  . jar \"  )  )  )  . getModificationTime (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createJarFile"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [ len ]  ;", "r . nextBytes ( bytes )  ;", "File   archiveFile    =    new   File (  (  ( p . toUri (  )  . getPath (  )  )     +     \"  . tar \"  )  )  ;", "archiveFile . createNewFile (  )  ;", "TarArchiveOutputStream   out    =    new   TarArchiveOutputStream ( new   FileOutputStream ( archiveFile )  )  ;", "TarArchiveEntry   entry    =    new   TarArchiveEntry ( p . getName (  )  )  ;", "entry . setSize ( bytes . length )  ;", "out . putArchiveEntry ( entry )  ;", "out . write ( bytes )  ;", "out . closeArchiveEntry (  )  ;", "out . close (  )  ;", "LocalResource   ret    =     . recordFactory . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( new   Path (  (  ( p . toString (  )  )     +     \"  . tar \"  )  )  )  )  ;", "ret . setSize ( len )  ;", "ret . setType ( ARCHIVE )  ;", "ret . setVisibility ( vis )  ;", "ret . setTimestamp ( files . getFileStatus ( new   Path (  (  ( p . toString (  )  )     +     \"  . tar \"  )  )  )  . getModificationTime (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createTarFile"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [ len ]  ;", "r . nextBytes ( bytes )  ;", "File   gzipFile    =    new   File (  (  ( p . toUri (  )  . getPath (  )  )     +     \"  . tar . gz \"  )  )  ;", "gzipFile . createNewFile (  )  ;", "TarArchiveOutputStream   out    =    new   TarArchiveOutputStream ( new   GZIPOutputStream ( new   FileOutputStream ( gzipFile )  )  )  ;", "TarArchiveEntry   entry    =    new   TarArchiveEntry ( p . getName (  )  )  ;", "entry . setSize ( bytes . length )  ;", "out . putArchiveEntry ( entry )  ;", "out . write ( bytes )  ;", "out . closeArchiveEntry (  )  ;", "out . close (  )  ;", "LocalResource   ret    =     . recordFactory . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( new   Path (  (  ( p . toString (  )  )     +     \"  . tar . gz \"  )  )  )  )  ;", "ret . setSize ( len )  ;", "ret . setType ( ARCHIVE )  ;", "ret . setVisibility ( vis )  ;", "ret . setTimestamp ( files . getFileStatus ( new   Path (  (  ( p . toString (  )  )     +     \"  . tar . gz \"  )  )  )  . getModificationTime (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createTgzFile"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "byte [  ]    bytes    =    new   byte [ len ]  ;", "r . nextBytes ( bytes )  ;", "File   archiveFile    =    new   File (  (  ( p . toUri (  )  . getPath (  )  )     +     \"  . zip \"  )  )  ;", "archiveFile . createNewFile (  )  ;", "ZipOutputStream   out    =    new   ZipOutputStream ( new   FileOutputStream ( archiveFile )  )  ;", "out . putNextEntry ( new   ZipEntry ( p . getName (  )  )  )  ;", "out . write ( bytes )  ;", "out . closeEntry (  )  ;", "out . close (  )  ;", "LocalResource   ret    =     . recordFactory . newRecordInstance ( LocalResource . class )  ;", "ret . setResource ( ConverterUtils . getYarnUrlFromPath ( new   Path (  (  ( p . toString (  )  )     +     \"  . zip \"  )  )  )  )  ;", "ret . setSize ( len )  ;", "ret . setType ( ARCHIVE )  ;", "ret . setVisibility ( vis )  ;", "ret . setTimestamp ( files . getFileStatus ( new   Path (  (  ( p . toString (  )  )     +     \"  . zip \"  )  )  )  . getModificationTime (  )  )  ;", "return   ret ;", "}", "METHOD_END"], "methodName": ["createZipFile"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "FileContext   fs    =    FileContext . getLocalFSFileContext (  )  ;", "fs . delete ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["deleteTestDir"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "FileContext   files    =    FileContext . getLocalFSFileContext ( conf )  ;", "final   Path   basedir    =    files . makeQualified ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  )  ;", "files . mkdir ( basedir ,    null ,    true )  ;", "conf . setStrings (  . class . getName (  )  ,    basedir . toString (  )  )  ;", "Random   rand    =    new   Random (  )  ;", "long   sharedSeed    =    rand . nextLong (  )  ;", "rand . setSeed ( sharedSeed )  ;", "System . out . println (  (  \" SEED :     \"     +    sharedSeed )  )  ;", "Map < LocalResource ,    Future < Path >  >    pending    =    new   HashMap < LocalResource ,    Future < Path >  >  (  )  ;", "ExecutorService   exec    =    Executors . newSingleThreadExecutor (  )  ;", "LocalDirAllocator   dirs    =    new   LocalDirAllocator (  . class . getName (  )  )  ;", "int   size    =     ( rand . nextInt (  5  1  2  )  )     +     5  1  2  ;", "LocalResourceVisibility   vis    =    LocalResourceVisibility . PRIVATE ;", "Path   p    =    new   Path ( basedir ,     (  \"  \"     +     1  )  )  ;", "LocalResource   rsrc    =    null ;", "switch    ( fileType )     {", "case   TAR    :", "rsrc    =     . createTarFile ( files ,    p ,    size ,    rand ,    vis )  ;", "break ;", "case   JAR    :", "rsrc    =     . createJarFile ( files ,    p ,    size ,    rand ,    vis )  ;", "rsrc . setType ( PATTERN )  ;", "break ;", "case   ZIP    :", "rsrc    =     . createZipFile ( files ,    p ,    size ,    rand ,    vis )  ;", "break ;", "case   TGZ    :", "rsrc    =     . createTgzFile ( files ,    p ,    size ,    rand ,    vis )  ;", "break ;", "}", "Path   destPath    =    dirs . getLocalPathForWrite ( basedir . toString (  )  ,    size ,    conf )  ;", "destPath    =    new   Path ( destPath ,    Long . toString (  . uniqueNumberGenerator . incrementAndGet (  )  )  )  ;", "FSDownload   fsd    =    new   FSDownload ( files ,    UserGroupInformation . getCurrentUser (  )  ,    conf ,    destPath ,    rsrc )  ;", "pending . put ( rsrc ,    exec . submit ( fsd )  )  ;", "exec . shutdown (  )  ;", "while    (  !  ( exec . awaitTermination (  1  0  0  0  ,    TimeUnit . MILLISECONDS )  )  )  ;", "Assert . assertTrue ( pending . get ( rsrc )  . isDone (  )  )  ;", "try    {", "FileStatus [  ]    filesstatus    =    files . getDefaultFileSystem (  )  . listStatus ( basedir )  ;", "for    ( FileStatus   filestatus    :    filesstatus )     {", "if    ( filestatus . isDirectory (  )  )     {", "FileStatus [  ]    childFiles    =    files . getDefaultFileSystem (  )  . listStatus ( filestatus . getPath (  )  )  ;", "for    ( FileStatus   childfile    :    childFiles )     {", "if    ( childfile . getPath (  )  . getName (  )  . startsWith (  \" tmp \"  )  )     {", "Assert . fail (  (  \" Tmp   File   should   not   have   been   there    \"     +     ( childfile . getPath (  )  )  )  )  ;", "}", "}", "}", "}", "}    catch    ( Exception   e )     {", "throw   new   IOException (  \" Failed   exec \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["downloadWithFileType"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "FileContext   files    =    FileContext . getLocalFSFileContext ( conf )  ;", "final   Path   basedir    =    files . makeQualified ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  )  ;", "files . mkdir ( basedir ,    null ,    true )  ;", "conf . setStrings (  . class . getName (  )  ,    basedir . toString (  )  )  ;", "Map < LocalResource ,    LocalResourceVisibility >    rsrcVis    =    new   HashMap < LocalResource ,    LocalResourceVisibility >  (  )  ;", "Random   rand    =    new   Random (  )  ;", "long   sharedSeed    =    rand . nextLong (  )  ;", "rand . setSeed ( sharedSeed )  ;", "System . out . println (  (  \" SEED :     \"     +    sharedSeed )  )  ;", "Map < LocalResource ,    Future < Path >  >    pending    =    new   HashMap < LocalResource ,    Future < Path >  >  (  )  ;", "ExecutorService   exec    =    Executors . newSingleThreadExecutor (  )  ;", "LocalDirAllocator   dirs    =    new   LocalDirAllocator (  . class . getName (  )  )  ;", "for    ( int   i    =     0  ;    i    <     5  ;     +  + i )     {", "LocalResourceVisibility   vis    =    LocalResourceVisibility . PRIVATE ;", "if    (  ( i    %     2  )     =  =     1  )     {", "vis    =    LocalResourceVisibility . APPLICATION ;", "}", "Path   p    =    new   Path ( basedir ,     (  (  \" dir \"     +    i )     +     \"  . jar \"  )  )  ;", "LocalResource   rsrc    =     . createJar ( files ,    p ,    vis )  ;", "rsrcVis . put ( rsrc ,    vis )  ;", "Path   destPath    =    dirs . getLocalPathForWrite ( basedir . toString (  )  ,    conf )  ;", "destPath    =    new   Path ( destPath ,    Long . toString (  . uniqueNumberGenerator . incrementAndGet (  )  )  )  ;", "FSDownload   fsd    =    new   FSDownload ( files ,    UserGroupInformation . getCurrentUser (  )  ,    conf ,    destPath ,    rsrc )  ;", "pending . put ( rsrc ,    exec . submit ( fsd )  )  ;", "}", "exec . shutdown (  )  ;", "while    (  !  ( exec . awaitTermination (  1  0  0  0  ,    TimeUnit . MILLISECONDS )  )  )  ;", "for    ( Future < Path >    path    :    pending . values (  )  )     {", "Assert . assertTrue ( path . isDone (  )  )  ;", "}", "try    {", "for    ( Map . Entry < LocalResource ,    Future < Path >  >    p    :    pending . entrySet (  )  )     {", "Path   localized    =    p . getValue (  )  . get (  )  ;", "FileStatus   status    =    files . getFileStatus ( localized )  ;", "System . out . println (  (  \" Testing   path    \"     +    localized )  )  ;", "assert   status . isDirectory (  )  ;", "assert   rsrcVis . containsKey ( p . getKey (  )  )  ;", "verifyPermsRecursively ( localized . getFileSystem ( conf )  ,    files ,    localized ,    rsrcVis . get ( p . getKey (  )  )  )  ;", "}", "}    catch    ( ExecutionException   e )     {", "throw   new   IOException (  \" Failed   exec \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["testDirDownload"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "FileContext   files    =    FileContext . getLocalFSFileContext ( conf )  ;", "final   Path   basedir    =    files . makeQualified ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  )  ;", "files . mkdir ( basedir ,    null ,    true )  ;", "conf . setStrings (  . class . getName (  )  ,    basedir . toString (  )  )  ;", "Map < LocalResource ,    LocalResourceVisibility >    rsrcVis    =    new   HashMap < LocalResource ,    LocalResourceVisibility >  (  )  ;", "Random   rand    =    new   Random (  )  ;", "long   sharedSeed    =    rand . nextLong (  )  ;", "rand . setSeed ( sharedSeed )  ;", "System . out . println (  (  \" SEED :     \"     +    sharedSeed )  )  ;", "Map < LocalResource ,    Future < Path >  >    pending    =    new   HashMap < LocalResource ,    Future < Path >  >  (  )  ;", "ExecutorService   exec    =    Executors . newSingleThreadExecutor (  )  ;", "LocalDirAllocator   dirs    =    new   LocalDirAllocator (  . class . getName (  )  )  ;", "int [  ]    sizes    =    new   int [  1  0  ]  ;", "for    ( int   i    =     0  ;    i    <     1  0  ;     +  + i )     {", "sizes [ i ]     =     ( rand . nextInt (  5  1  2  )  )     +     5  1  2  ;", "LocalResourceVisibility   vis    =    LocalResourceVisibility . PRIVATE ;", "if    (  ( i    %     2  )     =  =     1  )     {", "vis    =    LocalResourceVisibility . APPLICATION ;", "}", "Path   p    =    new   Path ( basedir ,     (  \"  \"     +    i )  )  ;", "LocalResource   rsrc    =     . createFile ( files ,    p ,    sizes [ i ]  ,    rand ,    vis )  ;", "rsrcVis . put ( rsrc ,    vis )  ;", "Path   destPath    =    dirs . getLocalPathForWrite ( basedir . toString (  )  ,    sizes [ i ]  ,    conf )  ;", "destPath    =    new   Path ( destPath ,    Long . toString (  . uniqueNumberGenerator . incrementAndGet (  )  )  )  ;", "FSDownload   fsd    =    new   FSDownload ( files ,    UserGroupInformation . getCurrentUser (  )  ,    conf ,    destPath ,    rsrc )  ;", "pending . put ( rsrc ,    exec . submit ( fsd )  )  ;", "}", "exec . shutdown (  )  ;", "while    (  !  ( exec . awaitTermination (  1  0  0  0  ,    TimeUnit . MILLISECONDS )  )  )  ;", "for    ( Future < Path >    path    :    pending . values (  )  )     {", "Assert . assertTrue ( path . isDone (  )  )  ;", "}", "try    {", "for    ( Map . Entry < LocalResource ,    Future < Path >  >    p    :    pending . entrySet (  )  )     {", "Path   localized    =    p . getValue (  )  . get (  )  ;", "assertEquals ( sizes [ Integer . valueOf ( localized . getName (  )  )  ]  ,    p . getKey (  )  . getSize (  )  )  ;", "FileStatus   status    =    files . getFileStatus ( localized . getParent (  )  )  ;", "FsPermission   perm    =    status . getPermission (  )  ;", "assertEquals (  \" Cache   directory   permissions   are   incorrect \"  ,    new   FsPermission (  (  ( short )     (  4  9  3  )  )  )  ,    perm )  ;", "status    =    files . getFileStatus ( localized )  ;", "perm    =    status . getPermission (  )  ;", "System . out . println (  (  (  (  \" File   permission    \"     +    perm )     +     \"    for   rsrc   vis    \"  )     +     ( p . getKey (  )  . getVisibility (  )  . name (  )  )  )  )  ;", "assert   rsrcVis . containsKey ( p . getKey (  )  )  ;", "Assert . assertTrue (  \" Private   file   should   be    5  0  0  \"  ,     (  ( perm . toShort (  )  )     =  =     ( FSDownload . PRIVATE _ FILE _ PERMS . toShort (  )  )  )  )  ;", "}", "}    catch    ( ExecutionException   e )     {", "throw   new   IOException (  \" Failed   exec \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["testDownload"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "downloadWithFileType ( TestFSDownload . TEST _ FILE _ TYPE . TAR )  ;", "}", "METHOD_END"], "methodName": ["testDownloadArchive"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "downloadWithFileType ( TestFSDownload . TEST _ FILE _ TYPE . TGZ )  ;", "}", "METHOD_END"], "methodName": ["testDownloadArchiveTgz"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "downloadWithFileType ( TestFSDownload . TEST _ FILE _ TYPE . ZIP )  ;", "}", "METHOD_END"], "methodName": ["testDownloadArchiveZip"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . set ( FS _ PERMISSIONS _ UMASK _ KEY ,     \"  0  7  7  \"  )  ;", "FileContext   files    =    FileContext . getLocalFSFileContext ( conf )  ;", "final   Path   basedir    =    files . makeQualified ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  )  ;", "files . mkdir ( basedir ,    null ,    true )  ;", "conf . setStrings (  . class . getName (  )  ,    basedir . toString (  )  )  ;", "Map < LocalResource ,    LocalResourceVisibility >    rsrcVis    =    new   HashMap < LocalResource ,    LocalResourceVisibility >  (  )  ;", "Random   rand    =    new   Random (  )  ;", "long   sharedSeed    =    rand . nextLong (  )  ;", "rand . setSeed ( sharedSeed )  ;", "System . out . println (  (  \" SEED :     \"     +    sharedSeed )  )  ;", "Map < LocalResource ,    Future < Path >  >    pending    =    new   HashMap < LocalResource ,    Future < Path >  >  (  )  ;", "ExecutorService   exec    =    Executors . newSingleThreadExecutor (  )  ;", "LocalDirAllocator   dirs    =    new   LocalDirAllocator (  . class . getName (  )  )  ;", "int   size    =     5  1  2  ;", "LocalResourceVisibility   vis    =    LocalResourceVisibility . PUBLIC ;", "Path   path    =    new   Path ( basedir ,     \" test - file \"  )  ;", "LocalResource   rsrc    =     . createFile ( files ,    path ,    size ,    rand ,    vis )  ;", "rsrcVis . put ( rsrc ,    vis )  ;", "Path   destPath    =    dirs . getLocalPathForWrite ( basedir . toString (  )  ,    size ,    conf )  ;", "destPath    =    new   Path ( destPath ,    Long . toString (  . uniqueNumberGenerator . incrementAndGet (  )  )  )  ;", "FSDownload   fsd    =    new   FSDownload ( files ,    UserGroupInformation . getCurrentUser (  )  ,    conf ,    destPath ,    rsrc )  ;", "pending . put ( rsrc ,    exec . submit ( fsd )  )  ;", "exec . shutdown (  )  ;", "while    (  !  ( exec . awaitTermination (  1  0  0  0  ,    TimeUnit . MILLISECONDS )  )  )  ;", "Assert . assertTrue ( pending . get ( rsrc )  . isDone (  )  )  ;", "try    {", "for    ( Map . Entry < LocalResource ,    Future < Path >  >    p    :    pending . entrySet (  )  )     {", "p . getValue (  )  . get (  )  ;", "Assert . fail (  \" We   localized   a   file   that   is   not   public .  \"  )  ;", "}", "}    catch    ( ExecutionException   e )     {", "Assert . assertTrue (  (  ( e . getCause (  )  )    instanceof   IOException )  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDownloadBadPublic"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "downloadWithFileType ( TestFSDownload . TEST _ FILE _ TYPE . JAR )  ;", "}", "METHOD_END"], "methodName": ["testDownloadPatternJar"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "final   Configuration   conf    =    new   Configuration (  )  ;", "FileContext   files    =    FileContext . getLocalFSFileContext ( conf )  ;", "Path   basedir    =    files . makeQualified ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  )  ;", "FileSystem   f    =    basedir . getFileSystem ( conf )  ;", "assumeTrue ( FSDownload . ancestorsHaveExecutePermissions ( f ,    basedir ,    null )  )  ;", "files . mkdir ( basedir ,    null ,    true )  ;", "conf . setStrings (  . class . getName (  )  ,    basedir . toString (  )  )  ;", "int   size    =     5  1  2  ;", "final   ConcurrentMap < Path ,    AtomicInteger >    counts    =    new   ConcurrentHashMap < Path ,    AtomicInteger >  (  )  ;", "final   CacheLoader < Path ,    Future < FileStatus >  >    loader    =    FSDownload . createStatusCacheLoader ( conf )  ;", "final   LoadingCache < Path ,    Future < FileStatus >  >    statCache    =    CacheBuilder . newBuilder (  )  . build ( new   CacheLoader < Path ,    Future < FileStatus >  >  (  )     {", "public   Future < FileStatus >    load ( Path   path )    throws   Exception    {", "AtomicInteger   count    =    counts . get ( path )  ;", "if    ( count    =  =    null )     {", "count    =    new   AtomicInteger (  0  )  ;", "AtomicInteger   existing    =    counts . putIfAbsent ( path ,    count )  ;", "if    ( existing    !  =    null )     {", "count    =    existing ;", "}", "}", "count . incrementAndGet (  )  ;", "return   loader . load ( path )  ;", "}", "}  )  ;", "final   int   fileCount    =     3  ;", "List < Callable < Boolean >  >    tasks    =    new   ArrayList < Callable < Boolean >  >  (  )  ;", "for    ( int   i    =     0  ;    i    <    fileCount ;    i +  +  )     {", "Random   rand    =    new   Random (  )  ;", "long   sharedSeed    =    rand . nextLong (  )  ;", "rand . setSeed ( sharedSeed )  ;", "System . out . println (  (  \" SEED :     \"     +    sharedSeed )  )  ;", "final   Path   path    =    new   Path ( basedir ,     (  \" test - file -  \"     +    i )  )  ;", ". createFile ( files ,    path ,    size ,    rand )  ;", "final   FileSystem   fs    =    path . getFileSystem ( conf )  ;", "final   FileStatus   sStat    =    fs . getFileStatus ( path )  ;", "tasks . add ( new   Callable < Boolean >  (  )     {", "public   Boolean   call (  )    throws   IOException    {", "return   FSDownload . isPublic ( fs ,    path ,    sStat ,    statCache )  ;", "}", "}  )  ;", "}", "ExecutorService   exec    =    Executors . newFixedThreadPool ( fileCount )  ;", "try    {", "List < Future < Boolean >  >    futures    =    exec . invokeAll ( tasks )  ;", "for    ( Future < Boolean >    future    :    futures )     {", "assertTrue ( future . get (  )  )  ;", "}", "for    ( AtomicInteger   count    :    counts . values (  )  )     {", "assertSame ( count . get (  )  ,     1  )  ;", "}", "}    finally    {", "exec . shutdown (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDownloadPublicWithStatCache"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "FileContext   files    =    FileContext . getLocalFSFileContext ( conf )  ;", "final   Path   basedir    =    files . makeQualified ( new   Path (  \" target \"  ,     . class . getSimpleName (  )  )  )  ;", "files . mkdir ( basedir ,    null ,    true )  ;", "conf . setStrings (  . class . getName (  )  ,    basedir . toString (  )  )  ;", "ExecutorService   singleThreadedExec    =    Executors . newSingleThreadExecutor (  )  ;", "LocalDirAllocator   dirs    =    new   LocalDirAllocator (  . class . getName (  )  )  ;", "Path   destPath    =    dirs . getLocalPathForWrite ( basedir . toString (  )  ,    conf )  ;", "destPath    =    new   Path ( destPath ,    Long . toString (  . uniqueNumberGenerator . incrementAndGet (  )  )  )  ;", "Path   p    =    new   Path ( basedir ,     (  (  \" dir \"     +     0  )     +     \"  . jar \"  )  )  ;", "LocalResourceVisibility   vis    =    LocalResourceVisibility . PRIVATE ;", "LocalResource   rsrc    =     . createJar ( files ,    p ,    vis )  ;", "FSDownload   fsd    =    new   FSDownload ( files ,    UserGroupInformation . getCurrentUser (  )  ,    conf ,    destPath ,    rsrc )  ;", "Future < Path >    rPath    =    singleThreadedExec . submit ( fsd )  ;", "singleThreadedExec . shutdown (  )  ;", "while    (  !  ( singleThreadedExec . awaitTermination (  1  0  0  0  ,    TimeUnit . MILLISECONDS )  )  )  ;", "Assert . assertTrue ( rPath . isDone (  )  )  ;", "Assert . assertEquals ( destPath ,    rPath . get (  )  . getParent (  )  )  ;", "}", "METHOD_END"], "methodName": ["testUniqueDestinationPath"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "FileStatus   status    =    files . getFileStatus ( p )  ;", "if    ( status . isDirectory (  )  )     {", "if    ( vis    =  =     ( LocalResourceVisibility . PUBLIC )  )     {", "Assert . assertTrue (  (  ( status . getPermission (  )  . toShort (  )  )     =  =     (  . PUBLIC _ DIR _ PERMS . toShort (  )  )  )  )  ;", "} else    {", "Assert . assertTrue (  (  ( status . getPermission (  )  . toShort (  )  )     =  =     (  . PRIVATE _ DIR _ PERMS . toShort (  )  )  )  )  ;", "}", "if    (  !  ( status . isSymlink (  )  )  )     {", "FileStatus [  ]    statuses    =    fs . listStatus ( p )  ;", "for    ( FileStatus   stat    :    statuses )     {", "verifyPermsRecursively ( fs ,    files ,    stat . getPath (  )  ,    vis )  ;", "}", "}", "} else    {", "if    ( vis    =  =     ( LocalResourceVisibility . PUBLIC )  )     {", "Assert . assertTrue (  (  ( status . getPermission (  )  . toShort (  )  )     =  =     (  . PUBLIC _ FILE _ PERMS . toShort (  )  )  )  )  ;", "} else    {", "Assert . assertTrue (  (  ( status . getPermission (  )  . toShort (  )  )     =  =     (  . PRIVATE _ FILE _ PERMS . toShort (  )  )  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["verifyPermsRecursively"], "fileName": "org.apache.hadoop.yarn.util.TestFSDownload"}, {"methodBody": ["METHOD_START", "{", "long   memTotal    =     4  0  5  8  8  6  4 L ;", "long   memFree    =     9  9  6  3  2 L ;", "long   inactive    =     5  6  7  7  3  2 L ;", "long   swapTotal    =     2  0  9  6  4  7  2 L ;", "long   swapFree    =     1  8  1  8  4  8  0 L ;", "File   tempFile    =    new   File (  . FAKE _ MEMFILE )  ;", "tempFile . deleteOnExit (  )  ;", "FileWriter   fWriter    =    new   FileWriter (  . FAKE _ MEMFILE )  ;", "fWriter . write ( String . format (  . MEMINFO _ FORMAT ,    memTotal ,    memFree ,    inactive ,    swapTotal ,    swapFree )  )  ;", "fWriter . close (  )  ;", "assertEquals (  . plugin . getAvailablePhysicalMemorySize (  )  ,     (  1  0  2  4 L    *     ( memFree    +    inactive )  )  )  ;", "assertEquals (  . plugin . getAvailableVirtualMemorySize (  )  ,     (  1  0  2  4 L    *     (  ( memFree    +    inactive )     +    swapFree )  )  )  ;", "assertEquals (  . plugin . getPhysicalMemorySize (  )  ,     (  1  0  2  4 L    *    memTotal )  )  ;", "assertEquals (  . plugin . getVirtualMemorySize (  )  ,     (  1  0  2  4 L    *     ( memTotal    +    swapTotal )  )  )  ;", "}", "METHOD_END"], "methodName": ["parsingProcMemFile"], "fileName": "org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "long   numProcessors    =     8  ;", "long   cpuFrequencyKHz    =     2  3  9  2  7  8  1  ;", "String   fileContent    =     \"  \"  ;", "for    ( int   i    =     0  ;    i    <    numProcessors ;    i +  +  )     {", "fileContent    +  =     ( String . format (  . CPUINFO _ FORMAT ,    i ,     ( cpuFrequencyKHz    /     1  0  0  0  .  0  )  )  )     +     \"  \\ n \"  ;", "}", "File   tempFile    =    new   File (  . FAKE _ CPUFILE )  ;", "tempFile . deleteOnExit (  )  ;", "FileWriter   fWriter    =    new   FileWriter (  . FAKE _ CPUFILE )  ;", "fWriter . write ( fileContent )  ;", "fWriter . close (  )  ;", "assertEquals (  . plugin . getNumProcessors (  )  ,    numProcessors )  ;", "assertEquals (  . plugin . getCpuFrequency (  )  ,    cpuFrequencyKHz )  ;", "long   uTime    =     5  4  9  7  2  9  9  4  ;", "long   nTime    =     1  8  8  8  6  0  ;", "long   sTime    =     1  9  8  0  3  3  7  3  ;", "tempFile    =    new   File (  . FAKE _ STATFILE )  ;", "tempFile . deleteOnExit (  )  ;", "updateStatFile ( uTime ,    nTime ,    sTime )  ;", "assertEquals (  . plugin . getCumulativeCpuTime (  )  ,     (  (  . FAKE _ JIFFY _ LENGTH )     *     (  ( uTime    +    nTime )     +    sTime )  )  )  ;", "assertEquals (  . plugin . getCpuUsage (  )  ,     (  ( float )     ( LinuxResourceCalculatorPlugin . UNAVAILABLE )  )  ,     0  .  0  )  ;", "uTime    +  =     1  0  0 L ;", ". plugin . advanceTime (  2  0  0 L )  ;", "updateStatFile ( uTime ,    nTime ,    sTime )  ;", "assertEquals (  . plugin . getCumulativeCpuTime (  )  ,     (  (  . FAKE _ JIFFY _ LENGTH )     *     (  ( uTime    +    nTime )     +    sTime )  )  )  ;", "assertEquals (  . plugin . getCpuUsage (  )  ,     6  .  2  5 F ,     0  .  0  )  ;", "uTime    +  =     6  0  0 L ;", ". plugin . advanceTime (  3  0  0 L )  ;", "updateStatFile ( uTime ,    nTime ,    sTime )  ;", "assertEquals (  . plugin . getCpuUsage (  )  ,     2  5  .  0 F ,     0  .  0  )  ;", "uTime    +  =     1 L ;", ". plugin . advanceTime (  1 L )  ;", "updateStatFile ( uTime ,    nTime ,    sTime )  ;", "assertEquals (  . plugin . getCumulativeCpuTime (  )  ,     (  (  . FAKE _ JIFFY _ LENGTH )     *     (  ( uTime    +    nTime )     +    sTime )  )  )  ;", "assertEquals (  . plugin . getCpuUsage (  )  ,     2  5  .  0 F ,     0  .  0  )  ;", "}", "METHOD_END"], "methodName": ["parsingProcStatAndCpuFile"], "fileName": "org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "FileWriter   fWriter    =    new   FileWriter ( TestLinuxResourceCalculatorPlugin . FAKE _ STATFILE )  ;", "fWriter . write ( String . format ( TestLinuxResourceCalculatorPlugin . STAT _ FILE _ FORMAT ,    uTime ,    nTime ,    sTime )  )  ;", "fWriter . close (  )  ;", "}", "METHOD_END"], "methodName": ["updateStatFile"], "fileName": "org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "ProcfsBasedProcessTree . ProcessSmapMemoryInfo   info    =    new   ProcfsBasedProcessTree . ProcessSmapMemoryInfo ( address )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . SIZE . name (  )  ,    entries [  0  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . RSS . name (  )  ,    entries [  1  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . PSS . name (  )  ,    entries [  2  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . SHARED _ CLEAN . name (  )  ,    entries [  3  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . SHARED _ DIRTY . name (  )  ,    entries [  4  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . PRIVATE _ CLEAN . name (  )  ,    entries [  5  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . PRIVATE _ DIRTY . name (  )  ,    entries [  6  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . REFERENCED . name (  )  ,    entries [  7  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . ANONYMOUS . name (  )  ,    entries [  8  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . ANON _ HUGE _ PAGES . name (  )  ,    entries [  9  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . SWAP . name (  )  ,    entries [  1  0  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . KERNEL _ PAGE _ SIZE . name (  )  ,    entries [  1  1  ]  )  ;", "info . setMemInfo ( ProcfsBasedProcessTree . MemInfo . MMU _ PAGE _ SIZE . name (  )  ,    entries [  1  2  ]  )  ;", "return   info ;", "}", "METHOD_END"], "methodName": ["constructMemoryMappingInfo"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( procMemInfo . length )  ;    i +  +  )     {", "List <  . ProcessSmapMemoryInfo >    memoryMappingList    =    procMemInfo [ i ]  . getMemoryInfoList (  )  ;", "memoryMappingList . add ( constructMemoryMappingInfo (  (  \"  7 f 5  6 c 1  7  7 c 0  0  0  -  7 f 5  6 c 1  7  7 d 0  0  0     \"     +     (  \" rw - p    0  0  0  1  0  0  0  0     0  8  :  0  2     4  0  3  7  1  5  5  8                                                           \"     +     \"  / grid /  0  / jdk 1  .  7  .  0  _  2  5  / jre / lib / amd 6  4  / libnio . so \"  )  )  ,    new   String [  ]  {     \"  4  \"  ,     \"  4  \"  ,     \"  2  5  \"  ,     \"  4  \"  ,     \"  2  5  \"  ,     \"  1  5  \"  ,     \"  1  0  \"  ,     \"  4  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  4  \"  ,     \"  4  \"     }  )  )  ;", "memoryMappingList . add ( constructMemoryMappingInfo (  (  \"  7 fb 0  9  3  8  2 e 0  0  0  -  7 fb 0  9  3  8  2 f 0  0  0    r -  - s    0  0  0  0  3  0  0  0     \"     +     \"  0  8  :  0  2     2  5  9  5  3  5  4  5  \"  )  ,    new   String [  ]  {     \"  4  \"  ,     \"  4  \"  ,     \"  2  5  \"  ,     \"  4  \"  ,     \"  0  \"  ,     \"  1  5  \"  ,     \"  1  0  \"  ,     \"  4  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  4  \"  ,     \"  4  \"     }  )  )  ;", "memoryMappingList . add ( constructMemoryMappingInfo (  \"  7 e 8  7  9  0  0  0  0  -  7 e 8 b 8  0  0  0  0    r - xs    0  0  0  0  0  0  0  0     0  0  :  0  0     0  \"  ,    new   String [  ]  {     \"  4  \"  ,     \"  4  \"  ,     \"  2  5  \"  ,     \"  4  \"  ,     \"  0  \"  ,     \"  1  5  \"  ,     \"  1  0  \"  ,     \"  4  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  4  \"  ,     \"  4  \"     }  )  )  ;", "memoryMappingList . add ( constructMemoryMappingInfo (  \"  7 da 6  7  7  0  0  0  -  7 e 0 dcf 0  0  0    rw - p    0  0  0  0  0  0  0  0     0  0  :  0  0     0  \"  ,    new   String [  ]  {     \"  4  \"  ,     \"  4  \"  ,     \"  2  5  \"  ,     \"  4  \"  ,     \"  5  0  \"  ,     \"  1  5  \"  ,     \"  1  0  \"  ,     \"  4  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  0  \"  ,     \"  4  \"  ,     \"  4  \"     }  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["createMemoryMappingInfo"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "return   new   ProcfsBasedProcessTree ( pid )  ;", "}", "METHOD_END"], "methodName": ["createProcessTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "return   new   ProcfsBasedProcessTree ( pid ,    procfsRootDir )  ;", "}", "METHOD_END"], "methodName": ["createProcessTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "TestProcfsBasedProcessTree . sendSignal ( pid ,     9  )  ;", "}", "METHOD_END"], "methodName": ["destroyProcessTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "BufferedReader   pidFile    =    null ;", "FileReader   fReader    =    null ;", "String   pid    =    null ;", "try    {", "fReader    =    new   FileReader ( pidFileName )  ;", "pidFile    =    new   BufferedReader ( fReader )  ;", "}    catch    ( FileNotFoundException   f )     {", ". LOG . debug (  (  \" PidFile   doesn ' t   exist    :     \"     +    pidFileName )  )  ;", "return   pid ;", "}", "try    {", "pid    =    pidFile . readLine (  )  ;", "}    catch    ( IOException   i )     {", ". LOG . error (  (  \" Failed   to   read   from    \"     +    pidFileName )  )  ;", "}    finally    {", "try    {", "if    ( fReader    !  =    null )     {", "fReader . close (  )  ;", "}", "try    {", "if    ( pidFile    !  =    null )     {", "pidFile . close (  )  ;", "}", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    pidFile )  )  ;", "}", "}    catch    ( IOException   i )     {", ". LOG . warn (  (  \" Error   closing   the   stream    \"     +    fReader )  )  ;", "}", "}", "return   pid ;", "}", "METHOD_END"], "methodName": ["getPidFromPidFile"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "File   f    =    new   File ( pidFile )  ;", "while    (  !  ( f . exists (  )  )  )     {", "try    {", "Thread . sleep (  5  0  0  )  ;", "}    catch    ( InterruptedException   ie )     {", "break ;", "}", "}", "return    . getPidFromPidFile ( pidFile )  ;", "}", "METHOD_END"], "methodName": ["getRogueTaskPID"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "try    {", "final   String   sigpid    =     (  . isSetsidAvailable (  )  )     ?     \"  -  \"     +    pid    :    pid ;", "try    {", ". sendSignal ( sigpid ,     0  )  ;", "}    catch    ( ExitCodeException   e )     {", "return   false ;", "}", "return   true ;", "}    catch    ( IOException   ignored )     {", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isAlive"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "for    ( String   pId    :    processTree . getCurrentProcessIDs (  )  )     {", "if    (  . isAlive ( pId )  )     {", "return   true ;", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isAnyProcessInTreeAlive"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "ShellCommandExecutor   shexec    =    null ;", "boolean   setsidSupported    =    true ;", "try    {", "String [  ]    args    =    new   String [  ]  {     \" setsid \"  ,     \" bash \"  ,     \"  - c \"  ,     \" echo    $  $  \"     }  ;", "shexec    =    new   ShellCommandExecutor ( args )  ;", "shexec . execute (  )  ;", "}    catch    ( IOException   ioe )     {", ". LOG . warn (  \" setsid   is   not   available   on   this   machine .    So   not   using   it .  \"  )  ;", "setsidSupported    =    false ;", "}    finally    {", ". LOG . info (  (  \" setsid   exited   with   exit   code    \"     +     ( shexec . getExitCode (  )  )  )  )  ;", "}", "return   setsidSupported ;", "}", "METHOD_END"], "methodName": ["isSetsidAvailable"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "ShellCommandExecutor   shexec    =    null ;", "String [  ]    arg    =    new   String [  ]  {     \" kill \"  ,     \"  -  \"     +    signal ,    pid    }  ;", "shexec    =    new   ShellCommandExecutor ( arg )  ;", "shexec . execute (  )  ;", "}", "METHOD_END"], "methodName": ["sendSignal"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    processTree . getConf (  )  ;", "if    ( conf    =  =    null )     {", "conf    =    new   Configuration (  )  ;", "}", "conf . setBoolean ( PROCFS _ USE _ SMAPS _ BASED _ RSS _ ENABLED ,    enableFlag )  ;", "processTree . setConf ( conf )  ;", "processTree . update (  )  ;", "}", "METHOD_END"], "methodName": ["setSmapsInProceTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "assumeTrue ( LINUX )  ;", "FileContext . getLocalFSFileContext (  )  . delete ( new   Path (  . TEST _ ROOT _ DIR . getAbsolutePath (  )  )  ,    true )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "for    ( String   pid    :    pids )     {", "File   pidDir    =    new   File ( procfsRootDir ,    pid )  ;", "pidDir . mkdir (  )  ;", "if    (  !  ( pidDir . exists (  )  )  )     {", "throw   new   IOException (  (  \" couldn ' t   make   process   directory   under    \"     +     \" fake   procfs \"  )  )  ;", "} else    {", ". LOG . info (  \" created   pid   dir \"  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setupPidDirs"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "if    ( procfsRootDir . exists (  )  )     {", "Assert . assertTrue ( FileUtil . fullyDelete ( procfsRootDir )  )  ;", "}", "Assert . assertTrue ( procfsRootDir . mkdirs (  )  )  ;", "}", "METHOD_END"], "methodName": ["setupProcfsRootDir"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "String [  ]    pids    =    new   String [  ]  {     \"  1  0  0  \"  ,     \"  2  0  0  \"  ,     \"  3  0  0  \"  ,     \"  4  0  0  \"     }  ;", "File   procfsRootDir    =    new   File (  . TEST _ ROOT _ DIR ,     \" proc \"  )  ;", "try    {", ". setupProcfsRootDir ( procfsRootDir )  ;", ". setupPidDirs ( procfsRootDir ,    pids )  ;", ". ProcessStatInfo [  ]    procInfos    =    new    . ProcessStatInfo [  4  ]  ;", "procInfos [  0  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  1  0  0  \"  ,     \" proc 1  \"  ,     \"  1  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  \"  ,     \"  2  0  0  \"     }  )  ;", "procInfos [  1  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  2  0  0  \"  ,     \" proc 2  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  2  0  0  0  0  0  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  0  \"  ,     \"  4  0  0  \"     }  )  ;", "procInfos [  2  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  3  0  0  \"  ,     \" proc 3  \"  ,     \"  2  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  3  0  0  0  0  0  \"  ,     \"  3  0  0  \"  ,     \"  3  0  0  0  \"  ,     \"  6  0  0  \"     }  )  ;", "procInfos [  3  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  4  0  0  \"  ,     \" proc 4  \"  ,     \"  1  \"  ,     \"  4  0  0  \"  ,     \"  4  0  0  \"  ,     \"  4  0  0  0  0  0  \"  ,     \"  4  0  0  \"  ,     \"  4  0  0  0  \"  ,     \"  8  0  0  \"     }  )  ;", "ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  ]    memInfo    =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  4  ]  ;", "memInfo [  0  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  1  0  0  \"  )  ;", "memInfo [  1  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  2  0  0  \"  )  ;", "memInfo [  2  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  3  0  0  \"  )  ;", "memInfo [  3  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  4  0  0  \"  )  ;", "createMemoryMappingInfo ( memInfo )  ;", ". writeStatFiles ( procfsRootDir ,    pids ,    procInfos ,    memInfo )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "ProcfsBasedProcessTree   processTree    =    createProcessTree (  \"  1  0  0  \"  ,    procfsRootDir . getAbsolutePath (  )  )  ;", "processTree . setConf ( conf )  ;", "processTree . updateProcessTree (  )  ;", "Assert . assertEquals (  \" Cumulative   virtual   memory   does   not   match \"  ,     6  0  0  0  0  0 L ,    processTree . getCumulativeVmem (  )  )  ;", "long   cumuRssMem    =     (  ( ProcfsBasedProcessTree . PAGE _ SIZE )     >     0  )     ?     6  0  0 L    *     ( ProcfsBasedProcessTree . PAGE _ SIZE )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   rss   memory   does   not   match \"  ,    cumuRssMem ,    processTree . getCumulativeRssmem (  )  )  ;", "long   cumuCpuTime    =     (  ( ProcfsBasedProcessTree . JIFFY _ LENGTH _ IN _ MILLIS )     >     0  )     ?     7  2  0  0 L    *     ( ProcfsBasedProcessTree . JIFFY _ LENGTH _ IN _ MILLIS )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   cpu   time   does   not   match \"  ,    cumuCpuTime ,    processTree . getCumulativeCpuTime (  )  )  ;", "setSmapsInProceTree ( processTree ,    true )  ;", "Assert . assertEquals (  \" Cumulative   rss   memory   does   not   match \"  ,     (  (  1  0  0     *     ( ProcfsBasedProcessTree . KB _ TO _ BYTES )  )     *     3  )  ,    processTree . getCumulativeRssmem (  )  )  ;", "procInfos [  0  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  1  0  0  \"  ,     \" proc 1  \"  ,     \"  1  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  0  0  \"  ,     \"  1  0  0  \"  ,     \"  2  0  0  0  \"  ,     \"  3  0  0  \"     }  )  ;", "procInfos [  1  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  2  0  0  \"  ,     \" proc 2  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  2  0  0  0  0  0  \"  ,     \"  2  0  0  \"  ,     \"  3  0  0  0  \"  ,     \"  5  0  0  \"     }  )  ;", ". writeStatFiles ( procfsRootDir ,    pids ,    procInfos ,    memInfo )  ;", "processTree . updateProcessTree (  )  ;", "cumuCpuTime    =     (  ( ProcfsBasedProcessTree . JIFFY _ LENGTH _ IN _ MILLIS )     >     0  )     ?     9  4  0  0 L    *     ( ProcfsBasedProcessTree . JIFFY _ LENGTH _ IN _ MILLIS )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   cpu   time   does   not   match \"  ,    cumuCpuTime ,    processTree . getCumulativeCpuTime (  )  )  ;", "}    finally    {", "FileUtil . fullyDelete ( procfsRootDir )  ;", "}", "}", "METHOD_END"], "methodName": ["testCpuAndMemoryForProcessTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "String   pid    =     \"  1  0  0  \"  ;", "File   procfsRootDir    =    new   File (  . TEST _ ROOT _ DIR ,     \" proc \"  )  ;", "try    {", ". setupProcfsRootDir ( procfsRootDir )  ;", "createProcessTree ( pid ,    procfsRootDir . getAbsolutePath (  )  )  ;", "Assert . assertTrue ( ProcfsBasedProcessTree . checkPidPgrpidForMatch ( pid ,    procfsRootDir . getAbsolutePath (  )  )  )  ;", "}    finally    {", "FileUtil . fullyDelete ( procfsRootDir )  ;", "}", "}", "METHOD_END"], "methodName": ["testDestroyProcessTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "testMemForOlderProcesses ( false )  ;", "testMemForOlderProcesses ( true )  ;", "}", "METHOD_END"], "methodName": ["testMemForOlderProcesses"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "String [  ]    pids    =    new   String [  ]  {     \"  1  0  0  \"  ,     \"  2  0  0  \"  ,     \"  3  0  0  \"  ,     \"  4  0  0  \"     }  ;", "File   procfsRootDir    =    new   File (  . TEST _ ROOT _ DIR ,     \" proc \"  )  ;", "try    {", ". setupProcfsRootDir ( procfsRootDir )  ;", ". setupPidDirs ( procfsRootDir ,    pids )  ;", ". ProcessStatInfo [  ]    procInfos    =    new    . ProcessStatInfo [  4  ]  ;", "procInfos [  0  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  1  0  0  \"  ,     \" proc 1  \"  ,     \"  1  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  0  0  \"  ,     \"  1  0  0  \"     }  )  ;", "procInfos [  1  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  2  0  0  \"  ,     \" proc 2  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  2  0  0  0  0  0  \"  ,     \"  2  0  0  \"     }  )  ;", "procInfos [  2  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  3  0  0  \"  ,     \" proc 3  \"  ,     \"  1  \"  ,     \"  3  0  0  \"  ,     \"  3  0  0  \"  ,     \"  3  0  0  0  0  0  \"  ,     \"  3  0  0  \"     }  )  ;", "procInfos [  3  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  4  0  0  \"  ,     \" proc 4  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  4  0  0  0  0  0  \"  ,     \"  4  0  0  \"     }  )  ;", "ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  ]    memInfo    =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  4  ]  ;", "memInfo [  0  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  1  0  0  \"  )  ;", "memInfo [  1  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  2  0  0  \"  )  ;", "memInfo [  2  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  3  0  0  \"  )  ;", "memInfo [  3  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  4  0  0  \"  )  ;", "createMemoryMappingInfo ( memInfo )  ;", ". writeStatFiles ( procfsRootDir ,    pids ,    procInfos ,    memInfo )  ;", "ProcfsBasedProcessTree   processTree    =    createProcessTree (  \"  1  0  0  \"  ,    procfsRootDir . getAbsolutePath (  )  )  ;", "setSmapsInProceTree ( processTree ,    smapEnabled )  ;", "Assert . assertEquals (  \" Cumulative   memory   does   not   match \"  ,     7  0  0  0  0  0 L ,    processTree . getCumulativeVmem (  )  )  ;", "String [  ]    newPids    =    new   String [  ]  {     \"  5  0  0  \"     }  ;", ". setupPidDirs ( procfsRootDir ,    newPids )  ;", ". ProcessStatInfo [  ]    newProcInfos    =    new    . ProcessStatInfo [  1  ]  ;", "newProcInfos [  0  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  5  0  0  \"  ,     \" proc 5  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  5  0  0  0  0  0  \"  ,     \"  5  0  0  \"     }  )  ;", "ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  ]    newMemInfos    =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  1  ]  ;", "newMemInfos [  0  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  5  0  0  \"  )  ;", "createMemoryMappingInfo ( newMemInfos )  ;", ". writeStatFiles ( procfsRootDir ,    newPids ,    newProcInfos ,    newMemInfos )  ;", "processTree . updateProcessTree (  )  ;", "Assert . assertEquals (  \" Cumulative   vmem   does   not   include   new   process \"  ,     1  2  0  0  0  0  0 L ,    processTree . getCumulativeVmem (  )  )  ;", "if    (  ! smapEnabled )     {", "long   cumuRssMem    =     (  ( ProcfsBasedProcessTree . PAGE _ SIZE )     >     0  )     ?     1  2  0  0 L    *     ( ProcfsBasedProcessTree . PAGE _ SIZE )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   rssmem   does   not   include   new   process \"  ,    cumuRssMem ,    processTree . getCumulativeRssmem (  )  )  ;", "} else    {", "Assert . assertEquals (  \" Cumulative   rssmem   does   not   include   new   process \"  ,     (  (  1  0  0     *     ( ProcfsBasedProcessTree . KB _ TO _ BYTES )  )     *     4  )  ,    processTree . getCumulativeRssmem (  )  )  ;", "}", "Assert . assertEquals (  \" Cumulative   vmem   shouldn ' t   have   included   new   process \"  ,     7  0  0  0  0  0 L ,    processTree . getCumulativeVmem (  1  )  )  ;", "if    (  ! smapEnabled )     {", "long   cumuRssMem    =     (  ( ProcfsBasedProcessTree . PAGE _ SIZE )     >     0  )     ?     7  0  0 L    *     ( ProcfsBasedProcessTree . PAGE _ SIZE )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   rssmem   shouldn ' t   have   included   new   process \"  ,    cumuRssMem ,    processTree . getCumulativeRssmem (  1  )  )  ;", "} else    {", "Assert . assertEquals (  \" Cumulative   rssmem   shouldn ' t   have   included   new   process \"  ,     (  (  1  0  0     *     ( ProcfsBasedProcessTree . KB _ TO _ BYTES )  )     *     3  )  ,    processTree . getCumulativeRssmem (  1  )  )  ;", "}", "newPids    =    new   String [  ]  {     \"  6  0  0  \"     }  ;", ". setupPidDirs ( procfsRootDir ,    newPids )  ;", "newProcInfos    =    new    . ProcessStatInfo [  1  ]  ;", "newProcInfos [  0  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  6  0  0  \"  ,     \" proc 6  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  6  0  0  0  0  0  \"  ,     \"  6  0  0  \"     }  )  ;", "newMemInfos    =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  1  ]  ;", "newMemInfos [  0  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  6  0  0  \"  )  ;", "createMemoryMappingInfo ( newMemInfos )  ;", ". writeStatFiles ( procfsRootDir ,    newPids ,    newProcInfos ,    newMemInfos )  ;", "processTree . updateProcessTree (  )  ;", "Assert . assertEquals (  \" Cumulative   vmem   shouldn ' t   have   included   new   processes \"  ,     7  0  0  0  0  0 L ,    processTree . getCumulativeVmem (  2  )  )  ;", "if    (  ! smapEnabled )     {", "long   cumuRssMem    =     (  ( ProcfsBasedProcessTree . PAGE _ SIZE )     >     0  )     ?     7  0  0 L    *     ( ProcfsBasedProcessTree . PAGE _ SIZE )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   rssmem   shouldn ' t   have   included   new   processes \"  ,    cumuRssMem ,    processTree . getCumulativeRssmem (  2  )  )  ;", "} else    {", "Assert . assertEquals (  \" Cumulative   rssmem   shouldn ' t   have   included   new   processes \"  ,     (  (  1  0  0     *     ( ProcfsBasedProcessTree . KB _ TO _ BYTES )  )     *     3  )  ,    processTree . getCumulativeRssmem (  2  )  )  ;", "}", "Assert . assertEquals (  \" Cumulative   vmem   shouldn ' t   have   included   new   processes \"  ,     1  2  0  0  0  0  0 L ,    processTree . getCumulativeVmem (  1  )  )  ;", "if    (  ! smapEnabled )     {", "long   cumuRssMem    =     (  ( ProcfsBasedProcessTree . PAGE _ SIZE )     >     0  )     ?     1  2  0  0 L    *     ( ProcfsBasedProcessTree . PAGE _ SIZE )     :     0 L ;", "Assert . assertEquals (  \" Cumulative   rssmem   shouldn ' t   have   included   new   processes \"  ,    cumuRssMem ,    processTree . getCumulativeRssmem (  1  )  )  ;", "} else    {", "Assert . assertEquals (  \" Cumulative   rssmem   shouldn ' t   have   included   new   processes \"  ,     (  (  1  0  0     *     ( ProcfsBasedProcessTree . KB _ TO _ BYTES )  )     *     4  )  ,    processTree . getCumulativeRssmem (  1  )  )  ;", "}", "Assert . assertEquals (  \" Getting   non - zero   vmem   for   processes   older   than    3    iterations \"  ,     0 L ,    processTree . getCumulativeVmem (  3  )  )  ;", "Assert . assertEquals (  \" Getting   non - zero   rssmem   for   processes   older   than    3    iterations \"  ,     0 L ,    processTree . getCumulativeRssmem (  3  )  )  ;", "Assert . assertEquals (  \" Getting   non - zero   rssmem   for   processes   older   than    3    iterations \"  ,     0 L ,    processTree . getCumulativeRssmem (  3  )  )  ;", "}    finally    {", "FileUtil . fullyDelete ( procfsRootDir )  ;", "}", "}", "METHOD_END"], "methodName": ["testMemForOlderProcesses"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "try    {", "Assert . assertTrue ( ProcfsBasedProcessTree . isAvailable (  )  )  ;", "}    catch    ( Exception   e )     {", ". LOG . info ( StringUtils . stringifyException ( e )  )  ;", "Assert . assertTrue (  \" ProcfsBaseProcessTree   should   be   available   on   Linux \"  ,    false )  ;", "return ;", "}", "Random   rm    =    new   Random (  )  ;", "File   tempFile    =    new   File (  . TEST _ ROOT _ DIR ,     (  (  (  ( getClass (  )  . getName (  )  )     +     \"  _ shellScript _  \"  )     +     ( rm . nextInt (  )  )  )     +     \"  . sh \"  )  )  ;", "tempFile . deleteOnExit (  )  ;", "shellScript    =     (  (  . TEST _ ROOT _ DIR )     +     ( File . separator )  )     +     ( tempFile . getName (  )  )  ;", "tempFile    =    new   File (  . TEST _ ROOT _ DIR ,     (  (  (  ( getClass (  )  . getName (  )  )     +     \"  _ pidFile _  \"  )     +     ( rm . nextInt (  )  )  )     +     \"  . pid \"  )  )  ;", "tempFile . deleteOnExit (  )  ;", "pidFile    =     (  (  . TEST _ ROOT _ DIR )     +     ( File . separator )  )     +     ( tempFile . getName (  )  )  ;", "lowestDescendant    =     (  (  . TEST _ ROOT _ DIR )     +     ( File . separator )  )     +     \" lowestDescendantPidFile \"  ;", "try    {", "FileWriter   fWriter    =    new   FileWriter ( shellScript )  ;", "fWriter . write (  (  (  (  (  (  (  (  (  (  (  (  \"  #    rogue   task \\ n \"     +     (  (  (  (  \" sleep    1  \\ n \"     +     \" echo   hello \\ n \"  )     +     \" if    [     $  1     - ne    0     ]  \\ n \"  )     +     \" then \\ n \"  )     +     \"    sh    \"  )  )     +     ( shellScript )  )     +     \"     $  (  (  $  1  -  1  )  )  \\ n \"  )     +     \" else \\ n \"  )     +     \"    echo    $  $     >     \"  )     +     ( lowestDescendant )  )     +     \"  \\ n \"  )     +     \"    while   true \\ n   do \\ n \"  )     +     \"       sleep    5  \\ n \"  )     +     \"    done \\ n \"  )     +     \" fi \"  )  )  ;", "fWriter . close (  )  ;", "}    catch    ( IOException   ioe )     {", ". LOG . info (  (  \" Error :     \"     +    ioe )  )  ;", "return ;", "}", "Thread   t    =    new    . RogueTaskThread (  )  ;", "t . start (  )  ;", "String   pid    =    getRogueTaskPID (  )  ;", ". LOG . info (  (  \" Root   process   pid :     \"     +    pid )  )  ;", "ProcfsBasedProcessTree   p    =    createProcessTree ( pid )  ;", "p . updateProcessTree (  )  ;", ". LOG . info (  (  \" ProcessTree :     \"     +     ( p . toString (  )  )  )  )  ;", "File   leaf    =    new   File ( lowestDescendant )  ;", "while    (  !  ( leaf . exists (  )  )  )     {", "try    {", "Thread . sleep (  5  0  0  )  ;", "}    catch    ( InterruptedException   ie )     {", "break ;", "}", "}", "p . updateProcessTree (  )  ;", ". LOG . info (  (  \" ProcessTree :     \"     +     ( p . toString (  )  )  )  )  ;", "String   processTreeDump    =    p . getProcessTreeDump (  )  ;", "destroyProcessTree ( pid )  ;", "boolean   isAlive    =    true ;", "for    ( int   tries    =     1  0  0  ;    tries    >     0  ;    tries -  -  )     {", "if    (  . isSetsidAvailable (  )  )     {", "isAlive    =     . isAnyProcessInTreeAlive ( p )  ;", "} else    {", "isAlive    =     . isAlive ( pid )  ;", "}", "if    (  ! isAlive )     {", "break ;", "}", "Thread . sleep (  1  0  0  )  ;", "}", "if    ( isAlive )     {", "fail (  \" ProcessTree   shouldn ' t   be   alive \"  )  ;", "}", ". LOG . info (  (  \" Process - tree   dump   follows :     \\ n \"     +    processTreeDump )  )  ;", "Assert . assertTrue (  \" Process - tree   dump   doesn ' t   start   with   a   proper   header \"  ,    processTreeDump . startsWith (  (  \"  \\ t |  -    PID   PPID   PGRPID   SESSID   CMD _ NAME    \"     +     (  \" USER _ MODE _ TIME ( MILLIS )    SYSTEM _ TIME ( MILLIS )    VMEM _ USAGE ( BYTES )     \"     +     \" RSSMEM _ USAGE ( PAGES )    FULL _ CMD _ LINE \\ n \"  )  )  )  )  ;", "for    ( int   i    =     . N ;    i    >  =     0  ;    i -  -  )     {", "String   cmdLineDump    =     (  (  (  \"  \\  \\  |  -     [  0  -  9  ]  +     [  0  -  9  ]  +     [  0  -  9  ]  +     [  0  -  9  ]  +     \\  \\  ( sh \\  \\  )  \"     +     \"     [  0  -  9  ]  +     [  0  -  9  ]  +     [  0  -  9  ]  +     [  0  -  9  ]  +    sh    \"  )     +     ( shellScript )  )     +     \"     \"  )     +    i ;", "Pattern   pat    =    Pattern . compile ( cmdLineDump )  ;", "Matcher   mat    =    pat . matcher ( processTreeDump )  ;", "Assert . assertTrue (  (  (  \" Process - tree   dump   doesn ' t   contain   the   cmdLineDump   of    \"     +    i )     +     \" th   process !  \"  )  ,    mat . find (  )  )  ;", "}", "try    {", "t . join (  2  0  0  0  )  ;", ". LOG . info (  \" RogueTaskThread   successfully   joined .  \"  )  ;", "}    catch    ( InterruptedException   ie )     {", ". LOG . info (  \" Interrupted   while   joining   RogueTaskThread .  \"  )  ;", "}", "p . updateProcessTree (  )  ;", "Assert . assertFalse (  \" ProcessTree   must   have   been   gone \"  ,     . isAlive ( pid )  )  ;", "Assert . assertTrue (  (  (  \" Cumulative   vmem   for   the   gone - process   is    \"     +     ( p . getCumulativeVmem (  )  )  )     +     \"     .    It   should   be   zero .  \"  )  ,     (  ( p . getCumulativeVmem (  )  )     =  =     0  )  )  ;", "Assert . assertTrue ( p . toString (  )  . equals (  \"  [     ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testProcessTree"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "String [  ]    pids    =    new   String [  ]  {     \"  1  0  0  \"  ,     \"  2  0  0  \"  ,     \"  3  0  0  \"  ,     \"  4  0  0  \"  ,     \"  5  0  0  \"  ,     \"  6  0  0  \"     }  ;", "File   procfsRootDir    =    new   File (  . TEST _ ROOT _ DIR ,     \" proc \"  )  ;", "try    {", ". setupProcfsRootDir ( procfsRootDir )  ;", ". setupPidDirs ( procfsRootDir ,    pids )  ;", "int   numProcesses    =    pids . length ;", ". ProcessStatInfo [  ]    procInfos    =    new    . ProcessStatInfo [ numProcesses ]  ;", "procInfos [  0  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  1  0  0  \"  ,     \" proc 1  \"  ,     \"  1  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  0  \"  ,     \"  2  0  0  \"     }  )  ;", "procInfos [  1  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  2  0  0  \"  ,     \" proc 2  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  2  0  0  0  0  0  \"  ,     \"  2  0  0  \"  ,     \"  2  0  0  0  \"  ,     \"  4  0  0  \"     }  )  ;", "procInfos [  2  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  3  0  0  \"  ,     \" proc 3  \"  ,     \"  2  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  3  0  0  0  0  0  \"  ,     \"  3  0  0  \"  ,     \"  3  0  0  0  \"  ,     \"  6  0  0  \"     }  )  ;", "procInfos [  3  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  4  0  0  \"  ,     \" proc 4  \"  ,     \"  2  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  4  0  0  0  0  0  \"  ,     \"  4  0  0  \"  ,     \"  4  0  0  0  \"  ,     \"  8  0  0  \"     }  )  ;", "procInfos [  4  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  5  0  0  \"  ,     \" proc 5  \"  ,     \"  4  0  0  \"  ,     \"  1  0  0  \"  ,     \"  1  0  0  \"  ,     \"  4  0  0  0  0  0  \"  ,     \"  4  0  0  \"  ,     \"  4  0  0  0  \"  ,     \"  8  0  0  \"     }  )  ;", "procInfos [  5  ]     =    new    . ProcessStatInfo ( new   String [  ]  {     \"  6  0  0  \"  ,     \" proc 6  \"  ,     \"  1  \"  ,     \"  1  \"  ,     \"  1  \"  ,     \"  4  0  0  0  0  0  \"  ,     \"  4  0  0  \"  ,     \"  4  0  0  0  \"  ,     \"  8  0  0  \"     }  )  ;", "ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  ]    memInfos    =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo [  6  ]  ;", "memInfos [  0  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  1  0  0  \"  )  ;", "memInfos [  1  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  2  0  0  \"  )  ;", "memInfos [  2  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  3  0  0  \"  )  ;", "memInfos [  3  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  4  0  0  \"  )  ;", "memInfos [  4  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  5  0  0  \"  )  ;", "memInfos [  5  ]     =    new   ProcfsBasedProcessTree . ProcessTreeSmapMemInfo (  \"  6  0  0  \"  )  ;", "String [  ]    cmdLines    =    new   String [ numProcesses ]  ;", "cmdLines [  0  ]     =     \" proc 1    arg 1    arg 2  \"  ;", "cmdLines [  1  ]     =     \" proc 2    arg 3    arg 4  \"  ;", "cmdLines [  2  ]     =     \" proc 3    arg 5    arg 6  \"  ;", "cmdLines [  3  ]     =     \" proc 4    arg 7    arg 8  \"  ;", "cmdLines [  4  ]     =     \" proc 5    arg 9    arg 1  0  \"  ;", "cmdLines [  5  ]     =     \" proc 6    arg 1  1    arg 1  2  \"  ;", "createMemoryMappingInfo ( memInfos )  ;", ". writeStatFiles ( procfsRootDir ,    pids ,    procInfos ,    memInfos )  ;", ". writeCmdLineFiles ( procfsRootDir ,    pids ,    cmdLines )  ;", "ProcfsBasedProcessTree   processTree    =    createProcessTree (  \"  1  0  0  \"  ,    procfsRootDir . getAbsolutePath (  )  )  ;", "processTree . updateProcessTree (  )  ;", "String   processTreeDump    =    processTree . getProcessTreeDump (  )  ;", ". LOG . info (  (  \" Process - tree   dump   follows :     \\ n \"     +    processTreeDump )  )  ;", "Assert . assertTrue (  \" Process - tree   dump   doesn ' t   start   with   a   proper   header \"  ,    processTreeDump . startsWith (  (  \"  \\ t |  -    PID   PPID   PGRPID   SESSID   CMD _ NAME    \"     +     (  \" USER _ MODE _ TIME ( MILLIS )    SYSTEM _ TIME ( MILLIS )    VMEM _ USAGE ( BYTES )     \"     +     \" RSSMEM _ USAGE ( PAGES )    FULL _ CMD _ LINE \\ n \"  )  )  )  )  ;", "for    ( int   i    =     0  ;    i    <     5  ;    i +  +  )     {", ". ProcessStatInfo   p    =    procInfos [ i ]  ;", "Assert . assertTrue (  (  \" Process - tree   dump   doesn ' t   contain   the   cmdLineDump   of   process    \"     +     ( p . pid )  )  ,    processTreeDump . contains (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \"  \\ t |  -     \"     +     ( p . pid )  )     +     \"     \"  )     +     ( p . ppid )  )     +     \"     \"  )     +     ( p . pgrpId )  )     +     \"     \"  )     +     ( p . session )  )     +     \"     (  \"  )     +     ( p . name )  )     +     \"  )     \"  )     +     ( p . utime )  )     +     \"     \"  )     +     ( p . stime )  )     +     \"     \"  )     +     ( p . vmem )  )     +     \"     \"  )     +     ( p . rssmemPage )  )     +     \"     \"  )     +     ( cmdLines [ i ]  )  )  )  )  ;", "}", ". ProcessStatInfo   p    =    procInfos [  5  ]  ;", "Assert . assertFalse (  (  \" Process - tree   dump   shouldn ' t   contain   the   cmdLineDump   of   process    \"     +     ( p . pid )  )  ,    processTreeDump . contains (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  (  \"  \\ t |  -     \"     +     ( p . pid )  )     +     \"     \"  )     +     ( p . ppid )  )     +     \"     \"  )     +     ( p . pgrpId )  )     +     \"     \"  )     +     ( p . session )  )     +     \"     (  \"  )     +     ( p . name )  )     +     \"  )     \"  )     +     ( p . utime )  )     +     \"     \"  )     +     ( p . stime )  )     +     \"     \"  )     +     ( p . vmem )  )     +     \"     \"  )     +     ( cmdLines [  5  ]  )  )  )  )  ;", "}    finally    {", "FileUtil . fullyDelete ( procfsRootDir )  ;", "}", "}", "METHOD_END"], "methodName": ["testProcessTreeDump"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( pids . length )  ;    i +  +  )     {", "File   statFile    =    new   File ( new   File ( procfsRootDir ,    pids [ i ]  )  ,    ProcfsBasedProcessTree . PROCFS _ CMDLINE _ FILE )  ;", "BufferedWriter   bw    =    null ;", "try    {", "bw    =    new   BufferedWriter ( new   FileWriter ( statFile )  )  ;", "bw . write ( cmdLines [ i ]  )  ;", ". LOG . info (  (  (  (  \" wrote   command - line   file   for    \"     +     ( pids [ i ]  )  )     +     \"    with   contents :     \"  )     +     ( cmdLines [ i ]  )  )  )  ;", "}    finally    {", "if    ( bw    !  =    null )     {", "bw . close (  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["writeCmdLineFiles"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <     ( pids . length )  ;    i +  +  )     {", "File   statFile    =    new   File ( new   File ( procfsRootDir ,    pids [ i ]  )  ,    ProcfsBasedProcessTree . PROCFS _ STAT _ FILE )  ;", "BufferedWriter   bw    =    null ;", "try    {", "FileWriter   fw    =    new   FileWriter ( statFile )  ;", "bw    =    new   BufferedWriter ( fw )  ;", "bw . write ( procs [ i ]  . getStatLine (  )  )  ;", ". LOG . info (  (  (  (  \" wrote   stat   file   for    \"     +     ( pids [ i ]  )  )     +     \"    with   contents :     \"  )     +     ( procs [ i ]  . getStatLine (  )  )  )  )  ;", "}    finally    {", "if    ( bw    !  =    null )     {", "bw . close (  )  ;", "}", "}", "if    ( smaps    !  =    null )     {", "File   smapFile    =    new   File ( new   File ( procfsRootDir ,    pids [ i ]  )  ,    ProcfsBasedProcessTree . SMAPS )  ;", "bw    =    null ;", "try    {", "FileWriter   fw    =    new   FileWriter ( smapFile )  ;", "bw    =    new   BufferedWriter ( fw )  ;", "bw . write ( smaps [ i ]  . toString (  )  )  ;", "bw . flush (  )  ;", ". LOG . info (  (  (  (  \" wrote   smap   file   for    \"     +     ( pids [ i ]  )  )     +     \"    with   contents :     \"  )     +     ( smaps [ i ]  . toString (  )  )  )  )  ;", "}    finally    {", "if    ( bw    !  =    null )     {", "bw . close (  )  ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["writeStatFiles"], "fileName": "org.apache.hadoop.yarn.util.TestProcfsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( NET _ TOPOLOGY _ NODE _ SWITCH _ MAPPING _ IMPL _ KEY ,     . MyResolver . class ,    DNSToSwitchMapping . class )  ;", "RackResolver . init ( conf )  ;", "try    {", "InetAddress   iaddr    =    InetAddress . getByName (  \" host 1  \"  )  ;", ". MyResolver . resolvedHost 1     =    iaddr . getHostAddress (  )  ;", "}    catch    ( UnknownHostException   e )     {", "}", "Node   node    =    RackResolver . resolve (  \" host 1  \"  )  ;", "Assert . assertEquals (  \"  / rack 1  \"  ,    node . getNetworkLocation (  )  )  ;", "node    =    RackResolver . resolve (  \" host 1  \"  )  ;", "Assert . assertEquals (  \"  / rack 1  \"  ,    node . getNetworkLocation (  )  )  ;", "node    =    RackResolver . resolve (  . invalidHost )  ;", "Assert . assertEquals ( DEFAULT _ RACK ,    node . getNetworkLocation (  )  )  ;", "}", "METHOD_END"], "methodName": ["testCaching"], "fileName": "org.apache.hadoop.yarn.util.TestRackResolver"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    new   Configuration (  )  ;", "conf . setClass ( NET _ TOPOLOGY _ NODE _ SWITCH _ MAPPING _ IMPL _ KEY ,     . class ,    DNSToSwitchMapping . class )  ;", "conf . set ( NET _ TOPOLOGY _ SCRIPT _ FILE _ NAME _ KEY ,     \" testScript \"  )  ;", "RackResolver . init ( conf )  ;", "Assert . assertEquals ( RackResolver . getDnsToSwitchMapping (  )  . toString (  )  ,     \" script - based   mapping   with   script   testScript \"  )  ;", "}", "METHOD_END"], "methodName": ["testScriptName"], "fileName": "org.apache.hadoop.yarn.util.TestRackResolverScriptBasedMapping"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculatorProcessTree   tree ;", "tree    =    ResourceCalculatorProcessTree . getResourceCalculatorProcessTree (  \"  1  \"  ,     . EmptyProcessTree . class ,    new   Configuration (  )  )  ;", "assertNotNull ( tree )  ;", "assertThat ( tree ,    instanceOf (  . EmptyProcessTree . class )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateInstance"], "fileName": "org.apache.hadoop.yarn.util.TestResourceCalculatorProcessTree"}, {"methodBody": ["METHOD_START", "{", "ResourceCalculatorProcessTree   tree ;", "Configuration   conf    =    new   Configuration (  )  ;", "tree    =    ResourceCalculatorProcessTree . getResourceCalculatorProcessTree (  \"  1  \"  ,     . EmptyProcessTree . class ,    conf )  ;", "assertNotNull ( tree )  ;", "assertThat ( tree . getConf (  )  ,    sameInstance ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["testCreatedInstanceConfigured"], "fileName": "org.apache.hadoop.yarn.util.TestResourceCalculatorProcessTree"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =    Times . elapsed (  1  0  ,     5  ,    true )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    -  1  \"  ,     (  -  1  )  ,    elapsed )  ;", "elapsed    =    Times . elapsed (  1  0  ,     5  ,    false )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    -  1  \"  ,     (  -  1  )  ,    elapsed )  ;", "elapsed    =    Times . elapsed ( Long . MAX _ VALUE ,     0  ,    true )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    -  1  \"  ,     (  -  1  )  ,    elapsed )  ;", "}", "METHOD_END"], "methodName": ["testFinishTimesAheadOfStartTimes"], "fileName": "org.apache.hadoop.yarn.util.TestTimes"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =    Times . elapsed (  5  ,     (  -  1  0  )  ,    false )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    -  1  \"  ,     (  -  1  )  ,    elapsed )  ;", "}", "METHOD_END"], "methodName": ["testNegativeFinishTimes"], "fileName": "org.apache.hadoop.yarn.util.TestTimes"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =    Times . elapsed (  (  -  5  )  ,     1  0  ,    true )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    0  \"  ,     0  ,    elapsed )  ;", "elapsed    =    Times . elapsed (  (  -  5  )  ,     1  0  ,    false )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    -  1  \"  ,     (  -  1  )  ,    elapsed )  ;", "}", "METHOD_END"], "methodName": ["testNegativeStartTimes"], "fileName": "org.apache.hadoop.yarn.util.TestTimes"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =    Times . elapsed (  (  -  5  )  ,     (  -  1  0  )  ,    false )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    -  1  \"  ,     (  -  1  )  ,    elapsed )  ;", "}", "METHOD_END"], "methodName": ["testNegativeStartandFinishTimes"], "fileName": "org.apache.hadoop.yarn.util.TestTimes"}, {"methodBody": ["METHOD_START", "{", "long   elapsed    =    Times . elapsed (  5  ,     1  0  ,    true )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    5  \"  ,     5  ,    elapsed )  ;", "elapsed    =    Times . elapsed (  5  ,     1  0  ,    false )  ;", "Assert . assertEquals (  \" Elapsed   time   is   not    5  \"  ,     5  ,    elapsed )  ;", "}", "METHOD_END"], "methodName": ["testPositiveStartandFinishTimes"], "fileName": "org.apache.hadoop.yarn.util.TestTimes"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( Shell . WINDOWS )  )     {", ". LOG . info (  \" Platform   not   Windows .    Not   testing \"  )  ;", "return ;", "}", "assertTrue (  \" WindowsBasedProcessTree   should   be   available   on   Windows \"  ,    WindowsBasedProcessTree . isAvailable (  )  )  ;", ". WindowsBasedProcessTreeTester   pTree    =    new    . WindowsBasedProcessTreeTester (  \"  -  1  \"  )  ;", "pTree . infoStr    =     \"  3  5  2  4  ,  1  0  2  4  ,  1  0  2  4  ,  5  0  0  \\ r \\ n 2  8  4  4  ,  1  0  2  4  ,  1  0  2  4  ,  5  0  0  \\ r \\ n \"  ;", "pTree . updateProcessTree (  )  ;", "assertTrue (  (  ( pTree . getCumulativeVmem (  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeVmem (  0  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeRssmem (  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeRssmem (  0  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeCpuTime (  )  )     =  =     1  0  0  0  )  )  ;", "pTree . infoStr    =     \"  3  5  2  4  ,  1  0  2  4  ,  1  0  2  4  ,  1  0  0  0  \\ r \\ n 2  8  4  4  ,  1  0  2  4  ,  1  0  2  4  ,  1  0  0  0  \\ r \\ n 1  2  3  4  ,  1  0  2  4  ,  1  0  2  4  ,  1  0  0  0  \\ r \\ n \"  ;", "pTree . updateProcessTree (  )  ;", "assertTrue (  (  ( pTree . getCumulativeVmem (  )  )     =  =     3  0  7  2  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeVmem (  1  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeRssmem (  )  )     =  =     3  0  7  2  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeRssmem (  1  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeCpuTime (  )  )     =  =     3  0  0  0  )  )  ;", "pTree . infoStr    =     \"  3  5  2  4  ,  1  0  2  4  ,  1  0  2  4  ,  1  5  0  0  \\ r \\ n 2  8  4  4  ,  1  0  2  4  ,  1  0  2  4  ,  1  5  0  0  \\ r \\ n \"  ;", "pTree . updateProcessTree (  )  ;", "assertTrue (  (  ( pTree . getCumulativeVmem (  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeVmem (  2  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeRssmem (  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeRssmem (  2  )  )     =  =     2  0  4  8  )  )  ;", "assertTrue (  (  ( pTree . getCumulativeCpuTime (  )  )     =  =     4  0  0  0  )  )  ;", "}", "METHOD_END"], "methodName": ["tree"], "fileName": "org.apache.hadoop.yarn.util.TestWindowsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "TestWindowsResourceCalculatorPlugin . WindowsResourceCalculatorPluginTester   tester    =    new   TestWindowsResourceCalculatorPlugin . WindowsResourceCalculatorPluginTester (  )  ;", "tester . infoStr    =    null ;", "tester . getAvailablePhysicalMemorySize (  )  ;", "}", "METHOD_END"], "methodName": ["errorInGetSystemInfo"], "fileName": "org.apache.hadoop.yarn.util.TestWindowsResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "TestWindowsResourceCalculatorPlugin . WindowsResourceCalculatorPluginTester   tester    =    new   TestWindowsResourceCalculatorPlugin . WindowsResourceCalculatorPluginTester (  )  ;", "tester . infoStr    =     \"  1  7  1  7  7  0  3  8  8  4  8  ,  8  5  8  9  4  6  7  6  4  8  ,  1  5  2  3  2  7  4  5  4  7  2  ,  6  4  0  0  4  1  7  7  9  2  ,  1  ,  2  8  0  5  0  0  0  ,  6  2  6  1  8  1  2  \\ r \\ n \"  ;", "tester . getAvailablePhysicalMemorySize (  )  ;", "assertTrue (  (  ( tester . vmemSize )     =  =     1  7  1  7  7  0  3  8  8  4  8 L )  )  ;", "assertTrue (  (  ( tester . memSize )     =  =     8  5  8  9  4  6  7  6  4  8 L )  )  ;", "assertTrue (  (  ( tester . vmemAvailable )     =  =     1  5  2  3  2  7  4  5  4  7  2 L )  )  ;", "assertTrue (  (  ( tester . memAvailable )     =  =     6  4  0  0  4  1  7  7  9  2 L )  )  ;", "assertTrue (  (  ( tester . numProcessors )     =  =     1  )  )  ;", "assertTrue (  (  ( tester . cpuFrequencyKhz )     =  =     2  8  0  5  0  0  0 L )  )  ;", "assertTrue (  (  ( tester . cumulativeCpuTimeMs )     =  =     6  2  6  1  8  1  2 L )  )  ;", "assertTrue (  (  ( tester . cpuUsage )     =  =     (  -  1  )  )  )  ;", "}", "METHOD_END"], "methodName": ["parseSystemInfoString"], "fileName": "org.apache.hadoop.yarn.util.TestWindowsResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "TestWindowsResourceCalculatorPlugin . WindowsResourceCalculatorPluginTester   tester    =    new   TestWindowsResourceCalculatorPlugin . WindowsResourceCalculatorPluginTester (  )  ;", "tester . infoStr    =     \"  1  7  1  7  7  0  3  8  8  4  8  ,  8  5  8  9  4  6  7  6  4  8  ,  1  5  2  3  2  7  4  5  4  7  2  ,  6  4  0  0  4  1  7  7  9  2  ,  1  ,  2  8  0  5  0  0  0  ,  6  2  6  1  8  1  2  \\ r \\ n \"  ;", "tester . getAvailablePhysicalMemorySize (  )  ;", "assertTrue (  (  ( tester . memAvailable )     =  =     6  4  0  0  4  1  7  7  9  2 L )  )  ;", "assertTrue (  (  ( tester . cpuUsage )     =  =     (  -  1  )  )  )  ;", "tester . infoStr    =     \"  1  7  1  7  7  0  3  8  8  4  8  ,  8  5  8  9  4  6  7  6  4  8  ,  1  5  2  3  2  7  4  5  4  7  2  ,  5  4  0  0  4  1  7  7  9  2  ,  1  ,  2  8  0  5  0  0  0  ,  6  2  6  1  8  1  2  \\ r \\ n \"  ;", "tester . getAvailablePhysicalMemorySize (  )  ;", "assertTrue (  (  ( tester . memAvailable )     =  =     6  4  0  0  4  1  7  7  9  2 L )  )  ;", "assertTrue (  (  ( tester . cpuUsage )     =  =     (  -  1  )  )  )  ;", "Thread . sleep (  1  5  0  0  )  ;", "tester . infoStr    =     \"  1  7  1  7  7  0  3  8  8  4  8  ,  8  5  8  9  4  6  7  6  4  8  ,  1  5  2  3  2  7  4  5  4  7  2  ,  5  4  0  0  4  1  7  7  9  2  ,  1  ,  2  8  0  5  0  0  0  ,  6  2  8  6  8  1  2  \\ r \\ n \"  ;", "tester . getAvailablePhysicalMemorySize (  )  ;", "assertTrue (  (  ( tester . memAvailable )     =  =     5  4  0  0  4  1  7  7  9  2 L )  )  ;", "assertTrue (  (  ( tester . cpuUsage )     >  =     0  .  1  )  )  ;", "}", "METHOD_END"], "methodName": ["refreshAndCpuUsage"], "fileName": "org.apache.hadoop.yarn.util.TestWindowsResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  \" getVersion   returned   Unknown \"  ,     (  !  ( YarnVersionInfo . getVersion (  )  . equals (  \" Unknown \"  )  )  )  )  ;", "assertTrue (  \" getUser   returned   Unknown \"  ,     (  !  ( YarnVersionInfo . getUser (  )  . equals (  \" Unknown \"  )  )  )  )  ;", "assertTrue (  \" getSrcChecksum   returned   Unknown \"  ,     (  !  ( YarnVersionInfo . getSrcChecksum (  )  . equals (  \" Unknown \"  )  )  )  )  ;", "assertNotNull (  \" getUrl   returned   null \"  ,    YarnVersionInfo . getUrl (  )  )  ;", "assertNotNull (  \" getRevision   returned   null \"  ,    YarnVersionInfo . getRevision (  )  )  ;", "assertNotNull (  \" getBranch   returned   null \"  ,    YarnVersionInfo . getBranch (  )  )  ;", "assertTrue (  \" getBuildVersion   check   doesn ' t   contain :    source   checksum \"  ,    YarnVersionInfo . getBuildVersion (  )  . contains (  \" source   checksum \"  )  )  ;", "}", "METHOD_END"], "methodName": ["versionInfoGenerated"], "fileName": "org.apache.hadoop.yarn.util.TestYarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   Times . elapsed ( started ,    finished ,    true )  ;", "}", "METHOD_END"], "methodName": ["elapsed"], "fileName": "org.apache.hadoop.yarn.util.Times"}, {"methodBody": ["METHOD_START", "{", "if    (  ( finished    >     0  )     &  &     ( started    >     0  )  )     {", "long   elapsed    =    finished    -    started ;", "if    ( elapsed    >  =     0  )     {", "return   elapsed ;", "} else    {", ". LOG . warn (  (  (  (  \" Finished   time    \"     +    finished )     +     \"    is   ahead   of   started   time    \"  )     +    started )  )  ;", "return    -  1  ;", "}", "}", "if    ( isRunning )     {", "long   current    =    System . currentTimeMillis (  )  ;", "long   elapsed    =     ( started    >     0  )     ?    current    -    started    :     0  ;", "if    ( elapsed    >  =     0  )     {", "return   elapsed ;", "} else    {", ". LOG . warn (  (  (  (  \" Current   time    \"     +    current )     +     \"    is   ahead   of   started   time    \"  )     +    started )  )  ;", "return    -  1  ;", "}", "} else    {", "return    -  1  ;", "}", "}", "METHOD_END"], "methodName": ["elapsed"], "fileName": "org.apache.hadoop.yarn.util.Times"}, {"methodBody": ["METHOD_START", "{", "return   ts    >     0     ?    String . valueOf ( Times . dateFormat . get (  )  . format ( new   Date ( ts )  )  )     :     \" N / A \"  ;", "}", "METHOD_END"], "methodName": ["format"], "fileName": "org.apache.hadoop.yarn.util.Times"}, {"methodBody": ["METHOD_START", "{", "String [  ]    processesStr    =    processesInfoStr . split (  \"  \\ r \\ n \"  )  ;", "Map < String ,     . ProcessInfo >    allProcs    =    new   HashMap < String ,     . ProcessInfo >  (  )  ;", "final   int   procInfoSplitCount    =     4  ;", "for    ( String   processStr    :    processesStr )     {", "if    ( processStr    !  =    null )     {", "String [  ]    procInfo    =    processStr . split (  \"  ,  \"  )  ;", "if    (  ( procInfo . length )     =  =    procInfoSplitCount )     {", "try    {", ". ProcessInfo   pInfo    =    new    . ProcessInfo (  )  ;", "pInfo . pid    =    procInfo [  0  ]  ;", "pInfo . vmem    =    Long . parseLong ( procInfo [  1  ]  )  ;", "pInfo . workingSet    =    Long . parseLong ( procInfo [  2  ]  )  ;", "pInfo . cpuTimeMs    =    Long . parseLong ( procInfo [  3  ]  )  ;", "allProcs . put ( pInfo . pid ,    pInfo )  ;", "}    catch    ( NumberFormatException   nfe )     {", ". LOG . debug (  (  \" Error   parsing   procInfo .  \"     +    nfe )  )  ;", "}", "} else    {", ". LOG . debug (  (  (  (  \" Expected   split   length   of   proc   info   to   be    \"     +    procInfoSplitCount )     +     \"  .    Got    \"  )     +     ( procInfo . length )  )  )  ;", "}", "}", "}", "return   allProcs ;", "}", "METHOD_END"], "methodName": ["createProcessInfo"], "fileName": "org.apache.hadoop.yarn.util.WindowsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "ShellCommandExecutor   shellExecutor    =    new   ShellCommandExecutor ( new   String [  ]  {    Shell . WINUTILS ,     \" task \"  ,     \" processList \"  ,    taskProcessId    }  )  ;", "try    {", "shellExecutor . execute (  )  ;", "return   shellExecutor . getOutput (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error ( StringUtils . stringifyException ( e )  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getAllProcessInfoFromShell"], "fileName": "org.apache.hadoop.yarn.util.WindowsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "if    ( Shell . WINDOWS )     {", "ShellCommandExecutor   shellExecutor    =    new   ShellCommandExecutor ( new   String [  ]  {    Shell . WINUTILS ,     \" help \"     }  )  ;", "try    {", "shellExecutor . execute (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error ( StringUtils . stringifyException ( e )  )  ;", "}    finally    {", "String   output    =    shellExecutor . getOutput (  )  ;", "if    (  ( output    !  =    null )     &  &     ( output . contains (  \" Prints   to   stdout   a   list   of   processes   in   the   task \"  )  )  )     {", "return   true ;", "}", "}", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isAvailable"], "fileName": "org.apache.hadoop.yarn.util.WindowsBasedProcessTree"}, {"methodBody": ["METHOD_START", "{", "ShellCommandExecutor   shellExecutor    =    new   ShellCommandExecutor ( new   String [  ]  {    Shell . WINUTILS ,     \" systeminfo \"     }  )  ;", "try    {", "shellExecutor . execute (  )  ;", "return   shellExecutor . getOutput (  )  ;", "}    catch    ( IOException   e )     {", ". LOG . error ( StringUtils . stringifyException ( e )  )  ;", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["getSystemInfoInfoFromShell"], "fileName": "org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "long   now    =    System . currentTimeMillis (  )  ;", "if    (  ( now    -     ( lastRefreshTime )  )     >     ( refreshIntervalMs )  )     {", "long   refreshInterval    =    now    -     ( lastRefreshTime )  ;", "lastRefreshTime    =    now ;", "long   lastCumCpuTimeMs    =    cumulativeCpuTimeMs ;", "reset (  )  ;", "String   sysInfoStr    =    getSystemInfoInfoFromShell (  )  ;", "if    ( sysInfoStr    !  =    null )     {", "final   int   sysInfoSplitCount    =     7  ;", "String [  ]    sysInfo    =    sysInfoStr . substring (  0  ,    sysInfoStr . indexOf (  \"  \\ r \\ n \"  )  )  . split (  \"  ,  \"  )  ;", "if    (  ( sysInfo . length )     =  =    sysInfoSplitCount )     {", "try    {", "vmemSize    =    Long . parseLong ( sysInfo [  0  ]  )  ;", "memSize    =    Long . parseLong ( sysInfo [  1  ]  )  ;", "vmemAvailable    =    Long . parseLong ( sysInfo [  2  ]  )  ;", "memAvailable    =    Long . parseLong ( sysInfo [  3  ]  )  ;", "numProcessors    =    Integer . parseInt ( sysInfo [  4  ]  )  ;", "cpuFrequencyKhz    =    Long . parseLong ( sysInfo [  5  ]  )  ;", "cumulativeCpuTimeMs    =    Long . parseLong ( sysInfo [  6  ]  )  ;", "if    ( lastCumCpuTimeMs    !  =     (  -  1  )  )     {", "cpuUsage    =     (  ( cumulativeCpuTimeMs )     -    lastCumCpuTimeMs )     /     ( refreshInterval    *     1  .  0 F )  ;", "}", "}    catch    ( NumberFormatException   nfe )     {", ". LOG . warn (  (  \" Error   parsing   sysInfo .  \"     +    nfe )  )  ;", "}", "} else    {", ". LOG . warn (  (  (  (  \" Expected   split   length   of   sysInfo   to   be    \"     +    sysInfoSplitCount )     +     \"  .    Got    \"  )     +     ( sysInfo . length )  )  )  ;", "}", "}", "}", "}", "METHOD_END"], "methodName": ["refreshIfNeeded"], "fileName": "org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "vmemSize    =     -  1  ;", "memSize    =     -  1  ;", "vmemAvailable    =     -  1  ;", "memAvailable    =     -  1  ;", "numProcessors    =     -  1  ;", "cpuFrequencyKhz    =     -  1  ;", "cumiveCpuTimeMs    =     -  1  ;", "cpuUsage    =     -  1  ;", "}", "METHOD_END"], "methodName": ["reset"], "fileName": "org.apache.hadoop.yarn.util.WindowsResourceCalculatorPlugin"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getBranch (  )  ;", "}", "METHOD_END"], "methodName": ["getBranch"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getBuildVersion (  )  ;", "}", "METHOD_END"], "methodName": ["getBuildVersion"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getDate (  )  ;", "}", "METHOD_END"], "methodName": ["getDate"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getRevision (  )  ;", "}", "METHOD_END"], "methodName": ["getRevision"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getSrcChecksum (  )  ;", "}", "METHOD_END"], "methodName": ["getSrcChecksum"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getUrl (  )  ;", "}", "METHOD_END"], "methodName": ["getUrl"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getUser (  )  ;", "}", "METHOD_END"], "methodName": ["getUser"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "return   YarnVersionInfo . YARN _ VERSION _ INFO .  _ getVersion (  )  ;", "}", "METHOD_END"], "methodName": ["getVersion"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "YarnVersionInfo . LOG . debug (  (  \" version :     \"     +     ( YarnVersionInfo . getVersion (  )  )  )  )  ;", "System . out . println (  (  \" Yarn    \"     +     ( YarnVersionInfo . getVersion (  )  )  )  )  ;", "System . out . println (  (  (  (  \" Subversion    \"     +     ( YarnVersionInfo . getUrl (  )  )  )     +     \"     - r    \"  )     +     ( YarnVersionInfo . getRevision (  )  )  )  )  ;", "System . out . println (  (  (  (  \" Compiled   by    \"     +     ( YarnVersionInfo . getUser (  )  )  )     +     \"    on    \"  )     +     ( YarnVersionInfo . getDate (  )  )  )  )  ;", "System . out . println (  (  \" From   source   with   checksum    \"     +     ( YarnVersionInfo . getSrcChecksum (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.util.YarnVersionInfo"}, {"methodBody": ["METHOD_START", "{", "if    (  ( r . getMemory (  )  )     =  =     0  .  0 F )     {", "return   true ;", "}", "return   false ;", "}", "METHOD_END"], "methodName": ["isInvalidDivisor"], "fileName": "org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator"}, {"methodBody": ["METHOD_START", "{", "return   dominant    ?    Math . max (  (  (  ( float )     ( resource . getMemory (  )  )  )     /     ( clusterResource . getMemory (  )  )  )  ,     (  (  ( float )     ( resource . getVirtualCores (  )  )  )     /     ( clusterResource . getVirtualCores (  )  )  )  )     :    Math . min (  (  (  ( float )     ( resource . getMemory (  )  )  )     /     ( clusterResource . getMemory (  )  )  )  ,     (  (  ( float )     ( resource . getVirtualCores (  )  )  )     /     ( clusterResource . getVirtualCores (  )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["getResourceAsValue"], "fileName": "org.apache.hadoop.yarn.util.resource.DominantResourceCalculator"}, {"methodBody": ["METHOD_START", "{", "if    ( b    =  =     0  )     {", ". LOG . info (  (  (  (  \" divideAndCeil   called   with   a =  \"     +    a )     +     \"    b =  \"  )     +    b )  )  ;", "return    0  ;", "}", "return    ( a    +     ( b    -     1  )  )     /    b ;", "}", "METHOD_END"], "methodName": ["divideAndCeil"], "fileName": "org.apache.hadoop.yarn.util.resource.ResourceCalculator"}, {"methodBody": ["METHOD_START", "{", "return   normalize ( r ,    minimumResource ,    maximumResource ,    minimumResource )  ;", "}", "METHOD_END"], "methodName": ["normalize"], "fileName": "org.apache.hadoop.yarn.util.resource.ResourceCalculator"}, {"methodBody": ["METHOD_START", "{", "return    ( a    /    b )     *    b ;", "}", "METHOD_END"], "methodName": ["roundDown"], "fileName": "org.apache.hadoop.yarn.util.resource.ResourceCalculator"}, {"methodBody": ["METHOD_START", "{", "return    ( ResourceCalculator . divideAndCeil ( a ,    b )  )     *    b ;", "}", "METHOD_END"], "methodName": ["roundUp"], "fileName": "org.apache.hadoop.yarn.util.resource.ResourceCalculator"}, {"methodBody": ["METHOD_START", "{", "return   Resources . addTo ( Resources . clone ( lhs )  ,    rhs )  ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "lhs . setMemory (  (  ( lhs . getMemory (  )  )     +     ( rhs . getMemory (  )  )  )  )  ;", "lhs . setVirtualCo (  (  ( lhs . getVirtualCo (  )  )     +     ( rhs . getVirtualCo (  )  )  )  )  ;", "return   lhs ;", "}", "METHOD_END"], "methodName": ["addTo"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . createResource ( res . getMemory (  )  ,    res . getVirtualCores (  )  )  ;", "}", "METHOD_END"], "methodName": ["clone"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . createResource ( Math . min ( lhs . getMemory (  )  ,    rhs . getMemory (  )  )  ,    Math . min ( lhs . getVirtualCores (  )  ,    rhs . getVirtualCores (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["componentwiseMin"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . createResource ( memory ,     ( memory    >     0     ?     1     :     0  )  )  ;", "}", "METHOD_END"], "methodName": ["createResource"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "Resource    =    Records . newRecord ( Resource . class )  ;", "setMemory ( memory )  ;", "setVirtualCores ( cores )  ;", "return", "}", "METHOD_END"], "methodName": ["createResource"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   resourceCalculator . divide ( clusterResource ,    lhs ,    rhs )  ;", "}", "METHOD_END"], "methodName": ["divide"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   resourceCalculator . divideAndCeil ( lhs ,    rhs )  ;", "}", "METHOD_END"], "methodName": ["divideAndCeil"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   lhs . equals ( rhs )  ;", "}", "METHOD_END"], "methodName": ["equals"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    (  ( smaller . getMemory (  )  )     <  =     ( bigger . getMemory (  )  )  )     &  &     (  ( smaller . getVirtualCores (  )  )     <  =     ( bigger . getVirtualCores (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["fitsIn"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    ( resourceCalculator . compare ( clusterResource ,    lhs ,    rhs )  )     >     0  ;", "}", "METHOD_END"], "methodName": ["greaterThan"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    ( resourceCalculator . compare ( clusterResource ,    lhs ,    rhs )  )     >  =     0  ;", "}", "METHOD_END"], "methodName": ["greaterThanOrEqual"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   resourceCalculator . isInvalidDivisor ( divisor )  ;", "}", "METHOD_END"], "methodName": ["isInvalidDivisor"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    ( resourceCalculator . compare ( clusterResource ,    lhs ,    rhs )  )     <     0  ;", "}", "METHOD_END"], "methodName": ["lessThan"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    ( resourceCalculator . compare ( clusterResource ,    lhs ,    rhs )  )     <  =     0  ;", "}", "METHOD_END"], "methodName": ["lessThanOrEqual"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    ( resourceCalculator . compare ( clusterResource ,    lhs ,    rhs )  )     >  =     0     ?    lhs    :    rhs ;", "}", "METHOD_END"], "methodName": ["max"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return    ( resourceCalculator . compare ( clusterResource ,    lhs ,    rhs )  )     <  =     0     ?    lhs    :    rhs ;", "}", "METHOD_END"], "methodName": ["min"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . multiplyTo ( Resources . clone ( lhs )  ,    by )  ;", "}", "METHOD_END"], "methodName": ["multiply"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   calculator . multiplyAndNormalizeDown ( lhs ,    by ,    factor )  ;", "}", "METHOD_END"], "methodName": ["multiplyAndNormalizeDown"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   calculator . multiplyAndNormalizeUp ( lhs ,    by ,    factor )  ;", "}", "METHOD_END"], "methodName": ["multiplyAndNormalizeUp"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "Resource   out    =    Resources . clone ( lhs )  ;", "out . setMemory (  (  ( int )     (  ( lhs . getMemory (  )  )     *    by )  )  )  ;", "out . setVirtualCores (  (  ( int )     (  ( lhs . getVirtualCores (  )  )     *    by )  )  )  ;", "return   out ;", "}", "METHOD_END"], "methodName": ["multiplyAndRoundDown"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "lhs . setMemory (  (  ( int )     (  ( lhs . getMemory (  )  )     *    by )  )  )  ;", "lhs . setVirtualCo (  (  ( int )     (  ( lhs . getVirtualCo (  )  )     *    by )  )  )  ;", "return   lhs ;", "}", "METHOD_END"], "methodName": ["multiplyTo"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . subtract ( Resources . NONE ,    resource )  ;", "}", "METHOD_END"], "methodName": ["negate"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . NONE ;", "}", "METHOD_END"], "methodName": ["none"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   calculator . normalize ( lhs ,    min ,    max ,    increment )  ;", "}", "METHOD_END"], "methodName": ["normalize"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   resourceCalculator . ratio ( lhs ,    rhs )  ;", "}", "METHOD_END"], "methodName": ["ratio"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   calculator . roundDown ( lhs ,    factor )  ;", "}", "METHOD_END"], "methodName": ["roundDown"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   calculator . roundUp ( lhs ,    factor )  ;", "}", "METHOD_END"], "methodName": ["roundUp"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . subtractFrom ( Resources . clone ( lhs )  ,    rhs )  ;", "}", "METHOD_END"], "methodName": ["subtract"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "lhs . setMemory (  (  ( lhs . getMemory (  )  )     -     ( rhs . getMemory (  )  )  )  )  ;", "lhs . setVirtualCo (  (  ( lhs . getVirtualCo (  )  )     -     ( rhs . getVirtualCo (  )  )  )  )  ;", "return   lhs ;", "}", "METHOD_END"], "methodName": ["subtractFrom"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "return   Resources . UNBOUNDED ;", "}", "METHOD_END"], "methodName": ["unbounded"], "fileName": "org.apache.hadoop.yarn.util.resource.Resources"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   timelineServiceAddr    =    TimelineUtils . getTimelineTokenServiceAddress ( conf )  ;", "return   SecurityUtil . buildTokenService ( timelineServiceAddr )  ;", "}", "METHOD_END"], "methodName": ["buildTimelineTokenService"], "fileName": "org.apache.hadoop.yarn.util.timeline.TimelineUtils"}, {"methodBody": ["METHOD_START", "{", "return   TimelineUtils . dumpTimelineRecordtoJSON ( o ,    false )  ;", "}", "METHOD_END"], "methodName": ["dumpTimelineRecordtoJSON"], "fileName": "org.apache.hadoop.yarn.util.timeline.TimelineUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( pretty )     {", "return    . mapper . writerWithDefaultPrettyPrinter (  )  . writeValueAsString ( o )  ;", "} else    {", "return    . mapper . writeValueAsString ( o )  ;", "}", "}", "METHOD_END"], "methodName": ["dumpTimelineRecordtoJSON"], "fileName": "org.apache.hadoop.yarn.util.timeline.TimelineUtils"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   timelineServiceAddr    =    null ;", "if    ( YarnConfiguration . useHttps ( conf )  )     {", "timelineServiceAddr    =    conf . getSocketAddr ( TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ PORT )  ;", "} else    {", "timelineServiceAddr    =    conf . getSocketAddr ( TIMELINE _ SERVICE _ WEBAPP _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ PORT )  ;", "}", "return   timelineServiceAddr ;", "}", "METHOD_END"], "methodName": ["getTimelineTokenServiceAddress"], "fileName": "org.apache.hadoop.yarn.util.timeline.TimelineUtils"}, {"methodBody": ["METHOD_START", "{", "return   get ( key ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["$"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "if    (  ( context )     =  =    null )     {", "if    (  ( injector )     =  =    null )     {", "throw   new   WebAppException ( StringHelper . join (  \" Error   accessing   RequestContext   from \\ n \"  ,     \" a   child   constructor ,    either   move   the   usage   of   the    \\ n \"  ,     \" methods   out   of   the   constructor   or   inject   the   RequestContext \\ n \"  ,     \" into   the   constructor \"  )  )  ;", "}", "context    =    injector . getInstance (  . RequestContext . class )  ;", "}", "return   context ;", "}", "METHOD_END"], "methodName": ["context"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . cookies (  )  ;", "}", "METHOD_END"], "methodName": ["cookies"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "render ( DefaultPage . class )  ;", "}", "METHOD_END"], "methodName": ["echo"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . error ;", "}", "METHOD_END"], "methodName": ["error"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . get ( key ,    defaultValue )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   injector . getInstance ( cls )  ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . devMode ;", "}", "METHOD_END"], "methodName": ["inDevMode"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   getInstance ( ResponseInfo . class )  . about ( about )  ;", "}", "METHOD_END"], "methodName": ["info"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . injector ;", "}", "METHOD_END"], "methodName": ["injector"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "context (  )  . rendered    =    true ;", "getInstance ( cls )  . render (  )  ;", "}", "METHOD_END"], "methodName": ["render"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "context (  )  . rendered    =    true ;", "response (  )  . setentType ( MimeType . JSON )  ;", "getInstance ( cls )  . toJSON ( writer (  )  )  ;", "}", "METHOD_END"], "methodName": ["renderJSON"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "Controller . LOG . debug (  \"  {  }  :     {  }  \"  ,    MimeType . JSON ,    object )  ;", "context (  )  . rendered    =    true ;", "context (  )  . response . setContentType ( MimeType . JSON )  ;", "try    {", "Controller . jsonMapper . writeValue ( writer (  )  ,    object )  ;", "}    catch    ( Exception   e )     {", "throw   new   WebAppException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["renderJSON"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "Controller . LOG . debug (  \"  {  }  :     {  }  \"  ,    MimeType . TEXT ,    s )  ;", "context (  )  . rendered    =    true ;", "response (  )  . setContentType ( MimeType . TEXT )  ;", "writer (  )  . print ( s )  ;", "}", "METHOD_END"], "methodName": ["renderText"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . request ;", "}", "METHOD_END"], "methodName": ["request"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . response ;", "}", "METHOD_END"], "methodName": ["response"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "context (  )  . set ( key ,    value )  ;", "}", "METHOD_END"], "methodName": ["set"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "context (  )  . setStatus ( status )  ;", "}", "METHOD_END"], "methodName": ["setStatus"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "set ( Params . TITLE ,    title )  ;", "}", "METHOD_END"], "methodName": ["setTitle"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "setTitle ( title )  ;", "set ( Params . TITLE _ LINK ,    url )  ;", "}", "METHOD_END"], "methodName": ["setTitle"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . status ;", "}", "METHOD_END"], "methodName": ["status"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . ujoin ( context (  )  . prefix ,    parts )  ;", "}", "METHOD_END"], "methodName": ["url"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "try    {", "return   response (  )  . getWriter (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   WebAppException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["writer"], "fileName": "org.apache.hadoop.yarn.webapp.Controller"}, {"methodBody": ["METHOD_START", "{", "RequestDispatcher   rd    =    getServletContext (  )  . getNamedDispatcher (  \" default \"  )  ;", "HttpServletRequest   wrapped    =    new   HttpServletReques ( req )     {", "public   String   getServletPath (  )     {", "return    \"  \"  ;", "}", "}  ;", "rd . forward ( wrapped ,    resp )  ;", "}", "METHOD_END"], "methodName": ["doGet"], "fileName": "org.apache.hadoop.yarn.webapp.DefaultWrapperServlet"}, {"methodBody": ["METHOD_START", "{", "checkState ( devMode ,     \" only   in   dev   mode \"  )  ;", "new   Timer (  \" webapp   exit \"  ,    true )  . schedule ( new   TimerTask (  )     {", "@ Override", "public   void   run (  )     {", ". LOG . info (  \" WebAppp    /  {  }    exiting .  .  .  \"  ,    webApp . name (  )  )  ;", "webApp . stop (  )  ;", "System . exit (  0  )  ;", "}", "}  ,     1  8  )  ;", "}", "METHOD_END"], "methodName": ["prepareToExit"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "String   st    =     ( devMode )     ?    ErrorPage . toStackTrace ( e ,     (  1  0  2  4     *     3  )  )     :     \" See   logs   for   stack   trace \"  ;", "res . setStatus ( res . SC _ FOUND )  ;", "Cookie   cookie    =    new   Cookie (  . STATUS _ COOKIE ,    String . valueOf (  5  0  0  )  )  ;", "cookie . setPath ( path )  ;", "res . addCookie ( cookie )  ;", "cookie    =    new   Cookie (  . ERROR _ COOKIE ,    st )  ;", "cookie . setPath ( path )  ;", "res . addCookie ( cookie )  ;", "res . setHeader (  \" Location \"  ,    path )  ;", "}", "METHOD_END"], "methodName": ["redirectToErrorPage"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "Dispatcher . LOG . debug (  \" removing   cookie    {  }    on    {  }  \"  ,    name ,    path )  ;", "Cookie   c    =    new   Cookie ( name ,     \"  \"  )  ;", "c . setMaxAge (  0  )  ;", "c . setPath ( path )  ;", "res . addCookie ( c )  ;", "}", "METHOD_END"], "methodName": ["removeCookie"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "Dispatcher . removeCookie ( res ,    Dispatcher . ERROR _ COOKIE ,    path )  ;", "Dispatcher . removeCookie ( res ,    Dispatcher . STATUS _ COOKIE ,    path )  ;", "}", "METHOD_END"], "methodName": ["removeErrorCookies"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "injector . getInstance ( cls )  . render (  )  ;", "}", "METHOD_END"], "methodName": ["render"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "Cookie [  ]    cookies    =    req . getCookies (  )  ;", "if    ( cookies    !  =    null )     {", "for    ( Cookie   cookie    :    cookies )     {", "rc . cookies (  )  . put ( cookie . getName (  )  ,    cookie )  ;", "}", "return   cookies . length ;", "}", "return    0  ;", "}", "METHOD_END"], "methodName": ["setCookieParams"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "devMode    =    choice ;", "}", "METHOD_END"], "methodName": ["setDevMode"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "checkState ( pathInfo . startsWith ( dest . prefix )  ,     \" prefix   should   match \"  )  ;", "if    (  (  ( dest . pathParams . size (  )  )     =  =     0  )     |  |     (  ( dest . prefix . length (  )  )     =  =     ( pathInfo . length (  )  )  )  )     {", "return ;", "}", "String [  ]    parts    =    Iterables . toArray ( WebApp . pathSplitter . split ( pathInfo . substring ( dest . prefix . length (  )  )  )  ,    String . class )  ;", ". LOG . debug (  \" parts =  {  }  ,    params =  {  }  \"  ,    parts ,    dest . pathParams )  ;", "for    ( int   i    =     0  ;     ( i    <     ( dest . pathParams . size (  )  )  )     &  &     ( i    <     ( parts . length )  )  ;     +  + i )     {", "String   key    =    dest . pathParams . get ( i )  ;", "if    (  ( key . charAt (  0  )  )     =  =     '  :  '  )     {", "rc . moreParams (  )  . put ( key . substring (  1  )  ,    parts [ i ]  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["setMoreParams"], "fileName": "org.apache.hadoop.yarn.webapp.Dispatcher"}, {"methodBody": ["METHOD_START", "{", "return   new   MyTestWebService . MyInfo (  )  ;", "}", "METHOD_END"], "methodName": ["get"], "fileName": "org.apache.hadoop.yarn.webapp.MyTestWebService"}, {"methodBody": ["METHOD_START", "{", "return   exception ;", "}", "METHOD_END"], "methodName": ["getException"], "fileName": "org.apache.hadoop.yarn.webapp.RemoteExceptionData"}, {"methodBody": ["METHOD_START", "{", "return   javaClassName ;", "}", "METHOD_END"], "methodName": ["getJavaClassName"], "fileName": "org.apache.hadoop.yarn.webapp.RemoteExceptionData"}, {"methodBody": ["METHOD_START", "{", "return   message ;", "}", "METHOD_END"], "methodName": ["getMessage"], "fileName": "org.apache.hadoop.yarn.webapp.RemoteExceptionData"}, {"methodBody": ["METHOD_START", "{", "ResponseInfo   info    =    new   ResponseInfo (  )  ;", "info . about    =    about ;", "return   info ;", "}", "METHOD_END"], "methodName": ["$about"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "items . add ( ResponseInfo . Item . of ( key ,    value ,    false )  )  ;", "return   this ;", "}", "METHOD_END"], "methodName": ["_"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "items . add ( ResponseInfo . Item . of ( key ,    url ,    anchor )  )  ;", "return   this ;", "}", "METHOD_END"], "methodName": ["_"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "items . add ( ResponseInfo . Item . of ( key ,    value ,    true )  )  ;", "return   this ;", "}", "METHOD_END"], "methodName": ["_r"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "return   about ;", "}", "METHOD_END"], "methodName": ["about"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "this . about    =    about ;", "return   this ;", "}", "METHOD_END"], "methodName": ["about"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "items . clear (  )  ;", "}", "METHOD_END"], "methodName": ["clear"], "fileName": "org.apache.hadoop.yarn.webapp.ResponseInfo"}, {"methodBody": ["METHOD_START", "{", "Router . LOG . debug (  \" adding    {  }  (  {  }  )  -  >  {  }  #  {  }  \"  ,    new   Object [  ]  {    path ,    names ,    cls ,    action    }  )  ;", "Router . Dest   dest    =    addController ( httpMethod ,    path ,    cls ,    action ,    names )  ;", "addDefaultView ( dest )  ;", "return   dest ;", "}", "METHOD_END"], "methodName": ["add"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "try    {", "Method   method    =    cls . getMethod ( action ,    null )  ;", ". Dest   dest    =    routes . get ( path )  ;", "if    ( dest    =  =    null )     {", "method . setAccessible ( true )  ;", "dest    =    new    . Dest ( path ,    method ,    cls ,    names ,    httpMethod )  ;", "routes . put ( path ,    dest )  ;", "return   dest ;", "}", "dest . methods . add ( httpMethod )  ;", "return   dest ;", "}    catch    ( NoSuchMethodException   nsme )     {", "throw   new   WebAppException (  (  ( action    +     \"  (  )    not   found   in    \"  )     +    cls )  )  ;", "}    catch    ( SecurityException   se )     {", "throw   new   WebAppException (  (  (  (  \" Security   exception   thrown   for    \"     +    action )     +     \"  (  )    in    \"  )     +    cls )  )  ;", "}", "}", "METHOD_END"], "methodName": ["addController"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "String   controllerName    =    dest . controllerClass . getSimpleName (  )  ;", "if    ( controllerName . endsWith (  \" Controller \"  )  )     {", "controllerName    =    controllerName . substring (  0  ,     (  ( controllerName . length (  )  )     -     1  0  )  )  ;", "}", "dest . defaultViewClass    =    find ( View . class ,    dest . controllerClass . getPackage (  )  . getName (  )  ,    StringHelper . join (  ( controllerName    +     \" View \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["addDefaultView"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "if    (  ( controller . equals (  \" default \"  )  )     &  &     ( action . equals (  \" index \"  )  )  )     {", "return    \"  /  \"  ;", "}", "if    ( action . equals (  \" index \"  )  )     {", "return   StringHelper . join (  '  /  '  ,    controller )  ;", "}", "return   StringHelper . pjoin (  \"  \"  ,    controller ,    action )  ;", "}", "METHOD_END"], "methodName": ["defaultPrefix"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "String   pkg    =    hostClass . getPackage (  )  . getName (  )  ;", "return   find ( cls ,    pkg ,    cname )  ;", "}", "METHOD_END"], "methodName": ["find"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "String   name    =    StringUtils . capitalize ( cname )  ;", "Class <  ?    extends   T >    found    =    load ( cls ,    StringHelper . djoin ( pkg ,    name )  )  ;", "if    ( found    =  =    null )     {", "found    =    load ( cls ,    StringHelper . djoin ( pkg ,     \"  \"  ,    name )  )  ;", "}", "if    ( found    =  =    null )     {", "found    =    load ( cls ,    StringHelper . join ( hostClass . getName (  )  ,     '  $  '  ,    name )  )  ;", "}", "return   found ;", "}", "METHOD_END"], "methodName": ["find"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "if    (  ( Router . SLASH . countIn ( dest . prefix )  )     >     1  )     {", "return   true ;", "}", "if    (  ( dest . prefix . length (  )  )     =  =     1  )     {", "return    (  ( dest . pathParams . size (  )  )     >     0  )     &  &     (  !  ( Router . maybeController ( path )  )  )  ;", "}", "return    (  ( dest . pathParams . size (  )  )     >     0  )     |  |     (  ( path . endsWith (  \"  /  \"  )  )     &  &     (  ( Router . SLASH . countIn ( path )  )     =  =     2  )  )  ;", "}", "METHOD_END"], "methodName": ["isGoodMatch"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "Router . LOG . debug (  \" trying :     {  }  \"  ,    className )  ;", "try    {", "Class <  ?  >    found    =    Class . forName ( className )  ;", "if    ( cls . isAssignableFrom ( found )  )     {", "Router . LOG . debug (  \" found    {  }  \"  ,    className )  ;", "return    (  ( Class <  ?    extends   T >  )     ( found )  )  ;", "}", "Router . LOG . warn (  \" found   a    {  }    but   it ' s   not   a    {  }  \"  ,    className ,    cls . getName (  )  )  ;", "}    catch    ( ClassNotFoundException   e )     {", "}", "return   null ;", "}", "METHOD_END"], "methodName": ["load"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "String   key    =    path ;", "do    {", ". Dest   dest    =    routes . get ( key )  ;", "if    (  ( dest    !  =    null )     &  &     (  . methodAllowed ( method ,    dest )  )  )     {", "if    (  (  ( Object )     ( key )  )     =  =    path )     {", ". LOG . debug (  \" exact   match   for    {  }  :     {  }  \"  ,    key ,    dest . action )  ;", "return   dest ;", "} else", "if    (  . isGoodMatch ( dest ,    path )  )     {", ". LOG . debug (  \" prefix   match 2    for    {  }  :     {  }  \"  ,    key ,    dest . action )  ;", "return   dest ;", "}", "return   resolveAction ( method ,    dest ,    path )  ;", "}", "Map . Entry < String ,     . Dest >    lower    =    routes . lowerEntry ( key )  ;", "if    ( lower    =  =    null )     {", "return   null ;", "}", "dest    =    lower . getValue (  )  ;", "if    (  . prefixMatches ( dest ,    path )  )     {", "if    (  . methodAllowed ( method ,    dest )  )     {", "if    (  . isGoodMatch ( dest ,    path )  )     {", ". LOG . debug (  \" prefix   match   for    {  }  :     {  }  \"  ,    lower . getKey (  )  ,    dest . action )  ;", "return   dest ;", "}", "return   resolveAction ( method ,    dest ,    path )  ;", "}", "int   slashPos    =    key . lastIndexOf (  '  /  '  )  ;", "key    =     ( slashPos    >     0  )     ?    path . substring (  0  ,    slashPos )     :     \"  /  \"  ;", "} else    {", "key    =     \"  /  \"  ;", "}", "}    while    ( true    )  ;", "}", "METHOD_END"], "methodName": ["lookupRoute"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "return   Router . controllerRe . matcher ( path )  . matches (  )  ;", "}", "METHOD_END"], "methodName": ["maybeController"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "return    ( dest . methods . contains ( method )  )     |  |     (  (  ( dest . methods . size (  )  )     =  =     1  )     &  &     ( dest . methods . contains ( WebApp . HTTP . GET )  )  )  ;", "}", "METHOD_END"], "methodName": ["methodAllowed"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "Router . LOG . debug (  \" checking   prefix    {  }  {  }    for   path :     {  }  \"  ,    new   Object [  ]  {    dest . prefix ,    dest . pathParams ,    path    }  )  ;", "if    (  !  ( path . startsWith ( dest . prefix )  )  )     {", "return   false ;", "}", "int   prefixLen    =    dest . prefix . length (  )  ;", "if    (  (  ( prefixLen    >     1  )     &  &     (  ( path . length (  )  )     >    prefixLen )  )     &  &     (  ( path . charAt ( prefixLen )  )     !  =     '  /  '  )  )     {", "return   false ;", "}", "return   true ;", "}", "METHOD_END"], "methodName": ["prefixMatches"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "WebApp . HTTP   method    =    WebApp . HTTP . valueOf ( httpMethod )  ;", ". Dest   dest    =    lookupRoute ( method ,    path )  ;", "if    ( dest    =  =    null )     {", "return   resolveDefault ( method ,    path )  ;", "}", "return   dest ;", "}", "METHOD_END"], "methodName": ["resolve"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "if    (  ( dest . prefix . length (  )  )     =  =     1  )     {", "return   null ;", "}", "checkState (  (  !  (  . isGoodMatch ( dest ,    path )  )  )  ,    dest . prefix )  ;", "checkState (  (  (  . SLASH . countIn ( path )  )     >     1  )  ,    path )  ;", "List < String >    parts    =    WebApp . parseRoute ( path )  ;", "String   controller    =    parts . get ( WebApp . R _ CONTROLLER )  ;", "String   action    =    parts . get ( WebApp . R _ ACTION )  ;", "return   add ( method ,    StringHelper . pjoin (  \"  \"  ,    controller ,    action )  ,    dest . controllerClass ,    action ,    null )  ;", "}", "METHOD_END"], "methodName": ["resolveAction"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "List < String >    parts    =    WebApp . parseRoute ( path )  ;", "String   controller    =    parts . get ( WebApp . R _ CONTROLLER )  ;", "String   action    =    parts . get ( WebApp . R _ ACTION )  ;", "Class <  ?    extends   Controller >    cls    =    find ( Controller . class ,    StringHelper . join ( controller ,     \" Controller \"  )  )  ;", "if    ( cls    =  =    null )     {", "cls    =    find ( Controller . class ,    controller )  ;", "}", "if    ( cls    =  =    null )     {", "throw   new   WebAppException ( StringHelper . join ( path ,     \"  :    controller   for    \"  ,    controller ,     \"    not   found \"  )  )  ;", "}", "return   add ( method ,    defaultPrefix ( controller ,    action )  ,    cls ,    action ,    null )  ;", "}", "METHOD_END"], "methodName": ["resolveDefault"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "hostClass    =    cls ;", "}", "METHOD_END"], "methodName": ["setHostClass"], "fileName": "org.apache.hadoop.yarn.webapp.Router"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo \"  ,     \" foo \"  ,     \" index \"  )  ,    WebApp . parseRoute (  \"  / foo \"  )  )  ;", "assertEquals ( Arrays . asList (  \"  / foo \"  ,     \" foo \"  ,     \" index \"  )  ,    WebApp . parseRoute (  \"  / foo /  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultAction"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  /  \"  ,     \" default \"  ,     \" index \"  ,     \"  : a \"  )  ,    WebApp . parseRoute (  \"  /  : a \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultCapture"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  /  \"  ,     \" default \"  ,     \" index \"  )  ,    WebApp . parseRoute (  \"  /  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testDefaultController"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo / action \"  ,     \" foo \"  ,     \" action \"  ,     \"  : a \"  )  ,    WebApp . parseRoute (  \"     / foo / action /     : a \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testLeadingPaddings"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo \"  ,     \" foo \"  ,     \" index \"  ,     \"  : a 1  \"  )  ,    WebApp . parseRoute (  \"  / foo /  : a 1  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testMissingAction"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "WebApp . parseRoute (  \" foo / bar \"  )  ;", "}", "METHOD_END"], "methodName": ["testMissingLeadingSlash"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo / action \"  ,     \" foo \"  ,     \" action \"  ,     \"  : a 1  \"  ,     \"  : a 2  \"  )  ,    WebApp . parseRoute (  \"  / foo / action /  : a 1  /  : a 2  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testNormalAction"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo / action / bar \"  ,     \" foo \"  ,     \" action \"  ,     \" bar \"  ,     \"  : a \"  )  ,    WebApp . parseRoute (  \"  / foo / action / bar /  : a \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testPartialCapture1"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo / action \"  ,     \" foo \"  ,     \" action \"  ,     \"  : a 1  \"  ,     \" bar \"  ,     \"  : a 2  \"  ,     \"  : a 3  \"  )  ,    WebApp . parseRoute (  \"  / foo / action /  : a 1  / bar /  : a 2  /  : a 3  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testPartialCapture2"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "assertEquals ( Arrays . asList (  \"  / foo / action \"  ,     \" foo \"  ,     \" action \"  ,     \"  : a \"  )  ,    WebApp . parseRoute (  \"  / foo / action /  /  : a    /     \"  )  )  ;", "assertEquals ( Arrays . asList (  \"  / foo / action \"  ,     \" foo \"  ,     \" action \"  )  ,    WebApp . parseRoute (  \"  / foo / action    /     \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testTrailingPaddings"], "fileName": "org.apache.hadoop.yarn.webapp.TestParseRoute"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( this )  ;", "injector . getInstance (  . MainView . class )  . render (  )  ;", "PrintWriter   out    =    injector . getInstance ( HttpServletResponse . class )  . getWriter (  )  ;", "out . flush (  )  ;", "verify ( out )  . print (  \" sub 1    text \"  )  ;", "verify ( out )  . print (  \" sub 2    text \"  )  ;", "verify ( out ,    times (  1  6  )  )  . println (  )  ;", "}", "METHOD_END"], "methodName": ["testSubView"], "fileName": "org.apache.hadoop.yarn.webapp.TestSubViews"}, {"methodBody": ["METHOD_START", "{", "return    (  \" http :  /  / localhost :  \"     +     ( app . port (  )  )  )     +     \"  /  \"  ;", "}", "METHOD_END"], "methodName": ["baseUrl"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "return   s ;", "}", "METHOD_END"], "methodName": ["echo"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "try    {", "StringBuilder   out    =    new   StringBuilder (  )  ;", "InputStream   in    =    new   URL ( url )  . openConnection (  )  . getInputStream (  )  ;", "byte [  ]    buffer    =    new   byte [  6  4     *     1  0  2  4  ]  ;", "int   len    =    in . read ( buffer )  ;", "while    ( len    >     0  )     {", "outpend ( new   String ( buffer ,     0  ,    len )  )  ;", "len    =    in . read ( buffer )  ;", "}", "return   out . toString (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getContent"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "try    {", "HttpURLConnection   c    =     (  ( HttpURLConnection )     ( new   URL ( url )  . enConnection (  )  )  )  ;", "return   c . getResponseCode (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["getResponseCode"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApps .  $ for (  \" test \"  ,    new   TestWebApp (  )  )  . at (  8  8  8  8  )  . inDevMode (  )  . start (  )  . joinThread (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for ( this )  . start (  )  ;", "app . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testCreate"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for ( this )  . at (  \"  0  .  0  .  0  .  0  :  5  0  0  0  0  \"  )  . start (  )  ;", "int   port    =    app . getListenerAddress (  )  . getPort (  )  ;", "assertEquals (  5  0  0  0  0  ,    port )  ;", "WebApp   app 2     =    WebApps .  $ for ( this )  . at (  \"  0  .  0  .  0  .  0  :  5  0  0  0  0  \"  )  . start (  )  ;", "app . stop (  )  ;", "app 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testCreateWithBindAddressNonZeroPort"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for ( this )  . at (  5  0  0  0  0  )  . start (  )  ;", "int   port    =    app . getListenerAddress (  )  . getPort (  )  ;", "assertEquals (  5  0  0  0  0  ,    port )  ;", "WebApp   app 2     =    WebApps .  $ for ( this )  . at (  5  0  0  0  0  )  . start (  )  ;", "app . stop (  )  ;", "app 2  . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testCreateWithNonZeroPort"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for ( this )  . at (  0  )  . start (  )  ;", "int   port    =    app . getListenerAddress (  )  . getPort (  )  ;", "assertTrue (  ( port    >     0  )  )  ;", "app . stop (  )  ;", "app    =    WebApps .  $ for ( this )  . at ( port )  . start (  )  ;", "assertEquals ( port ,    app . getListenerAddress (  )  . getPort (  )  )  ;", "app . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testCreateWithPort"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for (  \" test \"  ,    TestWebApp . class ,    this ,     \" ws \"  )  . start ( new   WebApp (  )     {", "@ Override", "public   void   setup (  )     {", "bind ( MyTestJAXBContextResolver . class )  ;", "bind ( MyTestWebService . class )  ;", "route (  \"  /  : foo \"  ,    TestWebApp . FooController . class )  ;", "route (  \"  / bar / foo \"  ,    TestWebApp . FooController . class ,     \" bar \"  )  ;", "route (  \"  / foo /  : foo \"  ,    TestWebApp . DefaultController . class )  ;", "route (  \"  / foo / bar /  : foo \"  ,    TestWebApp . DefaultController . class ,     \" index \"  )  ;", "}", "}  )  ;", "String   baseUrl    =    TestWebApp . baseUrl ( app )  ;", "try    {", "assertEquals (  \" foo \"  ,    TestWebApp . getContent ( baseUrl )  . trim (  )  )  ;", "assertEquals (  \" foo \"  ,    TestWebApp . getContent (  ( baseUrl    +     \" test \"  )  )  . trim (  )  )  ;", "assertEquals (  \" foo 1  \"  ,    TestWebApp . getContent (  ( baseUrl    +     \" test /  1  \"  )  )  . trim (  )  )  ;", "assertEquals (  \" bar \"  ,    TestWebApp . getContent (  ( baseUrl    +     \" test / bar / foo \"  )  )  . trim (  )  )  ;", "assertEquals (  \" default \"  ,    TestWebApp . getContent (  ( baseUrl    +     \" test / foo / bar \"  )  )  . trim (  )  )  ;", "assertEquals (  \" default 1  \"  ,    TestWebApp . getContent (  ( baseUrl    +     \" test / foo /  1  \"  )  )  . trim (  )  )  ;", "assertEquals (  \" default 2  \"  ,    TestWebApp . getContent (  ( baseUrl    +     \" test / foo / bar /  2  \"  )  )  . trim (  )  )  ;", "assertEquals (  4  0  4  ,    TestWebApp . getResponseCode (  ( baseUrl    +     \" test / goo \"  )  )  )  ;", "assertEquals (  2  0  0  ,    TestWebApp . getResponseCode (  ( baseUrl    +     \" ws / v 1  / test \"  )  )  )  ;", "assertTrue ( TestWebApp . getContent (  ( baseUrl    +     \" ws / v 1  / test \"  )  )  . contains (  \" myInfo \"  )  )  ;", "}    finally    {", "app . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testCustomRoutes"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for (  \" test \"  ,    this )  . start (  )  ;", "String   baseUrl    =     . baseUrl ( app )  ;", "try    {", "assertEquals (  \" foo \"  ,     . getContent (  ( baseUrl    +     \" test / foo \"  )  )  . trim (  )  )  ;", "assertEquals (  \" foo \"  ,     . getContent (  ( baseUrl    +     \" test / foo / index \"  )  )  . trim (  )  )  ;", "assertEquals (  \" bar \"  ,     . getContent (  ( baseUrl    +     \" test / foo / bar \"  )  )  . trim (  )  )  ;", "assertEquals (  \" default \"  ,     . getContent (  ( baseUrl    +     \" test \"  )  )  . trim (  )  )  ;", "assertEquals (  \" default \"  ,     . getContent (  ( baseUrl    +     \" test /  \"  )  )  . trim (  )  )  ;", "assertEquals (  \" default \"  ,     . getContent ( baseUrl )  . trim (  )  )  ;", "}    finally    {", "app . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testDefaultRoutes"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for (  \" test \"  ,    this )  . start (  )  ;", "assertEquals (  \"  / test \"  ,    app . getRedirectPath (  )  )  ;", "String [  ]    expectedPaths    =    new   String [  ]  {     \"  / test \"  ,     \"  / test /  *  \"     }  ;", "String [  ]    pathSpecs    =    app . getServePathSpecs (  )  ;", "assertEquals (  2  ,    pathSpecs . length )  ;", "for    ( int   i    =     0  ;    i    <     ( expectedPaths . length )  ;    i +  +  )     {", "assertTrue ( ArrayUtils . contains ( pathSpecs ,    expectedPaths [ i ]  )  )  ;", "}", "app . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testServePaths"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "WebApp   app    =    WebApps .  $ for (  \"  \"  ,    this )  . start (  )  ;", "assertEquals (  \"  /  \"  ,    app . getRedirectPath (  )  )  ;", "String [  ]    expectedPaths    =    new   String [  ]  {     \"  /  *  \"     }  ;", "String [  ]    pathSpecs    =    app . getServePathSpecs (  )  ;", "assertEquals (  1  ,    pathSpecs . length )  ;", "for    ( int   i    =     0  ;    i    <     ( expectedPaths . length )  ;    i +  +  )     {", "assertTrue ( ArrayUtils . contains ( pathSpecs ,    expectedPaths [ i ]  )  )  ;", "}", "app . stop (  )  ;", "}", "METHOD_END"], "methodName": ["testServePathsNoName"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "System . setProperty (  \" hadoop . log . dir \"  ,     \"  / Not / Existing / dir \"  )  ;", "WebApp   app    =    WebApps .  $ for (  \" test \"  ,    this )  . start ( new   WebApp (  )     {", "@ Override", "public   void   setup (  )     {", "route (  \"  /  \"  ,     . FooController . class )  ;", "}", "}  )  ;", "String   baseUrl    =     . baseUrl ( app )  ;", "try    {", "assertFalse (  \" foo \"  . equals (  . getContent (  ( baseUrl    +     \" static \"  )  )  . trim (  )  )  )  ;", "assertEquals (  4  0  4  ,     . getResponseCode (  ( baseUrl    +     \" logs \"  )  )  )  ;", "assertEquals (  \" foo \"  ,     . getContent ( baseUrl )  . trim (  )  )  ;", "}    finally    {", "app . stop (  )  ;", "}", "}", "METHOD_END"], "methodName": ["testYARNWebAppContext"], "fileName": "org.apache.hadoop.yarn.webapp.TestWebApp"}, {"methodBody": ["METHOD_START", "{", "return    $  ( key ,     \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["$"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "String   value    =    moreParams (  )  . get ( key )  ;", "if    ( value    =  =    null )     {", "value    =    request (  )  . getParameter ( key )  ;", "}", "return   value    =  =    null    ?    defaultValue    :    value ;", "}", "METHOD_END"], "methodName": ["$"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "if    (  ( vc )     =  =    null )     {", "if    (  ( injector )     =  =    null )     {", "throw   new   WebAppException ( StringHelper . join (  \" Error   accessingContext   from   a \\ n \"  ,     \" child   constructor ,    either   move   the   usage   of   the   methods \\ n \"  ,     \" out   of   the   constructor   or   inject   theContext   into   the \\ n \"  ,     \" constructor \"  )  )  ;", "}", "vc    =    injector . getInstanceContext . class )  ;", "}", "return   vc ;", "}", "METHOD_END"], "methodName": ["context"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . cookies (  )  ;", "}", "METHOD_END"], "methodName": ["cookies"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . error ;", "}", "METHOD_END"], "methodName": ["error"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   injector (  )  . getInstance ( cls )  ;", "}", "METHOD_END"], "methodName": ["getInstance"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . devMode ;", "}", "METHOD_END"], "methodName": ["inDevMode"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   getInstance ( ResponseInfo . class )  . about ( about )  ;", "}", "METHOD_END"], "methodName": ["info"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . injector ;", "}", "METHOD_END"], "methodName": ["injector"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . moreParams (  )  ;", "}", "METHOD_END"], "methodName": ["moreParams"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "try    {", "retu   response (  )  . getOutputStream (  )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebAppException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["outputStream"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "if    (  ( context (  )  . rc . prefix )     =  =    null )     {", "retu   root (  )  ;", "} else    {", "retu   StringHelper . ujoin ( root (  )  ,    context (  )  . rc . prefix )  ;", "}", "}", "METHOD_END"], "methodName": ["prefix"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "int   saved    =    context (  )  . nestLevel ;", "getInstance ( cls )  . renderPartial (  )  ;", "if    (  ( context (  )  . nestLevel )     !  =    saved )     {", "throw   new   WebAppException (  (  (  \"     \"     +     ( cls . getSimpleName (  )  )  )     +     \"    not   complete \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["render"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . request ;", "}", "METHOD_END"], "methodName": ["request"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . response ;", "}", "METHOD_END"], "methodName": ["response"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "String   root    =    System . getenv ( APPLICATION _ WEB _ PROXY _ BASE _ ENV )  ;", "if    (  ( root    =  =    null )     |  |     ( root . isEmpty (  )  )  )     {", "root    =     \"  /  \"  ;", "}", "return   root ;", "}", "METHOD_END"], "methodName": ["root"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . ujoin ( root (  )  ,    parts )  ;", "}", "METHOD_END"], "methodName": ["root_url"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "moreParams (  )  . put ( key ,    value )  ;", "}", "METHOD_END"], "methodName": ["set"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "set ( Params . TITLE ,    title )  ;", "}", "METHOD_END"], "methodName": ["setTitle"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "setTitle ( title )  ;", "set ( Pams . TITLE _ LINK ,    url )  ;", "}", "METHOD_END"], "methodName": ["setTitle"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   context (  )  . rc . status ;", "}", "METHOD_END"], "methodName": ["status"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . ujoin ( prefix (  )  ,    parts )  ;", "}", "METHOD_END"], "methodName": ["url"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "try    {", "retu   response (  )  . getWriter (  )  ;", "}    catch    ( IOException   e )     {", "throw   new   WebAppException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["writer"], "fileName": "org.apache.hadoop.yarn.webapp.View"}, {"methodBody": ["METHOD_START", "{", "this . servePathSpecs . add ( path )  ;", "}", "METHOD_END"], "methodName": ["addServePathSpec"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   conf ;", "}", "METHOD_END"], "methodName": ["conf"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "if    (  ( this . wsName )     !  =    null )     {", "String   regex    =     (  \"  (  ?  !  /  \"     +     ( this . wsName )  )     +     \"  )  \"  ;", "serveRegex ( regex )  . with ( DefaultWrapperServlet . class )  ;", "Map < String ,    String >    params    =    new   HashMap < String ,    String >  (  )  ;", "params . put ( FEATURE _ IMPLICIT _ VIEWABLES ,     \" true \"  )  ;", "params . put ( FEATURE _ FILTER _ FORWARD _ ON _  4  0  4  ,     \" true \"  )  ;", "params . put ( FEATURE _ XMLROOTELEMENT _ PROCESSING ,     \" true \"  )  ;", "params . put ( PROPERTY _ CONTAINER _ REQUEST _ FILTERS ,    GZIPContentEncodingFilter . class . getName (  )  )  ;", "params . put ( PROPERTY _ CONTAINER _ RESPONSE _ FILTERS ,    GZIPContentEncodingFilter . class . getName (  )  )  ;", "filter (  \"  /  *  \"  )  . through ( getFilterClass (  )  ,    params )  ;", "}", "}", "METHOD_END"], "methodName": ["configureWebAppServlets"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   checkNotNull ( httpServer ,     \" httpServer \"  )  . getConnectorAddress (  0  )  ;", "}", "METHOD_END"], "methodName": ["getListenerAddress"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "int   start    =     0  ;", "while    ( WHITESPACE . matches ( pathSpec . charAt ( start )  )  )     {", "+  + start ;", "}", "if    (  ( pathSpec . charAt ( start )  )     !  =     '  /  '  )     {", "throw   new   Exception (  (  \" Path   spec   syntax   error :     \"     +    pathSpec )  )  ;", "}", "int   ci    =    pathSpec . indexOf (  '  :  '  )  ;", "if    ( ci    =  =     (  -  1  )  )     {", "ci    =    pathSpec . length (  )  ;", "}", "if    ( ci    =  =     1  )     {", "return    \"  /  \"  ;", "}", "char   c ;", "do    {", "c    =    pathSpec . charAt (  (  -  - ci )  )  ;", "}    while    (  ( c    =  =     '  /  '  )     |  |     ( WHITESPACE . matches ( c )  )     )  ;", "return   pathSpec . substring ( start ,     ( ci    +     1  )  )  ;", "}", "METHOD_END"], "methodName": ["getPrefix"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   this . redirectPath ;", "}", "METHOD_END"], "methodName": ["getRedirectPath"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   this . servePathSpecs . toArray ( new   String [ this . servePathSpecs . size (  )  ]  )  ;", "}", "METHOD_END"], "methodName": ["getServePathSpecs"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   GuiceContainer . class ;", "}", "METHOD_END"], "methodName": ["getWebAppFilterClass"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   httpServer ;", "}", "METHOD_END"], "methodName": ["httpServer"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "try    {", "checkNotNull ( httpServer ,     \" httpServer \"  )  . join (  )  ;", "}    catch    ( InterruptedException   e )     {", ". LOG . info (  \" interrupted \"  ,    e )  ;", "}", "}", "METHOD_END"], "methodName": ["joinThread"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   this . name ;", "}", "METHOD_END"], "methodName": ["name"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "List < String >    result    =    Lists . newArrayList (  )  ;", "result . add (  . getPrefix ( checkNotNull ( pathSpec ,     \" pathSpec \"  )  )  )  ;", "Iterable < String >    parts    =     . pathSplitter . split ( pathSpec )  ;", "String   controller    =    null ;", "String   action    =    null ;", "for    ( String   s    :    parts )     {", "if    ( controller    =  =    null )     {", "if    (  ( s . charAt (  0  )  )     =  =     '  :  '  )     {", "controller    =     \" default \"  ;", "result . add ( controller )  ;", "action    =     \" index \"  ;", "result . add ( action )  ;", "} else    {", "controller    =    s ;", "}", "} else", "if    ( action    =  =    null )     {", "if    (  ( s . charAt (  0  )  )     =  =     '  :  '  )     {", "action    =     \" index \"  ;", "result . add ( action )  ;", "} else    {", "action    =    s ;", "}", "}", "result . add ( s )  ;", "}", "if    ( controller    =  =    null )     {", "result . add (  \" default \"  )  ;", "}", "if    ( action    =  =    null )     {", "result . add (  \" index \"  )  ;", "}", "return   result ;", "}", "METHOD_END"], "methodName": ["parseRoute"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   addr    =    checkNotNull ( httpServer ,     \" httpServer \"  )  . getConnectorAddress (  0  )  ;", "return   addr    =  =    null    ?     -  1     :    addr . getPort (  )  ;", "}", "METHOD_END"], "methodName": ["port"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "List < String >    res    =    WebApp . parseRoute ( pathSpec )  ;", "router . add ( WebApp . HTTP . GET ,    res . get ( WebApp . R _ PATH )  ,    cls ,    res . get ( WebApp . R _ ACTION )  ,    res . subList ( WebApp . R _ PARAMS ,    res . size (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["route"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "route ( WebApp . HTTP . GET ,    pathSpec ,    cls ,    action )  ;", "}", "METHOD_END"], "methodName": ["route"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "List < String >    res    =    WebApp . parseRoute ( pathSpec )  ;", "router . add ( method ,    res . get ( WebApp . R _ PATH )  ,    cls ,    action ,    res . subList ( WebApp . R _ PARAMS ,    res . size (  )  )  )  ;", "}", "METHOD_END"], "methodName": ["route"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   router ;", "}", "METHOD_END"], "methodName": ["router"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "this . conf    =    conf ;", "}", "METHOD_END"], "methodName": ["setConf"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "guiceFilter    =    instance ;", "}", "METHOD_END"], "methodName": ["setGuiceFilter"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "router . setHostClass ( cls )  ;", "}", "METHOD_END"], "methodName": ["setHostClass"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "httpServer    =    checkNotNull ( server ,     \" http   server \"  )  ;", "}", "METHOD_END"], "methodName": ["setHttpServer"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "this . name    =    name ;", "}", "METHOD_END"], "methodName": ["setName"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "this . redirectPath    =    path ;", "}", "METHOD_END"], "methodName": ["setRedirectPath"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "this . wsName    =    name ;", "}", "METHOD_END"], "methodName": ["setWebServices"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "try    {", "checkNotNull ( httpServer ,     \" httpServer \"  )  . stop (  )  ;", "checkNotNull ( guiceFilter ,     \" guiceFilter \"  )  . destroy (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   Exception ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["stop"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   this ;", "}", "METHOD_END"], "methodName": ["webApp"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   this . wsName ;", "}", "METHOD_END"], "methodName": ["wsName"], "fileName": "org.apache.hadoop.yarn.webapp.WebApp"}, {"methodBody": ["METHOD_START", "{", "return   WebApps .  $ for (  \"  \"  ,    app )  ;", "}", "METHOD_END"], "methodName": ["$for"], "fileName": "org.apache.hadoop.yarn.webapp.WebApps"}, {"methodBody": ["METHOD_START", "{", "return   WebApps .  $ for ( prefix ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["$for"], "fileName": "org.apache.hadoop.yarn.webapp.WebApps"}, {"methodBody": ["METHOD_START", "{", "return   WebApps .  $ for ( prefix ,     (  ( Class < T >  )     ( app . getClass (  )  )  )  ,    app )  ;", "}", "METHOD_END"], "methodName": ["$for"], "fileName": "org.apache.hadoop.yarn.webapp.WebApps"}, {"methodBody": ["METHOD_START", "{", "return   new   WebApps . Builder < T >  ( prefix ,    api ,    app )  ;", "}", "METHOD_END"], "methodName": ["$for"], "fileName": "org.apache.hadoop.yarn.webapp.WebApps"}, {"methodBody": ["METHOD_START", "{", "return   new   WebApps . Builder < T >  ( prefix ,    api ,    app ,    wsPrefix )  ;", "}", "METHOD_END"], "methodName": ["$for"], "fileName": "org.apache.hadoop.yarn.webapp.WebApps"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  (  (  ( print    +     \"    doesn ' t   contain   expected   string ,    got :     \"  )     +    got )     +     \"    expected :     \"  )     +    expected )  ,    got . contains ( expected )  )  ;", "}", "METHOD_END"], "methodName": ["checkStringContains"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  (  (  ( print    +     \"    is   not   equal ,    got :     \"  )     +    got )     +     \"    expected :     \"  )     +    expected )  ,    got . equals ( expected )  )  ;", "}", "METHOD_END"], "methodName": ["checkStringEqual"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "assertTrue (  (  (  (  ( print    +     \"    doesn ' t   match ,    got :     \"  )     +    got )     +     \"    expected :     \"  )     +    expected )  ,    got . matches ( expected )  )  ;", "}", "METHOD_END"], "methodName": ["checkStringMatch"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "Attr   at    =    element . getAttributeNode ( name )  ;", "if    ( at    !  =    null )     {", "retu   at . getValue (  )  ;", "}", "retu   null ;", "}", "METHOD_END"], "methodName": ["getXmlAttrString"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   val    =    WebServicesTestUtils . getXmlString ( element ,    name )  ;", "return   Boolean . parseBoolean ( val )  ;", "}", "METHOD_END"], "methodName": ["getXmlBoolean"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   val    =    WebServicesTestUtils . getXmlString ( element ,    name )  ;", "return   Float . parseFloat ( val )  ;", "}", "METHOD_END"], "methodName": ["getXmlFloat"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   val    =    WebServicesTestUtils . getXmlString ( element ,    name )  ;", "return   Integer . parseInt ( val )  ;", "}", "METHOD_END"], "methodName": ["getXmlInt"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "String   val    =    WebServicesTestUtils . getXmlString ( element ,    name )  ;", "return   Long . parseLong ( val )  ;", "}", "METHOD_END"], "methodName": ["getXmlLong"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "NodeList   id    =    element . getElementsByTagName ( name )  ;", "Element   line    =     (  ( Element )     ( id . item (  0  )  )  )  ;", "if    ( line    =  =    null )     {", "return   null ;", "}", "Node   first    =    line . getFirstChild (  )  ;", "if    ( first    =  =    null )     {", "return    \"  \"  ;", "}", "String   val    =    first . getNodeValue (  )  ;", "if    ( val    =  =    null )     {", "return    \"  \"  ;", "}", "return   val ;", "}", "METHOD_END"], "methodName": ["getXmlString"], "fileName": "org.apache.hadoop.yarn.webapp.WebServicesTestUtils"}, {"methodBody": ["METHOD_START", "{", "AnnotationIntrospector   introspector    =    new   JaxbAnnotationIntrospector (  )  ;", "mapper . setAnnotationIntrospector ( introspector )  ;", "mapper . setSerializationInclusion ( NON _ NULL )  ;", "}", "METHOD_END"], "methodName": ["configObjectMapper"], "fileName": "org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider"}, {"methodBody": ["METHOD_START", "{", "WebApps .  $ for ( new   HelloWorld (  )  )  . at (  8  8  8  8  )  . inDevMode (  )  . start (  )  . joinThread (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.webapp.example.HelloWorld"}, {"methodBody": ["METHOD_START", "{", "return    \" anything ,    really !  \"  ;", "}", "METHOD_END"], "methodName": ["anyAPI"], "fileName": "org.apache.hadoop.yarn.webapp.example.MyApp"}, {"methodBody": ["METHOD_START", "{", "WebApps .  $ for ( new   MyApp (  )  )  . at (  8  8  8  8  )  . inDevMode (  )  . start (  )  . joinThread (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.webapp.example.MyApp"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . A < T >  (  \" a \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["a_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . ABBR < T >  (  \" abbr \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["abbr_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . ADDRESS < T >  (  \" address \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["address_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . AREA < T >  (  \" area \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["area_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . B < T >  (  \" b \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["b_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BASE < T >  (  \" base \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["base_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BDO < T >  (  \" bdo \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["bdo_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BLOCKQUOTE < T >  (  \" blockquote \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["blockquote_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BODY < T >  (  \" body \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["body_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BLOCKQUOTE < T >  (  \" blockquote \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["bq_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BR < T >  (  \" br \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["br_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . BUTTON < T >  (  \" button \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["button_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . CAPTION < T >  (  \" caption \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["caption_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . CITE < T >  (  \" cite \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["cite_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . CODE < T >  (  \" code \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["code_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . COL < T >  (  \" col \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["col_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . COLGROUP < T >  (  \" colgroup \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["colgroup_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . DD < T >  (  \" dd \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["dd_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . DEL < T >  (  \" del \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["del_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . DFN < T >  (  \" dfn \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["dfn_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . DIV < T >  (  \" div \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["div_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . DL < T >  (  \" dl \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["dl_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . DT < T >  (  \" dt \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["dt_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . EM < T >  (  \" em \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["em_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . FIELDSET < T >  (  \" fieldset \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["fieldset_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . FORM < T >  (  \" form \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["form_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . H 1  < T >  (  \" h 1  \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["h1_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . H 2  < T >  (  \" h 2  \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["h2_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . H 3  < T >  (  \" h 3  \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["h3_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . H 4  < T >  (  \" h 4  \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["h4_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . H 5  < T >  (  \" h 5  \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["h5_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . H 6  < T >  (  \" h 6  \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["h6_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . HEAD < T >  (  \" head \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["head_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . HR < T >  (  \" hr \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["hr_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . I < T >  (  \" i \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["i_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . IMG < T >  (  \" img \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["img_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . INPUT < T >  (  \" input \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["input_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . INS < T >  (  \" ins \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["ins_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . KBD < T >  (  \" kbd \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["kbd_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . LABEL < T >  (  \" label \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["label_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . LEGEND < T >  (  \" legend \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["legend_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . LI < T >  (  \" li \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["li_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . LINK < T >  (  \" link \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["link_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . MAP < T >  (  \" map \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["map_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . META < T >  (  \" meta \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["meta_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . OBJECT < T >  (  \" object \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["object_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . OL < T >  (  \" ol \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["ol_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "EnumSet < HamletImpl . EOpt >    opts    =    EnumSet . of ( HamletImpl . EOpt . ENDTAG )  ;", "if    (  ! endTag )", "opts . remove ( HamletImpl . EOpt . ENDTAG )  ;", "if    ( inline )", "opts . add ( HamletImpl . EOpt . INLINE )  ;", "if    ( pre )", "opts . add ( HamletImpl . EOpt . PRE )  ;", "return   opts ;", "}", "METHOD_END"], "methodName": ["opt"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . OPTGROUP < T >  (  \" optgroup \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["optgroup_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . OPTION < T >  (  \" option \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["option_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . P < T >  (  \" p \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["p_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . PARAM < T >  (  \" param \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["param_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . PRE < T >  (  \" pre \"  ,    e ,    Hamlet . opt ( true ,    inline ,    true )  )  ;", "}", "METHOD_END"], "methodName": ["pre_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . Q < T >  (  \" q \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["q_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SAMP < T >  (  \" samp \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["samp_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SCRIPT < T >  (  \" script \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["script_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SELECT < T >  (  \" select \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["select_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SMALL < T >  (  \" small \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["small_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SPAN < T >  (  \" span \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["span_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . STRONG < T >  (  \" strong \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["strong_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . STYLE < T >  (  \" style \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["style_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SUB < T >  (  \" sub \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["sub_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . SUP < T >  (  \" sup \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["sup_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TABLE < T >  (  \" table \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["table_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TBODY < T >  (  \" tbody \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["tbody_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TD < T >  (  \" td \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["td_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TEXTAREA < T >  (  \" textarea \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["textarea_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TFOOT < T >  (  \" tfoot \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["tfoot_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TH < T >  (  \" th \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["th_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . THEAD < T >  (  \" thead \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["thead_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TITLE < T >  (  \" title \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["title_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . TR < T >  (  \" tr \"  ,    e ,    Hamlet . opt ( false ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["tr_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . UL < T >  (  \" ul \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["ul_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   new   Hamlet . VAR < T >  (  \" var \"  ,    e ,    Hamlet . opt ( true ,    inline ,    false )  )  ;", "}", "METHOD_END"], "methodName": ["var_"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.Hamlet"}, {"methodBody": ["METHOD_START", "{", "return   path . substring (  (  ( path . lastIndexOf (  '  /  '  )  )     +     1  )  )  ;", "}", "METHOD_END"], "methodName": ["basename"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "String   prev    =    null ;", "f    ( Object   o    :    args )     {", "String   s    =    String . valueOf ( o )  ;", "if    (  (  (  !  ( s . isEmpty (  )  )  )     &  &     (  !  ( s . equals (  \"  \\ n \"  )  )  )  )     &  &     (  ( prev    =  =    null )     |  |     ( prev . endsWith (  \"  \\ n \"  )  )  )  )     {", "indent ( indent )  ;", "}", "prev    =    s ;", "out . print ( s )  ;", "bytes    +  =    s . length (  )  ;", "}", "}", "METHOD_END"], "methodName": ["echo"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "String   methodName    =    method . getName (  )  ;", "String   attrName    =    methodName . substring (  1  )  . replace (  '  _  '  ,     '  -  '  )  ;", "Type [  ]    params    =    method . getGenericParameterTypes (  )  ;", "echo ( indent ,     \"  \\ n \"  ,     \"  @ Override \\ n \"  ,     \" public    \"  ,    className ,     ( topMode    ?     \"     \"     :     \"  < T >     \"  )  ,    methodName ,     \"  (  \"  )  ;", "if    (  ( params . length )     =  =     0  )     {", "puts (  0  ,     \"  )     {  \"  )  ;", "puts ( indent ,     \"       addAttr (  \\  \"  \"  ,    attrName ,     \"  \\  \"  ,    null )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    (  ( params . length )     =  =     1  )     {", "String   typeName    =    getTypeName ( params [  0  ]  )  ;", "puts (  0  ,    typeName ,     \"    value )     {  \"  )  ;", "if    ( typeName . equals (  \" EnumSet < LinkType >  \"  )  )     {", "puts ( indent ,     \"       addRelAttr (  \\  \"  \"  ,    attrName ,     \"  \\  \"  ,    value )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( typeName . equals (  \" EnumSet < Media >  \"  )  )     {", "puts ( indent ,     \"       addMediaAttr (  \\  \"  \"  ,    attrName ,     \"  \\  \"  ,    value )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "} else    {", "puts ( indent ,     \"       addAttr (  \\  \"  \"  ,    attrName ,     \"  \\  \"  ,    value )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "}", "} else    {", ". throwUnhandled ( className ,    method )  ;", "}", "}", "METHOD_END"], "methodName": ["genAttributeMethod"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "String   methodName    =    method . getName (  )  ;", "Class <  ?  >  [  ]    params    =    method . getParameterTypes (  )  ;", "if    (  ( topMode )     |  |     (  ( params . length )     >     0  )  )     {", "echo ( indent ,     \"  \\ n \"  ,     \"  @ Override \\ n \"  ,     \" public    \"  ,    className ,     ( topMode    ?     \"     \"     :     \"  < T >     \"  )  ,    methodName ,     \"  (  \"  )  ;", "}", "if    (  ( params . length )     =  =     0  )     {", "if    ( topMode )     {", "puts (  0  ,     \"  )     {  \"  )  ;", "puts ( indent ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "}", "} else", "if    (  ( params . length )     =  =     1  )     {", "if    ( methodName . equals (  \" base \"  )  )     {", "puts (  0  ,     \" String   href )     {  \"  )  ;", "puts ( indent ,     \"       return   base (  )  .  $ href ( href )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" script \"  )  )     {", "puts (  0  ,     \" String   src )     {  \"  )  ;", "puts ( indent ,     \"       return   setScriptSrc ( script (  )  ,    src )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" style \"  )  )     {", "puts (  0  ,     \" Object .  .  .    lines )     {  \"  )  ;", "puts ( indent ,     \"       return   style (  )  .  $ type (  \\  \" text / css \\  \"  )  .  _  ( lines )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" img \"  )  )     {", "puts (  0  ,     \" String   src )     {  \"  )  ;", "puts ( indent ,     \"       return    \"  ,    methodName ,     \"  (  )  .  $ src ( src )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    (  (  ( methodName . equals (  \" br \"  )  )     |  |     ( methodName . equals (  \" hr \"  )  )  )     |  |     ( methodName . equals (  \" col \"  )  )  )     {", "puts (  0  ,     \" String   selector )     {  \"  )  ;", "puts ( indent ,     \"       return   setSelector (  \"  ,    methodName ,     \"  (  )  ,    selector )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" link \"  )  )     {", "puts (  0  ,     \" String   href )     {  \"  )  ;", "puts ( indent ,     \"       return   setLinkHref (  \"  ,    methodName ,     \"  (  )  ,    href )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \"  _  \"  )  )     {", "if    ( params [  0  ]  . getSimpleName (  )  . equals (  \" Class \"  )  )     {", "puts (  0  ,     \" Class <  ?    extends   SubView >    cls )     {  \"  )  ;", "puts ( indent ,     \"        \"  ,     ( topMode    ?     \" subView \"     :     \"  _ v \"  )  ,     \"  ( cls )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "} else    {", "puts (  0  ,     \" Object .  .  .    lines )     {  \"  )  ;", "puts ( indent ,     \"        _ p (  \"  ,     . needsEscaping ( className )  ,     \"  ,    lines )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "}", "} else", "if    ( methodName . equals (  \"  _ r \"  )  )     {", "puts (  0  ,     \" Object .  .  .    lines )     {  \"  )  ;", "puts ( indent ,     \"        _ p ( false ,    lines )  ;  \\ n \"  ,     \"       return   this ;  \\ n \"  ,     \"  }  \"  )  ;", "} else    {", "puts (  0  ,     \" String   cdata )     {  \"  )  ;", "puts ( indent ,     \"       return    \"  ,    methodName ,     \"  (  )  .  _  ( cdata )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "}", "} else", "if    (  ( params . length )     =  =     2  )     {", "if    ( methodName . equals (  \" meta \"  )  )     {", "puts (  0  ,     \" String   name ,    String   content )     {  \"  )  ;", "puts ( indent ,     \"       return   meta (  )  .  $ name ( name )  .  $ content ( content )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" meta _ http \"  )  )     {", "puts (  0  ,     \" String   header ,    String   content )     {  \"  )  ;", "puts ( indent ,     \"       return   meta (  )  .  $ http _ equiv ( header )  .  $ content ( content )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" a \"  )  )     {", "puts (  0  ,     \" String   href ,    String   anchorText )     {  \"  )  ;", "puts ( indent ,     \"       return   a (  )  .  $ href ( href )  .  _  ( anchorText )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" bdo \"  )  )     {", "puts (  0  ,     \" Dir   dir ,    String   cdata )     {  \"  )  ;", "puts ( indent ,     \"       return   bdo (  )  .  $ dir ( dir )  .  _  ( cdata )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" label \"  )  )     {", "puts (  0  ,     \" String   forId ,    String   cdata )     {  \"  )  ;", "puts ( indent ,     \"       return   label (  )  .  $ for ( forId )  .  _  ( cdata )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    ( methodName . equals (  \" param \"  )  )     {", "puts (  0  ,     \" String   name ,    String   value )     {  \"  )  ;", "puts ( indent ,     \"       return   param (  )  .  $ name ( name )  .  $ value ( value )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else    {", "puts (  0  ,     \" String   selector ,    String   cdata )     {  \"  )  ;", "puts ( indent ,     \"       return   setSelector (  \"  ,    methodName ,     \"  (  )  ,    selector )  .  _  ( cdata )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "}", "} else", "if    (  ( params . length )     =  =     3  )     {", "if    ( methodName . equals (  \" a \"  )  )     {", "puts (  0  ,     \" String   selector ,    String   href ,    String   anchorText )     {  \"  )  ;", "puts ( indent ,     \"       return   setSelector ( a (  )  ,    selector )  \"  ,     \"  .  $ href ( href )  .  _  ( anchorText )  .  _  (  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "}", "} else    {", ". throwUnhandled ( className ,    method )  ;", "}", "}", "METHOD_END"], "methodName": ["genCurElementMethod"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "puts ( indent ,     \"  \\ n \"  ,     \" private    < T   extends    _  >     \"  ,    retName ,     \"  < T >     \"  ,    methodName ,     \"  _  ( T   e ,    boolean   inline )     {  \\ n \"  ,     \"       return   new    \"  ,    retName ,     \"  < T >  (  \\  \"  \"  ,    retName . toLowerCase ( Locale . US )  ,     \"  \\  \"  ,    e ,    opt (  \"  ,     (  !  ( endTagOptional . contains ( retName )  )  )  ,     \"  ,    inline ,     \"  ,    retName . equals (  \" PRE \"  )  ,     \"  )  )  ;     }  \"  )  ;", "}", "METHOD_END"], "methodName": ["genFactoryMethod"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "for    ( Method   method    :    cls . getDeclaredMethods (  )  )     {", "String   retName    =    method . getReturnType (  )  . getSimpleName (  )  ;", "String   methodName    =    method . getName (  )  ;", "if    (  ( methodName . charAt (  0  )  )     =  =     '  $  '  )", "continue ;", "if    (  ( isElement ( retName )  )     &  &     (  ( method . getParameterTypes (  )  . length )     =  =     0  )  )     {", "genFactoryMethod ( retName ,    methodName ,    indent )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["genFactoryMethods"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "String   specName    =    spec . getSimpleName (  )  ;", "for    ( Class <  ?  >    cls    :    spec . getClasses (  )  )     {", "String   className    =    cls . getSimpleName (  )  ;", "if    ( cls . isInterface (  )  )     {", "genFactoryMethods ( cls ,    indent )  ;", "}", "if    ( isElement ( className )  )     {", ". LOG . info (  \" Generating   class    {  }  < T >  \"  ,    className )  ;", "puts ( indent ,     \"  \\ n \"  ,     \" public   class    \"  ,    className ,     \"  < T   extends    _  >  \"  ,     \"    extends   EImp < T >    implements    \"  ,    specName ,     \"  .  \"  ,    className ,     \"     {  \\ n \"  ,     \"       public    \"  ,    className ,     \"  ( String   name ,    T   parent ,  \"  ,     \"    EnumSet < EOpt >    opts )     {  \\ n \"  ,     \"             super ( name ,    parent ,    opts )  ;  \\ n \"  ,     \"        }  \"  )  ;", "genMethods ( className ,    cls ,     ( indent    +     1  )  )  ;", "puts ( indent ,     \"  }  \"  )  ;", "} else", "if    ( className . equals (  \"  _ Html \"  )  )     {", "top    =    cls ;", "}", "}", "}", "METHOD_END"], "methodName": ["genImpl"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "topMode    =     (  ( top )     !  =    null )     &  &     ( cls . equals ( top )  )  ;", "for    ( Method   method    :    cls . getMethods (  )  )     {", "String   retName    =    method . getReturnType (  )  . getSimpleName (  )  ;", "if    (  ( method . getName (  )  . charAt (  0  )  )     =  =     '  $  '  )     {", "genAttributeMethod ( className ,    method ,    indent )  ;", "} else", "if    ( isElement ( retName )  )     {", "genNewElementMethod ( className ,    method ,    indent )  ;", "} else    {", "genCurElementMethod ( className ,    method ,    indent )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["genMethods"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "String   methodName    =    method . getName (  )  ;", "String   retName    =    method . getReturnType (  )  . getSimpleName (  )  ;", "Class <  ?  >  [  ]    params    =    method . getParameterTypes (  )  ;", "echo ( indent ,     \"  \\ n \"  ,     \"  @ Override \\ n \"  ,     \" public    \"  ,    retName ,     \"  <  \"  ,    className ,     ( topMode    ?     \"  >     \"     :     \"  < T >  >     \"  )  ,    methodName ,     \"  (  \"  )  ;", "if    (  ( params . length )     =  =     0  )     {", "puts (  0  ,     \"  )     {  \"  )  ;", "puts ( indent ,     ( topMode    ?     \"  \"     :     \"       closeAttrs (  )  ;  \\ n \"  )  ,     \"       return    \"  ,    retName . toLowerCase ( Locale . US )  ,     \"  _  ( this ,     \"  ,    isInline ( className ,    retName )  ,     \"  )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else", "if    (  ( params . length )     =  =     1  )     {", "puts (  0  ,     \" String   selector )     {  \"  )  ;", "puts ( indent ,     \"       return   setSelector (  \"  ,    methodName ,     \"  (  )  ,    selector )  ;  \\ n \"  ,     \"  }  \"  )  ;", "} else    {", ". throwUnhandled ( className ,    method )  ;", "}", "}", "METHOD_END"], "methodName": ["genNewElementMethod"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "HamletGen . LOG . info (  \" Generating    {  }    using    {  }    and    {  }  \"  ,    new   Object [  ]  {    outputName ,    specClass ,    implClass    }  )  ;", "out    =    new   PrintWriter (  ( outputName    +     \"  . java \"  )  ,     \" UTF -  8  \"  )  ;", "hamlet    =    basename ( outputName )  ;", "String   pkg    =    pkgName ( outputPkg ,    implClass . getPackage (  )  . getName (  )  )  ;", "puts (  0  ,     \"  /  /    Generated   by   HamletGen .    Do   NOT   edit !  \\ n \"  ,     \" package    \"  ,    pkg ,     \"  ;  \\ n \"  ,     \" import   PrintWriter ;  \\ n \"  ,     \" import   EnumSet ;  \\ n \"  ,     \" import   static   *  ;  \\ n \"  ,     \" import   static    \"  ,    implClass . getName (  )  ,     \"  . EOpt .  *  ;  \\ n \"  ,     \" import   SubView ;  \"  )  ;", "String   implClassName    =    implClass . getSimpleName (  )  ;", "if    (  !  ( implClass . getPackage (  )  . getName (  )  . equals ( pkg )  )  )     {", "puts (  0  ,     \" import    \"  ,    implClass . getName (  )  ,     '  ;  '  )  ;", "}", "puts (  0  ,     \"  \\ n \"  ,     \" public   class    \"  ,    hamlet ,     \"    extends    \"  ,    implClassName ,     \"    implements    \"  ,    specClass . getSimpleName (  )  ,     \"  .  _ Html    {  \\ n \"  ,     \"       public    \"  ,    hamlet ,     \"  ( PrintWriter   out ,    int   nestLevel ,  \"  ,     \"    boolean   wasInline )     {  \\ n \"  ,     \"             super ( out ,    nestLevel ,    wasInline )  ;  \\ n \"  ,     \"        }  \\ n \\ n \"  ,     \"       static   EnumSet < EOpt >    opt ( boolean   endTag ,    boolean   inline ,     \"  ,     \" boolean   pre )     {  \\ n \"  ,     \"             EnumSet < EOpt >    opts    =    of ( ENDTAG )  ;  \\ n \"  ,     \"             if    (  ! endTag )    opts . remove ( ENDTAG )  ;  \\ n \"  ,     \"             if    ( inline )    opts . add ( INLINE )  ;  \\ n \"  ,     \"             if    ( pre )    opts . add ( PRE )  ;  \\ n \"  ,     \"             return   opts ;  \\ n \"  ,     \"        }  \"  )  ;", "initLut ( specClass )  ;", "genImpl ( specClass ,    implClassName ,     1  )  ;", "HamletGen . LOG . info (  \" Generating    {  }    methods \"  ,    hamlet )  ;", "genMethods ( hamlet ,    top ,     1  )  ;", "puts (  0  ,     \"  }  \"  )  ;", "out . close (  )  ;", "HamletGen . LOG . info (  \" Wrote    {  }    bytes   to    {  }  . java \"  ,    bytes ,    outputName )  ;", "}", "METHOD_END"], "methodName": ["generate"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "if    ( type   instanceof   Class <  ?  >  )     {", "retu    (  ( Class <  ?  >  )     ( type )  )  . getSimpleName (  )  ;", "}", "ParameterizedType   pt    =     (  ( ParameterizedType )     ( type )  )  ;", "retu    (  (  (  (  ( Class <  ?  >  )     ( pt . getRawType (  )  )  )  . getSimpleName (  )  )     +     \"  <  \"  )     +     (  (  ( Class <  ?  >  )     ( pt . getActualTypeArguments (  )  [  0  ]  )  )  . getSimpleName (  )  )  )     +     \"  >  \"  ;", "}", "METHOD_END"], "methodName": ["getTypeName"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "for    ( int   i    =     0  ;    i    <    indent ;     +  + i )     {", "out . print (  \"        \"  )  ;", "bytes    +  =     2  ;", "}", "}", "METHOD_END"], "methodName": ["indent"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "endTagOptional . clear (  )  ;", "inlineElements . clear (  )  ;", "for    ( Class <  ?  >    cls    :    spec . getClasses (  )  )     {", "Annotation   a    =    cls . getAnnotation ( Spec . Element . class )  ;", "if    (  ( a    !  =    null )     &  &     (  !  (  (  ( Spec . Element )     ( a )  )  . endTag (  )  )  )  )     {", "endTagOptional . add ( cls . getSimpleName (  )  )  ;", "}", "if    ( cls . getSimpleName (  )  . equals (  \" Inline \"  )  )     {", "for    ( Method   method    :    cls . getMethods (  )  )     {", "String   retName    =    method . getReturnType (  )  . getSimpleName (  )  ;", "if    ( isElement ( retName )  )     {", "inlineElements . add ( retName )  ;", "}", "}", "}", "}", "}", "METHOD_END"], "methodName": ["initLut"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "return   HamletGen . elementRegex . matcher ( s )  . matches (  )  ;", "}", "METHOD_END"], "methodName": ["isElement"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  (  ( container . equals (  \" BODY \"  )  )     |  |     ( container . equals ( hamlet )  )  )     |  |     ( container . equals (  \" HEAD \"  )  )  )     |  |     ( container . equals (  \" HTML \"  )  )  )     &  &     (  (  ( className . equals (  \" INS \"  )  )     |  |     ( className . equals (  \" DEL \"  )  )  )     |  |     ( className . equals (  \" SCRIPT \"  )  )  )  )     {", "return   false ;", "}", "return   inlineElements . contains ( className )  ;", "}", "METHOD_END"], "methodName": ["isInline"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "CommandLine   cmd    =    new   GnuParser (  )  . parse ( HamletGen . opts ,    args )  ;", "if    ( cmd . hasOption (  \" help \"  )  )     {", "new   HelpFormatter (  )  . printHelp (  \" Usage :    hbgen    [ OPTIONS ]  \"  ,    HamletGen . opts )  ;", "return ;", "}", "Class <  ?  >    specClass    =    HamletSpec . class ;", "Class <  ?  >    implClass    =    HamletImpl . class ;", "String   outputClass    =     \" HamletTmp \"  ;", "String   outputPackage    =    implClass . getPackage (  )  . getName (  )  ;", "if    ( cmd . hasOption (  \" spec - class \"  )  )     {", "specClass    =    Class . forName ( cmd . getOptionValue (  \" spec - class \"  )  )  ;", "}", "if    ( cmd . hasOption (  \" impl - class \"  )  )     {", "implClass    =    Class . forName ( cmd . getOptionValue (  \" impl - class \"  )  )  ;", "}", "if    ( cmd . hasOption (  \" output - class \"  )  )     {", "outputClass    =    cmd . getOptionValue (  \" output - class \"  )  ;", "}", "if    ( cmd . hasOption (  \" output - package \"  )  )     {", "outputPackage    =    cmd . getOptionValue (  \" output - package \"  )  ;", "}", "new   HamletGen (  )  . generate ( specClass ,    implClass ,    outputClass ,    outputPackage )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "return    (  !  ( eleName . equals (  \" SCRIPT \"  )  )  )     &  &     (  !  ( eleName . equals (  \" STYLE \"  )  )  )  ;", "}", "METHOD_END"], "methodName": ["needsEscaping"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "if    (  ( pkg    =  =    null )     |  |     ( pkg . isEmpty (  )  )  )", "return   defaultPkg ;", "return   pkg ;", "}", "METHOD_END"], "methodName": ["pkgName"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "echo ( indent ,    args )  ;", "out . println (  )  ;", "+  +  ( bytes )  ;", "}", "METHOD_END"], "methodName": ["puts"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "throw   new   WebAppException (  (  (  (  \" Unhandled    \"     +    className )     +     \"  #  \"  )     +    method )  )  ;", "}", "METHOD_END"], "methodName": ["throwUnhandled"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletGen"}, {"methodBody": ["METHOD_START", "{", "return   out ;", "}", "METHOD_END"], "methodName": ["getWriter"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  ( opts . contains ( HamletImpl . EOpt . INLINE )  )     &  &     ( wasInline )  )     {", "return ;", "}", "if    ( wasInline )     {", "out . println (  )  ;", "}", "wasInline    =     ( opts . contains ( HamletImpl . EOpt . INLINE )  )     |  |     ( opts . contains ( HamletImpl . EOpt . PRE )  )  ;", "for    ( int   i    =     0  ;    i    <     ( nestLevel )  ;     +  + i )     {", "out . print ( HamletImpl . INDENT _ CHARS )  ;", "}", "+  +  ( indents )  ;", "}", "METHOD_END"], "methodName": ["indent"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "return   nestLevel ;", "}", "METHOD_END"], "methodName": ["nestLevel"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "String [  ]    result    =    new   String [  ]  {    null ,    null    }  ;", "Iterable < String >    rs    =     . SS . split ( selector )  ;", "Iterator < String >    it    =    rs . iterator (  )  ;", "if    ( it . hasNext (  )  )     {", "String   maybeId    =    it . next (  )  ;", "if    (  ( maybeId . charAt (  0  )  )     =  =     '  #  '  )     {", "result [  . S _ ID ]     =    maybeId . substring (  1  )  ;", "if    ( it . hasNext (  )  )     {", "result [  . S _ CLASS ]     =     . SJ . join ( Iterables . skip ( rs ,     1  )  )  ;", "}", "} else    {", "result [  . S _ CLASS ]     =     . SJ . join ( rs )  ;", "}", "return   result ;", "}", "throw   new   WebAppException (  (  \" Error   parsing   selector :     \"     +    selector )  )  ;", "}", "METHOD_END"], "methodName": ["parseSelector"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "sb . setLength (  0  )  ;", "sbpend (  '     '  ) pend ( name )  ;", "if    ( value    !  =    null )     {", "sbpend (  \"  =  \\  \"  \"  ) pend ( StringEseUtils . eseHtml ( value )  ) pend (  \"  \\  \"  \"  )  ;", "}", "out . print ( sb . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["printAttr"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "if    (  !  ( opts . contains ( HamletImpl . EOpt . ENDTAG )  )  )     {", "return ;", "}", "if    (  !  ( opts . contains ( HamletImpl . EOpt . PRE )  )  )     {", "indent ( opts )  ;", "} else    {", "wasInline    =    opts . contains ( HamletImpl . EOpt . INLINE )  ;", "}", "sb . setLength (  0  )  ;", "out . print ( sb . append (  \"  <  /  \"  )  . append ( name )  . append (  '  >  '  )  . toString (  )  )  ;", "if    (  !  ( opts . contains ( HamletImpl . EOpt . INLINE )  )  )     {", "out . println (  )  ;", "}", "}", "METHOD_END"], "methodName": ["printEndTag"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "indent ( opts )  ;", "sb . setLength (  0  )  ;", "out . print ( sbpend (  '  <  '  ) pend ( name )  . toString (  )  )  ;", "}", "METHOD_END"], "methodName": ["printStartTag"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "return   root ( name ,    EnumSet . of ( HamletImpl . EOpt . ENDTAG )  )  ;", "}", "METHOD_END"], "methodName": ["root"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "return   new   HamletImpl . Generic < T >  ( name ,    null ,    opts )  ;", "}", "METHOD_END"], "methodName": ["root"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( hr . dsWith (  \"  . css \"  )  )     {", "$ r (  \" stylht \"  )  ;", "}", "$ hr ( hr )  ;", "rurn", "}", "METHOD_END"], "methodName": ["setLinkHref"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "if    ( src . dsWith (  \"  . js \"  )  )     {", "$ typ \" tt / javascript \"  )  ;", "}", "$ src ( src )  ;", "rurn", "}", "METHOD_END"], "methodName": ["setScriptSrc"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "String [  ]    res    =    HamletImpl . parseSelector ( selector )  ;", "if    (  ( res [ HamletImpl . S _ ID ]  )     !  =    null )     {", "e .  $ id ( res [ HamletImpl . S _ ID ]  )  ;", "}", "if    (  ( res [ HamletImpl . S _ CLASS ]  )     !  =    null )     {", "e .  $ class ( res [ HamletImpl . S _ CLASS ]  )  ;", "}", "return   e ;", "}", "METHOD_END"], "methodName": ["setSelector"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "wasInline    =    state ;", "}", "METHOD_END"], "methodName": ["setWasInline"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "indent ( EnumSet . of ( HamletImpl . EOpt . ENDTAG )  )  ;", "sb . setLength (  0  )  ;", "out . print ( sb . append (  '  [  '  )  . append ( cls . getName (  )  )  . append (  '  ]  '  )  . toString (  )  )  ;", "out . println (  )  ;", "}", "METHOD_END"], "methodName": ["subView"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "return   wasInline ;", "}", "METHOD_END"], "methodName": ["wasInline"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.HamletImpl"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   out    =    spy ( new   PrintWriter ( System . out )  )  ;", "return   new    ( out ,     0  ,    false )  ;", "}", "METHOD_END"], "methodName": ["newHamlet"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "Hamlet   h    =    TestHamlet . newHamlet (  )  . meta _ http (  \" Content - type \"  ,     \" text / html ;    charset = utf -  8  \"  )  . title (  \" test   enum   attrs \"  )  . link (  )  .  $ rel (  \" stylesheet \"  )  .  $ media ( EnumSet . of ( HamletSpec . Media . screen ,    HamletSpec . Media . print )  )  .  $ type (  \" text / css \"  )  .  $ href (  \" style . css \"  )  .  _  (  )  . link (  )  .  $ rel ( EnumSet . of ( HamletSpec . LinkType . index ,    HamletSpec . LinkType . start )  )  .  $ href (  \" index . html \"  )  .  _  (  )  ;", "h . div (  \"  # content \"  )  .  _  (  \" content \"  )  .  _  (  )  ;", "PrintWriter   out    =    h . getWriter (  )  ;", "out . flush (  )  ;", "assertEquals (  0  ,    h . nestLevel )  ;", "verify ( out )  . print (  \"    media =  \\  \" screen ,    print \\  \"  \"  )  ;", "verify ( out )  . print (  \"    rel =  \\  \" start   index \\  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testEnumAttrs"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "Hamlet   h    =    TestHamlet . newHamlet (  )  . title (  \" test \"  )  . h 1  (  \" heading    1  \"  )  . p (  \"  # id . class \"  )  . b (  \" hello \"  )  . em (  \" world !  \"  )  .  _  (  )  . div (  \"  # footer \"  )  .  _  (  \" Brought   to   you   by \"  )  . a (  \" http :  /  / hostname /  \"  ,     \" Somebody \"  )  .  _  (  )  ;", "PrintWriter   out    =    h . getWriter (  )  ;", "out . flush (  )  ;", "assertEquals (  0  ,    h . nestLevel )  ;", "verify ( out )  . print (  \"  < title \"  )  ;", "verify ( out )  . print (  \" test \"  )  ;", "verify ( out )  . print (  \"  <  / title >  \"  )  ;", "verify ( out )  . print (  \"  < h 1  \"  )  ;", "verify ( out )  . print (  \" heading    1  \"  )  ;", "verify ( out )  . print (  \"  <  / h 1  >  \"  )  ;", "verify ( out )  . print (  \"  < p \"  )  ;", "verify ( out )  . print (  \"    id =  \\  \" id \\  \"  \"  )  ;", "verify ( out )  . print (  \"    class =  \\  \" class \\  \"  \"  )  ;", "verify ( out )  . print (  \"  < b \"  )  ;", "verify ( out )  . print (  \" hello \"  )  ;", "verify ( out )  . print (  \"  <  / b >  \"  )  ;", "verify ( out )  . print (  \"  < em \"  )  ;", "verify ( out )  . print (  \" world !  \"  )  ;", "verify ( out )  . print (  \"  <  / em >  \"  )  ;", "verify ( out )  . print (  \"  < div \"  )  ;", "verify ( out )  . print (  \"    id =  \\  \" footer \\  \"  \"  )  ;", "verify ( out )  . print (  \" Brought   to   you   by \"  )  ;", "verify ( out )  . print (  \"  < a \"  )  ;", "verify ( out )  . print (  \"    href =  \\  \" http :  /  / hostname /  \\  \"  \"  )  ;", "verify ( out )  . print (  \" Somebody \"  )  ;", "verify ( out )  . print (  \"  <  / a >  \"  )  ;", "verify ( out )  . print (  \"  <  / div >  \"  )  ;", "verify ( out ,    never (  )  )  . print (  \"  <  / p >  \"  )  ;", "}", "METHOD_END"], "methodName": ["testHamlet"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "Hamlet   h    =    TestHamlet . newHamlet (  )  . div (  )  . i (  \" inline   before   pre \"  )  . pre (  )  .  _  (  \" pre   text 1  \\ npre   text 2  \"  )  . i (  \" inline   in   pre \"  )  .  _  (  \" pre   text   after   inline \"  )  .  _  (  )  . i (  \" inline   after   pre \"  )  .  _  (  )  ;", "PrintWriter   out    =    h . getWriter (  )  ;", "out . flush (  )  ;", "assertEquals (  5  ,    h . indents )  ;", "}", "METHOD_END"], "methodName": ["testPreformatted"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "Hamlet   h    =    TestHamlet . newHamlet (  )  . script (  \" a . js \"  )  . script (  \" b . js \"  )  . style (  \" h 1     {    font - size :     1  .  2 em    }  \"  )  ;", "PrintWriter   out    =    h . getWriter (  )  ;", "out . flush (  )  ;", "assertEquals (  0  ,    h . nestLevel )  ;", "verify ( out ,    times (  2  )  )  . print (  \"    type =  \\  \" text / javascript \\  \"  \"  )  ;", "verify ( out )  . print (  \"    type =  \\  \" text / css \\  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testScriptStyle"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "Hamlet   h    =    TestHamlet . newHamlet (  )  . title (  \" test   sub - views \"  )  . div (  \"  # view 1  \"  )  .  _  ( TestHamlet . TestView 1  . class )  .  _  (  )  . div (  \"  # view 2  \"  )  .  _  ( TestHamlet . TestView 2  . class )  .  _  (  )  ;", "PrintWriter   out    =    h . getWriter (  )  ;", "out . flush (  )  ;", "assertEquals (  0  ,    h . nestLevel )  ;", "verify ( out )  . print (  (  (  \"  [  \"     +     ( TestHamlet . TestView 1  . class . getName (  )  )  )     +     \"  ]  \"  )  )  ;", "verify ( out )  . print (  (  (  \"  [  \"     +     ( TestHamlet . TestView 2  . class . getName (  )  )  )     +     \"  ]  \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testSubViews"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "Hamlet   h    =    TestHamlet . newHamlet (  )  . title (  \" test   table \"  )  . link (  \" style . css \"  )  ;", "HamletSpec . TABLE   t    =    h . table (  \"  # id \"  )  ;", "for    ( int   i    =     0  ;    i    <     3  ;     +  + i )     {", "t . tr (  )  . td (  \"  1  \"  )  . td (  \"  2  \"  )  .  _  (  )  ;", "}", "t .  _  (  )  ;", "PrintWriter   out    =    h . getWriter (  )  ;", "out . flush (  )  ;", "assertEquals (  0  ,    h . nestLevel )  ;", "verify ( out )  . print (  \"  < table \"  )  ;", "verify ( out )  . print (  \"  <  / table >  \"  )  ;", "verify ( out ,    never (  )  )  . print (  \"  <  / td >  \"  )  ;", "verify ( out ,    never (  )  )  . print (  \"  <  / tr >  \"  )  ;", "}", "METHOD_END"], "methodName": ["testTable"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamlet"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   out    =    spy ( new   PrintWriter ( System . out )  )  ;", "hi    =    new    ( out ,     0  ,    false )  ;", "hi . root (  \" start \"  )  .  _ attr (  \" name \"  ,     \" value \"  )  .  _  (  \" start   text \"  )  . elem (  \" sub \"  )  .  _ attr (  \" name \"  ,     \" value \"  )  .  _  (  \" sub   text \"  )  .  _  (  )  . elem (  \" sub 1  \"  )  .  _ noEndTag (  )  .  _ attr (  \" boolean \"  ,    null )  .  _  (  \" sub 1 text \"  )  .  _  (  )  .  _  (  \" start   text 2  \"  )  . elem (  \" pre \"  )  .  _ pre (  )  .  _  (  \" pre   text \"  )  . elem (  \" i \"  )  .  _ inline (  )  .  _  (  \" inline \"  )  .  _  (  )  .  _  (  )  . elem (  \" i \"  )  .  _ inline (  )  .  _  (  \" inline   after   pre \"  )  .  _  (  )  .  _  (  \" start   text 3  \"  )  . elem (  \" sub 2  \"  )  .  _  (  \" sub 2 text \"  )  .  _  (  )  . elem (  \" sub 3  \"  )  .  _ noEndTag (  )  .  _  (  \" sub 3 text \"  )  .  _  (  )  . elem (  \" sub 4  \"  )  .  _ noEndTag (  )  . elem (  \" i \"  )  .  _ inline (  )  .  _  (  \" inline \"  )  .  _  (  )  .  _  (  \" sub 4 text \"  )  .  _  (  )  .  _  (  )  ;", "out . flush (  )  ;", "assertEquals (  0  ,    hi . nestLevel )  ;", "assertEquals (  2  0  ,    hi . indents )  ;", "verify ( out )  . print (  \"  < start \"  )  ;", "verify ( out ,    times (  2  )  )  . print (  \"    name =  \\  \" value \\  \"  \"  )  ;", "verify ( out )  . print (  \"    boolean \"  )  ;", "verify ( out )  . print (  \"  <  / start >  \"  )  ;", "verify ( out ,    never (  )  )  . print (  \"  <  / sub 1  >  \"  )  ;", "verify ( out ,    never (  )  )  . print (  \"  <  / sub 3  >  \"  )  ;", "verify ( out ,    never (  )  )  . print (  \"  <  / sub 4  >  \"  )  ;", "}", "METHOD_END"], "methodName": ["testGeneric"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamletImpl"}, {"methodBody": ["METHOD_START", "{", "HamletSpec . LINK   link    =    mock ( HamletSpec . LINK . class )  ;", ". setLinkHref ( link ,     \" uri \"  )  ;", ". setLinkHref ( link ,     \" style . css \"  )  ;", "verify ( link )  .  $ href (  \" uri \"  )  ;", "verify ( link )  .  $ rel (  \" stylesheet \"  )  ;", "verify ( link )  .  $ href (  \" style . css \"  )  ;", "verifyNoMoreInteractions ( link )  ;", "}", "METHOD_END"], "methodName": ["testSetLinkHref"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamletImpl"}, {"methodBody": ["METHOD_START", "{", "HamletSpec . SCRIPT   script    =    mock ( HamletSpec . SCRIPT . class )  ;", ". setScriptSrc ( script ,     \" uri \"  )  ;", ". setScriptSrc ( script ,     \" script . js \"  )  ;", "verify ( script )  .  $ src (  \" uri \"  )  ;", "verify ( script )  .  $ type (  \" text / javascript \"  )  ;", "verify ( script )  .  $ src (  \" script . js \"  )  ;", "verifyNoMoreInteractions ( script )  ;", "}", "METHOD_END"], "methodName": ["testSetScriptSrc"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamletImpl"}, {"methodBody": ["METHOD_START", "{", "HamletSpec . CoreAttrs   e    =    mock ( HamletSpec . CoreAttrs . class )  ;", ". setSelector ( e ,     \"  # id . class \"  )  ;", "verify ( e )  .  $ id (  \" id \"  )  ;", "verify ( e )  .  $ class (  \" class \"  )  ;", "HamletSpec . H 1    t    =    mock ( HamletSpec . H 1  . class )  ;", ". setSelector ( t ,     \"  # id . class \"  )  .  _  (  \" heading \"  )  ;", "verify ( t )  .  $ id (  \" id \"  )  ;", "verify ( t )  .  $ class (  \" class \"  )  ;", "verify ( t )  .  _  (  \" heading \"  )  ;", "}", "METHOD_END"], "methodName": ["testSetSelector"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestHamletImpl"}, {"methodBody": ["METHOD_START", "{", "HamletImpl . parseSelector (  \"  \"  )  ;", "}", "METHOD_END"], "methodName": ["testMissingAll"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector"}, {"methodBody": ["METHOD_START", "{", "String [  ]    res    =    HamletImpl . parseSelector (  \"  # id \"  )  ;", "assertEquals (  \" id \"  ,    res [ HamletImpl . S _ ID ]  )  ;", "assertNull ( res [ HamletImpl . S _ CLASS ]  )  ;", "}", "METHOD_END"], "methodName": ["testMissingClass"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector"}, {"methodBody": ["METHOD_START", "{", "String [  ]    res    =    HamletImpl . parseSelector (  \"  . class \"  )  ;", "assertNull ( res [ HamletImpl . S _ ID ]  )  ;", "assertEquals (  \" class \"  ,    res [ HamletImpl . S _ CLASS ]  )  ;", "}", "METHOD_END"], "methodName": ["testMissingId"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector"}, {"methodBody": ["METHOD_START", "{", "String [  ]    res    =    HamletImpl . parseSelector (  \"  # id . class 1  . class 2  \"  )  ;", "assertEquals (  \" id \"  ,    res [ HamletImpl . S _ ID ]  )  ;", "assertEquals (  \" class 1    class 2  \"  ,    res [ HamletImpl . S _ CLASS ]  )  ;", "}", "METHOD_END"], "methodName": ["testMultiClass"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector"}, {"methodBody": ["METHOD_START", "{", "String [  ]    res    =    HamletImpl . parseSelector (  \"  # id . class \"  )  ;", "assertEquals (  \" id \"  ,    res [ HamletImpl . S _ ID ]  )  ;", "assertEquals (  \" class \"  ,    res [ HamletImpl . S _ CLASS ]  )  ;", "}", "METHOD_END"], "methodName": ["testNormal"], "fileName": "org.apache.hadoop.yarn.webapp.hamlet.TestParseSelector"}, {"methodBody": ["METHOD_START", "{", "int   bufferSize    =     6  5  5  3  6  ;", "char [  ]    cbuf    =    new   char [ bufferSize ]  ;", "boolean   foundLog    =    false ;", "String   logType    =    logReader . nextLog (  )  ;", "while    ( logType    !  =    null )     {", "if    (  (  ( desirType    =  =    null )     |  |     ( desirType . isEmpty (  )  )  )     |  |     ( desirType . equals ( logType )  )  )     {", "long   logLength    =    logReader . getCurrentLogLength (  )  ;", "if    ( foundLog )     {", "html . pre (  )  .  _  (  \"  \\ n \\ n \"  )  .  _  (  )  ;", "}", "html . p (  )  .  _  (  (  \" Log   Type :     \"     +    logType )  )  .  _  (  )  ;", "html . p (  )  .  _  (  (  \" Log   Length :     \"     +     ( Long . toString ( logLength )  )  )  )  .  _  (  )  ;", "long   start    =     (  ( logLimits . start )     <     0  )     ?    logLength    +     ( logLimits . start )     :    logLimits . start ;", "start    =     ( start    <     0  )     ?     0     :    start ;", "start    =     ( start    >    logLength )     ?    logLength    :    start ;", "long   end    =     (  ( logLimits . end )     <     0  )     ?    logLength    +     ( logLimits . end )     :    logLimits . end ;", "end    =     ( end    <     0  )     ?     0     :    end ;", "end    =     ( end    >    logLength )     ?    logLength    :    end ;", "end    =     ( end    <    start )     ?    start    :    end ;", "long   toRead    =    end    -    start ;", "if    ( toRead    <    logLength )     {", "html . p (  )  .  _  (  (  (  (  (  \" Showing    \"     +    toRead )     +     \"    bytes   of    \"  )     +    logLength )     +     \"    total .    Click    \"  )  )  . a ( url (  \" logs \"  ,     $  ( YarnWebParams . NM _ NODENAME )  ,     $  ( YarnWebParams . CONTAINER _ ID )  ,     $  ( YarnWebParams . ENTITY _ STRING )  ,     $  ( YarnWebParams . APP _ OWNER )  ,    logType ,     \"  ? start =  0  \"  )  ,     \" here \"  )  .  _  (  \"    for   the   full   log .  \"  )  .  _  (  )  ;", "}", "long   totalSkipped    =     0  ;", "while    ( totalSkipped    <    start )     {", "long   ret    =    logReader . skip (  ( start    -    totalSkipped )  )  ;", "if    ( ret    <     0  )     {", "throw   new   IOException (  \" Premature   EOF   from   container   log \"  )  ;", "}", "totalSkipped    +  =    ret ;", "}", "int   len    =     0  ;", "int   currentToRead    =     ( toRead    >    bufferSize )     ?    bufferSize    :     (  ( int )     ( toRead )  )  ;", "Hamlet . PRE < Hamlet >    pre    =    html . pre (  )  ;", "while    (  ( toRead    >     0  )     &  &     (  ( len    =    logReader . read ( cbuf ,     0  ,    currentToRead )  )     >     0  )  )     {", "pre .  _  ( new   String ( cbuf ,     0  ,    len )  )  ;", "toRead    =    toRead    -    len ;", "currentToRead    =     ( toRead    >    bufferSize )     ?    bufferSize    :     (  ( int )     ( toRead )  )  ;", "}", "pre .  _  (  )  ;", "foundLog    =    true ;", "}", "logType    =    logReader . nextLog (  )  ;", "}", "return   foundLog ;", "}", "METHOD_END"], "methodName": ["readContainerLogs"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "String   appOwner    =     $  ( YarnWebParams . APP _ OWNER )  ;", "if    (  ( appOwner    =  =    null )     |  |     ( appOwner . isEmpty (  )  )  )     {", "html . h 1  (  )  .  _  (  \" Cannot   get   container   logs   without   an   app   owner \"  )  .  _  (  )  ;", "}", "return   appOwner ;", "}", "METHOD_END"], "methodName": ["verifyAndGetAppOwner"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "String   containerIdStr    =     $  ( YarnWebParams . CONTAINER _ ID )  ;", "if    (  ( containerIdStr    =  =    null )     |  |     ( containerIdStr . isEmpty (  )  )  )     {", "html . h 1  (  )  .  _  (  \" Cannot   get   container   logs   without   a   ContainerId \"  )  .  _  (  )  ;", "return   null ;", "}", "ContainerId   containerId    =    null ;", "try    {", "containerId    =    ConverterUtils . toContainerId ( containerIdStr )  ;", "}    catch    ( IllegalArgumentException   e )     {", "html . h 1  (  )  .  _  (  (  \" Cannot   get   container   logs   for   invalid   containerId :     \"     +    containerIdStr )  )  .  _  (  )  ;", "return   null ;", "}", "return   containerId ;", "}", "METHOD_END"], "methodName": ["verifyAndGetContainerId"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "long   start    =     -  4  0  9  6  ;", "long   end    =    Long . MAX _ VALUE ;", "boolean   isValid    =    true ;", "String   startStr    =     $  (  \" start \"  )  ;", "if    (  ( startStr    !  =    null )     &  &     (  !  ( startStr . isEmpty (  )  )  )  )     {", "try    {", "start    =    Long . parseLong ( startStr )  ;", "}    catch    ( NumberFormatException   e )     {", "isValid    =    false ;", "html . h 1  (  )  .  _  (  (  \" Invalid   log   start   value :     \"     +    startStr )  )  .  _  (  )  ;", "}", "}", "String   endStr    =     $  (  \" end \"  )  ;", "if    (  ( endStr    !  =    null )     &  &     (  !  ( endStr . isEmpty (  )  )  )  )     {", "try    {", "end    =    Long . parseLong ( endStr )  ;", "}    catch    ( NumberFormatException   e )     {", "isValid    =    false ;", "html . h 1  (  )  .  _  (  (  \" Invalid   log   end   value :     \"     +    endStr )  )  .  _  (  )  ;", "}", "}", "if    (  ! isValid )     {", "return   null ;", "}", ". LogLimits   limits    =    new    . LogLimits (  )  ;", "limits . start    =    start ;", "limits . end    =    end ;", "return   limits ;", "}", "METHOD_END"], "methodName": ["verifyAndGetLogLimits"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "String   nodeIdStr    =     $  ( YarnWebParams . NM _ NODENAME )  ;", "if    (  ( nodeIdStr    =  =    null )     |  |     ( nodeIdStr . isEmpty (  )  )  )     {", "html . h 1  (  )  .  _  (  \" Cannot   get   container   logs   without   a   NodeId \"  )  .  _  (  )  ;", "return   null ;", "}", "NodeId   nodeId    =    null ;", "try    {", "nodeId    =    ConverterUtils . toNodeId ( nodeIdStr )  ;", "}    catch    ( IllegalArgumentException   e )     {", "html . h 1  (  )  .  _  (  (  \" Cannot   get   container   logs .    Invalid   nodeId :     \"     +    nodeIdStr )  )  .  _  (  )  ;", "return   null ;", "}", "return   nodeId ;", "}", "METHOD_END"], "methodName": ["verifyAndGetNodeId"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlock"}, {"methodBody": ["METHOD_START", "{", "return   params ;", "}", "METHOD_END"], "methodName": ["moreParams"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlockForTest"}, {"methodBody": ["METHOD_START", "{", "return   request ;", "}", "METHOD_END"], "methodName": ["request"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlockForTest"}, {"methodBody": ["METHOD_START", "{", "this . request    =    request ;", "}", "METHOD_END"], "methodName": ["setRequest"], "fileName": "org.apache.hadoop.yarn.webapp.log.AggregatedLogsBlockForTest"}, {"methodBody": ["METHOD_START", "{", "TestWebAppTests . LOG . info (  \" request :     {  }  \"  ,    req )  ;", "TestWebAppTests . LOG . info (  \" response :     {  }  \"  ,    res )  ;", "TestWebAppTests . LOG . info (  \" writer :     {  }  \"  ,    out )  ;", "}", "METHOD_END"], "methodName": ["logInstances"], "fileName": "org.apache.hadoop.yarn.webapp.test.TestWebAppTests"}, {"methodBody": ["METHOD_START", "{", "TestWebAppTests . Bar   bar    =    new   TestWebAppTests . Bar (  )  ;", "Injector   injector    =    WebAppTests . createMockInjector ( TestWebAppTests . Foo . class ,    bar )  ;", "logInstances ( injector . getInstance ( HttpServletRequest . class )  ,    injector . getInstance ( HttpServletResponse . class )  ,    injector . getInstance ( HttpServletResponse . class )  . getWriter (  )  )  ;", "assertSame ( bar ,    injector . getInstance ( TestWebAppTests . Foo . class )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateInjector"], "fileName": "org.apache.hadoop.yarn.webapp.test.TestWebAppTests"}, {"methodBody": ["METHOD_START", "{", "final   TestWebAppTests . FooBar   foobar    =    new   TestWebAppTests . FooBar (  )  ;", "TestWebAppTests . Bar   bar    =    new   TestWebAppTests . Bar (  )  ;", "Injector   injector    =    WebAppTests . createMockInjector ( TestWebAppTests . Foo . class ,    bar ,    new   AbstractModule (  )     {", "@ Override", "protected   void   configure (  )     {", "bind ( TestWebAppTests . Bar . class )  . toInstance ( foobar )  ;", "}", "}  )  ;", "assertNotSame ( bar ,    injector . getInstance ( TestWebAppTests . Bar . class )  )  ;", "assertSame ( foobar ,    injector . getInstance ( TestWebAppTests . Bar . class )  )  ;", "}", "METHOD_END"], "methodName": ["testCreateInjector2"], "fileName": "org.apache.hadoop.yarn.webapp.test.TestWebAppTests"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( this )  ;", "HttpServletRequest   req    =    injector . getInstance ( HttpServletRequest . class )  ;", "HttpServletResponse   res    =    injector . getInstance ( HttpServletResponse . class )  ;", "String   val    =    req . getParameter (  \" foo \"  )  ;", "PrintWriter   out    =    res . getWriter (  )  ;", "out . println (  \" Hello   world !  \"  )  ;", "logInstances ( req ,    res ,    out )  ;", "assertSame ( req ,    injector . getInstance ( HttpServletRequest . class )  )  ;", "assertSame ( res ,    injector . getInstance ( HttpServletResponse . class )  )  ;", "assertSame ( this ,    injector . getInstance (  . class )  )  ;", "verify ( req )  . getParameter (  \" foo \"  )  ;", "verify ( res )  . getWriter (  )  ;", "verify ( out )  . println (  \" Hello   world !  \"  )  ;", "}", "METHOD_END"], "methodName": ["testInstances"], "fileName": "org.apache.hadoop.yarn.webapp.test.TestWebAppTests"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( this )  ;", "assertSame ( injector . getInstance (  . ScopeTest . class )  ,    injector . getInstance (  . ScopeTest . class )  )  ;", "}", "METHOD_END"], "methodName": ["testRequestScope"], "fileName": "org.apache.hadoop.yarn.webapp.test.TestWebAppTests"}, {"methodBody": ["METHOD_START", "{", "return   WebAppTests . createMockInjector (  (  ( Class < T >  )     ( impl . getClass (  )  )  )  ,    impl )  ;", "}", "METHOD_END"], "methodName": ["createMockInjector"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "return   Guice . createInjector ( new   AbstractModule (  )     {", "final   PrintWriter   writer    =    spy ( new   PrintWriter ( System . out )  )  ;", "final   HttpServletRequest   request    =    createRequest (  )  ;", "final   HttpServletResponse   response    =    createResponse (  )  ;", "@ Override", "protected   void   configure (  )     {", "if    ( api    !  =    null )     {", "bind ( api )  . toInstance ( impl )  ;", "}", "bindScope ( RequestScoped . class ,    SINGLETON )  ;", "if    ( modules    !  =    null )     {", "for    ( Module   module    :    modules )     {", "install ( module )  ;", "}", "}", "}", "@ Provides", "HttpServletRequest   request (  )     {", "return   request ;", "}", "@ Provides", "HttpServletResponse   response (  )     {", "return   response ;", "}", "@ Provides", "PrintWriter   writer (  )     {", "return   writer ;", "}", "HttpServletRequest   createRequest (  )     {", "return   mock ( HttpServletRequest . class )  ;", "}", "HttpServletResponse   createResponse (  )     {", "try    {", "HttpServletResponse   res    =    mock ( HttpServletResponse . class )  ;", "when ( res . getWriter (  )  )  . thenReturn ( writer )  ;", "return   res ;", "}    catch    ( Exception   e )     {", "throw   new   Exception ( e )  ;", "}", "}", "}  )  ;", "}", "METHOD_END"], "methodName": ["createMockInjector"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "HttpServletResponse   res    =    injector . getInstance ( HttpServletResponse . class )  ;", "try    {", "res . getWriter (  )  . flush (  )  ;", "}    catch    ( Exception   e )     {", "throw   new   RuntimeException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["flushOutput"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "HttpServletResponse   res    =    injector . getInstance ( HttpServletResponse . class )  ;", "return   res . getWriter (  )  ;", "}", "METHOD_END"], "methodName": ["getPrintWriter"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "return   WebAppTests . testBlock ( block ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["testBlock"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( api ,    impl ,    modules )  ;", "injector . getInstance ( block )  . renderPartial (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "return   injector ;", "}", "METHOD_END"], "methodName": ["testBlock"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "return   WebAppTests . testController ( ctrlr ,    methodName ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["testController"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "try    {", "Injector   injector    =     . createMockInjector ( api ,    impl ,    modules )  ;", "Method   method    =    ctrlr . getMethod ( methodName ,     (  ( Class <  ?  >  [  ]  )     ( null )  )  )  ;", "method . invoke ( injector . getInstance ( ctrlr )  ,     (  ( Object [  ]  )     ( null )  )  )  ;", "return   injector ;", "}    catch    ( Exception   e )     {", "throw   new   WebAppException ( e )  ;", "}", "}", "METHOD_END"], "methodName": ["testController"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "return   WebAppTests . testPage ( page ,    null ,    null )  ;", "}", "METHOD_END"], "methodName": ["testPage"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "return   WebAppTests . testPage ( page ,    api ,    impl ,    null ,    modules )  ;", "}", "METHOD_END"], "methodName": ["testPage"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( api ,    impl ,    modules )  ;", "View   view    =    injector . getInstance ( page )  ;", "if    ( params    !  =    null )     {", "for    ( Map . Entry < String ,    String >    entry    :    params . entrySet (  )  )     {", "view . set ( entry . getKey (  )  ,    entry . getValue (  )  )  ;", "}", "}", "view . render (  )  ;", "WebAppTests . flushOutput ( injector )  ;", "return   injector ;", "}", "METHOD_END"], "methodName": ["testPage"], "fileName": "org.apache.hadoop.yarn.webapp.test.WebAppTests"}, {"methodBody": ["METHOD_START", "{", "File   testDir    =    new   File ( System . getProperty (  \" test . build . data \"  ,     \" target / test - dir \"  )  )  ;", "Configuration   conf    =    new   Configuration (  )  ;", "final   String   ourUrl    =     (  (  ( JavaKeyStoreProvider . SCHEME _ NAME )     +     \"  :  /  / file /  \"  )     +    testDir )     +     \"  / test . jks \"  ;", "File   file    =    new   File ( testDir ,     \" test . jks \"  )  ;", "file . delete (  )  ;", "conf . set ( CREDENTIAL _ PROVIDER _ PATH ,    ourUrl )  ;", "CredentialProvider   provider    =    CredentialProviderFactory . getProviders ( conf )  . get (  0  )  ;", "char [  ]    keypass    =    new   char [  ]  {     ' k '  ,     ' e '  ,     ' y '  ,     ' p '  ,     ' a '  ,     ' s '  ,     ' s '     }  ;", "char [  ]    storepass    =    new   char [  ]  {     ' s '  ,     ' t '  ,     ' o '  ,     ' r '  ,     ' e '  ,     ' p '  ,     ' a '  ,     ' s '  ,     ' s '     }  ;", "char [  ]    trustpass    =    new   char [  ]  {     ' t '  ,     ' r '  ,     ' u '  ,     ' s '  ,     ' t '  ,     ' p '  ,     ' a '  ,     ' s '  ,     ' s '     }  ;", "assertEquals ( null ,    provider . getCredentialEntry (  . WEB _ APP _ KEY _ PASSWORD _ KEY )  )  ;", "assertEquals ( null ,    provider . getCredentialEntry (  . WEB _ APP _ KEYSTORE _ PASSWORD _ KEY )  )  ;", "assertEquals ( null ,    provider . getCredentialEntry (  . WEB _ APP _ TRUSTSTORE _ PASSWORD _ KEY )  )  ;", "try    {", "provider . createCredentialEntry (  . WEB _ APP _ KEY _ PASSWORD _ KEY ,    keypass )  ;", "provider . createCredentialEntry (  . WEB _ APP _ KEYSTORE _ PASSWORD _ KEY ,    storepass )  ;", "provider . createCredentialEntry (  . WEB _ APP _ TRUSTSTORE _ PASSWORD _ KEY ,    trustpass )  ;", "provider . flush (  )  ;", "}    catch    ( Exception   e )     {", "e . printStackTrace (  )  ;", "throw   e ;", "}", "assertArrayEquals ( keypass ,    provider . getCredentialEntry (  . WEB _ APP _ KEY _ PASSWORD _ KEY )  . getCredential (  )  )  ;", "assertArrayEquals ( storepass ,    provider . getCredentialEntry (  . WEB _ APP _ KEYSTORE _ PASSWORD _ KEY )  . getCredential (  )  )  ;", "assertArrayEquals ( trustpass ,    provider . getCredentialEntry (  . WEB _ APP _ TRUSTSTORE _ PASSWORD _ KEY )  . getCredential (  )  )  ;", "return   conf ;", "}", "METHOD_END"], "methodName": ["provisionCredentialsForSSL"], "fileName": "org.apache.hadoop.yarn.webapp.util.TestWebAppUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    provisionCredentialsForSSL (  )  ;", "Assert . assertEquals (  \" keypass \"  ,     . getPassword ( conf ,     . WEB _ APP _ KEY _ PASSWORD _ KEY )  )  ;", "Assert . assertEquals (  \" storepass \"  ,     . getPassword ( conf ,     . WEB _ APP _ KEYSTORE _ PASSWORD _ KEY )  )  ;", "Assert . assertEquals (  \" trustpass \"  ,     . getPassword ( conf ,     . WEB _ APP _ TRUSTSTORE _ PASSWORD _ KEY )  )  ;", "Assert . assertEquals ( null ,     . getPassword ( conf ,     \" invalid - alias \"  )  )  ;", "}", "METHOD_END"], "methodName": ["testGetPassword"], "fileName": "org.apache.hadoop.yarn.webapp.util.TestWebAppUtils"}, {"methodBody": ["METHOD_START", "{", "Configuration   conf    =    provisionCredentialsForSSL (  )  ;", ". TestBuilder   builder    =     (  (  . TestBuilder )     ( new    . TestBuilder (  )  )  )  ;", "builder    =     (  (  . TestBuilder )     ( WebAppUtils . loadSslConfiguration ( builder ,    conf )  )  )  ;", "String   keypass    =     \" keypass \"  ;", "String   storepass    =     \" storepass \"  ;", "String   trustpass    =     \" trustpass \"  ;", "assertEquals ( keypass ,     (  (  . TestBuilder )     ( builder )  )  . keypass )  ;", "assertEquals ( storepass ,     (  (  . TestBuilder )     ( builder )  )  . keystorePassword )  ;", "assertEquals ( trustpass ,     (  (  . TestBuilder )     ( builder )  )  . truststorePassword )  ;", "}", "METHOD_END"], "methodName": ["testLoadSslConfiguration"], "fileName": "org.apache.hadoop.yarn.webapp.util.TestWebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( YarnConfiguration . useHttps ( conf )  )     {", "return   conf . get ( TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ HTTPS _ ADDRESS )  ;", "} else    {", "return   conf . get ( TIMELINE _ SERVICE _ WEBAPP _ ADDRESS ,    DEFAULT _ TIMELINE _ SERVICE _ WEBAPP _ ADDRESS )  ;", "}", "}", "METHOD_END"], "methodName": ["getAHSWebAppURLWithoutScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  (  (  (  (  (  (  ( serverHttpAddress    =  =    null )     |  |     ( serverHttpAddress . isEmpty (  )  )  )     |  |     ( allocatedNode    =  =    null )  )     |  |     ( allocatedNode . isEmpty (  )  )  )     |  |     ( containerId    =  =    null )  )     |  |     ( containerId . isEmpty (  )  )  )     |  |     ( entity    =  =    null )  )     |  |     ( entity . isEmpty (  )  )  )     |  |     ( user    =  =    null )  )     |  |     ( user . isEmpty (  )  )  )     {", "return   null ;", "}", "return   StringHelper . PATH _ JOINER . join ( serverHttpAddress ,     \" licationhistory \"  ,     \" logs \"  ,    allocatedNode ,    containerId ,    entity ,    user )  ;", "}", "METHOD_END"], "methodName": ["getAggregatedLogURL"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "return   YarnConfiguration . useHttps ( conf )     ?    WebAppUtils . HTTPS _ PREFIX    :    WebAppUtils . HTTP _ PREFIX ;", "}", "METHOD_END"], "methodName": ["getHttpSchemePrefix"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( YarnConfiguration . useHttps ( conf )  )     {", "return   conf . get ( NM _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ NM _ WEBAPP _ HTTPS _ ADDRESS )  ;", "} else    {", "return   conf . get ( NM _ WEBAPP _ ADDRESS ,    DEFAULT _ NM _ WEBAPP _ ADDRESS )  ;", "}", "}", "METHOD_END"], "methodName": ["getNMWebAppURLWithoutScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "String   password    =    null ;", "try    {", "char [  ]    passchars    =    conf . getPassword ( alias )  ;", "if    ( passchars    !  =    null )     {", "password    =    new   String ( passchars )  ;", "}", "}    catch    ( IOException   ioe )     {", "password    =    null ;", "}", "return   password ;", "}", "METHOD_END"], "methodName": ["getPassword"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "String   addr    =    conf . get ( PROXY _ ADDRESS )  ;", "if    (  ( addr    =  =    null )     |  |     ( addr . isEmpty (  )  )  )     {", "addr    =     . getResolvedRMWebAppURLWithoutScheme ( conf )  ;", "}", "return   addr ;", "}", "METHOD_END"], "methodName": ["getProxyHostAndPort"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "List < String >    addrs    =    new   ArrayList < String >  (  )  ;", "String   proxyAddr    =    conf . get ( PROXY _ ADDRESS )  ;", "if    (  ( proxyAddr    =  =    null )     |  |     ( proxyAddr . isEmpty (  )  )  )     {", "if    ( HAUtil . isHAEnabled ( conf )  )     {", "List < String >    haAddrs    =    RMHAUtils . getRMHAWebappAddresses ( new   conf . YarnConfiguration ( conf )  )  ;", "for    ( String   addr    :    haAddrs )     {", "try    {", "InetSocketAddress   socketAddr    =    NetUtils . createSocketAddr ( addr )  ;", "addrs . add ( WebAppUtils . getResolvedAddress ( socketAddr )  )  ;", "}    catch    ( IllegalArgumentException   e )     {", "}", "}", "}", "if    ( addrs . isEmpty (  )  )     {", "addrs . add ( WebAppUtils . getResolvedRMWebAppURLWithoutScheme ( conf )  )  ;", "}", "} else    {", "addrs . add ( proxyAddr )  ;", "}", "return   addrs ;", "}", "METHOD_END"], "methodName": ["getProxyHostsAndPortsForAmFilter"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "return    ( WebAppUtils . getHttpSchemePrefix ( conf )  )     +     ( WebAppUtils . getRMWebAppURLWithoutScheme ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["getRMWebAppURLWithScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( YarnConfiguration . useHttps ( conf )  )     {", "return   conf . get ( RM _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ HTTPS _ ADDRESS )  ;", "} else    {", "return   conf . get ( RM _ WEBAPP _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ ADDRESS )  ;", "}", "}", "METHOD_END"], "methodName": ["getRMWebAppURLWithoutScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "address    =    NetUtils . getConnectAddress ( address )  ;", "StringBuilder   sb    =    new   StringBuilder (  )  ;", "InetAddress   resolved    =    address . getAddress (  )  ;", "if    (  (  ( resolved    =  =    null )     |  |     ( resolved . isAnyLocalAddress (  )  )  )     |  |     ( resolved . isLoopbackAddress (  )  )  )     {", "String   lh    =    address . getHostName (  )  ;", "try    {", "lh    =    InetAddress . getLocalHost (  )  . getCanonicalHostName (  )  ;", "}    catch    ( UnknownHostException   e )     {", "}", "sb . append ( lh )  ;", "} else    {", "sb . append ( address . getHostName (  )  )  ;", "}", "sb . append (  \"  :  \"  )  . append ( address . getPort (  )  )  ;", "return   sb . toString (  )  ;", "}", "METHOD_END"], "methodName": ["getResolvedAddress"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "return    ( WebAppUtils . getHttpSchemePrefix ( conf )  )     +     ( WebAppUtils . getResolvedRMWebAppURLWithoutScheme ( conf )  )  ;", "}", "METHOD_END"], "methodName": ["getResolvedRMWebAppURLWithScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "return   WebAppUtils . getResolvedRMWebAppURLWithoutScheme ( conf ,     ( YarnConfiguration . useHttps ( conf )     ?    Policy . HTTPS _ ONLY    :    Policy . HTTP _ ONLY )  )  ;", "}", "METHOD_END"], "methodName": ["getResolvedRMWebAppURLWithoutScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "InetSocketAddress   address    =    null ;", "if    ( httpPolicy    =  =     ( Policy . HTTPS _ ONLY )  )     {", "address    =    conf . getSocketAddr ( RM _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ HTTPS _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ HTTPS _ PORT )  ;", "} else    {", "address    =    conf . getSocketAddr ( RM _ WEBAPP _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ ADDRESS ,    DEFAULT _ RM _ WEBAPP _ PORT )  ;", "}", "return    . getResolvedAddress ( address )  ;", "}", "METHOD_END"], "methodName": ["getResolvedRMWebAppURLWithoutScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  (  (  (  (  ( nodeHttpAddress    =  =    null )     |  |     ( nodeHttpAddress . isEmpty (  )  )  )     |  |     ( containerId    =  =    null )  )     |  |     ( containerId . isEmpty (  )  )  )     |  |     ( user    =  =    null )  )     |  |     ( user . isEmpty (  )  )  )     {", "retu   null ;", "}", "retu   StringHelper . PATH _ JOINER . join ( nodeHttpAddress ,     \" node \"  ,     \" containerlogs \"  ,    containerId ,    user )  ;", "}", "METHOD_END"], "methodName": ["getRunningLogURL"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  ( url . indexOf (  \"  :  /  /  \"  )  )     >     0  )     {", "return   url ;", "} else    {", "return   smePrefix    +    url ;", "}", "}", "METHOD_END"], "methodName": ["getURLWithScheme"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "String   host    =    conf . getTrimmed ( hostProperty )  ;", "if    (  ( host    !  =    null )     &  &     (  !  ( host . isEmpty (  )  )  )  )     {", "if    ( wRLWithoutScheme . contains (  \"  :  \"  )  )     {", "wRLWithoutScheme    =     ( host    +     \"  :  \"  )     +     ( wRLWithoutScheme . split (  \"  :  \"  )  [  1  ]  )  ;", "} else    {", "throw   new   YarnRuntimeException (  (  \" wRLWithoutScheme   must   include   port   specification   but   doesn ' t :     \"     +    wRLWithoutScheme )  )  ;", "}", "}", "return   wRLWithoutScheme ;", "}", "METHOD_END"], "methodName": ["getWebAppBindURL"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "return   WebAppUtils . loadSslConfiguration ( builder ,    null )  ;", "}", "METHOD_END"], "methodName": ["loadSslConfiguration"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( sslConf    =  =    null )     {", "sslConf    =    new   Configuration ( false )  ;", "}", "boolean   needsClientAuth    =    YarnConfiguration . YARN _ SSL _ CLIENT _ HTTPS _ NEED _ AUTH _ DEFAULT ;", "sslConf . addResource ( YARN _ SSL _ SERVER _ RESOURCE _ DEFAULT )  ;", "return   builder . needsClientAuth ( needsClientAuth )  . keyPassword (  . getPassword ( sslConf ,     . WEB _ APP _ KEY _ PASSWORD _ KEY )  )  . keyStore ( sslConf . get (  \" ssl . server . keystore . location \"  )  ,     . getPassword ( sslConf ,     . WEB _ APP _ KEYSTORE _ PASSWORD _ KEY )  ,    sslConf . get (  \" ssl . server . keystore . type \"  ,     \" jks \"  )  )  . trustStore ( sslConf . get (  \" ssl . server . truststore . location \"  )  ,     . getPassword ( sslConf ,     . WEB _ APP _ TRUSTSTORE _ PASSWORD _ KEY )  ,    sslConf . get (  \" ssl . server . truststore . type \"  ,     \" jks \"  )  )  ;", "}", "METHOD_END"], "methodName": ["loadSslConfiguration"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    ( YarnConfiguration . useHttps ( conf )  )     {", "conf . set ( NM _ WEBAPP _ HTTPS _ ADDRESS ,     (  ( hostName    +     \"  :  \"  )     +    port )  )  ;", "} else    {", "conf . set ( NM _ WEBAPP _ ADDRESS ,     (  ( hostName    +     \"  :  \"  )     +    port )  )  ;", "}", "}", "METHOD_END"], "methodName": ["setNMWebAppHostNameAndPort"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "String   resolvedAddress    =     ( hostname    +     \"  :  \"  )     +    port ;", "if    ( YConfiguration . useHttps ( conf )  )     {", "conf . set ( RM _ WEBAPP _ HTTPS _ ADDRESS ,    resolvedAddress )  ;", "} else    {", "conf . set ( RM _ WEBAPP _ ADDRESS ,    resolvedAddress )  ;", "}", "}", "METHOD_END"], "methodName": ["setRMWebAppHostnameAndPort"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "String   hostname    =    WebAppUtils . getRMWebAppURLWithoutScheme ( conf )  ;", "hostname    =     ( hostname . contains (  \"  :  \"  )  )     ?    hostname . substring (  0  ,    hostname . indexOf (  \"  :  \"  )  )     :    hostname ;", "WebAppUtils . setRMWebAppHostnameAndPort ( conf ,    hostname ,    port )  ;", "}", "METHOD_END"], "methodName": ["setRMWebAppPort"], "fileName": "org.apache.hadoop.yarn.webapp.util.WebAppUtils"}, {"methodBody": ["METHOD_START", "{", "if    (  !  (  $  ( Params . ERROR _ DETAILS )  . isEmpty (  )  )  )     {", "return    $  ( Params . ERROR _ DETAILS )  ;", "}", "if    (  ( error (  )  )     !  =    null )     {", "return    . toStackTrace ( error (  )  ,     (  1  0  2  4     *     6  4  )  )  ;", "}", "return    \" No   exception   was   thrown .  \"  ;", "}", "METHOD_END"], "methodName": ["errorDetails"], "fileName": "org.apache.hadoop.yarn.webapp.view.ErrorPage"}, {"methodBody": ["METHOD_START", "{", "CharArrayWriter   buffer    =    new   CharArrayWriter (  (  8     *     1  0  2  4  )  )  ;", "e . printStackTrace ( new   PrintWriter ( buffer )  )  ;", "return    ( buffer . size (  )  )     <    cutoff    ?    buffer . toString (  )     :    buffer . toString (  )  . substring (  0  ,    cutoff )  ;", "}", "METHOD_END"], "methodName": ["toStackTrace"], "fileName": "org.apache.hadoop.yarn.webapp.view.ErrorPage"}, {"methodBody": ["METHOD_START", "{", "return   Html . validIdRe . matcher ( id )  . matches (  )  ;", "}", "METHOD_END"], "methodName": ["isValidId"], "fileName": "org.apache.hadoop.yarn.webapp.view.Html"}, {"methodBody": ["METHOD_START", "{", "if    (  ( block )     =  =    null )     {", "block    =    new    . Block ( writer (  )  ,    context (  )  . nestLevel (  )  ,    context (  )  . wasInline (  )  )  ;", "}", "return   block ;", "}", "METHOD_END"], "methodName": ["block"], "fileName": "org.apache.hadoop.yarn.webapp.view.HtmlBlock"}, {"methodBody": ["METHOD_START", "{", "if    (  ( page )     =  =    null )     {", "page    =    new    . Page ( writer (  )  )  ;", "}", "return   page ;", "}", "METHOD_END"], "methodName": ["page"], "fileName": "org.apache.hadoop.yarn.webapp.view.HtmlPage"}, {"methodBody": ["METHOD_START", "{", "for    ( String   id    :    StringHelper . split (  $  ( JQueryUI . ACCORDION _ ID )  )  )     {", "if    ( Html . isValidId ( id )  )     {", "String   init    =     $  ( JQueryUI . initID ( JQueryUI . ACCORDION ,    id )  )  ;", "if    ( init . isEmpty (  )  )     {", "init    =     \"  { autoHeight :    false }  \"  ;", "}", "list . add ( StringHelper . join (  \"        $  (  '  #  \"  ,    id ,     \"  '  )  . accordion (  \"  ,    init ,     \"  )  ;  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["initAccordions"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "String   defaultInit    =     \"  { bJQueryUI :    true ,    sPaginationType :     ' full _ numbers '  }  \"  ;", "String   stateSaveInit    =     \" bStateSave    :    true ,     \"     +     (  (  (  \"  \\  \" fnStateSave \\  \"  :    function    ( oSettings ,    oData )     {     \"     +     \" sessionStorage . setItem (    oSettings . sTableId ,    JSON . stringify ( oData )     )  ;     }  ,     \"  )     +     \"  \\  \" fnStateLoad \\  \"  :    function    ( oSettings )     {     \"  )     +     \" return   JSON . parse (    sessionStorage . getItem ( oSettings . sTableId )     )  ;  }  ,     \"  )  ;", "for    ( String   id    :    StringHelper . split (  $  ( JQueryUI . DATATABLES _ ID )  )  )     {", "if    ( Html . isValidId ( id )  )     {", "String   init    =     $  ( JQueryUI . initID ( JQueryUI . DATATABLES ,    id )  )  ;", "if    ( init . isEmpty (  )  )     {", "init    =    defaultInit ;", "}", "int   pos    =     ( init . indexOf (  '  {  '  )  )     +     1  ;", "init    =    new   StringBuffer ( init )  . insert ( pos ,    stateSaveInit )  . toString (  )  ;", "list . add ( StringHelper . join ( id ,     \" DataTable    =        $  (  '  #  \"  ,    id ,     \"  '  )  . dataTable (  \"  ,    init ,     \"  )  . fnSetFilteringDelay (  1  8  8  )  ;  \"  )  )  ;", "String   postInit    =     $  ( JQueryUI . postInitID ( JQueryUI . DATATABLES ,    id )  )  ;", "if    (  !  ( postInit . isEmpty (  )  )  )     {", "list . add ( postInit )  ;", "}", "}", "}", "String   selector    =     $  ( JQueryUI . DATATABLES _ SELECTOR )  ;", "if    (  !  ( selector . isEmpty (  )  )  )     {", "String   init    =     $  ( JQueryUI . initSelector ( JQueryUI . DATATABLES )  )  ;", "if    ( init . isEmpty (  )  )     {", "init    =    defaultInit ;", "}", "int   pos    =     ( init . indexOf (  '  {  '  )  )     +     1  ;", "init    =    new   StringBuffer ( init )  . insert ( pos ,    stateSaveInit )  . toString (  )  ;", "list . add ( StringHelper . join (  \"        $  (  '  \"  ,    StringEscapeUtils . escapeJavaScript ( selector )  ,     \"  '  )  . dataTable (  \"  ,    init ,     \"  )  . fnSetFilteringDelay (  2  8  8  )  ;  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initDataTables"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "String   defaultInit    =     \"  { autoOpen :    false ,    show :    transfer ,    hide :    explode }  \"  ;", "for    ( String   id    :    StringHelper . split (  $  (  . DIALOG _ ID )  )  )     {", "if    ( Html . isValidId ( id )  )     {", "String   init    =     $  (  . initID (  . DIALOG ,    id )  )  ;", "if    ( init . isEmpty (  )  )     {", "init    =    defaultInit ;", "}", "String   opener    =     $  ( StringHelper . djoin (  . DIALOG ,    id ,     \" opener \"  )  )  ;", "list . add ( StringHelper . join (  \"        $  (  '  #  \"  ,    id ,     \"  '  )  . dialog (  \"  ,    init ,     \"  )  ;  \"  )  )  ;", "if    (  (  !  ( opener . isEmpty (  )  )  )     &  &     ( Html . isValidId ( opener )  )  )     {", "list . add ( StringHelper . join (  \"        $  (  '  #  \"  ,    opener ,     \"  '  )  . click ( function (  )     {     \"  ,     \"  $  (  '  #  \"  ,    id ,     \"  '  )  . dialog (  ' open '  )  ;    return   false ;     }  )  ;  \"  )  )  ;", "}", "}", "}", "String   selector    =     $  (  . DIALOG _ SELECTOR )  ;", "if    (  !  ( selector . isEmpty (  )  )  )     {", "String   init    =     $  (  . initSelector (  . DIALOG )  )  ;", "if    ( init . isEmpty (  )  )     {", "init    =    defaultInit ;", "}", "list . add ( StringHelper . join (  \"        $  (  '  \"  ,    StringEscapeUtils . escapeJavaScript ( selector )  ,     \"  '  )  . click ( function (  )     {     $  ( this )  . children (  '  . dialog '  )  . dialog (  \"  ,    init ,     \"  )  ;    return   false ;     }  )  ;  \"  )  )  ;", "}", "}", "METHOD_END"], "methodName": ["initDialogs"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . djoin ( name ,    id ,     \" init \"  )  ;", "}", "METHOD_END"], "methodName": ["initID"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "for    ( String   id    :    StringHelper . split (  $  ( JQueryUI . PROGRESSBAR _ ID )  )  )     {", "if    ( Html . isValidId ( id )  )     {", "String   init    =     $  ( JQueryUI . initID ( JQueryUI . PROGRESSBAR ,    id )  )  ;", "list . add ( StringHelper . join (  \"        $  (  '  #  \"  ,    id ,     \"  '  )  . progressbar (  \"  ,    init ,     \"  )  ;  \"  )  )  ;", "}", "}", "}", "METHOD_END"], "methodName": ["initProgressBars"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . djoin ( name ,     \" selector . init \"  )  ;", "}", "METHOD_END"], "methodName": ["initSelector"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "html . div (  \"  # jsnotice . ui - state - error \"  )  .  _  (  \" This   page   works   best   with   \"  )  .  _  (  )  ;", "html . script (  )  .  $ type (  \" text / javascript \"  )  .  _  (  \"  $  (  '  # jsnotice '  )  . hide (  )  ;  \"  )  .  _  (  )  ;", "}", "METHOD_END"], "methodName": ["jsnotice"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "return   StringHelper . djoin ( name ,    id ,     \" postinit \"  )  ;", "}", "METHOD_END"], "methodName": ["postInitID"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "return   new   StringBuilder (  \"  { bJQueryUI : true ,     \"  )  . append (  \" sPaginationType :     ' full _ numbers '  ,    iDisplayLength :  2  0  ,     \"  )  . append (  \" aLengthMenu :  [  2  0  ,     4  0  ,     6  0  ,     8  0  ,     1  0  0  ]  \"  )  ;", "}", "METHOD_END"], "methodName": ["tableInit"], "fileName": "org.apache.hadoop.yarn.webapp.view.JQueryUI"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . testPage ( ErrorPage . class )  ;", "}", "METHOD_END"], "methodName": ["testErrorPage"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestCommonViews"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testBlock ( FooterBlock . class )  ;", "}", "METHOD_END"], "methodName": ["testFooterBlock"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestCommonViews"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testBlock ( HeaderBlock . class )  ;", "}", "METHOD_END"], "methodName": ["testHeaderBlock"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestCommonViews"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . createMockInjector ( this )  ;", "ResponseInfo   info    =    injector . getInstance ( ResponseInfo . class )  ;", "}", "METHOD_END"], "methodName": ["testInfoBlock"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestCommonViews"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testBlock ( JQueryUI . class )  ;", "}", "METHOD_END"], "methodName": ["testJQueryUI"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestCommonViews"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testBlock ( TestHtmlBlock . ShortBlock . class )  ;", "}", "METHOD_END"], "methodName": ["testShortBlock"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestHtmlBlock"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testPage ( TestHtmlBlock . ShortPage . class )  ;", "}", "METHOD_END"], "methodName": ["testShortPage"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestHtmlBlock"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . testBlock ( TestHtmlBlock . TestBlock . class )  ;", "PrintWriter   out    =    injector . getInstance ( PrintWriter . class )  ;", "verify ( out )  . print (  \"    id =  \\  \" testid \\  \"  \"  )  ;", "verify ( out )  . print (  \" test   note \"  )  ;", "}", "METHOD_END"], "methodName": ["testUsual"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestHtmlBlock"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testPage ( TestHtmlPage . ShortView . class )  ;", "}", "METHOD_END"], "methodName": ["testShort"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestHtmlPage"}, {"methodBody": ["METHOD_START", "{", "Injector   injector    =    WebAppTests . testPage ( TestHtmlPage . TestView . class )  ;", "PrintWriter   out    =    injector . getInstance ( PrintWriter . class )  ;", "verify ( out )  . print (  \"    http - equiv =  \\  \" X - UA - Compatible \\  \"  \"  )  ;", "verify ( out )  . print (  \"    content =  \\  \" IE =  8  \\  \"  \"  )  ;", "verify ( out )  . print (  \"    http - equiv =  \\  \" Content - type \\  \"  \"  )  ;", "verify ( out )  . print ( String . format (  \"    content =  \\  \"  % s \\  \"  \"  ,    MimeType . HTML )  )  ;", "verify ( out )  . print (  \" test \"  )  ;", "verify ( out )  . print (  \"    id =  \\  \" testid \\  \"  \"  )  ;", "verify ( out )  . print (  \" test   note \"  )  ;", "}", "METHOD_END"], "methodName": ["testUsual"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestHtmlPage"}, {"methodBody": ["METHOD_START", "{", "TestInfoBlock . sw    =    new   StringWriter (  )  ;", "TestInfoBlock . pw    =    new   PrintWriter ( TestInfoBlock . sw )  ;", "}", "METHOD_END"], "methodName": ["setup"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestInfoBlock"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testBlock ( TestInfoBlock . JavaScriptInfoBlock . class )  ;", "TestInfoBlock . pw . flush (  )  ;", "String   output    =    TestInfoBlock . sw . toString (  )  ;", "assertFalse ( output . contains (  \"  < script >  \"  )  )  ;", "assertTrue ( output . contains ( TestInfoBlock . JAVASCRIPT _ ESCAPED )  )  ;", "}", "METHOD_END"], "methodName": ["testJavaScriptInfoBlock"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestInfoBlock"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testBlock ( TestInfoBlock . MultilineInfoBlock . class )  ;", "TestInfoBlock . pw . flush (  )  ;", "String   output    =    TestInfoBlock . sw . toString (  )  . replaceAll (  \"     +  \"  ,     \"     \"  )  ;", "String   expectedSinglelineData    =    String . format (  (  \"  < tr   class =  \\  \" odd \\  \"  >  % n \"     +     \"     < th >  % n   Single _ line _ value % n    < td >  % n   This   is   one   line .  % n \"  )  )  ;", "String   expectedMultilineData    =    String . format (  (  \"  < tr   class =  \\  \" even \\  \"  >  % n \"     +     (  (  \"     < th >  % n   Multiple _ line _ value % n    < td >  % n    < div >  % n \"     +     \"    This   is   first   line .  % n    <  / div >  % n    < div >  % n \"  )     +     \"    This   is   second   line .  % n    <  / div >  % n \"  )  )  )  ;", "assertTrue (  (  ( output . contains ( expectedSinglelineData )  )     &  &     ( output . contains ( expectedMultilineData )  )  )  )  ;", "}", "METHOD_END"], "methodName": ["testMultilineInfoBlock"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestInfoBlock"}, {"methodBody": ["METHOD_START", "{", "WebApps .  $ for (  \" test \"  )  . at (  8  8  8  8  )  . inDevMode (  )  . start (  )  . joinThread (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestTwoColumnCssPage"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testPage ( TwoColumnCssLayout . class )  ;", "}", "METHOD_END"], "methodName": ["shouldNotThrow"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestTwoColumnCssPage"}, {"methodBody": ["METHOD_START", "{", "WebApps .  $ for (  \" test \"  )  . at (  8  8  8  8  )  . inDevMode (  )  . start (  )  . joinThread (  )  ;", "}", "METHOD_END"], "methodName": ["main"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestTwoColumnLayout"}, {"methodBody": ["METHOD_START", "{", "WebAppTests . testPage ( TwoColumnLayout . class )  ;", "}", "METHOD_END"], "methodName": ["shouldNotThrow"], "fileName": "org.apache.hadoop.yarn.webapp.view.TestTwoColumnLayout"}, {"methodBody": ["METHOD_START", "{", "PrintWriter   out    =    writer (  )  ;", "f    ( Object   s    :    args )     {", "out . print ( s )  ;", "}", "}", "METHOD_END"], "methodName": ["echo"], "fileName": "org.apache.hadoop.yarn.webapp.view.TextView"}, {"methodBody": ["METHOD_START", "{", "echo ( args )  ;", "writer (  )  . println (  )  ;", "}", "METHOD_END"], "methodName": ["puts"], "fileName": "org.apache.hadoop.yarn.webapp.view.TextView"}, {"methodBody": ["METHOD_START", "{", "return   LipsumBlock . class ;", "}", "METHOD_END"], "methodName": ["content"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnCssLayout"}, {"methodBody": ["METHOD_START", "{", "return   FooterBlock . class ;", "}", "METHOD_END"], "methodName": ["footer"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnCssLayout"}, {"methodBody": ["METHOD_START", "{", "return   HeaderBlock . class ;", "}", "METHOD_END"], "methodName": ["header"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnCssLayout"}, {"methodBody": ["METHOD_START", "{", "return   NavBlock . class ;", "}", "METHOD_END"], "methodName": ["nav"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnCssLayout"}, {"methodBody": ["METHOD_START", "{", "return   LipsumBlock . class ;", "}", "METHOD_END"], "methodName": ["content"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnLayout"}, {"methodBody": ["METHOD_START", "{", "return   FooterBlock . class ;", "}", "METHOD_END"], "methodName": ["footer"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnLayout"}, {"methodBody": ["METHOD_START", "{", "return   HeaderBlock . class ;", "}", "METHOD_END"], "methodName": ["header"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnLayout"}, {"methodBody": ["METHOD_START", "{", "return   NavBlock . class ;", "}", "METHOD_END"], "methodName": ["nav"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnLayout"}, {"methodBody": ["METHOD_START", "{", "List < String >    styles    =    Lists . newArrayList (  )  ;", "styles . add ( StringHelper . join (  '  #  '  ,    tableId ,     \"  _ paginate   span    { font - weight : normal }  \"  )  )  ;", "styles . add ( StringHelper . join (  '  #  '  ,    tableId ,     \"     . progress    { width :  8 em }  \"  )  )  ;", "styles . add ( StringHelper . join (  '  #  '  ,    tableId ,     \"  _ processing    { top :  -  1  .  5 em ;    font - size :  1 em ;  \"  )  )  ;", "styles . add (  \"       color :  #  0  0  0  ;    background : rgba (  2  5  5  ,     2  5  5  ,     2  5  5  ,     0  .  8  )  }  \"  )  ;", "for    ( String   style    :    innerStyles )     {", "styles . add ( StringHelper . join (  '  #  '  ,    tableId ,     \"     \"  ,    style )  )  ;", "}", "html . style ( styles . toArray (  )  )  ;", "}", "METHOD_END"], "methodName": ["setTableStyles"], "fileName": "org.apache.hadoop.yarn.webapp.view.TwoColumnLayout"}]